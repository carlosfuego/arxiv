[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2505.09561v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09561v1",
                "updated": "2025-05-14T17:00:47Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    17,
                    0,
                    47,
                    2,
                    134,
                    0
                ],
                "published": "2025-05-14T17:00:47Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    17,
                    0,
                    47,
                    2,
                    134,
                    0
                ],
                "title": "Learning Long-Context Diffusion Policies via Past-Token Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Long-Context Diffusion Policies via Past-Token Prediction"
                },
                "summary": "Reasoning over long sequences of observations and actions is essential for\nmany robotic tasks. Yet, learning effective long-context policies from\ndemonstrations remains challenging. As context length increases, training\nbecomes increasingly expensive due to rising memory demands, and policy\nperformance often degrades as a result of spurious correlations. Recent methods\ntypically sidestep these issues by truncating context length, discarding\nhistorical information that may be critical for subsequent decisions. In this\npaper, we propose an alternative approach that explicitly regularizes the\nretention of past information. We first revisit the copycat problem in\nimitation learning and identify an opposite challenge in recent diffusion\npolicies: rather than over-relying on prior actions, they often fail to capture\nessential dependencies between past and future actions. To address this, we\nintroduce Past-Token Prediction (PTP), an auxiliary task in which the policy\nlearns to predict past action tokens alongside future ones. This regularization\nsignificantly improves temporal modeling in the policy head, with minimal\nreliance on visual representations. Building on this observation, we further\nintroduce a multistage training strategy: pre-train the visual encoder with\nshort contexts, and fine-tune the policy head using cached long-context\nembeddings. This strategy preserves the benefits of PTP while greatly reducing\nmemory and computational overhead. Finally, we extend PTP into a\nself-verification mechanism at test time, enabling the policy to score and\nselect candidates consistent with past actions during inference. Experiments\nacross four real-world and six simulated tasks demonstrate that our proposed\nmethod improves the performance of long-context diffusion policies by 3x and\naccelerates policy training by more than 10x.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning over long sequences of observations and actions is essential for\nmany robotic tasks. Yet, learning effective long-context policies from\ndemonstrations remains challenging. As context length increases, training\nbecomes increasingly expensive due to rising memory demands, and policy\nperformance often degrades as a result of spurious correlations. Recent methods\ntypically sidestep these issues by truncating context length, discarding\nhistorical information that may be critical for subsequent decisions. In this\npaper, we propose an alternative approach that explicitly regularizes the\nretention of past information. We first revisit the copycat problem in\nimitation learning and identify an opposite challenge in recent diffusion\npolicies: rather than over-relying on prior actions, they often fail to capture\nessential dependencies between past and future actions. To address this, we\nintroduce Past-Token Prediction (PTP), an auxiliary task in which the policy\nlearns to predict past action tokens alongside future ones. This regularization\nsignificantly improves temporal modeling in the policy head, with minimal\nreliance on visual representations. Building on this observation, we further\nintroduce a multistage training strategy: pre-train the visual encoder with\nshort contexts, and fine-tune the policy head using cached long-context\nembeddings. This strategy preserves the benefits of PTP while greatly reducing\nmemory and computational overhead. Finally, we extend PTP into a\nself-verification mechanism at test time, enabling the policy to score and\nselect candidates consistent with past actions during inference. Experiments\nacross four real-world and six simulated tasks demonstrate that our proposed\nmethod improves the performance of long-context diffusion policies by 3x and\naccelerates policy training by more than 10x."
                },
                "authors": [
                    {
                        "name": "Marcel Torne"
                    },
                    {
                        "name": "Andy Tang"
                    },
                    {
                        "name": "Yuejiang Liu"
                    },
                    {
                        "name": "Chelsea Finn"
                    }
                ],
                "author_detail": {
                    "name": "Chelsea Finn"
                },
                "author": "Chelsea Finn",
                "arxiv_comment": "Videos are available at https://long-context-dp.github.io",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09561v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09561v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.06738v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.06738v2",
                "updated": "2025-05-14T16:04:57Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    16,
                    4,
                    57,
                    2,
                    134,
                    0
                ],
                "published": "2025-05-10T19:06:37Z",
                "published_parsed": [
                    2025,
                    5,
                    10,
                    19,
                    6,
                    37,
                    5,
                    130,
                    0
                ],
                "title": "I Know What You Said: Unveiling Hardware Cache Side-Channels in Local\n  Large Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "I Know What You Said: Unveiling Hardware Cache Side-Channels in Local\n  Large Language Model Inference"
                },
                "summary": "Large Language Models (LLMs) that can be deployed locally have recently\ngained popularity for privacy-sensitive tasks, with companies such as Meta,\nGoogle, and Intel playing significant roles in their development. However, the\nsecurity of local LLMs through the lens of hardware cache side-channels remains\nunexplored. In this paper, we unveil novel side-channel vulnerabilities in\nlocal LLM inference: token value and token position leakage, which can expose\nboth the victim's input and output text, thereby compromising user privacy.\nSpecifically, we found that adversaries can infer the token values from the\ncache access patterns of the token embedding operation, and deduce the token\npositions from the timing of autoregressive decoding phases. To demonstrate the\npotential of these leaks, we design a novel eavesdropping attack framework\ntargeting both open-source and proprietary LLM inference systems. The attack\nframework does not directly interact with the victim's LLM and can be executed\nwithout privilege.\n  We evaluate the attack on a range of practical local LLM deployments (e.g.,\nLlama, Falcon, and Gemma), and the results show that our attack achieves\npromising accuracy. The restored output and input text have an average edit\ndistance of 5.2% and 17.3% to the ground truth, respectively. Furthermore, the\nreconstructed texts achieve average cosine similarity scores of 98.7% (input)\nand 98.0% (output).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) that can be deployed locally have recently\ngained popularity for privacy-sensitive tasks, with companies such as Meta,\nGoogle, and Intel playing significant roles in their development. However, the\nsecurity of local LLMs through the lens of hardware cache side-channels remains\nunexplored. In this paper, we unveil novel side-channel vulnerabilities in\nlocal LLM inference: token value and token position leakage, which can expose\nboth the victim's input and output text, thereby compromising user privacy.\nSpecifically, we found that adversaries can infer the token values from the\ncache access patterns of the token embedding operation, and deduce the token\npositions from the timing of autoregressive decoding phases. To demonstrate the\npotential of these leaks, we design a novel eavesdropping attack framework\ntargeting both open-source and proprietary LLM inference systems. The attack\nframework does not directly interact with the victim's LLM and can be executed\nwithout privilege.\n  We evaluate the attack on a range of practical local LLM deployments (e.g.,\nLlama, Falcon, and Gemma), and the results show that our attack achieves\npromising accuracy. The restored output and input text have an average edit\ndistance of 5.2% and 17.3% to the ground truth, respectively. Furthermore, the\nreconstructed texts achieve average cosine similarity scores of 98.7% (input)\nand 98.0% (output)."
                },
                "authors": [
                    {
                        "name": "Zibo Gao"
                    },
                    {
                        "name": "Junjie Hu"
                    },
                    {
                        "name": "Feng Guo"
                    },
                    {
                        "name": "Yixin Zhang"
                    },
                    {
                        "name": "Yinglong Han"
                    },
                    {
                        "name": "Siyuan Liu"
                    },
                    {
                        "name": "Haiyang Li"
                    },
                    {
                        "name": "Zhiqiang Lv"
                    }
                ],
                "author_detail": {
                    "name": "Zhiqiang Lv"
                },
                "author": "Zhiqiang Lv",
                "arxiv_comment": "Submitted for review in January 22, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.06738v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.06738v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "K.6.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10659v7",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10659v7",
                "updated": "2025-05-14T04:38:42Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    4,
                    38,
                    42,
                    2,
                    134,
                    0
                ],
                "published": "2024-11-16T01:39:44Z",
                "published_parsed": [
                    2024,
                    11,
                    16,
                    1,
                    39,
                    44,
                    5,
                    321,
                    0
                ],
                "title": "Spineless Traversal for Layout Invalidation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spineless Traversal for Layout Invalidation"
                },
                "summary": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty, and only those\nelements are processed to draw the next frame, dramatically reducing latency.\nHowever, the standard incremental layout algorithm must search the page for\ndirty elements, accessing auxiliary elements in the process. These auxiliary\nelements add cache misses and stalled cycles, and are responsible for a sizable\nfraction of all layout latency. We introduce a new, faster incremental layout\nalgorithm called Spineless Traversal. Spineless Traversal uses a\ncache-friendlier priority queue algorithm that avoids accessing auxiliary nodes\nand thus reduces cache traffic and stalls. This leads to dramatic speedups on\nthe most latency-critical interactions such as hovering, typing, and animation.\nMoreover, thanks to numerous low-level optimizations, Spineless Traversal is\ncompetitive across the whole spectrum of incremental layout workloads.\nSpineless Traversal is faster than the standard approach on 83.0% of 2216\nbenchmarks, with a mean speedup of 1.80x concentrated in the most\nlatency-critical interactions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty, and only those\nelements are processed to draw the next frame, dramatically reducing latency.\nHowever, the standard incremental layout algorithm must search the page for\ndirty elements, accessing auxiliary elements in the process. These auxiliary\nelements add cache misses and stalled cycles, and are responsible for a sizable\nfraction of all layout latency. We introduce a new, faster incremental layout\nalgorithm called Spineless Traversal. Spineless Traversal uses a\ncache-friendlier priority queue algorithm that avoids accessing auxiliary nodes\nand thus reduces cache traffic and stalls. This leads to dramatic speedups on\nthe most latency-critical interactions such as hovering, typing, and animation.\nMoreover, thanks to numerous low-level optimizations, Spineless Traversal is\ncompetitive across the whole spectrum of incremental layout workloads.\nSpineless Traversal is faster than the standard approach on 83.0% of 2216\nbenchmarks, with a mean speedup of 1.80x concentrated in the most\nlatency-critical interactions."
                },
                "authors": [
                    {
                        "name": "Marisa Kirisame"
                    },
                    {
                        "name": "Tiezhi Wang"
                    },
                    {
                        "name": "Pavel Panchekha"
                    }
                ],
                "author_detail": {
                    "name": "Pavel Panchekha"
                },
                "author": "Pavel Panchekha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10659v7",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10659v7",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18599v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18599v2",
                "updated": "2025-05-14T04:22:24Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    4,
                    22,
                    24,
                    2,
                    134,
                    0
                ],
                "published": "2025-03-24T11:56:50Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    11,
                    56,
                    50,
                    0,
                    83,
                    0
                ],
                "title": "Oaken: Fast and Efficient LLM Serving with Online-Offline Hybrid KV\n  Cache Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Oaken: Fast and Efficient LLM Serving with Online-Offline Hybrid KV\n  Cache Quantization"
                },
                "summary": "Modern Large Language Model serving system batches multiple requests to\nachieve high throughput, while batching attention operations is challenging,\nrendering memory bandwidth a critical bottleneck. The community relies on\nhigh-end GPUs with multiple high-bandwidth memory channels. Unfortunately,\nHBM's high bandwidth often comes at the expense of limited memory capacity,\nwhich reduces core utilization and increases costs. Recent advancements\nenabling longer contexts for LLMs have substantially increased the key-value\ncache size, further intensifying the pressures on memory capacity. The\nliterature has explored KV cache quantization techniques, which commonly use\nlow bitwidth for most values, selectively using higher bitwidth for outlier\nvalues. While this approach helps achieve high accuracy and low bitwidth\nsimultaneously, it comes with the limitation that cost for online outlier\ndetection is excessively high, negating the advantages. We propose Oaken, an\nacceleration solution that achieves high accuracy and high performance\nsimultaneously through co-designing algorithm and hardware. To effectively find\na sweet spot in the accuracy-performance trade-off space of KV cache\nquantization, Oaken employs an online-offline hybrid approach, setting outlier\nthresholds offline, which are then used to determine the quantization scale\nonline. To translate the proposed algorithmic technique into tangible\nperformance gains, Oaken also comes with custom quantization engines and memory\nmanagement units that can be integrated with any LLM accelerators. We built an\nOaken accelerator on top of an LLM accelerator, LPU, and conducted a\ncomprehensive evaluation. Our experiments show that for a batch size of 256,\nOaken achieves up to 1.58x throughput improvement over NVIDIA A100 GPU,\nincurring a minimal accuracy loss of only 0.54\\% on average, compared to\nstate-of-the-art KV cache quantization techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern Large Language Model serving system batches multiple requests to\nachieve high throughput, while batching attention operations is challenging,\nrendering memory bandwidth a critical bottleneck. The community relies on\nhigh-end GPUs with multiple high-bandwidth memory channels. Unfortunately,\nHBM's high bandwidth often comes at the expense of limited memory capacity,\nwhich reduces core utilization and increases costs. Recent advancements\nenabling longer contexts for LLMs have substantially increased the key-value\ncache size, further intensifying the pressures on memory capacity. The\nliterature has explored KV cache quantization techniques, which commonly use\nlow bitwidth for most values, selectively using higher bitwidth for outlier\nvalues. While this approach helps achieve high accuracy and low bitwidth\nsimultaneously, it comes with the limitation that cost for online outlier\ndetection is excessively high, negating the advantages. We propose Oaken, an\nacceleration solution that achieves high accuracy and high performance\nsimultaneously through co-designing algorithm and hardware. To effectively find\na sweet spot in the accuracy-performance trade-off space of KV cache\nquantization, Oaken employs an online-offline hybrid approach, setting outlier\nthresholds offline, which are then used to determine the quantization scale\nonline. To translate the proposed algorithmic technique into tangible\nperformance gains, Oaken also comes with custom quantization engines and memory\nmanagement units that can be integrated with any LLM accelerators. We built an\nOaken accelerator on top of an LLM accelerator, LPU, and conducted a\ncomprehensive evaluation. Our experiments show that for a batch size of 256,\nOaken achieves up to 1.58x throughput improvement over NVIDIA A100 GPU,\nincurring a minimal accuracy loss of only 0.54\\% on average, compared to\nstate-of-the-art KV cache quantization techniques."
                },
                "authors": [
                    {
                        "name": "Minsu Kim"
                    },
                    {
                        "name": "Seongmin Hong"
                    },
                    {
                        "name": "RyeoWook Ko"
                    },
                    {
                        "name": "Soongyu Choi"
                    },
                    {
                        "name": "Hunjong Lee"
                    },
                    {
                        "name": "Junsoo Kim"
                    },
                    {
                        "name": "Joo-Young Kim"
                    },
                    {
                        "name": "Jongse Park"
                    }
                ],
                "author_detail": {
                    "name": "Jongse Park"
                },
                "author": "Jongse Park",
                "arxiv_doi": "10.1145/3695053.3731019",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3695053.3731019",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.18599v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18599v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "16 pages, 14 figures, and 4 tables",
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09081v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09081v1",
                "updated": "2025-05-14T02:29:46Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    2,
                    29,
                    46,
                    2,
                    134,
                    0
                ],
                "published": "2025-05-14T02:29:46Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    2,
                    29,
                    46,
                    2,
                    134,
                    0
                ],
                "title": "SALM: A Multi-Agent Framework for Language Model-Driven Social Network\n  Simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SALM: A Multi-Agent Framework for Language Model-Driven Social Network\n  Simulation"
                },
                "summary": "Contemporary approaches to agent-based modeling (ABM) of social systems have\ntraditionally emphasized rule-based behaviors, limiting their ability to\ncapture nuanced dynamics by moving beyond predefined rules and leveraging\ncontextual understanding from LMs of human social interaction. This paper\npresents SALM (Social Agent LM Framework), a novel approach for integrating\nlanguage models (LMs) into social network simulation that achieves\nunprecedented temporal stability in multi-agent scenarios. Our primary\ncontributions include: (1) a hierarchical prompting architecture enabling\nstable simulation beyond 4,000 timesteps while reducing token usage by 73%, (2)\nan attention-based memory system achieving 80% cache hit rates (95% CI [78%,\n82%]) with sub-linear memory growth of 9.5%, and (3) formal bounds on\npersonality stability. Through extensive validation against SNAP ego networks,\nwe demonstrate the first LLM-based framework capable of modeling long-term\nsocial phenomena while maintaining empirically validated behavioral fidelity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contemporary approaches to agent-based modeling (ABM) of social systems have\ntraditionally emphasized rule-based behaviors, limiting their ability to\ncapture nuanced dynamics by moving beyond predefined rules and leveraging\ncontextual understanding from LMs of human social interaction. This paper\npresents SALM (Social Agent LM Framework), a novel approach for integrating\nlanguage models (LMs) into social network simulation that achieves\nunprecedented temporal stability in multi-agent scenarios. Our primary\ncontributions include: (1) a hierarchical prompting architecture enabling\nstable simulation beyond 4,000 timesteps while reducing token usage by 73%, (2)\nan attention-based memory system achieving 80% cache hit rates (95% CI [78%,\n82%]) with sub-linear memory growth of 9.5%, and (3) formal bounds on\npersonality stability. Through extensive validation against SNAP ego networks,\nwe demonstrate the first LLM-based framework capable of modeling long-term\nsocial phenomena while maintaining empirically validated behavioral fidelity."
                },
                "authors": [
                    {
                        "name": "Gaurav Koley"
                    }
                ],
                "author_detail": {
                    "name": "Gaurav Koley"
                },
                "author": "Gaurav Koley",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09081v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09081v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09040v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09040v1",
                "updated": "2025-05-14T00:41:44Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    0,
                    41,
                    44,
                    2,
                    134,
                    0
                ],
                "published": "2025-05-14T00:41:44Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    0,
                    41,
                    44,
                    2,
                    134,
                    0
                ],
                "title": "RT-cache: Efficient Robot Trajectory Retrieval System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RT-cache: Efficient Robot Trajectory Retrieval System"
                },
                "summary": "This paper introduces RT-cache, a novel trajectorymemory pipeline that\naccelerates real-world robot inference by leveraging big-data retrieval and\nlearning from experience. While modern Vision-Language-Action (VLA) models can\nhandle diverse robotic tasks, they often incur high per-step inference costs,\nresulting in significant latency, sometimes minutes per task. In contrast,\nRT-cache stores a large-scale Memory of previously successful robot\ntrajectories and retrieves relevant multistep motion snippets, drastically\nreducing inference overhead. By integrating a Memory Builder with a Trajectory\nRetrieval, we develop an efficient retrieval process that remains tractable\neven for extremely large datasets. RT-cache flexibly accumulates real-world\nexperiences and replays them whenever the current scene matches past states,\nadapting quickly to new or unseen environments with only a few additional\nsamples. Experiments on the Open-X Embodiment Dataset and other real-world data\ndemonstrate that RT-cache completes tasks both faster and more successfully\nthan a baseline lacking retrieval, suggesting a practical, data-driven solution\nfor real-time manipulation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces RT-cache, a novel trajectorymemory pipeline that\naccelerates real-world robot inference by leveraging big-data retrieval and\nlearning from experience. While modern Vision-Language-Action (VLA) models can\nhandle diverse robotic tasks, they often incur high per-step inference costs,\nresulting in significant latency, sometimes minutes per task. In contrast,\nRT-cache stores a large-scale Memory of previously successful robot\ntrajectories and retrieves relevant multistep motion snippets, drastically\nreducing inference overhead. By integrating a Memory Builder with a Trajectory\nRetrieval, we develop an efficient retrieval process that remains tractable\neven for extremely large datasets. RT-cache flexibly accumulates real-world\nexperiences and replays them whenever the current scene matches past states,\nadapting quickly to new or unseen environments with only a few additional\nsamples. Experiments on the Open-X Embodiment Dataset and other real-world data\ndemonstrate that RT-cache completes tasks both faster and more successfully\nthan a baseline lacking retrieval, suggesting a practical, data-driven solution\nfor real-time manipulation."
                },
                "authors": [
                    {
                        "name": "Owen Kwon"
                    },
                    {
                        "name": "Abraham George"
                    },
                    {
                        "name": "Alison Bartsch"
                    },
                    {
                        "name": "Amir Barati Farimani"
                    }
                ],
                "author_detail": {
                    "name": "Amir Barati Farimani"
                },
                "author": "Amir Barati Farimani",
                "arxiv_comment": "9 pages, 5 figures. Submitted to an IEEE robotics conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09040v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09040v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08958v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08958v1",
                "updated": "2025-05-13T20:51:59Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    20,
                    51,
                    59,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T20:51:59Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    20,
                    51,
                    59,
                    1,
                    133,
                    0
                ],
                "title": "Adaptive Entanglement Generation for Quantum Routing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Entanglement Generation for Quantum Routing"
                },
                "summary": "Entanglement generation in long-distance quantum networks is a difficult\nprocess due to resource limitations and the probabilistic nature of\nentanglement swapping. To maximize success probability, existing quantum\nrouting algorithms employ computationally expensive solutions (e.g., linear\nprogramming) to determine which links to entangle and use for end-to-end\nentanglement generation. Such optimization methods, however, cannot meet the\ndelay requirements of real-world quantum networks, necessitating swift yet\nefficient real-time optimization models. In this paper, we propose\nreinforcement learning (RL)-based models to determine which links to entangle\nand proactively swap to meet connection requests. We show that the proposed\nRL-based approach is 20x faster compared to linear programming. Moreover, we\nshow that one can take advantage of the longevity of entanglements to (i) cache\nentangled links for future use and (ii) proactively swap entanglement on\nhigh-demand path segments, thereby increasing the likelihood of request\nsuccess. Through comprehensive simulations, we demonstrate that caching unused\nentanglements leads to a 10-15% improvement in the performance of\nstate-of-the-art quantum routing algorithms. Complementing caching with\nproactive entanglement swapping further enhances the request success rate by up\nto 52.55%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Entanglement generation in long-distance quantum networks is a difficult\nprocess due to resource limitations and the probabilistic nature of\nentanglement swapping. To maximize success probability, existing quantum\nrouting algorithms employ computationally expensive solutions (e.g., linear\nprogramming) to determine which links to entangle and use for end-to-end\nentanglement generation. Such optimization methods, however, cannot meet the\ndelay requirements of real-world quantum networks, necessitating swift yet\nefficient real-time optimization models. In this paper, we propose\nreinforcement learning (RL)-based models to determine which links to entangle\nand proactively swap to meet connection requests. We show that the proposed\nRL-based approach is 20x faster compared to linear programming. Moreover, we\nshow that one can take advantage of the longevity of entanglements to (i) cache\nentangled links for future use and (ii) proactively swap entanglement on\nhigh-demand path segments, thereby increasing the likelihood of request\nsuccess. Through comprehensive simulations, we demonstrate that caching unused\nentanglements leads to a 10-15% improvement in the performance of\nstate-of-the-art quantum routing algorithms. Complementing caching with\nproactive entanglement swapping further enhances the request success rate by up\nto 52.55%."
                },
                "authors": [
                    {
                        "name": "Tasdiqul Islam"
                    },
                    {
                        "name": "Md Arifuzzaman"
                    },
                    {
                        "name": "Engin Arslan"
                    }
                ],
                "author_detail": {
                    "name": "Engin Arslan"
                },
                "author": "Engin Arslan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08958v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08958v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01723v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01723v5",
                "updated": "2025-05-13T17:43:47Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    17,
                    43,
                    47,
                    1,
                    133,
                    0
                ],
                "published": "2024-10-02T16:34:29Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    16,
                    34,
                    29,
                    2,
                    276,
                    0
                ],
                "title": "HarmoniCa: Harmonizing Training and Inference for Better Feature Caching\n  in Diffusion Transformer Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HarmoniCa: Harmonizing Training and Inference for Better Feature Caching\n  in Diffusion Transformer Acceleration"
                },
                "summary": "Diffusion Transformers (DiTs) excel in generative tasks but face practical\ndeployment challenges due to high inference costs. Feature caching, which\nstores and retrieves redundant computations, offers the potential for\nacceleration. Existing learning-based caching, though adaptive, overlooks the\nimpact of the prior timestep. It also suffers from misaligned\nobjectives--aligned predicted noise vs. high-quality images--between training\nand inference. These two discrepancies compromise both performance and\nefficiency. To this end, we harmonize training and inference with a novel\nlearning-based caching framework dubbed HarmoniCa. It first incorporates\nStep-Wise Denoising Training (SDT) to ensure the continuity of the denoising\nprocess, where prior steps can be leveraged. In addition, an Image Error\nProxy-Guided Objective (IEPO) is applied to balance image quality against cache\nutilization through an efficient proxy to approximate the image error.\nExtensive experiments across $8$ models, $4$ samplers, and resolutions from\n$256\\times256$ to $2K$ demonstrate superior performance and speedup of our\nframework. For instance, it achieves over $40\\%$ latency reduction (i.e.,\n$2.07\\times$ theoretical speedup) and improved performance on PixArt-$\\alpha$.\nRemarkably, our image-free approach reduces training time by $25\\%$ compared\nwith the previous method. Our code is available at\nhttps://github.com/ModelTC/HarmoniCa.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiTs) excel in generative tasks but face practical\ndeployment challenges due to high inference costs. Feature caching, which\nstores and retrieves redundant computations, offers the potential for\nacceleration. Existing learning-based caching, though adaptive, overlooks the\nimpact of the prior timestep. It also suffers from misaligned\nobjectives--aligned predicted noise vs. high-quality images--between training\nand inference. These two discrepancies compromise both performance and\nefficiency. To this end, we harmonize training and inference with a novel\nlearning-based caching framework dubbed HarmoniCa. It first incorporates\nStep-Wise Denoising Training (SDT) to ensure the continuity of the denoising\nprocess, where prior steps can be leveraged. In addition, an Image Error\nProxy-Guided Objective (IEPO) is applied to balance image quality against cache\nutilization through an efficient proxy to approximate the image error.\nExtensive experiments across $8$ models, $4$ samplers, and resolutions from\n$256\\times256$ to $2K$ demonstrate superior performance and speedup of our\nframework. For instance, it achieves over $40\\%$ latency reduction (i.e.,\n$2.07\\times$ theoretical speedup) and improved performance on PixArt-$\\alpha$.\nRemarkably, our image-free approach reduces training time by $25\\%$ compared\nwith the previous method. Our code is available at\nhttps://github.com/ModelTC/HarmoniCa."
                },
                "authors": [
                    {
                        "name": "Yushi Huang"
                    },
                    {
                        "name": "Zining Wang"
                    },
                    {
                        "name": "Ruihao Gong"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Xinjie Zhang"
                    },
                    {
                        "name": "Jinyang Guo"
                    },
                    {
                        "name": "Xianglong Liu"
                    },
                    {
                        "name": "Jun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhang"
                },
                "author": "Jun Zhang",
                "arxiv_comment": "Accepted by ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01723v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01723v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08587v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08587v1",
                "updated": "2025-05-13T13:58:22Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    13,
                    58,
                    22,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T13:58:22Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    13,
                    58,
                    22,
                    1,
                    133,
                    0
                ],
                "title": "Two-Level Sketching Alternating Anderson acceleration for Complex\n  Physics Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Two-Level Sketching Alternating Anderson acceleration for Complex\n  Physics Applications"
                },
                "summary": "We present a novel two-level sketching extension of the Alternating\nAnderson-Picard (AAP) method for accelerating fixed-point iterations in\nchallenging single- and multi-physics simulations governed by discretized\npartial differential equations. Our approach combines a static, physics-based\nprojection that reduces the least-squares problem to the most informative field\n(e.g., via Schur-complement insight) with a dynamic, algebraic sketching stage\ndriven by a backward stability analysis under Lipschitz continuity. We\nintroduce inexpensive estimators for stability thresholds and cache-aware\nrandomized selection strategies to balance computational cost against\nmemory-access overhead. The resulting algorithm solves reduced least-squares\nsystems in place, minimizes memory footprints, and seamlessly alternates\nbetween low-cost Picard updates and Anderson mixing. Implemented in Julia, our\ntwo-level sketching AAP achieves up to 50% time-to-solution reductions compared\nto standard Anderson acceleration-without degrading convergence rates-on\nbenchmark problems including Stokes, p-Laplacian, Bidomain, and Navier-Stokes\nformulations at varying problem sizes. These results demonstrate the method's\nrobustness, scalability, and potential for integration into high-performance\nscientific computing frameworks. Our implementation is available open-source in\nthe AAP.jl library.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a novel two-level sketching extension of the Alternating\nAnderson-Picard (AAP) method for accelerating fixed-point iterations in\nchallenging single- and multi-physics simulations governed by discretized\npartial differential equations. Our approach combines a static, physics-based\nprojection that reduces the least-squares problem to the most informative field\n(e.g., via Schur-complement insight) with a dynamic, algebraic sketching stage\ndriven by a backward stability analysis under Lipschitz continuity. We\nintroduce inexpensive estimators for stability thresholds and cache-aware\nrandomized selection strategies to balance computational cost against\nmemory-access overhead. The resulting algorithm solves reduced least-squares\nsystems in place, minimizes memory footprints, and seamlessly alternates\nbetween low-cost Picard updates and Anderson mixing. Implemented in Julia, our\ntwo-level sketching AAP achieves up to 50% time-to-solution reductions compared\nto standard Anderson acceleration-without degrading convergence rates-on\nbenchmark problems including Stokes, p-Laplacian, Bidomain, and Navier-Stokes\nformulations at varying problem sizes. These results demonstrate the method's\nrobustness, scalability, and potential for integration into high-performance\nscientific computing frameworks. Our implementation is available open-source in\nthe AAP.jl library."
                },
                "authors": [
                    {
                        "name": "Nicolás A. Barnafi"
                    },
                    {
                        "name": "Massimiliano Lupo Pasini"
                    }
                ],
                "author_detail": {
                    "name": "Massimiliano Lupo Pasini"
                },
                "author": "Massimiliano Lupo Pasini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08587v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08587v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.NA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "65N12, 65N22, 65K10, 65F10, 65F99, 65B99",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13989v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13989v2",
                "updated": "2025-05-13T09:36:03Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    9,
                    36,
                    3,
                    1,
                    133,
                    0
                ],
                "published": "2025-04-18T13:46:58Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    13,
                    46,
                    58,
                    4,
                    108,
                    0
                ],
                "title": "Gradual Binary Search and Dimension Expansion : A general method for\n  activation quantization in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gradual Binary Search and Dimension Expansion : A general method for\n  activation quantization in LLMs"
                },
                "summary": "Large language models (LLMs) have become pivotal in artificial intelligence,\ndemonstrating strong capabilities in reasoning, understanding, and generating\ndata. However, their deployment on edge devices is hindered by their\nsubstantial size, often reaching several billion parameters. Quantization is a\nwidely used method to reduce memory usage and inference time, however LLMs\npresent unique challenges due to the prevalence of outliers in their\nactivations. In this work, we leverage the theoretical advantages of Hadamard\nmatrices over random rotation matrices to push the boundaries of quantization\nin LLMs. We demonstrate that Hadamard matrices are more effective in reducing\noutliers, which are a significant obstacle in achieving low-bit quantization.\nOur method based on a gradual binary search enables 3-bit quantization for\nweights, activations, and key-value (KV) caches, resulting in a 40% increase in\naccuracy on common benchmarks compared to SoTA methods. We extend the use of\nrotation matrices to support non-power-of-2 embedding dimensions, similar to\nthe Qwen architecture, by employing the Paley algorithm. We theoretically\ndemonstrates the superiority of Hadamard matrices in reducing outliers.We\nachieved 3-bit quantization for weights, activations, and KV cache,\nsignificantly enhancing model performance. Our experimental results on multiple\nmodels family like Mistral, LLaMA, and Qwen demonstrate the effectiveness of\nour approach, outperforming existing methods and enabling practical 3-bit\nquantization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have become pivotal in artificial intelligence,\ndemonstrating strong capabilities in reasoning, understanding, and generating\ndata. However, their deployment on edge devices is hindered by their\nsubstantial size, often reaching several billion parameters. Quantization is a\nwidely used method to reduce memory usage and inference time, however LLMs\npresent unique challenges due to the prevalence of outliers in their\nactivations. In this work, we leverage the theoretical advantages of Hadamard\nmatrices over random rotation matrices to push the boundaries of quantization\nin LLMs. We demonstrate that Hadamard matrices are more effective in reducing\noutliers, which are a significant obstacle in achieving low-bit quantization.\nOur method based on a gradual binary search enables 3-bit quantization for\nweights, activations, and key-value (KV) caches, resulting in a 40% increase in\naccuracy on common benchmarks compared to SoTA methods. We extend the use of\nrotation matrices to support non-power-of-2 embedding dimensions, similar to\nthe Qwen architecture, by employing the Paley algorithm. We theoretically\ndemonstrates the superiority of Hadamard matrices in reducing outliers.We\nachieved 3-bit quantization for weights, activations, and KV cache,\nsignificantly enhancing model performance. Our experimental results on multiple\nmodels family like Mistral, LLaMA, and Qwen demonstrate the effectiveness of\nour approach, outperforming existing methods and enabling practical 3-bit\nquantization."
                },
                "authors": [
                    {
                        "name": "Lucas Maisonnave"
                    },
                    {
                        "name": "Cyril Moineau"
                    },
                    {
                        "name": "Olivier Bichler"
                    },
                    {
                        "name": "Fabrice Rastello"
                    }
                ],
                "author_detail": {
                    "name": "Fabrice Rastello"
                },
                "author": "Fabrice Rastello",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13989v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13989v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08261v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08261v1",
                "updated": "2025-05-13T06:24:48Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    6,
                    24,
                    48,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T06:24:48Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    6,
                    24,
                    48,
                    1,
                    133,
                    0
                ],
                "title": "Enhancing Cache-Augmented Generation (CAG) with Adaptive Contextual\n  Compression for Scalable Knowledge Integration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Cache-Augmented Generation (CAG) with Adaptive Contextual\n  Compression for Scalable Knowledge Integration"
                },
                "summary": "The rapid progress in large language models (LLMs) has paved the way for\nnovel approaches in knowledge-intensive tasks. Among these, Cache-Augmented\nGeneration (CAG) has emerged as a promising alternative to Retrieval-Augmented\nGeneration (RAG). CAG minimizes retrieval latency and simplifies system design\nby preloading knowledge into the model's context. However, challenges persist\nin scaling CAG to accommodate large and dynamic knowledge bases effectively.\nThis paper introduces Adaptive Contextual Compression (ACC), an innovative\ntechnique designed to dynamically compress and manage context inputs, enabling\nefficient utilization of the extended memory capabilities of modern LLMs. To\nfurther address the limitations of standalone CAG, we propose a Hybrid CAG-RAG\nFramework, which integrates selective retrieval to augment preloaded contexts\nin scenarios requiring additional information. Comprehensive evaluations on\ndiverse datasets highlight the proposed methods' ability to enhance\nscalability, optimize efficiency, and improve multi-hop reasoning performance,\noffering practical solutions for real-world knowledge integration challenges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid progress in large language models (LLMs) has paved the way for\nnovel approaches in knowledge-intensive tasks. Among these, Cache-Augmented\nGeneration (CAG) has emerged as a promising alternative to Retrieval-Augmented\nGeneration (RAG). CAG minimizes retrieval latency and simplifies system design\nby preloading knowledge into the model's context. However, challenges persist\nin scaling CAG to accommodate large and dynamic knowledge bases effectively.\nThis paper introduces Adaptive Contextual Compression (ACC), an innovative\ntechnique designed to dynamically compress and manage context inputs, enabling\nefficient utilization of the extended memory capabilities of modern LLMs. To\nfurther address the limitations of standalone CAG, we propose a Hybrid CAG-RAG\nFramework, which integrates selective retrieval to augment preloaded contexts\nin scenarios requiring additional information. Comprehensive evaluations on\ndiverse datasets highlight the proposed methods' ability to enhance\nscalability, optimize efficiency, and improve multi-hop reasoning performance,\noffering practical solutions for real-world knowledge integration challenges."
                },
                "authors": [
                    {
                        "name": "Rishabh Agrawal"
                    },
                    {
                        "name": "Himanshu Kumar"
                    }
                ],
                "author_detail": {
                    "name": "Himanshu Kumar"
                },
                "author": "Himanshu Kumar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08261v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08261v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.07692v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.07692v1",
                "updated": "2025-05-12T15:58:39Z",
                "updated_parsed": [
                    2025,
                    5,
                    12,
                    15,
                    58,
                    39,
                    0,
                    132,
                    0
                ],
                "published": "2025-05-12T15:58:39Z",
                "published_parsed": [
                    2025,
                    5,
                    12,
                    15,
                    58,
                    39,
                    0,
                    132,
                    0
                ],
                "title": "ABase: the Multi-Tenant NoSQL Serverless Database for Diverse and\n  Dynamic Workloads in Large-scale Cloud Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ABase: the Multi-Tenant NoSQL Serverless Database for Diverse and\n  Dynamic Workloads in Large-scale Cloud Environments"
                },
                "summary": "Multi-tenant architectures enhance the elasticity and resource utilization of\nNoSQL databases by allowing multiple tenants to co-locate and share resources.\nHowever, in large-scale cloud environments, the diverse and dynamic nature of\nworkloads poses significant challenges for multi-tenant NoSQL databases. Based\non our practical observations, we have identified three crucial challenges: (1)\nthe impact of caching on performance isolation, as cache hits alter request\nexecution and resource consumption, leading to inaccurate traffic control; (2)\nthe dynamic changes in traffic, with changes in tenant traffic trends causing\nthrottling or resource wastage, and changes in access distribution causing hot\nkey pressure or cache hit ratio drops; and (3) the imbalanced layout of data\nnodes due to tenants' diverse resource requirements, leading to low resource\nutilization. To address these challenges, we introduce ABase, a multi-tenant\nNoSQL serverless database developed at ByteDance. ABase introduces a two-layer\ncaching mechanism with a cache-aware isolation mechanism to ensure accurate\nresource consumption estimates. Furthermore, ABase employs a predictive\nautoscaling policy to dynamically adjust resources in response to tenant\ntraffic changes and a multi-resource rescheduling algorithm to balance resource\nutilization across data nodes. With these innovations, ABase has successfully\nserved ByteDance's large-scale cloud environment, supporting a total workload\nthat has achieved a peak QPS of over 13 billion and total storage exceeding 1\nEB.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-tenant architectures enhance the elasticity and resource utilization of\nNoSQL databases by allowing multiple tenants to co-locate and share resources.\nHowever, in large-scale cloud environments, the diverse and dynamic nature of\nworkloads poses significant challenges for multi-tenant NoSQL databases. Based\non our practical observations, we have identified three crucial challenges: (1)\nthe impact of caching on performance isolation, as cache hits alter request\nexecution and resource consumption, leading to inaccurate traffic control; (2)\nthe dynamic changes in traffic, with changes in tenant traffic trends causing\nthrottling or resource wastage, and changes in access distribution causing hot\nkey pressure or cache hit ratio drops; and (3) the imbalanced layout of data\nnodes due to tenants' diverse resource requirements, leading to low resource\nutilization. To address these challenges, we introduce ABase, a multi-tenant\nNoSQL serverless database developed at ByteDance. ABase introduces a two-layer\ncaching mechanism with a cache-aware isolation mechanism to ensure accurate\nresource consumption estimates. Furthermore, ABase employs a predictive\nautoscaling policy to dynamically adjust resources in response to tenant\ntraffic changes and a multi-resource rescheduling algorithm to balance resource\nutilization across data nodes. With these innovations, ABase has successfully\nserved ByteDance's large-scale cloud environment, supporting a total workload\nthat has achieved a peak QPS of over 13 billion and total storage exceeding 1\nEB."
                },
                "authors": [
                    {
                        "name": "Rong Kang"
                    },
                    {
                        "name": "Yanbin Chen"
                    },
                    {
                        "name": "Ye Liu"
                    },
                    {
                        "name": "Fuxin Jiang"
                    },
                    {
                        "name": "Qingshuo Li"
                    },
                    {
                        "name": "Miao Ma"
                    },
                    {
                        "name": "Jian Liu"
                    },
                    {
                        "name": "Guangliang Zhao"
                    },
                    {
                        "name": "Tieying Zhang"
                    },
                    {
                        "name": "Jianjun Chen"
                    },
                    {
                        "name": "Lei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Lei Zhang"
                },
                "author": "Lei Zhang",
                "arxiv_comment": "SIGMOD 2025 accepted",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.07692v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.07692v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.07680v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.07680v1",
                "updated": "2025-05-12T15:46:28Z",
                "updated_parsed": [
                    2025,
                    5,
                    12,
                    15,
                    46,
                    28,
                    0,
                    132,
                    0
                ],
                "published": "2025-05-12T15:46:28Z",
                "published_parsed": [
                    2025,
                    5,
                    12,
                    15,
                    46,
                    28,
                    0,
                    132,
                    0
                ],
                "title": "SpecRouter: Adaptive Routing for Multi-Level Speculative Decoding in\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpecRouter: Adaptive Routing for Multi-Level Speculative Decoding in\n  Large Language Models"
                },
                "summary": "Large Language Models (LLMs) present a critical trade-off between inference\nquality and computational cost: larger models offer superior capabilities but\nincur significant latency, while smaller models are faster but less powerful.\nExisting serving strategies often employ fixed model scales or static two-stage\nspeculative decoding, failing to dynamically adapt to the varying complexities\nof user requests or fluctuations in system performance. This paper introduces\n\\systemname{}, a novel framework that reimagines LLM inference as an adaptive\nrouting problem solved through multi-level speculative decoding. \\systemname{}\ndynamically constructs and optimizes inference \"paths\" (chains of models) based\non real-time feedback, addressing the limitations of static approaches. Our\ncontributions are threefold: (1) An \\textbf{adaptive model chain scheduling}\nmechanism that leverages performance profiling (execution times) and predictive\nsimilarity metrics (derived from token distribution divergence) to continuously\nselect the optimal sequence of draft and verifier models, minimizing predicted\nlatency per generated token. (2) A \\textbf{multi-level collaborative\nverification} framework where intermediate models within the selected chain can\nvalidate speculative tokens, reducing the verification burden on the final,\nmost powerful target model. (3) A \\textbf{synchronized state management} system\nproviding efficient, consistent KV cache handling across heterogeneous models\nin the chain, including precise, low-overhead rollbacks tailored for\nasynchronous batch processing inherent in multi-level speculation. Preliminary\nexperiments demonstrate the validity of our method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) present a critical trade-off between inference\nquality and computational cost: larger models offer superior capabilities but\nincur significant latency, while smaller models are faster but less powerful.\nExisting serving strategies often employ fixed model scales or static two-stage\nspeculative decoding, failing to dynamically adapt to the varying complexities\nof user requests or fluctuations in system performance. This paper introduces\n\\systemname{}, a novel framework that reimagines LLM inference as an adaptive\nrouting problem solved through multi-level speculative decoding. \\systemname{}\ndynamically constructs and optimizes inference \"paths\" (chains of models) based\non real-time feedback, addressing the limitations of static approaches. Our\ncontributions are threefold: (1) An \\textbf{adaptive model chain scheduling}\nmechanism that leverages performance profiling (execution times) and predictive\nsimilarity metrics (derived from token distribution divergence) to continuously\nselect the optimal sequence of draft and verifier models, minimizing predicted\nlatency per generated token. (2) A \\textbf{multi-level collaborative\nverification} framework where intermediate models within the selected chain can\nvalidate speculative tokens, reducing the verification burden on the final,\nmost powerful target model. (3) A \\textbf{synchronized state management} system\nproviding efficient, consistent KV cache handling across heterogeneous models\nin the chain, including precise, low-overhead rollbacks tailored for\nasynchronous batch processing inherent in multi-level speculation. Preliminary\nexperiments demonstrate the validity of our method."
                },
                "authors": [
                    {
                        "name": "Hang Wu"
                    },
                    {
                        "name": "Jianian Zhu"
                    },
                    {
                        "name": "Yinghui Li"
                    },
                    {
                        "name": "Haojie Wang"
                    },
                    {
                        "name": "Biao Hou"
                    },
                    {
                        "name": "Jidong Zhai"
                    }
                ],
                "author_detail": {
                    "name": "Jidong Zhai"
                },
                "author": "Jidong Zhai",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.07680v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.07680v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.07350v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.07350v1",
                "updated": "2025-05-12T08:44:10Z",
                "updated_parsed": [
                    2025,
                    5,
                    12,
                    8,
                    44,
                    10,
                    0,
                    132,
                    0
                ],
                "published": "2025-05-12T08:44:10Z",
                "published_parsed": [
                    2025,
                    5,
                    12,
                    8,
                    44,
                    10,
                    0,
                    132,
                    0
                ],
                "title": "All-optical electric field sensing with nanodiamond-doped polymer thin\n  films",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "All-optical electric field sensing with nanodiamond-doped polymer thin\n  films"
                },
                "summary": "The nitrogen-vacancy (NV) center is a photoluminescent defect in diamond that\nexists in different charge states, NV$^-$ and NV$^0$, that are sensitive to the\nNV's nanoscale environment. Here, we show that photoluminescence (PL) from NV\ncenters in fluorescent nanodiamonds (FNDs) can be employed for all-optical\nvoltage sensing based on electric field-induced NV charge state modulation.\nMore than 95% of FNDs integrated into a capacitor device show a transient\nincrease in NV$^-$ PL intensity of up to 31% within 0.1 ms after application of\nan external voltage, accompanied by a simultaneous decrease in NV$^0$ PL. The\nchange in NV$^-$ PL increases with increasing applied voltage from 0 to 100 V,\ncorresponding to an electric field of 0 to 625 kV cm$^ {-1}$ in our devices.\nThe electric field sensitivity of a single FND is 19 V cm$^{-1}$ Hz$^ {-1/2}$.\nWe investigate the NV charge state photodynamics on the millisecond timescale\nand find that the change in NV PL strongly depends on the rate of\nphotoexcitation. We propose a model that qualitatively explains the observed\nchanges in NV PL based on an electric field-induced redistribution of\nphotoexcited electrons from substitutional nitrogen defects to NV centers,\nleading to a transient conversion of NV$^0$ to NV$^-$ centers upon application\nof an external voltage. Our results contribute to the development of FNDs as\nreliable, all-optical, nanoscale electric field sensors in solid-state systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The nitrogen-vacancy (NV) center is a photoluminescent defect in diamond that\nexists in different charge states, NV$^-$ and NV$^0$, that are sensitive to the\nNV's nanoscale environment. Here, we show that photoluminescence (PL) from NV\ncenters in fluorescent nanodiamonds (FNDs) can be employed for all-optical\nvoltage sensing based on electric field-induced NV charge state modulation.\nMore than 95% of FNDs integrated into a capacitor device show a transient\nincrease in NV$^-$ PL intensity of up to 31% within 0.1 ms after application of\nan external voltage, accompanied by a simultaneous decrease in NV$^0$ PL. The\nchange in NV$^-$ PL increases with increasing applied voltage from 0 to 100 V,\ncorresponding to an electric field of 0 to 625 kV cm$^ {-1}$ in our devices.\nThe electric field sensitivity of a single FND is 19 V cm$^{-1}$ Hz$^ {-1/2}$.\nWe investigate the NV charge state photodynamics on the millisecond timescale\nand find that the change in NV PL strongly depends on the rate of\nphotoexcitation. We propose a model that qualitatively explains the observed\nchanges in NV PL based on an electric field-induced redistribution of\nphotoexcited electrons from substitutional nitrogen defects to NV centers,\nleading to a transient conversion of NV$^0$ to NV$^-$ centers upon application\nof an external voltage. Our results contribute to the development of FNDs as\nreliable, all-optical, nanoscale electric field sensors in solid-state systems."
                },
                "authors": [
                    {
                        "name": "Roy Styles"
                    },
                    {
                        "name": "Mengke Han"
                    },
                    {
                        "name": "Toon Goris"
                    },
                    {
                        "name": "James Partridge"
                    },
                    {
                        "name": "Brett C. Johnson"
                    },
                    {
                        "name": "Blanca del Rosal"
                    },
                    {
                        "name": "Amanda N. Abraham"
                    },
                    {
                        "name": "Heike Ebendorff-Heidepriem"
                    },
                    {
                        "name": "Brant C. Gibson"
                    },
                    {
                        "name": "Nikolai Dontschuk"
                    },
                    {
                        "name": "Jean-Philippe Tetienne"
                    },
                    {
                        "name": "Philipp Reineck"
                    }
                ],
                "author_detail": {
                    "name": "Philipp Reineck"
                },
                "author": "Philipp Reineck",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.07350v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.07350v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mes-hall",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mes-hall",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.07274v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.07274v1",
                "updated": "2025-05-12T06:53:24Z",
                "updated_parsed": [
                    2025,
                    5,
                    12,
                    6,
                    53,
                    24,
                    0,
                    132,
                    0
                ],
                "published": "2025-05-12T06:53:24Z",
                "published_parsed": [
                    2025,
                    5,
                    12,
                    6,
                    53,
                    24,
                    0,
                    132,
                    0
                ],
                "title": "Cache-Efficient Posterior Sampling for Reinforcement Learning with\n  LLM-Derived Priors Across Discrete and Continuous Domains",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-Efficient Posterior Sampling for Reinforcement Learning with\n  LLM-Derived Priors Across Discrete and Continuous Domains"
                },
                "summary": "Integrating large language models (LLMs) as priors in reinforcement learning\n(RL) offers significant advantages but comes with substantial computational\ncosts. We present a principled cache-efficient framework for posterior sampling\nwith LLM-derived priors that dramatically reduces these costs while maintaining\nhigh performance. At the core of our approach is an adaptive caching mechanism,\nwhere cache parameters are meta-optimized using surrogate gradients derived\nfrom policy performance. This design enables efficient inference across both\ndiscrete text environments (e.g., TextWorld, ALFWorld) and continuous control\ndomains (e.g., MuJoCo), achieving a 3.8--4.7$\\times$ reduction in LLM queries\nand 4.0--12.0$\\times$ lower median latencies (85--93\\,ms on a consumer GPU)\nwhile retaining 96--98\\% of uncached performance. Our theoretical analysis\nprovides KL divergence bounds on approximation quality, validated empirically.\nThe framework extends to offline RL, where our CQL-Prior variant improves\nperformance by 14--29\\% and reduces training time by 38--40\\%. Extensive\nevaluations across a diverse suite of eight tasks demonstrate the\ngeneralizability and practical viability of LLM-guided RL in\nresource-constrained settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating large language models (LLMs) as priors in reinforcement learning\n(RL) offers significant advantages but comes with substantial computational\ncosts. We present a principled cache-efficient framework for posterior sampling\nwith LLM-derived priors that dramatically reduces these costs while maintaining\nhigh performance. At the core of our approach is an adaptive caching mechanism,\nwhere cache parameters are meta-optimized using surrogate gradients derived\nfrom policy performance. This design enables efficient inference across both\ndiscrete text environments (e.g., TextWorld, ALFWorld) and continuous control\ndomains (e.g., MuJoCo), achieving a 3.8--4.7$\\times$ reduction in LLM queries\nand 4.0--12.0$\\times$ lower median latencies (85--93\\,ms on a consumer GPU)\nwhile retaining 96--98\\% of uncached performance. Our theoretical analysis\nprovides KL divergence bounds on approximation quality, validated empirically.\nThe framework extends to offline RL, where our CQL-Prior variant improves\nperformance by 14--29\\% and reduces training time by 38--40\\%. Extensive\nevaluations across a diverse suite of eight tasks demonstrate the\ngeneralizability and practical viability of LLM-guided RL in\nresource-constrained settings."
                },
                "authors": [
                    {
                        "name": "Ibne Farabi Shihab"
                    },
                    {
                        "name": "Sanjeda Akter"
                    },
                    {
                        "name": "Anuj Sharma"
                    }
                ],
                "author_detail": {
                    "name": "Anuj Sharma"
                },
                "author": "Anuj Sharma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.07274v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.07274v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.07239v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.07239v1",
                "updated": "2025-05-12T05:29:30Z",
                "updated_parsed": [
                    2025,
                    5,
                    12,
                    5,
                    29,
                    30,
                    0,
                    132,
                    0
                ],
                "published": "2025-05-12T05:29:30Z",
                "published_parsed": [
                    2025,
                    5,
                    12,
                    5,
                    29,
                    30,
                    0,
                    132,
                    0
                ],
                "title": "Comet: Accelerating Private Inference for Large Language Model by\n  Predicting Activation Sparsity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comet: Accelerating Private Inference for Large Language Model by\n  Predicting Activation Sparsity"
                },
                "summary": "With the growing use of large language models (LLMs) hosted on cloud\nplatforms to offer inference services, privacy concerns about the potential\nleakage of sensitive information are escalating. Secure multi-party computation\n(MPC) is a promising solution to protect the privacy in LLM inference. However,\nMPC requires frequent inter-server communication, causing high performance\noverhead.\n  Inspired by the prevalent activation sparsity of LLMs, where most neuron are\nnot activated after non-linear activation functions, we propose an efficient\nprivate inference system, Comet. This system employs an accurate and fast\npredictor to predict the sparsity distribution of activation function output.\nAdditionally, we introduce a new private inference protocol. It efficiently and\nsecurely avoids computations involving zero values by exploiting the spatial\nlocality of the predicted sparse distribution. While this computation-avoidance\napproach impacts the spatiotemporal continuity of KV cache entries, we address\nthis challenge with a low-communication overhead cache refilling strategy that\nmerges miss requests and incorporates a prefetching mechanism. Finally, we\nevaluate Comet on four common LLMs and compare it with six state-of-the-art\nprivate inference systems. Comet achieves a 1.87x-2.63x speedup and a\n1.94x-2.64x communication reduction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the growing use of large language models (LLMs) hosted on cloud\nplatforms to offer inference services, privacy concerns about the potential\nleakage of sensitive information are escalating. Secure multi-party computation\n(MPC) is a promising solution to protect the privacy in LLM inference. However,\nMPC requires frequent inter-server communication, causing high performance\noverhead.\n  Inspired by the prevalent activation sparsity of LLMs, where most neuron are\nnot activated after non-linear activation functions, we propose an efficient\nprivate inference system, Comet. This system employs an accurate and fast\npredictor to predict the sparsity distribution of activation function output.\nAdditionally, we introduce a new private inference protocol. It efficiently and\nsecurely avoids computations involving zero values by exploiting the spatial\nlocality of the predicted sparse distribution. While this computation-avoidance\napproach impacts the spatiotemporal continuity of KV cache entries, we address\nthis challenge with a low-communication overhead cache refilling strategy that\nmerges miss requests and incorporates a prefetching mechanism. Finally, we\nevaluate Comet on four common LLMs and compare it with six state-of-the-art\nprivate inference systems. Comet achieves a 1.87x-2.63x speedup and a\n1.94x-2.64x communication reduction."
                },
                "authors": [
                    {
                        "name": "Guang Yan"
                    },
                    {
                        "name": "Yuhui Zhang"
                    },
                    {
                        "name": "Zimu Guo"
                    },
                    {
                        "name": "Lutan Zhao"
                    },
                    {
                        "name": "Xiaojun Chen"
                    },
                    {
                        "name": "Chen Wang"
                    },
                    {
                        "name": "Wenhao Wang"
                    },
                    {
                        "name": "Dan Meng"
                    },
                    {
                        "name": "Rui Hou"
                    }
                ],
                "author_detail": {
                    "name": "Rui Hou"
                },
                "author": "Rui Hou",
                "arxiv_doi": "10.1109/SP61157.2025.00182",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/SP61157.2025.00182",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2505.07239v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.07239v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted to SP 2025",
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.07203v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.07203v1",
                "updated": "2025-05-12T03:22:29Z",
                "updated_parsed": [
                    2025,
                    5,
                    12,
                    3,
                    22,
                    29,
                    0,
                    132,
                    0
                ],
                "published": "2025-05-12T03:22:29Z",
                "published_parsed": [
                    2025,
                    5,
                    12,
                    3,
                    22,
                    29,
                    0,
                    132,
                    0
                ],
                "title": "PrefillOnly: An Inference Engine for Prefill-only Workloads in Large\n  Language Model Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PrefillOnly: An Inference Engine for Prefill-only Workloads in Large\n  Language Model Applications"
                },
                "summary": "Besides typical generative applications, like ChatGPT, GitHub Copilot, and\nCursor, we observe an emerging trend that LLMs are increasingly used in\ntraditional discriminative tasks, such as recommendation, credit verification,\nand data labeling. The key characteristic of these emerging use cases is that\nthe LLM generates only a single output token, rather than an arbitrarily long\nsequence of tokens. We call this prefill-only workload. However, since existing\nLLM engines assume arbitrary output lengths, they fail to leverage the unique\nproperties of prefill-only workloads. In this paper, we present PrefillOnly,\nthe first LLM inference engine that improves the inference throughput and\nlatency by fully embracing the properties of prefill-only workloads. First,\nsince it generates only one token, PrefillOnly only needs to store the KV cache\nof only the last computed layer, rather than of all layers. This drastically\nreduces the GPU memory footprint of LLM inference and allows handling long\ninputs without using solutions that reduces throughput, such as cross-GPU KV\ncache parallelization. Second, because the output length is fixed, rather than\narbitrary, PrefillOnly can precisely determine the job completion time (JCT) of\neach prefill-only request before it starts. This enables efficient JCT-aware\nscheduling policies such as shortest remaining job first. PrefillOnly can\nprocess upto 4x larger queries per second without inflating average and P99\nlatency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Besides typical generative applications, like ChatGPT, GitHub Copilot, and\nCursor, we observe an emerging trend that LLMs are increasingly used in\ntraditional discriminative tasks, such as recommendation, credit verification,\nand data labeling. The key characteristic of these emerging use cases is that\nthe LLM generates only a single output token, rather than an arbitrarily long\nsequence of tokens. We call this prefill-only workload. However, since existing\nLLM engines assume arbitrary output lengths, they fail to leverage the unique\nproperties of prefill-only workloads. In this paper, we present PrefillOnly,\nthe first LLM inference engine that improves the inference throughput and\nlatency by fully embracing the properties of prefill-only workloads. First,\nsince it generates only one token, PrefillOnly only needs to store the KV cache\nof only the last computed layer, rather than of all layers. This drastically\nreduces the GPU memory footprint of LLM inference and allows handling long\ninputs without using solutions that reduces throughput, such as cross-GPU KV\ncache parallelization. Second, because the output length is fixed, rather than\narbitrary, PrefillOnly can precisely determine the job completion time (JCT) of\neach prefill-only request before it starts. This enables efficient JCT-aware\nscheduling policies such as shortest remaining job first. PrefillOnly can\nprocess upto 4x larger queries per second without inflating average and P99\nlatency."
                },
                "authors": [
                    {
                        "name": "Kuntai Du"
                    },
                    {
                        "name": "Bowen Wang"
                    },
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Yiming Cheng"
                    },
                    {
                        "name": "Qing Lan"
                    },
                    {
                        "name": "Hejian Sang"
                    },
                    {
                        "name": "Yihua Cheng"
                    },
                    {
                        "name": "Jiayi Yao"
                    },
                    {
                        "name": "Xiaoxuan Liu"
                    },
                    {
                        "name": "Yifan Qiao"
                    },
                    {
                        "name": "Ion Stoica"
                    },
                    {
                        "name": "Junchen Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Junchen Jiang"
                },
                "author": "Junchen Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.07203v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.07203v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.06901v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.06901v1",
                "updated": "2025-05-11T08:44:31Z",
                "updated_parsed": [
                    2025,
                    5,
                    11,
                    8,
                    44,
                    31,
                    6,
                    131,
                    0
                ],
                "published": "2025-05-11T08:44:31Z",
                "published_parsed": [
                    2025,
                    5,
                    11,
                    8,
                    44,
                    31,
                    6,
                    131,
                    0
                ],
                "title": "Ecco: Improving Memory Bandwidth and Capacity for LLMs via Entropy-aware\n  Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ecco: Improving Memory Bandwidth and Capacity for LLMs via Entropy-aware\n  Cache Compression"
                },
                "summary": "Large language models (LLMs) have demonstrated transformative capabilities\nacross diverse artificial intelligence applications, yet their deployment is\nhindered by substantial memory and computational demands, especially in\nresource-constrained environments. Quantization techniques have emerged as a\ncritical solution, reducing data precision to enhance memory and computational\nefficiency. However, existing methods often suffer from high runtime overheads\nand potential accuracy degradation. To address these challenges, we propose\nEcco, an entropy-based cache compression technique tailored for LLMs. Ecco\ncombines group-wise and non-uniform quantization with pre-defined shared\nk-means patterns and Huffman coding to exploit the inherent entropy\ncharacteristics of LLM cache data. Recognizing the inefficiencies of\ntraditional Huffman coding in terms of parallelism and latency, we introduce a\nnovel parallel Huffman-based decoding process with a multi-stage pipeline\ndesign, reducing latency by two orders of magnitude and achieving throughput\ncomparable to GPU L2 caches. Comprehensive evaluations demonstrate that Ecco\nachieves an up to 2.9$\\times$ and 1.9$\\times$ speedup over the state-of-the-art\nAWQ and SmoothQuant framework, 2.4$\\times$ over the Olive accelerator, all\nwhile increasing memory capacity by nearly 4$\\times$ and maintaining\nstate-of-the-art LLM accuracy. These results underscore the effectiveness of\nour entropy-based cache compression in enhancing LLM performance and\nefficiency, paving the way for more deployable large-scale AI models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated transformative capabilities\nacross diverse artificial intelligence applications, yet their deployment is\nhindered by substantial memory and computational demands, especially in\nresource-constrained environments. Quantization techniques have emerged as a\ncritical solution, reducing data precision to enhance memory and computational\nefficiency. However, existing methods often suffer from high runtime overheads\nand potential accuracy degradation. To address these challenges, we propose\nEcco, an entropy-based cache compression technique tailored for LLMs. Ecco\ncombines group-wise and non-uniform quantization with pre-defined shared\nk-means patterns and Huffman coding to exploit the inherent entropy\ncharacteristics of LLM cache data. Recognizing the inefficiencies of\ntraditional Huffman coding in terms of parallelism and latency, we introduce a\nnovel parallel Huffman-based decoding process with a multi-stage pipeline\ndesign, reducing latency by two orders of magnitude and achieving throughput\ncomparable to GPU L2 caches. Comprehensive evaluations demonstrate that Ecco\nachieves an up to 2.9$\\times$ and 1.9$\\times$ speedup over the state-of-the-art\nAWQ and SmoothQuant framework, 2.4$\\times$ over the Olive accelerator, all\nwhile increasing memory capacity by nearly 4$\\times$ and maintaining\nstate-of-the-art LLM accuracy. These results underscore the effectiveness of\nour entropy-based cache compression in enhancing LLM performance and\nefficiency, paving the way for more deployable large-scale AI models."
                },
                "authors": [
                    {
                        "name": "Feng Cheng"
                    },
                    {
                        "name": "Cong Guo"
                    },
                    {
                        "name": "Chiyue Wei"
                    },
                    {
                        "name": "Junyao Zhang"
                    },
                    {
                        "name": "Changchun Zhou"
                    },
                    {
                        "name": "Edward Hanson"
                    },
                    {
                        "name": "Jiaqi Zhang"
                    },
                    {
                        "name": "Xiaoxiao Liu"
                    },
                    {
                        "name": "Hai \"Helen\" Li"
                    },
                    {
                        "name": "Yiran Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yiran Chen"
                },
                "author": "Yiran Chen",
                "arxiv_comment": "ISCA 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.06901v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.06901v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.06625v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.06625v1",
                "updated": "2025-05-10T12:16:50Z",
                "updated_parsed": [
                    2025,
                    5,
                    10,
                    12,
                    16,
                    50,
                    5,
                    130,
                    0
                ],
                "published": "2025-05-10T12:16:50Z",
                "published_parsed": [
                    2025,
                    5,
                    10,
                    12,
                    16,
                    50,
                    5,
                    130,
                    0
                ],
                "title": "CaMDN: Enhancing Cache Efficiency for Multi-tenant DNNs on Integrated\n  NPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CaMDN: Enhancing Cache Efficiency for Multi-tenant DNNs on Integrated\n  NPUs"
                },
                "summary": "With the rapid development of DNN applications, multi-tenant execution, where\nmultiple DNNs are co-located on a single SoC, is becoming a prevailing trend.\nAlthough many methods are proposed in prior works to improve multi-tenant\nperformance, the impact of shared cache is not well studied. This paper\nproposes CaMDN, an architecture-scheduling co-design to enhance cache\nefficiency for multi-tenant DNNs on integrated NPUs. Specifically, a\nlightweight architecture is proposed to support model-exclusive, NPU-controlled\nregions inside shared cache to eliminate unexpected cache contention. Moreover,\na cache scheduling method is proposed to improve shared cache utilization. In\nparticular, it includes a cache-aware mapping method for adaptability to the\nvarying available cache capacity and a dynamic allocation algorithm to adjust\nthe usage among co-located DNNs at runtime. Compared to prior works, CaMDN\nreduces the memory access by 33.4% on average and achieves a model speedup of\nup to 2.56$\\times$ (1.88$\\times$ on average).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid development of DNN applications, multi-tenant execution, where\nmultiple DNNs are co-located on a single SoC, is becoming a prevailing trend.\nAlthough many methods are proposed in prior works to improve multi-tenant\nperformance, the impact of shared cache is not well studied. This paper\nproposes CaMDN, an architecture-scheduling co-design to enhance cache\nefficiency for multi-tenant DNNs on integrated NPUs. Specifically, a\nlightweight architecture is proposed to support model-exclusive, NPU-controlled\nregions inside shared cache to eliminate unexpected cache contention. Moreover,\na cache scheduling method is proposed to improve shared cache utilization. In\nparticular, it includes a cache-aware mapping method for adaptability to the\nvarying available cache capacity and a dynamic allocation algorithm to adjust\nthe usage among co-located DNNs at runtime. Compared to prior works, CaMDN\nreduces the memory access by 33.4% on average and achieves a model speedup of\nup to 2.56$\\times$ (1.88$\\times$ on average)."
                },
                "authors": [
                    {
                        "name": "Tianhao Cai"
                    },
                    {
                        "name": "Liang Wang"
                    },
                    {
                        "name": "Limin Xiao"
                    },
                    {
                        "name": "Meng Han"
                    },
                    {
                        "name": "Zeyu Wang"
                    },
                    {
                        "name": "Lin Sun"
                    },
                    {
                        "name": "Xiaojian Liao"
                    }
                ],
                "author_detail": {
                    "name": "Xiaojian Liao"
                },
                "author": "Xiaojian Liao",
                "arxiv_comment": "7 pages, 9 figures. This paper has been accepted to the 2025 Design\n  Automation Conference (DAC)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.06625v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.06625v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.06556v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.06556v1",
                "updated": "2025-05-10T07:57:02Z",
                "updated_parsed": [
                    2025,
                    5,
                    10,
                    7,
                    57,
                    2,
                    5,
                    130,
                    0
                ],
                "published": "2025-05-10T07:57:02Z",
                "published_parsed": [
                    2025,
                    5,
                    10,
                    7,
                    57,
                    2,
                    5,
                    130,
                    0
                ],
                "title": "TierBase: A Workload-Driven Cost-Optimized Key-Value Store",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TierBase: A Workload-Driven Cost-Optimized Key-Value Store"
                },
                "summary": "In the current era of data-intensive applications, the demand for\nhigh-performance, cost-effective storage solutions is paramount. This paper\nintroduces a Space-Performance Cost Model for key-value store, designed to\nguide cost-effective storage configuration decisions. The model quantifies the\ntrade-offs between performance and storage costs, providing a framework for\noptimizing resource allocation in large-scale data serving environments. Guided\nby this cost model, we present TierBase, a distributed key-value store\ndeveloped by Ant Group that optimizes total cost by strategically synchronizing\ndata between cache and storage tiers, maximizing resource utilization and\neffectively handling skewed workloads. To enhance cost-efficiency, TierBase\nincorporates several optimization techniques, including pre-trained data\ncompression, elastic threading mechanisms, and the utilization of persistent\nmemory. We detail TierBase's architecture, key components, and the\nimplementation of cost optimization strategies. Extensive evaluations using\nboth synthetic benchmarks and real-world workloads demonstrate TierBase's\nsuperior cost-effectiveness compared to existing solutions. Furthermore, case\nstudies from Ant Group's production environments showcase TierBase's ability to\nachieve up to 62% cost reduction in primary scenarios, highlighting its\npractical impact in large-scale online data serving.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the current era of data-intensive applications, the demand for\nhigh-performance, cost-effective storage solutions is paramount. This paper\nintroduces a Space-Performance Cost Model for key-value store, designed to\nguide cost-effective storage configuration decisions. The model quantifies the\ntrade-offs between performance and storage costs, providing a framework for\noptimizing resource allocation in large-scale data serving environments. Guided\nby this cost model, we present TierBase, a distributed key-value store\ndeveloped by Ant Group that optimizes total cost by strategically synchronizing\ndata between cache and storage tiers, maximizing resource utilization and\neffectively handling skewed workloads. To enhance cost-efficiency, TierBase\nincorporates several optimization techniques, including pre-trained data\ncompression, elastic threading mechanisms, and the utilization of persistent\nmemory. We detail TierBase's architecture, key components, and the\nimplementation of cost optimization strategies. Extensive evaluations using\nboth synthetic benchmarks and real-world workloads demonstrate TierBase's\nsuperior cost-effectiveness compared to existing solutions. Furthermore, case\nstudies from Ant Group's production environments showcase TierBase's ability to\nachieve up to 62% cost reduction in primary scenarios, highlighting its\npractical impact in large-scale online data serving."
                },
                "authors": [
                    {
                        "name": "Zhitao Shen"
                    },
                    {
                        "name": "Shiyu Yang"
                    },
                    {
                        "name": "Weibo Chen"
                    },
                    {
                        "name": "Kunming Wang"
                    },
                    {
                        "name": "Yue Li"
                    },
                    {
                        "name": "Jiabao Jin"
                    },
                    {
                        "name": "Wei Jia"
                    },
                    {
                        "name": "Junwei Chen"
                    },
                    {
                        "name": "Yuan Su"
                    },
                    {
                        "name": "Xiaoxia Duan"
                    },
                    {
                        "name": "Wei Chen"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Jie Song"
                    },
                    {
                        "name": "Ruoyi Ruan"
                    },
                    {
                        "name": "Xuemin Lin"
                    }
                ],
                "author_detail": {
                    "name": "Xuemin Lin"
                },
                "author": "Xuemin Lin",
                "arxiv_comment": "Accepted by ICDE 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.06556v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.06556v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.07872v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.07872v1",
                "updated": "2025-05-09T21:05:20Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    21,
                    5,
                    20,
                    4,
                    129,
                    0
                ],
                "published": "2025-05-09T21:05:20Z",
                "published_parsed": [
                    2025,
                    5,
                    9,
                    21,
                    5,
                    20,
                    4,
                    129,
                    0
                ],
                "title": "Revenue Optimization in Video Caching Networks with Privacy-Preserving\n  Demand Predictions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revenue Optimization in Video Caching Networks with Privacy-Preserving\n  Demand Predictions"
                },
                "summary": "Performance of video streaming, which accounts for most of the traffic in\nwireless communication, can be significantly improved by caching popular videos\nat the wireless edge. Determining the cache content that optimizes performance\n(defined via a revenue function) is thus an important task, and prediction of\nthe future demands based on past history can make this process much more\nefficient. However, since practical video caching networks involve various\nparties (e.g., users, isp, and csp) that do not wish to reveal information such\nas past history to each other, privacy-preserving solutions are required.\nMotivated by this, we propose a proactive caching method based on users'\nprivacy-preserving multi-slot future demand predictions -- obtained from a\ntrained Transformer -- to optimize revenue. Specifically, we first use a\nprivacy-preserving fl algorithm to train a Transformer to predict multi-slot\nfuture demands of the users. However, prediction accuracy is not perfect and\ndecreases the farther into the future the prediction is done. We model the\nimpact of prediction errors invoking the file popularities, based on which we\nformulate a long-term system revenue optimization to make the cache placement\ndecisions. As the formulated problem is NP-hard, we use a greedy algorithm to\nefficiently obtain an approximate solution. Simulation results validate that\n(i) the fl solution achieves results close to the centralized\n(non-privacy-preserving) solution and (ii) optimization of revenue may provide\ndifferent solutions than the classical chr criterion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance of video streaming, which accounts for most of the traffic in\nwireless communication, can be significantly improved by caching popular videos\nat the wireless edge. Determining the cache content that optimizes performance\n(defined via a revenue function) is thus an important task, and prediction of\nthe future demands based on past history can make this process much more\nefficient. However, since practical video caching networks involve various\nparties (e.g., users, isp, and csp) that do not wish to reveal information such\nas past history to each other, privacy-preserving solutions are required.\nMotivated by this, we propose a proactive caching method based on users'\nprivacy-preserving multi-slot future demand predictions -- obtained from a\ntrained Transformer -- to optimize revenue. Specifically, we first use a\nprivacy-preserving fl algorithm to train a Transformer to predict multi-slot\nfuture demands of the users. However, prediction accuracy is not perfect and\ndecreases the farther into the future the prediction is done. We model the\nimpact of prediction errors invoking the file popularities, based on which we\nformulate a long-term system revenue optimization to make the cache placement\ndecisions. As the formulated problem is NP-hard, we use a greedy algorithm to\nefficiently obtain an approximate solution. Simulation results validate that\n(i) the fl solution achieves results close to the centralized\n(non-privacy-preserving) solution and (ii) optimization of revenue may provide\ndifferent solutions than the classical chr criterion."
                },
                "authors": [
                    {
                        "name": "Yijing Zhang"
                    },
                    {
                        "name": "Ferdous Pervej"
                    },
                    {
                        "name": "Andreas F. Molisch"
                    }
                ],
                "author_detail": {
                    "name": "Andreas F. Molisch"
                },
                "author": "Andreas F. Molisch",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.07872v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.07872v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.06095v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.06095v3",
                "updated": "2025-05-09T07:26:29Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    7,
                    26,
                    29,
                    4,
                    129,
                    0
                ],
                "published": "2024-06-10T08:26:27Z",
                "published_parsed": [
                    2024,
                    6,
                    10,
                    8,
                    26,
                    27,
                    0,
                    162,
                    0
                ],
                "title": "An extension of C++ with memory-centric specifications for HPC to reduce\n  memory footprints and streamline MPI development",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An extension of C++ with memory-centric specifications for HPC to reduce\n  memory footprints and streamline MPI development"
                },
                "summary": "The C++ programming language and its cousins lean towards a\nmemory-inefficient storage of structs: The compiler inserts helper bits such\nthat individual instance variables fit to byte or cache boundaries, while it is\nnot able to exploit knowledge about the range of integers, enums or bitsets.\nFurthermore, the language provides neither support for data exchange via MPI\nnor for arbitrary floating-point precisions. We propose C++ attributes through\nwhich developers can guide the compiler what memory arrangements would be\nbeneficial: Can multiple booleans or integers with limited range be squeezed\ninto one bit field, do floating point numbers hold fewer significant bits than\nin the IEEE standard, or does the code benefit from a MPI datatype for subsets\nof attributes? The extension offers the opportunity to fall back to normal\nalignment via plain C++ assignments, no dependencies upon external libraries\nare introduced, and the resulting code remains standard C++ subject to some\nweakened guarantees on addresses and pointer arithmetics. Our work implements\nthe language annotations within LLVM and demonstrates their potential impact,\nboth upon the runtime and the memory footprint, through smoothed particle\nhydrodynamics (SPH) benchmarks. They uncover the potential gains in terms of\nperformance and development productivity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The C++ programming language and its cousins lean towards a\nmemory-inefficient storage of structs: The compiler inserts helper bits such\nthat individual instance variables fit to byte or cache boundaries, while it is\nnot able to exploit knowledge about the range of integers, enums or bitsets.\nFurthermore, the language provides neither support for data exchange via MPI\nnor for arbitrary floating-point precisions. We propose C++ attributes through\nwhich developers can guide the compiler what memory arrangements would be\nbeneficial: Can multiple booleans or integers with limited range be squeezed\ninto one bit field, do floating point numbers hold fewer significant bits than\nin the IEEE standard, or does the code benefit from a MPI datatype for subsets\nof attributes? The extension offers the opportunity to fall back to normal\nalignment via plain C++ assignments, no dependencies upon external libraries\nare introduced, and the resulting code remains standard C++ subject to some\nweakened guarantees on addresses and pointer arithmetics. Our work implements\nthe language annotations within LLVM and demonstrates their potential impact,\nboth upon the runtime and the memory footprint, through smoothed particle\nhydrodynamics (SPH) benchmarks. They uncover the potential gains in terms of\nperformance and development productivity."
                },
                "authors": [
                    {
                        "name": "Pawel K. Radtke"
                    },
                    {
                        "name": "Cristian G. Barrera-Hinojosa"
                    },
                    {
                        "name": "Mladen Ivkovic"
                    },
                    {
                        "name": "Tobias Weinzierl"
                    }
                ],
                "author_detail": {
                    "name": "Tobias Weinzierl"
                },
                "author": "Tobias Weinzierl",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.06095v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.06095v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05829v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05829v1",
                "updated": "2025-05-09T06:56:17Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    6,
                    56,
                    17,
                    4,
                    129,
                    0
                ],
                "published": "2025-05-09T06:56:17Z",
                "published_parsed": [
                    2025,
                    5,
                    9,
                    6,
                    56,
                    17,
                    4,
                    129,
                    0
                ],
                "title": "Accelerating Diffusion Transformer via Increment-Calibrated Caching with\n  Channel-Aware Singular Value Decomposition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Transformer via Increment-Calibrated Caching with\n  Channel-Aware Singular Value Decomposition"
                },
                "summary": "Diffusion transformer (DiT) models have achieved remarkable success in image\ngeneration, thanks for their exceptional generative capabilities and\nscalability. Nonetheless, the iterative nature of diffusion models (DMs)\nresults in high computation complexity, posing challenges for deployment.\nAlthough existing cache-based acceleration methods try to utilize the inherent\ntemporal similarity to skip redundant computations of DiT, the lack of\ncorrection may induce potential quality degradation. In this paper, we propose\nincrement-calibrated caching, a training-free method for DiT acceleration,\nwhere the calibration parameters are generated from the pre-trained model\nitself with low-rank approximation. To deal with the possible correction\nfailure arising from outlier activations, we introduce channel-aware Singular\nValue Decomposition (SVD), which further strengthens the calibration effect.\nExperimental results show that our method always achieve better performance\nthan existing naive caching methods with a similar computation resource budget.\nWhen compared with 35-step DDIM, our method eliminates more than 45%\ncomputation and improves IS by 12 at the cost of less than 0.06 FID increase.\nCode is available at https://github.com/ccccczzy/icc.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion transformer (DiT) models have achieved remarkable success in image\ngeneration, thanks for their exceptional generative capabilities and\nscalability. Nonetheless, the iterative nature of diffusion models (DMs)\nresults in high computation complexity, posing challenges for deployment.\nAlthough existing cache-based acceleration methods try to utilize the inherent\ntemporal similarity to skip redundant computations of DiT, the lack of\ncorrection may induce potential quality degradation. In this paper, we propose\nincrement-calibrated caching, a training-free method for DiT acceleration,\nwhere the calibration parameters are generated from the pre-trained model\nitself with low-rank approximation. To deal with the possible correction\nfailure arising from outlier activations, we introduce channel-aware Singular\nValue Decomposition (SVD), which further strengthens the calibration effect.\nExperimental results show that our method always achieve better performance\nthan existing naive caching methods with a similar computation resource budget.\nWhen compared with 35-step DDIM, our method eliminates more than 45%\ncomputation and improves IS by 12 at the cost of less than 0.06 FID increase.\nCode is available at https://github.com/ccccczzy/icc."
                },
                "authors": [
                    {
                        "name": "Zhiyuan Chen"
                    },
                    {
                        "name": "Keyi Li"
                    },
                    {
                        "name": "Yifan Jia"
                    },
                    {
                        "name": "Le Ye"
                    },
                    {
                        "name": "Yufei Ma"
                    }
                ],
                "author_detail": {
                    "name": "Yufei Ma"
                },
                "author": "Yufei Ma",
                "arxiv_comment": "accepted by CVPR2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05829v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05829v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05772v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05772v1",
                "updated": "2025-05-09T04:17:05Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    4,
                    17,
                    5,
                    4,
                    129,
                    0
                ],
                "published": "2025-05-09T04:17:05Z",
                "published_parsed": [
                    2025,
                    5,
                    9,
                    4,
                    17,
                    5,
                    4,
                    129,
                    0
                ],
                "title": "Sparse Attention Remapping with Clustering for Efficient LLM Decoding on\n  PIM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse Attention Remapping with Clustering for Efficient LLM Decoding on\n  PIM"
                },
                "summary": "Transformer-based models are the foundation of modern machine learning, but\ntheir execution, particularly during autoregressive decoding in large language\nmodels (LLMs), places significant pressure on memory systems due to frequent\nmemory accesses and growing key-value (KV) caches. This creates a bottleneck in\nmemory bandwidth, especially as context lengths increase. Processing-in-memory\n(PIM) architectures are a promising solution, offering high internal bandwidth\nand compute parallelism near memory. However, current PIM designs are primarily\noptimized for dense attention and struggle with the dynamic, irregular access\npatterns introduced by modern KV cache sparsity techniques. Consequently, they\nsuffer from workload imbalance, reducing throughput and resource utilization.\nIn this work, we propose STARC, a novel sparsity-optimized data mapping scheme\ntailored specifically for efficient LLM decoding on PIM architectures. STARC\nclusters KV pairs by semantic similarity and maps them to contiguous memory\nregions aligned with PIM bank structures. During decoding, queries retrieve\nrelevant tokens at cluster granularity by matching against precomputed\ncentroids, enabling selective attention and parallel processing without\nfrequent reclustering or data movement overhead. Experiments on the HBM-PIM\nsystem show that, compared to common token-wise sparsity methods, STARC reduces\nattention-layer latency by 19%--31% and energy consumption by 19%--27%. Under a\nKV cache budget of 1024, it achieves up to 54%--74% latency reduction and\n45%--67% energy reduction compared to full KV cache retrieval. Meanwhile, STARC\nmaintains model accuracy comparable to state-of-the-art sparse attention\nmethods, demonstrating its effectiveness in enabling efficient and\nhardware-friendly long-context LLM inference on PIM architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based models are the foundation of modern machine learning, but\ntheir execution, particularly during autoregressive decoding in large language\nmodels (LLMs), places significant pressure on memory systems due to frequent\nmemory accesses and growing key-value (KV) caches. This creates a bottleneck in\nmemory bandwidth, especially as context lengths increase. Processing-in-memory\n(PIM) architectures are a promising solution, offering high internal bandwidth\nand compute parallelism near memory. However, current PIM designs are primarily\noptimized for dense attention and struggle with the dynamic, irregular access\npatterns introduced by modern KV cache sparsity techniques. Consequently, they\nsuffer from workload imbalance, reducing throughput and resource utilization.\nIn this work, we propose STARC, a novel sparsity-optimized data mapping scheme\ntailored specifically for efficient LLM decoding on PIM architectures. STARC\nclusters KV pairs by semantic similarity and maps them to contiguous memory\nregions aligned with PIM bank structures. During decoding, queries retrieve\nrelevant tokens at cluster granularity by matching against precomputed\ncentroids, enabling selective attention and parallel processing without\nfrequent reclustering or data movement overhead. Experiments on the HBM-PIM\nsystem show that, compared to common token-wise sparsity methods, STARC reduces\nattention-layer latency by 19%--31% and energy consumption by 19%--27%. Under a\nKV cache budget of 1024, it achieves up to 54%--74% latency reduction and\n45%--67% energy reduction compared to full KV cache retrieval. Meanwhile, STARC\nmaintains model accuracy comparable to state-of-the-art sparse attention\nmethods, demonstrating its effectiveness in enabling efficient and\nhardware-friendly long-context LLM inference on PIM architectures."
                },
                "authors": [
                    {
                        "name": "Zehao Fan"
                    },
                    {
                        "name": "Garrett Gagnon"
                    },
                    {
                        "name": "Zhenyu Liu"
                    },
                    {
                        "name": "Liu Liu"
                    }
                ],
                "author_detail": {
                    "name": "Liu Liu"
                },
                "author": "Liu Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05772v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05772v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17264v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17264v3",
                "updated": "2025-05-09T00:31:24Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    0,
                    31,
                    24,
                    4,
                    129,
                    0
                ],
                "published": "2024-09-25T18:21:05Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    18,
                    21,
                    5,
                    2,
                    269,
                    0
                ],
                "title": "Medha: Efficiently Serving Multi-Million Context Length LLM Inference\n  Requests Without Approximations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Medha: Efficiently Serving Multi-Million Context Length LLM Inference\n  Requests Without Approximations"
                },
                "summary": "As large language models (LLMs) handle increasingly longer contexts, serving\nlong inference requests of millions of tokens presents unique challenges. We\nshow that existing work for long context inference is largely based on\ntechniques from long context training, and does not handle the high variability\nin input lengths during inference. This leads to inefficient resource\nutilization, server fragmentation, and head-of-line (HOL) blocking.\n  We present Medha, an end-to-end system for efficient long-context LLM\ninference that addresses these challenges through fine-grained time sharing.\nMedha introduces three key innovations: (1) the mechanism of adaptive prefill\nchunking to help mitigate HOL blocking with preemption; (2) two new parallelism\nstrategies: Sequence Pipeline Parallelism (SPP) to reduce time-to-first-token\nby pipelining prefill chunks, and KV-Cache Parallelism (KVP) to lower\ntime-peroutput-token by distributing decoding across servers; and (3) a novel\ninput-length aware least remaining slack scheduling to meet Service Level\nObjectives (SLOs).\n  Medha enables exact inference scaling beyond 10 million tokens, maintaining\nhigh throughput and low latency across mixed-length workloads. Compared to\nstate-of-the-art systems, Medha reduces server fragmentation, cuts median\nlatency by up to 30x, and improves throughput by over 5x, delivering\nproduction-scale long-context inference without compromising performance on\nshorter requests.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) handle increasingly longer contexts, serving\nlong inference requests of millions of tokens presents unique challenges. We\nshow that existing work for long context inference is largely based on\ntechniques from long context training, and does not handle the high variability\nin input lengths during inference. This leads to inefficient resource\nutilization, server fragmentation, and head-of-line (HOL) blocking.\n  We present Medha, an end-to-end system for efficient long-context LLM\ninference that addresses these challenges through fine-grained time sharing.\nMedha introduces three key innovations: (1) the mechanism of adaptive prefill\nchunking to help mitigate HOL blocking with preemption; (2) two new parallelism\nstrategies: Sequence Pipeline Parallelism (SPP) to reduce time-to-first-token\nby pipelining prefill chunks, and KV-Cache Parallelism (KVP) to lower\ntime-peroutput-token by distributing decoding across servers; and (3) a novel\ninput-length aware least remaining slack scheduling to meet Service Level\nObjectives (SLOs).\n  Medha enables exact inference scaling beyond 10 million tokens, maintaining\nhigh throughput and low latency across mixed-length workloads. Compared to\nstate-of-the-art systems, Medha reduces server fragmentation, cuts median\nlatency by up to 30x, and improves throughput by over 5x, delivering\nproduction-scale long-context inference without compromising performance on\nshorter requests."
                },
                "authors": [
                    {
                        "name": "Amey Agrawal"
                    },
                    {
                        "name": "Haoran Qiu"
                    },
                    {
                        "name": "Junda Chen"
                    },
                    {
                        "name": "Íñigo Goiri"
                    },
                    {
                        "name": "Chaojie Zhang"
                    },
                    {
                        "name": "Rayyan Shahid"
                    },
                    {
                        "name": "Ramachandran Ramjee"
                    },
                    {
                        "name": "Alexey Tumanov"
                    },
                    {
                        "name": "Esha Choukse"
                    }
                ],
                "author_detail": {
                    "name": "Esha Choukse"
                },
                "author": "Esha Choukse",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17264v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17264v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05251v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05251v1",
                "updated": "2025-05-08T13:56:20Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    13,
                    56,
                    20,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T13:56:20Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    13,
                    56,
                    20,
                    3,
                    128,
                    0
                ],
                "title": "High Altitude Platform-Based Caching and Multicasting for Rural\n  Connectivity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High Altitude Platform-Based Caching and Multicasting for Rural\n  Connectivity"
                },
                "summary": "Providing efficient and reliable content delivery in rural areas remains a\nsignificant challenge due to the lack of communication infrastructure. To\nbridge the digital divide, this paper investigates the potential of leveraging\nmultiple high-altitude platforms (HAPs) for energy-efficient content delivery\nin wide rural regions. Each caching-enabled HAP is equipped with both\nFree-Space Optical (FSO) transceivers for backhaul links and Radio Frequency\n(RF) antenna arrays for access links. To further enhance network efficiency, we\nconsider a network coding-based multicasting scheme, where different types of\ncontent are treated as distinct multicast sessions. With the objective of\nminimizing long-term power cost, we propose a hierarchical framework that\nintegrates deep reinforcement learn-ing (DRL) and convex optimization to\njointly optimize dynamic caching strategies and resource allocation across the\nnetwork. Simulation results demonstrate that our approach significantly reduces\npower cost compared to several baseline approaches, providing a practical\nsolution for improving rural connectivity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Providing efficient and reliable content delivery in rural areas remains a\nsignificant challenge due to the lack of communication infrastructure. To\nbridge the digital divide, this paper investigates the potential of leveraging\nmultiple high-altitude platforms (HAPs) for energy-efficient content delivery\nin wide rural regions. Each caching-enabled HAP is equipped with both\nFree-Space Optical (FSO) transceivers for backhaul links and Radio Frequency\n(RF) antenna arrays for access links. To further enhance network efficiency, we\nconsider a network coding-based multicasting scheme, where different types of\ncontent are treated as distinct multicast sessions. With the objective of\nminimizing long-term power cost, we propose a hierarchical framework that\nintegrates deep reinforcement learn-ing (DRL) and convex optimization to\njointly optimize dynamic caching strategies and resource allocation across the\nnetwork. Simulation results demonstrate that our approach significantly reduces\npower cost compared to several baseline approaches, providing a practical\nsolution for improving rural connectivity."
                },
                "authors": [
                    {
                        "name": "Yongqiang Zhang"
                    },
                    {
                        "name": "Mustafa A. Kishk"
                    },
                    {
                        "name": "Mohamed-Slim Alouini"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed-Slim Alouini"
                },
                "author": "Mohamed-Slim Alouini",
                "arxiv_comment": "13 pages, 8 figures, submitted to IEEE journals for possible\n  publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05251v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05251v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "49",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.4.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05130v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05130v1",
                "updated": "2025-05-08T11:07:35Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    11,
                    7,
                    35,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T11:07:35Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    11,
                    7,
                    35,
                    3,
                    128,
                    0
                ],
                "title": "CacheFL: Efficient Federated Cache Model Fine-Tuning for Vision-Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CacheFL: Efficient Federated Cache Model Fine-Tuning for Vision-Language\n  Models"
                },
                "summary": "Large pre-trained Vision-Language Models (VLMs), such as Contrastive\nLanguage-Image Pre-training (CLIP), have exhibited remarkable zero-shot\nperformance across various image classification tasks. Fine-tuning these models\non domain-specific datasets further enhances their effectiveness for downstream\napplications. However, fine-tuning in cloud environments raises significant\nconcerns regarding data security and privacy. Federated Learning (FL) offers a\ndecentralized solution by enabling model training across local clients without\ncentralizing sensitive data, but the high communication and computation costs\nof transmitting full pre-trained models during training limit its scalability.\nAdditionally, non-Independent and Identically Distributed (non-IID) data across\nlocal clients can negatively impact model convergence and performance. To\naddress these challenges, we propose CacheFL, a novel federated learning method\nthat replaces traditional full model fine-tuning with lightweight cache model\nfine-tuning. The cache model is initialized using a class-balanced dataset\ngenerated by a generative pre-trained model, effectively mitigating the impact\nof non-IID data. This cache model is then distributed to local clients for\nfine-tuning, and the updated parameters from each client are aggregated on the\nserver and redistributed. With the updated cache model, the classification\nperformance of CLIP is improved after just a few epochs. By limiting the\ntraining and communication to the cache model, CacheFL significantly reduces\nresource demands while ensuring data privacy and security. Extensive\nexperiments conducted on ImageNet and 10 additional datasets demonstrate that\nCacheFL outperforms traditional approaches in terms of classification accuracy,\nresource efficiency, and privacy preservation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large pre-trained Vision-Language Models (VLMs), such as Contrastive\nLanguage-Image Pre-training (CLIP), have exhibited remarkable zero-shot\nperformance across various image classification tasks. Fine-tuning these models\non domain-specific datasets further enhances their effectiveness for downstream\napplications. However, fine-tuning in cloud environments raises significant\nconcerns regarding data security and privacy. Federated Learning (FL) offers a\ndecentralized solution by enabling model training across local clients without\ncentralizing sensitive data, but the high communication and computation costs\nof transmitting full pre-trained models during training limit its scalability.\nAdditionally, non-Independent and Identically Distributed (non-IID) data across\nlocal clients can negatively impact model convergence and performance. To\naddress these challenges, we propose CacheFL, a novel federated learning method\nthat replaces traditional full model fine-tuning with lightweight cache model\nfine-tuning. The cache model is initialized using a class-balanced dataset\ngenerated by a generative pre-trained model, effectively mitigating the impact\nof non-IID data. This cache model is then distributed to local clients for\nfine-tuning, and the updated parameters from each client are aggregated on the\nserver and redistributed. With the updated cache model, the classification\nperformance of CLIP is improved after just a few epochs. By limiting the\ntraining and communication to the cache model, CacheFL significantly reduces\nresource demands while ensuring data privacy and security. Extensive\nexperiments conducted on ImageNet and 10 additional datasets demonstrate that\nCacheFL outperforms traditional approaches in terms of classification accuracy,\nresource efficiency, and privacy preservation."
                },
                "authors": [
                    {
                        "name": "Mengjun Yi"
                    },
                    {
                        "name": "Hanwen Zhang"
                    },
                    {
                        "name": "Hui Dou"
                    },
                    {
                        "name": "Jian Zhao"
                    },
                    {
                        "name": "Furao Shen"
                    }
                ],
                "author_detail": {
                    "name": "Furao Shen"
                },
                "author": "Furao Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05130v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05130v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03762v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03762v2",
                "updated": "2025-05-08T09:05:51Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    9,
                    5,
                    51,
                    3,
                    128,
                    0
                ],
                "published": "2025-04-20T17:48:54Z",
                "published_parsed": [
                    2025,
                    4,
                    20,
                    17,
                    48,
                    54,
                    6,
                    110,
                    0
                ],
                "title": "CVA6S+: A Superscalar RISC-V Core with High-Throughput Memory\n  Architecture",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CVA6S+: A Superscalar RISC-V Core with High-Throughput Memory\n  Architecture"
                },
                "summary": "Open-source RISC-V cores are increasingly adopted in high-end embedded\ndomains such as automotive, where maximizing instructions per cycle (IPC) is\nbecoming critical. Building on the industry-supported open-source CVA6 core and\nits superscalar variant, CVA6S, we introduce CVA6S+, an enhanced version\nincorporating improved branch prediction, register renaming and enhanced\noperand forwarding. These optimizations enable CVA6S+ to achieve a 43.5%\nperformance improvement over the scalar configuration and 10.9% over CVA6S,\nwith an area overhead of just 9.30% over the scalar core (CVA6). Furthermore,\nwe integrate CVA6S+ with the OpenHW Core-V High-Performance L1 Dcache\n(HPDCache) and report a 74.1% bandwidth improvement over the legacy CVA6 cache\nsubsystem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open-source RISC-V cores are increasingly adopted in high-end embedded\ndomains such as automotive, where maximizing instructions per cycle (IPC) is\nbecoming critical. Building on the industry-supported open-source CVA6 core and\nits superscalar variant, CVA6S, we introduce CVA6S+, an enhanced version\nincorporating improved branch prediction, register renaming and enhanced\noperand forwarding. These optimizations enable CVA6S+ to achieve a 43.5%\nperformance improvement over the scalar configuration and 10.9% over CVA6S,\nwith an area overhead of just 9.30% over the scalar core (CVA6). Furthermore,\nwe integrate CVA6S+ with the OpenHW Core-V High-Performance L1 Dcache\n(HPDCache) and report a 74.1% bandwidth improvement over the legacy CVA6 cache\nsubsystem."
                },
                "authors": [
                    {
                        "name": "Riccardo Tedeschi"
                    },
                    {
                        "name": "Gianmarco Ottavi"
                    },
                    {
                        "name": "Côme Allart"
                    },
                    {
                        "name": "Nils Wistoff"
                    },
                    {
                        "name": "Zexin Fu"
                    },
                    {
                        "name": "Filippo Grillotti"
                    },
                    {
                        "name": "Fabio De Ambroggi"
                    },
                    {
                        "name": "Elio Guidetti"
                    },
                    {
                        "name": "Jean-Baptiste Rigaud"
                    },
                    {
                        "name": "Olivier Potin"
                    },
                    {
                        "name": "Jean Roch Coulon"
                    },
                    {
                        "name": "César Fuguet"
                    },
                    {
                        "name": "Luca Benini"
                    },
                    {
                        "name": "Davide Rossi"
                    }
                ],
                "author_detail": {
                    "name": "Davide Rossi"
                },
                "author": "Davide Rossi",
                "arxiv_comment": "3 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03762v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03762v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12110v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12110v2",
                "updated": "2025-05-08T07:55:38Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    7,
                    55,
                    38,
                    3,
                    128,
                    0
                ],
                "published": "2024-06-17T21:43:39Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    21,
                    43,
                    39,
                    0,
                    169,
                    0
                ],
                "title": "CacheSquash: Making caches speculation-aware",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CacheSquash: Making caches speculation-aware"
                },
                "summary": "Speculation is key to achieving high CPU performance, yet it enables risks\nlike Spectre attacks which remain a significant challenge to mitigate without\nincurring substantial performance overheads. These attacks typically unfold in\nthree stages: access, transmit, and receive. Typically, they exploit a cache\ntiming side channel during the transmit and receive phases: speculatively\naccessing sensitive data (access), altering cache state (transmit), and then\nutilizing a cache timing attack (e.g., Flush+Reload) to extract the secret\n(receive). Our key observation is that Spectre attacks only require the\ntransmit instruction to execute and dispatch a request to the cache hierarchy.\nIt need not complete before a misprediction is detected (and mis-speculated\ninstructions squashed) because responses from memory that arrive at the cache\nafter squashing still alter cache state. We propose a novel mitigation,\nCacheSquash, that cancels mis-speculated memory accesses. Immediately upon\nsquashing, a cancellation is sent to the cache hierarchy, propagating\ndownstream and preventing any changes to caches that have not yet received a\nresponse. This minimizes cache state changes, thereby reducing the likelihood\nof Spectre attacks succeeding. We implement CacheSquash on gem5 and show that\nit thwarts practical Spectre attacks, with near-zero performance overheads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculation is key to achieving high CPU performance, yet it enables risks\nlike Spectre attacks which remain a significant challenge to mitigate without\nincurring substantial performance overheads. These attacks typically unfold in\nthree stages: access, transmit, and receive. Typically, they exploit a cache\ntiming side channel during the transmit and receive phases: speculatively\naccessing sensitive data (access), altering cache state (transmit), and then\nutilizing a cache timing attack (e.g., Flush+Reload) to extract the secret\n(receive). Our key observation is that Spectre attacks only require the\ntransmit instruction to execute and dispatch a request to the cache hierarchy.\nIt need not complete before a misprediction is detected (and mis-speculated\ninstructions squashed) because responses from memory that arrive at the cache\nafter squashing still alter cache state. We propose a novel mitigation,\nCacheSquash, that cancels mis-speculated memory accesses. Immediately upon\nsquashing, a cancellation is sent to the cache hierarchy, propagating\ndownstream and preventing any changes to caches that have not yet received a\nresponse. This minimizes cache state changes, thereby reducing the likelihood\nof Spectre attacks succeeding. We implement CacheSquash on gem5 and show that\nit thwarts practical Spectre attacks, with near-zero performance overheads."
                },
                "authors": [
                    {
                        "name": "Hossam ElAtali"
                    },
                    {
                        "name": "N. Asokan"
                    }
                ],
                "author_detail": {
                    "name": "N. Asokan"
                },
                "author": "N. Asokan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12110v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12110v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01658v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01658v2",
                "updated": "2025-05-08T07:08:40Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    7,
                    8,
                    40,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-03T02:47:43Z",
                "published_parsed": [
                    2025,
                    5,
                    3,
                    2,
                    47,
                    43,
                    5,
                    123,
                    0
                ],
                "title": "A Survey on Inference Engines for Large Language Models: Perspectives on\n  Optimization and Efficiency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Inference Engines for Large Language Models: Perspectives on\n  Optimization and Efficiency"
                },
                "summary": "Large language models (LLMs) are widely applied in chatbots, code generators,\nand search engines. Workloads such as chain-of-thought, complex reasoning, and\nagent services significantly increase the inference cost by invoking the model\nrepeatedly. Optimization methods such as parallelism, compression, and caching\nhave been adopted to reduce costs, but the diverse service requirements make it\nhard to select the right method. Recently, specialized LLM inference engines\nhave emerged as a key component for integrating the optimization methods into\nservice-oriented infrastructures. However, a systematic study on inference\nengines is still lacking. This paper provides a comprehensive evaluation of 25\nopen-source and commercial inference engines. We examine each inference engine\nin terms of ease-of-use, ease-of-deployment, general-purpose support,\nscalability, and suitability for throughput- and latency-aware computation.\nFurthermore, we explore the design goals of each inference engine by\ninvestigating the optimization techniques it supports. In addition, we assess\nthe ecosystem maturity of open source inference engines and handle the\nperformance and cost policy of commercial solutions. We outline future research\ndirections that include support for complex LLM-based services, support of\nvarious hardware, and enhanced security, offering practical guidance to\nresearchers and developers in selecting and designing optimized LLM inference\nengines. We also provide a public repository to continually track developments\nin this fast-evolving field:\nhttps://github.com/sihyeong/Awesome-LLM-Inference-Engine",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are widely applied in chatbots, code generators,\nand search engines. Workloads such as chain-of-thought, complex reasoning, and\nagent services significantly increase the inference cost by invoking the model\nrepeatedly. Optimization methods such as parallelism, compression, and caching\nhave been adopted to reduce costs, but the diverse service requirements make it\nhard to select the right method. Recently, specialized LLM inference engines\nhave emerged as a key component for integrating the optimization methods into\nservice-oriented infrastructures. However, a systematic study on inference\nengines is still lacking. This paper provides a comprehensive evaluation of 25\nopen-source and commercial inference engines. We examine each inference engine\nin terms of ease-of-use, ease-of-deployment, general-purpose support,\nscalability, and suitability for throughput- and latency-aware computation.\nFurthermore, we explore the design goals of each inference engine by\ninvestigating the optimization techniques it supports. In addition, we assess\nthe ecosystem maturity of open source inference engines and handle the\nperformance and cost policy of commercial solutions. We outline future research\ndirections that include support for complex LLM-based services, support of\nvarious hardware, and enhanced security, offering practical guidance to\nresearchers and developers in selecting and designing optimized LLM inference\nengines. We also provide a public repository to continually track developments\nin this fast-evolving field:\nhttps://github.com/sihyeong/Awesome-LLM-Inference-Engine"
                },
                "authors": [
                    {
                        "name": "Sihyeong Park"
                    },
                    {
                        "name": "Sungryeol Jeon"
                    },
                    {
                        "name": "Chaelyn Lee"
                    },
                    {
                        "name": "Seokhun Jeon"
                    },
                    {
                        "name": "Byung-Soo Kim"
                    },
                    {
                        "name": "Jemin Lee"
                    }
                ],
                "author_detail": {
                    "name": "Jemin Lee"
                },
                "author": "Jemin Lee",
                "arxiv_comment": "Under review; 65 pages; 27 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01658v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01658v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04896v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04896v1",
                "updated": "2025-05-08T02:16:08Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    2,
                    16,
                    8,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T02:16:08Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    2,
                    16,
                    8,
                    3,
                    128,
                    0
                ],
                "title": "Memory Under Siege: A Comprehensive Survey of Side-Channel Attacks on\n  Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory Under Siege: A Comprehensive Survey of Side-Channel Attacks on\n  Memory"
                },
                "summary": "Side-channel attacks on memory (SCAM) exploit unintended data leaks from\nmemory subsystems to infer sensitive information, posing significant threats to\nsystem security. These attacks exploit vulnerabilities in memory access\npatterns, cache behaviors, and other microarchitectural features to bypass\ntraditional security measures. The purpose of this research is to examine SCAM,\nclassify various attack techniques, and evaluate existing defense mechanisms.\nIt guides researchers and industry professionals in improving memory security\nand mitigating emerging threats. We begin by identifying the major\nvulnerabilities in the memory system that are frequently exploited in SCAM,\nsuch as cache timing, speculative execution, \\textit{Rowhammer}, and other\nsophisticated approaches. Next, we outline a comprehensive taxonomy that\nsystematically classifies these attacks based on their types, target systems,\nattack vectors, and adversarial capabilities required to execute them. In\naddition, we review the current landscape of mitigation strategies, emphasizing\ntheir strengths and limitations. This work aims to provide a comprehensive\noverview of memory-based side-channel attacks with the goal of providing\nsignificant insights for researchers and practitioners to better understand,\ndetect, and mitigate SCAM risks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Side-channel attacks on memory (SCAM) exploit unintended data leaks from\nmemory subsystems to infer sensitive information, posing significant threats to\nsystem security. These attacks exploit vulnerabilities in memory access\npatterns, cache behaviors, and other microarchitectural features to bypass\ntraditional security measures. The purpose of this research is to examine SCAM,\nclassify various attack techniques, and evaluate existing defense mechanisms.\nIt guides researchers and industry professionals in improving memory security\nand mitigating emerging threats. We begin by identifying the major\nvulnerabilities in the memory system that are frequently exploited in SCAM,\nsuch as cache timing, speculative execution, \\textit{Rowhammer}, and other\nsophisticated approaches. Next, we outline a comprehensive taxonomy that\nsystematically classifies these attacks based on their types, target systems,\nattack vectors, and adversarial capabilities required to execute them. In\naddition, we review the current landscape of mitigation strategies, emphasizing\ntheir strengths and limitations. This work aims to provide a comprehensive\noverview of memory-based side-channel attacks with the goal of providing\nsignificant insights for researchers and practitioners to better understand,\ndetect, and mitigate SCAM risks."
                },
                "authors": [
                    {
                        "name": "MD Mahady Hassan"
                    },
                    {
                        "name": "Shanto Roy"
                    },
                    {
                        "name": "Reza Rahaeimehr"
                    }
                ],
                "author_detail": {
                    "name": "Reza Rahaeimehr"
                },
                "author": "Reza Rahaeimehr",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04896v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04896v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04556v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04556v1",
                "updated": "2025-05-07T16:44:21Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    16,
                    44,
                    21,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T16:44:21Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    16,
                    44,
                    21,
                    2,
                    127,
                    0
                ],
                "title": "Comparing CPU and GPU compute of PERMANOVA on MI300A",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comparing CPU and GPU compute of PERMANOVA on MI300A"
                },
                "summary": "Comparing the tradeoffs of CPU and GPU compute for memory-heavy algorithms is\noften challenging, due to the drastically different memory subsystems on host\nCPUs and discrete GPUs. The AMD MI300A is an exception, since it sports both\nCPU and GPU cores in a single package, all backed by the same type of HBM\nmemory. In this paper we analyze the performance of Permutational Multivariate\nAnalysis of Variance (PERMANOVA), a non-parametric method that tests whether\ntwo or more groups of objects are significantly different based on a\ncategorical factor. This method is memory-bound and has been recently optimized\nfor CPU cache locality. Our tests show that GPU cores on the MI300A prefer the\nbrute force approach instead, significantly outperforming the CPU-based\nimplementation. The significant benefit of Simultaneous Multithreading (SMT)\nwas also a pleasant surprise.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comparing the tradeoffs of CPU and GPU compute for memory-heavy algorithms is\noften challenging, due to the drastically different memory subsystems on host\nCPUs and discrete GPUs. The AMD MI300A is an exception, since it sports both\nCPU and GPU cores in a single package, all backed by the same type of HBM\nmemory. In this paper we analyze the performance of Permutational Multivariate\nAnalysis of Variance (PERMANOVA), a non-parametric method that tests whether\ntwo or more groups of objects are significantly different based on a\ncategorical factor. This method is memory-bound and has been recently optimized\nfor CPU cache locality. Our tests show that GPU cores on the MI300A prefer the\nbrute force approach instead, significantly outperforming the CPU-based\nimplementation. The significant benefit of Simultaneous Multithreading (SMT)\nwas also a pleasant surprise."
                },
                "authors": [
                    {
                        "name": "Igor Sfiligoi"
                    }
                ],
                "author_detail": {
                    "name": "Igor Sfiligoi"
                },
                "author": "Igor Sfiligoi",
                "arxiv_comment": "7 pages, 1 figure, Accepted at PEARC25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04556v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04556v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04466v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04466v1",
                "updated": "2025-05-07T14:37:13Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    14,
                    37,
                    13,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T14:37:13Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    14,
                    37,
                    13,
                    2,
                    127,
                    0
                ],
                "title": "Securing Immersive 360 Video Streams through Attribute-Based Selective\n  Encryption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Securing Immersive 360 Video Streams through Attribute-Based Selective\n  Encryption"
                },
                "summary": "Delivering high-quality, secure 360{\\deg} video content introduces unique\nchallenges, primarily due to the high bitrates and interactive demands of\nimmersive media. Traditional HTTPS-based methods, although widely used, face\nlimitations in computational efficiency and scalability when securing these\nhigh-resolution streams. To address these issues, this paper proposes a novel\nframework integrating Attribute-Based Encryption (ABE) with selective\nencryption techniques tailored specifically for tiled 360{\\deg} video\nstreaming. Our approach employs selective encryption of frames at varying\nlevels to reduce computational overhead while ensuring robust protection\nagainst unauthorized access.\n  Moreover, we explore viewport-adaptive encryption, dynamically encrypting\nmore frames within tiles occupying larger portions of the viewer's field of\nview. This targeted method significantly enhances security in critical viewing\nareas without unnecessary overhead in peripheral regions. We deploy and\nevaluate our proposed approach using the CloudLab testbed, comparing its\nperformance against traditional HTTPS streaming. Experimental results\ndemonstrate that our ABE-based model achieves reduced computational load on\nintermediate caches, improves cache hit rates, and maintains comparable visual\nquality to HTTPS, as assessed by Video Multimethod Assessment Fusion (VMAF).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Delivering high-quality, secure 360{\\deg} video content introduces unique\nchallenges, primarily due to the high bitrates and interactive demands of\nimmersive media. Traditional HTTPS-based methods, although widely used, face\nlimitations in computational efficiency and scalability when securing these\nhigh-resolution streams. To address these issues, this paper proposes a novel\nframework integrating Attribute-Based Encryption (ABE) with selective\nencryption techniques tailored specifically for tiled 360{\\deg} video\nstreaming. Our approach employs selective encryption of frames at varying\nlevels to reduce computational overhead while ensuring robust protection\nagainst unauthorized access.\n  Moreover, we explore viewport-adaptive encryption, dynamically encrypting\nmore frames within tiles occupying larger portions of the viewer's field of\nview. This targeted method significantly enhances security in critical viewing\nareas without unnecessary overhead in peripheral regions. We deploy and\nevaluate our proposed approach using the CloudLab testbed, comparing its\nperformance against traditional HTTPS streaming. Experimental results\ndemonstrate that our ABE-based model achieves reduced computational load on\nintermediate caches, improves cache hit rates, and maintains comparable visual\nquality to HTTPS, as assessed by Video Multimethod Assessment Fusion (VMAF)."
                },
                "authors": [
                    {
                        "name": "Mohammad Waquas Usmani"
                    },
                    {
                        "name": "Susmit Shannigrahi"
                    },
                    {
                        "name": "Michael Zink"
                    }
                ],
                "author_detail": {
                    "name": "Michael Zink"
                },
                "author": "Michael Zink",
                "arxiv_comment": "8 pages plus references, 10 figures, some with subfigures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04466v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04466v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04421v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04421v1",
                "updated": "2025-05-07T13:54:26Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    13,
                    54,
                    26,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T13:54:26Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    13,
                    54,
                    26,
                    2,
                    127,
                    0
                ],
                "title": "LONGER: Scaling Up Long Sequence Modeling in Industrial Recommenders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LONGER: Scaling Up Long Sequence Modeling in Industrial Recommenders"
                },
                "summary": "Modeling ultra-long user behavior sequences is critical for capturing both\nlong- and short-term preferences in industrial recommender systems. Existing\nsolutions typically rely on two-stage retrieval or indirect modeling paradigms,\nincuring upstream-downstream inconsistency and computational inefficiency. In\nthis paper, we present LONGER, a Long-sequence Optimized traNsformer for\nGPU-Efficient Recommenders. LONGER incorporates (i) a global token mechanism\nfor stabilizing attention over long contexts, (ii) a token merge module with\nlightweight InnerTransformers and hybrid attention strategy to reduce quadratic\ncomplexity, and (iii) a series of engineering optimizations, including training\nwith mixed-precision and activation recomputation, KV cache serving, and the\nfully synchronous model training and serving framework for unified GPU-based\ndense and sparse parameter updates. LONGER consistently outperforms strong\nbaselines in both offline metrics and online A/B testing in both advertising\nand e-commerce services at ByteDance, validating its consistent effectiveness\nand industrial-level scaling laws. Currently, LONGER has been fully deployed at\nmore than 10 influential scenarios at ByteDance, serving billion users.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling ultra-long user behavior sequences is critical for capturing both\nlong- and short-term preferences in industrial recommender systems. Existing\nsolutions typically rely on two-stage retrieval or indirect modeling paradigms,\nincuring upstream-downstream inconsistency and computational inefficiency. In\nthis paper, we present LONGER, a Long-sequence Optimized traNsformer for\nGPU-Efficient Recommenders. LONGER incorporates (i) a global token mechanism\nfor stabilizing attention over long contexts, (ii) a token merge module with\nlightweight InnerTransformers and hybrid attention strategy to reduce quadratic\ncomplexity, and (iii) a series of engineering optimizations, including training\nwith mixed-precision and activation recomputation, KV cache serving, and the\nfully synchronous model training and serving framework for unified GPU-based\ndense and sparse parameter updates. LONGER consistently outperforms strong\nbaselines in both offline metrics and online A/B testing in both advertising\nand e-commerce services at ByteDance, validating its consistent effectiveness\nand industrial-level scaling laws. Currently, LONGER has been fully deployed at\nmore than 10 influential scenarios at ByteDance, serving billion users."
                },
                "authors": [
                    {
                        "name": "Zheng Chai"
                    },
                    {
                        "name": "Qin Ren"
                    },
                    {
                        "name": "Xijun Xiao"
                    },
                    {
                        "name": "Huizhi Yang"
                    },
                    {
                        "name": "Bo Han"
                    },
                    {
                        "name": "Sijun Zhang"
                    },
                    {
                        "name": "Di Chen"
                    },
                    {
                        "name": "Hui Lu"
                    },
                    {
                        "name": "Wenlin Zhao"
                    },
                    {
                        "name": "Lele Yu"
                    },
                    {
                        "name": "Xionghang Xie"
                    },
                    {
                        "name": "Shiru Ren"
                    },
                    {
                        "name": "Xiang Sun"
                    },
                    {
                        "name": "Yaocheng Tan"
                    },
                    {
                        "name": "Peng Xu"
                    },
                    {
                        "name": "Yuchao Zheng"
                    },
                    {
                        "name": "Di Wu"
                    }
                ],
                "author_detail": {
                    "name": "Di Wu"
                },
                "author": "Di Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04421v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04421v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13779v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13779v3",
                "updated": "2025-05-15T03:29:15Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    3,
                    29,
                    15,
                    3,
                    135,
                    0
                ],
                "published": "2024-12-18T12:16:41Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    12,
                    16,
                    41,
                    2,
                    353,
                    0
                ],
                "title": "Rehearsal-Free Continual Federated Learning with Synergistic Synaptic\n  Intelligence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rehearsal-Free Continual Federated Learning with Synergistic Synaptic\n  Intelligence"
                },
                "summary": "Continual Federated Learning (CFL) allows distributed devices to\ncollaboratively learn novel concepts from continuously shifting training data\nwhile avoiding knowledge forgetting of previously seen tasks. To tackle this\nchallenge, most current CFL approaches rely on extensive rehearsal of previous\ndata. Despite effectiveness, rehearsal comes at a cost to memory, and it may\nalso violate data privacy. Considering these, we seek to apply regularization\ntechniques to CFL by considering their cost-efficient properties that do not\nrequire sample caching or rehearsal. Specifically, we first apply traditional\nregularization techniques to CFL and observe that existing regularization\ntechniques, especially synaptic intelligence, can achieve promising results\nunder homogeneous data distribution but fail when the data is heterogeneous.\nBased on this observation, we propose a simple yet effective regularization\nalgorithm for CFL named FedSSI, which tailors the synaptic intelligence for the\nCFL with heterogeneous data settings. FedSSI can not only reduce computational\noverhead without rehearsal but also address the data heterogeneity issue.\nExtensive experiments show that FedSSI achieves superior performance compared\nto state-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continual Federated Learning (CFL) allows distributed devices to\ncollaboratively learn novel concepts from continuously shifting training data\nwhile avoiding knowledge forgetting of previously seen tasks. To tackle this\nchallenge, most current CFL approaches rely on extensive rehearsal of previous\ndata. Despite effectiveness, rehearsal comes at a cost to memory, and it may\nalso violate data privacy. Considering these, we seek to apply regularization\ntechniques to CFL by considering their cost-efficient properties that do not\nrequire sample caching or rehearsal. Specifically, we first apply traditional\nregularization techniques to CFL and observe that existing regularization\ntechniques, especially synaptic intelligence, can achieve promising results\nunder homogeneous data distribution but fail when the data is heterogeneous.\nBased on this observation, we propose a simple yet effective regularization\nalgorithm for CFL named FedSSI, which tailors the synaptic intelligence for the\nCFL with heterogeneous data settings. FedSSI can not only reduce computational\noverhead without rehearsal but also address the data heterogeneity issue.\nExtensive experiments show that FedSSI achieves superior performance compared\nto state-of-the-art methods."
                },
                "authors": [
                    {
                        "name": "Yichen Li"
                    },
                    {
                        "name": "Yuying Wang"
                    },
                    {
                        "name": "Haozhao Wang"
                    },
                    {
                        "name": "Yining Qi"
                    },
                    {
                        "name": "Tianzhe Xiao"
                    },
                    {
                        "name": "Ruixuan Li"
                    }
                ],
                "author_detail": {
                    "name": "Ruixuan Li"
                },
                "author": "Ruixuan Li",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2403.05890",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13779v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13779v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04326v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04326v1",
                "updated": "2025-05-07T11:21:12Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    11,
                    21,
                    12,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T11:21:12Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    11,
                    21,
                    12,
                    2,
                    127,
                    0
                ],
                "title": "Design and Evaluation of an NDN-Based Network for Distributed Digital\n  Twins",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Design and Evaluation of an NDN-Based Network for Distributed Digital\n  Twins"
                },
                "summary": "Digital twins (DT) have received significant attention due to their numerous\nbenefits, such as real-time data analytics and cost reduction in production. DT\nserves as a fundamental component of many applications, encompassing smart\nmanufacturing, intelligent vehicles, and smart cities. By using Machine\nLearning (ML) and Artificial Intelligence (AI) techniques, DTs can efficiently\nfacilitate decision-making and productivity by simulating the status and\nchanges of a physical entity. To handle the massive amount of data brought by\nDTs, it is challenging to achieve low response latency for data fetching over\nexisting IP-based networks. IP-based networks use host addresses for end-to-end\ncommunication, making data distribution between DTs inefficient. Thus, we\npropose to use DTs in a distributed manner over Named Data Networking (NDN)\nnetworks. NDN is data-centric where data is routed based on content names,\ndynamically adjusting paths to optimize latency. Popular data is cached in\nnetwork nodes, reducing data transmission and network congestion. Since data is\nfetched by content names, users and mobile devices can move freely without IP\naddress reassignment. By using in-network caching and adaptive routing, we\nreckon NDN is an ideal fit for Future G Networks in the context of Digital\nTwins. We compared DTs in edge scenarios with cloud scenarios over NDN and\nIP-based networks to validate our insights. Extensive simulation results show\nthat using DT in the edge reduces response latency by 10.2x. This position\npaper represents an initial investigation into the gap in distributed DTs over\nNDN, serving as an early-stage study.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital twins (DT) have received significant attention due to their numerous\nbenefits, such as real-time data analytics and cost reduction in production. DT\nserves as a fundamental component of many applications, encompassing smart\nmanufacturing, intelligent vehicles, and smart cities. By using Machine\nLearning (ML) and Artificial Intelligence (AI) techniques, DTs can efficiently\nfacilitate decision-making and productivity by simulating the status and\nchanges of a physical entity. To handle the massive amount of data brought by\nDTs, it is challenging to achieve low response latency for data fetching over\nexisting IP-based networks. IP-based networks use host addresses for end-to-end\ncommunication, making data distribution between DTs inefficient. Thus, we\npropose to use DTs in a distributed manner over Named Data Networking (NDN)\nnetworks. NDN is data-centric where data is routed based on content names,\ndynamically adjusting paths to optimize latency. Popular data is cached in\nnetwork nodes, reducing data transmission and network congestion. Since data is\nfetched by content names, users and mobile devices can move freely without IP\naddress reassignment. By using in-network caching and adaptive routing, we\nreckon NDN is an ideal fit for Future G Networks in the context of Digital\nTwins. We compared DTs in edge scenarios with cloud scenarios over NDN and\nIP-based networks to validate our insights. Extensive simulation results show\nthat using DT in the edge reduces response latency by 10.2x. This position\npaper represents an initial investigation into the gap in distributed DTs over\nNDN, serving as an early-stage study."
                },
                "authors": [
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Zihan Jia"
                    },
                    {
                        "name": "Ze Wang"
                    },
                    {
                        "name": "Lin Cui"
                    },
                    {
                        "name": "Fung Po Tso"
                    }
                ],
                "author_detail": {
                    "name": "Fung Po Tso"
                },
                "author": "Fung Po Tso",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04326v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04326v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04216v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04216v1",
                "updated": "2025-05-07T08:10:39Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    8,
                    10,
                    39,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T08:10:39Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    8,
                    10,
                    39,
                    2,
                    127,
                    0
                ],
                "title": "Computational Model for Photoionization in Pure SF6 Streamer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computational Model for Photoionization in Pure SF6 Streamer"
                },
                "summary": "Photoionization plays a crucial role in achieving spatial numerical\nconvergence and accurate quantitative predictions in SF6 streamer simulations,\nbut accurate models for SF6 photoionization remains limited, motivating this\npaper. First, we develop a computational model for SF6 photoionization and\nprovide the detailed modeling process. Then, we perform comparative studies\nagainst simplified approaches. The results demonstrate that the proposed model\neffectively captures the non-local effects of SF6 photoionization, enhancing\nboth the spatial numerical convergence and the accuracy of the streamer\nstructure. Finally, we perform comparative studies by artificially increasing\nthe photoionization intensity through multiplying the photoionization source\nterm Sph by a factor of 10 (10*Sph) relative to the baseline intensity.\nRegarding breakdown voltage prediction, 10*Sph leads to a significant\nunderestimation of the breakdown voltage for positive streamers, introducing\nerrors greater than 0.5 kV, while exerting a relatively small impact on\nnegative streamers. Regarding streamer propagation dynamics, 10*Sph reduces the\ncontraction at the positive streamer head and significantly lowers the local\nfield by more than 700 Td, thereby slowing down its speed. In contrast, 10*Sph\nhas little impact on the morphology of the negative streamers and slightly\nenhances the local field by less than 200 Td, thereby consistently accelerating\nits propagation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Photoionization plays a crucial role in achieving spatial numerical\nconvergence and accurate quantitative predictions in SF6 streamer simulations,\nbut accurate models for SF6 photoionization remains limited, motivating this\npaper. First, we develop a computational model for SF6 photoionization and\nprovide the detailed modeling process. Then, we perform comparative studies\nagainst simplified approaches. The results demonstrate that the proposed model\neffectively captures the non-local effects of SF6 photoionization, enhancing\nboth the spatial numerical convergence and the accuracy of the streamer\nstructure. Finally, we perform comparative studies by artificially increasing\nthe photoionization intensity through multiplying the photoionization source\nterm Sph by a factor of 10 (10*Sph) relative to the baseline intensity.\nRegarding breakdown voltage prediction, 10*Sph leads to a significant\nunderestimation of the breakdown voltage for positive streamers, introducing\nerrors greater than 0.5 kV, while exerting a relatively small impact on\nnegative streamers. Regarding streamer propagation dynamics, 10*Sph reduces the\ncontraction at the positive streamer head and significantly lowers the local\nfield by more than 700 Td, thereby slowing down its speed. In contrast, 10*Sph\nhas little impact on the morphology of the negative streamers and slightly\nenhances the local field by less than 200 Td, thereby consistently accelerating\nits propagation."
                },
                "authors": [
                    {
                        "name": "Zihao Feng"
                    }
                ],
                "author_detail": {
                    "name": "Zihao Feng"
                },
                "author": "Zihao Feng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04216v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04216v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12224v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12224v2",
                "updated": "2025-05-07T07:57:21Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    7,
                    57,
                    21,
                    2,
                    127,
                    0
                ],
                "published": "2025-02-17T14:54:14Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    14,
                    54,
                    14,
                    0,
                    48,
                    0
                ],
                "title": "Fate: Fast Edge Inference of Mixture-of-Experts Models via Cross-Layer\n  Gate",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fate: Fast Edge Inference of Mixture-of-Experts Models via Cross-Layer\n  Gate"
                },
                "summary": "Large Language Models (LLMs) have demonstrated impressive performance across\nvarious tasks, and their application in edge scenarios has attracted\nsignificant attention. However, sparse-activated Mixture-of-Experts (MoE)\nmodels, which are well suited for edge scenarios, have received relatively\nlittle attention due to their high memory demands. Offload-based methods have\nbeen proposed to address this challenge, but they face difficulties with expert\nprediction. Inaccurate expert predictions can result in prolonged inference\ndelays. To promote the application of MoE models in edge scenarios, we propose\nFate, an offloading system designed for MoE models to enable efficient\ninference in resource-constrained environments. The key insight behind Fate is\nthat gate inputs from adjacent layers can be effectively used for expert\nprefetching, achieving high prediction accuracy without additional GPU\noverhead. Furthermore, Fate employs a shallow-favoring expert caching strategy\nthat increases the expert hit rate to 99\\%. Additionally, Fate integrates\ntailored quantization strategies for cache optimization and IO efficiency.\nExperimental results show that, compared to Load on Demand and Expert\nActivation Path-based method, Fate achieves up to 4.5x and 1.9x speedups in\nprefill speed and up to 4.1x and 2.2x speedups in decoding speed, respectively,\nwhile maintaining inference quality. Moreover, Fate's performance improvements\nare scalable across different memory budgets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated impressive performance across\nvarious tasks, and their application in edge scenarios has attracted\nsignificant attention. However, sparse-activated Mixture-of-Experts (MoE)\nmodels, which are well suited for edge scenarios, have received relatively\nlittle attention due to their high memory demands. Offload-based methods have\nbeen proposed to address this challenge, but they face difficulties with expert\nprediction. Inaccurate expert predictions can result in prolonged inference\ndelays. To promote the application of MoE models in edge scenarios, we propose\nFate, an offloading system designed for MoE models to enable efficient\ninference in resource-constrained environments. The key insight behind Fate is\nthat gate inputs from adjacent layers can be effectively used for expert\nprefetching, achieving high prediction accuracy without additional GPU\noverhead. Furthermore, Fate employs a shallow-favoring expert caching strategy\nthat increases the expert hit rate to 99\\%. Additionally, Fate integrates\ntailored quantization strategies for cache optimization and IO efficiency.\nExperimental results show that, compared to Load on Demand and Expert\nActivation Path-based method, Fate achieves up to 4.5x and 1.9x speedups in\nprefill speed and up to 4.1x and 2.2x speedups in decoding speed, respectively,\nwhile maintaining inference quality. Moreover, Fate's performance improvements\nare scalable across different memory budgets."
                },
                "authors": [
                    {
                        "name": "Zhiyuan Fang"
                    },
                    {
                        "name": "Zicong Hong"
                    },
                    {
                        "name": "Yuegui Huang"
                    },
                    {
                        "name": "Yufeng Lyu"
                    },
                    {
                        "name": "Wuhui Chen"
                    },
                    {
                        "name": "Yue Yu"
                    },
                    {
                        "name": "Fan Yu"
                    },
                    {
                        "name": "Zibin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zibin Zheng"
                },
                "author": "Zibin Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12224v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12224v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04129v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04129v1",
                "updated": "2025-05-07T05:00:10Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    5,
                    0,
                    10,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T05:00:10Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    5,
                    0,
                    10,
                    2,
                    127,
                    0
                ],
                "title": "Maxing Out the SVM: Performance Impact of Memory and Program Cache Sizes\n  in the Agave Validator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Maxing Out the SVM: Performance Impact of Memory and Program Cache Sizes\n  in the Agave Validator"
                },
                "summary": "In this paper we analyze some of the bottlenecks in the execution pipeline of\nSolana's Agave validator client, focusing on RAM and program cache usage under\nmainnet conditions. Through a series of controlled experiments, we measure the\nvalidator's throughput and resource efficiency as RAM availability ranges\nbetween 128 GB to 1,536 GB (1.5 TB). We discover that the validator performance\ndegrades significantly below 256 GB, with transaction processing falling behind\nreal-time block production. Additionally, we study the program cache behavior,\nidentifying inefficiencies in program eviction and load latency. Our results\nprovide practical guidance for hardware provisioning and suggest improvements\nto the Solana execution and caching strategy, reducing latency due to the\nprogram cache by 90%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper we analyze some of the bottlenecks in the execution pipeline of\nSolana's Agave validator client, focusing on RAM and program cache usage under\nmainnet conditions. Through a series of controlled experiments, we measure the\nvalidator's throughput and resource efficiency as RAM availability ranges\nbetween 128 GB to 1,536 GB (1.5 TB). We discover that the validator performance\ndegrades significantly below 256 GB, with transaction processing falling behind\nreal-time block production. Additionally, we study the program cache behavior,\nidentifying inefficiencies in program eviction and load latency. Our results\nprovide practical guidance for hardware provisioning and suggest improvements\nto the Solana execution and caching strategy, reducing latency due to the\nprogram cache by 90%."
                },
                "authors": [
                    {
                        "name": "Turan Vural"
                    },
                    {
                        "name": "Yuki Yuminaga"
                    },
                    {
                        "name": "Alex Petrosyan"
                    },
                    {
                        "name": "Ben Livshits"
                    }
                ],
                "author_detail": {
                    "name": "Ben Livshits"
                },
                "author": "Ben Livshits",
                "arxiv_comment": "15 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04129v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04129v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02380v7",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02380v7",
                "updated": "2025-05-15T13:48:40Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    13,
                    48,
                    40,
                    3,
                    135,
                    0
                ],
                "published": "2025-01-04T20:59:34Z",
                "published_parsed": [
                    2025,
                    1,
                    4,
                    20,
                    59,
                    34,
                    5,
                    4,
                    0
                ],
                "title": "Reciprocating Locks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reciprocating Locks"
                },
                "summary": "We present \"Reciprocating Locks\", a novel mutual exclusion locking algorithm,\ntargeting cache-coherent shared memory (CC), that enjoys a number of desirable\nproperties. The doorway arrival phase and the release operation both run in\nconstant-time. Waiting threads use local spinning and only a single waiting\nelement is required per thread, regardless of the number of locks a thread\nmight hold at a given time. While our lock does not provide strict FIFO\nadmission, it bounds bypass and has strong anti-starvation properties. The lock\nis compact, space efficient, and has been intentionally designed to be readily\nusable in real-world general purpose computing environments such as the linux\nkernel, pthreads, or C++. We show the lock exhibits high throughput under\ncontention and low latency in the uncontended case. The performance of\nReciprocating Locks is competitive with and often better than the best\nstate-of-the-art scalable spin locks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present \"Reciprocating Locks\", a novel mutual exclusion locking algorithm,\ntargeting cache-coherent shared memory (CC), that enjoys a number of desirable\nproperties. The doorway arrival phase and the release operation both run in\nconstant-time. Waiting threads use local spinning and only a single waiting\nelement is required per thread, regardless of the number of locks a thread\nmight hold at a given time. While our lock does not provide strict FIFO\nadmission, it bounds bypass and has strong anti-starvation properties. The lock\nis compact, space efficient, and has been intentionally designed to be readily\nusable in real-world general purpose computing environments such as the linux\nkernel, pthreads, or C++. We show the lock exhibits high throughput under\ncontention and low latency in the uncontended case. The performance of\nReciprocating Locks is competitive with and often better than the best\nstate-of-the-art scalable spin locks."
                },
                "authors": [
                    {
                        "name": "Dave Dice"
                    },
                    {
                        "name": "Alex Kogan"
                    }
                ],
                "author_detail": {
                    "name": "Alex Kogan"
                },
                "author": "Alex Kogan",
                "arxiv_comment": "Added additional variations in appendix, at the request of\n  collaborators who want to prove various properties",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02380v7",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02380v7",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.4.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12240v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12240v3",
                "updated": "2025-05-06T15:23:12Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    15,
                    23,
                    12,
                    1,
                    126,
                    0
                ],
                "published": "2025-04-16T16:45:19Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    16,
                    45,
                    19,
                    2,
                    106,
                    0
                ],
                "title": "Cobra: Efficient Line Art COlorization with BRoAder References",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cobra: Efficient Line Art COlorization with BRoAder References"
                },
                "summary": "The comic production industry requires reference-based line art colorization\nwith high accuracy, efficiency, contextual consistency, and flexible control. A\ncomic page often involves diverse characters, objects, and backgrounds, which\ncomplicates the coloring process. Despite advancements in diffusion models for\nimage generation, their application in line art colorization remains limited,\nfacing challenges related to handling extensive reference images,\ntime-consuming inference, and flexible control. We investigate the necessity of\nextensive contextual image guidance on the quality of line art colorization. To\naddress these challenges, we introduce Cobra, an efficient and versatile method\nthat supports color hints and utilizes over 200 reference images while\nmaintaining low latency. Central to Cobra is a Causal Sparse DiT architecture,\nwhich leverages specially designed positional encodings, causal sparse\nattention, and Key-Value Cache to effectively manage long-context references\nand ensure color identity consistency. Results demonstrate that Cobra achieves\naccurate line art colorization through extensive contextual reference,\nsignificantly enhancing inference speed and interactivity, thereby meeting\ncritical industrial demands. We release our codes and models on our project\npage: https://zhuang2002.github.io/Cobra/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The comic production industry requires reference-based line art colorization\nwith high accuracy, efficiency, contextual consistency, and flexible control. A\ncomic page often involves diverse characters, objects, and backgrounds, which\ncomplicates the coloring process. Despite advancements in diffusion models for\nimage generation, their application in line art colorization remains limited,\nfacing challenges related to handling extensive reference images,\ntime-consuming inference, and flexible control. We investigate the necessity of\nextensive contextual image guidance on the quality of line art colorization. To\naddress these challenges, we introduce Cobra, an efficient and versatile method\nthat supports color hints and utilizes over 200 reference images while\nmaintaining low latency. Central to Cobra is a Causal Sparse DiT architecture,\nwhich leverages specially designed positional encodings, causal sparse\nattention, and Key-Value Cache to effectively manage long-context references\nand ensure color identity consistency. Results demonstrate that Cobra achieves\naccurate line art colorization through extensive contextual reference,\nsignificantly enhancing inference speed and interactivity, thereby meeting\ncritical industrial demands. We release our codes and models on our project\npage: https://zhuang2002.github.io/Cobra/."
                },
                "authors": [
                    {
                        "name": "Junhao Zhuang"
                    },
                    {
                        "name": "Lingen Li"
                    },
                    {
                        "name": "Xuan Ju"
                    },
                    {
                        "name": "Zhaoyang Zhang"
                    },
                    {
                        "name": "Chun Yuan"
                    },
                    {
                        "name": "Ying Shan"
                    }
                ],
                "author_detail": {
                    "name": "Ying Shan"
                },
                "author": "Ying Shan",
                "arxiv_comment": "Project page with code: https://zhuang2002.github.io/Cobra/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12240v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12240v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02922v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02922v1",
                "updated": "2025-05-05T18:01:17Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    18,
                    1,
                    17,
                    0,
                    125,
                    0
                ],
                "published": "2025-05-05T18:01:17Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    18,
                    1,
                    17,
                    0,
                    125,
                    0
                ],
                "title": "RetroInfer: A Vector-Storage Approach for Scalable Long-Context LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RetroInfer: A Vector-Storage Approach for Scalable Long-Context LLM\n  Inference"
                },
                "summary": "The growing context lengths of large language models (LLMs) pose significant\nchallenges for efficient inference, primarily due to GPU memory and bandwidth\nconstraints. We present RetroInfer, a novel system that reconceptualizes the\nkey-value (KV) cache as a vector storage system which exploits the inherent\nattention sparsity to accelerate long-context LLM inference. At its core is the\nwave index, an Attention-aWare VEctor index that enables efficient and accurate\nretrieval of critical tokens through techniques such as tripartite attention\napproximation, accuracy-bounded attention estimation, and segmented clustering.\nComplementing this is the wave buffer, which coordinates KV cache placement and\noverlaps computation and data transfer across GPU and CPU to sustain high\nthroughput. Unlike prior sparsity-based methods that struggle with token\nselection and hardware coordination, RetroInfer delivers robust performance\nwithout compromising model accuracy. Experiments on long-context benchmarks\nshow up to 4.5X speedup over full attention within GPU memory limits and up to\n10.5X over sparse attention baselines when KV cache is extended to CPU memory,\nall while preserving full-attention-level accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing context lengths of large language models (LLMs) pose significant\nchallenges for efficient inference, primarily due to GPU memory and bandwidth\nconstraints. We present RetroInfer, a novel system that reconceptualizes the\nkey-value (KV) cache as a vector storage system which exploits the inherent\nattention sparsity to accelerate long-context LLM inference. At its core is the\nwave index, an Attention-aWare VEctor index that enables efficient and accurate\nretrieval of critical tokens through techniques such as tripartite attention\napproximation, accuracy-bounded attention estimation, and segmented clustering.\nComplementing this is the wave buffer, which coordinates KV cache placement and\noverlaps computation and data transfer across GPU and CPU to sustain high\nthroughput. Unlike prior sparsity-based methods that struggle with token\nselection and hardware coordination, RetroInfer delivers robust performance\nwithout compromising model accuracy. Experiments on long-context benchmarks\nshow up to 4.5X speedup over full attention within GPU memory limits and up to\n10.5X over sparse attention baselines when KV cache is extended to CPU memory,\nall while preserving full-attention-level accuracy."
                },
                "authors": [
                    {
                        "name": "Yaoqi Chen"
                    },
                    {
                        "name": "Jinkai Zhang"
                    },
                    {
                        "name": "Baotong Lu"
                    },
                    {
                        "name": "Qianxi Zhang"
                    },
                    {
                        "name": "Chengruidong Zhang"
                    },
                    {
                        "name": "Jingjia Luo"
                    },
                    {
                        "name": "Di Liu"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Qi Chen"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Bailu Ding"
                    },
                    {
                        "name": "Xiao Yan"
                    },
                    {
                        "name": "Jiawei Jiang"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Mingxing Zhang"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Mao Yang"
                    }
                ],
                "author_detail": {
                    "name": "Mao Yang"
                },
                "author": "Mao Yang",
                "arxiv_comment": "16 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02922v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02922v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02533v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02533v1",
                "updated": "2025-05-05T10:16:16Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    10,
                    16,
                    16,
                    0,
                    125,
                    0
                ],
                "published": "2025-05-05T10:16:16Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    10,
                    16,
                    16,
                    0,
                    125,
                    0
                ],
                "title": "Large Language Model Partitioning for Low-Latency Inference at the Edge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model Partitioning for Low-Latency Inference at the Edge"
                },
                "summary": "Large Language Models (LLMs) based on autoregressive, decoder-only\nTransformers generate text one token at a time, where a token represents a\ndiscrete unit of text. As each newly produced token is appended to the partial\noutput sequence, the length grows and so does the memory and compute load, due\nto the expanding key-value caches, which store intermediate representations of\nall previously generated tokens in the multi-head attention (MHA) layer. As\nthis iterative process steadily increases memory and compute demands,\nlayer-based partitioning in resource-constrained edge environments often\nresults in memory overload or high inference latency. To address this and\nreduce inference latency, we propose a resource-aware Transformer architecture\npartitioning algorithm, where the partitioning decision is updated at regular\nintervals during token generation. The approach is myopic in that it is based\non instantaneous information about device resource availability and network\nlink bandwidths. When first executed, the algorithm places blocks on devices,\nand in later executions, it migrates these blocks among devices so that the sum\nof migration delay and inference delay remains low. Our approach partitions the\ndecoder at the attention head level, co-locating each attention head with its\nkey-value cache and allowing dynamic migrations whenever resources become\ntight. By allocating different attention heads to different devices, we exploit\nparallel execution of attention heads and thus achieve substantial reductions\nin inference delays. Our experiments show that in small-scale settings (3-5\ndevices), the proposed method achieves within 15 to 20 percent of an exact\noptimal solver's latency, while in larger-scale tests it achieves notable\nimprovements in inference speed and memory usage compared to state-of-the-art\nlayer-based partitioning approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) based on autoregressive, decoder-only\nTransformers generate text one token at a time, where a token represents a\ndiscrete unit of text. As each newly produced token is appended to the partial\noutput sequence, the length grows and so does the memory and compute load, due\nto the expanding key-value caches, which store intermediate representations of\nall previously generated tokens in the multi-head attention (MHA) layer. As\nthis iterative process steadily increases memory and compute demands,\nlayer-based partitioning in resource-constrained edge environments often\nresults in memory overload or high inference latency. To address this and\nreduce inference latency, we propose a resource-aware Transformer architecture\npartitioning algorithm, where the partitioning decision is updated at regular\nintervals during token generation. The approach is myopic in that it is based\non instantaneous information about device resource availability and network\nlink bandwidths. When first executed, the algorithm places blocks on devices,\nand in later executions, it migrates these blocks among devices so that the sum\nof migration delay and inference delay remains low. Our approach partitions the\ndecoder at the attention head level, co-locating each attention head with its\nkey-value cache and allowing dynamic migrations whenever resources become\ntight. By allocating different attention heads to different devices, we exploit\nparallel execution of attention heads and thus achieve substantial reductions\nin inference delays. Our experiments show that in small-scale settings (3-5\ndevices), the proposed method achieves within 15 to 20 percent of an exact\noptimal solver's latency, while in larger-scale tests it achieves notable\nimprovements in inference speed and memory usage compared to state-of-the-art\nlayer-based partitioning approaches."
                },
                "authors": [
                    {
                        "name": "Dimitrios Kafetzis"
                    },
                    {
                        "name": "Ramin Khalili"
                    },
                    {
                        "name": "Iordanis Koutsopoulos"
                    }
                ],
                "author_detail": {
                    "name": "Iordanis Koutsopoulos"
                },
                "author": "Iordanis Koutsopoulos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02533v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02533v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02346v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02346v1",
                "updated": "2025-05-05T04:01:56Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    4,
                    1,
                    56,
                    0,
                    125,
                    0
                ],
                "published": "2025-05-05T04:01:56Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    4,
                    1,
                    56,
                    0,
                    125,
                    0
                ],
                "title": "An Empirical Study on the Performance and Energy Usage of Compiled\n  Python Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Empirical Study on the Performance and Energy Usage of Compiled\n  Python Code"
                },
                "summary": "Python is a popular programming language known for its ease of learning and\nextensive libraries. However, concerns about performance and energy consumption\nhave led to the development of compilers to enhance Python code efficiency.\nDespite the proven benefits of existing compilers on the efficiency of Python\ncode, there is limited analysis comparing their performance and energy\nefficiency, particularly considering code characteristics and factors like CPU\nfrequency and core count. Our study investigates how compilation impacts the\nperformance and energy consumption of Python code, using seven benchmarks\ncompiled with eight different tools: PyPy, Numba, Nuitka, Mypyc, Codon, Cython,\nPyston-lite, and the experimental Python 3.13 version, compared to CPython. The\nbenchmarks are single-threaded and executed on an NUC and a server, measuring\nenergy usage, execution time, memory usage, and Last-Level Cache (LLC) miss\nrates at a fixed frequency and on a single core. The results show that\ncompilation can significantly enhance execution time, energy and memory usage,\nwith Codon, PyPy, and Numba achieving over 90\\% speed and energy improvements.\nNuitka optimizes memory usage consistently on both testbeds. The impact of\ncompilation on LLC miss rate is not clear since it varies considerably across\nbenchmarks for each compiler. Our study is important for researchers and\npractitioners focused on improving Python code performance and energy\nefficiency. We outline future research directions, such as exploring caching\neffects on energy usage. Our findings help practitioners choose the best\ncompiler based on their efficiency benefits and accessibility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Python is a popular programming language known for its ease of learning and\nextensive libraries. However, concerns about performance and energy consumption\nhave led to the development of compilers to enhance Python code efficiency.\nDespite the proven benefits of existing compilers on the efficiency of Python\ncode, there is limited analysis comparing their performance and energy\nefficiency, particularly considering code characteristics and factors like CPU\nfrequency and core count. Our study investigates how compilation impacts the\nperformance and energy consumption of Python code, using seven benchmarks\ncompiled with eight different tools: PyPy, Numba, Nuitka, Mypyc, Codon, Cython,\nPyston-lite, and the experimental Python 3.13 version, compared to CPython. The\nbenchmarks are single-threaded and executed on an NUC and a server, measuring\nenergy usage, execution time, memory usage, and Last-Level Cache (LLC) miss\nrates at a fixed frequency and on a single core. The results show that\ncompilation can significantly enhance execution time, energy and memory usage,\nwith Codon, PyPy, and Numba achieving over 90\\% speed and energy improvements.\nNuitka optimizes memory usage consistently on both testbeds. The impact of\ncompilation on LLC miss rate is not clear since it varies considerably across\nbenchmarks for each compiler. Our study is important for researchers and\npractitioners focused on improving Python code performance and energy\nefficiency. We outline future research directions, such as exploring caching\neffects on energy usage. Our findings help practitioners choose the best\ncompiler based on their efficiency benefits and accessibility."
                },
                "authors": [
                    {
                        "name": "Vincenzo Stoico"
                    },
                    {
                        "name": "Andrei Calin Dragomir"
                    },
                    {
                        "name": "Patricia Lago"
                    }
                ],
                "author_detail": {
                    "name": "Patricia Lago"
                },
                "author": "Patricia Lago",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02346v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02346v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10375v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10375v2",
                "updated": "2025-05-04T09:49:42Z",
                "updated_parsed": [
                    2025,
                    5,
                    4,
                    9,
                    49,
                    42,
                    6,
                    124,
                    0
                ],
                "published": "2024-12-16T07:59:21Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    7,
                    59,
                    21,
                    0,
                    351,
                    0
                ],
                "title": "DAOP: Data-Aware Offloading and Predictive Pre-Calculation for Efficient\n  MoE Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DAOP: Data-Aware Offloading and Predictive Pre-Calculation for Efficient\n  MoE Inference"
                },
                "summary": "Mixture-of-Experts (MoE) models, though highly effective for various machine\nlearning tasks, face significant deployment challenges on memory-constrained\ndevices. While GPUs offer fast inference, their limited memory compared to CPUs\nmeans not all experts can be stored on the GPU simultaneously, necessitating\nfrequent, costly data transfers from CPU memory, often negating GPU speed\nadvantages. To address this, we present DAOP, an on-device MoE inference engine\nto optimize parallel GPU-CPU execution. DAOP dynamically allocates experts\nbetween CPU and GPU based on per-sequence activation patterns, and selectively\npre-calculates predicted experts on CPUs to minimize transfer latency. This\napproach enables efficient resource utilization across various expert cache\nratios while maintaining model accuracy through a novel graceful degradation\nmechanism. Comprehensive evaluations across various datasets show that DAOP\noutperforms traditional expert caching and prefetching methods by up to 8.20x\nand offloading techniques by 1.35x while maintaining accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) models, though highly effective for various machine\nlearning tasks, face significant deployment challenges on memory-constrained\ndevices. While GPUs offer fast inference, their limited memory compared to CPUs\nmeans not all experts can be stored on the GPU simultaneously, necessitating\nfrequent, costly data transfers from CPU memory, often negating GPU speed\nadvantages. To address this, we present DAOP, an on-device MoE inference engine\nto optimize parallel GPU-CPU execution. DAOP dynamically allocates experts\nbetween CPU and GPU based on per-sequence activation patterns, and selectively\npre-calculates predicted experts on CPUs to minimize transfer latency. This\napproach enables efficient resource utilization across various expert cache\nratios while maintaining model accuracy through a novel graceful degradation\nmechanism. Comprehensive evaluations across various datasets show that DAOP\noutperforms traditional expert caching and prefetching methods by up to 8.20x\nand offloading techniques by 1.35x while maintaining accuracy."
                },
                "authors": [
                    {
                        "name": "Yujie Zhang"
                    },
                    {
                        "name": "Shivam Aggarwal"
                    },
                    {
                        "name": "Tulika Mitra"
                    }
                ],
                "author_detail": {
                    "name": "Tulika Mitra"
                },
                "author": "Tulika Mitra",
                "arxiv_comment": "7 pages, 10 figures, Accepted by DATE Conference 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10375v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10375v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02027v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02027v1",
                "updated": "2025-05-04T08:30:00Z",
                "updated_parsed": [
                    2025,
                    5,
                    4,
                    8,
                    30,
                    0,
                    6,
                    124,
                    0
                ],
                "published": "2025-05-04T08:30:00Z",
                "published_parsed": [
                    2025,
                    5,
                    4,
                    8,
                    30,
                    0,
                    6,
                    124,
                    0
                ],
                "title": "GraphPrompter: Multi-stage Adaptive Prompt Optimization for Graph\n  In-Context Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GraphPrompter: Multi-stage Adaptive Prompt Optimization for Graph\n  In-Context Learning"
                },
                "summary": "Graph In-Context Learning, with the ability to adapt pre-trained graph models\nto novel and diverse downstream graphs without updating any parameters, has\ngained much attention in the community. The key to graph in-context learning is\nto perform downstream graphs conditioned on chosen prompt examples. Existing\nmethods randomly select subgraphs or edges as prompts, leading to noisy graph\nprompts and inferior model performance. Additionally, due to the gap between\npre-training and testing graphs, when the number of classes in the testing\ngraphs is much greater than that in the training, the in-context learning\nability will also significantly deteriorate. To tackle the aforementioned\nchallenges, we develop a multi-stage adaptive prompt optimization method\nGraphPrompter, which optimizes the entire process of generating, selecting, and\nusing graph prompts for better in-context learning capabilities. Firstly,\nPrompt Generator introduces a reconstruction layer to highlight the most\ninformative edges and reduce irrelevant noise for graph prompt construction.\nFurthermore, in the selection stage, Prompt Selector employs the $k$-nearest\nneighbors algorithm and pre-trained selection layers to dynamically choose\nappropriate samples and minimize the influence of irrelevant prompts. Finally,\nwe leverage a Prompt Augmenter with a cache replacement strategy to enhance the\ngeneralization capability of the pre-trained model on new datasets. Extensive\nexperiments show that GraphPrompter effectively enhances the in-context\nlearning ability of graph models. On average across all the settings, our\napproach surpasses the state-of-the-art baselines by over 8%. Our code is\nreleased at https://github.com/karin0018/GraphPrompter.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph In-Context Learning, with the ability to adapt pre-trained graph models\nto novel and diverse downstream graphs without updating any parameters, has\ngained much attention in the community. The key to graph in-context learning is\nto perform downstream graphs conditioned on chosen prompt examples. Existing\nmethods randomly select subgraphs or edges as prompts, leading to noisy graph\nprompts and inferior model performance. Additionally, due to the gap between\npre-training and testing graphs, when the number of classes in the testing\ngraphs is much greater than that in the training, the in-context learning\nability will also significantly deteriorate. To tackle the aforementioned\nchallenges, we develop a multi-stage adaptive prompt optimization method\nGraphPrompter, which optimizes the entire process of generating, selecting, and\nusing graph prompts for better in-context learning capabilities. Firstly,\nPrompt Generator introduces a reconstruction layer to highlight the most\ninformative edges and reduce irrelevant noise for graph prompt construction.\nFurthermore, in the selection stage, Prompt Selector employs the $k$-nearest\nneighbors algorithm and pre-trained selection layers to dynamically choose\nappropriate samples and minimize the influence of irrelevant prompts. Finally,\nwe leverage a Prompt Augmenter with a cache replacement strategy to enhance the\ngeneralization capability of the pre-trained model on new datasets. Extensive\nexperiments show that GraphPrompter effectively enhances the in-context\nlearning ability of graph models. On average across all the settings, our\napproach surpasses the state-of-the-art baselines by over 8%. Our code is\nreleased at https://github.com/karin0018/GraphPrompter."
                },
                "authors": [
                    {
                        "name": "Rui Lv"
                    },
                    {
                        "name": "Zaixi Zhang"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Qi Liu"
                    },
                    {
                        "name": "Weibo Gao"
                    },
                    {
                        "name": "Jiawei Liu"
                    },
                    {
                        "name": "Jiaxia Yan"
                    },
                    {
                        "name": "Linan Yue"
                    },
                    {
                        "name": "Fangzhou Yao"
                    }
                ],
                "author_detail": {
                    "name": "Fangzhou Yao"
                },
                "author": "Fangzhou Yao",
                "arxiv_comment": "14 pages. IEEE International Conference on Data Engineering\n  (ICDE'2025), accepted",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02027v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02027v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07578v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07578v3",
                "updated": "2025-05-03T04:07:07Z",
                "updated_parsed": [
                    2025,
                    5,
                    3,
                    4,
                    7,
                    7,
                    5,
                    123,
                    0
                ],
                "published": "2025-02-11T14:25:20Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    14,
                    25,
                    20,
                    1,
                    42,
                    0
                ],
                "title": "PIM Is All You Need: A CXL-Enabled GPU-Free System for Large Language\n  Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PIM Is All You Need: A CXL-Enabled GPU-Free System for Large Language\n  Model Inference"
                },
                "summary": "Large Language Model (LLM) inference uses an autoregressive manner to\ngenerate one token at a time, which exhibits notably lower operational\nintensity compared to earlier Machine Learning (ML) models such as encoder-only\ntransformers and Convolutional Neural Networks. At the same time, LLMs possess\nlarge parameter sizes and use key-value caches to store context information.\nModern LLMs support context windows with up to 1 million tokens to generate\nversatile text, audio, and video content. A large key-value cache unique to\neach prompt requires a large memory capacity, limiting the inference batch\nsize. Both low operational intensity and limited batch size necessitate a high\nmemory bandwidth. However, contemporary hardware systems for ML model\ndeployment, such as GPUs and TPUs, are primarily optimized for compute\nthroughput. This mismatch challenges the efficient deployment of advanced LLMs\nand makes users pay for expensive compute resources that are poorly utilized\nfor the memory-bound LLM inference tasks.\n  We propose CENT, a CXL-ENabled GPU-Free sysTem for LLM inference, which\nharnesses CXL memory expansion capabilities to accommodate substantial LLM\nsizes, and utilizes near-bank processing units to deliver high memory\nbandwidth, eliminating the need for expensive GPUs. CENT exploits a scalable\nCXL network to support peer-to-peer and collective communication primitives\nacross CXL devices. We implement various parallelism strategies to distribute\nLLMs across these devices. Compared to GPU baselines with maximum supported\nbatch sizes and similar average power, CENT achieves 2.3$\\times$ higher\nthroughput and consumes 2.9$\\times$ less energy. CENT enhances the Total Cost\nof Ownership (TCO), generating 5.2$\\times$ more tokens per dollar than GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) inference uses an autoregressive manner to\ngenerate one token at a time, which exhibits notably lower operational\nintensity compared to earlier Machine Learning (ML) models such as encoder-only\ntransformers and Convolutional Neural Networks. At the same time, LLMs possess\nlarge parameter sizes and use key-value caches to store context information.\nModern LLMs support context windows with up to 1 million tokens to generate\nversatile text, audio, and video content. A large key-value cache unique to\neach prompt requires a large memory capacity, limiting the inference batch\nsize. Both low operational intensity and limited batch size necessitate a high\nmemory bandwidth. However, contemporary hardware systems for ML model\ndeployment, such as GPUs and TPUs, are primarily optimized for compute\nthroughput. This mismatch challenges the efficient deployment of advanced LLMs\nand makes users pay for expensive compute resources that are poorly utilized\nfor the memory-bound LLM inference tasks.\n  We propose CENT, a CXL-ENabled GPU-Free sysTem for LLM inference, which\nharnesses CXL memory expansion capabilities to accommodate substantial LLM\nsizes, and utilizes near-bank processing units to deliver high memory\nbandwidth, eliminating the need for expensive GPUs. CENT exploits a scalable\nCXL network to support peer-to-peer and collective communication primitives\nacross CXL devices. We implement various parallelism strategies to distribute\nLLMs across these devices. Compared to GPU baselines with maximum supported\nbatch sizes and similar average power, CENT achieves 2.3$\\times$ higher\nthroughput and consumes 2.9$\\times$ less energy. CENT enhances the Total Cost\nof Ownership (TCO), generating 5.2$\\times$ more tokens per dollar than GPUs."
                },
                "authors": [
                    {
                        "name": "Yufeng Gu"
                    },
                    {
                        "name": "Alireza Khadem"
                    },
                    {
                        "name": "Sumanth Umesh"
                    },
                    {
                        "name": "Ning Liang"
                    },
                    {
                        "name": "Xavier Servot"
                    },
                    {
                        "name": "Onur Mutlu"
                    },
                    {
                        "name": "Ravi Iyer"
                    },
                    {
                        "name": "Reetuparna Das"
                    }
                ],
                "author_detail": {
                    "name": "Reetuparna Das"
                },
                "author": "Reetuparna Das",
                "arxiv_doi": "10.1145/3676641.3716267",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3676641.3716267",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.07578v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07578v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "In Proceedings of the 30th ACM International Conference on\n  Architectural Support for Programming Languages and Operating Systems, Volume\n  2 (ASPLOS'25)",
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20335v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20335v2",
                "updated": "2025-05-03T01:10:30Z",
                "updated_parsed": [
                    2025,
                    5,
                    3,
                    1,
                    10,
                    30,
                    5,
                    123,
                    0
                ],
                "published": "2025-04-29T00:58:59Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    0,
                    58,
                    59,
                    1,
                    119,
                    0
                ],
                "title": "VA-CDH: A Variance-Aware Method to Optimize Latency for Caching with\n  Delayed Hits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VA-CDH: A Variance-Aware Method to Optimize Latency for Caching with\n  Delayed Hits"
                },
                "summary": "Caches are fundamental to latency-sensitive systems like Content Delivery\nNetworks (CDNs) and Mobile Edge Computing (MEC). However, the delayed hit\nphenomenon where multiple requests for an object occur during its fetch from\nthe remote server after a miss significantly inflates user-perceived latency.\nWhile recent algorithms acknowledge delayed hits by estimating the resulting\naggregate delay, they predominantly focus on its mean value. We identify and\ndemonstrate that such approaches are insufficient, as the real aggregate delay\nfrequently exhibits substantial variance in the true production system, leading\nto suboptimal latency performance when ignored. Thus, we propose VA-CDH, a\nvariance-aware method to optimize latency for caching with delayed hits. It\nemploys a novel ranking function that explicitly incorporates both the\nempirically estimated mean and standard deviation of aggregate delay, allowing\ncaching decisions to account for its variation. We derive the analytical\ndistribution of aggregate delay under Poisson arrivals as a theoretical\ncontribution, offering more statistical insight beyond the mean value. Through\nthe simulations conducted on synthetic and real-world datasets, we show that\nVA-CDH reduces the total latency by 1%-6% approximately compared to\nstate-of-the-art algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caches are fundamental to latency-sensitive systems like Content Delivery\nNetworks (CDNs) and Mobile Edge Computing (MEC). However, the delayed hit\nphenomenon where multiple requests for an object occur during its fetch from\nthe remote server after a miss significantly inflates user-perceived latency.\nWhile recent algorithms acknowledge delayed hits by estimating the resulting\naggregate delay, they predominantly focus on its mean value. We identify and\ndemonstrate that such approaches are insufficient, as the real aggregate delay\nfrequently exhibits substantial variance in the true production system, leading\nto suboptimal latency performance when ignored. Thus, we propose VA-CDH, a\nvariance-aware method to optimize latency for caching with delayed hits. It\nemploys a novel ranking function that explicitly incorporates both the\nempirically estimated mean and standard deviation of aggregate delay, allowing\ncaching decisions to account for its variation. We derive the analytical\ndistribution of aggregate delay under Poisson arrivals as a theoretical\ncontribution, offering more statistical insight beyond the mean value. Through\nthe simulations conducted on synthetic and real-world datasets, we show that\nVA-CDH reduces the total latency by 1%-6% approximately compared to\nstate-of-the-art algorithms."
                },
                "authors": [
                    {
                        "name": "Bowen Jiang"
                    },
                    {
                        "name": "Chaofan Ma"
                    },
                    {
                        "name": "Duo Wang"
                    }
                ],
                "author_detail": {
                    "name": "Duo Wang"
                },
                "author": "Duo Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20335v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20335v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13298v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13298v3",
                "updated": "2025-05-02T13:55:21Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    13,
                    55,
                    21,
                    4,
                    122,
                    0
                ],
                "published": "2025-01-23T00:57:01Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    0,
                    57,
                    1,
                    3,
                    23,
                    0
                ],
                "title": "Collaborative Coded Caching for Partially Connected Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Collaborative Coded Caching for Partially Connected Networks"
                },
                "summary": "Coded caching leverages the differences in user cache memories to achieve\ngains that scale with the total cache size, alleviating network congestion due\nto high-quality content requests. Additionally, distributing transmitters over\na wide area can mitigate the adverse effects of path loss. In this work, we\nconsider a partially connected network where the channel between distributed\ntransmitters (helpers) and users is modeled as a distributed\nmultiple-input-multiple-output (MIMO) Gaussian broadcast channel. We propose a\nnovel delivery scheme consisting of two phases: partitioning and transmission.\nIn the partitioning phase, users with identical cache profiles are partitioned\ninto the minimum number of sets, such that users within each set can\nsuccessfully decode their desired message from a joint transmission enabled by\nMIMO precoding. To optimally partition the users, we employ the branch and\nbound method. In the transmission phase, each partition is treated as a single\nentity, and codewords are multicast to partitions with distinct cache profiles.\nThe proposed delivery scheme is applicable to any partially connected network,\nand while the partitioning is optimal, the overall delivery scheme, including\ntransmission, is heuristic. Interestingly, simulation results show that its\nperformance closely approximates that of the fully connected optimal solution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coded caching leverages the differences in user cache memories to achieve\ngains that scale with the total cache size, alleviating network congestion due\nto high-quality content requests. Additionally, distributing transmitters over\na wide area can mitigate the adverse effects of path loss. In this work, we\nconsider a partially connected network where the channel between distributed\ntransmitters (helpers) and users is modeled as a distributed\nmultiple-input-multiple-output (MIMO) Gaussian broadcast channel. We propose a\nnovel delivery scheme consisting of two phases: partitioning and transmission.\nIn the partitioning phase, users with identical cache profiles are partitioned\ninto the minimum number of sets, such that users within each set can\nsuccessfully decode their desired message from a joint transmission enabled by\nMIMO precoding. To optimally partition the users, we employ the branch and\nbound method. In the transmission phase, each partition is treated as a single\nentity, and codewords are multicast to partitions with distinct cache profiles.\nThe proposed delivery scheme is applicable to any partially connected network,\nand while the partitioning is optimal, the overall delivery scheme, including\ntransmission, is heuristic. Interestingly, simulation results show that its\nperformance closely approximates that of the fully connected optimal solution."
                },
                "authors": [
                    {
                        "name": "Kagan Akcay"
                    },
                    {
                        "name": "Eleftherios Lampiris"
                    },
                    {
                        "name": "MohammadJavad Salehi"
                    },
                    {
                        "name": "Giuseppe Caire"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Caire"
                },
                "author": "Giuseppe Caire",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13298v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13298v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01164v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01164v1",
                "updated": "2025-05-02T10:13:12Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    10,
                    13,
                    12,
                    4,
                    122,
                    0
                ],
                "published": "2025-05-02T10:13:12Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    10,
                    13,
                    12,
                    4,
                    122,
                    0
                ],
                "title": "CaGR-RAG: Context-aware Query Grouping for Disk-based Vector Search in\n  RAG Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CaGR-RAG: Context-aware Query Grouping for Disk-based Vector Search in\n  RAG Systems"
                },
                "summary": "Modern embedding models capture both semantic and syntactic structures of\nqueries, often mapping different queries to similar regions in vector space.\nThis results in non-uniform cluster access patterns in disk-based vector search\nsystems, particularly in Retrieval Augmented Generation (RAG) framework. While\nexisting approaches optimize individual queries, they overlook the impact of\ncluster access patterns, failing to account for the locality effects of queries\nthat access similar clusters. This oversight reduces cache efficiency and\nincreases search latency due to excessive disk I/O. To address this, we\nintroduce CaGR-RAG, a context-aware query grouping mechanism that organizes\nqueries based on shared cluster access patterns. Additionally, it incorporates\nopportunistic cluster prefetching to minimize cache misses during transitions\nbetween query groups, further optimizing retrieval performance. Experimental\nresults show that CaGR-RAG reduces 99th percentile tail latency by up to 51.55%\nwhile consistently maintaining a higher cache hit ratio than the baseline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern embedding models capture both semantic and syntactic structures of\nqueries, often mapping different queries to similar regions in vector space.\nThis results in non-uniform cluster access patterns in disk-based vector search\nsystems, particularly in Retrieval Augmented Generation (RAG) framework. While\nexisting approaches optimize individual queries, they overlook the impact of\ncluster access patterns, failing to account for the locality effects of queries\nthat access similar clusters. This oversight reduces cache efficiency and\nincreases search latency due to excessive disk I/O. To address this, we\nintroduce CaGR-RAG, a context-aware query grouping mechanism that organizes\nqueries based on shared cluster access patterns. Additionally, it incorporates\nopportunistic cluster prefetching to minimize cache misses during transitions\nbetween query groups, further optimizing retrieval performance. Experimental\nresults show that CaGR-RAG reduces 99th percentile tail latency by up to 51.55%\nwhile consistently maintaining a higher cache hit ratio than the baseline."
                },
                "authors": [
                    {
                        "name": "Yeonwoo Jeong"
                    },
                    {
                        "name": "Kyuli Park"
                    },
                    {
                        "name": "Hyunji Cho"
                    },
                    {
                        "name": "Sungyong Park"
                    }
                ],
                "author_detail": {
                    "name": "Sungyong Park"
                },
                "author": "Sungyong Park",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01164v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01164v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01002v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01002v1",
                "updated": "2025-05-02T04:57:06Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    4,
                    57,
                    6,
                    4,
                    122,
                    0
                ],
                "published": "2025-05-02T04:57:06Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    4,
                    57,
                    6,
                    4,
                    122,
                    0
                ],
                "title": "High Voltage Delivery and Distribution for the NEXT-100 Time Projection\n  Chamber",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High Voltage Delivery and Distribution for the NEXT-100 Time Projection\n  Chamber"
                },
                "summary": "A critical element in the realization of large liquid and gas time projection\nchambers (TPCs) is the delivery and distribution of high voltages into and\naround the detector. Such experiments require of order tens of kilovolts to\nenable electron drift over meter-scale distances. This paper describes the\ndesign and operation of the cathode feedthrough and high voltage distribution\nthrough the field cage of the NEXT-100 experiment, an underground TPC that will\nsearch for neutrinoless double beta decay $0\\nu\\beta\\beta$. The feedthrough has\nbeen demonstrated to hold pressures up to 20~bar and sustain voltages as high\nas -65~kV, and the TPC is operating stably at its design high voltages. The\nsystem has been realized within the constraints of a stringent radiopurity\nbudget and is now being used to execute a suite of sensitive double beta decay\nanalyses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A critical element in the realization of large liquid and gas time projection\nchambers (TPCs) is the delivery and distribution of high voltages into and\naround the detector. Such experiments require of order tens of kilovolts to\nenable electron drift over meter-scale distances. This paper describes the\ndesign and operation of the cathode feedthrough and high voltage distribution\nthrough the field cage of the NEXT-100 experiment, an underground TPC that will\nsearch for neutrinoless double beta decay $0\\nu\\beta\\beta$. The feedthrough has\nbeen demonstrated to hold pressures up to 20~bar and sustain voltages as high\nas -65~kV, and the TPC is operating stably at its design high voltages. The\nsystem has been realized within the constraints of a stringent radiopurity\nbudget and is now being used to execute a suite of sensitive double beta decay\nanalyses."
                },
                "authors": [
                    {
                        "name": "NEXT Collaboration"
                    },
                    {
                        "name": "C. Adams"
                    },
                    {
                        "name": "H. Almazán"
                    },
                    {
                        "name": "V. Álvarez"
                    },
                    {
                        "name": "K. Bailey"
                    },
                    {
                        "name": "R. Guenette"
                    },
                    {
                        "name": "B. J. P. Jones"
                    },
                    {
                        "name": "S. Johnston"
                    },
                    {
                        "name": "K. Mistry"
                    },
                    {
                        "name": "F. Monrabal"
                    },
                    {
                        "name": "D. R. Nygren"
                    },
                    {
                        "name": "B. Palmeiro"
                    },
                    {
                        "name": "L. Rogers"
                    },
                    {
                        "name": "J. Waldschmidt"
                    },
                    {
                        "name": "B. Aparicio"
                    },
                    {
                        "name": "A. I. Aranburu"
                    },
                    {
                        "name": "L. Arazi"
                    },
                    {
                        "name": "I. J. Arnquist"
                    },
                    {
                        "name": "F. Auria-Luna"
                    },
                    {
                        "name": "S. Ayet"
                    },
                    {
                        "name": "C. D. R. Azevedo"
                    },
                    {
                        "name": "F. Ballester"
                    },
                    {
                        "name": "M. del Barrio-Torregrosa"
                    },
                    {
                        "name": "A. Bayo"
                    },
                    {
                        "name": "J. M. Benlloch-Rodríguez"
                    },
                    {
                        "name": "F. I. G. M. Borges"
                    },
                    {
                        "name": "A. Brodolin"
                    },
                    {
                        "name": "S. Cárcel"
                    },
                    {
                        "name": "A. Castillo"
                    },
                    {
                        "name": "L. Cid"
                    },
                    {
                        "name": "C. A. N. Conde"
                    },
                    {
                        "name": "T. Contreras"
                    },
                    {
                        "name": "F. P. Cossío"
                    },
                    {
                        "name": "R. Coupe"
                    },
                    {
                        "name": "E. Dey"
                    },
                    {
                        "name": "G. Díaz"
                    },
                    {
                        "name": "C. Echevarria"
                    },
                    {
                        "name": "M. Elorza"
                    },
                    {
                        "name": "J. Escada"
                    },
                    {
                        "name": "R. Esteve"
                    },
                    {
                        "name": "R. Felkai"
                    },
                    {
                        "name": "L. M. P. Fernandes"
                    },
                    {
                        "name": "P. Ferrario"
                    },
                    {
                        "name": "A. L. Ferreira"
                    },
                    {
                        "name": "F. W. Foss"
                    },
                    {
                        "name": "Z. Freixa"
                    },
                    {
                        "name": "J. García-Barrena"
                    },
                    {
                        "name": "J. J. Gómez-Cadenas"
                    },
                    {
                        "name": "J. W. R. Grocott"
                    },
                    {
                        "name": "R. Guenette"
                    },
                    {
                        "name": "J. Hauptman"
                    },
                    {
                        "name": "C. A. O. Henriques"
                    },
                    {
                        "name": "J. A. Hernando Morata"
                    },
                    {
                        "name": "P. Herrero-Gómez"
                    },
                    {
                        "name": "V. Herrero"
                    },
                    {
                        "name": "C. Hervés Carrete"
                    },
                    {
                        "name": "Y. Ifergan"
                    },
                    {
                        "name": "F. Kellerer"
                    },
                    {
                        "name": "L. Larizgoitia"
                    },
                    {
                        "name": "A. Larumbe"
                    },
                    {
                        "name": "P. Lebrun"
                    },
                    {
                        "name": "F. Lopez"
                    },
                    {
                        "name": "N. López-March"
                    },
                    {
                        "name": "R. Madigan"
                    },
                    {
                        "name": "R. D. P. Mano"
                    },
                    {
                        "name": "A. P. Marques"
                    },
                    {
                        "name": "J. Martín-Albo"
                    },
                    {
                        "name": "G. Martínez-Lema"
                    },
                    {
                        "name": "M. Martínez-Vara"
                    },
                    {
                        "name": "R. L. Miller"
                    },
                    {
                        "name": "J. Molina-Canteras"
                    },
                    {
                        "name": "F. Monrabal"
                    },
                    {
                        "name": "C. M. B. Monteiro"
                    },
                    {
                        "name": "F. J. Mora"
                    },
                    {
                        "name": "P. Novella"
                    },
                    {
                        "name": "A. Nuñez"
                    },
                    {
                        "name": "E. Oblak"
                    },
                    {
                        "name": "J. Palacio"
                    },
                    {
                        "name": "B. Palmeiro"
                    },
                    {
                        "name": "A. Para"
                    },
                    {
                        "name": "A. Pazos"
                    },
                    {
                        "name": "J. Pelegrin"
                    },
                    {
                        "name": "M. Pérez Maneiro"
                    },
                    {
                        "name": "M. Querol"
                    },
                    {
                        "name": "J. Renner"
                    },
                    {
                        "name": "I. Rivilla"
                    },
                    {
                        "name": "C. Rogero"
                    },
                    {
                        "name": "B. Romeo"
                    },
                    {
                        "name": "C. Romo-Luque"
                    },
                    {
                        "name": "V. San Nacienciano"
                    },
                    {
                        "name": "F. P. Santos"
                    },
                    {
                        "name": "J. M. F. dos Santos"
                    },
                    {
                        "name": "M. Seemann"
                    },
                    {
                        "name": "I. Shomroni"
                    },
                    {
                        "name": "P. A. O. C. Silva"
                    },
                    {
                        "name": "A. Simón"
                    },
                    {
                        "name": "S. R. Soleti"
                    },
                    {
                        "name": "M. Sorel"
                    },
                    {
                        "name": "J. Soto-Oton"
                    },
                    {
                        "name": "J. M. R. Teixeira"
                    },
                    {
                        "name": "S. Teruel-Pardo"
                    },
                    {
                        "name": "J. F. Toledo"
                    },
                    {
                        "name": "C. Tonnelé"
                    },
                    {
                        "name": "S. Torelli"
                    },
                    {
                        "name": "J. Torrent"
                    },
                    {
                        "name": "A. Trettin"
                    },
                    {
                        "name": "A. Usón"
                    },
                    {
                        "name": "P. R. G. Valle"
                    },
                    {
                        "name": "J. F. C. A. Veloso"
                    },
                    {
                        "name": "J. Waiton"
                    },
                    {
                        "name": "A. Yubero-Navarro"
                    }
                ],
                "author_detail": {
                    "name": "A. Yubero-Navarro"
                },
                "author": "A. Yubero-Navarro",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01002v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01002v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00962v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00962v1",
                "updated": "2025-05-02T02:36:23Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    2,
                    36,
                    23,
                    4,
                    122,
                    0
                ],
                "published": "2025-05-02T02:36:23Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    2,
                    36,
                    23,
                    4,
                    122,
                    0
                ],
                "title": "The Open-Source BlackParrot-BedRock Cache Coherence System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Open-Source BlackParrot-BedRock Cache Coherence System"
                },
                "summary": "This dissertation revisits the topic of programmable cache coherence engines\nin the context of modern shared-memory multicore processors. First, the\nopen-source BedRock cache coherence protocol is described. BedRock employs the\ncanonical MOESIF coherence states and reduces implementation burden by\neliminating transient coherence states from the protocol. The protocol's design\ncomplexity, concurrency, and verification effort are analyzed and compared to a\ncanonical directory-based invalidate coherence protocol. Second, the\narchitecture and microarchitecture of three separate cache coherence\ndirectories implementing the BedRock protocol within the BlackParrot 64-bit\nRISC-V multicore processor, collectively called BlackParrot-BedRock\n(BP-BedRock), are described. A fixed-function coherence directory engine\nimplementation provides a baseline design for performance and area comparisons.\nA microcode-programmable coherence directory implementation demonstrates the\nfeasibility of implementing a programmable coherence engine capable of\nmaintaining sufficient protocol processing performance. A hybrid fixed-function\nand programmable coherence directory blends the protocol processing performance\nof the fixed-function design with the programmable flexibility of the\nmicrocode-programmable design. Collectively, the BedRock coherence protocol and\nits three BP-BedRock implementations demonstrate the feasibility and challenges\nof including programmable logic within the coherence system of modern\nshared-memory multicore processors, paving the way for future research into the\napplication- and system-level benefits of programmable coherence engines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This dissertation revisits the topic of programmable cache coherence engines\nin the context of modern shared-memory multicore processors. First, the\nopen-source BedRock cache coherence protocol is described. BedRock employs the\ncanonical MOESIF coherence states and reduces implementation burden by\neliminating transient coherence states from the protocol. The protocol's design\ncomplexity, concurrency, and verification effort are analyzed and compared to a\ncanonical directory-based invalidate coherence protocol. Second, the\narchitecture and microarchitecture of three separate cache coherence\ndirectories implementing the BedRock protocol within the BlackParrot 64-bit\nRISC-V multicore processor, collectively called BlackParrot-BedRock\n(BP-BedRock), are described. A fixed-function coherence directory engine\nimplementation provides a baseline design for performance and area comparisons.\nA microcode-programmable coherence directory implementation demonstrates the\nfeasibility of implementing a programmable coherence engine capable of\nmaintaining sufficient protocol processing performance. A hybrid fixed-function\nand programmable coherence directory blends the protocol processing performance\nof the fixed-function design with the programmable flexibility of the\nmicrocode-programmable design. Collectively, the BedRock coherence protocol and\nits three BP-BedRock implementations demonstrate the feasibility and challenges\nof including programmable logic within the coherence system of modern\nshared-memory multicore processors, paving the way for future research into the\napplication- and system-level benefits of programmable coherence engines."
                },
                "authors": [
                    {
                        "name": "Mark Unruh Wyse"
                    }
                ],
                "author_detail": {
                    "name": "Mark Unruh Wyse"
                },
                "author": "Mark Unruh Wyse",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00962v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00962v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00901v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00901v1",
                "updated": "2025-05-01T22:32:29Z",
                "updated_parsed": [
                    2025,
                    5,
                    1,
                    22,
                    32,
                    29,
                    3,
                    121,
                    0
                ],
                "published": "2025-05-01T22:32:29Z",
                "published_parsed": [
                    2025,
                    5,
                    1,
                    22,
                    32,
                    29,
                    3,
                    121,
                    0
                ],
                "title": "Heterogeneous Memory Benchmarking Toolkit",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Heterogeneous Memory Benchmarking Toolkit"
                },
                "summary": "This paper presents an open-source kernel-level heterogeneous memory\ncharacterization framework (MemScope) for embedded systems that enables users\nto understand and precisely characterize the temporal behavior of all available\nmemory modules under configurable contention stress scenarios. Since\nkernel-level provides a high degree of control over allocation, cache\nmaintenance, $CPUs$, interrupts, and I/O device activity, seeking the most\naccurate way to benchmark heterogeneous memory subsystems, would be achieved by\nimplementing it in the kernel. This gives us the privilege to directly map\npieces of contiguous physical memory and instantiate allocators, allowing us to\nfinely control cores to create and eliminate interference. Additionally, we can\nminimize noise and interruptions, guaranteeing more consistent and precise\nresults compared to equivalent user-space solutions. Running our Framework on a\nXilinx Zynq UltraScale+ ZCU102 CPU_FPGA platform, demonstrates its capability\nto precisely benchmark bandwidth and latency across various memory types,\nincluding PL-side DRAM and BRAM, in a multi-core system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents an open-source kernel-level heterogeneous memory\ncharacterization framework (MemScope) for embedded systems that enables users\nto understand and precisely characterize the temporal behavior of all available\nmemory modules under configurable contention stress scenarios. Since\nkernel-level provides a high degree of control over allocation, cache\nmaintenance, $CPUs$, interrupts, and I/O device activity, seeking the most\naccurate way to benchmark heterogeneous memory subsystems, would be achieved by\nimplementing it in the kernel. This gives us the privilege to directly map\npieces of contiguous physical memory and instantiate allocators, allowing us to\nfinely control cores to create and eliminate interference. Additionally, we can\nminimize noise and interruptions, guaranteeing more consistent and precise\nresults compared to equivalent user-space solutions. Running our Framework on a\nXilinx Zynq UltraScale+ ZCU102 CPU_FPGA platform, demonstrates its capability\nto precisely benchmark bandwidth and latency across various memory types,\nincluding PL-side DRAM and BRAM, in a multi-core system."
                },
                "authors": [
                    {
                        "name": "Golsana Ghaemi"
                    },
                    {
                        "name": "Kazem Taram"
                    },
                    {
                        "name": "Renato Mancuso"
                    }
                ],
                "author_detail": {
                    "name": "Renato Mancuso"
                },
                "author": "Renato Mancuso",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00901v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00901v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00817v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00817v1",
                "updated": "2025-05-01T19:18:56Z",
                "updated_parsed": [
                    2025,
                    5,
                    1,
                    19,
                    18,
                    56,
                    3,
                    121,
                    0
                ],
                "published": "2025-05-01T19:18:56Z",
                "published_parsed": [
                    2025,
                    5,
                    1,
                    19,
                    18,
                    56,
                    3,
                    121,
                    0
                ],
                "title": "Spill The Beans: Exploiting CPU Cache Side-Channels to Leak Tokens from\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spill The Beans: Exploiting CPU Cache Side-Channels to Leak Tokens from\n  Large Language Models"
                },
                "summary": "Side-channel attacks on shared hardware resources increasingly threaten\nconfidentiality, especially with the rise of Large Language Models (LLMs). In\nthis work, we introduce Spill The Beans, a novel application of cache\nside-channels to leak tokens generated by an LLM. By co-locating an attack\nprocess on the same hardware as the victim model, we flush and reload embedding\nvectors from the embedding layer, where each token corresponds to a unique\nembedding vector. When accessed during token generation, it results in a cache\nhit detectable by our attack on shared lower-level caches.\n  A significant challenge is the massive size of LLMs, which, by nature of\ntheir compute intensive operation, quickly evicts embedding vectors from the\ncache. We address this by balancing the number of tokens monitored against the\namount of information leaked. Monitoring more tokens increases potential\nvocabulary leakage but raises the chance of missing cache hits due to eviction;\nmonitoring fewer tokens improves detection reliability but limits vocabulary\ncoverage.\n  Through extensive experimentation, we demonstrate the feasibility of leaking\ntokens from LLMs via cache side-channels. Our findings reveal a new\nvulnerability in LLM deployments, highlighting that even sophisticated models\nare susceptible to traditional side-channel attacks. We discuss the\nimplications for privacy and security in LLM-serving infrastructures and\nsuggest considerations for mitigating such threats. For proof of concept we\nconsider two concrete attack scenarios: Our experiments show that an attacker\ncan recover as much as 80%-90% of a high entropy API key with single shot\nmonitoring. As for English text we can reach a 40% recovery rate with a single\nshot. We should note that the rate highly depends on the monitored token set\nand these rates can be improved by targeting more specialized output domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Side-channel attacks on shared hardware resources increasingly threaten\nconfidentiality, especially with the rise of Large Language Models (LLMs). In\nthis work, we introduce Spill The Beans, a novel application of cache\nside-channels to leak tokens generated by an LLM. By co-locating an attack\nprocess on the same hardware as the victim model, we flush and reload embedding\nvectors from the embedding layer, where each token corresponds to a unique\nembedding vector. When accessed during token generation, it results in a cache\nhit detectable by our attack on shared lower-level caches.\n  A significant challenge is the massive size of LLMs, which, by nature of\ntheir compute intensive operation, quickly evicts embedding vectors from the\ncache. We address this by balancing the number of tokens monitored against the\namount of information leaked. Monitoring more tokens increases potential\nvocabulary leakage but raises the chance of missing cache hits due to eviction;\nmonitoring fewer tokens improves detection reliability but limits vocabulary\ncoverage.\n  Through extensive experimentation, we demonstrate the feasibility of leaking\ntokens from LLMs via cache side-channels. Our findings reveal a new\nvulnerability in LLM deployments, highlighting that even sophisticated models\nare susceptible to traditional side-channel attacks. We discuss the\nimplications for privacy and security in LLM-serving infrastructures and\nsuggest considerations for mitigating such threats. For proof of concept we\nconsider two concrete attack scenarios: Our experiments show that an attacker\ncan recover as much as 80%-90% of a high entropy API key with single shot\nmonitoring. As for English text we can reach a 40% recovery rate with a single\nshot. We should note that the rate highly depends on the monitored token set\nand these rates can be improved by targeting more specialized output domains."
                },
                "authors": [
                    {
                        "name": "Andrew Adiletta"
                    },
                    {
                        "name": "Berk Sunar"
                    }
                ],
                "author_detail": {
                    "name": "Berk Sunar"
                },
                "author": "Berk Sunar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00817v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00817v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "K.6.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00768v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00768v1",
                "updated": "2025-05-01T18:00:40Z",
                "updated_parsed": [
                    2025,
                    5,
                    1,
                    18,
                    0,
                    40,
                    3,
                    121,
                    0
                ],
                "published": "2025-05-01T18:00:40Z",
                "published_parsed": [
                    2025,
                    5,
                    1,
                    18,
                    0,
                    40,
                    3,
                    121,
                    0
                ],
                "title": "Optomechanical resource for fault-tolerant quantum computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optomechanical resource for fault-tolerant quantum computing"
                },
                "summary": "Fusion-based quantum computing with dual-rail qubits is a leading candidate\nfor scalable quantum computing using linear optics. This paradigm requires\nsingle photons which are entangled into small resource states before being fed\ninto a fusion network. The most common sources for single optical photons and\nfor small entangled states are probabilistic and heralded. The realization of a\nsingle reliable deterministic source requires many redundant probabilistic\nsources and a complex optical network for rerouting and retiming probabilistic\noutputs. In this work, we show how optomechanics enables reliable production of\nresources for photonic quantum computing without the redundancy of the\nall-optical approach. This is achieved by using acoustic modes as caches of\nquantum resources, ranging from single-particle states to small entangled\nstates, with on-demand read-out. The advantages of acoustic modes as optical\nquantum memories, compared to other technologies, include their intrinsically\nlong lifetimes and that they are solid state, highly tailorable, and\ninsensitive to electromagnetic noise. We show how the resource states can be\nprepared directly in the acoustic modes using optical controls. This is still\nprobabilistic and heralded, as in the all-optical approach, but the acoustic\nmodes act as a quantum memory which is integrated into the production of the\nstates. The quantum states may be deterministically transferred from acoustic\nmodes to optical modes, on demand, with another optical drive.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fusion-based quantum computing with dual-rail qubits is a leading candidate\nfor scalable quantum computing using linear optics. This paradigm requires\nsingle photons which are entangled into small resource states before being fed\ninto a fusion network. The most common sources for single optical photons and\nfor small entangled states are probabilistic and heralded. The realization of a\nsingle reliable deterministic source requires many redundant probabilistic\nsources and a complex optical network for rerouting and retiming probabilistic\noutputs. In this work, we show how optomechanics enables reliable production of\nresources for photonic quantum computing without the redundancy of the\nall-optical approach. This is achieved by using acoustic modes as caches of\nquantum resources, ranging from single-particle states to small entangled\nstates, with on-demand read-out. The advantages of acoustic modes as optical\nquantum memories, compared to other technologies, include their intrinsically\nlong lifetimes and that they are solid state, highly tailorable, and\ninsensitive to electromagnetic noise. We show how the resource states can be\nprepared directly in the acoustic modes using optical controls. This is still\nprobabilistic and heralded, as in the all-optical approach, but the acoustic\nmodes act as a quantum memory which is integrated into the production of the\nstates. The quantum states may be deterministically transferred from acoustic\nmodes to optical modes, on demand, with another optical drive."
                },
                "authors": [
                    {
                        "name": "Margaret Pavlovich"
                    },
                    {
                        "name": "Peter Rakich"
                    },
                    {
                        "name": "Shruti Puri"
                    }
                ],
                "author_detail": {
                    "name": "Shruti Puri"
                },
                "author": "Shruti Puri",
                "arxiv_comment": "19 pages, 9 figures. Supplement 29 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00768v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00768v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00570v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00570v1",
                "updated": "2025-05-01T14:53:12Z",
                "updated_parsed": [
                    2025,
                    5,
                    1,
                    14,
                    53,
                    12,
                    3,
                    121,
                    0
                ],
                "published": "2025-05-01T14:53:12Z",
                "published_parsed": [
                    2025,
                    5,
                    1,
                    14,
                    53,
                    12,
                    3,
                    121,
                    0
                ],
                "title": "FreqKV: Frequency Domain Key-Value Compression for Efficient Context\n  Window Extension",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FreqKV: Frequency Domain Key-Value Compression for Efficient Context\n  Window Extension"
                },
                "summary": "Extending the context window in large language models (LLMs) is essential for\napplications involving long-form content generation. However, the linear\nincrease in key-value (KV) cache memory requirements and the quadratic\ncomplexity of self-attention with respect to sequence length present\nsignificant challenges during fine-tuning and inference. Existing methods\nsuffer from performance degradation when extending to longer contexts. In this\nwork, we introduce a novel context extension method that optimizes both\nfine-tuning and inference efficiency. Our method exploits a key observation: in\nthe frequency domain, the energy distribution of the KV cache is primarily\nconcentrated in low-frequency components. By filtering out the high-frequency\ncomponents, the KV cache can be effectively compressed with minimal information\nloss. Building on this insight, we propose an efficient compression technique,\nFreqKV, that iteratively compresses the increasing KV cache to a fixed size in\nthe frequency domain, applicable to both fine-tuning and inference. FreqKV\nintroduces no additional parameters or architectural modifications. With\nminimal fine-tuning, LLMs can learn to leverage the limited cache that is\ncompressed in the frequency domain and extend the context window efficiently.\nExperiments on various long context language modeling and understanding tasks\ndemonstrate the efficiency and efficacy of the proposed method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extending the context window in large language models (LLMs) is essential for\napplications involving long-form content generation. However, the linear\nincrease in key-value (KV) cache memory requirements and the quadratic\ncomplexity of self-attention with respect to sequence length present\nsignificant challenges during fine-tuning and inference. Existing methods\nsuffer from performance degradation when extending to longer contexts. In this\nwork, we introduce a novel context extension method that optimizes both\nfine-tuning and inference efficiency. Our method exploits a key observation: in\nthe frequency domain, the energy distribution of the KV cache is primarily\nconcentrated in low-frequency components. By filtering out the high-frequency\ncomponents, the KV cache can be effectively compressed with minimal information\nloss. Building on this insight, we propose an efficient compression technique,\nFreqKV, that iteratively compresses the increasing KV cache to a fixed size in\nthe frequency domain, applicable to both fine-tuning and inference. FreqKV\nintroduces no additional parameters or architectural modifications. With\nminimal fine-tuning, LLMs can learn to leverage the limited cache that is\ncompressed in the frequency domain and extend the context window efficiently.\nExperiments on various long context language modeling and understanding tasks\ndemonstrate the efficiency and efficacy of the proposed method."
                },
                "authors": [
                    {
                        "name": "Jushi Kai"
                    },
                    {
                        "name": "Boyi Zeng"
                    },
                    {
                        "name": "Yixuan Wang"
                    },
                    {
                        "name": "Haoli Bai"
                    },
                    {
                        "name": "Bo Jiang"
                    },
                    {
                        "name": "Zhouhan Lin"
                    }
                ],
                "author_detail": {
                    "name": "Zhouhan Lin"
                },
                "author": "Zhouhan Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00570v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00570v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00315v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00315v1",
                "updated": "2025-05-01T05:22:11Z",
                "updated_parsed": [
                    2025,
                    5,
                    1,
                    5,
                    22,
                    11,
                    3,
                    121,
                    0
                ],
                "published": "2025-05-01T05:22:11Z",
                "published_parsed": [
                    2025,
                    5,
                    1,
                    5,
                    22,
                    11,
                    3,
                    121,
                    0
                ],
                "title": "Mixture of Sparse Attention: Content-Based Learnable Sparse Attention\n  via Expert-Choice Routing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture of Sparse Attention: Content-Based Learnable Sparse Attention\n  via Expert-Choice Routing"
                },
                "summary": "Recent advances in large language models highlighted the excessive quadratic\ncost of self-attention. Despite the significant research efforts, subquadratic\nattention methods still suffer from inferior performance in practice. We\nhypothesize that dynamic, learned content-based sparsity can lead to more\nefficient attention mechanisms. We present Mixture of Sparse Attention (MoSA),\na novel approach inspired by Mixture of Experts (MoE) with expert choice\nrouting. MoSA dynamically selects tokens for each attention head, allowing\narbitrary sparse attention patterns. By selecting $k$ tokens from a sequence of\nlength $T$, MoSA reduces the computational complexity of each attention head\nfrom $O(T^2)$ to $O(k^2 + T)$. This enables using more heads within the same\ncomputational budget, allowing higher specialization. We show that among the\ntested sparse attention variants, MoSA is the only one that can outperform the\ndense baseline, sometimes with up to 27% better perplexity for an identical\ncompute budget. MoSA can also reduce the resource usage compared to dense\nself-attention. Despite using torch implementation without an optimized kernel,\nperplexity-matched MoSA models are simultaneously faster in wall-clock time,\nrequire less memory for training, and drastically reduce the size of the\nKV-cache compared to the dense transformer baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models highlighted the excessive quadratic\ncost of self-attention. Despite the significant research efforts, subquadratic\nattention methods still suffer from inferior performance in practice. We\nhypothesize that dynamic, learned content-based sparsity can lead to more\nefficient attention mechanisms. We present Mixture of Sparse Attention (MoSA),\na novel approach inspired by Mixture of Experts (MoE) with expert choice\nrouting. MoSA dynamically selects tokens for each attention head, allowing\narbitrary sparse attention patterns. By selecting $k$ tokens from a sequence of\nlength $T$, MoSA reduces the computational complexity of each attention head\nfrom $O(T^2)$ to $O(k^2 + T)$. This enables using more heads within the same\ncomputational budget, allowing higher specialization. We show that among the\ntested sparse attention variants, MoSA is the only one that can outperform the\ndense baseline, sometimes with up to 27% better perplexity for an identical\ncompute budget. MoSA can also reduce the resource usage compared to dense\nself-attention. Despite using torch implementation without an optimized kernel,\nperplexity-matched MoSA models are simultaneously faster in wall-clock time,\nrequire less memory for training, and drastically reduce the size of the\nKV-cache compared to the dense transformer baselines."
                },
                "authors": [
                    {
                        "name": "Piotr Piękos"
                    },
                    {
                        "name": "Róbert Csordás"
                    },
                    {
                        "name": "Jürgen Schmidhuber"
                    }
                ],
                "author_detail": {
                    "name": "Jürgen Schmidhuber"
                },
                "author": "Jürgen Schmidhuber",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00315v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00315v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.04532v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.04532v3",
                "updated": "2025-05-01T02:14:05Z",
                "updated_parsed": [
                    2025,
                    5,
                    1,
                    2,
                    14,
                    5,
                    3,
                    121,
                    0
                ],
                "published": "2024-05-07T17:59:30Z",
                "published_parsed": [
                    2024,
                    5,
                    7,
                    17,
                    59,
                    30,
                    1,
                    128,
                    0
                ],
                "title": "QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM\n  Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM\n  Serving"
                },
                "summary": "Quantization can accelerate large language model (LLM) inference. Going\nbeyond INT8 quantization, the research community is actively exploring even\nlower precision, such as INT4. Nonetheless, state-of-the-art INT4 quantization\ntechniques only accelerate low-batch, edge LLM inference, failing to deliver\nperformance gains in large-batch, cloud-based LLM serving. We uncover a\ncritical issue: existing INT4 quantization methods suffer from significant\nruntime overhead (20-90%) when dequantizing either weights or partial sums on\nGPUs. To address this challenge, we introduce QoQ, a W4A8KV4 quantization\nalgorithm with 4-bit weight, 8-bit activation, and 4-bit KV cache. QoQ stands\nfor quattuor-octo-quattuor, which represents 4-8-4 in Latin. QoQ is implemented\nby the QServe inference library that achieves measured speedup. The key insight\ndriving QServe is that the efficiency of LLM serving on GPUs is critically\ninfluenced by operations on low-throughput CUDA cores. Building upon this\ninsight, in QoQ algorithm, we introduce progressive quantization that can allow\nlow dequantization overhead in W4A8 GEMM. Additionally, we develop\nSmoothAttention to effectively mitigate the accuracy degradation incurred by\n4-bit KV quantization. In the QServe system, we perform compute-aware weight\nreordering and take advantage of register-level parallelism to reduce\ndequantization latency. We also make fused attention memory-bound, harnessing\nthe performance gain brought by KV4 quantization. As a result, QServe improves\nthe maximum achievable serving throughput of Llama-3-8B by 1.2x on A100, 1.4x\non L40S; and Qwen1.5-72B by 2.4x on A100, 3.5x on L40S, compared to\nTensorRT-LLM. Remarkably, QServe on L40S GPU can achieve even higher throughput\nthan TensorRT-LLM on A100. Thus, QServe effectively reduces the dollar cost of\nLLM serving by 3x. Code is available at\nhttps://github.com/mit-han-lab/omniserve.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization can accelerate large language model (LLM) inference. Going\nbeyond INT8 quantization, the research community is actively exploring even\nlower precision, such as INT4. Nonetheless, state-of-the-art INT4 quantization\ntechniques only accelerate low-batch, edge LLM inference, failing to deliver\nperformance gains in large-batch, cloud-based LLM serving. We uncover a\ncritical issue: existing INT4 quantization methods suffer from significant\nruntime overhead (20-90%) when dequantizing either weights or partial sums on\nGPUs. To address this challenge, we introduce QoQ, a W4A8KV4 quantization\nalgorithm with 4-bit weight, 8-bit activation, and 4-bit KV cache. QoQ stands\nfor quattuor-octo-quattuor, which represents 4-8-4 in Latin. QoQ is implemented\nby the QServe inference library that achieves measured speedup. The key insight\ndriving QServe is that the efficiency of LLM serving on GPUs is critically\ninfluenced by operations on low-throughput CUDA cores. Building upon this\ninsight, in QoQ algorithm, we introduce progressive quantization that can allow\nlow dequantization overhead in W4A8 GEMM. Additionally, we develop\nSmoothAttention to effectively mitigate the accuracy degradation incurred by\n4-bit KV quantization. In the QServe system, we perform compute-aware weight\nreordering and take advantage of register-level parallelism to reduce\ndequantization latency. We also make fused attention memory-bound, harnessing\nthe performance gain brought by KV4 quantization. As a result, QServe improves\nthe maximum achievable serving throughput of Llama-3-8B by 1.2x on A100, 1.4x\non L40S; and Qwen1.5-72B by 2.4x on A100, 3.5x on L40S, compared to\nTensorRT-LLM. Remarkably, QServe on L40S GPU can achieve even higher throughput\nthan TensorRT-LLM on A100. Thus, QServe effectively reduces the dollar cost of\nLLM serving by 3x. Code is available at\nhttps://github.com/mit-han-lab/omniserve."
                },
                "authors": [
                    {
                        "name": "Yujun Lin"
                    },
                    {
                        "name": "Haotian Tang"
                    },
                    {
                        "name": "Shang Yang"
                    },
                    {
                        "name": "Zhekai Zhang"
                    },
                    {
                        "name": "Guangxuan Xiao"
                    },
                    {
                        "name": "Chuang Gan"
                    },
                    {
                        "name": "Song Han"
                    }
                ],
                "author_detail": {
                    "name": "Song Han"
                },
                "author": "Song Han",
                "arxiv_comment": "The first three authors contribute equally to this project and are\n  listed in the alphabetical order. Yujun Lin leads the quantization algorithm,\n  Haotian Tang and Shang Yang lead the GPU kernels and the serving system. Code\n  is available at https://github.com/mit-han-lab/omniserve",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.04532v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.04532v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19602v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19602v2",
                "updated": "2025-05-01T00:13:06Z",
                "updated_parsed": [
                    2025,
                    5,
                    1,
                    0,
                    13,
                    6,
                    3,
                    121,
                    0
                ],
                "published": "2025-04-28T09:04:30Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    9,
                    4,
                    30,
                    0,
                    118,
                    0
                ],
                "title": "Soft-Label Caching and Sharpening for Communication-Efficient Federated\n  Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Soft-Label Caching and Sharpening for Communication-Efficient Federated\n  Distillation"
                },
                "summary": "Federated Learning (FL) enables collaborative model training across\ndecentralized clients, enhancing privacy by keeping data local. Yet\nconventional FL, relying on frequent parameter-sharing, suffers from high\ncommunication overhead and limited model heterogeneity. Distillation-based FL\napproaches address these issues by sharing predictions (soft-labels) instead,\nbut they often involve redundant transmissions across communication rounds,\nreducing efficiency. We propose SCARLET, a novel framework integrating\nsynchronized soft-label caching and an enhanced Entropy Reduction Aggregation\n(Enhanced ERA) mechanism. SCARLET minimizes redundant communication by reusing\ncached soft-labels, achieving up to 50% reduction in communication costs\ncompared to existing methods while maintaining accuracy. Enhanced ERA can be\ntuned to adapt to non-IID data variations, ensuring robust aggregation and\nperformance in diverse client scenarios. Experimental evaluations demonstrate\nthat SCARLET consistently outperforms state-of-the-art distillation-based FL\nmethods in terms of accuracy and communication efficiency. The implementation\nof SCARLET is publicly available at https://github.com/kitsuyaazuma/SCARLET.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) enables collaborative model training across\ndecentralized clients, enhancing privacy by keeping data local. Yet\nconventional FL, relying on frequent parameter-sharing, suffers from high\ncommunication overhead and limited model heterogeneity. Distillation-based FL\napproaches address these issues by sharing predictions (soft-labels) instead,\nbut they often involve redundant transmissions across communication rounds,\nreducing efficiency. We propose SCARLET, a novel framework integrating\nsynchronized soft-label caching and an enhanced Entropy Reduction Aggregation\n(Enhanced ERA) mechanism. SCARLET minimizes redundant communication by reusing\ncached soft-labels, achieving up to 50% reduction in communication costs\ncompared to existing methods while maintaining accuracy. Enhanced ERA can be\ntuned to adapt to non-IID data variations, ensuring robust aggregation and\nperformance in diverse client scenarios. Experimental evaluations demonstrate\nthat SCARLET consistently outperforms state-of-the-art distillation-based FL\nmethods in terms of accuracy and communication efficiency. The implementation\nof SCARLET is publicly available at https://github.com/kitsuyaazuma/SCARLET."
                },
                "authors": [
                    {
                        "name": "Kitsuya Azuma"
                    },
                    {
                        "name": "Takayuki Nishio"
                    },
                    {
                        "name": "Yuichi Kitagawa"
                    },
                    {
                        "name": "Wakako Nakano"
                    },
                    {
                        "name": "Takahito Tanimura"
                    }
                ],
                "author_detail": {
                    "name": "Takahito Tanimura"
                },
                "author": "Takahito Tanimura",
                "arxiv_comment": "15 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19602v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19602v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19243v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19243v2",
                "updated": "2025-04-30T19:48:41Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    19,
                    48,
                    41,
                    2,
                    120,
                    0
                ],
                "published": "2025-01-31T15:58:15Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    15,
                    58,
                    15,
                    4,
                    31,
                    0
                ],
                "title": "Accelerating Diffusion Transformer via Error-Optimized Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Transformer via Error-Optimized Cache"
                },
                "summary": "Diffusion Transformer (DiT) is a crucial method for content generation.\nHowever, it needs a lot of time to sample. Many studies have attempted to use\ncaching to reduce the time consumption of sampling. Existing caching methods\naccelerate generation by reusing DiT features from the previous time step and\nskipping calculations in the next, but they tend to locate and cache low-error\nmodules without focusing on reducing caching-induced errors, resulting in a\nsharp decline in generated content quality when increasing caching intensity.\nTo solve this problem, we propose the Error-Optimized Cache (EOC). This method\nintroduces three key improvements: (1) Prior knowledge extraction: Extract and\nprocess the caching differences; (2) A judgment method for cache optimization:\nDetermine whether certain caching steps need to be optimized; (3) Cache\noptimization: reduce caching errors. Experiments show that this algorithm\nsignificantly reduces the error accumulation caused by caching, especially\nexcessive caching. On the ImageNet dataset, without substantially increasing\nthe computational load, this method improves the FID of the generated images\nwhen the rule-based model FORA has a caching level of 75%, 50%, and 25%, and\nthe training-based model Learning-to-cache has a caching level of 22%.\nSpecifically, the FID values change from 30.454 to 21.690 (28.8%), from 6.857\nto 5.821 (15.1%), from 3.870 to 3.692 (4.6%), and from 3.539 to 3.451 (2.5%)\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformer (DiT) is a crucial method for content generation.\nHowever, it needs a lot of time to sample. Many studies have attempted to use\ncaching to reduce the time consumption of sampling. Existing caching methods\naccelerate generation by reusing DiT features from the previous time step and\nskipping calculations in the next, but they tend to locate and cache low-error\nmodules without focusing on reducing caching-induced errors, resulting in a\nsharp decline in generated content quality when increasing caching intensity.\nTo solve this problem, we propose the Error-Optimized Cache (EOC). This method\nintroduces three key improvements: (1) Prior knowledge extraction: Extract and\nprocess the caching differences; (2) A judgment method for cache optimization:\nDetermine whether certain caching steps need to be optimized; (3) Cache\noptimization: reduce caching errors. Experiments show that this algorithm\nsignificantly reduces the error accumulation caused by caching, especially\nexcessive caching. On the ImageNet dataset, without substantially increasing\nthe computational load, this method improves the FID of the generated images\nwhen the rule-based model FORA has a caching level of 75%, 50%, and 25%, and\nthe training-based model Learning-to-cache has a caching level of 22%.\nSpecifically, the FID values change from 30.454 to 21.690 (28.8%), from 6.857\nto 5.821 (15.1%), from 3.870 to 3.692 (4.6%), and from 3.539 to 3.451 (2.5%)\nrespectively."
                },
                "authors": [
                    {
                        "name": "Junxiang Qiu"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Jinda Lu"
                    },
                    {
                        "name": "Lin Liu"
                    },
                    {
                        "name": "Houcheng Jiang"
                    },
                    {
                        "name": "Xingyu Zhu"
                    },
                    {
                        "name": "Yanbin Hao"
                    }
                ],
                "author_detail": {
                    "name": "Yanbin Hao"
                },
                "author": "Yanbin Hao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19243v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19243v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00074v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00074v1",
                "updated": "2025-04-30T18:00:02Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    18,
                    0,
                    2,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-30T18:00:02Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    18,
                    0,
                    2,
                    2,
                    120,
                    0
                ],
                "title": "SDW driven \"magnetic breakdown\" in a d-wave altermagnet KV$_2$Se$_2$O",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SDW driven \"magnetic breakdown\" in a d-wave altermagnet KV$_2$Se$_2$O"
                },
                "summary": "Altermagnets, combining zero net magnetization with intrinsic spin splitting,\ndemonstrate unique quantum phenomena crucial for spintronic applications.\nKV$_2$Se$_2$O is proven to be a d-wave altermagnet with phase transition from a\ncheckerboard-type (C-type) antiferromagnetic (AFM) state to a spin density wave\n(SDW) state as the temperature decreases. After phase transition, the apparent\nparadox emerges where angle-resolved photoemission spectroscopy (ARPES) reveals\nnegligible Fermi surface modifications, while physical property measurement\nsystem (PPMS) measurements uncover substantial changes in transport properties.\nOur study explores the microscopic mechanisms governing phase-dependent\ntransport properties of KV$_2$Se$_2$O base on first-principles calculations.\nThe spin canting driven by periodic spin modulation in the SDW phase reduces\nthe magnetic symmetry of KV$_2$Se$_2$O. The resultant band degeneracy lifting\nand Fermi surface reconstruction induce the ``magnetic breakdown\" phenomenon,\nwhich alters carrier trajectories, modifies carrier concentration, strengthens\nelectron-hole compensation, and ultimately accounts for the contrasting\nmagnetic-field-dependent Hall resistivity relative to the C-type AFM state. Our\nwork proposes an innovative method for identifying the electronic structure\nevolution across phase transitions from transport signatures, providing a novel\nparadigm for altermagnets research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Altermagnets, combining zero net magnetization with intrinsic spin splitting,\ndemonstrate unique quantum phenomena crucial for spintronic applications.\nKV$_2$Se$_2$O is proven to be a d-wave altermagnet with phase transition from a\ncheckerboard-type (C-type) antiferromagnetic (AFM) state to a spin density wave\n(SDW) state as the temperature decreases. After phase transition, the apparent\nparadox emerges where angle-resolved photoemission spectroscopy (ARPES) reveals\nnegligible Fermi surface modifications, while physical property measurement\nsystem (PPMS) measurements uncover substantial changes in transport properties.\nOur study explores the microscopic mechanisms governing phase-dependent\ntransport properties of KV$_2$Se$_2$O base on first-principles calculations.\nThe spin canting driven by periodic spin modulation in the SDW phase reduces\nthe magnetic symmetry of KV$_2$Se$_2$O. The resultant band degeneracy lifting\nand Fermi surface reconstruction induce the ``magnetic breakdown\" phenomenon,\nwhich alters carrier trajectories, modifies carrier concentration, strengthens\nelectron-hole compensation, and ultimately accounts for the contrasting\nmagnetic-field-dependent Hall resistivity relative to the C-type AFM state. Our\nwork proposes an innovative method for identifying the electronic structure\nevolution across phase transitions from transport signatures, providing a novel\nparadigm for altermagnets research."
                },
                "authors": [
                    {
                        "name": "Xu Yan"
                    },
                    {
                        "name": "Ziyin Song"
                    },
                    {
                        "name": "Juntao Song"
                    },
                    {
                        "name": "Zhong Fang"
                    },
                    {
                        "name": "Hongming Weng"
                    },
                    {
                        "name": "Quansheng Wu"
                    }
                ],
                "author_detail": {
                    "name": "Quansheng Wu"
                },
                "author": "Quansheng Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00074v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00074v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21594v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21594v1",
                "updated": "2025-04-30T12:51:59Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    12,
                    51,
                    59,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-30T12:51:59Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    12,
                    51,
                    59,
                    2,
                    120,
                    0
                ],
                "title": "Switching Transients in Constrained Transformer-Line/Cable\n  Configurations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Switching Transients in Constrained Transformer-Line/Cable\n  Configurations"
                },
                "summary": "This paper investigates the transient phenomena that occur in two special\ncases in the Netherlands: (A) during the energization of a power transformer\nvia a cable feeder and (B) the energization of a power transformer together\nwith an overhead line (OHL). In Case A a 7 km long 150 kV cable and a 150/50 kV\ntransformer are connected and energized at the same time. In Case B a 150/50 kV\ntransformer and a short 50 kV OHL are connected and energized simultaneously.\nThe reason behind this kind of situations is related to space restrictions and\ncost efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates the transient phenomena that occur in two special\ncases in the Netherlands: (A) during the energization of a power transformer\nvia a cable feeder and (B) the energization of a power transformer together\nwith an overhead line (OHL). In Case A a 7 km long 150 kV cable and a 150/50 kV\ntransformer are connected and energized at the same time. In Case B a 150/50 kV\ntransformer and a short 50 kV OHL are connected and energized simultaneously.\nThe reason behind this kind of situations is related to space restrictions and\ncost efficiency."
                },
                "authors": [
                    {
                        "name": "Y. Xiang"
                    },
                    {
                        "name": "L. Wu"
                    },
                    {
                        "name": "K. Velitsikakis"
                    },
                    {
                        "name": "A. L. J. Janssen"
                    }
                ],
                "author_detail": {
                    "name": "A. L. J. Janssen"
                },
                "author": "A. L. J. Janssen",
                "arxiv_comment": "11 pages, 17 figures, CIGRE conference 2016",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21594v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21594v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00745v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00745v1",
                "updated": "2025-04-30T08:08:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    8,
                    8,
                    15,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-30T08:08:15Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    8,
                    8,
                    15,
                    2,
                    120,
                    0
                ],
                "title": "Responsive DNN Adaptation for Video Analytics against Environment Shift\n  via Hierarchical Mobile-Cloud Collaborations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Responsive DNN Adaptation for Video Analytics against Environment Shift\n  via Hierarchical Mobile-Cloud Collaborations"
                },
                "summary": "Mobile video analysis systems often encounter various deploying environments,\nwhere environment shifts present greater demands for responsiveness in\nadaptations of deployed \"expert DNN models\". Existing model adaptation\nframeworks primarily operate in a cloud-centric way, exhibiting degraded\nperformance during adaptation and delayed reactions to environment shifts.\nInstead, this paper proposes MOCHA, a novel framework optimizing the\nresponsiveness of continuous model adaptation through hierarchical\ncollaborations between mobile and cloud resources. Specifically, MOCHA (1)\nreduces adaptation response delays by performing on-device model reuse and fast\nfine-tuning before requesting cloud model retrieval and end-to-end retraining;\n(2) accelerates history expert model retrieval by organizing them into a\nstructured taxonomy utilizing domain semantics analyzed by a cloud foundation\nmodel as indices; (3) enables efficient local model reuse by maintaining\nonboard expert model caches for frequent scenes, which proactively prefetch\nmodel weights from the cloud model database. Extensive evaluations with\nreal-world videos on three DNN tasks show MOCHA improves the model accuracy\nduring adaptation by up to 6.8% while saving the response delay and retraining\ntime by up to 35.5x and 3.0x respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mobile video analysis systems often encounter various deploying environments,\nwhere environment shifts present greater demands for responsiveness in\nadaptations of deployed \"expert DNN models\". Existing model adaptation\nframeworks primarily operate in a cloud-centric way, exhibiting degraded\nperformance during adaptation and delayed reactions to environment shifts.\nInstead, this paper proposes MOCHA, a novel framework optimizing the\nresponsiveness of continuous model adaptation through hierarchical\ncollaborations between mobile and cloud resources. Specifically, MOCHA (1)\nreduces adaptation response delays by performing on-device model reuse and fast\nfine-tuning before requesting cloud model retrieval and end-to-end retraining;\n(2) accelerates history expert model retrieval by organizing them into a\nstructured taxonomy utilizing domain semantics analyzed by a cloud foundation\nmodel as indices; (3) enables efficient local model reuse by maintaining\nonboard expert model caches for frequent scenes, which proactively prefetch\nmodel weights from the cloud model database. Extensive evaluations with\nreal-world videos on three DNN tasks show MOCHA improves the model accuracy\nduring adaptation by up to 6.8% while saving the response delay and retraining\ntime by up to 35.5x and 3.0x respectively."
                },
                "authors": [
                    {
                        "name": "Maozhe Zhao"
                    },
                    {
                        "name": "Shengzhong Liu"
                    },
                    {
                        "name": "Fan Wu"
                    },
                    {
                        "name": "Guihai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Guihai Chen"
                },
                "author": "Guihai Chen",
                "arxiv_comment": "Sensys 2025 final version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00745v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00745v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21230v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21230v1",
                "updated": "2025-04-29T23:43:59Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    23,
                    43,
                    59,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T23:43:59Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    23,
                    43,
                    59,
                    1,
                    119,
                    0
                ],
                "title": "Kimina Lean Server: Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Kimina Lean Server: Technical Report"
                },
                "summary": "We introduce the Kimina Lean Server, an open-source project that enables fast\nand scalable interaction with Lean 4 via a unified REST API, designed as a\nsimple verifier for reinforcement learning pipelines. Built on top of the Lean\nFRO's LeanREPL, it combines server-side parallelization by managing multiple\nLean REPL processes in parallel, with an LRU caching strategy that reuses Lean\nimports across multiple requests. These features help reduce initialization\noverhead and allow large-scale batch processing of Lean code. The client-side\ninterface allows users to submit batches of proofs and receive Lean feedback,\nincluding extracted tactics and tactic states via infotree processing. These\nfeatures enable a high-performance, scalable workflow for both interaction and\nextraction of proofs, tactics, and tactic states. We open source our\nimplementation on GitHub.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce the Kimina Lean Server, an open-source project that enables fast\nand scalable interaction with Lean 4 via a unified REST API, designed as a\nsimple verifier for reinforcement learning pipelines. Built on top of the Lean\nFRO's LeanREPL, it combines server-side parallelization by managing multiple\nLean REPL processes in parallel, with an LRU caching strategy that reuses Lean\nimports across multiple requests. These features help reduce initialization\noverhead and allow large-scale batch processing of Lean code. The client-side\ninterface allows users to submit batches of proofs and receive Lean feedback,\nincluding extracted tactics and tactic states via infotree processing. These\nfeatures enable a high-performance, scalable workflow for both interaction and\nextraction of proofs, tactics, and tactic states. We open source our\nimplementation on GitHub."
                },
                "authors": [
                    {
                        "name": "Marco Dos Santos"
                    },
                    {
                        "name": "Haiming Wang"
                    },
                    {
                        "name": "Hugues de Saxcé"
                    },
                    {
                        "name": "Ran Wang"
                    },
                    {
                        "name": "Mantas Baksys"
                    },
                    {
                        "name": "Mert Unsal"
                    },
                    {
                        "name": "Junqi Liu"
                    },
                    {
                        "name": "Zhengying Liu"
                    },
                    {
                        "name": "Jia Li"
                    }
                ],
                "author_detail": {
                    "name": "Jia Li"
                },
                "author": "Jia Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21230v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21230v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21228v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21228v1",
                "updated": "2025-04-29T23:42:21Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    23,
                    42,
                    21,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T23:42:21Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    23,
                    42,
                    21,
                    1,
                    119,
                    0
                ],
                "title": "CachePrune: Neural-Based Attribution Defense Against Indirect Prompt\n  Injection Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CachePrune: Neural-Based Attribution Defense Against Indirect Prompt\n  Injection Attacks"
                },
                "summary": "Large Language Models (LLMs) are identified as being susceptible to indirect\nprompt injection attack, where the model undesirably deviates from\nuser-provided instructions by executing tasks injected in the prompt context.\nThis vulnerability stems from LLMs' inability to distinguish between data and\ninstructions within a prompt. In this paper, we propose CachePrune that defends\nagainst this attack by identifying and pruning task-triggering neurons from the\nKV cache of the input prompt context. By pruning such neurons, we encourage the\nLLM to treat the text spans of input prompt context as only pure data, instead\nof any indicator of instruction following. These neurons are identified via\nfeature attribution with a loss function induced from an upperbound of the\nDirect Preference Optimization (DPO) objective. We show that such a loss\nfunction enables effective feature attribution with only a few samples. We\nfurther improve on the quality of feature attribution, by exploiting an\nobserved triggering effect in instruction following. Our approach does not\nimpose any formatting on the original prompt or introduce extra test-time LLM\ncalls. Experiments show that CachePrune significantly reduces attack success\nrates without compromising the response quality. Note: This paper aims to\ndefend against indirect prompt injection attacks, with the goal of developing\nmore secure and robust AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are identified as being susceptible to indirect\nprompt injection attack, where the model undesirably deviates from\nuser-provided instructions by executing tasks injected in the prompt context.\nThis vulnerability stems from LLMs' inability to distinguish between data and\ninstructions within a prompt. In this paper, we propose CachePrune that defends\nagainst this attack by identifying and pruning task-triggering neurons from the\nKV cache of the input prompt context. By pruning such neurons, we encourage the\nLLM to treat the text spans of input prompt context as only pure data, instead\nof any indicator of instruction following. These neurons are identified via\nfeature attribution with a loss function induced from an upperbound of the\nDirect Preference Optimization (DPO) objective. We show that such a loss\nfunction enables effective feature attribution with only a few samples. We\nfurther improve on the quality of feature attribution, by exploiting an\nobserved triggering effect in instruction following. Our approach does not\nimpose any formatting on the original prompt or introduce extra test-time LLM\ncalls. Experiments show that CachePrune significantly reduces attack success\nrates without compromising the response quality. Note: This paper aims to\ndefend against indirect prompt injection attacks, with the goal of developing\nmore secure and robust AI systems."
                },
                "authors": [
                    {
                        "name": "Rui Wang"
                    },
                    {
                        "name": "Junda Wu"
                    },
                    {
                        "name": "Yu Xia"
                    },
                    {
                        "name": "Tong Yu"
                    },
                    {
                        "name": "Ruiyi Zhang"
                    },
                    {
                        "name": "Ryan Rossi"
                    },
                    {
                        "name": "Lina Yao"
                    },
                    {
                        "name": "Julian McAuley"
                    }
                ],
                "author_detail": {
                    "name": "Julian McAuley"
                },
                "author": "Julian McAuley",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21228v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21228v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12322v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12322v2",
                "updated": "2025-04-29T17:54:42Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    17,
                    54,
                    42,
                    1,
                    119,
                    0
                ],
                "published": "2025-01-21T17:41:54Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    17,
                    41,
                    54,
                    1,
                    21,
                    0
                ],
                "title": "An Achievable Scheme for the K-user Linear Computation Broadcast Channel",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Achievable Scheme for the K-user Linear Computation Broadcast Channel"
                },
                "summary": "This paper presents a new achievable scheme for the K-user Linear Computation\nBroadcast Channel (K-LCBC). A K-LCBC comprises data stored on a server and K\nusers, each aiming to retrieve a desired linear function of the data by\nleveraging their prior locally available side information in the form of\nanother linear function of the data. The proposed scheme is based on a subspace\ndecomposition derived from representable polymatroid spaces. This decomposition\nenables the server to effectively design multicast messages that simultaneously\nbenefit multiple users and allow users to eliminate interference using their\navailable side information. This work extends existing results for the 3-LCBC\nby introducing a linear programming framework to optimize multicast\nopportunities across an arbitrary number of users. The proposed approach can be\nused to derive achievable scheme for the K-user coded caching problem with\nlinear coded placement and scalar linear function retrieval, which was our\noriginal motivation to investigate the K-LCBC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a new achievable scheme for the K-user Linear Computation\nBroadcast Channel (K-LCBC). A K-LCBC comprises data stored on a server and K\nusers, each aiming to retrieve a desired linear function of the data by\nleveraging their prior locally available side information in the form of\nanother linear function of the data. The proposed scheme is based on a subspace\ndecomposition derived from representable polymatroid spaces. This decomposition\nenables the server to effectively design multicast messages that simultaneously\nbenefit multiple users and allow users to eliminate interference using their\navailable side information. This work extends existing results for the 3-LCBC\nby introducing a linear programming framework to optimize multicast\nopportunities across an arbitrary number of users. The proposed approach can be\nused to derive achievable scheme for the K-user coded caching problem with\nlinear coded placement and scalar linear function retrieval, which was our\noriginal motivation to investigate the K-LCBC."
                },
                "authors": [
                    {
                        "name": "Yinbin Ma"
                    },
                    {
                        "name": "Daniela Tuninetti"
                    }
                ],
                "author_detail": {
                    "name": "Daniela Tuninetti"
                },
                "author": "Daniela Tuninetti",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12322v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12322v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12397v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12397v2",
                "updated": "2025-04-29T14:25:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    14,
                    25,
                    8,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-16T18:03:21Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    18,
                    3,
                    21,
                    2,
                    106,
                    0
                ],
                "title": "Activated LoRA: Fine-tuned LLMs for Intrinsics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Activated LoRA: Fine-tuned LLMs for Intrinsics"
                },
                "summary": "Low-Rank Adaptation (LoRA) has emerged as a highly efficient framework for\nfinetuning the weights of large foundation models, and has become the go-to\nmethod for data-driven customization of LLMs. Despite the promise of highly\ncustomized behaviors and capabilities, switching between relevant LoRAs in a\nmultiturn setting is highly inefficient, as the key-value (KV) cache of the\nentire turn history must be recomputed with the LoRA weights before generation\ncan begin. To address this problem, we propose Activated LoRA (aLoRA), which\nmodifies the LoRA framework to only adapt weights for the tokens in the\nsequence \\emph{after} the aLoRA is invoked. This change crucially allows aLoRA\nto accept the base model's KV cache of the input string, meaning that aLoRA can\nbe instantly activated whenever needed in a chain without recomputing the\ncache. This enables building what we call \\emph{intrinsics}, i.e. highly\nspecialized models invoked to perform well-defined operations on portions of an\ninput chain or conversation that otherwise uses the base model by default. We\nuse aLoRA to train a set of intrinsics models, demonstrating competitive\naccuracy with standard LoRA while achieving significant inference benefits.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Rank Adaptation (LoRA) has emerged as a highly efficient framework for\nfinetuning the weights of large foundation models, and has become the go-to\nmethod for data-driven customization of LLMs. Despite the promise of highly\ncustomized behaviors and capabilities, switching between relevant LoRAs in a\nmultiturn setting is highly inefficient, as the key-value (KV) cache of the\nentire turn history must be recomputed with the LoRA weights before generation\ncan begin. To address this problem, we propose Activated LoRA (aLoRA), which\nmodifies the LoRA framework to only adapt weights for the tokens in the\nsequence \\emph{after} the aLoRA is invoked. This change crucially allows aLoRA\nto accept the base model's KV cache of the input string, meaning that aLoRA can\nbe instantly activated whenever needed in a chain without recomputing the\ncache. This enables building what we call \\emph{intrinsics}, i.e. highly\nspecialized models invoked to perform well-defined operations on portions of an\ninput chain or conversation that otherwise uses the base model by default. We\nuse aLoRA to train a set of intrinsics models, demonstrating competitive\naccuracy with standard LoRA while achieving significant inference benefits."
                },
                "authors": [
                    {
                        "name": "Kristjan Greenewald"
                    },
                    {
                        "name": "Luis Lastras"
                    },
                    {
                        "name": "Thomas Parnell"
                    },
                    {
                        "name": "Vraj Shah"
                    },
                    {
                        "name": "Lucian Popa"
                    },
                    {
                        "name": "Giulio Zizzo"
                    },
                    {
                        "name": "Chulaka Gunasekara"
                    },
                    {
                        "name": "Ambrish Rawat"
                    },
                    {
                        "name": "David Cox"
                    }
                ],
                "author_detail": {
                    "name": "David Cox"
                },
                "author": "David Cox",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2504.11704",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12397v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12397v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20246v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20246v1",
                "updated": "2025-04-28T20:30:59Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    20,
                    30,
                    59,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T20:30:59Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    20,
                    30,
                    59,
                    0,
                    118,
                    0
                ],
                "title": "Tree embedding based mapping system for low-latency mobile applications\n  in multi-access networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tree embedding based mapping system for low-latency mobile applications\n  in multi-access networks"
                },
                "summary": "Low-latency applications like AR/VR and online gaming need fast, stable\nconnections. New technologies such as V2X, LEO satellites, and 6G bring unique\nchallenges in mobility management. Traditional solutions based on centralized\nor distributed anchors often fall short in supporting rapid mobility due to\ninefficient routing, low versatility, and insufficient multi-access support. In\nthis paper, we design a new end-to-end system for tracking multi-connected\nmobile devices at scale and optimizing performance for latency-sensitive,\nhighly dynamic applications. Our system, based on the locator/ID separation\nprinciple, extends to multi-access networks without requiring specialized\nrouters or caching. Using a novel tree embedding-based overlay, we enable fast\nsession setup while allowing endpoints to directly handle mobility between\nthem. Evaluation with real network data shows our solution cuts connection\nlatency to 7.42% inflation over the shortest path, compared to LISP's 359\\% due\nto cache misses. It also significantly reduces location update overhead and\ndisruption time during mobility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-latency applications like AR/VR and online gaming need fast, stable\nconnections. New technologies such as V2X, LEO satellites, and 6G bring unique\nchallenges in mobility management. Traditional solutions based on centralized\nor distributed anchors often fall short in supporting rapid mobility due to\ninefficient routing, low versatility, and insufficient multi-access support. In\nthis paper, we design a new end-to-end system for tracking multi-connected\nmobile devices at scale and optimizing performance for latency-sensitive,\nhighly dynamic applications. Our system, based on the locator/ID separation\nprinciple, extends to multi-access networks without requiring specialized\nrouters or caching. Using a novel tree embedding-based overlay, we enable fast\nsession setup while allowing endpoints to directly handle mobility between\nthem. Evaluation with real network data shows our solution cuts connection\nlatency to 7.42% inflation over the shortest path, compared to LISP's 359\\% due\nto cache misses. It also significantly reduces location update overhead and\ndisruption time during mobility."
                },
                "authors": [
                    {
                        "name": "Yu Mi"
                    },
                    {
                        "name": "Randeep Bhatia"
                    },
                    {
                        "name": "Fang Hao"
                    },
                    {
                        "name": "An Wang"
                    },
                    {
                        "name": "Steve Benno"
                    },
                    {
                        "name": "Tv Lakshman"
                    }
                ],
                "author_detail": {
                    "name": "Tv Lakshman"
                },
                "author": "Tv Lakshman",
                "arxiv_comment": "Accepted by IEEE INFOCOM 2025-IEEE Conference on Computer\n  Communications",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20246v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20246v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.12747v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.12747v3",
                "updated": "2025-04-28T17:17:53Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    17,
                    17,
                    53,
                    0,
                    118,
                    0
                ],
                "published": "2024-05-21T12:59:59Z",
                "published_parsed": [
                    2024,
                    5,
                    21,
                    12,
                    59,
                    59,
                    1,
                    142,
                    0
                ],
                "title": "Hierarchical Coded Caching with Low Subpacketization and Coding Delay\n  using Combinatorial t-Designs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hierarchical Coded Caching with Low Subpacketization and Coding Delay\n  using Combinatorial t-Designs"
                },
                "summary": "Coded caching scheme originally proposed by Maddah-Ali and Niesen (MN)\nconsidered a broadcast network consisting of a single server connected to a set\nof users each having a cache memory. Motivated by practical scenarios,\nKaramchandani \\textit{et al.} in [16] proposed a coded caching scheme for a\ntwo-layer hierarchical network consisting of a single server connected to\nmultiple mirror sites and each mirror site connected to a distinct set of\nusers, in which both mirror sites and users having cache memories. Low\nsubpacketization level coded caching schemes are desirable for practical\nimplementations. Placement delivery array (PDA) was proposed as a tool to\ndesign coded caching schemes with reduced subpacketization level by Yan\n\\textit{et al.} in [4]. Schemes with reduced subpacketization levels are\nstudied extensively in the literature for single-layer networks. Kong\n\\textit{et al.} in [17] proposed a structure called hierarchical placement\ndelivery arrays (HPDA), which characterizes a hierarchical coded caching system\nand also proposed a class of HPDAs that gives low subpacketization level\nschemes by using two PDAs. Low subpacketization level hierarchical schemes\nusing combinatorial $t$-designs is proposed in [20]. Apart from that there is\nno other existing work that discusses the subpacketization problem in a\nhierarchical network. This paper proposes a class of HPDA construction that\ngives low subpacketization level hierarchical coded caching schemes, by first\nconstructing a new class of PDAs. Compared with the existing schemes, in cases\nwhere the system parameters and subpacketization level are the same, the\nproposed hierarchical scheme has a better coding delay. Further, the new class\nof PDAs constructed either subsumes several known PDA constructions or achieves\nbetter transmission load for the same system parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coded caching scheme originally proposed by Maddah-Ali and Niesen (MN)\nconsidered a broadcast network consisting of a single server connected to a set\nof users each having a cache memory. Motivated by practical scenarios,\nKaramchandani \\textit{et al.} in [16] proposed a coded caching scheme for a\ntwo-layer hierarchical network consisting of a single server connected to\nmultiple mirror sites and each mirror site connected to a distinct set of\nusers, in which both mirror sites and users having cache memories. Low\nsubpacketization level coded caching schemes are desirable for practical\nimplementations. Placement delivery array (PDA) was proposed as a tool to\ndesign coded caching schemes with reduced subpacketization level by Yan\n\\textit{et al.} in [4]. Schemes with reduced subpacketization levels are\nstudied extensively in the literature for single-layer networks. Kong\n\\textit{et al.} in [17] proposed a structure called hierarchical placement\ndelivery arrays (HPDA), which characterizes a hierarchical coded caching system\nand also proposed a class of HPDAs that gives low subpacketization level\nschemes by using two PDAs. Low subpacketization level hierarchical schemes\nusing combinatorial $t$-designs is proposed in [20]. Apart from that there is\nno other existing work that discusses the subpacketization problem in a\nhierarchical network. This paper proposes a class of HPDA construction that\ngives low subpacketization level hierarchical coded caching schemes, by first\nconstructing a new class of PDAs. Compared with the existing schemes, in cases\nwhere the system parameters and subpacketization level are the same, the\nproposed hierarchical scheme has a better coding delay. Further, the new class\nof PDAs constructed either subsumes several known PDA constructions or achieves\nbetter transmission load for the same system parameters."
                },
                "authors": [
                    {
                        "name": "Rashid Ummer N. T."
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "IEEE Internet of Things Journal (Accepted for publication). The\n  Hierarchical coded caching scheme in this updated version unifies the scheme\n  in the previous version and the schemes in arxiv:2402.07188. This version\n  includes a more comprehensive performance analysis. To reflect these the\n  title has been updated",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.12747v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.12747v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19984v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19984v1",
                "updated": "2025-04-28T16:59:13Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    16,
                    59,
                    13,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T16:59:13Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    16,
                    59,
                    13,
                    0,
                    118,
                    0
                ],
                "title": "3D MPSoC with On-Chip Cache Support -- Design and Exploitation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D MPSoC with On-Chip Cache Support -- Design and Exploitation"
                },
                "summary": "The increasing density of transistors in Integrated Circuits (ICs) has\nenabled the development of highly integrated Systems-on-Chip (SoCs) and, more\nrecently, Multiprocessor Systems-on-Chip (MPSoCs). To address scalability\nchallenges in communication and memory performance, three-dimensional (3D)\nNetwork-on-Chip (NoC) architectures have emerged, offering improvements in\ncommunication latency and throughput. However, memory system efficiency remains\na critical bottleneck in NoC-based designs. This work proposes the design and\nexperimental exploration of 3D MPSoCs with on-chip cache support by employing\ndistinct communication infrastructures for inter-processor and memory\ninteractions. Specifically, packet-based NoCs are adopted for inter-processor\ncommunication, while a crossbar-based infrastructure supports a cache coherence\nhierarchy for memory access. A two-layer system architecture is introduced,\ncombining a Uniform Memory Access (UMA) model within clusters and a No Remote\nMemory Access (NORMA) model between clusters, aiming to balance scalability and\ncoherence requirements. Emerging memory technologies such as PCRAM and MRAM are\nexplored to optimize performance, energy consumption, and area usage.\nExperimental evaluations are conducted using the Gem5 simulator, targeting a\nmodel based on the ARM Versatile Express platform. The outcomes of this study\naim to enhance MPSoC scalability while meeting the stringent demands of\nmemory-centric applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing density of transistors in Integrated Circuits (ICs) has\nenabled the development of highly integrated Systems-on-Chip (SoCs) and, more\nrecently, Multiprocessor Systems-on-Chip (MPSoCs). To address scalability\nchallenges in communication and memory performance, three-dimensional (3D)\nNetwork-on-Chip (NoC) architectures have emerged, offering improvements in\ncommunication latency and throughput. However, memory system efficiency remains\na critical bottleneck in NoC-based designs. This work proposes the design and\nexperimental exploration of 3D MPSoCs with on-chip cache support by employing\ndistinct communication infrastructures for inter-processor and memory\ninteractions. Specifically, packet-based NoCs are adopted for inter-processor\ncommunication, while a crossbar-based infrastructure supports a cache coherence\nhierarchy for memory access. A two-layer system architecture is introduced,\ncombining a Uniform Memory Access (UMA) model within clusters and a No Remote\nMemory Access (NORMA) model between clusters, aiming to balance scalability and\ncoherence requirements. Emerging memory technologies such as PCRAM and MRAM are\nexplored to optimize performance, energy consumption, and area usage.\nExperimental evaluations are conducted using the Gem5 simulator, targeting a\nmodel based on the ARM Versatile Express platform. The outcomes of this study\naim to enhance MPSoC scalability while meeting the stringent demands of\nmemory-centric applications."
                },
                "authors": [
                    {
                        "name": "Rodrigo Cataldo"
                    },
                    {
                        "name": "Cesar Marcon"
                    },
                    {
                        "name": "Debora Matos"
                    }
                ],
                "author_detail": {
                    "name": "Debora Matos"
                },
                "author": "Debora Matos",
                "arxiv_comment": "Progress Seminar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19984v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19984v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19874v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19874v1",
                "updated": "2025-04-28T15:05:35Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    15,
                    5,
                    35,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T15:05:35Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    15,
                    5,
                    35,
                    0,
                    118,
                    0
                ],
                "title": "TurboQuant: Online Vector Quantization with Near-optimal Distortion Rate",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TurboQuant: Online Vector Quantization with Near-optimal Distortion Rate"
                },
                "summary": "Vector quantization, a problem rooted in Shannon's source coding theory, aims\nto quantize high-dimensional Euclidean vectors while minimizing distortion in\ntheir geometric structure. We propose TurboQuant to address both mean-squared\nerror (MSE) and inner product distortion, overcoming limitations of existing\nmethods that fail to achieve optimal distortion rates. Our data-oblivious\nalgorithms, suitable for online applications, achieve near-optimal distortion\nrates (within a small constant factor) across all bit-widths and dimensions.\nTurboQuant achieves this by randomly rotating input vectors, inducing a\nconcentrated Beta distribution on coordinates, and leveraging the\nnear-independence property of distinct coordinates in high dimensions to simply\napply optimal scalar quantizers per each coordinate. Recognizing that\nMSE-optimal quantizers introduce bias in inner product estimation, we propose a\ntwo-stage approach: applying an MSE quantizer followed by a 1-bit Quantized JL\n(QJL) transform on the residual, resulting in an unbiased inner product\nquantizer. We also provide a formal proof of the information-theoretic lower\nbounds on best achievable distortion rate by any vector quantizer,\ndemonstrating that TurboQuant closely matches these bounds, differing only by a\nsmall constant ($\\approx 2.7$) factor. Experimental results validate our\ntheoretical findings, showing that for KV cache quantization, we achieve\nabsolute quality neutrality with 3.5 bits per channel and marginal quality\ndegradation with 2.5 bits per channel. Furthermore, in nearest neighbor search\ntasks, our method outperforms existing product quantization techniques in\nrecall while reducing indexing time to virtually zero.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vector quantization, a problem rooted in Shannon's source coding theory, aims\nto quantize high-dimensional Euclidean vectors while minimizing distortion in\ntheir geometric structure. We propose TurboQuant to address both mean-squared\nerror (MSE) and inner product distortion, overcoming limitations of existing\nmethods that fail to achieve optimal distortion rates. Our data-oblivious\nalgorithms, suitable for online applications, achieve near-optimal distortion\nrates (within a small constant factor) across all bit-widths and dimensions.\nTurboQuant achieves this by randomly rotating input vectors, inducing a\nconcentrated Beta distribution on coordinates, and leveraging the\nnear-independence property of distinct coordinates in high dimensions to simply\napply optimal scalar quantizers per each coordinate. Recognizing that\nMSE-optimal quantizers introduce bias in inner product estimation, we propose a\ntwo-stage approach: applying an MSE quantizer followed by a 1-bit Quantized JL\n(QJL) transform on the residual, resulting in an unbiased inner product\nquantizer. We also provide a formal proof of the information-theoretic lower\nbounds on best achievable distortion rate by any vector quantizer,\ndemonstrating that TurboQuant closely matches these bounds, differing only by a\nsmall constant ($\\approx 2.7$) factor. Experimental results validate our\ntheoretical findings, showing that for KV cache quantization, we achieve\nabsolute quality neutrality with 3.5 bits per channel and marginal quality\ndegradation with 2.5 bits per channel. Furthermore, in nearest neighbor search\ntasks, our method outperforms existing product quantization techniques in\nrecall while reducing indexing time to virtually zero."
                },
                "authors": [
                    {
                        "name": "Amir Zandieh"
                    },
                    {
                        "name": "Majid Daliri"
                    },
                    {
                        "name": "Majid Hadian"
                    },
                    {
                        "name": "Vahab Mirrokni"
                    }
                ],
                "author_detail": {
                    "name": "Vahab Mirrokni"
                },
                "author": "Vahab Mirrokni",
                "arxiv_comment": "25 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19874v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19874v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19867v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19867v1",
                "updated": "2025-04-28T15:00:03Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    15,
                    0,
                    3,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T15:00:03Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    15,
                    0,
                    3,
                    0,
                    118,
                    0
                ],
                "title": "semi-PD: Towards Efficient LLM Serving via Phase-Wise Disaggregated\n  Computation and Unified Storage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "semi-PD: Towards Efficient LLM Serving via Phase-Wise Disaggregated\n  Computation and Unified Storage"
                },
                "summary": "Existing large language model (LLM) serving systems fall into two categories:\n1) a unified system where prefill phase and decode phase are co-located on the\nsame GPU, sharing the unified computational resource and storage, and 2) a\ndisaggregated system where the two phases are disaggregated to different GPUs.\nThe design of the disaggregated system addresses the latency interference and\nsophisticated scheduling issues in the unified system but leads to storage\nchallenges including 1) replicated weights for both phases that prevent\nflexible deployment, 2) KV cache transfer overhead between the two phases, 3)\nstorage imbalance that causes substantial wasted space of the GPU capacity, and\n4) suboptimal resource adjustment arising from the difficulties in migrating KV\ncache. Such storage inefficiency delivers poor serving performance under high\nrequest rates.\n  In this paper, we identify that the advantage of the disaggregated system\nlies in the disaggregated computation, i.e., partitioning the computational\nresource to enable the asynchronous computation of two phases. Thus, we propose\na novel LLM serving system, semi-PD, characterized by disaggregated computation\nand unified storage. In semi-PD, we introduce a computation resource controller\nto achieve disaggregated computation at the streaming multi-processor (SM)\nlevel, and a unified memory manager to manage the asynchronous memory access\nfrom both phases. semi-PD has a low-overhead resource adjustment mechanism\nbetween the two phases, and a service-level objective (SLO) aware dynamic\npartitioning algorithm to optimize the SLO attainment. Compared to\nstate-of-the-art systems, semi-PD maintains lower latency at higher request\nrates, reducing the average end-to-end latency per request by 1.27-2.58x on\nDeepSeek series models, and serves 1.55-1.72x more requests adhering to latency\nconstraints on Llama series models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing large language model (LLM) serving systems fall into two categories:\n1) a unified system where prefill phase and decode phase are co-located on the\nsame GPU, sharing the unified computational resource and storage, and 2) a\ndisaggregated system where the two phases are disaggregated to different GPUs.\nThe design of the disaggregated system addresses the latency interference and\nsophisticated scheduling issues in the unified system but leads to storage\nchallenges including 1) replicated weights for both phases that prevent\nflexible deployment, 2) KV cache transfer overhead between the two phases, 3)\nstorage imbalance that causes substantial wasted space of the GPU capacity, and\n4) suboptimal resource adjustment arising from the difficulties in migrating KV\ncache. Such storage inefficiency delivers poor serving performance under high\nrequest rates.\n  In this paper, we identify that the advantage of the disaggregated system\nlies in the disaggregated computation, i.e., partitioning the computational\nresource to enable the asynchronous computation of two phases. Thus, we propose\na novel LLM serving system, semi-PD, characterized by disaggregated computation\nand unified storage. In semi-PD, we introduce a computation resource controller\nto achieve disaggregated computation at the streaming multi-processor (SM)\nlevel, and a unified memory manager to manage the asynchronous memory access\nfrom both phases. semi-PD has a low-overhead resource adjustment mechanism\nbetween the two phases, and a service-level objective (SLO) aware dynamic\npartitioning algorithm to optimize the SLO attainment. Compared to\nstate-of-the-art systems, semi-PD maintains lower latency at higher request\nrates, reducing the average end-to-end latency per request by 1.27-2.58x on\nDeepSeek series models, and serves 1.55-1.72x more requests adhering to latency\nconstraints on Llama series models."
                },
                "authors": [
                    {
                        "name": "Ke Hong"
                    },
                    {
                        "name": "Lufang Chen"
                    },
                    {
                        "name": "Zhong Wang"
                    },
                    {
                        "name": "Xiuhong Li"
                    },
                    {
                        "name": "Qiuli Mao"
                    },
                    {
                        "name": "Jianping Ma"
                    },
                    {
                        "name": "Chao Xiong"
                    },
                    {
                        "name": "Guanyu Wu"
                    },
                    {
                        "name": "Buhe Han"
                    },
                    {
                        "name": "Guohao Dai"
                    },
                    {
                        "name": "Yun Liang"
                    },
                    {
                        "name": "Yu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Wang"
                },
                "author": "Yu Wang",
                "arxiv_comment": "18 pages, 16 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19867v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19867v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19601v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19601v1",
                "updated": "2025-04-28T09:03:45Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    9,
                    3,
                    45,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T09:03:45Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    9,
                    3,
                    45,
                    0,
                    118,
                    0
                ],
                "title": "Characterizing the Optimal Memory-Rate Tradeoff in Secure Coded Caching\n  for Small Buffer or Small Rate",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Characterizing the Optimal Memory-Rate Tradeoff in Secure Coded Caching\n  for Small Buffer or Small Rate"
                },
                "summary": "We consider the secure coded caching problem proposed by Ravindrakumar et. al\nwhere no user can obtain information about files other than the one requested.\nWe first propose three new schemes for the three cases of cache size $M=1$,\n$N=2$ files and arbitrary $K$ users, delivery rate $ R=1$, arbitrary $N$ files\nand $K$ users, and the general case for arbitrary $N$ files and $K$ users,\nrespectively. Then we derive converse results by characterizing new properties\nof secure coded caching schemes. As a result, we characterize the two\nend-points of the optimal memory-rate tradeoff curve for arbitrary number of\nusers and files. Furthermore, for the case of $N=2$ files and arbitrary number\nof users, we also characterize a segment of the optimal memory-rate tradeoff\ncurve, where the cache size is relatively small.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider the secure coded caching problem proposed by Ravindrakumar et. al\nwhere no user can obtain information about files other than the one requested.\nWe first propose three new schemes for the three cases of cache size $M=1$,\n$N=2$ files and arbitrary $K$ users, delivery rate $ R=1$, arbitrary $N$ files\nand $K$ users, and the general case for arbitrary $N$ files and $K$ users,\nrespectively. Then we derive converse results by characterizing new properties\nof secure coded caching schemes. As a result, we characterize the two\nend-points of the optimal memory-rate tradeoff curve for arbitrary number of\nusers and files. Furthermore, for the case of $N=2$ files and arbitrary number\nof users, we also characterize a segment of the optimal memory-rate tradeoff\ncurve, where the cache size is relatively small."
                },
                "authors": [
                    {
                        "name": "Han Fang"
                    },
                    {
                        "name": "Nan Liu"
                    },
                    {
                        "name": "Wei Kang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Kang"
                },
                "author": "Wei Kang",
                "arxiv_comment": "Submitted to IEEE Transactions on Information Theory",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19601v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19601v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19561v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19561v1",
                "updated": "2025-04-28T08:12:30Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    8,
                    12,
                    30,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T08:12:30Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    8,
                    12,
                    30,
                    0,
                    118,
                    0
                ],
                "title": "Quantifying Memory Utilization with Effective State-Size",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantifying Memory Utilization with Effective State-Size"
                },
                "summary": "The need to develop a general framework for architecture analysis is becoming\nincreasingly important, given the expanding design space of sequence models. To\nthis end, we draw insights from classical signal processing and control theory,\nto develop a quantitative measure of \\textit{memory utilization}: the internal\nmechanisms through which a model stores past information to produce future\noutputs. This metric, which we call \\textbf{\\textit{effective state-size}}\n(ESS), is tailored to the fundamental class of systems with\n\\textit{input-invariant} and \\textit{input-varying linear operators},\nencompassing a variety of computational units such as variants of attention,\nconvolutions, and recurrences. Unlike prior work on memory utilization, which\neither relies on raw operator visualizations (e.g. attention maps), or simply\nthe total \\textit{memory capacity} (i.e. cache size) of a model, our metrics\nprovide highly interpretable and actionable measurements. In particular, we\nshow how ESS can be leveraged to improve initialization strategies, inform\nnovel regularizers and advance the performance-efficiency frontier through\nmodel distillation. Furthermore, we demonstrate that the effect of context\ndelimiters (such as end-of-speech tokens) on ESS highlights cross-architectural\ndifferences in how large language models utilize their available memory to\nrecall information. Overall, we find that ESS provides valuable insights into\nthe dynamics that dictate memory utilization, enabling the design of more\nefficient and effective sequence models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The need to develop a general framework for architecture analysis is becoming\nincreasingly important, given the expanding design space of sequence models. To\nthis end, we draw insights from classical signal processing and control theory,\nto develop a quantitative measure of \\textit{memory utilization}: the internal\nmechanisms through which a model stores past information to produce future\noutputs. This metric, which we call \\textbf{\\textit{effective state-size}}\n(ESS), is tailored to the fundamental class of systems with\n\\textit{input-invariant} and \\textit{input-varying linear operators},\nencompassing a variety of computational units such as variants of attention,\nconvolutions, and recurrences. Unlike prior work on memory utilization, which\neither relies on raw operator visualizations (e.g. attention maps), or simply\nthe total \\textit{memory capacity} (i.e. cache size) of a model, our metrics\nprovide highly interpretable and actionable measurements. In particular, we\nshow how ESS can be leveraged to improve initialization strategies, inform\nnovel regularizers and advance the performance-efficiency frontier through\nmodel distillation. Furthermore, we demonstrate that the effect of context\ndelimiters (such as end-of-speech tokens) on ESS highlights cross-architectural\ndifferences in how large language models utilize their available memory to\nrecall information. Overall, we find that ESS provides valuable insights into\nthe dynamics that dictate memory utilization, enabling the design of more\nefficient and effective sequence models."
                },
                "authors": [
                    {
                        "name": "Rom N. Parnichkun"
                    },
                    {
                        "name": "Neehal Tumma"
                    },
                    {
                        "name": "Armin W. Thomas"
                    },
                    {
                        "name": "Alessandro Moro"
                    },
                    {
                        "name": "Qi An"
                    },
                    {
                        "name": "Taiji Suzuki"
                    },
                    {
                        "name": "Atsushi Yamashita"
                    },
                    {
                        "name": "Michael Poli"
                    },
                    {
                        "name": "Stefano Massaroli"
                    }
                ],
                "author_detail": {
                    "name": "Stefano Massaroli"
                },
                "author": "Stefano Massaroli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19561v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19561v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19475v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19475v1",
                "updated": "2025-04-28T04:31:24Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    4,
                    31,
                    24,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T04:31:24Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    4,
                    31,
                    24,
                    0,
                    118,
                    0
                ],
                "title": "Prisma: An Open Source Toolkit for Mechanistic Interpretability in\n  Vision and Video",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prisma: An Open Source Toolkit for Mechanistic Interpretability in\n  Vision and Video"
                },
                "summary": "Robust tooling and publicly available pre-trained models have helped drive\nrecent advances in mechanistic interpretability for language models. However,\nsimilar progress in vision mechanistic interpretability has been hindered by\nthe lack of accessible frameworks and pre-trained weights. We present Prisma\n(Access the codebase here: https://github.com/Prisma-Multimodal/ViT-Prisma), an\nopen-source framework designed to accelerate vision mechanistic\ninterpretability research, providing a unified toolkit for accessing 75+ vision\nand video transformers; support for sparse autoencoder (SAE), transcoder, and\ncrosscoder training; a suite of 80+ pre-trained SAE weights; activation\ncaching, circuit analysis tools, and visualization tools; and educational\nresources. Our analysis reveals surprising findings, including that effective\nvision SAEs can exhibit substantially lower sparsity patterns than language\nSAEs, and that in some instances, SAE reconstructions can decrease model loss.\nPrisma enables new research directions for understanding vision model internals\nwhile lowering barriers to entry in this emerging field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust tooling and publicly available pre-trained models have helped drive\nrecent advances in mechanistic interpretability for language models. However,\nsimilar progress in vision mechanistic interpretability has been hindered by\nthe lack of accessible frameworks and pre-trained weights. We present Prisma\n(Access the codebase here: https://github.com/Prisma-Multimodal/ViT-Prisma), an\nopen-source framework designed to accelerate vision mechanistic\ninterpretability research, providing a unified toolkit for accessing 75+ vision\nand video transformers; support for sparse autoencoder (SAE), transcoder, and\ncrosscoder training; a suite of 80+ pre-trained SAE weights; activation\ncaching, circuit analysis tools, and visualization tools; and educational\nresources. Our analysis reveals surprising findings, including that effective\nvision SAEs can exhibit substantially lower sparsity patterns than language\nSAEs, and that in some instances, SAE reconstructions can decrease model loss.\nPrisma enables new research directions for understanding vision model internals\nwhile lowering barriers to entry in this emerging field."
                },
                "authors": [
                    {
                        "name": "Sonia Joseph"
                    },
                    {
                        "name": "Praneet Suresh"
                    },
                    {
                        "name": "Lorenz Hufe"
                    },
                    {
                        "name": "Edward Stevinson"
                    },
                    {
                        "name": "Robert Graham"
                    },
                    {
                        "name": "Yash Vadi"
                    },
                    {
                        "name": "Danilo Bzdok"
                    },
                    {
                        "name": "Sebastian Lapuschkin"
                    },
                    {
                        "name": "Lee Sharkey"
                    },
                    {
                        "name": "Blake Aaron Richards"
                    }
                ],
                "author_detail": {
                    "name": "Blake Aaron Richards"
                },
                "author": "Blake Aaron Richards",
                "arxiv_comment": "4 pages, 3 figures, 9 tables. Oral and Tutorial at the CVPR\n  Mechanistic Interpretability for Vision (MIV) Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19475v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19475v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18001v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18001v2",
                "updated": "2025-04-28T04:02:30Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    4,
                    2,
                    30,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-25T01:10:49Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    1,
                    10,
                    49,
                    4,
                    115,
                    0
                ],
                "title": "From Cluster to Desktop: A Cache-Accelerated INR framework for\n  Interactive Visualization of Tera-Scale Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Cluster to Desktop: A Cache-Accelerated INR framework for\n  Interactive Visualization of Tera-Scale Data"
                },
                "summary": "Machine learning has enabled the use of implicit neural representations\n(INRs) to efficiently compress and reconstruct massive scientific datasets.\nHowever, despite advances in fast INR rendering algorithms, INR-based rendering\nremains computationally expensive, as computing data values from an INR is\nsignificantly slower than reading them from GPU memory. This bottleneck\ncurrently restricts interactive INR visualization to professional workstations.\nTo address this challenge, we introduce an INR rendering framework accelerated\nby a scalable, multi-resolution GPU cache capable of efficiently representing\ntera-scale datasets. By minimizing redundant data queries and prioritizing\nnovel volume regions, our method reduces the number of INR computations per\nframe, achieving an average 5x speedup over the state-of-the-art INR rendering\nmethod while still maintaining high visualization quality. Coupled with\nexisting hardware-accelerated INR compressors, our framework enables scientists\nto generate and compress massive datasets in situ on high-performance computing\nplatforms and then interactively explore them on consumer-grade hardware post\nhoc.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning has enabled the use of implicit neural representations\n(INRs) to efficiently compress and reconstruct massive scientific datasets.\nHowever, despite advances in fast INR rendering algorithms, INR-based rendering\nremains computationally expensive, as computing data values from an INR is\nsignificantly slower than reading them from GPU memory. This bottleneck\ncurrently restricts interactive INR visualization to professional workstations.\nTo address this challenge, we introduce an INR rendering framework accelerated\nby a scalable, multi-resolution GPU cache capable of efficiently representing\ntera-scale datasets. By minimizing redundant data queries and prioritizing\nnovel volume regions, our method reduces the number of INR computations per\nframe, achieving an average 5x speedup over the state-of-the-art INR rendering\nmethod while still maintaining high visualization quality. Coupled with\nexisting hardware-accelerated INR compressors, our framework enables scientists\nto generate and compress massive datasets in situ on high-performance computing\nplatforms and then interactively explore them on consumer-grade hardware post\nhoc."
                },
                "authors": [
                    {
                        "name": "Daniel Zavorotny"
                    },
                    {
                        "name": "Qi Wu"
                    },
                    {
                        "name": "David Bauer"
                    },
                    {
                        "name": "Kwan-Liu Ma"
                    }
                ],
                "author_detail": {
                    "name": "Kwan-Liu Ma"
                },
                "author": "Kwan-Liu Ma",
                "arxiv_comment": "11 pages, 11 figures, EGPGV25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18001v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18001v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.12150v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.12150v3",
                "updated": "2025-04-28T02:58:27Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    2,
                    58,
                    27,
                    0,
                    118,
                    0
                ],
                "published": "2025-03-15T14:13:23Z",
                "published_parsed": [
                    2025,
                    3,
                    15,
                    14,
                    13,
                    23,
                    5,
                    74,
                    0
                ],
                "title": "Point-Cache: Test-time Dynamic and Hierarchical Cache for Robust and\n  Generalizable Point Cloud Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Point-Cache: Test-time Dynamic and Hierarchical Cache for Robust and\n  Generalizable Point Cloud Analysis"
                },
                "summary": "This paper proposes a general solution to enable point cloud recognition\nmodels to handle distribution shifts at test time. Unlike prior methods, which\nrely heavily on training data (often inaccessible during online inference) and\nare limited to recognizing a fixed set of point cloud classes predefined during\ntraining, we explore a more practical and challenging scenario: adapting the\nmodel solely based on online test data to recognize both previously seen\nclasses and novel, unseen classes at test time. To this end, we develop\n\\textbf{Point-Cache}, a hierarchical cache model that captures essential clues\nof online test samples, particularly focusing on the global structure of point\nclouds and their local-part details. Point-Cache, which serves as a rich 3D\nknowledge base, is dynamically managed to prioritize the inclusion of\nhigh-quality samples. Designed as a plug-and-play module, our method can be\nflexibly integrated into large multimodal 3D models to support open-vocabulary\npoint cloud recognition. Notably, our solution operates with efficiency\ncomparable to zero-shot inference, as it is entirely training-free. Point-Cache\ndemonstrates substantial gains across 8 challenging benchmarks and 4\nrepresentative large 3D models, highlighting its effectiveness. Code is\navailable at https://github.com/auniquesun/Point-Cache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes a general solution to enable point cloud recognition\nmodels to handle distribution shifts at test time. Unlike prior methods, which\nrely heavily on training data (often inaccessible during online inference) and\nare limited to recognizing a fixed set of point cloud classes predefined during\ntraining, we explore a more practical and challenging scenario: adapting the\nmodel solely based on online test data to recognize both previously seen\nclasses and novel, unseen classes at test time. To this end, we develop\n\\textbf{Point-Cache}, a hierarchical cache model that captures essential clues\nof online test samples, particularly focusing on the global structure of point\nclouds and their local-part details. Point-Cache, which serves as a rich 3D\nknowledge base, is dynamically managed to prioritize the inclusion of\nhigh-quality samples. Designed as a plug-and-play module, our method can be\nflexibly integrated into large multimodal 3D models to support open-vocabulary\npoint cloud recognition. Notably, our solution operates with efficiency\ncomparable to zero-shot inference, as it is entirely training-free. Point-Cache\ndemonstrates substantial gains across 8 challenging benchmarks and 4\nrepresentative large 3D models, highlighting its effectiveness. Code is\navailable at https://github.com/auniquesun/Point-Cache."
                },
                "authors": [
                    {
                        "name": "Hongyu Sun"
                    },
                    {
                        "name": "Qiuhong Ke"
                    },
                    {
                        "name": "Ming Cheng"
                    },
                    {
                        "name": "Yongcai Wang"
                    },
                    {
                        "name": "Deying Li"
                    },
                    {
                        "name": "Chenhui Gou"
                    },
                    {
                        "name": "Jianfei Cai"
                    }
                ],
                "author_detail": {
                    "name": "Jianfei Cai"
                },
                "author": "Jianfei Cai",
                "arxiv_comment": "Accepted by CVPR 2025; 24 pages, 14 figures, 18 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.12150v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.12150v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19365v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19365v1",
                "updated": "2025-04-27T22:05:14Z",
                "updated_parsed": [
                    2025,
                    4,
                    27,
                    22,
                    5,
                    14,
                    6,
                    117,
                    0
                ],
                "published": "2025-04-27T22:05:14Z",
                "published_parsed": [
                    2025,
                    4,
                    27,
                    22,
                    5,
                    14,
                    6,
                    117,
                    0
                ],
                "title": "AGILE: Lightweight and Efficient Asynchronous GPU-SSD Integration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AGILE: Lightweight and Efficient Asynchronous GPU-SSD Integration"
                },
                "summary": "Graphics Processing Units (GPUs) have become essential for computationally\nintensive applications. However, emerging workloads such as recommender\nsystems, graph analytics, and data analytics often involve processing data\nexceeding GPU on-chip memory capacity. To mitigate this issue, existing\nsolutions enable GPUs to use CPU DRAM or SSDs as external memory. Among them,\nthe GPU-centric approach lets GPU threads directly initiate NVMe requests,\neliminating CPU intervention overhead over traditional methods. However, the\nSOTA GPU-centric approach adopts a synchronous IO model, and threads must\ntolerate the long latency in communication before starting any tasks.\n  In this work, we propose AGILE, a lightweight and efficient asynchronous\nlibrary allowing GPU threads to access SSDs asynchronously while eliminating\ndeadlock risks. AGILE also integrates a flexible software cache using GPU\nHigh-Bandwidth Mamory (HBM). We demonstrate that the asynchronous GPU-centric\nIO achieves up to 1.88$\\times$ improvement in workloads with different\ncomputation-to-communication (CTC) ratios. We also compare AGILE with the SOTA\nwork BaM on Deep Learning Recommendation Models (DLRM) with various settings,\nand the results show that AGILE achieves 1.75$\\times$ performance improvement\ndue to its efficient design and the overlapping strategy enabled by an\nasynchronous IO model. We further evaluate AGILE's API overhead on graph\napplications, and the results demonstrate AGILE reduces software cache overhead\nby up to 3.12$\\times$ and overhead in NVMe IO requests by up to 2.85$\\times$.\nCompared with BaM, AGILE consumes fewer registers and exhibits up to\n1.32$\\times$ reduction in the usage of registers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graphics Processing Units (GPUs) have become essential for computationally\nintensive applications. However, emerging workloads such as recommender\nsystems, graph analytics, and data analytics often involve processing data\nexceeding GPU on-chip memory capacity. To mitigate this issue, existing\nsolutions enable GPUs to use CPU DRAM or SSDs as external memory. Among them,\nthe GPU-centric approach lets GPU threads directly initiate NVMe requests,\neliminating CPU intervention overhead over traditional methods. However, the\nSOTA GPU-centric approach adopts a synchronous IO model, and threads must\ntolerate the long latency in communication before starting any tasks.\n  In this work, we propose AGILE, a lightweight and efficient asynchronous\nlibrary allowing GPU threads to access SSDs asynchronously while eliminating\ndeadlock risks. AGILE also integrates a flexible software cache using GPU\nHigh-Bandwidth Mamory (HBM). We demonstrate that the asynchronous GPU-centric\nIO achieves up to 1.88$\\times$ improvement in workloads with different\ncomputation-to-communication (CTC) ratios. We also compare AGILE with the SOTA\nwork BaM on Deep Learning Recommendation Models (DLRM) with various settings,\nand the results show that AGILE achieves 1.75$\\times$ performance improvement\ndue to its efficient design and the overlapping strategy enabled by an\nasynchronous IO model. We further evaluate AGILE's API overhead on graph\napplications, and the results demonstrate AGILE reduces software cache overhead\nby up to 3.12$\\times$ and overhead in NVMe IO requests by up to 2.85$\\times$.\nCompared with BaM, AGILE consumes fewer registers and exhibits up to\n1.32$\\times$ reduction in the usage of registers."
                },
                "authors": [
                    {
                        "name": "Zhuoping Yang"
                    },
                    {
                        "name": "Jinming Zhuang"
                    },
                    {
                        "name": "Xingzhen Chen"
                    },
                    {
                        "name": "Alex K. Jones"
                    },
                    {
                        "name": "Peipei Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Peipei Zhou"
                },
                "author": "Peipei Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19365v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19365v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19266v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19266v1",
                "updated": "2025-04-27T14:46:43Z",
                "updated_parsed": [
                    2025,
                    4,
                    27,
                    14,
                    46,
                    43,
                    6,
                    117,
                    0
                ],
                "published": "2025-04-27T14:46:43Z",
                "published_parsed": [
                    2025,
                    4,
                    27,
                    14,
                    46,
                    43,
                    6,
                    117,
                    0
                ],
                "title": "OpenFusion++: An Open-vocabulary Real-time Scene Understanding System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OpenFusion++: An Open-vocabulary Real-time Scene Understanding System"
                },
                "summary": "Real-time open-vocabulary scene understanding is essential for efficient 3D\nperception in applications such as vision-language navigation, embodied\nintelligence, and augmented reality. However, existing methods suffer from\nimprecise instance segmentation, static semantic updates, and limited handling\nof complex queries. To address these issues, we present OpenFusion++, a\nTSDF-based real-time 3D semantic-geometric reconstruction system. Our approach\nrefines 3D point clouds by fusing confidence maps from foundational models,\ndynamically updates global semantic labels via an adaptive cache based on\ninstance area, and employs a dual-path encoding framework that integrates\nobject attributes with environmental context for precise query responses.\nExperiments on the ICL, Replica, ScanNet, and ScanNet++ datasets demonstrate\nthat OpenFusion++ significantly outperforms the baseline in both semantic\naccuracy and query responsiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time open-vocabulary scene understanding is essential for efficient 3D\nperception in applications such as vision-language navigation, embodied\nintelligence, and augmented reality. However, existing methods suffer from\nimprecise instance segmentation, static semantic updates, and limited handling\nof complex queries. To address these issues, we present OpenFusion++, a\nTSDF-based real-time 3D semantic-geometric reconstruction system. Our approach\nrefines 3D point clouds by fusing confidence maps from foundational models,\ndynamically updates global semantic labels via an adaptive cache based on\ninstance area, and employs a dual-path encoding framework that integrates\nobject attributes with environmental context for precise query responses.\nExperiments on the ICL, Replica, ScanNet, and ScanNet++ datasets demonstrate\nthat OpenFusion++ significantly outperforms the baseline in both semantic\naccuracy and query responsiveness."
                },
                "authors": [
                    {
                        "name": "Xiaofeng Jin"
                    },
                    {
                        "name": "Matteo Frosi"
                    },
                    {
                        "name": "Matteo Matteucci"
                    }
                ],
                "author_detail": {
                    "name": "Matteo Matteucci"
                },
                "author": "Matteo Matteucci",
                "arxiv_comment": "8 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19266v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19266v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T45, 68U05",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.10; I.4.8",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19191v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19191v1",
                "updated": "2025-04-27T10:48:56Z",
                "updated_parsed": [
                    2025,
                    4,
                    27,
                    10,
                    48,
                    56,
                    6,
                    117,
                    0
                ],
                "published": "2025-04-27T10:48:56Z",
                "published_parsed": [
                    2025,
                    4,
                    27,
                    10,
                    48,
                    56,
                    6,
                    117,
                    0
                ],
                "title": "WuNeng: Hybrid State with Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WuNeng: Hybrid State with Attention"
                },
                "summary": "The WuNeng architecture introduces a novel approach to enhancing the\nexpressivity and power of large language models by integrating recurrent neural\nnetwork (RNN)-based RWKV-7 with advanced attention mechanisms, prioritizing\nheightened contextual coherence over reducing KV cache size. Building upon the\nhybrid-head concept from Hymba, WuNeng augments standard multi-head attention\nwith additional RWKV-7 state-driven heads, rather than replacing existing\nheads, to enrich the model's representational capacity. A cross-head\ninteraction technique fosters dynamic synergy among standard, state-driven, and\nnewly introduced middle heads, leveraging concatenation, additive modulation,\nand gated fusion for robust information integration. Furthermore, a multi-token\nstate processing mechanism harnesses the continuous RWKV-7 state to capture\nintricate, sequence-wide dependencies, significantly boosting expressivity.\nRemarkably, these enhancements are achieved with minimal additional parameters,\nensuring efficiency while empowering the model to excel in complex reasoning\nand sequence generation tasks. WuNeng sets a new standard for balancing\nexpressivity and computational efficiency in modern neural architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The WuNeng architecture introduces a novel approach to enhancing the\nexpressivity and power of large language models by integrating recurrent neural\nnetwork (RNN)-based RWKV-7 with advanced attention mechanisms, prioritizing\nheightened contextual coherence over reducing KV cache size. Building upon the\nhybrid-head concept from Hymba, WuNeng augments standard multi-head attention\nwith additional RWKV-7 state-driven heads, rather than replacing existing\nheads, to enrich the model's representational capacity. A cross-head\ninteraction technique fosters dynamic synergy among standard, state-driven, and\nnewly introduced middle heads, leveraging concatenation, additive modulation,\nand gated fusion for robust information integration. Furthermore, a multi-token\nstate processing mechanism harnesses the continuous RWKV-7 state to capture\nintricate, sequence-wide dependencies, significantly boosting expressivity.\nRemarkably, these enhancements are achieved with minimal additional parameters,\nensuring efficiency while empowering the model to excel in complex reasoning\nand sequence generation tasks. WuNeng sets a new standard for balancing\nexpressivity and computational efficiency in modern neural architectures."
                },
                "authors": [
                    {
                        "name": "Liu Xiao"
                    },
                    {
                        "name": "Li Zhiyuan"
                    },
                    {
                        "name": "Lin Yueyu"
                    }
                ],
                "author_detail": {
                    "name": "Lin Yueyu"
                },
                "author": "Lin Yueyu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19191v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19191v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10883v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10883v2",
                "updated": "2025-04-26T12:07:35Z",
                "updated_parsed": [
                    2025,
                    4,
                    26,
                    12,
                    7,
                    35,
                    5,
                    116,
                    0
                ],
                "published": "2024-11-16T20:40:08Z",
                "published_parsed": [
                    2024,
                    11,
                    16,
                    20,
                    40,
                    8,
                    5,
                    321,
                    0
                ],
                "title": "I Know What You Sync: Covert and Side Channel Attacks on File Systems\n  via syncfs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "I Know What You Sync: Covert and Side Channel Attacks on File Systems\n  via syncfs"
                },
                "summary": "Operating Systems enforce logical isolation using abstractions such as\nprocesses, containers, and isolation technologies to protect a system from\nmalicious or buggy code. In this paper, we show new types of side channels\nthrough the file system that break this logical isolation. The file system\nplays a critical role in the operating system, managing all I/O activities\nbetween the application layer and the physical storage device. We observe that\nthe file system implementation is shared, leading to timing leakage when using\ncommon I/O system calls. Specifically, we found that modern operating systems\ntake advantage of any flush operation (which saves cached blocks in memory to\nthe SSD or disk) to flush all of the I/O buffers, even those used by other\nisolation domains. Thus, by measuring the delay of syncfs, the attacker can\ninfer the I/O behavior of victim programs. We then demonstrate a syncfs covert\nchannel attack on multiple file systems, including both Linux native file\nsystems and the Windows file system, achieving a maximum bandwidth of 5 Kbps\nwith an error rate of 0.15% on Linux and 7.6 Kbps with an error rate of 1.9% on\nWindows. In addition, we construct three side-channel attacks targeting both\nLinux and Android devices. On Linux devices, we implement a website\nfingerprinting attack and a video fingerprinting attack by tracking the write\npatterns of temporary buffering files. On Android devices, we design an\napplication fingerprinting attack that leaks application write patterns during\nboot-up. The attacks achieve over 90% F1 score, precision, and recall. Finally,\nwe demonstrate that these attacks can be exploited across containers\nimplementing a container detection technique and a cross-container covert\nchannel attack.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Operating Systems enforce logical isolation using abstractions such as\nprocesses, containers, and isolation technologies to protect a system from\nmalicious or buggy code. In this paper, we show new types of side channels\nthrough the file system that break this logical isolation. The file system\nplays a critical role in the operating system, managing all I/O activities\nbetween the application layer and the physical storage device. We observe that\nthe file system implementation is shared, leading to timing leakage when using\ncommon I/O system calls. Specifically, we found that modern operating systems\ntake advantage of any flush operation (which saves cached blocks in memory to\nthe SSD or disk) to flush all of the I/O buffers, even those used by other\nisolation domains. Thus, by measuring the delay of syncfs, the attacker can\ninfer the I/O behavior of victim programs. We then demonstrate a syncfs covert\nchannel attack on multiple file systems, including both Linux native file\nsystems and the Windows file system, achieving a maximum bandwidth of 5 Kbps\nwith an error rate of 0.15% on Linux and 7.6 Kbps with an error rate of 1.9% on\nWindows. In addition, we construct three side-channel attacks targeting both\nLinux and Android devices. On Linux devices, we implement a website\nfingerprinting attack and a video fingerprinting attack by tracking the write\npatterns of temporary buffering files. On Android devices, we design an\napplication fingerprinting attack that leaks application write patterns during\nboot-up. The attacks achieve over 90% F1 score, precision, and recall. Finally,\nwe demonstrate that these attacks can be exploited across containers\nimplementing a container detection technique and a cross-container covert\nchannel attack."
                },
                "authors": [
                    {
                        "name": "Cheng Gu"
                    },
                    {
                        "name": "Yicheng Zhang"
                    },
                    {
                        "name": "Nael Abu-Ghazaleh"
                    }
                ],
                "author_detail": {
                    "name": "Nael Abu-Ghazaleh"
                },
                "author": "Nael Abu-Ghazaleh",
                "arxiv_comment": "Accepted to IEEE S&P 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10883v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10883v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21465v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21465v3",
                "updated": "2025-04-25T19:40:54Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    19,
                    40,
                    54,
                    4,
                    115,
                    0
                ],
                "published": "2024-10-28T19:08:12Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    19,
                    8,
                    12,
                    0,
                    302,
                    0
                ],
                "title": "ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM\n  Inference"
                },
                "summary": "With the widespread deployment of long-context large language models (LLMs),\nthere has been a growing demand for efficient support of high-throughput\ninference. However, as the key-value (KV) cache expands with the sequence\nlength, the increasing memory footprint and the need to access it for each\ntoken generation both result in low throughput when serving long-context LLMs.\nWhile various dynamic sparse attention methods have been proposed to speed up\ninference while maintaining generation quality, they either fail to\nsufficiently reduce GPU memory consumption or introduce significant decoding\nlatency by offloading the KV cache to the CPU. We present ShadowKV, a\nhigh-throughput long-context LLM inference system that stores the low-rank key\ncache and offloads the value cache to reduce the memory footprint for larger\nbatch sizes and longer sequences. To minimize decoding latency, ShadowKV\nemploys an accurate KV selection strategy that reconstructs minimal sparse KV\npairs on-the-fly. By evaluating ShadowKV on a broad range of benchmarks,\nincluding RULER, LongBench, and Needle In A Haystack, and models like\nLlama-3.1-8B, Llama-3-8B-1M, GLM-4-9B-1M, Yi-9B-200K, Phi-3-Mini-128K, and\nQwen2-7B-128K, we demonstrate that it can support up to 6$\\times$ larger batch\nsizes and boost throughput by up to 3.04$\\times$ on an A100 GPU without\nsacrificing accuracy, even surpassing the performance achievable with infinite\nbatch size under the assumption of infinite GPU memory. The code is available\nat https://github.com/bytedance/ShadowKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the widespread deployment of long-context large language models (LLMs),\nthere has been a growing demand for efficient support of high-throughput\ninference. However, as the key-value (KV) cache expands with the sequence\nlength, the increasing memory footprint and the need to access it for each\ntoken generation both result in low throughput when serving long-context LLMs.\nWhile various dynamic sparse attention methods have been proposed to speed up\ninference while maintaining generation quality, they either fail to\nsufficiently reduce GPU memory consumption or introduce significant decoding\nlatency by offloading the KV cache to the CPU. We present ShadowKV, a\nhigh-throughput long-context LLM inference system that stores the low-rank key\ncache and offloads the value cache to reduce the memory footprint for larger\nbatch sizes and longer sequences. To minimize decoding latency, ShadowKV\nemploys an accurate KV selection strategy that reconstructs minimal sparse KV\npairs on-the-fly. By evaluating ShadowKV on a broad range of benchmarks,\nincluding RULER, LongBench, and Needle In A Haystack, and models like\nLlama-3.1-8B, Llama-3-8B-1M, GLM-4-9B-1M, Yi-9B-200K, Phi-3-Mini-128K, and\nQwen2-7B-128K, we demonstrate that it can support up to 6$\\times$ larger batch\nsizes and boost throughput by up to 3.04$\\times$ on an A100 GPU without\nsacrificing accuracy, even surpassing the performance achievable with infinite\nbatch size under the assumption of infinite GPU memory. The code is available\nat https://github.com/bytedance/ShadowKV."
                },
                "authors": [
                    {
                        "name": "Hanshi Sun"
                    },
                    {
                        "name": "Li-Wen Chang"
                    },
                    {
                        "name": "Wenlei Bao"
                    },
                    {
                        "name": "Size Zheng"
                    },
                    {
                        "name": "Ningxin Zheng"
                    },
                    {
                        "name": "Xin Liu"
                    },
                    {
                        "name": "Harry Dong"
                    },
                    {
                        "name": "Yuejie Chi"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21465v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21465v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18434v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18434v1",
                "updated": "2025-04-25T15:45:36Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    15,
                    45,
                    36,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T15:45:36Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    15,
                    45,
                    36,
                    4,
                    115,
                    0
                ],
                "title": "Constructing Hamiltonian Decompositions of Complete $k$-Uniform\n  Hypergraphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Constructing Hamiltonian Decompositions of Complete $k$-Uniform\n  Hypergraphs"
                },
                "summary": "Motivated by the wide-ranging applications of Hamiltonian decompositions in\ndistributed computing, coded caching, routing, resource allocation, load\nbalancing, and fault tolerance, our work presents a comprehensive design for\nHamiltonian decompositions of complete $k$-uniform hypergraphs $K_n^k$.\nBuilding upon the resolution of the long-standing conjecture of the existence\nof Hamiltonian decompositions of complete hypergraphs, a problem that was\nresolved using existence-based methods, our contribution goes beyond the\nprevious explicit designs, which were confined to the specific cases of $k=2$\nand $k=3$, by providing explicit designs for all $k$ and $n$ prime, allowing\nfor a broad applicability of Hamiltonian decompositions in various settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Motivated by the wide-ranging applications of Hamiltonian decompositions in\ndistributed computing, coded caching, routing, resource allocation, load\nbalancing, and fault tolerance, our work presents a comprehensive design for\nHamiltonian decompositions of complete $k$-uniform hypergraphs $K_n^k$.\nBuilding upon the resolution of the long-standing conjecture of the existence\nof Hamiltonian decompositions of complete hypergraphs, a problem that was\nresolved using existence-based methods, our contribution goes beyond the\nprevious explicit designs, which were confined to the specific cases of $k=2$\nand $k=3$, by providing explicit designs for all $k$ and $n$ prime, allowing\nfor a broad applicability of Hamiltonian decompositions in various settings."
                },
                "authors": [
                    {
                        "name": "Javad Maheri"
                    },
                    {
                        "name": "Petros Elia"
                    }
                ],
                "author_detail": {
                    "name": "Petros Elia"
                },
                "author": "Petros Elia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18434v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18434v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18432v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18432v1",
                "updated": "2025-04-25T15:44:38Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    15,
                    44,
                    38,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T15:44:38Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    15,
                    44,
                    38,
                    4,
                    115,
                    0
                ],
                "title": "FlexiNS: A SmartNIC-Centric, Line-Rate and Flexible Network Stack",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlexiNS: A SmartNIC-Centric, Line-Rate and Flexible Network Stack"
                },
                "summary": "As the gap between network and CPU speeds rapidly increases, the CPU-centric\nnetwork stack proves inadequate due to excessive CPU and memory overhead. While\nhardware-offloaded network stacks alleviate these issues, they suffer from\nlimited flexibility in both control and data planes. Offloading network stack\nto off-path SmartNIC seems promising to provide high flexibility; however,\nthroughput remains constrained by inherent SmartNIC architectural limitations.\n  To this end, we design FlexiNS, a SmartNIC-centric network stack with\nsoftware transport programmability and line-rate packet processing\ncapabilities. To grapple with the limitation of SmartNIC-induced challenges,\nFlexiNS introduces: (a) a header-only offloading TX path; (b) an\nunlimited-working-set in-cache processing RX path; (c) a high-performance\nDMA-only notification pipe; and (d) a programmable offloading engine. We\nprototype FlexiNS using Nvidia BlueField-3 SmartNIC and provide out-of-the-box\nRDMA IBV verbs compatibility to users. FlexiNS achieves 2.2$\\times$ higher\nthroughput than the microkernel-based baseline in block storage disaggregation\nand 1.3$\\times$ higher throughput than the hardware-offloaded baseline in\nKVCache transfer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the gap between network and CPU speeds rapidly increases, the CPU-centric\nnetwork stack proves inadequate due to excessive CPU and memory overhead. While\nhardware-offloaded network stacks alleviate these issues, they suffer from\nlimited flexibility in both control and data planes. Offloading network stack\nto off-path SmartNIC seems promising to provide high flexibility; however,\nthroughput remains constrained by inherent SmartNIC architectural limitations.\n  To this end, we design FlexiNS, a SmartNIC-centric network stack with\nsoftware transport programmability and line-rate packet processing\ncapabilities. To grapple with the limitation of SmartNIC-induced challenges,\nFlexiNS introduces: (a) a header-only offloading TX path; (b) an\nunlimited-working-set in-cache processing RX path; (c) a high-performance\nDMA-only notification pipe; and (d) a programmable offloading engine. We\nprototype FlexiNS using Nvidia BlueField-3 SmartNIC and provide out-of-the-box\nRDMA IBV verbs compatibility to users. FlexiNS achieves 2.2$\\times$ higher\nthroughput than the microkernel-based baseline in block storage disaggregation\nand 1.3$\\times$ higher throughput than the hardware-offloaded baseline in\nKVCache transfer."
                },
                "authors": [
                    {
                        "name": "Xuzheng Chen"
                    },
                    {
                        "name": "Jie Zhang"
                    },
                    {
                        "name": "Baolin Zhu"
                    },
                    {
                        "name": "Xueying Zhu"
                    },
                    {
                        "name": "Zhongqing Chen"
                    },
                    {
                        "name": "Shu Ma"
                    },
                    {
                        "name": "Lingjun Zhu"
                    },
                    {
                        "name": "Chao Shi"
                    },
                    {
                        "name": "Yin Zhang"
                    },
                    {
                        "name": "Zeke Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zeke Wang"
                },
                "author": "Zeke Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18432v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18432v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18242v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18242v1",
                "updated": "2025-04-25T10:43:23Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    10,
                    43,
                    23,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T10:43:23Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    10,
                    43,
                    23,
                    4,
                    115,
                    0
                ],
                "title": "Demand Private Coded Caching: Small Cache Size",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demand Private Coded Caching: Small Cache Size"
                },
                "summary": "We investigate the demand private coded caching problem, which is an $(N,K)$\ncoded caching problem with $N$ files, $K$ users, each equipped with a cache of\nsize $M$, and an additional privacy constraint on user demands, i.e., each user\ncan not gain any information about the demands of other users. We focus on\nscenarios where the size of users' caches is small, aiming to further\ncharacterize the fundamental limits of this problem. We first present a new\nvirtual-user-based achievable scheme for arbitrary number of users and files,\nand two MDS-code-based achievable schemes for the case $N \\le K$. With a newly\nderived converse bound for the case $N \\le K$, these proposed schemes lead to\nthe optimal memory-rate tradeoff of the demand private coded caching problem\nfor $M \\in \\big[0, \\frac{N}{(K+1)(N-1)} \\big] $ where $N \\le K \\le 2N-2$, and\nthe optimal memory-rate tradeoff for $M \\in \\big[0, \\frac{1}{K+1} \\big] $ where\n$ K > 2N-2$. Moreover, for the case of 2 files and arbitrary number of users,\nby deriving another new converse bound, the optimal memory-rate tradeoff is\ncharacterized for $M\\in \\big[0,\\frac{2}{K}\\big] \\cup\n\\big[\\frac{2(K-1)}{K+1},2\\big]$. Finally, we provide the optimal memory-rate\ntradeoff of the demand private coded caching problem for 2 files and 3 users.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate the demand private coded caching problem, which is an $(N,K)$\ncoded caching problem with $N$ files, $K$ users, each equipped with a cache of\nsize $M$, and an additional privacy constraint on user demands, i.e., each user\ncan not gain any information about the demands of other users. We focus on\nscenarios where the size of users' caches is small, aiming to further\ncharacterize the fundamental limits of this problem. We first present a new\nvirtual-user-based achievable scheme for arbitrary number of users and files,\nand two MDS-code-based achievable schemes for the case $N \\le K$. With a newly\nderived converse bound for the case $N \\le K$, these proposed schemes lead to\nthe optimal memory-rate tradeoff of the demand private coded caching problem\nfor $M \\in \\big[0, \\frac{N}{(K+1)(N-1)} \\big] $ where $N \\le K \\le 2N-2$, and\nthe optimal memory-rate tradeoff for $M \\in \\big[0, \\frac{1}{K+1} \\big] $ where\n$ K > 2N-2$. Moreover, for the case of 2 files and arbitrary number of users,\nby deriving another new converse bound, the optimal memory-rate tradeoff is\ncharacterized for $M\\in \\big[0,\\frac{2}{K}\\big] \\cup\n\\big[\\frac{2(K-1)}{K+1},2\\big]$. Finally, we provide the optimal memory-rate\ntradeoff of the demand private coded caching problem for 2 files and 3 users."
                },
                "authors": [
                    {
                        "name": "Qinyi Lu"
                    },
                    {
                        "name": "Nan Liu"
                    },
                    {
                        "name": "Wei Kang"
                    },
                    {
                        "name": "Chunguo Li"
                    }
                ],
                "author_detail": {
                    "name": "Chunguo Li"
                },
                "author": "Chunguo Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18242v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18242v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18082v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18082v1",
                "updated": "2025-04-25T05:16:53Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    5,
                    16,
                    53,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T05:16:53Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    5,
                    16,
                    53,
                    4,
                    115,
                    0
                ],
                "title": "Efficient GNN Training Through Structure-Aware Randomized Mini-Batching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient GNN Training Through Structure-Aware Randomized Mini-Batching"
                },
                "summary": "Graph Neural Networks (GNNs) enable learning on realworld graphs and\nmini-batch training has emerged as the de facto standard for training GNNs\nbecause it can scale to very large graphs and improve convergence. Current\nmini-batch construction policies largely ignore efficiency considerations of\nGNN training. Specifically, existing mini-batching techniques employ\nrandomization schemes to improve accuracy and convergence. However, these\nrandomization schemes are often agnostic to the structural properties of the\ngraph (for eg. community structure), resulting in highly irregular memory\naccess patterns during GNN training that make suboptimal use of on-chip GPU\ncaches. On the other hand, while deterministic mini-batching based solely on\ngraph structure delivers fast runtime performance, the lack of randomness\ncompromises both the final model accuracy and training convergence speed. In\nthis paper, we present Community-structure-aware Randomized Mini-batching\n(COMM-RAND), a novel methodology that bridges the gap between the above\nextremes. COMM-RAND allows practitioners to explore the space between pure\nrandomness and pure graph structural awareness during mini-batch construction,\nleading to significantly more efficient GNN training with similar accuracy. We\nevaluated COMM-RAND across four popular graph learning benchmarks. COMM-RAND\ncuts down GNN training time by up to 2.76x (1.8x on average) while achieving an\naccuracy that is within 1.79% points (0.42% on average) compared to popular\nrandom mini-batching approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Neural Networks (GNNs) enable learning on realworld graphs and\nmini-batch training has emerged as the de facto standard for training GNNs\nbecause it can scale to very large graphs and improve convergence. Current\nmini-batch construction policies largely ignore efficiency considerations of\nGNN training. Specifically, existing mini-batching techniques employ\nrandomization schemes to improve accuracy and convergence. However, these\nrandomization schemes are often agnostic to the structural properties of the\ngraph (for eg. community structure), resulting in highly irregular memory\naccess patterns during GNN training that make suboptimal use of on-chip GPU\ncaches. On the other hand, while deterministic mini-batching based solely on\ngraph structure delivers fast runtime performance, the lack of randomness\ncompromises both the final model accuracy and training convergence speed. In\nthis paper, we present Community-structure-aware Randomized Mini-batching\n(COMM-RAND), a novel methodology that bridges the gap between the above\nextremes. COMM-RAND allows practitioners to explore the space between pure\nrandomness and pure graph structural awareness during mini-batch construction,\nleading to significantly more efficient GNN training with similar accuracy. We\nevaluated COMM-RAND across four popular graph learning benchmarks. COMM-RAND\ncuts down GNN training time by up to 2.76x (1.8x on average) while achieving an\naccuracy that is within 1.79% points (0.42% on average) compared to popular\nrandom mini-batching approaches."
                },
                "authors": [
                    {
                        "name": "Vignesh Balaji"
                    },
                    {
                        "name": "Christos Kozyrakis"
                    },
                    {
                        "name": "Gal Chechik"
                    },
                    {
                        "name": "Haggai Maron"
                    }
                ],
                "author_detail": {
                    "name": "Haggai Maron"
                },
                "author": "Haggai Maron",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18082v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18082v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14335v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14335v2",
                "updated": "2025-04-25T05:08:45Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    5,
                    8,
                    45,
                    4,
                    115,
                    0
                ],
                "published": "2024-12-18T21:09:08Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    21,
                    9,
                    8,
                    2,
                    353,
                    0
                ],
                "title": "Optimizing ML Concurrent Computation and Communication with GPU DMA\n  Engines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing ML Concurrent Computation and Communication with GPU DMA\n  Engines"
                },
                "summary": "Concurrent computation and communication (C3) is a pervasive paradigm in ML\nand other domains, making its performance optimization crucial. In this paper,\nwe carefully characterize C3 in ML on GPUs, which are most widely deployed for\nML training and inference. We observe that while C3 leads to performance\nuplifts, the uplifts are far lower than ideal speedups (serial computation and\ncommunication versus maximum of computation or communication; all times from\nisolated executions). That is, C3 on average achieves only 21% of ideal\nspeedup. This is so, due to known challenges of compute and memory interference\nbetween concurrent GPU kernels (that is, sharing of GPU's compute units, caches\nand HBM).\n  To attain better performance for C3, first, we evaluate dual strategies of\nschedule prioritization and careful resource partitioning of compute units on\nGPUs to push performance attained with C3 (on average 42% of ideal speedup). We\nalso provide heuristics that can guide a runtime while employing these\nstrategies. To further enhance C3 performance, we propose to mitigate C3\ninterference by offloading communication tasks to the GPU's DMA engines. To\nthis end, we build concurrent communication collectives (ConCCL)\nproof-of-concepts that harness DMA engines for communication. We show how\nConCCL considerably closes the gap between realized and ideal speedup for C3\n(on average 72% of ideal speedup is realized, up to 1.67x speedup). Overall,\nour work makes a strong case for GPU DMA engine advancements to better support\nC3 on GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Concurrent computation and communication (C3) is a pervasive paradigm in ML\nand other domains, making its performance optimization crucial. In this paper,\nwe carefully characterize C3 in ML on GPUs, which are most widely deployed for\nML training and inference. We observe that while C3 leads to performance\nuplifts, the uplifts are far lower than ideal speedups (serial computation and\ncommunication versus maximum of computation or communication; all times from\nisolated executions). That is, C3 on average achieves only 21% of ideal\nspeedup. This is so, due to known challenges of compute and memory interference\nbetween concurrent GPU kernels (that is, sharing of GPU's compute units, caches\nand HBM).\n  To attain better performance for C3, first, we evaluate dual strategies of\nschedule prioritization and careful resource partitioning of compute units on\nGPUs to push performance attained with C3 (on average 42% of ideal speedup). We\nalso provide heuristics that can guide a runtime while employing these\nstrategies. To further enhance C3 performance, we propose to mitigate C3\ninterference by offloading communication tasks to the GPU's DMA engines. To\nthis end, we build concurrent communication collectives (ConCCL)\nproof-of-concepts that harness DMA engines for communication. We show how\nConCCL considerably closes the gap between realized and ideal speedup for C3\n(on average 72% of ideal speedup is realized, up to 1.67x speedup). Overall,\nour work makes a strong case for GPU DMA engine advancements to better support\nC3 on GPUs."
                },
                "authors": [
                    {
                        "name": "Anirudha Agrawal"
                    },
                    {
                        "name": "Shaizeen Aga"
                    },
                    {
                        "name": "Suchita Pati"
                    },
                    {
                        "name": "Mahzabeen Islam"
                    }
                ],
                "author_detail": {
                    "name": "Mahzabeen Islam"
                },
                "author": "Mahzabeen Islam",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14335v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14335v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16620v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16620v2",
                "updated": "2025-04-25T05:05:49Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    5,
                    5,
                    49,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-23T11:18:34Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    11,
                    18,
                    34,
                    2,
                    113,
                    0
                ],
                "title": "Fluctuated lattice-driven charge density wave far above the condensation\n  temperature in kagome superconductor KV$_3$Sb$_5$",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fluctuated lattice-driven charge density wave far above the condensation\n  temperature in kagome superconductor KV$_3$Sb$_5$"
                },
                "summary": "The kagome material AV$_3$Sb$_5$ exhibits multiple exotic orders, including\nan unconventional charge density wave (CDW). Elucidating the underlying\nmechanism behind the CDW transition is crucial for unraveling the complex\ninteractions among these phases. However, the driving force of the CDW remains\na topic of debate due to the intertwined interactions among the system's\nvarious excitations. Here we investigated the CDW transition in KV$_3$Sb$_5$ by\nisolating the ultrafast electronic phase transition using time- and\nangleresolved photoemission spectroscopy. An ultrafast electronic phase\ntransition was observed at a critical photoexcitation fluence, F$_c$, without\nreduction in CDW lattice-distortion-induced band folding. This folded band\npersisted up to 150 K under equilibrium heating, well above the CDW\ncondensation temperature of T$_c$ = 78 K. Notably, the pump-induced band shifts\nat F$_c$ were comparable to those caused by thermal effects at T$_c$. These\nfindings suggest that in KV$_3$Sb$_5$, a fluctuating lattice-driven in-plane\nCDW emerges above 150 K, with out-of-plane electronic correlations leading to\nthe $2\\times2 \\times 2$ CDW near T$_c$, offering key insights into the\ninterplay between the electronic and structural dynamics in AV$_3$Sb$_5$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The kagome material AV$_3$Sb$_5$ exhibits multiple exotic orders, including\nan unconventional charge density wave (CDW). Elucidating the underlying\nmechanism behind the CDW transition is crucial for unraveling the complex\ninteractions among these phases. However, the driving force of the CDW remains\na topic of debate due to the intertwined interactions among the system's\nvarious excitations. Here we investigated the CDW transition in KV$_3$Sb$_5$ by\nisolating the ultrafast electronic phase transition using time- and\nangleresolved photoemission spectroscopy. An ultrafast electronic phase\ntransition was observed at a critical photoexcitation fluence, F$_c$, without\nreduction in CDW lattice-distortion-induced band folding. This folded band\npersisted up to 150 K under equilibrium heating, well above the CDW\ncondensation temperature of T$_c$ = 78 K. Notably, the pump-induced band shifts\nat F$_c$ were comparable to those caused by thermal effects at T$_c$. These\nfindings suggest that in KV$_3$Sb$_5$, a fluctuating lattice-driven in-plane\nCDW emerges above 150 K, with out-of-plane electronic correlations leading to\nthe $2\\times2 \\times 2$ CDW near T$_c$, offering key insights into the\ninterplay between the electronic and structural dynamics in AV$_3$Sb$_5$."
                },
                "authors": [
                    {
                        "name": "Haoran Liu"
                    },
                    {
                        "name": "Shaofeng Duan"
                    },
                    {
                        "name": "Xiangqi Liu"
                    },
                    {
                        "name": "Zhihua Liu"
                    },
                    {
                        "name": "Shichong Wang"
                    },
                    {
                        "name": "Lingxiao Gu"
                    },
                    {
                        "name": "Jiongyu Huang"
                    },
                    {
                        "name": "Wenxuan Yang"
                    },
                    {
                        "name": "Jianzhe Liu"
                    },
                    {
                        "name": "Dong Qian"
                    },
                    {
                        "name": "Yanfeng Guo"
                    },
                    {
                        "name": "Wentao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wentao Zhang"
                },
                "author": "Wentao Zhang",
                "arxiv_doi": "10.1016/j.scib.2025.02.018",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.scib.2025.02.018",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.16620v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16620v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "21 pages, 4 figures",
                "arxiv_journal_ref": "Science Bulletin 70, 1211-1214 (2025)",
                "arxiv_primary_category": {
                    "term": "cond-mat.str-el",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.str-el",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.supr-con",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17995v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17995v1",
                "updated": "2025-04-25T00:41:43Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    0,
                    41,
                    43,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T00:41:43Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    0,
                    41,
                    43,
                    4,
                    115,
                    0
                ],
                "title": "Role of On-site and Inter-site Coulomb Interactions in KV$_3$Sb$_5$: A\n  first-principles DFT+$U$+$V$ study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Role of On-site and Inter-site Coulomb Interactions in KV$_3$Sb$_5$: A\n  first-principles DFT+$U$+$V$ study"
                },
                "summary": "Nonlocal Coulomb interactions play a crucial role in stabilizing distinct\nelectronic phases in kagome materials. In this work, we systematically\ninvestigate the effects of on-site ($U$) and inter-site ($V$) Coulomb\ninteractions on the electronic structure and stability of charge-density-wave\n(CDW) phases in the kagome metal KV$_3$Sb$_5$ using density functional theory\n(DFT+$U$+$V$) calculations. We demonstrate that $V$ promotes the formation and\nstability of CDW phases, whereas $U$ suppresses these phases, highlighting a\nfundamental competition between local and nonlocal electronic correlations. By\ndirectly comparing our theoretical results with angle-resolved photoemission\nspectroscopy (ARPES) data, we identify realistic values of $U$ and $V$ that\naccurately describe the electronic band structure of KV$_3$Sb$_5$. Our findings\nestablish a detailed $U$-$V$ phase diagram for KV$_3$Sb$_5$, offering valuable\ninsights into the correlated electronic states in kagome metals and serving as\na foundation for future explorations of correlation-driven phenomena in related\nmaterials.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nonlocal Coulomb interactions play a crucial role in stabilizing distinct\nelectronic phases in kagome materials. In this work, we systematically\ninvestigate the effects of on-site ($U$) and inter-site ($V$) Coulomb\ninteractions on the electronic structure and stability of charge-density-wave\n(CDW) phases in the kagome metal KV$_3$Sb$_5$ using density functional theory\n(DFT+$U$+$V$) calculations. We demonstrate that $V$ promotes the formation and\nstability of CDW phases, whereas $U$ suppresses these phases, highlighting a\nfundamental competition between local and nonlocal electronic correlations. By\ndirectly comparing our theoretical results with angle-resolved photoemission\nspectroscopy (ARPES) data, we identify realistic values of $U$ and $V$ that\naccurately describe the electronic band structure of KV$_3$Sb$_5$. Our findings\nestablish a detailed $U$-$V$ phase diagram for KV$_3$Sb$_5$, offering valuable\ninsights into the correlated electronic states in kagome metals and serving as\na foundation for future explorations of correlation-driven phenomena in related\nmaterials."
                },
                "authors": [
                    {
                        "name": "Indukuru Ramesh Reddy"
                    },
                    {
                        "name": "Sayandeep Ghosh"
                    },
                    {
                        "name": "Bongjae Kim"
                    },
                    {
                        "name": "Chang-Jong Kang"
                    }
                ],
                "author_detail": {
                    "name": "Chang-Jong Kang"
                },
                "author": "Chang-Jong Kang",
                "arxiv_comment": "8 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17995v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17995v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.str-el",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.str-el",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17866v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17866v1",
                "updated": "2025-04-24T18:09:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    18,
                    9,
                    25,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T18:09:25Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    18,
                    9,
                    25,
                    3,
                    114,
                    0
                ],
                "title": "Updated parameters of the LArQL model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Updated parameters of the LArQL model"
                },
                "summary": "The need for a microscopic description of scintillation light generation in\nliquid argon becomes increasingly desirable with the upcoming operation of\nlarge scale LArTPCs in the next decade. While a detailed mathematical account\nof the process is still to be achieved, a phenomenological model for\nsimultaneously treating ionization and scintillation, LArQL, has been\nsuccessfully employed to describe the range of electric fields from 0 to 0.75\nkV/cm and dE/dx from 2 to 40 MeV/cm providing the anti-correlation between the\nfree ionization charge and scintillation light. A reanalysis of the original\nmodel parameter values has been performed within a global fit procedure and is\npresented.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The need for a microscopic description of scintillation light generation in\nliquid argon becomes increasingly desirable with the upcoming operation of\nlarge scale LArTPCs in the next decade. While a detailed mathematical account\nof the process is still to be achieved, a phenomenological model for\nsimultaneously treating ionization and scintillation, LArQL, has been\nsuccessfully employed to describe the range of electric fields from 0 to 0.75\nkV/cm and dE/dx from 2 to 40 MeV/cm providing the anti-correlation between the\nfree ionization charge and scintillation light. A reanalysis of the original\nmodel parameter values has been performed within a global fit procedure and is\npresented."
                },
                "authors": [
                    {
                        "name": "L. Paulucci"
                    },
                    {
                        "name": "F. Cavanna"
                    },
                    {
                        "name": "V. Vale"
                    },
                    {
                        "name": "F. Marinho"
                    }
                ],
                "author_detail": {
                    "name": "F. Marinho"
                },
                "author": "F. Marinho",
                "arxiv_comment": "Part of the proceedings of LIDINE 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17866v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17866v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-ex",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17584v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17584v1",
                "updated": "2025-04-24T14:14:07Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    14,
                    14,
                    7,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T14:14:07Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    14,
                    14,
                    7,
                    3,
                    114,
                    0
                ],
                "title": "L3: DIMM-PIM Integrated Architecture and Coordination for Scalable\n  Long-Context LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "L3: DIMM-PIM Integrated Architecture and Coordination for Scalable\n  Long-Context LLM Inference"
                },
                "summary": "Large Language Models (LLMs) increasingly require processing long text\nsequences, but GPU memory limitations force difficult trade-offs between memory\ncapacity and bandwidth. While HBM-based acceleration offers high bandwidth, its\ncapacity remains constrained. Offloading data to host-side DIMMs improves\ncapacity but introduces costly data swapping overhead. We identify that the\ncritical memory bottleneck lies in the decoding phase of multi-head attention\n(MHA) exclusively, which demands substantial capacity for storing KV caches and\nhigh bandwidth for attention computation. Our key insight reveals this\noperation uniquely aligns with modern DIMM-based processing-in-memory (PIM)\narchitectures, which offers scalability of both capacity and bandwidth.\n  Based on this observation and insight, we propose L3, a hardware-software\nco-designed system integrating DIMM-PIM and GPU devices. L3 introduces three\ninnovations: First, hardware redesigns resolve data layout mismatches and\ncomputational element mismatches in DIMM-PIM, enhancing LLM inference\nutilization. Second, communication optimization enables hiding the data\ntransfer overhead with the computation. Third, an adaptive scheduler\ncoordinates GPU-DIMM-PIM operations to maximize parallelism between devices.\nEvaluations using real-world traces show L3 achieves up to 6.1$\\times$ speedup\nover state-of-the-art HBM-PIM solutions while significantly improving batch\nsizes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) increasingly require processing long text\nsequences, but GPU memory limitations force difficult trade-offs between memory\ncapacity and bandwidth. While HBM-based acceleration offers high bandwidth, its\ncapacity remains constrained. Offloading data to host-side DIMMs improves\ncapacity but introduces costly data swapping overhead. We identify that the\ncritical memory bottleneck lies in the decoding phase of multi-head attention\n(MHA) exclusively, which demands substantial capacity for storing KV caches and\nhigh bandwidth for attention computation. Our key insight reveals this\noperation uniquely aligns with modern DIMM-based processing-in-memory (PIM)\narchitectures, which offers scalability of both capacity and bandwidth.\n  Based on this observation and insight, we propose L3, a hardware-software\nco-designed system integrating DIMM-PIM and GPU devices. L3 introduces three\ninnovations: First, hardware redesigns resolve data layout mismatches and\ncomputational element mismatches in DIMM-PIM, enhancing LLM inference\nutilization. Second, communication optimization enables hiding the data\ntransfer overhead with the computation. Third, an adaptive scheduler\ncoordinates GPU-DIMM-PIM operations to maximize parallelism between devices.\nEvaluations using real-world traces show L3 achieves up to 6.1$\\times$ speedup\nover state-of-the-art HBM-PIM solutions while significantly improving batch\nsizes."
                },
                "authors": [
                    {
                        "name": "Qingyuan Liu"
                    },
                    {
                        "name": "Liyan Chen"
                    },
                    {
                        "name": "Yanning Yang"
                    },
                    {
                        "name": "Haocheng Wang"
                    },
                    {
                        "name": "Dong Du"
                    },
                    {
                        "name": "Zhigang Mao"
                    },
                    {
                        "name": "Naifeng Jing"
                    },
                    {
                        "name": "Yubin Xia"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "arxiv_comment": "16 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17584v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17584v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17554v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17554v1",
                "updated": "2025-04-24T13:47:35Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    13,
                    47,
                    35,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T13:47:35Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    13,
                    47,
                    35,
                    3,
                    114,
                    0
                ],
                "title": "Rethinking PM Crash Consistency in the CXL Era",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking PM Crash Consistency in the CXL Era"
                },
                "summary": "Persistent Memory (PM) introduces new opportunities for designing\ncrash-consistent applications without the traditional storage overheads.\nHowever, ensuring crash consistency in PM demands intricate knowledge of CPU,\ncache, and memory interactions. Hardware and software mechanisms have been\nproposed to ease this burden, but neither proved sufficient, prompting a\nvariety of bug detection tools.\n  With the sunset of Intel Optane comes the rise of Compute Express Link (CXL)\nfor PM. In this position paper, we discuss the impact of CXL's disaggregated\nand heterogeneous nature in the development of crash-consistent PM\napplications, and outline three research directions: hardware primitives,\npersistency frameworks, and bug detection tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Persistent Memory (PM) introduces new opportunities for designing\ncrash-consistent applications without the traditional storage overheads.\nHowever, ensuring crash consistency in PM demands intricate knowledge of CPU,\ncache, and memory interactions. Hardware and software mechanisms have been\nproposed to ease this burden, but neither proved sufficient, prompting a\nvariety of bug detection tools.\n  With the sunset of Intel Optane comes the rise of Compute Express Link (CXL)\nfor PM. In this position paper, we discuss the impact of CXL's disaggregated\nand heterogeneous nature in the development of crash-consistent PM\napplications, and outline three research directions: hardware primitives,\npersistency frameworks, and bug detection tools."
                },
                "authors": [
                    {
                        "name": "João Oliveira"
                    },
                    {
                        "name": "João Gonçalves"
                    },
                    {
                        "name": "Miguel Matos"
                    }
                ],
                "author_detail": {
                    "name": "Miguel Matos"
                },
                "author": "Miguel Matos",
                "arxiv_comment": "5 pages (2 extra pages for references), 1 figure, 2 algorithms",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17554v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17554v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.ET",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08141v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08141v3",
                "updated": "2025-04-24T08:39:13Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    8,
                    39,
                    13,
                    3,
                    114,
                    0
                ],
                "published": "2024-09-12T15:34:23Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    15,
                    34,
                    23,
                    3,
                    256,
                    0
                ],
                "title": "Rethinking Programmed I/O for Fast Devices, Cheap Cores, and Coherent\n  Interconnects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Programmed I/O for Fast Devices, Cheap Cores, and Coherent\n  Interconnects"
                },
                "summary": "Conventional wisdom holds that an efficient interface between an OS running\non a CPU and a high-bandwidth I/O device should use Direct Memory Access (DMA)\nto offload data transfer, descriptor rings for buffering and queuing, and\ninterrupts for asynchrony between cores and device.\n  In this paper we question this wisdom in the light of two trends: modern and\nemerging cache-coherent interconnects like CXL3.0, and workloads, particularly\nmicroservices and serverless computing. Like some others before us, we argue\nthat the assumptions of the DMA-based model are obsolete, and in many use-cases\nprogrammed I/O, where the CPU explicitly transfers data and control information\nto and from a device via loads and stores, delivers a more efficient system.\n  However, we push this idea much further. We show, in a real hardware\nimplementation, the gains in latency for fine-grained communication achievable\nusing an open cache-coherence protocol which exposes cache transitions to a\nsmart device, and that throughput is competitive with DMA over modern\ninterconnects. We also demonstrate three use-cases: fine-grained RPC-style\ninvocation of functions on an accelerator, offloading of operators in a\nstreaming dataflow engine, and a network interface targeting serverless\nfunctions, comparing our use of coherence with both traditional DMA-style\ninteraction and a highly-optimized implementation using memory-mapped\nprogrammed I/O over PCIe.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conventional wisdom holds that an efficient interface between an OS running\non a CPU and a high-bandwidth I/O device should use Direct Memory Access (DMA)\nto offload data transfer, descriptor rings for buffering and queuing, and\ninterrupts for asynchrony between cores and device.\n  In this paper we question this wisdom in the light of two trends: modern and\nemerging cache-coherent interconnects like CXL3.0, and workloads, particularly\nmicroservices and serverless computing. Like some others before us, we argue\nthat the assumptions of the DMA-based model are obsolete, and in many use-cases\nprogrammed I/O, where the CPU explicitly transfers data and control information\nto and from a device via loads and stores, delivers a more efficient system.\n  However, we push this idea much further. We show, in a real hardware\nimplementation, the gains in latency for fine-grained communication achievable\nusing an open cache-coherence protocol which exposes cache transitions to a\nsmart device, and that throughput is competitive with DMA over modern\ninterconnects. We also demonstrate three use-cases: fine-grained RPC-style\ninvocation of functions on an accelerator, offloading of operators in a\nstreaming dataflow engine, and a network interface targeting serverless\nfunctions, comparing our use of coherence with both traditional DMA-style\ninteraction and a highly-optimized implementation using memory-mapped\nprogrammed I/O over PCIe."
                },
                "authors": [
                    {
                        "name": "Anastasiia Ruzhanskaia"
                    },
                    {
                        "name": "Pengcheng Xu"
                    },
                    {
                        "name": "David Cock"
                    },
                    {
                        "name": "Timothy Roscoe"
                    }
                ],
                "author_detail": {
                    "name": "Timothy Roscoe"
                },
                "author": "Timothy Roscoe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08141v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08141v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15192v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15192v2",
                "updated": "2025-04-24T04:36:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    4,
                    36,
                    20,
                    3,
                    114,
                    0
                ],
                "published": "2025-02-21T04:07:00Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    4,
                    7,
                    0,
                    4,
                    52,
                    0
                ],
                "title": "SPAARC: Spatial Proximity and Association based prefetching for\n  Augmented Reality in edge Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SPAARC: Spatial Proximity and Association based prefetching for\n  Augmented Reality in edge Cache"
                },
                "summary": "Mobile Augmented Reality (MAR) applications face performance challenges due\nto their high computational demands and need for low-latency responses.\nTraditional approaches like on-device storage or reactive data fetching from\nthe cloud often result in limited AR experiences or unacceptable lag. Edge\ncaching, which caches AR objects closer to the user, provides a promising\nsolution. However, existing edge caching approaches do not consider AR-specific\nfeatures such as AR object sizes, user interactions, and physical location.\nThis paper investigates how to further optimize edge caching by employing\nAR-aware prefetching techniques. We present SPAARC, a Spatial Proximity and\nAssociation-based Prefetching policy specifically designed for MAR Caches.\nSPAARC intelligently prioritizes the caching of virtual objects based on their\nassociation with other similar objects and the user's proximity to them. It\nalso considers the recency of associations and uses a lazy fetching strategy to\nefficiently manage edge resources and maximize Quality of Experience (QoE).\n  Through extensive evaluation using both synthetic and real-world workloads,\nwe demonstrate that SPAARC significantly improves cache hit rates compared to\nstandard caching algorithms, achieving gains ranging from 3% to 40% while\nreducing the need for on-demand data retrieval from the cloud. Further, we\npresent an adaptive tuning algorithm that automatically tunes SPAARC parameters\nto achieve optimal performance. Our findings demonstrate the potential of\nSPAARC to substantially enhance the user experience in MAR applications by\nensuring the timely availability of virtual objects.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mobile Augmented Reality (MAR) applications face performance challenges due\nto their high computational demands and need for low-latency responses.\nTraditional approaches like on-device storage or reactive data fetching from\nthe cloud often result in limited AR experiences or unacceptable lag. Edge\ncaching, which caches AR objects closer to the user, provides a promising\nsolution. However, existing edge caching approaches do not consider AR-specific\nfeatures such as AR object sizes, user interactions, and physical location.\nThis paper investigates how to further optimize edge caching by employing\nAR-aware prefetching techniques. We present SPAARC, a Spatial Proximity and\nAssociation-based Prefetching policy specifically designed for MAR Caches.\nSPAARC intelligently prioritizes the caching of virtual objects based on their\nassociation with other similar objects and the user's proximity to them. It\nalso considers the recency of associations and uses a lazy fetching strategy to\nefficiently manage edge resources and maximize Quality of Experience (QoE).\n  Through extensive evaluation using both synthetic and real-world workloads,\nwe demonstrate that SPAARC significantly improves cache hit rates compared to\nstandard caching algorithms, achieving gains ranging from 3% to 40% while\nreducing the need for on-demand data retrieval from the cloud. Further, we\npresent an adaptive tuning algorithm that automatically tunes SPAARC parameters\nto achieve optimal performance. Our findings demonstrate the potential of\nSPAARC to substantially enhance the user experience in MAR applications by\nensuring the timely availability of virtual objects."
                },
                "authors": [
                    {
                        "name": "Nikhil Sreekumar"
                    },
                    {
                        "name": "Abhishek Chandra"
                    },
                    {
                        "name": "Jon Weissman"
                    }
                ],
                "author_detail": {
                    "name": "Jon Weissman"
                },
                "author": "Jon Weissman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15192v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15192v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.ET",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14992v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14992v2",
                "updated": "2025-04-24T04:13:49Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    4,
                    13,
                    49,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-21T09:41:26Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    9,
                    41,
                    26,
                    0,
                    111,
                    0
                ],
                "title": "Efficient Pretraining Length Scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Pretraining Length Scaling"
                },
                "summary": "Recent advances in large language models have demonstrated the effectiveness\nof length scaling during post-training, yet its potential in pre-training\nremains underexplored. We present the Parallel Hidden Decoding Transformer\n(\\textit{PHD}-Transformer), a novel framework that enables efficient length\nscaling during pre-training while maintaining inference efficiency.\n\\textit{PHD}-Transformer achieves this through an innovative KV cache\nmanagement strategy that distinguishes between original tokens and hidden\ndecoding tokens. By retaining only the KV cache of original tokens for\nlong-range dependencies while immediately discarding hidden decoding tokens\nafter use, our approach maintains the same KV cache size as the vanilla\ntransformer while enabling effective length scaling. To further enhance\nperformance, we introduce two optimized variants: \\textit{PHD-SWA} employs\nsliding window attention to preserve local dependencies, while\n\\textit{PHD-CSWA} implements chunk-wise sliding window attention to eliminate\nlinear growth in pre-filling time. Extensive experiments demonstrate consistent\nimprovements across multiple benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models have demonstrated the effectiveness\nof length scaling during post-training, yet its potential in pre-training\nremains underexplored. We present the Parallel Hidden Decoding Transformer\n(\\textit{PHD}-Transformer), a novel framework that enables efficient length\nscaling during pre-training while maintaining inference efficiency.\n\\textit{PHD}-Transformer achieves this through an innovative KV cache\nmanagement strategy that distinguishes between original tokens and hidden\ndecoding tokens. By retaining only the KV cache of original tokens for\nlong-range dependencies while immediately discarding hidden decoding tokens\nafter use, our approach maintains the same KV cache size as the vanilla\ntransformer while enabling effective length scaling. To further enhance\nperformance, we introduce two optimized variants: \\textit{PHD-SWA} employs\nsliding window attention to preserve local dependencies, while\n\\textit{PHD-CSWA} implements chunk-wise sliding window attention to eliminate\nlinear growth in pre-filling time. Extensive experiments demonstrate consistent\nimprovements across multiple benchmarks."
                },
                "authors": [
                    {
                        "name": "Bohong Wu"
                    },
                    {
                        "name": "Shen Yan"
                    },
                    {
                        "name": "Sijun Zhang"
                    },
                    {
                        "name": "Jianqiao Lu"
                    },
                    {
                        "name": "Yutao Zeng"
                    },
                    {
                        "name": "Ya Wang"
                    },
                    {
                        "name": "Xun Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Xun Zhou"
                },
                "author": "Xun Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14992v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14992v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02441v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02441v2",
                "updated": "2025-04-24T01:47:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    1,
                    47,
                    25,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-03T09:58:19Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    9,
                    58,
                    19,
                    3,
                    93,
                    0
                ],
                "title": "Cognitive Memory in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cognitive Memory in Large Language Models"
                },
                "summary": "This paper examines memory mechanisms in Large Language Models (LLMs),\nemphasizing their importance for context-rich responses, reduced\nhallucinations, and improved efficiency. It categorizes memory into sensory,\nshort-term, and long-term, with sensory memory corresponding to input prompts,\nshort-term memory processing immediate context, and long-term memory\nimplemented via external databases or structures. The text-based memory section\ncovers acquisition (selection and summarization), management (updating,\naccessing, storing, and resolving conflicts), and utilization (full-text\nsearch, SQL queries, semantic search). The KV cache-based memory section\ndiscusses selection methods (regularity-based summarization, score-based\napproaches, special token embeddings) and compression techniques (low-rank\ncompression, KV merging, multimodal compression), along with management\nstrategies like offloading and shared attention mechanisms. Parameter-based\nmemory methods (LoRA, TTT, MoE) transform memories into model parameters to\nenhance efficiency, while hidden-state-based memory approaches (chunk\nmechanisms, recurrent transformers, Mamba model) improve long-text processing\nby combining RNN hidden states with current methods. Overall, the paper offers\na comprehensive analysis of LLM memory mechanisms, highlighting their\nsignificance and future research directions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper examines memory mechanisms in Large Language Models (LLMs),\nemphasizing their importance for context-rich responses, reduced\nhallucinations, and improved efficiency. It categorizes memory into sensory,\nshort-term, and long-term, with sensory memory corresponding to input prompts,\nshort-term memory processing immediate context, and long-term memory\nimplemented via external databases or structures. The text-based memory section\ncovers acquisition (selection and summarization), management (updating,\naccessing, storing, and resolving conflicts), and utilization (full-text\nsearch, SQL queries, semantic search). The KV cache-based memory section\ndiscusses selection methods (regularity-based summarization, score-based\napproaches, special token embeddings) and compression techniques (low-rank\ncompression, KV merging, multimodal compression), along with management\nstrategies like offloading and shared attention mechanisms. Parameter-based\nmemory methods (LoRA, TTT, MoE) transform memories into model parameters to\nenhance efficiency, while hidden-state-based memory approaches (chunk\nmechanisms, recurrent transformers, Mamba model) improve long-text processing\nby combining RNN hidden states with current methods. Overall, the paper offers\na comprehensive analysis of LLM memory mechanisms, highlighting their\nsignificance and future research directions."
                },
                "authors": [
                    {
                        "name": "Lianlei Shan"
                    },
                    {
                        "name": "Shixian Luo"
                    },
                    {
                        "name": "Zezhou Zhu"
                    },
                    {
                        "name": "Yu Yuan"
                    },
                    {
                        "name": "Yong Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Wu"
                },
                "author": "Yong Wu",
                "arxiv_comment": "37 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02441v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02441v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15364v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15364v2",
                "updated": "2025-04-23T18:02:55Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    18,
                    2,
                    55,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-21T18:12:46Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    18,
                    12,
                    46,
                    0,
                    111,
                    0
                ],
                "title": "KeyDiff: Key Similarity-Based KV Cache Eviction for Long-Context LLM\n  Inference in Resource-Constrained Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KeyDiff: Key Similarity-Based KV Cache Eviction for Long-Context LLM\n  Inference in Resource-Constrained Environments"
                },
                "summary": "In this work, we demonstrate that distinctive keys during LLM inference tend\nto have high attention scores. We explore this phenomenon and propose KeyDiff,\na training-free KV cache eviction method based on key similarity. This method\nfacilitates the deployment of LLM-based application requiring long input\nprompts in resource-constrained environments with limited memory and compute\nbudgets. Unlike other KV cache eviction methods, KeyDiff can process\narbitrarily long prompts within strict resource constraints and efficiently\ngenerate responses. We demonstrate that KeyDiff computes the optimal solution\nto a KV cache selection problem that maximizes key diversity, providing a\ntheoretical understanding of KeyDiff. Notably,KeyDiff does not rely on\nattention scores, allowing the use of optimized attention mechanisms like\nFlashAttention. We demonstrate the effectiveness of KeyDiff across diverse\ntasks and models, illustrating a performance gap of less than 0.04\\% with 8K\ncache budget ($\\sim$ 23\\% KV cache reduction) from the non-evicting baseline on\nthe LongBench benchmark for Llama 3.1-8B and Llama 3.2-3B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we demonstrate that distinctive keys during LLM inference tend\nto have high attention scores. We explore this phenomenon and propose KeyDiff,\na training-free KV cache eviction method based on key similarity. This method\nfacilitates the deployment of LLM-based application requiring long input\nprompts in resource-constrained environments with limited memory and compute\nbudgets. Unlike other KV cache eviction methods, KeyDiff can process\narbitrarily long prompts within strict resource constraints and efficiently\ngenerate responses. We demonstrate that KeyDiff computes the optimal solution\nto a KV cache selection problem that maximizes key diversity, providing a\ntheoretical understanding of KeyDiff. Notably,KeyDiff does not rely on\nattention scores, allowing the use of optimized attention mechanisms like\nFlashAttention. We demonstrate the effectiveness of KeyDiff across diverse\ntasks and models, illustrating a performance gap of less than 0.04\\% with 8K\ncache budget ($\\sim$ 23\\% KV cache reduction) from the non-evicting baseline on\nthe LongBench benchmark for Llama 3.1-8B and Llama 3.2-3B."
                },
                "authors": [
                    {
                        "name": "Junyoung Park"
                    },
                    {
                        "name": "Dalton Jones"
                    },
                    {
                        "name": "Matt J Morse"
                    },
                    {
                        "name": "Raghavv Goel"
                    },
                    {
                        "name": "Mingu Lee"
                    },
                    {
                        "name": "Chris Lott"
                    }
                ],
                "author_detail": {
                    "name": "Chris Lott"
                },
                "author": "Chris Lott",
                "arxiv_comment": "8 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15364v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15364v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15437v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15437v2",
                "updated": "2025-04-23T15:02:16Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    15,
                    2,
                    16,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-21T21:01:57Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    21,
                    1,
                    57,
                    0,
                    111,
                    0
                ],
                "title": "Iris: A Next Generation Digital Pathology Rendering Engine",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Iris: A Next Generation Digital Pathology Rendering Engine"
                },
                "summary": "Digital pathology is a tool of rapidly evolving importance within the\ndiscipline of pathology. Whole slide imaging promises numerous advantages;\nhowever, adoption is limited by challenges in ease of use and speed of\nhigh-quality image rendering relative to the simplicity and visual quality of\nglass slides. We introduce Iris, a new high-performance digital pathology\nrendering system. Specifically, we outline and detail the performance metrics\nof Iris Core, the core rendering engine technology. Iris Core comprises machine\ncode modules written from the ground up in C++ and using Vulkan, a low-level\nand low-overhead cross-platform graphical processing unit application program\ninterface, and our novel rapid tile buffering algorithms. We provide a detailed\nexplanation of Iris Core's system architecture, including the stateless\nisolation of core processes, interprocess communication paradigms, and explicit\nsynchronization paradigms that provide powerful control over the graphical\nprocessing unit. Iris Core achieves slide rendering at the sustained maximum\nframe rate on all tested platforms and buffers an entire new slide field of,\nview without overlapping pixels, in 10 ms with enhanced detail in 30 ms. It is\nable to buffer and compute high-fidelity reduction-enhancements for viewing\nlow-power cytology with increased visual quality at a rate of 100-160 us per\nslide tile, and with a cumulative median buffering rate of 1.36 GB of\ndecompressed image data per second. This buffering rate allows for an entirely\nnew field of view to be fully buffered and rendered in less than a single\nmonitor refresh on a standard display, and high detail features within 2-3\nmonitor refresh frames. These metrics far exceed previously published\nspecifications, beyond an order of magnitude in some contexts. The system shows\nno slowing with high use loads, but rather increases performance due to cache\nmechanisms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital pathology is a tool of rapidly evolving importance within the\ndiscipline of pathology. Whole slide imaging promises numerous advantages;\nhowever, adoption is limited by challenges in ease of use and speed of\nhigh-quality image rendering relative to the simplicity and visual quality of\nglass slides. We introduce Iris, a new high-performance digital pathology\nrendering system. Specifically, we outline and detail the performance metrics\nof Iris Core, the core rendering engine technology. Iris Core comprises machine\ncode modules written from the ground up in C++ and using Vulkan, a low-level\nand low-overhead cross-platform graphical processing unit application program\ninterface, and our novel rapid tile buffering algorithms. We provide a detailed\nexplanation of Iris Core's system architecture, including the stateless\nisolation of core processes, interprocess communication paradigms, and explicit\nsynchronization paradigms that provide powerful control over the graphical\nprocessing unit. Iris Core achieves slide rendering at the sustained maximum\nframe rate on all tested platforms and buffers an entire new slide field of,\nview without overlapping pixels, in 10 ms with enhanced detail in 30 ms. It is\nable to buffer and compute high-fidelity reduction-enhancements for viewing\nlow-power cytology with increased visual quality at a rate of 100-160 us per\nslide tile, and with a cumulative median buffering rate of 1.36 GB of\ndecompressed image data per second. This buffering rate allows for an entirely\nnew field of view to be fully buffered and rendered in less than a single\nmonitor refresh on a standard display, and high detail features within 2-3\nmonitor refresh frames. These metrics far exceed previously published\nspecifications, beyond an order of magnitude in some contexts. The system shows\nno slowing with high use loads, but rather increases performance due to cache\nmechanisms."
                },
                "authors": [
                    {
                        "name": "Ryan Erik Landvater"
                    },
                    {
                        "name": "Ulysses Balis"
                    }
                ],
                "author_detail": {
                    "name": "Ulysses Balis"
                },
                "author": "Ulysses Balis",
                "arxiv_doi": "10.1016/j.jpi.2024.100414",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.jpi.2024.100414",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.15437v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15437v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "11 pages, 8 figures",
                "arxiv_journal_ref": "Journal of Pathology Informatics, 16, 100414 (2025)",
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10138v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10138v2",
                "updated": "2025-04-23T10:48:52Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    10,
                    48,
                    52,
                    2,
                    113,
                    0
                ],
                "published": "2025-01-17T12:01:28Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    12,
                    1,
                    28,
                    4,
                    17,
                    0
                ],
                "title": "The NIC should be part of the OS",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The NIC should be part of the OS"
                },
                "summary": "The network interface adapter (NIC) is a critical component of a cloud server\noccupying a unique position. Not only is network performance vital to efficient\noperation of the machine, but unlike compute accelerators like GPUs, the\nnetwork subsystem must react to unpredictable events like the arrival of a\nnetwork packet and communicate with the appropriate application end point with\nminimal latency.\n  Current approaches to server stacks navigate a trade-off between flexibility,\nefficiency, and performance: the fastest kernel-bypass approaches dedicate\ncores to applications, busy-wait on receive queues, etc. while more flexible\napproaches appropriate to more dynamic workload mixes incur much greater\nsoftware overhead on the data path.\n  However, we reject this trade-off, which we ascribe to an arbitrary (and\nsub-optimal) split in system state between the OS and the NIC. Instead, by\nexploiting the properties of cache-coherent interconnects and integrating the\nNIC closely with the OS kernel, we can achieve something surprising:\nperformance for RPC workloads better than the fastest kernelbypass approaches\nwithout sacrificing the robustness and dynamic adaptation of kernel-based\nnetwork subsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The network interface adapter (NIC) is a critical component of a cloud server\noccupying a unique position. Not only is network performance vital to efficient\noperation of the machine, but unlike compute accelerators like GPUs, the\nnetwork subsystem must react to unpredictable events like the arrival of a\nnetwork packet and communicate with the appropriate application end point with\nminimal latency.\n  Current approaches to server stacks navigate a trade-off between flexibility,\nefficiency, and performance: the fastest kernel-bypass approaches dedicate\ncores to applications, busy-wait on receive queues, etc. while more flexible\napproaches appropriate to more dynamic workload mixes incur much greater\nsoftware overhead on the data path.\n  However, we reject this trade-off, which we ascribe to an arbitrary (and\nsub-optimal) split in system state between the OS and the NIC. Instead, by\nexploiting the properties of cache-coherent interconnects and integrating the\nNIC closely with the OS kernel, we can achieve something surprising:\nperformance for RPC workloads better than the fastest kernelbypass approaches\nwithout sacrificing the robustness and dynamic adaptation of kernel-based\nnetwork subsystems."
                },
                "authors": [
                    {
                        "name": "Pengcheng Xu"
                    },
                    {
                        "name": "Timothy Roscoe"
                    }
                ],
                "author_detail": {
                    "name": "Timothy Roscoe"
                },
                "author": "Timothy Roscoe",
                "arxiv_doi": "10.1145/3713082.3730388",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3713082.3730388",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.10138v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10138v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Camera ready for HotOS'25",
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14051v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14051v2",
                "updated": "2025-04-23T05:04:58Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    5,
                    4,
                    58,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-18T19:46:54Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    19,
                    46,
                    54,
                    4,
                    108,
                    0
                ],
                "title": "CAOTE: KV Caching through Attention Output Error based Token Eviction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAOTE: KV Caching through Attention Output Error based Token Eviction"
                },
                "summary": "While long context support of large language models has extended their\nabilities, it also incurs challenges in memory and compute which becomes\ncrucial bottlenecks in resource-restricted devices. Token eviction, a widely\nadopted post-training methodology designed to alleviate the bottlenecks by\nevicting less important tokens from the cache, typically uses attention scores\nas proxy metrics for token importance. However, one major limitation of\nattention score as a token-wise importance metrics is that it lacks the\ninformation about contribution of tokens to the attention output. In this\npaper, we propose a simple eviction criterion based on the contribution of\ncached tokens to attention outputs. Our method, CAOTE, optimizes for eviction\nerror due to token eviction, by seamlessly integrating attention scores and\nvalue vectors. This is the first method which uses value vector information on\ntop of attention-based eviction scores. Additionally, CAOTE can act as a\nmeta-heuristic method with flexible usage with any token eviction method. We\nshow that CAOTE, when combined with the state-of-the-art attention score-based\nmethods, always improves accuracies on the downstream task, indicating the\nimportance of leveraging information from values during token eviction process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While long context support of large language models has extended their\nabilities, it also incurs challenges in memory and compute which becomes\ncrucial bottlenecks in resource-restricted devices. Token eviction, a widely\nadopted post-training methodology designed to alleviate the bottlenecks by\nevicting less important tokens from the cache, typically uses attention scores\nas proxy metrics for token importance. However, one major limitation of\nattention score as a token-wise importance metrics is that it lacks the\ninformation about contribution of tokens to the attention output. In this\npaper, we propose a simple eviction criterion based on the contribution of\ncached tokens to attention outputs. Our method, CAOTE, optimizes for eviction\nerror due to token eviction, by seamlessly integrating attention scores and\nvalue vectors. This is the first method which uses value vector information on\ntop of attention-based eviction scores. Additionally, CAOTE can act as a\nmeta-heuristic method with flexible usage with any token eviction method. We\nshow that CAOTE, when combined with the state-of-the-art attention score-based\nmethods, always improves accuracies on the downstream task, indicating the\nimportance of leveraging information from values during token eviction process."
                },
                "authors": [
                    {
                        "name": "Raghavv Goel"
                    },
                    {
                        "name": "Junyoung Park"
                    },
                    {
                        "name": "Mukul Gagrani"
                    },
                    {
                        "name": "Dalton Jones"
                    },
                    {
                        "name": "Matthew Morse"
                    },
                    {
                        "name": "Harper Langston"
                    },
                    {
                        "name": "Mingu Lee"
                    },
                    {
                        "name": "Chris Lott"
                    }
                ],
                "author_detail": {
                    "name": "Chris Lott"
                },
                "author": "Chris Lott",
                "arxiv_comment": "14 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14051v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14051v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2505.09614v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09614v1",
                "updated": "2025-05-14T17:59:35Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    17,
                    59,
                    35,
                    2,
                    134,
                    0
                ],
                "published": "2025-05-14T17:59:35Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    17,
                    59,
                    35,
                    2,
                    134,
                    0
                ],
                "title": "Language Agents Mirror Human Causal Reasoning Biases. How Can We Help\n  Them Think Like Scientists?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Agents Mirror Human Causal Reasoning Biases. How Can We Help\n  Them Think Like Scientists?"
                },
                "summary": "Language model (LM) agents are increasingly used as autonomous\ndecision-makers who need to actively gather information to guide their\ndecisions. A crucial cognitive skill for such agents is the efficient\nexploration and understanding of the causal structure of the world -- key to\nrobust, scientifically grounded reasoning. Yet, it remains unclear whether LMs\npossess this capability or exhibit systematic biases leading to erroneous\nconclusions. In this work, we examine LMs' ability to explore and infer causal\nrelationships, using the well-established \"Blicket Test\" paradigm from\ndevelopmental psychology. We find that LMs reliably infer the common, intuitive\ndisjunctive causal relationships but systematically struggle with the unusual,\nyet equally (or sometimes even more) evidenced conjunctive ones. This\n\"disjunctive bias\" persists across model families, sizes, and prompting\nstrategies, and performance further declines as task complexity increases.\nInterestingly, an analogous bias appears in human adults, suggesting that LMs\nmay have inherited deep-seated reasoning heuristics from their training data.\nTo this end, we quantify similarities between LMs and humans, finding that LMs\nexhibit adult-like inference profiles (but not children-like). Finally, we\npropose a test-time sampling method which explicitly samples and eliminates\nhypotheses about causal relationships from the LM. This scalable approach\nsignificantly reduces the disjunctive bias and moves LMs closer to the goal of\nscientific, causally rigorous reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language model (LM) agents are increasingly used as autonomous\ndecision-makers who need to actively gather information to guide their\ndecisions. A crucial cognitive skill for such agents is the efficient\nexploration and understanding of the causal structure of the world -- key to\nrobust, scientifically grounded reasoning. Yet, it remains unclear whether LMs\npossess this capability or exhibit systematic biases leading to erroneous\nconclusions. In this work, we examine LMs' ability to explore and infer causal\nrelationships, using the well-established \"Blicket Test\" paradigm from\ndevelopmental psychology. We find that LMs reliably infer the common, intuitive\ndisjunctive causal relationships but systematically struggle with the unusual,\nyet equally (or sometimes even more) evidenced conjunctive ones. This\n\"disjunctive bias\" persists across model families, sizes, and prompting\nstrategies, and performance further declines as task complexity increases.\nInterestingly, an analogous bias appears in human adults, suggesting that LMs\nmay have inherited deep-seated reasoning heuristics from their training data.\nTo this end, we quantify similarities between LMs and humans, finding that LMs\nexhibit adult-like inference profiles (but not children-like). Finally, we\npropose a test-time sampling method which explicitly samples and eliminates\nhypotheses about causal relationships from the LM. This scalable approach\nsignificantly reduces the disjunctive bias and moves LMs closer to the goal of\nscientific, causally rigorous reasoning."
                },
                "authors": [
                    {
                        "name": "Anthony GX-Chen"
                    },
                    {
                        "name": "Dongyan Lin"
                    },
                    {
                        "name": "Mandana Samiei"
                    },
                    {
                        "name": "Doina Precup"
                    },
                    {
                        "name": "Blake A. Richards"
                    },
                    {
                        "name": "Rob Fergus"
                    },
                    {
                        "name": "Kenneth Marino"
                    }
                ],
                "author_detail": {
                    "name": "Kenneth Marino"
                },
                "author": "Kenneth Marino",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09614v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09614v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09612v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09612v1",
                "updated": "2025-05-14T17:59:17Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    17,
                    59,
                    17,
                    2,
                    134,
                    0
                ],
                "published": "2025-05-14T17:59:17Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    17,
                    59,
                    17,
                    2,
                    134,
                    0
                ],
                "title": "Adaptively-weighted Nearest Neighbors for Matrix Completion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptively-weighted Nearest Neighbors for Matrix Completion"
                },
                "summary": "In this technical note, we introduce and analyze AWNN: an adaptively weighted\nnearest neighbor method for performing matrix completion. Nearest neighbor (NN)\nmethods are widely used in missing data problems across multiple disciplines\nsuch as in recommender systems and for performing counterfactual inference in\npanel data settings. Prior works have shown that in addition to being very\nintuitive and easy to implement, NN methods enjoy nice theoretical guarantees.\nHowever, the performance of majority of the NN methods rely on the appropriate\nchoice of the radii and the weights assigned to each member in the nearest\nneighbor set and despite several works on nearest neighbor methods in the past\ntwo decades, there does not exist a systematic approach of choosing the radii\nand the weights without relying on methods like cross-validation. AWNN\naddresses this challenge by judiciously balancing the bias variance trade off\ninherent in weighted nearest-neighbor regression. We provide theoretical\nguarantees for the proposed method under minimal assumptions and support the\ntheory via synthetic experiments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this technical note, we introduce and analyze AWNN: an adaptively weighted\nnearest neighbor method for performing matrix completion. Nearest neighbor (NN)\nmethods are widely used in missing data problems across multiple disciplines\nsuch as in recommender systems and for performing counterfactual inference in\npanel data settings. Prior works have shown that in addition to being very\nintuitive and easy to implement, NN methods enjoy nice theoretical guarantees.\nHowever, the performance of majority of the NN methods rely on the appropriate\nchoice of the radii and the weights assigned to each member in the nearest\nneighbor set and despite several works on nearest neighbor methods in the past\ntwo decades, there does not exist a systematic approach of choosing the radii\nand the weights without relying on methods like cross-validation. AWNN\naddresses this challenge by judiciously balancing the bias variance trade off\ninherent in weighted nearest-neighbor regression. We provide theoretical\nguarantees for the proposed method under minimal assumptions and support the\ntheory via synthetic experiments."
                },
                "authors": [
                    {
                        "name": "Tathagata Sadhukhan"
                    },
                    {
                        "name": "Manit Paul"
                    },
                    {
                        "name": "Raaz Dwivedi"
                    }
                ],
                "author_detail": {
                    "name": "Raaz Dwivedi"
                },
                "author": "Raaz Dwivedi",
                "arxiv_comment": "25 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09612v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09612v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09610v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09610v1",
                "updated": "2025-05-14T17:58:40Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    17,
                    58,
                    40,
                    2,
                    134,
                    0
                ],
                "published": "2025-05-14T17:58:40Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    17,
                    58,
                    40,
                    2,
                    134,
                    0
                ],
                "title": "Customizing a Large Language Model for VHDL Design of High-Performance\n  Microprocessors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Customizing a Large Language Model for VHDL Design of High-Performance\n  Microprocessors"
                },
                "summary": "The use of Large Language Models (LLMs) in hardware design has taken off in\nrecent years, principally through its incorporation in tools that increase chip\ndesigner productivity. There has been considerable discussion about the use of\nLLMs in RTL specifications of chip designs, for which the two most popular\nlanguages are Verilog and VHDL. LLMs and their use in Verilog design has\nreceived significant attention due to the higher popularity of the language,\nbut little attention so far has been given to VHDL despite its continued\npopularity in the industry. There has also been little discussion about the\nunique needs of organizations that engage in high-performance processor design,\nand techniques to deploy AI solutions in these settings. In this paper, we\ndescribe our journey in developing a Large Language Model (LLM) specifically\nfor the purpose of explaining VHDL code, a task that has particular importance\nin an organization with decades of experience and assets in high-performance\nprocessor design. We show how we developed test sets specific to our needs and\nused them for evaluating models as we performed extended pretraining (EPT) of a\nbase LLM. Expert evaluation of the code explanations produced by the EPT model\nincreased to 69% compared to a base model rating of 43%. We further show how we\ndeveloped an LLM-as-a-judge to gauge models similar to expert evaluators. This\nled us to deriving and evaluating a host of new models, including an\ninstruction-tuned version of the EPT model with an expected expert evaluator\nrating of 71%. Our experiments also indicate that with the potential use of\nnewer base models, this rating can be pushed to 85% and beyond. We conclude\nwith a discussion on further improving the quality of hardware design LLMs\nusing exciting new developments in the Generative AI world.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The use of Large Language Models (LLMs) in hardware design has taken off in\nrecent years, principally through its incorporation in tools that increase chip\ndesigner productivity. There has been considerable discussion about the use of\nLLMs in RTL specifications of chip designs, for which the two most popular\nlanguages are Verilog and VHDL. LLMs and their use in Verilog design has\nreceived significant attention due to the higher popularity of the language,\nbut little attention so far has been given to VHDL despite its continued\npopularity in the industry. There has also been little discussion about the\nunique needs of organizations that engage in high-performance processor design,\nand techniques to deploy AI solutions in these settings. In this paper, we\ndescribe our journey in developing a Large Language Model (LLM) specifically\nfor the purpose of explaining VHDL code, a task that has particular importance\nin an organization with decades of experience and assets in high-performance\nprocessor design. We show how we developed test sets specific to our needs and\nused them for evaluating models as we performed extended pretraining (EPT) of a\nbase LLM. Expert evaluation of the code explanations produced by the EPT model\nincreased to 69% compared to a base model rating of 43%. We further show how we\ndeveloped an LLM-as-a-judge to gauge models similar to expert evaluators. This\nled us to deriving and evaluating a host of new models, including an\ninstruction-tuned version of the EPT model with an expected expert evaluator\nrating of 71%. Our experiments also indicate that with the potential use of\nnewer base models, this rating can be pushed to 85% and beyond. We conclude\nwith a discussion on further improving the quality of hardware design LLMs\nusing exciting new developments in the Generative AI world."
                },
                "authors": [
                    {
                        "name": "Nicolas Dupuis"
                    },
                    {
                        "name": "Ravi Nair"
                    },
                    {
                        "name": "Shyam Ramji"
                    },
                    {
                        "name": "Sean McClintock"
                    },
                    {
                        "name": "Nishant Chauhan"
                    },
                    {
                        "name": "Priyanka Nagpal"
                    },
                    {
                        "name": "Bart Blaner"
                    },
                    {
                        "name": "Ken Valk"
                    },
                    {
                        "name": "Leon Stok"
                    },
                    {
                        "name": "Ruchir Puri"
                    }
                ],
                "author_detail": {
                    "name": "Ruchir Puri"
                },
                "author": "Ruchir Puri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09610v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09610v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09608v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09608v1",
                "updated": "2025-05-14T17:57:27Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    17,
                    57,
                    27,
                    2,
                    134,
                    0
                ],
                "published": "2025-05-14T17:57:27Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    17,
                    57,
                    27,
                    2,
                    134,
                    0
                ],
                "title": "LightLab: Controlling Light Sources in Images with Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LightLab: Controlling Light Sources in Images with Diffusion Models"
                },
                "summary": "We present a simple, yet effective diffusion-based method for fine-grained,\nparametric control over light sources in an image. Existing relighting methods\neither rely on multiple input views to perform inverse rendering at inference\ntime, or fail to provide explicit control over light changes. Our method\nfine-tunes a diffusion model on a small set of real raw photograph pairs,\nsupplemented by synthetically rendered images at scale, to elicit its\nphotorealistic prior for relighting. We leverage the linearity of light to\nsynthesize image pairs depicting controlled light changes of either a target\nlight source or ambient illumination. Using this data and an appropriate\nfine-tuning scheme, we train a model for precise illumination changes with\nexplicit control over light intensity and color. Lastly, we show how our method\ncan achieve compelling light editing results, and outperforms existing methods\nbased on user preference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a simple, yet effective diffusion-based method for fine-grained,\nparametric control over light sources in an image. Existing relighting methods\neither rely on multiple input views to perform inverse rendering at inference\ntime, or fail to provide explicit control over light changes. Our method\nfine-tunes a diffusion model on a small set of real raw photograph pairs,\nsupplemented by synthetically rendered images at scale, to elicit its\nphotorealistic prior for relighting. We leverage the linearity of light to\nsynthesize image pairs depicting controlled light changes of either a target\nlight source or ambient illumination. Using this data and an appropriate\nfine-tuning scheme, we train a model for precise illumination changes with\nexplicit control over light intensity and color. Lastly, we show how our method\ncan achieve compelling light editing results, and outperforms existing methods\nbased on user preference."
                },
                "authors": [
                    {
                        "name": "Nadav Magar"
                    },
                    {
                        "name": "Amir Hertz"
                    },
                    {
                        "name": "Eric Tabellion"
                    },
                    {
                        "name": "Yael Pritch"
                    },
                    {
                        "name": "Alex Rav-Acha"
                    },
                    {
                        "name": "Ariel Shamir"
                    },
                    {
                        "name": "Yedid Hoshen"
                    }
                ],
                "author_detail": {
                    "name": "Yedid Hoshen"
                },
                "author": "Yedid Hoshen",
                "arxiv_doi": "10.1145/3721238.3730696",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3721238.3730696",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2505.09608v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09608v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Project Page: https://nadmag.github.io/LightLab/",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09602v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09602v1",
                "updated": "2025-05-14T17:52:10Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    17,
                    52,
                    10,
                    2,
                    134,
                    0
                ],
                "published": "2025-05-14T17:52:10Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    17,
                    52,
                    10,
                    2,
                    134,
                    0
                ],
                "title": "Adversarial Suffix Filtering: a Defense Pipeline for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adversarial Suffix Filtering: a Defense Pipeline for LLMs"
                },
                "summary": "Large Language Models (LLMs) are increasingly embedded in autonomous systems\nand public-facing environments, yet they remain susceptible to jailbreak\nvulnerabilities that may undermine their security and trustworthiness.\nAdversarial suffixes are considered to be the current state-of-the-art\njailbreak, consistently outperforming simpler methods and frequently succeeding\neven in black-box settings. Existing defenses rely on access to the internal\narchitecture of models limiting diverse deployment, increase memory and\ncomputation footprints dramatically, or can be bypassed with simple prompt\nengineering methods. We introduce $\\textbf{Adversarial Suffix Filtering}$\n(ASF), a lightweight novel model-agnostic defensive pipeline designed to\nprotect LLMs against adversarial suffix attacks. ASF functions as an input\npreprocessor and sanitizer that detects and filters adversarially crafted\nsuffixes in prompts, effectively neutralizing malicious injections. We\ndemonstrate that ASF provides comprehensive defense capabilities across both\nblack-box and white-box attack settings, reducing the attack efficacy of\nstate-of-the-art adversarial suffix generation methods to below 4%, while only\nminimally affecting the target model's capabilities in non-adversarial\nscenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly embedded in autonomous systems\nand public-facing environments, yet they remain susceptible to jailbreak\nvulnerabilities that may undermine their security and trustworthiness.\nAdversarial suffixes are considered to be the current state-of-the-art\njailbreak, consistently outperforming simpler methods and frequently succeeding\neven in black-box settings. Existing defenses rely on access to the internal\narchitecture of models limiting diverse deployment, increase memory and\ncomputation footprints dramatically, or can be bypassed with simple prompt\nengineering methods. We introduce $\\textbf{Adversarial Suffix Filtering}$\n(ASF), a lightweight novel model-agnostic defensive pipeline designed to\nprotect LLMs against adversarial suffix attacks. ASF functions as an input\npreprocessor and sanitizer that detects and filters adversarially crafted\nsuffixes in prompts, effectively neutralizing malicious injections. We\ndemonstrate that ASF provides comprehensive defense capabilities across both\nblack-box and white-box attack settings, reducing the attack efficacy of\nstate-of-the-art adversarial suffix generation methods to below 4%, while only\nminimally affecting the target model's capabilities in non-adversarial\nscenarios."
                },
                "authors": [
                    {
                        "name": "David Khachaturov"
                    },
                    {
                        "name": "Robert Mullins"
                    }
                ],
                "author_detail": {
                    "name": "Robert Mullins"
                },
                "author": "Robert Mullins",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09602v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09602v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09600v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09600v1",
                "updated": "2025-05-14T17:50:28Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    17,
                    50,
                    28,
                    2,
                    134,
                    0
                ],
                "published": "2025-05-14T17:50:28Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    17,
                    50,
                    28,
                    2,
                    134,
                    0
                ],
                "title": "Accelerating the time-domain LISA response model with central finite\n  differences and hybridization techniques",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating the time-domain LISA response model with central finite\n  differences and hybridization techniques"
                },
                "summary": "Accurate and efficient modeling of the Laser Interferometer Space Antenna\n(LISA) response is crucial for gravitational-wave (GW) data analysis. A key\ncomputational challenge lies in evaluating time-delay interferometry (TDI)\nvariables, which require projecting GW polarizations onto the LISA arms at\ndifferent retarded times. Without approximations, the full LISA response is\ncomputationally expensive, and traditional approaches, such as the\nlong-wavelength approximation, accelerate the response calculation at the cost\nof reducing accuracy at high frequencies. In this work, we introduce a novel\nhybrid time-domain response for LISA that balances computational efficiency and\naccuracy across the binary's evolution. Our method implements a fast\nlow-frequency approximation during the early inspiral$\\unicode{x2013}$where\nmost binaries spend most of the time in the sensitive frequency band of\nLISA$\\unicode{x2013}$while reserving the computationally intensive\nfull-response calculations for the late inspiral, merger, and ringdown phases.\nThe low-frequency approximation (LFA) is based on Taylor expanding the response\nquantities around a chosen evaluation time such that time delays correspond to\ncentral finite differences. Our hybrid approach supports CPU and GPU\nimplementations, TDI generations 1.5 and 2.0, and flexible time-delay\ncomplexity, and has the potential to accelerate parts of the global fit and\nreduce energy consumption. We also test our LFA and hybrid responses on\neccentric binaries, and we perform parameter estimation for a \"golden\" binary.\nAdditionally, we assess the efficacy of our low-frequency response for \"deep\nalerts\" by performing inspiral-only Bayesian inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate and efficient modeling of the Laser Interferometer Space Antenna\n(LISA) response is crucial for gravitational-wave (GW) data analysis. A key\ncomputational challenge lies in evaluating time-delay interferometry (TDI)\nvariables, which require projecting GW polarizations onto the LISA arms at\ndifferent retarded times. Without approximations, the full LISA response is\ncomputationally expensive, and traditional approaches, such as the\nlong-wavelength approximation, accelerate the response calculation at the cost\nof reducing accuracy at high frequencies. In this work, we introduce a novel\nhybrid time-domain response for LISA that balances computational efficiency and\naccuracy across the binary's evolution. Our method implements a fast\nlow-frequency approximation during the early inspiral$\\unicode{x2013}$where\nmost binaries spend most of the time in the sensitive frequency band of\nLISA$\\unicode{x2013}$while reserving the computationally intensive\nfull-response calculations for the late inspiral, merger, and ringdown phases.\nThe low-frequency approximation (LFA) is based on Taylor expanding the response\nquantities around a chosen evaluation time such that time delays correspond to\ncentral finite differences. Our hybrid approach supports CPU and GPU\nimplementations, TDI generations 1.5 and 2.0, and flexible time-delay\ncomplexity, and has the potential to accelerate parts of the global fit and\nreduce energy consumption. We also test our LFA and hybrid responses on\neccentric binaries, and we perform parameter estimation for a \"golden\" binary.\nAdditionally, we assess the efficacy of our low-frequency response for \"deep\nalerts\" by performing inspiral-only Bayesian inference."
                },
                "authors": [
                    {
                        "name": "Jorge Valencia"
                    },
                    {
                        "name": "Sascha Husa"
                    }
                ],
                "author_detail": {
                    "name": "Sascha Husa"
                },
                "author": "Sascha Husa",
                "arxiv_comment": "46 pages, 16 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09600v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09600v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09598v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09598v1",
                "updated": "2025-05-14T17:47:00Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    17,
                    47,
                    0,
                    2,
                    134,
                    0
                ],
                "published": "2025-05-14T17:47:00Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    17,
                    47,
                    0,
                    2,
                    134,
                    0
                ],
                "title": "How Hungry is AI? Benchmarking Energy, Water, and Carbon Footprint of\n  LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Hungry is AI? Benchmarking Energy, Water, and Carbon Footprint of\n  LLM Inference"
                },
                "summary": "As large language models (LLMs) spread across industries, understanding their\nenvironmental footprint at the inference level is no longer optional; it is\nessential. However, most existing studies exclude proprietary models, overlook\ninfrastructural variability and overhead, or focus solely on training, even as\ninference increasingly dominates AI's environmental impact. To bridge this gap,\nthis paper introduces a novel infrastructure-aware benchmarking framework for\nquantifying the environmental footprint of LLM inference across 30\nstate-of-the-art models as deployed in commercial data centers. Our framework\ncombines public API performance data with region-specific environmental\nmultipliers and statistical inference of hardware configurations. We\nadditionally utilize cross-efficiency Data Envelopment Analysis (DEA) to rank\nmodels by performance relative to environmental cost. Our results show that o3\nand DeepSeek-R1 emerge as the most energy-intensive models, consuming over 33\nWh per long prompt, more than 70 times the consumption of GPT-4.1 nano, and\nthat Claude-3.7 Sonnet ranks highest in eco-efficiency. While a single short\nGPT-4o query consumes 0.43 Wh, scaling this to 700 million queries/day results\nin substantial annual environmental impacts. These include electricity use\ncomparable to 35,000 U.S. homes, freshwater evaporation matching the annual\ndrinking needs of 1.2 million people, and carbon emissions requiring a\nChicago-sized forest to offset. These findings illustrate a growing paradox:\nalthough individual queries are efficient, their global scale drives\ndisproportionate resource consumption. Our study provides a standardized,\nempirically grounded methodology for benchmarking the sustainability of LLM\ndeployments, laying a foundation for future environmental accountability in AI\ndevelopment and sustainability standards.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) spread across industries, understanding their\nenvironmental footprint at the inference level is no longer optional; it is\nessential. However, most existing studies exclude proprietary models, overlook\ninfrastructural variability and overhead, or focus solely on training, even as\ninference increasingly dominates AI's environmental impact. To bridge this gap,\nthis paper introduces a novel infrastructure-aware benchmarking framework for\nquantifying the environmental footprint of LLM inference across 30\nstate-of-the-art models as deployed in commercial data centers. Our framework\ncombines public API performance data with region-specific environmental\nmultipliers and statistical inference of hardware configurations. We\nadditionally utilize cross-efficiency Data Envelopment Analysis (DEA) to rank\nmodels by performance relative to environmental cost. Our results show that o3\nand DeepSeek-R1 emerge as the most energy-intensive models, consuming over 33\nWh per long prompt, more than 70 times the consumption of GPT-4.1 nano, and\nthat Claude-3.7 Sonnet ranks highest in eco-efficiency. While a single short\nGPT-4o query consumes 0.43 Wh, scaling this to 700 million queries/day results\nin substantial annual environmental impacts. These include electricity use\ncomparable to 35,000 U.S. homes, freshwater evaporation matching the annual\ndrinking needs of 1.2 million people, and carbon emissions requiring a\nChicago-sized forest to offset. These findings illustrate a growing paradox:\nalthough individual queries are efficient, their global scale drives\ndisproportionate resource consumption. Our study provides a standardized,\nempirically grounded methodology for benchmarking the sustainability of LLM\ndeployments, laying a foundation for future environmental accountability in AI\ndevelopment and sustainability standards."
                },
                "authors": [
                    {
                        "name": "Nidhal Jegham"
                    },
                    {
                        "name": "Marwen Abdelatti"
                    },
                    {
                        "name": "Lassad Elmoubarki"
                    },
                    {
                        "name": "Abdeltawab Hendawi"
                    }
                ],
                "author_detail": {
                    "name": "Abdeltawab Hendawi"
                },
                "author": "Abdeltawab Hendawi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09598v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09598v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09595v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09595v1",
                "updated": "2025-05-14T17:43:40Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    17,
                    43,
                    40,
                    2,
                    134,
                    0
                ],
                "published": "2025-05-14T17:43:40Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    17,
                    43,
                    40,
                    2,
                    134,
                    0
                ],
                "title": "WorldView-Bench: A Benchmark for Evaluating Global Cultural Perspectives\n  in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WorldView-Bench: A Benchmark for Evaluating Global Cultural Perspectives\n  in Large Language Models"
                },
                "summary": "Large Language Models (LLMs) are predominantly trained and aligned in ways\nthat reinforce Western-centric epistemologies and socio-cultural norms, leading\nto cultural homogenization and limiting their ability to reflect global\ncivilizational plurality. Existing benchmarking frameworks fail to adequately\ncapture this bias, as they rely on rigid, closed-form assessments that overlook\nthe complexity of cultural inclusivity. To address this, we introduce\nWorldView-Bench, a benchmark designed to evaluate Global Cultural Inclusivity\n(GCI) in LLMs by analyzing their ability to accommodate diverse worldviews. Our\napproach is grounded in the Multiplex Worldview proposed by Senturk et al.,\nwhich distinguishes between Uniplex models, reinforcing cultural\nhomogenization, and Multiplex models, which integrate diverse perspectives.\nWorldView-Bench measures Cultural Polarization, the exclusion of alternative\nperspectives, through free-form generative evaluation rather than conventional\ncategorical benchmarks. We implement applied multiplexity through two\nintervention strategies: (1) Contextually-Implemented Multiplex LLMs, where\nsystem prompts embed multiplexity principles, and (2) Multi-Agent System\n(MAS)-Implemented Multiplex LLMs, where multiple LLM agents representing\ndistinct cultural perspectives collaboratively generate responses. Our results\ndemonstrate a significant increase in Perspectives Distribution Score (PDS)\nentropy from 13% at baseline to 94% with MAS-Implemented Multiplex LLMs,\nalongside a shift toward positive sentiment (67.7%) and enhanced cultural\nbalance. These findings highlight the potential of multiplex-aware AI\nevaluation in mitigating cultural bias in LLMs, paving the way for more\ninclusive and ethically aligned AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are predominantly trained and aligned in ways\nthat reinforce Western-centric epistemologies and socio-cultural norms, leading\nto cultural homogenization and limiting their ability to reflect global\ncivilizational plurality. Existing benchmarking frameworks fail to adequately\ncapture this bias, as they rely on rigid, closed-form assessments that overlook\nthe complexity of cultural inclusivity. To address this, we introduce\nWorldView-Bench, a benchmark designed to evaluate Global Cultural Inclusivity\n(GCI) in LLMs by analyzing their ability to accommodate diverse worldviews. Our\napproach is grounded in the Multiplex Worldview proposed by Senturk et al.,\nwhich distinguishes between Uniplex models, reinforcing cultural\nhomogenization, and Multiplex models, which integrate diverse perspectives.\nWorldView-Bench measures Cultural Polarization, the exclusion of alternative\nperspectives, through free-form generative evaluation rather than conventional\ncategorical benchmarks. We implement applied multiplexity through two\nintervention strategies: (1) Contextually-Implemented Multiplex LLMs, where\nsystem prompts embed multiplexity principles, and (2) Multi-Agent System\n(MAS)-Implemented Multiplex LLMs, where multiple LLM agents representing\ndistinct cultural perspectives collaboratively generate responses. Our results\ndemonstrate a significant increase in Perspectives Distribution Score (PDS)\nentropy from 13% at baseline to 94% with MAS-Implemented Multiplex LLMs,\nalongside a shift toward positive sentiment (67.7%) and enhanced cultural\nbalance. These findings highlight the potential of multiplex-aware AI\nevaluation in mitigating cultural bias in LLMs, paving the way for more\ninclusive and ethically aligned AI systems."
                },
                "authors": [
                    {
                        "name": "Abdullah Mushtaq"
                    },
                    {
                        "name": "Imran Taj"
                    },
                    {
                        "name": "Rafay Naeem"
                    },
                    {
                        "name": "Ibrahim Ghaznavi"
                    },
                    {
                        "name": "Junaid Qadir"
                    }
                ],
                "author_detail": {
                    "name": "Junaid Qadir"
                },
                "author": "Junaid Qadir",
                "arxiv_comment": "Preprint. Submitted to the Journal of Artificial Intelligence\n  Research (JAIR) on April 29, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09595v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09595v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09583v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09583v2",
                "updated": "2025-05-15T14:03:10Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    14,
                    3,
                    10,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-14T17:31:16Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    17,
                    31,
                    16,
                    2,
                    134,
                    0
                ],
                "title": "Beyond Likes: How Normative Feedback Complements Engagement Signals on\n  Social Media",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Likes: How Normative Feedback Complements Engagement Signals on\n  Social Media"
                },
                "summary": "Many online platforms incorporate engagement signals--such as likes and\nupvotes--into their content ranking systems and interface design. These signals\nare designed to boost user engagement. However, they can unintentionally\nelevate content that is less inclusive and may not support normatively\ndesirable behavior. This issue becomes especially concerning when toxic content\ncorrelates strongly with popularity indicators such as likes and upvotes. In\nthis study, we propose structured prosocial feedback as a complementary signal\nto likes and upvotes--one that highlights content quality based on normative\ncriteria to help address the limitations of conventional engagement signals. We\nbegin by designing and implementing a machine learning feedback system powered\nby a large language model (LLM), which evaluates user comments based on\nprinciples of positive psychology, such as individual well-being, constructive\nsocial media use, and character strengths. We then conduct a pre-registered\nuser study to examine how existing peer-based and the new expert-based feedback\ninteract to shape users' selection of comments in a social media setting.\nResults show that peer feedback increases conformity to popularity cues, while\nexpert feedback shifts preferences toward normatively higher-quality content.\nMoreover, incorporating expert feedback alongside peer evaluations improves\nalignment with expert assessments and contributes to a less toxic community\nenvironment. This illustrates the added value of normative cues--such as expert\nscores generated by LLMs using psychological rubrics--and underscores the\npotential benefits of incorporating such signals into platform feedback systems\nto foster healthier online environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many online platforms incorporate engagement signals--such as likes and\nupvotes--into their content ranking systems and interface design. These signals\nare designed to boost user engagement. However, they can unintentionally\nelevate content that is less inclusive and may not support normatively\ndesirable behavior. This issue becomes especially concerning when toxic content\ncorrelates strongly with popularity indicators such as likes and upvotes. In\nthis study, we propose structured prosocial feedback as a complementary signal\nto likes and upvotes--one that highlights content quality based on normative\ncriteria to help address the limitations of conventional engagement signals. We\nbegin by designing and implementing a machine learning feedback system powered\nby a large language model (LLM), which evaluates user comments based on\nprinciples of positive psychology, such as individual well-being, constructive\nsocial media use, and character strengths. We then conduct a pre-registered\nuser study to examine how existing peer-based and the new expert-based feedback\ninteract to shape users' selection of comments in a social media setting.\nResults show that peer feedback increases conformity to popularity cues, while\nexpert feedback shifts preferences toward normatively higher-quality content.\nMoreover, incorporating expert feedback alongside peer evaluations improves\nalignment with expert assessments and contributes to a less toxic community\nenvironment. This illustrates the added value of normative cues--such as expert\nscores generated by LLMs using psychological rubrics--and underscores the\npotential benefits of incorporating such signals into platform feedback systems\nto foster healthier online environments."
                },
                "authors": [
                    {
                        "name": "Yuchen Wu"
                    },
                    {
                        "name": "Mingduo Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Mingduo Zhao"
                },
                "author": "Mingduo Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09583v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09583v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09578v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09578v1",
                "updated": "2025-05-14T17:30:15Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    17,
                    30,
                    15,
                    2,
                    134,
                    0
                ],
                "published": "2025-05-14T17:30:15Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    17,
                    30,
                    15,
                    2,
                    134,
                    0
                ],
                "title": "The vertical structure of debris discs and the role of disc gravity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The vertical structure of debris discs and the role of disc gravity"
                },
                "summary": "Debris discs provide valuable insights into the formation and evolution of\nexoplanetary systems. Their structures are commonly attributed to planetary\nperturbations, serving as probes of as-yet-undetected planets. However, most\nstudies of planet-debris disc interactions ignore the disc's gravity, treating\nit as a collection of massless planetesimals. Here, using an analytical model,\nwe investigate how the vertical structure of a back-reacting debris disc\nresponds to secular perturbations from an inner, inclined planet. Considering\nthe disc's axisymmetric potential, we identify two dynamical regimes:\nplanet-dominated and disc-dominated, which may coexist, separated by a\nsecular-inclination resonance. In the planet-dominated regime ($M_d/m_p\\ll1$),\nwe recover the classical result: a transient warp propagates outward until the\ndisc settles into a box-like structure centered around the planetary orbit's\ninitial inclination $I_p(0)$, with a distance-independent aspect ratio\n$\\mathcal{H}(R)\\approx I_p(0)$. In contrast, in the disc-dominated regime\n($M_d/m_p\\gtrsim1$), the disc exhibits dynamical rigidity, remaining thin and\nmisaligned, with significantly suppressed inclinations and a sharply declining\naspect ratio, $\\mathcal{H}(R)\\propto I_p(0)R^{-7/2}$. In the intermediate\nregime ($M_d/m_p\\lesssim1$), the system exhibits a secular-inclination\nresonance, leading to long-lived, warp-like structures and a bimodal\ninclination distribution, containing both dynamically hot and cold populations.\nWe provide analytic formulae describing these effects as a function of system\nparameters. We also find that the vertical density profile is intrinsically\nnon-Gaussian and recommend fitting observations with non-zero slopes of\n$\\mathcal{H}(R)$. Our results may be used to infer planetary parameters and\ndebris disc masses based on observed warps and scale heights, as demonstrated\nfor HD110058 and $\\beta$ Pic.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Debris discs provide valuable insights into the formation and evolution of\nexoplanetary systems. Their structures are commonly attributed to planetary\nperturbations, serving as probes of as-yet-undetected planets. However, most\nstudies of planet-debris disc interactions ignore the disc's gravity, treating\nit as a collection of massless planetesimals. Here, using an analytical model,\nwe investigate how the vertical structure of a back-reacting debris disc\nresponds to secular perturbations from an inner, inclined planet. Considering\nthe disc's axisymmetric potential, we identify two dynamical regimes:\nplanet-dominated and disc-dominated, which may coexist, separated by a\nsecular-inclination resonance. In the planet-dominated regime ($M_d/m_p\\ll1$),\nwe recover the classical result: a transient warp propagates outward until the\ndisc settles into a box-like structure centered around the planetary orbit's\ninitial inclination $I_p(0)$, with a distance-independent aspect ratio\n$\\mathcal{H}(R)\\approx I_p(0)$. In contrast, in the disc-dominated regime\n($M_d/m_p\\gtrsim1$), the disc exhibits dynamical rigidity, remaining thin and\nmisaligned, with significantly suppressed inclinations and a sharply declining\naspect ratio, $\\mathcal{H}(R)\\propto I_p(0)R^{-7/2}$. In the intermediate\nregime ($M_d/m_p\\lesssim1$), the system exhibits a secular-inclination\nresonance, leading to long-lived, warp-like structures and a bimodal\ninclination distribution, containing both dynamically hot and cold populations.\nWe provide analytic formulae describing these effects as a function of system\nparameters. We also find that the vertical density profile is intrinsically\nnon-Gaussian and recommend fitting observations with non-zero slopes of\n$\\mathcal{H}(R)$. Our results may be used to infer planetary parameters and\ndebris disc masses based on observed warps and scale heights, as demonstrated\nfor HD110058 and $\\beta$ Pic."
                },
                "authors": [
                    {
                        "name": "Antranik A. Sefilian"
                    },
                    {
                        "name": "Kaitlin M. Kratter"
                    },
                    {
                        "name": "Mark C. Wyatt"
                    },
                    {
                        "name": "Cristobal Petrovich"
                    },
                    {
                        "name": "Philippe Thébault"
                    },
                    {
                        "name": "Renu Malhotra"
                    },
                    {
                        "name": "Virginie Faramaz-Gorka"
                    }
                ],
                "author_detail": {
                    "name": "Virginie Faramaz-Gorka"
                },
                "author": "Virginie Faramaz-Gorka",
                "arxiv_comment": "Submitted to MNRAS (28 Pages, 13 Figures, 1 Table). Comments are\n  welcome",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09578v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09578v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.EP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09576v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09576v1",
                "updated": "2025-05-14T17:29:19Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    17,
                    29,
                    19,
                    2,
                    134,
                    0
                ],
                "published": "2025-05-14T17:29:19Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    17,
                    29,
                    19,
                    2,
                    134,
                    0
                ],
                "title": "Ethics and Persuasion in Reinforcement Learning from Human Feedback: A\n  Procedural Rhetorical Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ethics and Persuasion in Reinforcement Learning from Human Feedback: A\n  Procedural Rhetorical Approach"
                },
                "summary": "Since 2022, versions of generative AI chatbots such as ChatGPT and Claude\nhave been trained using a specialized technique called Reinforcement Learning\nfrom Human Feedback (RLHF) to fine-tune language model output using feedback\nfrom human annotators. As a result, the integration of RLHF has greatly\nenhanced the outputs of these large language models (LLMs) and made the\ninteractions and responses appear more \"human-like\" than those of previous\nversions using only supervised learning. The increasing convergence of human\nand machine-written text has potentially severe ethical, sociotechnical, and\npedagogical implications relating to transparency, trust, bias, and\ninterpersonal relations. To highlight these implications, this paper presents a\nrhetorical analysis of some of the central procedures and processes currently\nbeing reshaped by RLHF-enhanced generative AI chatbots: upholding language\nconventions, information seeking practices, and expectations for social\nrelationships. Rhetorical investigations of generative AI and LLMs have, to\nthis point, focused largely on the persuasiveness of the content generated.\nUsing Ian Bogost's concept of procedural rhetoric, this paper shifts the site\nof rhetorical investigation from content analysis to the underlying mechanisms\nof persuasion built into RLHF-enhanced LLMs. In doing so, this theoretical\ninvestigation opens a new direction for further inquiry in AI ethics that\nconsiders how procedures rerouted through AI-driven technologies might\nreinforce hegemonic language use, perpetuate biases, decontextualize learning,\nand encroach upon human relationships. It will therefore be of interest to\neducators, researchers, scholars, and the growing number of users of generative\nAI chatbots.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Since 2022, versions of generative AI chatbots such as ChatGPT and Claude\nhave been trained using a specialized technique called Reinforcement Learning\nfrom Human Feedback (RLHF) to fine-tune language model output using feedback\nfrom human annotators. As a result, the integration of RLHF has greatly\nenhanced the outputs of these large language models (LLMs) and made the\ninteractions and responses appear more \"human-like\" than those of previous\nversions using only supervised learning. The increasing convergence of human\nand machine-written text has potentially severe ethical, sociotechnical, and\npedagogical implications relating to transparency, trust, bias, and\ninterpersonal relations. To highlight these implications, this paper presents a\nrhetorical analysis of some of the central procedures and processes currently\nbeing reshaped by RLHF-enhanced generative AI chatbots: upholding language\nconventions, information seeking practices, and expectations for social\nrelationships. Rhetorical investigations of generative AI and LLMs have, to\nthis point, focused largely on the persuasiveness of the content generated.\nUsing Ian Bogost's concept of procedural rhetoric, this paper shifts the site\nof rhetorical investigation from content analysis to the underlying mechanisms\nof persuasion built into RLHF-enhanced LLMs. In doing so, this theoretical\ninvestigation opens a new direction for further inquiry in AI ethics that\nconsiders how procedures rerouted through AI-driven technologies might\nreinforce hegemonic language use, perpetuate biases, decontextualize learning,\nand encroach upon human relationships. It will therefore be of interest to\neducators, researchers, scholars, and the growing number of users of generative\nAI chatbots."
                },
                "authors": [
                    {
                        "name": "Shannon Lodoen"
                    },
                    {
                        "name": "Alexi Orchard"
                    }
                ],
                "author_detail": {
                    "name": "Alexi Orchard"
                },
                "author": "Alexi Orchard",
                "arxiv_comment": "10 pages, 1 figure, Accepted version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09576v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09576v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15507v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15507v3",
                "updated": "2025-05-14T17:25:36Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    17,
                    25,
                    36,
                    2,
                    134,
                    0
                ],
                "published": "2025-02-21T15:04:48Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    15,
                    4,
                    48,
                    4,
                    52,
                    0
                ],
                "title": "Activation Steering in Neural Theorem Provers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Activation Steering in Neural Theorem Provers"
                },
                "summary": "Large Language Models (LLMs) have shown promise in proving formal theorems\nusing proof assistants like Lean. However, current state of the art language\nmodels struggles to predict next step in proofs leading practitioners to use\ndifferent sampling techniques to improve LLMs capabilities. We observe that the\nLLM is capable of predicting the correct tactic; however, it faces challenges\nin ranking it appropriately within the set of candidate tactics, affecting the\noverall selection process. To overcome this hurdle, we use activation steering\nto guide LLMs responses to improve the generations at the time of inference.\nOur results suggest that activation steering offers a promising lightweight\nalternative to specialized fine-tuning for enhancing theorem proving\ncapabilities in LLMs, particularly valuable in resource-constrained\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown promise in proving formal theorems\nusing proof assistants like Lean. However, current state of the art language\nmodels struggles to predict next step in proofs leading practitioners to use\ndifferent sampling techniques to improve LLMs capabilities. We observe that the\nLLM is capable of predicting the correct tactic; however, it faces challenges\nin ranking it appropriately within the set of candidate tactics, affecting the\noverall selection process. To overcome this hurdle, we use activation steering\nto guide LLMs responses to improve the generations at the time of inference.\nOur results suggest that activation steering offers a promising lightweight\nalternative to specialized fine-tuning for enhancing theorem proving\ncapabilities in LLMs, particularly valuable in resource-constrained\nenvironments."
                },
                "authors": [
                    {
                        "name": "Shashank Kirtania"
                    }
                ],
                "author_detail": {
                    "name": "Shashank Kirtania"
                },
                "author": "Shashank Kirtania",
                "arxiv_comment": "incorrect explanation for a concept, need to revise and update!",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15507v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15507v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.12662v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.12662v2",
                "updated": "2025-05-14T17:20:26Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    17,
                    20,
                    26,
                    2,
                    134,
                    0
                ],
                "published": "2025-03-16T21:34:11Z",
                "published_parsed": [
                    2025,
                    3,
                    16,
                    21,
                    34,
                    11,
                    6,
                    75,
                    0
                ],
                "title": "TuneNSearch: a hybrid transfer learning and local search approach for\n  solving vehicle routing problems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TuneNSearch: a hybrid transfer learning and local search approach for\n  solving vehicle routing problems"
                },
                "summary": "This paper introduces TuneNSearch, a hybrid transfer learning and local\nsearch approach for addressing different variants of vehicle routing problems\n(VRP). Recently, multi-task learning has gained much attention for solving VRP\nvariants. However, this adaptability often compromises the performance of the\nmodels. To address this challenge, we first pre-train a reinforcement learning\nmodel on the multi-depot VRP, followed by a short fine-tuning phase to adapt it\nto different variants. By leveraging the complexity of the multi-depot VRP, the\npre-trained model learns richer node representations and gains more\ntransferable knowledge compared to models trained on simpler routing problems,\nsuch as the traveling salesman problem. TuneNSearch employs, in the first\nstage, a Transformer-based architecture, augmented with a residual edge-graph\nattention network to capture the impact of edge distances and residual\nconnections between layers. This architecture allows for a more precise capture\nof graph-structured data, improving the encoding of VRP's features. After\ninference, our model is also coupled with a second stage composed of a local\nsearch algorithm, which yields substantial performance gains with minimal\ncomputational overhead added. Results show that TuneNSearch outperforms many\nexisting state-of-the-art models trained for each VRP variant, requiring only\none-fifth of the training epochs. Our approach demonstrates strong\ngeneralization, achieving high performance across different tasks,\ndistributions and problem sizes, thus addressing a long-standing gap in the\nliterature.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces TuneNSearch, a hybrid transfer learning and local\nsearch approach for addressing different variants of vehicle routing problems\n(VRP). Recently, multi-task learning has gained much attention for solving VRP\nvariants. However, this adaptability often compromises the performance of the\nmodels. To address this challenge, we first pre-train a reinforcement learning\nmodel on the multi-depot VRP, followed by a short fine-tuning phase to adapt it\nto different variants. By leveraging the complexity of the multi-depot VRP, the\npre-trained model learns richer node representations and gains more\ntransferable knowledge compared to models trained on simpler routing problems,\nsuch as the traveling salesman problem. TuneNSearch employs, in the first\nstage, a Transformer-based architecture, augmented with a residual edge-graph\nattention network to capture the impact of edge distances and residual\nconnections between layers. This architecture allows for a more precise capture\nof graph-structured data, improving the encoding of VRP's features. After\ninference, our model is also coupled with a second stage composed of a local\nsearch algorithm, which yields substantial performance gains with minimal\ncomputational overhead added. Results show that TuneNSearch outperforms many\nexisting state-of-the-art models trained for each VRP variant, requiring only\none-fifth of the training epochs. Our approach demonstrates strong\ngeneralization, achieving high performance across different tasks,\ndistributions and problem sizes, thus addressing a long-standing gap in the\nliterature."
                },
                "authors": [
                    {
                        "name": "Arthur Corrêa"
                    },
                    {
                        "name": "Cristóvão Silva"
                    },
                    {
                        "name": "Liming Xu"
                    },
                    {
                        "name": "Alexandra Brintrup"
                    },
                    {
                        "name": "Samuel Moniz"
                    }
                ],
                "author_detail": {
                    "name": "Samuel Moniz"
                },
                "author": "Samuel Moniz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.12662v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.12662v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09569v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09569v1",
                "updated": "2025-05-14T17:11:23Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    17,
                    11,
                    23,
                    2,
                    134,
                    0
                ],
                "published": "2025-05-14T17:11:23Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    17,
                    11,
                    23,
                    2,
                    134,
                    0
                ],
                "title": "MIGRATION-BENCH: Repository-Level Code Migration Benchmark from Java 8",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MIGRATION-BENCH: Repository-Level Code Migration Benchmark from Java 8"
                },
                "summary": "With the rapid advancement of powerful large language models (LLMs) in recent\nyears, a wide range of software engineering tasks can now be addressed using\nLLMs, significantly enhancing productivity and scalability. Numerous benchmark\ndatasets have been developed to evaluate the coding capabilities of these\nmodels, while they primarily focus on problem-solving and issue-resolution\ntasks. In contrast, we introduce a new coding benchmark MIGRATION-BENCH with a\ndistinct focus: code migration. MIGRATION-BENCH aims to serve as a\ncomprehensive benchmark for migration from Java 8 to the latest long-term\nsupport (LTS) versions (Java 17, 21), MIGRATION-BENCH includes a full dataset\nand its subset selected with $5,102$ and $300$ repositories respectively.\nSelected is a representative subset curated for complexity and difficulty,\noffering a versatile resource to support research in the field of code\nmigration. Additionally, we provide a comprehensive evaluation framework to\nfacilitate rigorous and standardized assessment of LLMs on this challenging\ntask. We further propose SD-Feedback and demonstrate that LLMs can effectively\ntackle repository-level code migration to Java 17. For the selected subset with\nClaude-3.5-Sonnet-v2, SD-Feedback achieves 62.33% and 27.00% success rate\n(pass@1) for minimal and maximal migration respectively. The benchmark dataset\nand source code are available at:\nhttps://huggingface.co/collections/AmazonScience and\nhttps://github.com/amazon-science/self_debug respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid advancement of powerful large language models (LLMs) in recent\nyears, a wide range of software engineering tasks can now be addressed using\nLLMs, significantly enhancing productivity and scalability. Numerous benchmark\ndatasets have been developed to evaluate the coding capabilities of these\nmodels, while they primarily focus on problem-solving and issue-resolution\ntasks. In contrast, we introduce a new coding benchmark MIGRATION-BENCH with a\ndistinct focus: code migration. MIGRATION-BENCH aims to serve as a\ncomprehensive benchmark for migration from Java 8 to the latest long-term\nsupport (LTS) versions (Java 17, 21), MIGRATION-BENCH includes a full dataset\nand its subset selected with $5,102$ and $300$ repositories respectively.\nSelected is a representative subset curated for complexity and difficulty,\noffering a versatile resource to support research in the field of code\nmigration. Additionally, we provide a comprehensive evaluation framework to\nfacilitate rigorous and standardized assessment of LLMs on this challenging\ntask. We further propose SD-Feedback and demonstrate that LLMs can effectively\ntackle repository-level code migration to Java 17. For the selected subset with\nClaude-3.5-Sonnet-v2, SD-Feedback achieves 62.33% and 27.00% success rate\n(pass@1) for minimal and maximal migration respectively. The benchmark dataset\nand source code are available at:\nhttps://huggingface.co/collections/AmazonScience and\nhttps://github.com/amazon-science/self_debug respectively."
                },
                "authors": [
                    {
                        "name": "Linbo Liu"
                    },
                    {
                        "name": "Xinle Liu"
                    },
                    {
                        "name": "Qiang Zhou"
                    },
                    {
                        "name": "Lin Chen"
                    },
                    {
                        "name": "Yihan Liu"
                    },
                    {
                        "name": "Hoan Nguyen"
                    },
                    {
                        "name": "Behrooz Omidvar-Tehrani"
                    },
                    {
                        "name": "Xi Shen"
                    },
                    {
                        "name": "Jun Huan"
                    },
                    {
                        "name": "Omer Tripp"
                    },
                    {
                        "name": "Anoop Deoras"
                    }
                ],
                "author_detail": {
                    "name": "Anoop Deoras"
                },
                "author": "Anoop Deoras",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09569v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09569v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01618v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01618v2",
                "updated": "2025-05-14T17:09:58Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    17,
                    9,
                    58,
                    2,
                    134,
                    0
                ],
                "published": "2025-05-02T22:45:14Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    22,
                    45,
                    14,
                    4,
                    122,
                    0
                ],
                "title": "Don't be lazy: CompleteP enables compute-efficient deep transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Don't be lazy: CompleteP enables compute-efficient deep transformers"
                },
                "summary": "We study compute efficiency of LLM training when using different\nparameterizations, i.e., rules for adjusting model and optimizer\nhyperparameters (HPs) as model size changes. Some parameterizations fail to\ntransfer optimal base HPs (such as learning rate) across changes in model\ndepth, requiring practitioners to either re-tune these HPs as they scale up\n(expensive), or accept sub-optimal training when re-tuning is prohibitive. Even\nwhen they achieve HP transfer, we develop theory to show parameterizations may\nstill exist in the lazy learning regime where layers learn only features close\nto their linearization, preventing effective use of depth and nonlinearity.\nFinally, we identify and adopt the parameterization we call CompleteP that\nachieves both depth-wise HP transfer and non-lazy learning in all layers.\nCompleteP enables a wider range of model width/depth ratios to remain\ncompute-efficient, unlocking shapes better suited for different hardware\nsettings and operational contexts. Moreover, CompleteP enables 12-34% compute\nefficiency improvements over the prior state-of-the-art.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study compute efficiency of LLM training when using different\nparameterizations, i.e., rules for adjusting model and optimizer\nhyperparameters (HPs) as model size changes. Some parameterizations fail to\ntransfer optimal base HPs (such as learning rate) across changes in model\ndepth, requiring practitioners to either re-tune these HPs as they scale up\n(expensive), or accept sub-optimal training when re-tuning is prohibitive. Even\nwhen they achieve HP transfer, we develop theory to show parameterizations may\nstill exist in the lazy learning regime where layers learn only features close\nto their linearization, preventing effective use of depth and nonlinearity.\nFinally, we identify and adopt the parameterization we call CompleteP that\nachieves both depth-wise HP transfer and non-lazy learning in all layers.\nCompleteP enables a wider range of model width/depth ratios to remain\ncompute-efficient, unlocking shapes better suited for different hardware\nsettings and operational contexts. Moreover, CompleteP enables 12-34% compute\nefficiency improvements over the prior state-of-the-art."
                },
                "authors": [
                    {
                        "name": "Nolan Dey"
                    },
                    {
                        "name": "Bin Claire Zhang"
                    },
                    {
                        "name": "Lorenzo Noci"
                    },
                    {
                        "name": "Mufan Li"
                    },
                    {
                        "name": "Blake Bordelon"
                    },
                    {
                        "name": "Shane Bergsma"
                    },
                    {
                        "name": "Cengiz Pehlevan"
                    },
                    {
                        "name": "Boris Hanin"
                    },
                    {
                        "name": "Joel Hestness"
                    }
                ],
                "author_detail": {
                    "name": "Joel Hestness"
                },
                "author": "Joel Hestness",
                "arxiv_comment": "10 main pages, 16 appendix pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01618v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01618v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09561v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09561v1",
                "updated": "2025-05-14T17:00:47Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    17,
                    0,
                    47,
                    2,
                    134,
                    0
                ],
                "published": "2025-05-14T17:00:47Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    17,
                    0,
                    47,
                    2,
                    134,
                    0
                ],
                "title": "Learning Long-Context Diffusion Policies via Past-Token Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Long-Context Diffusion Policies via Past-Token Prediction"
                },
                "summary": "Reasoning over long sequences of observations and actions is essential for\nmany robotic tasks. Yet, learning effective long-context policies from\ndemonstrations remains challenging. As context length increases, training\nbecomes increasingly expensive due to rising memory demands, and policy\nperformance often degrades as a result of spurious correlations. Recent methods\ntypically sidestep these issues by truncating context length, discarding\nhistorical information that may be critical for subsequent decisions. In this\npaper, we propose an alternative approach that explicitly regularizes the\nretention of past information. We first revisit the copycat problem in\nimitation learning and identify an opposite challenge in recent diffusion\npolicies: rather than over-relying on prior actions, they often fail to capture\nessential dependencies between past and future actions. To address this, we\nintroduce Past-Token Prediction (PTP), an auxiliary task in which the policy\nlearns to predict past action tokens alongside future ones. This regularization\nsignificantly improves temporal modeling in the policy head, with minimal\nreliance on visual representations. Building on this observation, we further\nintroduce a multistage training strategy: pre-train the visual encoder with\nshort contexts, and fine-tune the policy head using cached long-context\nembeddings. This strategy preserves the benefits of PTP while greatly reducing\nmemory and computational overhead. Finally, we extend PTP into a\nself-verification mechanism at test time, enabling the policy to score and\nselect candidates consistent with past actions during inference. Experiments\nacross four real-world and six simulated tasks demonstrate that our proposed\nmethod improves the performance of long-context diffusion policies by 3x and\naccelerates policy training by more than 10x.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning over long sequences of observations and actions is essential for\nmany robotic tasks. Yet, learning effective long-context policies from\ndemonstrations remains challenging. As context length increases, training\nbecomes increasingly expensive due to rising memory demands, and policy\nperformance often degrades as a result of spurious correlations. Recent methods\ntypically sidestep these issues by truncating context length, discarding\nhistorical information that may be critical for subsequent decisions. In this\npaper, we propose an alternative approach that explicitly regularizes the\nretention of past information. We first revisit the copycat problem in\nimitation learning and identify an opposite challenge in recent diffusion\npolicies: rather than over-relying on prior actions, they often fail to capture\nessential dependencies between past and future actions. To address this, we\nintroduce Past-Token Prediction (PTP), an auxiliary task in which the policy\nlearns to predict past action tokens alongside future ones. This regularization\nsignificantly improves temporal modeling in the policy head, with minimal\nreliance on visual representations. Building on this observation, we further\nintroduce a multistage training strategy: pre-train the visual encoder with\nshort contexts, and fine-tune the policy head using cached long-context\nembeddings. This strategy preserves the benefits of PTP while greatly reducing\nmemory and computational overhead. Finally, we extend PTP into a\nself-verification mechanism at test time, enabling the policy to score and\nselect candidates consistent with past actions during inference. Experiments\nacross four real-world and six simulated tasks demonstrate that our proposed\nmethod improves the performance of long-context diffusion policies by 3x and\naccelerates policy training by more than 10x."
                },
                "authors": [
                    {
                        "name": "Marcel Torne"
                    },
                    {
                        "name": "Andy Tang"
                    },
                    {
                        "name": "Yuejiang Liu"
                    },
                    {
                        "name": "Chelsea Finn"
                    }
                ],
                "author_detail": {
                    "name": "Chelsea Finn"
                },
                "author": "Chelsea Finn",
                "arxiv_comment": "Videos are available at https://long-context-dp.github.io",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09561v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09561v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21813v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21813v3",
                "updated": "2025-05-14T16:57:13Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    16,
                    57,
                    13,
                    2,
                    134,
                    0
                ],
                "published": "2025-03-25T18:20:04Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    18,
                    20,
                    4,
                    1,
                    84,
                    0
                ],
                "title": "OAEI-LLM-T: A TBox Benchmark Dataset for Understanding Large Language\n  Model Hallucinations in Ontology Matching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OAEI-LLM-T: A TBox Benchmark Dataset for Understanding Large Language\n  Model Hallucinations in Ontology Matching"
                },
                "summary": "Hallucinations are often inevitable in downstream tasks using large language\nmodels (LLMs). To tackle the substantial challenge of addressing hallucinations\nfor LLM-based ontology matching (OM) systems, we introduce a new benchmark\ndataset OAEI-LLM-T. The dataset evolves from seven TBox datasets in the\nOntology Alignment Evaluation Initiative (OAEI), capturing hallucinations of\nten different LLMs performing OM tasks. These OM-specific hallucinations are\norganised into two primary categories and six sub-categories. We showcase the\nusefulness of the dataset in constructing an LLM leaderboard for OM tasks and\nfor fine-tuning LLMs used in OM tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hallucinations are often inevitable in downstream tasks using large language\nmodels (LLMs). To tackle the substantial challenge of addressing hallucinations\nfor LLM-based ontology matching (OM) systems, we introduce a new benchmark\ndataset OAEI-LLM-T. The dataset evolves from seven TBox datasets in the\nOntology Alignment Evaluation Initiative (OAEI), capturing hallucinations of\nten different LLMs performing OM tasks. These OM-specific hallucinations are\norganised into two primary categories and six sub-categories. We showcase the\nusefulness of the dataset in constructing an LLM leaderboard for OM tasks and\nfor fine-tuning LLMs used in OM tasks."
                },
                "authors": [
                    {
                        "name": "Zhangcheng Qiang"
                    },
                    {
                        "name": "Kerry Taylor"
                    },
                    {
                        "name": "Weiqing Wang"
                    },
                    {
                        "name": "Jing Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Jing Jiang"
                },
                "author": "Jing Jiang",
                "arxiv_comment": "14 pages, 4 figures, 4 tables, 2 prompt templates",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21813v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21813v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17773v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17773v2",
                "updated": "2025-05-14T16:56:46Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    16,
                    56,
                    46,
                    2,
                    134,
                    0
                ],
                "published": "2024-09-26T12:09:39Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    12,
                    9,
                    39,
                    3,
                    270,
                    0
                ],
                "title": "Red giant - jet collisions in galactic nuclei I: 3D hydrodynamical model\n  of a few stellar orbits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Red giant - jet collisions in galactic nuclei I: 3D hydrodynamical model\n  of a few stellar orbits"
                },
                "summary": "Several models have been proposed to explain missing red giants (RGs) near\nthe Galactic centre. Recently, a scenario has been suggested that predicts,\namong other processes, a long-term ablation of the surface layers of RGs during\ntheir repetitive passages through the Galactic jet (Zaja\\v{c}ek et al., 2020).\nIn this study, we perform detailed three-dimensional numerical modelling of\nthis phenomenon. We calculate the ablation rate of the surface layers of a RG\norbiting the supermassive black hole (SMBH) as it passes through the nuclear\njet. In particular, we model the jet-star interaction for approximately 10\npassages for the closer orbital distance of $10^{-3}\\,\\text{pc}$ and 2 passages\nfor $10^{-2}\\,\\text{pc}$. We find that the mass loss due to ablation by the jet\nbehaves with time as $\\Delta M_{\\star}\\propto \\sqrt{t}$ and the total ablated\nmass during a single active galactic nucleus (AGN) phase ($10^5$ years) is\n$\\sim 10^{-4}\\,M_{\\odot}$. We arrive at similar rates of the stellar ablation\nfor the relatively smaller jet luminosity $10^{42}\\,\\text{erg}\\,\\text{s}^{-1}$\nas in the previous analytical calculations. For larger jet luminosities of\n$10^{44}$ and $10^{48}\\,\\text{erg}\\,\\text{s}^{-1}$, the ablation rates inferred\nfrom $\\sim 10$ interactions as well as extrapolated power-law fits are\nsignificantly lower than analytical values. Overall, the mass ablation rate per\ninteraction and the extrapolated cumulative mass loss during the jet activity\nare comparable to the stellar-wind mass loss. For the smallest orbital distance\nof $10^{-3}\\,\\text{pc}$, we also track the thermal behaviour of the stellar\nsurface layer, whose temperature appears to grow rapidly during the first 10\npassages from $\\sim 3600\\,{\\rm K}$ (spectral type M) to $\\sim 8500\\,{\\rm K}$\n(spectral type A). RG-jet interactions can thus lead to observable changes in\nthe nuclear stellar population during the jet existence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Several models have been proposed to explain missing red giants (RGs) near\nthe Galactic centre. Recently, a scenario has been suggested that predicts,\namong other processes, a long-term ablation of the surface layers of RGs during\ntheir repetitive passages through the Galactic jet (Zaja\\v{c}ek et al., 2020).\nIn this study, we perform detailed three-dimensional numerical modelling of\nthis phenomenon. We calculate the ablation rate of the surface layers of a RG\norbiting the supermassive black hole (SMBH) as it passes through the nuclear\njet. In particular, we model the jet-star interaction for approximately 10\npassages for the closer orbital distance of $10^{-3}\\,\\text{pc}$ and 2 passages\nfor $10^{-2}\\,\\text{pc}$. We find that the mass loss due to ablation by the jet\nbehaves with time as $\\Delta M_{\\star}\\propto \\sqrt{t}$ and the total ablated\nmass during a single active galactic nucleus (AGN) phase ($10^5$ years) is\n$\\sim 10^{-4}\\,M_{\\odot}$. We arrive at similar rates of the stellar ablation\nfor the relatively smaller jet luminosity $10^{42}\\,\\text{erg}\\,\\text{s}^{-1}$\nas in the previous analytical calculations. For larger jet luminosities of\n$10^{44}$ and $10^{48}\\,\\text{erg}\\,\\text{s}^{-1}$, the ablation rates inferred\nfrom $\\sim 10$ interactions as well as extrapolated power-law fits are\nsignificantly lower than analytical values. Overall, the mass ablation rate per\ninteraction and the extrapolated cumulative mass loss during the jet activity\nare comparable to the stellar-wind mass loss. For the smallest orbital distance\nof $10^{-3}\\,\\text{pc}$, we also track the thermal behaviour of the stellar\nsurface layer, whose temperature appears to grow rapidly during the first 10\npassages from $\\sim 3600\\,{\\rm K}$ (spectral type M) to $\\sim 8500\\,{\\rm K}$\n(spectral type A). RG-jet interactions can thus lead to observable changes in\nthe nuclear stellar population during the jet existence."
                },
                "authors": [
                    {
                        "name": "Petr Kurfürst"
                    },
                    {
                        "name": "Michal Zajaček"
                    },
                    {
                        "name": "Norbert Werner"
                    },
                    {
                        "name": "Jiří Krtička"
                    }
                ],
                "author_detail": {
                    "name": "Jiří Krtička"
                },
                "arxiv_affiliation": "MUNI, Brno",
                "author": "Jiří Krtička",
                "arxiv_comment": "22 pages, 23 figures, 2 tables; accepted by the MNRAS Main journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17773v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17773v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09551v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09551v1",
                "updated": "2025-05-14T16:49:47Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    16,
                    49,
                    47,
                    2,
                    134,
                    0
                ],
                "published": "2025-05-14T16:49:47Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    16,
                    49,
                    47,
                    2,
                    134,
                    0
                ],
                "title": "Fast Learning in Quantitative Finance with Extreme Learning Machine",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast Learning in Quantitative Finance with Extreme Learning Machine"
                },
                "summary": "This paper demonstrates that a broad class of problems in quantitative\nfinance, including those previously addressed using deep neural networks, can\nbe efficiently solved using single-layer neural networks without iterative\ngradient-based training, namely extreme learning machine (ELM). ELM utilizes a\nsingle-layer network with randomly initialized hidden nodes and analytically\ncomputed output weights obtained via convex optimization, enabling rapid\ntraining and inference. Both supervised and unsupervised learning tasks are\nexplored.\n  In supervised learning, ELM is employed to learn parametric option pricing\nfunctions, predict intraday stock returns, and complete implied volatility\nsurfaces. Compared with deep neural networks, Gaussian process regression, and\nlogistic regression, ELM achieves higher computational speed, comparable\naccuracy, and superior generalization.\n  In unsupervised learning, ELM numerically solves Black-Scholes-type PDEs, and\noutperforms Physics-Informed Neural Networks in training speed without losing\nprecision. The approximation and generalization abilities of ELM are briefly\ndiscussed.\n  The findings establish ELM as a practical and efficient tool for various\ntasks in quantitative finance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper demonstrates that a broad class of problems in quantitative\nfinance, including those previously addressed using deep neural networks, can\nbe efficiently solved using single-layer neural networks without iterative\ngradient-based training, namely extreme learning machine (ELM). ELM utilizes a\nsingle-layer network with randomly initialized hidden nodes and analytically\ncomputed output weights obtained via convex optimization, enabling rapid\ntraining and inference. Both supervised and unsupervised learning tasks are\nexplored.\n  In supervised learning, ELM is employed to learn parametric option pricing\nfunctions, predict intraday stock returns, and complete implied volatility\nsurfaces. Compared with deep neural networks, Gaussian process regression, and\nlogistic regression, ELM achieves higher computational speed, comparable\naccuracy, and superior generalization.\n  In unsupervised learning, ELM numerically solves Black-Scholes-type PDEs, and\noutperforms Physics-Informed Neural Networks in training speed without losing\nprecision. The approximation and generalization abilities of ELM are briefly\ndiscussed.\n  The findings establish ELM as a practical and efficient tool for various\ntasks in quantitative finance."
                },
                "authors": [
                    {
                        "name": "Liexin Cheng"
                    },
                    {
                        "name": "Xue Cheng"
                    },
                    {
                        "name": "Shuaiqiang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Shuaiqiang Liu"
                },
                "author": "Shuaiqiang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09551v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09551v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-fin.CP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-fin.CP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00949v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00949v3",
                "updated": "2025-05-14T16:47:23Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    16,
                    47,
                    23,
                    2,
                    134,
                    0
                ],
                "published": "2025-05-02T01:35:35Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    1,
                    35,
                    35,
                    4,
                    122,
                    0
                ],
                "title": "Llama-Nemotron: Efficient Reasoning Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Llama-Nemotron: Efficient Reasoning Models"
                },
                "summary": "We introduce the Llama-Nemotron series of models, an open family of\nheterogeneous reasoning models that deliver exceptional reasoning capabilities,\ninference efficiency, and an open license for enterprise use. The family comes\nin three sizes -- Nano (8B), Super (49B), and Ultra (253B) -- and performs\ncompetitively with state-of-the-art reasoning models such as DeepSeek-R1 while\noffering superior inference throughput and memory efficiency. In this report,\nwe discuss the training procedure for these models, which entails using neural\narchitecture search from Llama 3 models for accelerated inference, knowledge\ndistillation, and continued pretraining, followed by a reasoning-focused\npost-training stage consisting of two main parts: supervised fine-tuning and\nlarge scale reinforcement learning. Llama-Nemotron models are the first\nopen-source models to support a dynamic reasoning toggle, allowing users to\nswitch between standard chat and reasoning modes during inference. To further\nsupport open research and facilitate model development, we provide the\nfollowing resources: 1. We release the Llama-Nemotron reasoning models --\nLN-Nano, LN-Super, and LN-Ultra -- under the commercially permissive NVIDIA\nOpen Model License Agreement. 2. We release the complete post-training dataset:\nLlama-Nemotron-Post-Training-Dataset. 3. We also release our training\ncodebases: NeMo, NeMo-Aligner, and Megatron-LM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce the Llama-Nemotron series of models, an open family of\nheterogeneous reasoning models that deliver exceptional reasoning capabilities,\ninference efficiency, and an open license for enterprise use. The family comes\nin three sizes -- Nano (8B), Super (49B), and Ultra (253B) -- and performs\ncompetitively with state-of-the-art reasoning models such as DeepSeek-R1 while\noffering superior inference throughput and memory efficiency. In this report,\nwe discuss the training procedure for these models, which entails using neural\narchitecture search from Llama 3 models for accelerated inference, knowledge\ndistillation, and continued pretraining, followed by a reasoning-focused\npost-training stage consisting of two main parts: supervised fine-tuning and\nlarge scale reinforcement learning. Llama-Nemotron models are the first\nopen-source models to support a dynamic reasoning toggle, allowing users to\nswitch between standard chat and reasoning modes during inference. To further\nsupport open research and facilitate model development, we provide the\nfollowing resources: 1. We release the Llama-Nemotron reasoning models --\nLN-Nano, LN-Super, and LN-Ultra -- under the commercially permissive NVIDIA\nOpen Model License Agreement. 2. We release the complete post-training dataset:\nLlama-Nemotron-Post-Training-Dataset. 3. We also release our training\ncodebases: NeMo, NeMo-Aligner, and Megatron-LM."
                },
                "authors": [
                    {
                        "name": "Akhiad Bercovich"
                    },
                    {
                        "name": "Itay Levy"
                    },
                    {
                        "name": "Izik Golan"
                    },
                    {
                        "name": "Mohammad Dabbah"
                    },
                    {
                        "name": "Ran El-Yaniv"
                    },
                    {
                        "name": "Omri Puny"
                    },
                    {
                        "name": "Ido Galil"
                    },
                    {
                        "name": "Zach Moshe"
                    },
                    {
                        "name": "Tomer Ronen"
                    },
                    {
                        "name": "Najeeb Nabwani"
                    },
                    {
                        "name": "Ido Shahaf"
                    },
                    {
                        "name": "Oren Tropp"
                    },
                    {
                        "name": "Ehud Karpas"
                    },
                    {
                        "name": "Ran Zilberstein"
                    },
                    {
                        "name": "Jiaqi Zeng"
                    },
                    {
                        "name": "Soumye Singhal"
                    },
                    {
                        "name": "Alexander Bukharin"
                    },
                    {
                        "name": "Yian Zhang"
                    },
                    {
                        "name": "Tugrul Konuk"
                    },
                    {
                        "name": "Gerald Shen"
                    },
                    {
                        "name": "Ameya Sunil Mahabaleshwarkar"
                    },
                    {
                        "name": "Bilal Kartal"
                    },
                    {
                        "name": "Yoshi Suhara"
                    },
                    {
                        "name": "Olivier Delalleau"
                    },
                    {
                        "name": "Zijia Chen"
                    },
                    {
                        "name": "Zhilin Wang"
                    },
                    {
                        "name": "David Mosallanezhad"
                    },
                    {
                        "name": "Adi Renduchintala"
                    },
                    {
                        "name": "Haifeng Qian"
                    },
                    {
                        "name": "Dima Rekesh"
                    },
                    {
                        "name": "Fei Jia"
                    },
                    {
                        "name": "Somshubra Majumdar"
                    },
                    {
                        "name": "Vahid Noroozi"
                    },
                    {
                        "name": "Wasi Uddin Ahmad"
                    },
                    {
                        "name": "Sean Narenthiran"
                    },
                    {
                        "name": "Aleksander Ficek"
                    },
                    {
                        "name": "Mehrzad Samadi"
                    },
                    {
                        "name": "Jocelyn Huang"
                    },
                    {
                        "name": "Siddhartha Jain"
                    },
                    {
                        "name": "Igor Gitman"
                    },
                    {
                        "name": "Ivan Moshkov"
                    },
                    {
                        "name": "Wei Du"
                    },
                    {
                        "name": "Shubham Toshniwal"
                    },
                    {
                        "name": "George Armstrong"
                    },
                    {
                        "name": "Branislav Kisacanin"
                    },
                    {
                        "name": "Matvei Novikov"
                    },
                    {
                        "name": "Daria Gitman"
                    },
                    {
                        "name": "Evelina Bakhturina"
                    },
                    {
                        "name": "Jane Polak Scowcroft"
                    },
                    {
                        "name": "John Kamalu"
                    },
                    {
                        "name": "Dan Su"
                    },
                    {
                        "name": "Kezhi Kong"
                    },
                    {
                        "name": "Markus Kliegl"
                    },
                    {
                        "name": "Rabeeh Karimi"
                    },
                    {
                        "name": "Ying Lin"
                    },
                    {
                        "name": "Sanjeev Satheesh"
                    },
                    {
                        "name": "Jupinder Parmar"
                    },
                    {
                        "name": "Pritam Gundecha"
                    },
                    {
                        "name": "Brandon Norick"
                    },
                    {
                        "name": "Joseph Jennings"
                    },
                    {
                        "name": "Shrimai Prabhumoye"
                    },
                    {
                        "name": "Syeda Nahida Akter"
                    },
                    {
                        "name": "Mostofa Patwary"
                    },
                    {
                        "name": "Abhinav Khattar"
                    },
                    {
                        "name": "Deepak Narayanan"
                    },
                    {
                        "name": "Roger Waleffe"
                    },
                    {
                        "name": "Jimmy Zhang"
                    },
                    {
                        "name": "Bor-Yiing Su"
                    },
                    {
                        "name": "Guyue Huang"
                    },
                    {
                        "name": "Terry Kong"
                    },
                    {
                        "name": "Parth Chadha"
                    },
                    {
                        "name": "Sahil Jain"
                    },
                    {
                        "name": "Christine Harvey"
                    },
                    {
                        "name": "Elad Segal"
                    },
                    {
                        "name": "Jining Huang"
                    },
                    {
                        "name": "Sergey Kashirsky"
                    },
                    {
                        "name": "Robert McQueen"
                    },
                    {
                        "name": "Izzy Putterman"
                    },
                    {
                        "name": "George Lam"
                    },
                    {
                        "name": "Arun Venkatesan"
                    },
                    {
                        "name": "Sherry Wu"
                    },
                    {
                        "name": "Vinh Nguyen"
                    },
                    {
                        "name": "Manoj Kilaru"
                    },
                    {
                        "name": "Andrew Wang"
                    },
                    {
                        "name": "Anna Warno"
                    },
                    {
                        "name": "Abhilash Somasamudramath"
                    },
                    {
                        "name": "Sandip Bhaskar"
                    },
                    {
                        "name": "Maka Dong"
                    },
                    {
                        "name": "Nave Assaf"
                    },
                    {
                        "name": "Shahar Mor"
                    },
                    {
                        "name": "Omer Ullman Argov"
                    },
                    {
                        "name": "Scot Junkin"
                    },
                    {
                        "name": "Oleksandr Romanenko"
                    },
                    {
                        "name": "Pedro Larroy"
                    },
                    {
                        "name": "Monika Katariya"
                    },
                    {
                        "name": "Marco Rovinelli"
                    },
                    {
                        "name": "Viji Balas"
                    },
                    {
                        "name": "Nicholas Edelman"
                    },
                    {
                        "name": "Anahita Bhiwandiwalla"
                    },
                    {
                        "name": "Muthu Subramaniam"
                    },
                    {
                        "name": "Smita Ithape"
                    },
                    {
                        "name": "Karthik Ramamoorthy"
                    },
                    {
                        "name": "Yuting Wu"
                    },
                    {
                        "name": "Suguna Varshini Velury"
                    },
                    {
                        "name": "Omri Almog"
                    },
                    {
                        "name": "Joyjit Daw"
                    },
                    {
                        "name": "Denys Fridman"
                    },
                    {
                        "name": "Erick Galinkin"
                    },
                    {
                        "name": "Michael Evans"
                    },
                    {
                        "name": "Shaona Ghosh"
                    },
                    {
                        "name": "Katherine Luna"
                    },
                    {
                        "name": "Leon Derczynski"
                    },
                    {
                        "name": "Nikki Pope"
                    },
                    {
                        "name": "Eileen Long"
                    },
                    {
                        "name": "Seth Schneider"
                    },
                    {
                        "name": "Guillermo Siman"
                    },
                    {
                        "name": "Tomasz Grzegorzek"
                    },
                    {
                        "name": "Pablo Ribalta"
                    },
                    {
                        "name": "Monika Katariya"
                    },
                    {
                        "name": "Chris Alexiuk"
                    },
                    {
                        "name": "Joey Conway"
                    },
                    {
                        "name": "Trisha Saar"
                    },
                    {
                        "name": "Ann Guan"
                    },
                    {
                        "name": "Krzysztof Pawelec"
                    },
                    {
                        "name": "Shyamala Prayaga"
                    },
                    {
                        "name": "Oleksii Kuchaiev"
                    },
                    {
                        "name": "Boris Ginsburg"
                    },
                    {
                        "name": "Oluwatobi Olabiyi"
                    },
                    {
                        "name": "Kari Briski"
                    },
                    {
                        "name": "Jonathan Cohen"
                    },
                    {
                        "name": "Bryan Catanzaro"
                    },
                    {
                        "name": "Jonah Alben"
                    },
                    {
                        "name": "Yonatan Geifman"
                    },
                    {
                        "name": "Eric Chung"
                    }
                ],
                "author_detail": {
                    "name": "Eric Chung"
                },
                "author": "Eric Chung",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00949v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00949v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.09304v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.09304v2",
                "updated": "2025-05-14T16:12:55Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    16,
                    12,
                    55,
                    2,
                    134,
                    0
                ],
                "published": "2024-07-12T14:37:58Z",
                "published_parsed": [
                    2024,
                    7,
                    12,
                    14,
                    37,
                    58,
                    4,
                    194,
                    0
                ],
                "title": "Criticality-amplified quantum probing of a spontaneous collapse model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Criticality-amplified quantum probing of a spontaneous collapse model"
                },
                "summary": "Spontaneous collapse models, which are phenomenological mechanisms introduced\nand designed to account for dynamical wavepacket reduction, are attracting a\ngrowing interest from the community interested in the characterisation of the\nquantum-to-classical transition. Here, we introduce a quantum-probing approach\nto the quest of deriving metrological upper bounds on the free parameters of\nsuch empirical models. To illustrate our approach, we consider an extended\nquantum Ising chain whose elements are -- either individually or collectively\n-- affected by a mechanism responsible for spontaneous collapse. We explore\nconfigurations involving out-of-equilibrium states of the chain, which allows\nus to infer information about the collapse mechanism before it is completely\nscrambled from the state of the system. Moreover, we investigate potential\namplification effects on the probing performance based on the exploitation of\nquantum criticality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spontaneous collapse models, which are phenomenological mechanisms introduced\nand designed to account for dynamical wavepacket reduction, are attracting a\ngrowing interest from the community interested in the characterisation of the\nquantum-to-classical transition. Here, we introduce a quantum-probing approach\nto the quest of deriving metrological upper bounds on the free parameters of\nsuch empirical models. To illustrate our approach, we consider an extended\nquantum Ising chain whose elements are -- either individually or collectively\n-- affected by a mechanism responsible for spontaneous collapse. We explore\nconfigurations involving out-of-equilibrium states of the chain, which allows\nus to infer information about the collapse mechanism before it is completely\nscrambled from the state of the system. Moreover, we investigate potential\namplification effects on the probing performance based on the exploitation of\nquantum criticality."
                },
                "authors": [
                    {
                        "name": "Giorgio Zicari"
                    },
                    {
                        "name": "Matteo Carlesso"
                    },
                    {
                        "name": "Andrea Trombettoni"
                    },
                    {
                        "name": "Mauro Paternostro"
                    }
                ],
                "author_detail": {
                    "name": "Mauro Paternostro"
                },
                "author": "Mauro Paternostro",
                "arxiv_comment": "13 pages, 8 figures. Comments welcome!",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.09304v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.09304v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05235v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05235v2",
                "updated": "2025-05-14T16:12:07Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    16,
                    12,
                    7,
                    2,
                    134,
                    0
                ],
                "published": "2025-04-07T16:19:50Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    16,
                    19,
                    50,
                    0,
                    97,
                    0
                ],
                "title": "IAEmu: Learning Galaxy Intrinsic Alignment Correlations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IAEmu: Learning Galaxy Intrinsic Alignment Correlations"
                },
                "summary": "The intrinsic alignments (IA) of galaxies, a key contaminant in weak lensing\nanalyses, arise from correlations in galaxy shapes driven by tidal interactions\nand galaxy formation processes. Accurate IA modeling is essential for robust\ncosmological inference, but current approaches rely on perturbative methods\nthat break down on nonlinear scales or on expensive simulations. We introduce\nIAEmu, a neural network-based emulator that predicts the galaxy\nposition-position ($\\xi$), position-orientation ($\\omega$), and\norientation-orientation ($\\eta$) correlation functions and their uncertainties\nusing mock catalogs based on the halo occupation distribution (HOD) framework.\nCompared to simulations, IAEmu achieves ~3% average error for $\\xi$ and ~5% for\n$\\omega$, while capturing the stochasticity of $\\eta$ without overfitting. The\nemulator provides both aleatoric and epistemic uncertainties, helping identify\nregions where predictions may be less reliable. We also demonstrate\ngeneralization to non-HOD alignment signals by fitting to IllustrisTNG\nhydrodynamical simulation data. As a fully differentiable neural network, IAEmu\nenables $\\sim$10,000$\\times$ speed-ups in mapping HOD parameters to correlation\nfunctions on GPUs, compared to CPU-based simulations. This acceleration\nfacilitates inverse modeling via gradient-based sampling, making IAEmu a\npowerful surrogate model for galaxy bias and IA studies with direct\napplications to Stage IV weak lensing surveys.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The intrinsic alignments (IA) of galaxies, a key contaminant in weak lensing\nanalyses, arise from correlations in galaxy shapes driven by tidal interactions\nand galaxy formation processes. Accurate IA modeling is essential for robust\ncosmological inference, but current approaches rely on perturbative methods\nthat break down on nonlinear scales or on expensive simulations. We introduce\nIAEmu, a neural network-based emulator that predicts the galaxy\nposition-position ($\\xi$), position-orientation ($\\omega$), and\norientation-orientation ($\\eta$) correlation functions and their uncertainties\nusing mock catalogs based on the halo occupation distribution (HOD) framework.\nCompared to simulations, IAEmu achieves ~3% average error for $\\xi$ and ~5% for\n$\\omega$, while capturing the stochasticity of $\\eta$ without overfitting. The\nemulator provides both aleatoric and epistemic uncertainties, helping identify\nregions where predictions may be less reliable. We also demonstrate\ngeneralization to non-HOD alignment signals by fitting to IllustrisTNG\nhydrodynamical simulation data. As a fully differentiable neural network, IAEmu\nenables $\\sim$10,000$\\times$ speed-ups in mapping HOD parameters to correlation\nfunctions on GPUs, compared to CPU-based simulations. This acceleration\nfacilitates inverse modeling via gradient-based sampling, making IAEmu a\npowerful surrogate model for galaxy bias and IA studies with direct\napplications to Stage IV weak lensing surveys."
                },
                "authors": [
                    {
                        "name": "Sneh Pandya"
                    },
                    {
                        "name": "Yuanyuan Yang"
                    },
                    {
                        "name": "Nicholas Van Alfen"
                    },
                    {
                        "name": "Jonathan Blazek"
                    },
                    {
                        "name": "Robin Walters"
                    }
                ],
                "author_detail": {
                    "name": "Robin Walters"
                },
                "author": "Robin Walters",
                "arxiv_comment": "16 pages, 10 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05235v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05235v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.06738v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.06738v2",
                "updated": "2025-05-14T16:04:57Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    16,
                    4,
                    57,
                    2,
                    134,
                    0
                ],
                "published": "2025-05-10T19:06:37Z",
                "published_parsed": [
                    2025,
                    5,
                    10,
                    19,
                    6,
                    37,
                    5,
                    130,
                    0
                ],
                "title": "I Know What You Said: Unveiling Hardware Cache Side-Channels in Local\n  Large Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "I Know What You Said: Unveiling Hardware Cache Side-Channels in Local\n  Large Language Model Inference"
                },
                "summary": "Large Language Models (LLMs) that can be deployed locally have recently\ngained popularity for privacy-sensitive tasks, with companies such as Meta,\nGoogle, and Intel playing significant roles in their development. However, the\nsecurity of local LLMs through the lens of hardware cache side-channels remains\nunexplored. In this paper, we unveil novel side-channel vulnerabilities in\nlocal LLM inference: token value and token position leakage, which can expose\nboth the victim's input and output text, thereby compromising user privacy.\nSpecifically, we found that adversaries can infer the token values from the\ncache access patterns of the token embedding operation, and deduce the token\npositions from the timing of autoregressive decoding phases. To demonstrate the\npotential of these leaks, we design a novel eavesdropping attack framework\ntargeting both open-source and proprietary LLM inference systems. The attack\nframework does not directly interact with the victim's LLM and can be executed\nwithout privilege.\n  We evaluate the attack on a range of practical local LLM deployments (e.g.,\nLlama, Falcon, and Gemma), and the results show that our attack achieves\npromising accuracy. The restored output and input text have an average edit\ndistance of 5.2% and 17.3% to the ground truth, respectively. Furthermore, the\nreconstructed texts achieve average cosine similarity scores of 98.7% (input)\nand 98.0% (output).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) that can be deployed locally have recently\ngained popularity for privacy-sensitive tasks, with companies such as Meta,\nGoogle, and Intel playing significant roles in their development. However, the\nsecurity of local LLMs through the lens of hardware cache side-channels remains\nunexplored. In this paper, we unveil novel side-channel vulnerabilities in\nlocal LLM inference: token value and token position leakage, which can expose\nboth the victim's input and output text, thereby compromising user privacy.\nSpecifically, we found that adversaries can infer the token values from the\ncache access patterns of the token embedding operation, and deduce the token\npositions from the timing of autoregressive decoding phases. To demonstrate the\npotential of these leaks, we design a novel eavesdropping attack framework\ntargeting both open-source and proprietary LLM inference systems. The attack\nframework does not directly interact with the victim's LLM and can be executed\nwithout privilege.\n  We evaluate the attack on a range of practical local LLM deployments (e.g.,\nLlama, Falcon, and Gemma), and the results show that our attack achieves\npromising accuracy. The restored output and input text have an average edit\ndistance of 5.2% and 17.3% to the ground truth, respectively. Furthermore, the\nreconstructed texts achieve average cosine similarity scores of 98.7% (input)\nand 98.0% (output)."
                },
                "authors": [
                    {
                        "name": "Zibo Gao"
                    },
                    {
                        "name": "Junjie Hu"
                    },
                    {
                        "name": "Feng Guo"
                    },
                    {
                        "name": "Yixin Zhang"
                    },
                    {
                        "name": "Yinglong Han"
                    },
                    {
                        "name": "Siyuan Liu"
                    },
                    {
                        "name": "Haiyang Li"
                    },
                    {
                        "name": "Zhiqiang Lv"
                    }
                ],
                "author_detail": {
                    "name": "Zhiqiang Lv"
                },
                "author": "Zhiqiang Lv",
                "arxiv_comment": "Submitted for review in January 22, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.06738v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.06738v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "K.6.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09500v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09500v1",
                "updated": "2025-05-14T15:50:45Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    15,
                    50,
                    45,
                    2,
                    134,
                    0
                ],
                "published": "2025-05-14T15:50:45Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    15,
                    50,
                    45,
                    2,
                    134,
                    0
                ],
                "title": "Layered Unlearning for Adversarial Relearning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Layered Unlearning for Adversarial Relearning"
                },
                "summary": "Our goal is to understand how post-training methods, such as fine-tuning,\nalignment, and unlearning, modify language model behavior and representations.\nWe are particularly interested in the brittle nature of these modifications\nthat makes them easy to bypass through prompt engineering or relearning. Recent\nresults suggest that post-training induces shallow context-dependent\n``circuits'' that suppress specific response patterns. This could be one\nexplanation for the brittleness of post-training. To test this hypothesis, we\ndesign an unlearning algorithm, Layered Unlearning (LU), that creates distinct\ninhibitory mechanisms for a growing subset of the data. By unlearning the first\n$i$ folds while retaining the remaining $k - i$ at the $i$th of $k$ stages, LU\nlimits the ability of relearning on a subset of data to recover the full\ndataset. We evaluate LU through a combination of synthetic and large language\nmodel (LLM) experiments. We find that LU improves robustness to adversarial\nrelearning for several different unlearning methods. Our results contribute to\nthe state-of-the-art of machine unlearning and provide insight into the effect\nof post-training updates.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Our goal is to understand how post-training methods, such as fine-tuning,\nalignment, and unlearning, modify language model behavior and representations.\nWe are particularly interested in the brittle nature of these modifications\nthat makes them easy to bypass through prompt engineering or relearning. Recent\nresults suggest that post-training induces shallow context-dependent\n``circuits'' that suppress specific response patterns. This could be one\nexplanation for the brittleness of post-training. To test this hypothesis, we\ndesign an unlearning algorithm, Layered Unlearning (LU), that creates distinct\ninhibitory mechanisms for a growing subset of the data. By unlearning the first\n$i$ folds while retaining the remaining $k - i$ at the $i$th of $k$ stages, LU\nlimits the ability of relearning on a subset of data to recover the full\ndataset. We evaluate LU through a combination of synthetic and large language\nmodel (LLM) experiments. We find that LU improves robustness to adversarial\nrelearning for several different unlearning methods. Our results contribute to\nthe state-of-the-art of machine unlearning and provide insight into the effect\nof post-training updates."
                },
                "authors": [
                    {
                        "name": "Timothy Qian"
                    },
                    {
                        "name": "Vinith Suriyakumar"
                    },
                    {
                        "name": "Ashia Wilson"
                    },
                    {
                        "name": "Dylan Hadfield-Menell"
                    }
                ],
                "author_detail": {
                    "name": "Dylan Hadfield-Menell"
                },
                "author": "Dylan Hadfield-Menell",
                "arxiv_comment": "37 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09500v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09500v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09478v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09478v1",
                "updated": "2025-05-14T15:29:15Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    15,
                    29,
                    15,
                    2,
                    134,
                    0
                ],
                "published": "2025-05-14T15:29:15Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    15,
                    29,
                    15,
                    2,
                    134,
                    0
                ],
                "title": "Card Sorting Simulator: Augmenting Design of Logical Information\n  Architectures with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Card Sorting Simulator: Augmenting Design of Logical Information\n  Architectures with Large Language Models"
                },
                "summary": "Card sorting is a common ideation technique that elicits information on\nusers' mental organization of content and functionality by having them sort\nitems into categories. For more robust card sorting research, digital card\nsorting tools could benefit from providing quick automated feedback. Our\nobjective of this research is to advance toward an instrument that applies\nartificial intelligence (AI) to augment card sorting. For this purpose, we\ndevelop the Card Sorting Simulator, a prototype tool that leverages Large\nLanguage Models (LLMs) to generate informative categorizations of cards. To\nilluminate how aligned the simulation is with card sorting by actual\nparticipants, and to inform the instrument's design decisions, we conducted a\ngeneralizability-focused comparative study. We obtained 28 pre-existing card\nsorting studies from real practitioners, comprising 1,399 participants, along\nwith diverse contents and origins. With this dataset, we conducted a\ncomprehensive and nuanced analysis of the agreement between actual card sorting\nresults (clusterings of cards) and synthetic clusterings across a multitude of\nLLMs and prompt designs. Mutual information scores indicate a good degree of\nagreement to real result clustering, although similarity matrices also\ndemonstrate inconsistencies from mental models, which can be attributed to\ntheir top-down nature. Furthermore, the number of cards or complexity of their\nlabels impact the accuracy of its simulation. These findings bolster the case\nfor AI augmentation in card sorting research as a source of meaningful\npreliminary feedback and highlight the need for further study for the\ndevelopment and validation of intelligent user research tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Card sorting is a common ideation technique that elicits information on\nusers' mental organization of content and functionality by having them sort\nitems into categories. For more robust card sorting research, digital card\nsorting tools could benefit from providing quick automated feedback. Our\nobjective of this research is to advance toward an instrument that applies\nartificial intelligence (AI) to augment card sorting. For this purpose, we\ndevelop the Card Sorting Simulator, a prototype tool that leverages Large\nLanguage Models (LLMs) to generate informative categorizations of cards. To\nilluminate how aligned the simulation is with card sorting by actual\nparticipants, and to inform the instrument's design decisions, we conducted a\ngeneralizability-focused comparative study. We obtained 28 pre-existing card\nsorting studies from real practitioners, comprising 1,399 participants, along\nwith diverse contents and origins. With this dataset, we conducted a\ncomprehensive and nuanced analysis of the agreement between actual card sorting\nresults (clusterings of cards) and synthetic clusterings across a multitude of\nLLMs and prompt designs. Mutual information scores indicate a good degree of\nagreement to real result clustering, although similarity matrices also\ndemonstrate inconsistencies from mental models, which can be attributed to\ntheir top-down nature. Furthermore, the number of cards or complexity of their\nlabels impact the accuracy of its simulation. These findings bolster the case\nfor AI augmentation in card sorting research as a source of meaningful\npreliminary feedback and highlight the need for further study for the\ndevelopment and validation of intelligent user research tools."
                },
                "authors": [
                    {
                        "name": "Eduard Kuric"
                    },
                    {
                        "name": "Peter Demcak"
                    },
                    {
                        "name": "Matus Krajcovic"
                    }
                ],
                "author_detail": {
                    "name": "Matus Krajcovic"
                },
                "author": "Matus Krajcovic",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09478v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09478v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09477v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09477v1",
                "updated": "2025-05-14T15:28:43Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    15,
                    28,
                    43,
                    2,
                    134,
                    0
                ],
                "published": "2025-05-14T15:28:43Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    15,
                    28,
                    43,
                    2,
                    134,
                    0
                ],
                "title": "Deploying Foundation Model-Enabled Air and Ground Robots in the Field:\n  Challenges and Opportunities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying Foundation Model-Enabled Air and Ground Robots in the Field:\n  Challenges and Opportunities"
                },
                "summary": "The integration of foundation models (FMs) into robotics has enabled robots\nto understand natural language and reason about the semantics in their\nenvironments. However, existing FM-enabled robots primary operate in\nclosed-world settings, where the robot is given a full prior map or has a full\nview of its workspace. This paper addresses the deployment of FM-enabled robots\nin the field, where missions often require a robot to operate in large-scale\nand unstructured environments. To effectively accomplish these missions, robots\nmust actively explore their environments, navigate obstacle-cluttered terrain,\nhandle unexpected sensor inputs, and operate with compute constraints. We\ndiscuss recent deployments of SPINE, our LLM-enabled autonomy framework, in\nfield robotic settings. To the best of our knowledge, we present the first\ndemonstration of large-scale LLM-enabled robot planning in unstructured\nenvironments with several kilometers of missions. SPINE is agnostic to a\nparticular LLM, which allows us to distill small language models capable of\nrunning onboard size, weight and power (SWaP) limited platforms. Via\npreliminary model distillation work, we then present the first language-driven\nUAV planner using on-device language models. We conclude our paper by proposing\nseveral promising directions for future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of foundation models (FMs) into robotics has enabled robots\nto understand natural language and reason about the semantics in their\nenvironments. However, existing FM-enabled robots primary operate in\nclosed-world settings, where the robot is given a full prior map or has a full\nview of its workspace. This paper addresses the deployment of FM-enabled robots\nin the field, where missions often require a robot to operate in large-scale\nand unstructured environments. To effectively accomplish these missions, robots\nmust actively explore their environments, navigate obstacle-cluttered terrain,\nhandle unexpected sensor inputs, and operate with compute constraints. We\ndiscuss recent deployments of SPINE, our LLM-enabled autonomy framework, in\nfield robotic settings. To the best of our knowledge, we present the first\ndemonstration of large-scale LLM-enabled robot planning in unstructured\nenvironments with several kilometers of missions. SPINE is agnostic to a\nparticular LLM, which allows us to distill small language models capable of\nrunning onboard size, weight and power (SWaP) limited platforms. Via\npreliminary model distillation work, we then present the first language-driven\nUAV planner using on-device language models. We conclude our paper by proposing\nseveral promising directions for future research."
                },
                "authors": [
                    {
                        "name": "Zachary Ravichandran"
                    },
                    {
                        "name": "Fernando Cladera"
                    },
                    {
                        "name": "Jason Hughes"
                    },
                    {
                        "name": "Varun Murali"
                    },
                    {
                        "name": "M. Ani Hsieh"
                    },
                    {
                        "name": "George J. Pappas"
                    },
                    {
                        "name": "Camillo J. Taylor"
                    },
                    {
                        "name": "Vijay Kumar"
                    }
                ],
                "author_detail": {
                    "name": "Vijay Kumar"
                },
                "author": "Vijay Kumar",
                "arxiv_comment": "Accepted to the IEEE ICRA Workshop on Field Robotics 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09477v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09477v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11523v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11523v2",
                "updated": "2025-05-14T15:19:54Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    15,
                    19,
                    54,
                    2,
                    134,
                    0
                ],
                "published": "2024-12-16T07:59:23Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    7,
                    59,
                    23,
                    0,
                    351,
                    0
                ],
                "title": "ON as ALC: Active Loop Closing Object Goal Navigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ON as ALC: Active Loop Closing Object Goal Navigation"
                },
                "summary": "In simultaneous localization and mapping, active loop closing (ALC) is an\nactive vision problem that aims to visually guide a robot to maximize the\nchances of revisiting previously visited points, thereby resetting the drift\nerrors accumulated in the incrementally built map during travel. However,\ncurrent mainstream navigation strategies that leverage such incomplete maps as\nworkspace prior knowledge often fail in modern long-term autonomy long-distance\ntravel scenarios where map accumulation errors become significant. To address\nthese limitations of map-based navigation, this paper is the first to explore\nmapless navigation in the embodied AI field, in particular, to utilize\nobject-goal navigation (commonly abbreviated as ON, ObjNav, or OGN) techniques\nthat efficiently explore target objects without using such a prior map.\nSpecifically, in this work, we start from an off-the-shelf mapless ON planner,\nextend it to utilize a prior map, and further show that the performance in\nlong-distance ALC (LD-ALC) can be maximized by minimizing ``ALC loss\" and ``ON\nloss\". This study highlights a simple and effective approach, called ALC-ON\n(ALCON), to accelerate the progress of challenging long-distance ALC technology\nby leveraging the growing frontier-guided, data-driven, and LLM-guided ON\ntechnologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In simultaneous localization and mapping, active loop closing (ALC) is an\nactive vision problem that aims to visually guide a robot to maximize the\nchances of revisiting previously visited points, thereby resetting the drift\nerrors accumulated in the incrementally built map during travel. However,\ncurrent mainstream navigation strategies that leverage such incomplete maps as\nworkspace prior knowledge often fail in modern long-term autonomy long-distance\ntravel scenarios where map accumulation errors become significant. To address\nthese limitations of map-based navigation, this paper is the first to explore\nmapless navigation in the embodied AI field, in particular, to utilize\nobject-goal navigation (commonly abbreviated as ON, ObjNav, or OGN) techniques\nthat efficiently explore target objects without using such a prior map.\nSpecifically, in this work, we start from an off-the-shelf mapless ON planner,\nextend it to utilize a prior map, and further show that the performance in\nlong-distance ALC (LD-ALC) can be maximized by minimizing ``ALC loss\" and ``ON\nloss\". This study highlights a simple and effective approach, called ALC-ON\n(ALCON), to accelerate the progress of challenging long-distance ALC technology\nby leveraging the growing frontier-guided, data-driven, and LLM-guided ON\ntechnologies."
                },
                "authors": [
                    {
                        "name": "Daiki Iwata"
                    },
                    {
                        "name": "Kanji Tanaka"
                    },
                    {
                        "name": "Shoya Miyazaki"
                    },
                    {
                        "name": "Kouki Terashima"
                    }
                ],
                "author_detail": {
                    "name": "Kouki Terashima"
                },
                "author": "Kouki Terashima",
                "arxiv_comment": "Draft version of a conference paper with 7 pages, 5 figures, and 1\n  table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11523v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11523v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03390v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03390v2",
                "updated": "2025-05-14T15:13:39Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    15,
                    13,
                    39,
                    2,
                    134,
                    0
                ],
                "published": "2025-04-04T12:03:11Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    12,
                    3,
                    11,
                    4,
                    94,
                    0
                ],
                "title": "Eigen-inference by Marchenko-Pastur inversion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Eigen-inference by Marchenko-Pastur inversion"
                },
                "summary": "A new formula for Marchenko-Pastur inversion is derived and used for\ninference of population linear spectral statistics. The formula allows for fast\nand accurate estimation of the Stieltjes transform of the population spectral\ndistribution $s_H(z)$, when $z$ is sufficiently far from the support of the\npopulation spectral distribution $H$. If the dimension $d$ and the sample size\n$n$ go to infinity simultaneously such that $\\frac{d}{n} \\rightarrow c>0$, the\nestimation error is shown to be asymptotically less than\n$\\frac{n^{\\varepsilon}}{n}$ for arbitrary $\\varepsilon > 0$. By integrating\nalong a curve around the support of $H$, estimators for population linear\nspectral statistics are constructed, which benefit from this convergence speed\nof $\\frac{n^{\\varepsilon}}{n}$.\n  The new method of estimating the Stieltjes transforms $s_H(z)$ is also\napplied to the numerical construction of estimators for the population\neigenvalues, which in a simulation study are demonstrated to outperform\nstate-of-the-art Ledoit-Wolf estimators.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A new formula for Marchenko-Pastur inversion is derived and used for\ninference of population linear spectral statistics. The formula allows for fast\nand accurate estimation of the Stieltjes transform of the population spectral\ndistribution $s_H(z)$, when $z$ is sufficiently far from the support of the\npopulation spectral distribution $H$. If the dimension $d$ and the sample size\n$n$ go to infinity simultaneously such that $\\frac{d}{n} \\rightarrow c>0$, the\nestimation error is shown to be asymptotically less than\n$\\frac{n^{\\varepsilon}}{n}$ for arbitrary $\\varepsilon > 0$. By integrating\nalong a curve around the support of $H$, estimators for population linear\nspectral statistics are constructed, which benefit from this convergence speed\nof $\\frac{n^{\\varepsilon}}{n}$.\n  The new method of estimating the Stieltjes transforms $s_H(z)$ is also\napplied to the numerical construction of estimators for the population\neigenvalues, which in a simulation study are demonstrated to outperform\nstate-of-the-art Ledoit-Wolf estimators."
                },
                "authors": [
                    {
                        "name": "Ben Deitmar"
                    }
                ],
                "author_detail": {
                    "name": "Ben Deitmar"
                },
                "author": "Ben Deitmar",
                "arxiv_comment": "44 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03390v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03390v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09450v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09450v1",
                "updated": "2025-05-14T15:01:59Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    15,
                    1,
                    59,
                    2,
                    134,
                    0
                ],
                "published": "2025-05-14T15:01:59Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    15,
                    1,
                    59,
                    2,
                    134,
                    0
                ],
                "title": "MrTrack: Register Mamba for Needle Tracking with Rapid Reciprocating\n  Motion during Ultrasound-Guided Aspiration Biopsy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MrTrack: Register Mamba for Needle Tracking with Rapid Reciprocating\n  Motion during Ultrasound-Guided Aspiration Biopsy"
                },
                "summary": "Ultrasound-guided fine needle aspiration (FNA) biopsy is a common minimally\ninvasive diagnostic procedure. However, an aspiration needle tracker addressing\nrapid reciprocating motion is still missing. MrTrack, an aspiration needle\ntracker with a mamba-based register mechanism, is proposed. MrTrack leverages a\nMamba-based register extractor to sequentially distill global context from each\nhistorical search map, storing these temporal cues in a register bank. The\nMamba-based register retriever then retrieves temporal prompts from the\nregister bank to provide external cues when current vision features are\ntemporarily unusable due to rapid reciprocating motion and imaging degradation.\nA self-supervised register diversify loss is proposed to encourage feature\ndiversity and dimension independence within the learned register, mitigating\nfeature collapse. Comprehensive experiments conducted on both motorized and\nmanual aspiration datasets demonstrate that MrTrack not only outperforms\nstate-of-the-art trackers in accuracy and robustness but also achieves superior\ninference efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ultrasound-guided fine needle aspiration (FNA) biopsy is a common minimally\ninvasive diagnostic procedure. However, an aspiration needle tracker addressing\nrapid reciprocating motion is still missing. MrTrack, an aspiration needle\ntracker with a mamba-based register mechanism, is proposed. MrTrack leverages a\nMamba-based register extractor to sequentially distill global context from each\nhistorical search map, storing these temporal cues in a register bank. The\nMamba-based register retriever then retrieves temporal prompts from the\nregister bank to provide external cues when current vision features are\ntemporarily unusable due to rapid reciprocating motion and imaging degradation.\nA self-supervised register diversify loss is proposed to encourage feature\ndiversity and dimension independence within the learned register, mitigating\nfeature collapse. Comprehensive experiments conducted on both motorized and\nmanual aspiration datasets demonstrate that MrTrack not only outperforms\nstate-of-the-art trackers in accuracy and robustness but also achieves superior\ninference efficiency."
                },
                "authors": [
                    {
                        "name": "Yuelin Zhang"
                    },
                    {
                        "name": "Qingpeng Ding"
                    },
                    {
                        "name": "Long Lei"
                    },
                    {
                        "name": "Yongxuan Feng"
                    },
                    {
                        "name": "Raymond Shing-Yan Tang"
                    },
                    {
                        "name": "Shing Shin Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Shing Shin Cheng"
                },
                "author": "Shing Shin Cheng",
                "arxiv_comment": "Early Accepted by MICCAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09450v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09450v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09439v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09439v1",
                "updated": "2025-05-14T14:47:16Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    14,
                    47,
                    16,
                    2,
                    134,
                    0
                ],
                "published": "2025-05-14T14:47:16Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    14,
                    47,
                    16,
                    2,
                    134,
                    0
                ],
                "title": "Omni-R1: Do You Really Need Audio to Fine-Tune Your Audio LLM?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Omni-R1: Do You Really Need Audio to Fine-Tune Your Audio LLM?"
                },
                "summary": "We propose Omni-R1 which fine-tunes a recent multi-modal LLM, Qwen2.5-Omni,\non an audio question answering dataset with the reinforcement learning method\nGRPO. This leads to new State-of-the-Art performance on the recent MMAU\nbenchmark. Omni-R1 achieves the highest accuracies on the sounds, music,\nspeech, and overall average categories, both on the Test-mini and Test-full\nsplits. To understand the performance improvement, we tested models both with\nand without audio and found that much of the performance improvement from GRPO\ncould be attributed to better text-based reasoning. We also made a surprising\ndiscovery that fine-tuning without audio on a text-only dataset was effective\nat improving the audio-based performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose Omni-R1 which fine-tunes a recent multi-modal LLM, Qwen2.5-Omni,\non an audio question answering dataset with the reinforcement learning method\nGRPO. This leads to new State-of-the-Art performance on the recent MMAU\nbenchmark. Omni-R1 achieves the highest accuracies on the sounds, music,\nspeech, and overall average categories, both on the Test-mini and Test-full\nsplits. To understand the performance improvement, we tested models both with\nand without audio and found that much of the performance improvement from GRPO\ncould be attributed to better text-based reasoning. We also made a surprising\ndiscovery that fine-tuning without audio on a text-only dataset was effective\nat improving the audio-based performance."
                },
                "authors": [
                    {
                        "name": "Andrew Rouditchenko"
                    },
                    {
                        "name": "Saurabhchand Bhati"
                    },
                    {
                        "name": "Edson Araujo"
                    },
                    {
                        "name": "Samuel Thomas"
                    },
                    {
                        "name": "Hilde Kuehne"
                    },
                    {
                        "name": "Rogerio Feris"
                    },
                    {
                        "name": "James Glass"
                    }
                ],
                "author_detail": {
                    "name": "James Glass"
                },
                "author": "James Glass",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09439v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09439v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09438v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09438v1",
                "updated": "2025-05-14T14:46:32Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    14,
                    46,
                    32,
                    2,
                    134,
                    0
                ],
                "published": "2025-05-14T14:46:32Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    14,
                    46,
                    32,
                    2,
                    134,
                    0
                ],
                "title": "Evaluating GPT- and Reasoning-based Large Language Models on Physics\n  Olympiad Problems: Surpassing Human Performance and Implications for\n  Educational Assessment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating GPT- and Reasoning-based Large Language Models on Physics\n  Olympiad Problems: Surpassing Human Performance and Implications for\n  Educational Assessment"
                },
                "summary": "Large language models (LLMs) are now widely accessible, reaching learners at\nall educational levels. This development has raised concerns that their use may\ncircumvent essential learning processes and compromise the integrity of\nestablished assessment formats. In physics education, where problem solving\nplays a central role in instruction and assessment, it is therefore essential\nto understand the physics-specific problem-solving capabilities of LLMs. Such\nunderstanding is key to informing responsible and pedagogically sound\napproaches to integrating LLMs into instruction and assessment. This study\ntherefore compares the problem-solving performance of a general-purpose LLM\n(GPT-4o, using varying prompting techniques) and a reasoning-optimized model\n(o1-preview) with that of participants of the German Physics Olympiad, based on\na set of well-defined Olympiad problems. In addition to evaluating the\ncorrectness of the generated solutions, the study analyzes characteristic\nstrengths and limitations of LLM-generated solutions. The findings of this\nstudy indicate that both tested LLMs (GPT-4o and o1-preview) demonstrate\nadvanced problem-solving capabilities on Olympiad-type physics problems, on\naverage outperforming the human participants. Prompting techniques had little\neffect on GPT-4o's performance, while o1-preview almost consistently\noutperformed both GPT-4o and the human benchmark. Based on these findings, the\nstudy discusses implications for the design of summative and formative\nassessment in physics education, including how to uphold assessment integrity\nand support students in critically engaging with LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are now widely accessible, reaching learners at\nall educational levels. This development has raised concerns that their use may\ncircumvent essential learning processes and compromise the integrity of\nestablished assessment formats. In physics education, where problem solving\nplays a central role in instruction and assessment, it is therefore essential\nto understand the physics-specific problem-solving capabilities of LLMs. Such\nunderstanding is key to informing responsible and pedagogically sound\napproaches to integrating LLMs into instruction and assessment. This study\ntherefore compares the problem-solving performance of a general-purpose LLM\n(GPT-4o, using varying prompting techniques) and a reasoning-optimized model\n(o1-preview) with that of participants of the German Physics Olympiad, based on\na set of well-defined Olympiad problems. In addition to evaluating the\ncorrectness of the generated solutions, the study analyzes characteristic\nstrengths and limitations of LLM-generated solutions. The findings of this\nstudy indicate that both tested LLMs (GPT-4o and o1-preview) demonstrate\nadvanced problem-solving capabilities on Olympiad-type physics problems, on\naverage outperforming the human participants. Prompting techniques had little\neffect on GPT-4o's performance, while o1-preview almost consistently\noutperformed both GPT-4o and the human benchmark. Based on these findings, the\nstudy discusses implications for the design of summative and formative\nassessment in physics education, including how to uphold assessment integrity\nand support students in critically engaging with LLMs."
                },
                "authors": [
                    {
                        "name": "Paul Tschisgale"
                    },
                    {
                        "name": "Holger Maus"
                    },
                    {
                        "name": "Fabian Kieser"
                    },
                    {
                        "name": "Ben Kroehs"
                    },
                    {
                        "name": "Stefan Petersen"
                    },
                    {
                        "name": "Peter Wulff"
                    }
                ],
                "author_detail": {
                    "name": "Peter Wulff"
                },
                "author": "Peter Wulff",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09438v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09438v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ed-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ed-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09436v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09436v1",
                "updated": "2025-05-14T14:44:30Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    14,
                    44,
                    30,
                    2,
                    134,
                    0
                ],
                "published": "2025-05-14T14:44:30Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    14,
                    44,
                    30,
                    2,
                    134,
                    0
                ],
                "title": "CXMArena: Unified Dataset to benchmark performance in realistic CXM\n  Scenarios",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CXMArena: Unified Dataset to benchmark performance in realistic CXM\n  Scenarios"
                },
                "summary": "Large Language Models (LLMs) hold immense potential for revolutionizing\nCustomer Experience Management (CXM), particularly in contact center\noperations. However, evaluating their practical utility in complex operational\nenvironments is hindered by data scarcity (due to privacy concerns) and the\nlimitations of current benchmarks. Existing benchmarks often lack realism,\nfailing to incorporate deep knowledge base (KB) integration, real-world noise,\nor critical operational tasks beyond conversational fluency. To bridge this\ngap, we introduce CXMArena, a novel, large-scale synthetic benchmark dataset\nspecifically designed for evaluating AI in operational CXM contexts. Given the\ndiversity in possible contact center features, we have developed a scalable\nLLM-powered pipeline that simulates the brand's CXM entities that form the\nfoundation of our datasets-such as knowledge articles including product\nspecifications, issue taxonomies, and contact center conversations. The\nentities closely represent real-world distribution because of controlled noise\ninjection (informed by domain experts) and rigorous automated validation.\nBuilding on this, we release CXMArena, which provides dedicated benchmarks\ntargeting five important operational tasks: Knowledge Base Refinement, Intent\nPrediction, Agent Quality Adherence, Article Search, and Multi-turn RAG with\nIntegrated Tools. Our baseline experiments underscore the benchmark's\ndifficulty: even state of the art embedding and generation models achieve only\n68% accuracy on article search, while standard embedding methods yield a low F1\nscore of 0.3 for knowledge base refinement, highlighting significant challenges\nfor current models necessitating complex pipelines and solutions over\nconventional techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) hold immense potential for revolutionizing\nCustomer Experience Management (CXM), particularly in contact center\noperations. However, evaluating their practical utility in complex operational\nenvironments is hindered by data scarcity (due to privacy concerns) and the\nlimitations of current benchmarks. Existing benchmarks often lack realism,\nfailing to incorporate deep knowledge base (KB) integration, real-world noise,\nor critical operational tasks beyond conversational fluency. To bridge this\ngap, we introduce CXMArena, a novel, large-scale synthetic benchmark dataset\nspecifically designed for evaluating AI in operational CXM contexts. Given the\ndiversity in possible contact center features, we have developed a scalable\nLLM-powered pipeline that simulates the brand's CXM entities that form the\nfoundation of our datasets-such as knowledge articles including product\nspecifications, issue taxonomies, and contact center conversations. The\nentities closely represent real-world distribution because of controlled noise\ninjection (informed by domain experts) and rigorous automated validation.\nBuilding on this, we release CXMArena, which provides dedicated benchmarks\ntargeting five important operational tasks: Knowledge Base Refinement, Intent\nPrediction, Agent Quality Adherence, Article Search, and Multi-turn RAG with\nIntegrated Tools. Our baseline experiments underscore the benchmark's\ndifficulty: even state of the art embedding and generation models achieve only\n68% accuracy on article search, while standard embedding methods yield a low F1\nscore of 0.3 for knowledge base refinement, highlighting significant challenges\nfor current models necessitating complex pipelines and solutions over\nconventional techniques."
                },
                "authors": [
                    {
                        "name": "Raghav Garg"
                    },
                    {
                        "name": "Kapil Sharma"
                    },
                    {
                        "name": "Karan Gupta"
                    }
                ],
                "author_detail": {
                    "name": "Karan Gupta"
                },
                "author": "Karan Gupta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09436v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09436v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09427v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09427v2",
                "updated": "2025-05-15T07:22:20Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    7,
                    22,
                    20,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-14T14:28:24Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    14,
                    28,
                    24,
                    2,
                    134,
                    0
                ],
                "title": "SafePath: Conformal Prediction for Safe LLM-Based Autonomous Navigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SafePath: Conformal Prediction for Safe LLM-Based Autonomous Navigation"
                },
                "summary": "Large Language Models (LLMs) show growing promise in autonomous driving by\nreasoning over complex traffic scenarios to generate path plans. However, their\ntendencies toward overconfidence, and hallucinations raise critical safety\nconcerns. We introduce SafePath, a modular framework that augments LLM-based\npath planning with formal safety guarantees using conformal prediction.\nSafePath operates in three stages. In the first stage, we use an LLM that\ngenerates a set of diverse candidate paths, exploring possible trajectories\nbased on agent behaviors and environmental cues. In the second stage, SafePath\nfilters out high-risk trajectories while guaranteeing that at least one safe\noption is included with a user-defined probability, through a multiple-choice\nquestion-answering formulation that integrates conformal prediction. In the\nfinal stage, our approach selects the path with the lowest expected collision\nrisk when uncertainty is low or delegates control to a human when uncertainty\nis high. We theoretically prove that SafePath guarantees a safe trajectory with\na user-defined probability, and we show how its human delegation rate can be\ntuned to balance autonomy and safety. Extensive experiments on nuScenes and\nHighway-env show that SafePath reduces planning uncertainty by 77\\% and\ncollision rates by up to 70\\%, demonstrating effectiveness in making LLM-driven\npath planning more safer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) show growing promise in autonomous driving by\nreasoning over complex traffic scenarios to generate path plans. However, their\ntendencies toward overconfidence, and hallucinations raise critical safety\nconcerns. We introduce SafePath, a modular framework that augments LLM-based\npath planning with formal safety guarantees using conformal prediction.\nSafePath operates in three stages. In the first stage, we use an LLM that\ngenerates a set of diverse candidate paths, exploring possible trajectories\nbased on agent behaviors and environmental cues. In the second stage, SafePath\nfilters out high-risk trajectories while guaranteeing that at least one safe\noption is included with a user-defined probability, through a multiple-choice\nquestion-answering formulation that integrates conformal prediction. In the\nfinal stage, our approach selects the path with the lowest expected collision\nrisk when uncertainty is low or delegates control to a human when uncertainty\nis high. We theoretically prove that SafePath guarantees a safe trajectory with\na user-defined probability, and we show how its human delegation rate can be\ntuned to balance autonomy and safety. Extensive experiments on nuScenes and\nHighway-env show that SafePath reduces planning uncertainty by 77\\% and\ncollision rates by up to 70\\%, demonstrating effectiveness in making LLM-driven\npath planning more safer."
                },
                "authors": [
                    {
                        "name": "Achref Doula"
                    },
                    {
                        "name": "Max Mühlhäuser"
                    },
                    {
                        "name": "Alejandro Sanchez Guinea"
                    }
                ],
                "author_detail": {
                    "name": "Alejandro Sanchez Guinea"
                },
                "author": "Alejandro Sanchez Guinea",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09427v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09427v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24293v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24293v2",
                "updated": "2025-05-14T13:59:02Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    13,
                    59,
                    2,
                    2,
                    134,
                    0
                ],
                "published": "2025-03-31T16:41:16Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    16,
                    41,
                    16,
                    0,
                    90,
                    0
                ],
                "title": "Is analogy enough to draw novel adjective-noun inferences?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is analogy enough to draw novel adjective-noun inferences?"
                },
                "summary": "Recent work (Ross et al., 2025, 2024) has argued that the ability of humans\nand LLMs respectively to generalize to novel adjective-noun combinations shows\nthat they each have access to a compositional mechanism to determine the\nphrase's meaning and derive inferences. We study whether these inferences can\ninstead be derived by analogy to known inferences, without need for\ncomposition. We investigate this by (1) building a model of analogical\nreasoning using similarity over lexical items, and (2) asking human\nparticipants to reason by analogy. While we find that this strategy works well\nfor a large proportion of the dataset of Ross et al. (2025), there are novel\ncombinations for which both humans and LLMs derive convergent inferences but\nwhich are not well handled by analogy. We thus conclude that the mechanism\nhumans and LLMs use to generalize in these cases cannot be fully reduced to\nanalogy, and likely involves composition.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent work (Ross et al., 2025, 2024) has argued that the ability of humans\nand LLMs respectively to generalize to novel adjective-noun combinations shows\nthat they each have access to a compositional mechanism to determine the\nphrase's meaning and derive inferences. We study whether these inferences can\ninstead be derived by analogy to known inferences, without need for\ncomposition. We investigate this by (1) building a model of analogical\nreasoning using similarity over lexical items, and (2) asking human\nparticipants to reason by analogy. While we find that this strategy works well\nfor a large proportion of the dataset of Ross et al. (2025), there are novel\ncombinations for which both humans and LLMs derive convergent inferences but\nwhich are not well handled by analogy. We thus conclude that the mechanism\nhumans and LLMs use to generalize in these cases cannot be fully reduced to\nanalogy, and likely involves composition."
                },
                "authors": [
                    {
                        "name": "Hayley Ross"
                    },
                    {
                        "name": "Kathryn Davidson"
                    },
                    {
                        "name": "Najoung Kim"
                    }
                ],
                "author_detail": {
                    "name": "Najoung Kim"
                },
                "author": "Najoung Kim",
                "arxiv_comment": "9 pages (17 pages with appendix). Accepted to SCiL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24293v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24293v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10199v7",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10199v7",
                "updated": "2025-05-14T13:55:50Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    13,
                    55,
                    50,
                    2,
                    134,
                    0
                ],
                "published": "2025-03-13T09:34:33Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    9,
                    34,
                    33,
                    3,
                    72,
                    0
                ],
                "title": "Optimal Estimation and Uncertainty Quantification for Stochastic Inverse\n  Problems via Variational Bayesian Methods",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimal Estimation and Uncertainty Quantification for Stochastic Inverse\n  Problems via Variational Bayesian Methods"
                },
                "summary": "The Bayesian inversion method demonstrates significant potential for solving\ninverse problems, enabling both point estimation and uncertainty\nquantification. However, Bayesian maximum a posteriori (MAP) estimation may\nbecome unstable when handling data from diverse distributions (e.g., solutions\nof stochastic partial differential equations (SPDEs)). Additionally, Monte\nCarlo sampling methods are computationally expensive. To address these\nchallenges, we propose a novel two-stage optimization method based on optimal\ncontrol theory and variational Bayesian methods. This method not only achieves\nstable solutions for stochastic inverse problems but also efficiently\nquantifies the uncertainty of the solutions. In the first stage, we introduce a\nnew weighting formulation to ensure the stability of the Bayesian MAP\nestimation. In the second stage, we derive the necessary condition to\nefficiently quantify the uncertainty of the solutions, by combining the new\nweighting formula with variational inference. Furthermore, we establish an\nerror estimation theorem that relates the exact solution to the optimally\nestimated solution under different amounts of observed data. Finally, the\nefficiency of the proposed method is demonstrated through numerical examples.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Bayesian inversion method demonstrates significant potential for solving\ninverse problems, enabling both point estimation and uncertainty\nquantification. However, Bayesian maximum a posteriori (MAP) estimation may\nbecome unstable when handling data from diverse distributions (e.g., solutions\nof stochastic partial differential equations (SPDEs)). Additionally, Monte\nCarlo sampling methods are computationally expensive. To address these\nchallenges, we propose a novel two-stage optimization method based on optimal\ncontrol theory and variational Bayesian methods. This method not only achieves\nstable solutions for stochastic inverse problems but also efficiently\nquantifies the uncertainty of the solutions. In the first stage, we introduce a\nnew weighting formulation to ensure the stability of the Bayesian MAP\nestimation. In the second stage, we derive the necessary condition to\nefficiently quantify the uncertainty of the solutions, by combining the new\nweighting formula with variational inference. Furthermore, we establish an\nerror estimation theorem that relates the exact solution to the optimally\nestimated solution under different amounts of observed data. Finally, the\nefficiency of the proposed method is demonstrated through numerical examples."
                },
                "authors": [
                    {
                        "name": "Ruibiao Song"
                    },
                    {
                        "name": "Liying Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Liying Zhang"
                },
                "author": "Liying Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10199v7",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10199v7",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.NA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09396v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09396v1",
                "updated": "2025-05-14T13:51:24Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    13,
                    51,
                    24,
                    2,
                    134,
                    0
                ],
                "published": "2025-05-14T13:51:24Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    13,
                    51,
                    24,
                    2,
                    134,
                    0
                ],
                "title": "The Influence of Human-inspired Agentic Sophistication in LLM-driven\n  Strategic Reasoners",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Influence of Human-inspired Agentic Sophistication in LLM-driven\n  Strategic Reasoners"
                },
                "summary": "The rapid rise of large language models (LLMs) has shifted artificial\nintelligence (AI) research toward agentic systems, motivating the use of weaker\nand more flexible notions of agency. However, this shift raises key questions\nabout the extent to which LLM-based agents replicate human strategic reasoning,\nparticularly in game-theoretic settings. In this context, we examine the role\nof agentic sophistication in shaping artificial reasoners' performance by\nevaluating three agent designs: a simple game-theoretic model, an unstructured\nLLM-as-agent model, and an LLM integrated into a traditional agentic framework.\nUsing guessing games as a testbed, we benchmarked these agents against human\nparticipants across general reasoning patterns and individual role-based\nobjectives. Furthermore, we introduced obfuscated game scenarios to assess\nagents' ability to generalise beyond training distributions. Our analysis,\ncovering over 2000 reasoning samples across 25 agent configurations, shows that\nhuman-inspired cognitive structures can enhance LLM agents' alignment with\nhuman strategic behaviour. Still, the relationship between agentic design\ncomplexity and human-likeness is non-linear, highlighting a critical dependence\non underlying LLM capabilities and suggesting limits to simple architectural\naugmentation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid rise of large language models (LLMs) has shifted artificial\nintelligence (AI) research toward agentic systems, motivating the use of weaker\nand more flexible notions of agency. However, this shift raises key questions\nabout the extent to which LLM-based agents replicate human strategic reasoning,\nparticularly in game-theoretic settings. In this context, we examine the role\nof agentic sophistication in shaping artificial reasoners' performance by\nevaluating three agent designs: a simple game-theoretic model, an unstructured\nLLM-as-agent model, and an LLM integrated into a traditional agentic framework.\nUsing guessing games as a testbed, we benchmarked these agents against human\nparticipants across general reasoning patterns and individual role-based\nobjectives. Furthermore, we introduced obfuscated game scenarios to assess\nagents' ability to generalise beyond training distributions. Our analysis,\ncovering over 2000 reasoning samples across 25 agent configurations, shows that\nhuman-inspired cognitive structures can enhance LLM agents' alignment with\nhuman strategic behaviour. Still, the relationship between agentic design\ncomplexity and human-likeness is non-linear, highlighting a critical dependence\non underlying LLM capabilities and suggesting limits to simple architectural\naugmentation."
                },
                "authors": [
                    {
                        "name": "Vince Trencsenyi"
                    },
                    {
                        "name": "Agnieszka Mensfelt"
                    },
                    {
                        "name": "Kostas Stathis"
                    }
                ],
                "author_detail": {
                    "name": "Kostas Stathis"
                },
                "author": "Kostas Stathis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09396v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09396v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09395v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09395v1",
                "updated": "2025-05-14T13:50:44Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    13,
                    50,
                    44,
                    2,
                    134,
                    0
                ],
                "published": "2025-05-14T13:50:44Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    13,
                    50,
                    44,
                    2,
                    134,
                    0
                ],
                "title": "Quantum-Enhanced Parameter-Efficient Learning for Typhoon Trajectory\n  Forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum-Enhanced Parameter-Efficient Learning for Typhoon Trajectory\n  Forecasting"
                },
                "summary": "Typhoon trajectory forecasting is essential for disaster preparedness but\nremains computationally demanding due to the complexity of atmospheric dynamics\nand the resource requirements of deep learning models. Quantum-Train (QT), a\nhybrid quantum-classical framework that leverages quantum neural networks\n(QNNs) to generate trainable parameters exclusively during training,\neliminating the need for quantum hardware at inference time. Building on QT's\nsuccess across multiple domains, including image classification, reinforcement\nlearning, flood prediction, and large language model (LLM) fine-tuning, we\nintroduce Quantum Parameter Adaptation (QPA) for efficient typhoon forecasting\nmodel learning. Integrated with an Attention-based Multi-ConvGRU model, QPA\nenables parameter-efficient training while maintaining predictive accuracy.\nThis work represents the first application of quantum machine learning (QML) to\nlarge-scale typhoon trajectory prediction, offering a scalable and\nenergy-efficient approach to climate modeling. Our results demonstrate that QPA\nsignificantly reduces the number of trainable parameters while preserving\nperformance, making high-performance forecasting more accessible and\nsustainable through hybrid quantum-classical learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Typhoon trajectory forecasting is essential for disaster preparedness but\nremains computationally demanding due to the complexity of atmospheric dynamics\nand the resource requirements of deep learning models. Quantum-Train (QT), a\nhybrid quantum-classical framework that leverages quantum neural networks\n(QNNs) to generate trainable parameters exclusively during training,\neliminating the need for quantum hardware at inference time. Building on QT's\nsuccess across multiple domains, including image classification, reinforcement\nlearning, flood prediction, and large language model (LLM) fine-tuning, we\nintroduce Quantum Parameter Adaptation (QPA) for efficient typhoon forecasting\nmodel learning. Integrated with an Attention-based Multi-ConvGRU model, QPA\nenables parameter-efficient training while maintaining predictive accuracy.\nThis work represents the first application of quantum machine learning (QML) to\nlarge-scale typhoon trajectory prediction, offering a scalable and\nenergy-efficient approach to climate modeling. Our results demonstrate that QPA\nsignificantly reduces the number of trainable parameters while preserving\nperformance, making high-performance forecasting more accessible and\nsustainable through hybrid quantum-classical learning."
                },
                "authors": [
                    {
                        "name": "Chen-Yu Liu"
                    },
                    {
                        "name": "Kuan-Cheng Chen"
                    },
                    {
                        "name": "Yi-Chien Chen"
                    },
                    {
                        "name": "Samuel Yen-Chi Chen"
                    },
                    {
                        "name": "Wei-Hao Huang"
                    },
                    {
                        "name": "Wei-Jia Huang"
                    },
                    {
                        "name": "Yen-Jui Chang"
                    }
                ],
                "author_detail": {
                    "name": "Yen-Jui Chang"
                },
                "author": "Yen-Jui Chang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09395v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09395v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09393v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09393v1",
                "updated": "2025-05-14T13:48:36Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    13,
                    48,
                    36,
                    2,
                    134,
                    0
                ],
                "published": "2025-05-14T13:48:36Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    13,
                    48,
                    36,
                    2,
                    134,
                    0
                ],
                "title": "UMotion: Uncertainty-driven Human Motion Estimation from Inertial and\n  Ultra-wideband Units",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UMotion: Uncertainty-driven Human Motion Estimation from Inertial and\n  Ultra-wideband Units"
                },
                "summary": "Sparse wearable inertial measurement units (IMUs) have gained popularity for\nestimating 3D human motion. However, challenges such as pose ambiguity, data\ndrift, and limited adaptability to diverse bodies persist. To address these\nissues, we propose UMotion, an uncertainty-driven, online fusing-all state\nestimation framework for 3D human shape and pose estimation, supported by six\nintegrated, body-worn ultra-wideband (UWB) distance sensors with IMUs. UWB\nsensors measure inter-node distances to infer spatial relationships, aiding in\nresolving pose ambiguities and body shape variations when combined with\nanthropometric data. Unfortunately, IMUs are prone to drift, and UWB sensors\nare affected by body occlusions. Consequently, we develop a tightly coupled\nUnscented Kalman Filter (UKF) framework that fuses uncertainties from sensor\ndata and estimated human motion based on individual body shape. The UKF\niteratively refines IMU and UWB measurements by aligning them with uncertain\nhuman motion constraints in real-time, producing optimal estimates for each.\nExperiments on both synthetic and real-world datasets demonstrate the\neffectiveness of UMotion in stabilizing sensor data and the improvement over\nstate of the art in pose accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse wearable inertial measurement units (IMUs) have gained popularity for\nestimating 3D human motion. However, challenges such as pose ambiguity, data\ndrift, and limited adaptability to diverse bodies persist. To address these\nissues, we propose UMotion, an uncertainty-driven, online fusing-all state\nestimation framework for 3D human shape and pose estimation, supported by six\nintegrated, body-worn ultra-wideband (UWB) distance sensors with IMUs. UWB\nsensors measure inter-node distances to infer spatial relationships, aiding in\nresolving pose ambiguities and body shape variations when combined with\nanthropometric data. Unfortunately, IMUs are prone to drift, and UWB sensors\nare affected by body occlusions. Consequently, we develop a tightly coupled\nUnscented Kalman Filter (UKF) framework that fuses uncertainties from sensor\ndata and estimated human motion based on individual body shape. The UKF\niteratively refines IMU and UWB measurements by aligning them with uncertain\nhuman motion constraints in real-time, producing optimal estimates for each.\nExperiments on both synthetic and real-world datasets demonstrate the\neffectiveness of UMotion in stabilizing sensor data and the improvement over\nstate of the art in pose accuracy."
                },
                "authors": [
                    {
                        "name": "Huakun Liu"
                    },
                    {
                        "name": "Hiroki Ota"
                    },
                    {
                        "name": "Xin Wei"
                    },
                    {
                        "name": "Yutaro Hirao"
                    },
                    {
                        "name": "Monica Perusquia-Hernandez"
                    },
                    {
                        "name": "Hideaki Uchiyama"
                    },
                    {
                        "name": "Kiyoshi Kiyokawa"
                    }
                ],
                "author_detail": {
                    "name": "Kiyoshi Kiyokawa"
                },
                "author": "Kiyoshi Kiyokawa",
                "arxiv_comment": "Accepted by CVPR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09393v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09393v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09388v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09388v1",
                "updated": "2025-05-14T13:41:34Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    13,
                    41,
                    34,
                    2,
                    134,
                    0
                ],
                "published": "2025-05-14T13:41:34Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    13,
                    41,
                    34,
                    2,
                    134,
                    0
                ],
                "title": "Qwen3 Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Qwen3 Technical Report"
                },
                "summary": "In this work, we present Qwen3, the latest version of the Qwen model family.\nQwen3 comprises a series of large language models (LLMs) designed to advance\nperformance, efficiency, and multilingual capabilities. The Qwen3 series\nincludes models of both dense and Mixture-of-Expert (MoE) architectures, with\nparameter scales ranging from 0.6 to 235 billion. A key innovation in Qwen3 is\nthe integration of thinking mode (for complex, multi-step reasoning) and\nnon-thinking mode (for rapid, context-driven responses) into a unified\nframework. This eliminates the need to switch between different models--such as\nchat-optimized models (e.g., GPT-4o) and dedicated reasoning models (e.g.,\nQwQ-32B)--and enables dynamic mode switching based on user queries or chat\ntemplates. Meanwhile, Qwen3 introduces a thinking budget mechanism, allowing\nusers to allocate computational resources adaptively during inference, thereby\nbalancing latency and performance based on task complexity. Moreover, by\nleveraging the knowledge from the flagship models, we significantly reduce the\ncomputational resources required to build smaller-scale models, while ensuring\ntheir highly competitive performance. Empirical evaluations demonstrate that\nQwen3 achieves state-of-the-art results across diverse benchmarks, including\ntasks in code generation, mathematical reasoning, agent tasks, etc.,\ncompetitive against larger MoE models and proprietary models. Compared to its\npredecessor Qwen2.5, Qwen3 expands multilingual support from 29 to 119\nlanguages and dialects, enhancing global accessibility through improved\ncross-lingual understanding and generation capabilities. To facilitate\nreproducibility and community-driven research and development, all Qwen3 models\nare publicly accessible under Apache 2.0.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we present Qwen3, the latest version of the Qwen model family.\nQwen3 comprises a series of large language models (LLMs) designed to advance\nperformance, efficiency, and multilingual capabilities. The Qwen3 series\nincludes models of both dense and Mixture-of-Expert (MoE) architectures, with\nparameter scales ranging from 0.6 to 235 billion. A key innovation in Qwen3 is\nthe integration of thinking mode (for complex, multi-step reasoning) and\nnon-thinking mode (for rapid, context-driven responses) into a unified\nframework. This eliminates the need to switch between different models--such as\nchat-optimized models (e.g., GPT-4o) and dedicated reasoning models (e.g.,\nQwQ-32B)--and enables dynamic mode switching based on user queries or chat\ntemplates. Meanwhile, Qwen3 introduces a thinking budget mechanism, allowing\nusers to allocate computational resources adaptively during inference, thereby\nbalancing latency and performance based on task complexity. Moreover, by\nleveraging the knowledge from the flagship models, we significantly reduce the\ncomputational resources required to build smaller-scale models, while ensuring\ntheir highly competitive performance. Empirical evaluations demonstrate that\nQwen3 achieves state-of-the-art results across diverse benchmarks, including\ntasks in code generation, mathematical reasoning, agent tasks, etc.,\ncompetitive against larger MoE models and proprietary models. Compared to its\npredecessor Qwen2.5, Qwen3 expands multilingual support from 29 to 119\nlanguages and dialects, enhancing global accessibility through improved\ncross-lingual understanding and generation capabilities. To facilitate\nreproducibility and community-driven research and development, all Qwen3 models\nare publicly accessible under Apache 2.0."
                },
                "authors": [
                    {
                        "name": "An Yang"
                    },
                    {
                        "name": "Anfeng Li"
                    },
                    {
                        "name": "Baosong Yang"
                    },
                    {
                        "name": "Beichen Zhang"
                    },
                    {
                        "name": "Binyuan Hui"
                    },
                    {
                        "name": "Bo Zheng"
                    },
                    {
                        "name": "Bowen Yu"
                    },
                    {
                        "name": "Chang Gao"
                    },
                    {
                        "name": "Chengen Huang"
                    },
                    {
                        "name": "Chenxu Lv"
                    },
                    {
                        "name": "Chujie Zheng"
                    },
                    {
                        "name": "Dayiheng Liu"
                    },
                    {
                        "name": "Fan Zhou"
                    },
                    {
                        "name": "Fei Huang"
                    },
                    {
                        "name": "Feng Hu"
                    },
                    {
                        "name": "Hao Ge"
                    },
                    {
                        "name": "Haoran Wei"
                    },
                    {
                        "name": "Huan Lin"
                    },
                    {
                        "name": "Jialong Tang"
                    },
                    {
                        "name": "Jian Yang"
                    },
                    {
                        "name": "Jianhong Tu"
                    },
                    {
                        "name": "Jianwei Zhang"
                    },
                    {
                        "name": "Jianxin Yang"
                    },
                    {
                        "name": "Jiaxi Yang"
                    },
                    {
                        "name": "Jing Zhou"
                    },
                    {
                        "name": "Jingren Zhou"
                    },
                    {
                        "name": "Junyang Lin"
                    },
                    {
                        "name": "Kai Dang"
                    },
                    {
                        "name": "Keqin Bao"
                    },
                    {
                        "name": "Kexin Yang"
                    },
                    {
                        "name": "Le Yu"
                    },
                    {
                        "name": "Lianghao Deng"
                    },
                    {
                        "name": "Mei Li"
                    },
                    {
                        "name": "Mingfeng Xue"
                    },
                    {
                        "name": "Mingze Li"
                    },
                    {
                        "name": "Pei Zhang"
                    },
                    {
                        "name": "Peng Wang"
                    },
                    {
                        "name": "Qin Zhu"
                    },
                    {
                        "name": "Rui Men"
                    },
                    {
                        "name": "Ruize Gao"
                    },
                    {
                        "name": "Shixuan Liu"
                    },
                    {
                        "name": "Shuang Luo"
                    },
                    {
                        "name": "Tianhao Li"
                    },
                    {
                        "name": "Tianyi Tang"
                    },
                    {
                        "name": "Wenbiao Yin"
                    },
                    {
                        "name": "Xingzhang Ren"
                    },
                    {
                        "name": "Xinyu Wang"
                    },
                    {
                        "name": "Xinyu Zhang"
                    },
                    {
                        "name": "Xuancheng Ren"
                    },
                    {
                        "name": "Yang Fan"
                    },
                    {
                        "name": "Yang Su"
                    },
                    {
                        "name": "Yichang Zhang"
                    },
                    {
                        "name": "Yinger Zhang"
                    },
                    {
                        "name": "Yu Wan"
                    },
                    {
                        "name": "Yuqiong Liu"
                    },
                    {
                        "name": "Zekun Wang"
                    },
                    {
                        "name": "Zeyu Cui"
                    },
                    {
                        "name": "Zhenru Zhang"
                    },
                    {
                        "name": "Zhipeng Zhou"
                    },
                    {
                        "name": "Zihan Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Zihan Qiu"
                },
                "author": "Zihan Qiu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09388v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09388v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23238v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23238v2",
                "updated": "2025-05-14T13:12:57Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    13,
                    12,
                    57,
                    2,
                    134,
                    0
                ],
                "published": "2024-10-30T17:25:57Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    17,
                    25,
                    57,
                    2,
                    304,
                    0
                ],
                "title": "Full-waveform earthquake source inversion using simulation-based\n  inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Full-waveform earthquake source inversion using simulation-based\n  inference"
                },
                "summary": "This paper presents a novel framework for full-waveform seismic source\ninversion using simulation-based inference (SBI). Traditional probabilistic\napproaches often rely on simplifying assumptions about data errors, which we\nshow can lead to inaccurate uncertainty quantification. SBI addresses this\nlimitation by building an empirical probabilistic model of the data errors\nusing machine learning models, known as neural density estimators, which can\nthen be integrated into the Bayesian inference framework. We apply the SBI\nframework to point-source moment tensor inversions as well as joint moment\ntensor and time-location inversions. We construct a range of synthetic examples\nto explore the quality of the SBI solutions, as well as to compare the SBI\nresults with standard Gaussian likelihood-based Bayesian inversions. We then\ndemonstrate that under real seismic noise, common Gaussian likelihood\nassumptions for treating full-waveform data yield overconfident posterior\ndistributions that underestimate the moment tensor component uncertainties by\nup to a factor of 3. We contrast this with SBI, which produces well-calibrated\nposteriors that generally agree with the true seismic source parameters, and\noffers an order-of-magnitude reduction in the number of simulations required to\nperform inference compared to standard Monte Carlo techniques. Finally, we\napply our methodology to a pair of moderate magnitude earthquakes in the North\nAtlantic. We utilise seismic waveforms recorded by the recent UPFLOW ocean\nbottom seismometer array as well as by regional land stations in the Azores,\ncomparing full moment tensor and source-time location posteriors between SBI\nand a Gaussian likelihood approach. We find that our adaptation of SBI can be\ndirectly applied to real earthquake sources to efficiently produce high quality\nposterior distributions that significantly improve upon Gaussian likelihood\napproaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a novel framework for full-waveform seismic source\ninversion using simulation-based inference (SBI). Traditional probabilistic\napproaches often rely on simplifying assumptions about data errors, which we\nshow can lead to inaccurate uncertainty quantification. SBI addresses this\nlimitation by building an empirical probabilistic model of the data errors\nusing machine learning models, known as neural density estimators, which can\nthen be integrated into the Bayesian inference framework. We apply the SBI\nframework to point-source moment tensor inversions as well as joint moment\ntensor and time-location inversions. We construct a range of synthetic examples\nto explore the quality of the SBI solutions, as well as to compare the SBI\nresults with standard Gaussian likelihood-based Bayesian inversions. We then\ndemonstrate that under real seismic noise, common Gaussian likelihood\nassumptions for treating full-waveform data yield overconfident posterior\ndistributions that underestimate the moment tensor component uncertainties by\nup to a factor of 3. We contrast this with SBI, which produces well-calibrated\nposteriors that generally agree with the true seismic source parameters, and\noffers an order-of-magnitude reduction in the number of simulations required to\nperform inference compared to standard Monte Carlo techniques. Finally, we\napply our methodology to a pair of moderate magnitude earthquakes in the North\nAtlantic. We utilise seismic waveforms recorded by the recent UPFLOW ocean\nbottom seismometer array as well as by regional land stations in the Azores,\ncomparing full moment tensor and source-time location posteriors between SBI\nand a Gaussian likelihood approach. We find that our adaptation of SBI can be\ndirectly applied to real earthquake sources to efficiently produce high quality\nposterior distributions that significantly improve upon Gaussian likelihood\napproaches."
                },
                "authors": [
                    {
                        "name": "A. A. Saoulis"
                    },
                    {
                        "name": "D. Piras"
                    },
                    {
                        "name": "A. Spurio Mancini"
                    },
                    {
                        "name": "B. Joachimi"
                    },
                    {
                        "name": "A. M. G. Ferreira"
                    }
                ],
                "author_detail": {
                    "name": "A. M. G. Ferreira"
                },
                "author": "A. M. G. Ferreira",
                "arxiv_doi": "10.1093/gji/ggaf112",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1093/gji/ggaf112",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2410.23238v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23238v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "22 + 11 pages, 11 + 11 figures. Now published in GJI",
                "arxiv_journal_ref": "Geophysical Journal International 241.3 (2025): 1740-1761",
                "arxiv_primary_category": {
                    "term": "physics.geo-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.geo-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.data-an",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09361v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09361v1",
                "updated": "2025-05-14T13:11:39Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    13,
                    11,
                    39,
                    2,
                    134,
                    0
                ],
                "published": "2025-05-14T13:11:39Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    13,
                    11,
                    39,
                    2,
                    134,
                    0
                ],
                "title": "Efficient Mixed Precision Quantization in Graph Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Mixed Precision Quantization in Graph Neural Networks"
                },
                "summary": "Graph Neural Networks (GNNs) have become essential for handling large-scale\ngraph applications. However, the computational demands of GNNs necessitate the\ndevelopment of efficient methods to accelerate inference. Mixed precision\nquantization emerges as a promising solution to enhance the efficiency of GNN\narchitectures without compromising prediction performance. Compared to\nconventional deep learning architectures, GNN layers contain a wider set of\ncomponents that can be quantized, including message passing functions,\naggregation functions, update functions, the inputs, learnable parameters, and\noutputs of these functions. In this paper, we introduce a theorem for efficient\nquantized message passing to aggregate integer messages. It guarantees\nnumerical equality of the aggregated messages using integer values with respect\nto those obtained with full (FP32) precision. Based on this theorem, we\nintroduce the Mixed Precision Quantization for GNN (MixQ-GNN) framework, which\nflexibly selects effective integer bit-widths for all components within GNN\nlayers. Our approach systematically navigates the wide set of possible\nbit-width combinations, addressing the challenge of optimizing efficiency while\naiming at maintaining comparable prediction performance. MixQ-GNN integrates\nwith existing GNN quantization methods, utilizing their graph structure\nadvantages to achieve higher prediction performance. On average, MixQ-GNN\nachieved reductions in bit operations of 5.5x for node classification and 5.1x\nfor graph classification compared to architectures represented in FP32\nprecision.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Neural Networks (GNNs) have become essential for handling large-scale\ngraph applications. However, the computational demands of GNNs necessitate the\ndevelopment of efficient methods to accelerate inference. Mixed precision\nquantization emerges as a promising solution to enhance the efficiency of GNN\narchitectures without compromising prediction performance. Compared to\nconventional deep learning architectures, GNN layers contain a wider set of\ncomponents that can be quantized, including message passing functions,\naggregation functions, update functions, the inputs, learnable parameters, and\noutputs of these functions. In this paper, we introduce a theorem for efficient\nquantized message passing to aggregate integer messages. It guarantees\nnumerical equality of the aggregated messages using integer values with respect\nto those obtained with full (FP32) precision. Based on this theorem, we\nintroduce the Mixed Precision Quantization for GNN (MixQ-GNN) framework, which\nflexibly selects effective integer bit-widths for all components within GNN\nlayers. Our approach systematically navigates the wide set of possible\nbit-width combinations, addressing the challenge of optimizing efficiency while\naiming at maintaining comparable prediction performance. MixQ-GNN integrates\nwith existing GNN quantization methods, utilizing their graph structure\nadvantages to achieve higher prediction performance. On average, MixQ-GNN\nachieved reductions in bit operations of 5.5x for node classification and 5.1x\nfor graph classification compared to architectures represented in FP32\nprecision."
                },
                "authors": [
                    {
                        "name": "Samir Moustafa"
                    },
                    {
                        "name": "Nils M. Kriege"
                    },
                    {
                        "name": "Wilfried N. Gansterer"
                    }
                ],
                "author_detail": {
                    "name": "Wilfried N. Gansterer"
                },
                "author": "Wilfried N. Gansterer",
                "arxiv_doi": "10.1109/ICDE65448.2025.00301",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/ICDE65448.2025.00301",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2505.09361v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09361v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18899v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18899v2",
                "updated": "2025-05-14T13:10:18Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    13,
                    10,
                    18,
                    2,
                    134,
                    0
                ],
                "published": "2024-10-24T16:40:16Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    16,
                    40,
                    16,
                    3,
                    298,
                    0
                ],
                "title": "SKATR: A Self-Supervised Summary Transformer for SKA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SKATR: A Self-Supervised Summary Transformer for SKA"
                },
                "summary": "The Square Kilometer Array will initiate a new era of radio astronomy by\nallowing 3D imaging of the Universe during Cosmic Dawn and Reionization. Modern\nmachine learning is crucial to analyse the highly structured and complex\nsignal. However, accurate training data is expensive to simulate, and\nsupervised learning may not generalize. We introduce a self-supervised vision\ntransformer, SKATR, whose learned encoding can be cheaply adapted for\ndownstream tasks on 21cm maps. Focusing on regression and generative inference\nof astrophysical and cosmological parameters, we demonstrate that SKATR\nrepresentations are maximally informative and that SKATR generalises\nout-of-domain to differently-simulated, noised, and higher-resolution datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Square Kilometer Array will initiate a new era of radio astronomy by\nallowing 3D imaging of the Universe during Cosmic Dawn and Reionization. Modern\nmachine learning is crucial to analyse the highly structured and complex\nsignal. However, accurate training data is expensive to simulate, and\nsupervised learning may not generalize. We introduce a self-supervised vision\ntransformer, SKATR, whose learned encoding can be cheaply adapted for\ndownstream tasks on 21cm maps. Focusing on regression and generative inference\nof astrophysical and cosmological parameters, we demonstrate that SKATR\nrepresentations are maximally informative and that SKATR generalises\nout-of-domain to differently-simulated, noised, and higher-resolution datasets."
                },
                "authors": [
                    {
                        "name": "Ayodele Ore"
                    },
                    {
                        "name": "Caroline Heneka"
                    },
                    {
                        "name": "Tilman Plehn"
                    }
                ],
                "author_detail": {
                    "name": "Tilman Plehn"
                },
                "author": "Tilman Plehn",
                "arxiv_doi": "10.21468/SciPostPhys.18.5.155",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.21468/SciPostPhys.18.5.155",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2410.18899v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18899v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "v2: Match published version",
                "arxiv_journal_ref": "SciPost Phys. 18, 155 (2025)",
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.03134v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.03134v4",
                "updated": "2025-05-14T12:53:14Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    12,
                    53,
                    14,
                    2,
                    134,
                    0
                ],
                "published": "2023-05-04T20:29:48Z",
                "published_parsed": [
                    2023,
                    5,
                    4,
                    20,
                    29,
                    48,
                    3,
                    124,
                    0
                ],
                "title": "Debiased Inference for Dynamic Nonlinear Panels with Multi-dimensional\n  Heterogeneities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Debiased Inference for Dynamic Nonlinear Panels with Multi-dimensional\n  Heterogeneities"
                },
                "summary": "We introduce a generic class of dynamic nonlinear heterogeneous parameter\nmodels that incorporate individual and time fixed effects in both the intercept\nand slope. These models are subject to the incidental parameter problem, in\nthat the limiting distribution of the point estimator is not centered at zero,\nand that test statistics do not follow their standard asymptotic distributions\nas in the absence of the fixed effects. To address the problem, we develop an\nanalytical bias correction procedure to construct a bias-corrected likelihood.\nThe resulting estimator follows an asymptotic normal distribution with mean\nzero. Moreover, likelihood-based tests statistics -- including\nlikelihood-ratio, Lagrange-multiplier, and Wald tests -- follow the limiting\nchi-squared distribution under the null hypothesis. Simulations demonstrate the\neffectiveness of the proposed correction method, and an empirical application\non the labor force participation of single mothers underscores its practical\nimportance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a generic class of dynamic nonlinear heterogeneous parameter\nmodels that incorporate individual and time fixed effects in both the intercept\nand slope. These models are subject to the incidental parameter problem, in\nthat the limiting distribution of the point estimator is not centered at zero,\nand that test statistics do not follow their standard asymptotic distributions\nas in the absence of the fixed effects. To address the problem, we develop an\nanalytical bias correction procedure to construct a bias-corrected likelihood.\nThe resulting estimator follows an asymptotic normal distribution with mean\nzero. Moreover, likelihood-based tests statistics -- including\nlikelihood-ratio, Lagrange-multiplier, and Wald tests -- follow the limiting\nchi-squared distribution under the null hypothesis. Simulations demonstrate the\neffectiveness of the proposed correction method, and an empirical application\non the labor force participation of single mothers underscores its practical\nimportance."
                },
                "authors": [
                    {
                        "name": "Xuan Leng"
                    },
                    {
                        "name": "Jiaming Mao"
                    },
                    {
                        "name": "Yutao Sun"
                    }
                ],
                "author_detail": {
                    "name": "Yutao Sun"
                },
                "author": "Yutao Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2305.03134v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.03134v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14219v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14219v2",
                "updated": "2025-05-14T12:50:18Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    12,
                    50,
                    18,
                    2,
                    134,
                    0
                ],
                "published": "2025-04-19T08:03:06Z",
                "published_parsed": [
                    2025,
                    4,
                    19,
                    8,
                    3,
                    6,
                    5,
                    109,
                    0
                ],
                "title": "PRISM: A Unified Framework for Photorealistic Reconstruction and\n  Intrinsic Scene Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PRISM: A Unified Framework for Photorealistic Reconstruction and\n  Intrinsic Scene Modeling"
                },
                "summary": "We present PRISM, a unified framework that enables multiple image generation\nand editing tasks in a single foundational model. Starting from a pre-trained\ntext-to-image diffusion model, PRISM proposes an effective fine-tuning strategy\nto produce RGB images along with intrinsic maps (referred to as X layers)\nsimultaneously. Unlike previous approaches, which infer intrinsic properties\nindividually or require separate models for decomposition and conditional\ngeneration, PRISM maintains consistency across modalities by generating all\nintrinsic layers jointly. It supports diverse tasks, including text-to-RGBX\ngeneration, RGB-to-X decomposition, and X-to-RGBX conditional generation.\nAdditionally, PRISM enables both global and local image editing through\nconditioning on selected intrinsic layers and text prompts. Extensive\nexperiments demonstrate the competitive performance of PRISM both for intrinsic\nimage decomposition and conditional image generation while preserving the base\nmodel's text-to-image generation capability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present PRISM, a unified framework that enables multiple image generation\nand editing tasks in a single foundational model. Starting from a pre-trained\ntext-to-image diffusion model, PRISM proposes an effective fine-tuning strategy\nto produce RGB images along with intrinsic maps (referred to as X layers)\nsimultaneously. Unlike previous approaches, which infer intrinsic properties\nindividually or require separate models for decomposition and conditional\ngeneration, PRISM maintains consistency across modalities by generating all\nintrinsic layers jointly. It supports diverse tasks, including text-to-RGBX\ngeneration, RGB-to-X decomposition, and X-to-RGBX conditional generation.\nAdditionally, PRISM enables both global and local image editing through\nconditioning on selected intrinsic layers and text prompts. Extensive\nexperiments demonstrate the competitive performance of PRISM both for intrinsic\nimage decomposition and conditional image generation while preserving the base\nmodel's text-to-image generation capability."
                },
                "authors": [
                    {
                        "name": "Alara Dirik"
                    },
                    {
                        "name": "Tuanfeng Wang"
                    },
                    {
                        "name": "Duygu Ceylan"
                    },
                    {
                        "name": "Stefanos Zafeiriou"
                    },
                    {
                        "name": "Anna Frühstück"
                    }
                ],
                "author_detail": {
                    "name": "Anna Frühstück"
                },
                "author": "Anna Frühstück",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14219v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14219v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09343v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09343v1",
                "updated": "2025-05-14T12:39:03Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    12,
                    39,
                    3,
                    2,
                    134,
                    0
                ],
                "published": "2025-05-14T12:39:03Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    12,
                    39,
                    3,
                    2,
                    134,
                    0
                ],
                "title": "Insights into DeepSeek-V3: Scaling Challenges and Reflections on\n  Hardware for AI Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Insights into DeepSeek-V3: Scaling Challenges and Reflections on\n  Hardware for AI Architectures"
                },
                "summary": "The rapid scaling of large language models (LLMs) has unveiled critical\nlimitations in current hardware architectures, including constraints in memory\ncapacity, computational efficiency, and interconnection bandwidth. DeepSeek-V3,\ntrained on 2,048 NVIDIA H800 GPUs, demonstrates how hardware-aware model\nco-design can effectively address these challenges, enabling cost-efficient\ntraining and inference at scale. This paper presents an in-depth analysis of\nthe DeepSeek-V3/R1 model architecture and its AI infrastructure, highlighting\nkey innovations such as Multi-head Latent Attention (MLA) for enhanced memory\nefficiency, Mixture of Experts (MoE) architectures for optimized\ncomputation-communication trade-offs, FP8 mixed-precision training to unlock\nthe full potential of hardware capabilities, and a Multi-Plane Network Topology\nto minimize cluster-level network overhead. Building on the hardware\nbottlenecks encountered during DeepSeek-V3's development, we engage in a\nbroader discussion with academic and industry peers on potential future\nhardware directions, including precise low-precision computation units,\nscale-up and scale-out convergence, and innovations in low-latency\ncommunication fabrics. These insights underscore the critical role of hardware\nand model co-design in meeting the escalating demands of AI workloads, offering\na practical blueprint for innovation in next-generation AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid scaling of large language models (LLMs) has unveiled critical\nlimitations in current hardware architectures, including constraints in memory\ncapacity, computational efficiency, and interconnection bandwidth. DeepSeek-V3,\ntrained on 2,048 NVIDIA H800 GPUs, demonstrates how hardware-aware model\nco-design can effectively address these challenges, enabling cost-efficient\ntraining and inference at scale. This paper presents an in-depth analysis of\nthe DeepSeek-V3/R1 model architecture and its AI infrastructure, highlighting\nkey innovations such as Multi-head Latent Attention (MLA) for enhanced memory\nefficiency, Mixture of Experts (MoE) architectures for optimized\ncomputation-communication trade-offs, FP8 mixed-precision training to unlock\nthe full potential of hardware capabilities, and a Multi-Plane Network Topology\nto minimize cluster-level network overhead. Building on the hardware\nbottlenecks encountered during DeepSeek-V3's development, we engage in a\nbroader discussion with academic and industry peers on potential future\nhardware directions, including precise low-precision computation units,\nscale-up and scale-out convergence, and innovations in low-latency\ncommunication fabrics. These insights underscore the critical role of hardware\nand model co-design in meeting the escalating demands of AI workloads, offering\na practical blueprint for innovation in next-generation AI systems."
                },
                "authors": [
                    {
                        "name": "Chenggang Zhao"
                    },
                    {
                        "name": "Chengqi Deng"
                    },
                    {
                        "name": "Chong Ruan"
                    },
                    {
                        "name": "Damai Dai"
                    },
                    {
                        "name": "Huazuo Gao"
                    },
                    {
                        "name": "Jiashi Li"
                    },
                    {
                        "name": "Liyue Zhang"
                    },
                    {
                        "name": "Panpan Huang"
                    },
                    {
                        "name": "Shangyan Zhou"
                    },
                    {
                        "name": "Shirong Ma"
                    },
                    {
                        "name": "Wenfeng Liang"
                    },
                    {
                        "name": "Ying He"
                    },
                    {
                        "name": "Yuqing Wang"
                    },
                    {
                        "name": "Yuxuan Liu"
                    },
                    {
                        "name": "Y. X. Wei"
                    }
                ],
                "author_detail": {
                    "name": "Y. X. Wei"
                },
                "author": "Y. X. Wei",
                "arxiv_comment": "This is the author's version of the work. It is posted here for your\n  personal use. Not for redistribution. The definitive version will appear as\n  part of the Industry Track in Proceedings of the 52nd Annual International\n  Symposium on Computer Architecture (ISCA '25)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09343v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09343v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.16149v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.16149v6",
                "updated": "2025-05-14T12:39:02Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    12,
                    39,
                    2,
                    2,
                    134,
                    0
                ],
                "published": "2024-03-24T13:43:43Z",
                "published_parsed": [
                    2024,
                    3,
                    24,
                    13,
                    43,
                    43,
                    6,
                    84,
                    0
                ],
                "title": "Analyzing Consumer IoT Traffic from Security and Privacy Perspectives: a\n  Comprehensive Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analyzing Consumer IoT Traffic from Security and Privacy Perspectives: a\n  Comprehensive Survey"
                },
                "summary": "The Consumer Internet of Things (CIoT), a notable segment within the IoT\ndomain, involves the integration of IoT technology into consumer electronics\nand devices, such as smart homes and smart wearables. Compared to traditional\nIoT fields, CIoT differs notably in target users, product types, and design\napproaches. While offering convenience to users, it also raises new security\nand privacy concerns. Network traffic analysis, a widely used technique in the\nsecurity community, has been extensively applied to investigate these concerns\nabout CIoT. Compared to traditional network traffic analysis in fields like\nmobile apps and websites, CIoT introduces unique characteristics that pose new\nchallenges and research opportunities. Researchers have made significant\ncontributions in this area. To aid researchers in understanding the application\nof traffic analysis tools for assessing CIoT security and privacy risks, this\nsurvey reviews 310 publications on traffic analysis within the CIoT security\nand privacy domain from January 2018 to June 2024, focusing on three research\nquestions. Our work: 1) outlines the CIoT traffic analysis process and\nhighlights its differences from general network traffic analysis. 2) summarizes\nand classifies existing research into four categories according to its\napplication objectives: device fingerprinting, user activity inference,\nmalicious traffic detection, and measurement. 3) explores emerging challenges\nand potential future research directions based on each step of the CIoT traffic\nanalysis process. This will provide new insights to the community and guide the\nindustry towards safer product designs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Consumer Internet of Things (CIoT), a notable segment within the IoT\ndomain, involves the integration of IoT technology into consumer electronics\nand devices, such as smart homes and smart wearables. Compared to traditional\nIoT fields, CIoT differs notably in target users, product types, and design\napproaches. While offering convenience to users, it also raises new security\nand privacy concerns. Network traffic analysis, a widely used technique in the\nsecurity community, has been extensively applied to investigate these concerns\nabout CIoT. Compared to traditional network traffic analysis in fields like\nmobile apps and websites, CIoT introduces unique characteristics that pose new\nchallenges and research opportunities. Researchers have made significant\ncontributions in this area. To aid researchers in understanding the application\nof traffic analysis tools for assessing CIoT security and privacy risks, this\nsurvey reviews 310 publications on traffic analysis within the CIoT security\nand privacy domain from January 2018 to June 2024, focusing on three research\nquestions. Our work: 1) outlines the CIoT traffic analysis process and\nhighlights its differences from general network traffic analysis. 2) summarizes\nand classifies existing research into four categories according to its\napplication objectives: device fingerprinting, user activity inference,\nmalicious traffic detection, and measurement. 3) explores emerging challenges\nand potential future research directions based on each step of the CIoT traffic\nanalysis process. This will provide new insights to the community and guide the\nindustry towards safer product designs."
                },
                "authors": [
                    {
                        "name": "Yan Jia"
                    },
                    {
                        "name": "Yuxin Song"
                    },
                    {
                        "name": "Zihou Liu"
                    },
                    {
                        "name": "Qingyin Tan"
                    },
                    {
                        "name": "Yang Song"
                    },
                    {
                        "name": "Yu Zhang"
                    },
                    {
                        "name": "Zheli Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zheli Liu"
                },
                "author": "Zheli Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.16149v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.16149v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09339v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09339v1",
                "updated": "2025-05-14T12:34:58Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    12,
                    34,
                    58,
                    2,
                    134,
                    0
                ],
                "published": "2025-05-14T12:34:58Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    12,
                    34,
                    58,
                    2,
                    134,
                    0
                ],
                "title": "RAG-Enabled Intent Reasoning for Application-Network Interaction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAG-Enabled Intent Reasoning for Application-Network Interaction"
                },
                "summary": "Intent-based network (IBN) is a promising solution to automate network\noperation and management. IBN aims to offer human-tailored network interaction,\nallowing the network to communicate in a way that aligns with the network\nusers' language, rather than requiring the network users to understand the\ntechnical language of the network/devices. Nowadays, different applications\ninteract with the network, each with its own specialized needs and domain\nlanguage. Creating semantic languages (i.e., ontology-based languages) and\nassociating them with each application to facilitate intent translation lacks\ntechnical expertise and is neither practical nor scalable. To tackle the\naforementioned problem, we propose a context-aware AI framework that utilizes\nmachine reasoning (MR), retrieval augmented generation (RAG), and generative AI\ntechnologies to interpret intents from different applications and generate\nstructured network intents. The proposed framework allows for\ngeneralized/domain-specific intent expression and overcomes the drawbacks of\nlarge language models (LLMs) and vanilla-RAG framework. The experimental\nresults show that our proposed intent-RAG framework outperforms the LLM and\nvanilla-RAG framework in intent translation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intent-based network (IBN) is a promising solution to automate network\noperation and management. IBN aims to offer human-tailored network interaction,\nallowing the network to communicate in a way that aligns with the network\nusers' language, rather than requiring the network users to understand the\ntechnical language of the network/devices. Nowadays, different applications\ninteract with the network, each with its own specialized needs and domain\nlanguage. Creating semantic languages (i.e., ontology-based languages) and\nassociating them with each application to facilitate intent translation lacks\ntechnical expertise and is neither practical nor scalable. To tackle the\naforementioned problem, we propose a context-aware AI framework that utilizes\nmachine reasoning (MR), retrieval augmented generation (RAG), and generative AI\ntechnologies to interpret intents from different applications and generate\nstructured network intents. The proposed framework allows for\ngeneralized/domain-specific intent expression and overcomes the drawbacks of\nlarge language models (LLMs) and vanilla-RAG framework. The experimental\nresults show that our proposed intent-RAG framework outperforms the LLM and\nvanilla-RAG framework in intent translation."
                },
                "authors": [
                    {
                        "name": "Salwa Mostafa"
                    },
                    {
                        "name": "Mohamed K. Abdel-Aziz"
                    },
                    {
                        "name": "Mohammed S. Elbamby"
                    },
                    {
                        "name": "Mehdi Bennis"
                    }
                ],
                "author_detail": {
                    "name": "Mehdi Bennis"
                },
                "author": "Mehdi Bennis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09339v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09339v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09338v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09338v1",
                "updated": "2025-05-14T12:33:05Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    12,
                    33,
                    5,
                    2,
                    134,
                    0
                ],
                "published": "2025-05-14T12:33:05Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    12,
                    33,
                    5,
                    2,
                    134,
                    0
                ],
                "title": "Llama See, Llama Do: A Mechanistic Perspective on Contextual Entrainment\n  and Distraction in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Llama See, Llama Do: A Mechanistic Perspective on Contextual Entrainment\n  and Distraction in LLMs"
                },
                "summary": "We observe a novel phenomenon, contextual entrainment, across a wide range of\nlanguage models (LMs) and prompt settings, providing a new mechanistic\nperspective on how LMs become distracted by ``irrelevant'' contextual\ninformation in the input prompt. Specifically, LMs assign significantly higher\nlogits (or probabilities) to any tokens that have previously appeared in the\ncontext prompt, even for random tokens. This suggests that contextual\nentrainment is a mechanistic phenomenon, occurring independently of the\nrelevance or semantic relation of the tokens to the question or the rest of the\nsentence. We find statistically significant evidence that the magnitude of\ncontextual entrainment is influenced by semantic factors. Counterfactual\nprompts have a greater effect compared to factual ones, suggesting that while\ncontextual entrainment is a mechanistic phenomenon, it is modulated by semantic\nfactors.\n  We hypothesise that there is a circuit of attention heads -- the entrainment\nheads -- that corresponds to the contextual entrainment phenomenon. Using a\nnovel entrainment head discovery method based on differentiable masking, we\nidentify these heads across various settings. When we ``turn off'' these heads,\ni.e., set their outputs to zero, the effect of contextual entrainment is\nsignificantly attenuated, causing the model to generate output that capitulates\nto what it would produce if no distracting context were provided. Our discovery\nof contextual entrainment, along with our investigation into LM distraction via\nthe entrainment heads, marks a key step towards the mechanistic analysis and\nmitigation of the distraction problem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We observe a novel phenomenon, contextual entrainment, across a wide range of\nlanguage models (LMs) and prompt settings, providing a new mechanistic\nperspective on how LMs become distracted by ``irrelevant'' contextual\ninformation in the input prompt. Specifically, LMs assign significantly higher\nlogits (or probabilities) to any tokens that have previously appeared in the\ncontext prompt, even for random tokens. This suggests that contextual\nentrainment is a mechanistic phenomenon, occurring independently of the\nrelevance or semantic relation of the tokens to the question or the rest of the\nsentence. We find statistically significant evidence that the magnitude of\ncontextual entrainment is influenced by semantic factors. Counterfactual\nprompts have a greater effect compared to factual ones, suggesting that while\ncontextual entrainment is a mechanistic phenomenon, it is modulated by semantic\nfactors.\n  We hypothesise that there is a circuit of attention heads -- the entrainment\nheads -- that corresponds to the contextual entrainment phenomenon. Using a\nnovel entrainment head discovery method based on differentiable masking, we\nidentify these heads across various settings. When we ``turn off'' these heads,\ni.e., set their outputs to zero, the effect of contextual entrainment is\nsignificantly attenuated, causing the model to generate output that capitulates\nto what it would produce if no distracting context were provided. Our discovery\nof contextual entrainment, along with our investigation into LM distraction via\nthe entrainment heads, marks a key step towards the mechanistic analysis and\nmitigation of the distraction problem."
                },
                "authors": [
                    {
                        "name": "Jingcheng Niu"
                    },
                    {
                        "name": "Xingdi Yuan"
                    },
                    {
                        "name": "Tong Wang"
                    },
                    {
                        "name": "Hamidreza Saghir"
                    },
                    {
                        "name": "Amir H. Abdi"
                    }
                ],
                "author_detail": {
                    "name": "Amir H. Abdi"
                },
                "author": "Amir H. Abdi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09338v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09338v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03343v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03343v2",
                "updated": "2025-05-14T12:32:17Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    12,
                    32,
                    17,
                    2,
                    134,
                    0
                ],
                "published": "2024-11-02T17:29:47Z",
                "published_parsed": [
                    2024,
                    11,
                    2,
                    17,
                    29,
                    47,
                    5,
                    307,
                    0
                ],
                "title": "What Features in Prompts Jailbreak LLMs? Investigating the Mechanisms\n  Behind Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What Features in Prompts Jailbreak LLMs? Investigating the Mechanisms\n  Behind Attacks"
                },
                "summary": "Jailbreaks have been a central focus of research regarding the safety and\nreliability of large language models (LLMs), yet the mechanisms underlying\nthese attacks remain poorly understood. While previous studies have\npredominantly relied on linear methods to detect jailbreak attempts and model\nrefusals, we take a different approach by examining both linear and non-linear\nfeatures in prompts that lead to successful jailbreaks. First, we introduce a\nnovel dataset comprising 10,800 jailbreak attempts spanning 35 diverse attack\nmethods. Leveraging this dataset, we train probes to classify successful from\nunsuccessful jailbreaks using the latent representations corresponding to\nprompt tokens. Notably, we find that even when probes achieve high accuracy in\npredicting the success of jailbreaks, their performance often fails to\ngeneralize to unseen attack methods. This reveals that different jailbreaking\nstrategies exploit different non-linear, non-universal features. Next, we\ndemonstrate that non-linear probes provide a powerful tool for steering model\nbehavior. Specifically, we use these probes to guide targeted latent space\nperturbations, enabling us to effectively modulate the model's robustness\nagainst jailbreaks. Overall, our findings challenge the assumption that\njailbreaks can be fully understood through linear or simple universal prompt\nfeatures alone, highlighting the importance of a nuanced understanding of the\nmechanisms behind LLM vulnerabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Jailbreaks have been a central focus of research regarding the safety and\nreliability of large language models (LLMs), yet the mechanisms underlying\nthese attacks remain poorly understood. While previous studies have\npredominantly relied on linear methods to detect jailbreak attempts and model\nrefusals, we take a different approach by examining both linear and non-linear\nfeatures in prompts that lead to successful jailbreaks. First, we introduce a\nnovel dataset comprising 10,800 jailbreak attempts spanning 35 diverse attack\nmethods. Leveraging this dataset, we train probes to classify successful from\nunsuccessful jailbreaks using the latent representations corresponding to\nprompt tokens. Notably, we find that even when probes achieve high accuracy in\npredicting the success of jailbreaks, their performance often fails to\ngeneralize to unseen attack methods. This reveals that different jailbreaking\nstrategies exploit different non-linear, non-universal features. Next, we\ndemonstrate that non-linear probes provide a powerful tool for steering model\nbehavior. Specifically, we use these probes to guide targeted latent space\nperturbations, enabling us to effectively modulate the model's robustness\nagainst jailbreaks. Overall, our findings challenge the assumption that\njailbreaks can be fully understood through linear or simple universal prompt\nfeatures alone, highlighting the importance of a nuanced understanding of the\nmechanisms behind LLM vulnerabilities."
                },
                "authors": [
                    {
                        "name": "Nathalie Kirch"
                    },
                    {
                        "name": "Constantin Weisser"
                    },
                    {
                        "name": "Severin Field"
                    },
                    {
                        "name": "Helen Yannakoudakis"
                    },
                    {
                        "name": "Stephen Casper"
                    }
                ],
                "author_detail": {
                    "name": "Stephen Casper"
                },
                "author": "Stephen Casper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03343v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03343v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17752v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17752v2",
                "updated": "2025-05-14T12:22:01Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    12,
                    22,
                    1,
                    2,
                    134,
                    0
                ],
                "published": "2025-01-29T16:41:15Z",
                "published_parsed": [
                    2025,
                    1,
                    29,
                    16,
                    41,
                    15,
                    2,
                    29,
                    0
                ],
                "title": "On the Partitioning of GPU Power among Multi-Instances",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Partitioning of GPU Power among Multi-Instances"
                },
                "summary": "Efficient power management in cloud data centers is essential for reducing\ncosts, enhancing performance, and minimizing environmental impact. GPUs,\ncritical for tasks like machine learning (ML) and GenAI, are major contributors\nto power consumption. NVIDIA's Multi-Instance GPU (MIG) technology improves GPU\nutilization by enabling isolated partitions with per-partition resource\ntracking, facilitating GPU sharing by multiple tenants. However, accurately\napportioning GPU power consumption among MIG instances remains challenging due\nto a lack of hardware support. This paper addresses this challenge by\ndeveloping software methods to estimate power usage per MIG partition. We\nanalyze NVIDIA GPU utilization metrics and find that light-weight methods with\ngood accuracy can be difficult to construct. We hence explore the use of\nML-based power models to enable accurate, partition-level power estimation. Our\nfindings reveal that a single generic offline power model or modeling method is\nnot applicable across diverse workloads, especially with concurrent MIG usage,\nand that online models constructed using partition-level utilization metrics of\nworkloads under execution can significantly improve accuracy. Using NVIDIA A100\nGPUs, we demonstrate this approach for accurate partition-level power\nestimation for workloads including matrix multiplication and Large Language\nModel inference, contributing to transparent and fair carbon reporting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient power management in cloud data centers is essential for reducing\ncosts, enhancing performance, and minimizing environmental impact. GPUs,\ncritical for tasks like machine learning (ML) and GenAI, are major contributors\nto power consumption. NVIDIA's Multi-Instance GPU (MIG) technology improves GPU\nutilization by enabling isolated partitions with per-partition resource\ntracking, facilitating GPU sharing by multiple tenants. However, accurately\napportioning GPU power consumption among MIG instances remains challenging due\nto a lack of hardware support. This paper addresses this challenge by\ndeveloping software methods to estimate power usage per MIG partition. We\nanalyze NVIDIA GPU utilization metrics and find that light-weight methods with\ngood accuracy can be difficult to construct. We hence explore the use of\nML-based power models to enable accurate, partition-level power estimation. Our\nfindings reveal that a single generic offline power model or modeling method is\nnot applicable across diverse workloads, especially with concurrent MIG usage,\nand that online models constructed using partition-level utilization metrics of\nworkloads under execution can significantly improve accuracy. Using NVIDIA A100\nGPUs, we demonstrate this approach for accurate partition-level power\nestimation for workloads including matrix multiplication and Large Language\nModel inference, contributing to transparent and fair carbon reporting."
                },
                "authors": [
                    {
                        "name": "Tirth Vamja"
                    },
                    {
                        "name": "Kaustabha Ray"
                    },
                    {
                        "name": "Felix George"
                    },
                    {
                        "name": "UmaMaheswari C Devi"
                    }
                ],
                "author_detail": {
                    "name": "UmaMaheswari C Devi"
                },
                "author": "UmaMaheswari C Devi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17752v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17752v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09319v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09319v1",
                "updated": "2025-05-14T12:16:40Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    12,
                    16,
                    40,
                    2,
                    134,
                    0
                ],
                "published": "2025-05-14T12:16:40Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    12,
                    16,
                    40,
                    2,
                    134,
                    0
                ],
                "title": "Statistical Modeling and Uncertainty Estimation of LLM Inference Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Statistical Modeling and Uncertainty Estimation of LLM Inference Systems"
                },
                "summary": "Large Language Model (LLM) inference systems present significant challenges\nin statistical performance characterization due to dynamic workload variations,\ndiverse hardware architectures, and complex interactions between model size,\nbatch processing, and throughput requirements. Accurate statistical\ncharacterization enables better workload scheduling, adaptive resource\nprovisioning, and cost-aware inference optimization, making it crucial for\nimproving efficiency in large-scale AI deployments. Traditional analytical\nmodels provide explainability but cannot cover the vast diversity of real-world\nworkloads, making it impossible to benchmark every scenario in advance. Machine\nlearning (ML) approaches effectively predict performance for non-benchmarked\ncases but struggle when extrapolating beyond their observed training space. To\naddress these limitations for LLM inference systems, we propose an Analytical\nwith Learning Augmentation (ALA) framework that bridges analytical modeling\nwith \\ml for robust statistical prediction and uncertainty estimation in LLM\ninference workloads. Our method employs an analytical throughput model with\nparameters estimated for benchmarked workloads, then extends to unobserved\nconfigurations using \\ml predictions. We enhance this with simulated annealing\nto exploit subsets of the workload data point combinations and develop an error\npredictor. Finally, we quantify uncertainty based on vector space similarity\nbetween new and observed workloads to ensure robust generalization. Through\nextensive experimentation on diverse LLM inference workloads, we demonstrate\nthat our framework achieves low median errors while maintaining adaptability to\nnew inference scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) inference systems present significant challenges\nin statistical performance characterization due to dynamic workload variations,\ndiverse hardware architectures, and complex interactions between model size,\nbatch processing, and throughput requirements. Accurate statistical\ncharacterization enables better workload scheduling, adaptive resource\nprovisioning, and cost-aware inference optimization, making it crucial for\nimproving efficiency in large-scale AI deployments. Traditional analytical\nmodels provide explainability but cannot cover the vast diversity of real-world\nworkloads, making it impossible to benchmark every scenario in advance. Machine\nlearning (ML) approaches effectively predict performance for non-benchmarked\ncases but struggle when extrapolating beyond their observed training space. To\naddress these limitations for LLM inference systems, we propose an Analytical\nwith Learning Augmentation (ALA) framework that bridges analytical modeling\nwith \\ml for robust statistical prediction and uncertainty estimation in LLM\ninference workloads. Our method employs an analytical throughput model with\nparameters estimated for benchmarked workloads, then extends to unobserved\nconfigurations using \\ml predictions. We enhance this with simulated annealing\nto exploit subsets of the workload data point combinations and develop an error\npredictor. Finally, we quantify uncertainty based on vector space similarity\nbetween new and observed workloads to ensure robust generalization. Through\nextensive experimentation on diverse LLM inference workloads, we demonstrate\nthat our framework achieves low median errors while maintaining adaptability to\nnew inference scenarios."
                },
                "authors": [
                    {
                        "name": "Kaustabha Ray"
                    },
                    {
                        "name": "Nelson Mimura Gonzalez"
                    },
                    {
                        "name": "Bruno Wassermann"
                    },
                    {
                        "name": "Rachel Tzoref-Brill"
                    },
                    {
                        "name": "Dean H. Lorenz"
                    }
                ],
                "author_detail": {
                    "name": "Dean H. Lorenz"
                },
                "author": "Dean H. Lorenz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09319v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09319v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09316v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09316v1",
                "updated": "2025-05-14T12:13:38Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    12,
                    13,
                    38,
                    2,
                    134,
                    0
                ],
                "published": "2025-05-14T12:13:38Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    12,
                    13,
                    38,
                    2,
                    134,
                    0
                ],
                "title": "Scent of Knowledge: Optimizing Search-Enhanced Reasoning with\n  Information Foraging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scent of Knowledge: Optimizing Search-Enhanced Reasoning with\n  Information Foraging"
                },
                "summary": "Augmenting large language models (LLMs) with external retrieval has become a\nstandard method to address their inherent knowledge cutoff limitations.\nHowever, traditional retrieval-augmented generation methods employ static,\npre-inference retrieval strategies, making them inadequate for complex tasks\ninvolving ambiguous, multi-step, or evolving information needs. Recent advances\nin test-time scaling techniques have demonstrated significant potential in\nenabling LLMs to dynamically interact with external tools, motivating the shift\ntoward adaptive inference-time retrieval. Inspired by Information Foraging\nTheory (IFT), we propose InForage, a reinforcement learning framework that\nformalizes retrieval-augmented reasoning as a dynamic information-seeking\nprocess. Unlike existing approaches, InForage explicitly rewards intermediate\nretrieval quality, encouraging LLMs to iteratively gather and integrate\ninformation through adaptive search behaviors. To facilitate training, we\nconstruct a human-guided dataset capturing iterative search and reasoning\ntrajectories for complex, real-world web tasks. Extensive evaluations across\ngeneral question answering, multi-hop reasoning tasks, and a newly developed\nreal-time web QA dataset demonstrate InForage's superior performance over\nbaseline methods. These results highlight InForage's effectiveness in building\nrobust, adaptive, and efficient reasoning agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Augmenting large language models (LLMs) with external retrieval has become a\nstandard method to address their inherent knowledge cutoff limitations.\nHowever, traditional retrieval-augmented generation methods employ static,\npre-inference retrieval strategies, making them inadequate for complex tasks\ninvolving ambiguous, multi-step, or evolving information needs. Recent advances\nin test-time scaling techniques have demonstrated significant potential in\nenabling LLMs to dynamically interact with external tools, motivating the shift\ntoward adaptive inference-time retrieval. Inspired by Information Foraging\nTheory (IFT), we propose InForage, a reinforcement learning framework that\nformalizes retrieval-augmented reasoning as a dynamic information-seeking\nprocess. Unlike existing approaches, InForage explicitly rewards intermediate\nretrieval quality, encouraging LLMs to iteratively gather and integrate\ninformation through adaptive search behaviors. To facilitate training, we\nconstruct a human-guided dataset capturing iterative search and reasoning\ntrajectories for complex, real-world web tasks. Extensive evaluations across\ngeneral question answering, multi-hop reasoning tasks, and a newly developed\nreal-time web QA dataset demonstrate InForage's superior performance over\nbaseline methods. These results highlight InForage's effectiveness in building\nrobust, adaptive, and efficient reasoning agents."
                },
                "authors": [
                    {
                        "name": "Hongjin Qian"
                    },
                    {
                        "name": "Zheng Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zheng Liu"
                },
                "author": "Zheng Liu",
                "arxiv_comment": "16 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09316v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09316v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21620v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21620v4",
                "updated": "2025-05-14T11:56:07Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    11,
                    56,
                    7,
                    2,
                    134,
                    0
                ],
                "published": "2025-03-27T15:39:30Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    15,
                    39,
                    30,
                    3,
                    86,
                    0
                ],
                "title": "UI-R1: Enhancing Efficient Action Prediction of GUI Agents by\n  Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UI-R1: Enhancing Efficient Action Prediction of GUI Agents by\n  Reinforcement Learning"
                },
                "summary": "The recent DeepSeek-R1 has showcased the emergence of reasoning capabilities\nin LLMs through reinforcement learning (RL) with rule-based rewards. Despite\nits success in language models, its application in multi-modal domains,\nparticularly in graphic user interface (GUI) agent tasks, remains\nunder-explored. To address this issue, we propose UI-R1, the first framework to\nexplore how rule-based RL can enhance the reasoning capabilities of multimodal\nlarge language models (MLLMs) for GUI action prediction tasks. Specifically,\nUI-R1 introduces a novel rule-based action reward, enabling model optimization\nvia policy-based algorithms such as Group Relative Policy Optimization (GRPO).\nFor efficient training, we curate a small yet high-quality dataset of 136\nchallenging tasks, encompassing five common action types on mobile devices.\nExperimental results demonstrate that our proposed UI-R1-3B achieves\nsignificant improvements over the base model (i.e. Qwen2.5-VL-3B) on both\nin-domain (ID) and out-of-domain (OOD) tasks, with average accuracy gains of\n22.1% on ScreenSpot, 6.0% on ScreenSpot-Pro, and 12.7% on ANDROIDCONTROL.\nFurthermore, UI-R1-3B delivers competitive performance compared to larger\nmodels (e.g., OS-Atlas-7B) trained via supervised fine-tuning (SFT) on 76K\nsamples. We additionally develop an optimized version, UI-R1-E-3B, which\nsignificantly improves both grounding efficiency and accuracy. These results\nunderscore the potential of rule-based reinforcement learning to advance GUI\nunderstanding and control, paving the way for future research in this domain.\nCode website: https://github.com/lll6gg/UI-R1.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent DeepSeek-R1 has showcased the emergence of reasoning capabilities\nin LLMs through reinforcement learning (RL) with rule-based rewards. Despite\nits success in language models, its application in multi-modal domains,\nparticularly in graphic user interface (GUI) agent tasks, remains\nunder-explored. To address this issue, we propose UI-R1, the first framework to\nexplore how rule-based RL can enhance the reasoning capabilities of multimodal\nlarge language models (MLLMs) for GUI action prediction tasks. Specifically,\nUI-R1 introduces a novel rule-based action reward, enabling model optimization\nvia policy-based algorithms such as Group Relative Policy Optimization (GRPO).\nFor efficient training, we curate a small yet high-quality dataset of 136\nchallenging tasks, encompassing five common action types on mobile devices.\nExperimental results demonstrate that our proposed UI-R1-3B achieves\nsignificant improvements over the base model (i.e. Qwen2.5-VL-3B) on both\nin-domain (ID) and out-of-domain (OOD) tasks, with average accuracy gains of\n22.1% on ScreenSpot, 6.0% on ScreenSpot-Pro, and 12.7% on ANDROIDCONTROL.\nFurthermore, UI-R1-3B delivers competitive performance compared to larger\nmodels (e.g., OS-Atlas-7B) trained via supervised fine-tuning (SFT) on 76K\nsamples. We additionally develop an optimized version, UI-R1-E-3B, which\nsignificantly improves both grounding efficiency and accuracy. These results\nunderscore the potential of rule-based reinforcement learning to advance GUI\nunderstanding and control, paving the way for future research in this domain.\nCode website: https://github.com/lll6gg/UI-R1."
                },
                "authors": [
                    {
                        "name": "Zhengxi Lu"
                    },
                    {
                        "name": "Yuxiang Chai"
                    },
                    {
                        "name": "Yaxuan Guo"
                    },
                    {
                        "name": "Xi Yin"
                    },
                    {
                        "name": "Liang Liu"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Han Xiao"
                    },
                    {
                        "name": "Shuai Ren"
                    },
                    {
                        "name": "Guanjing Xiong"
                    },
                    {
                        "name": "Hongsheng Li"
                    }
                ],
                "author_detail": {
                    "name": "Hongsheng Li"
                },
                "author": "Hongsheng Li",
                "arxiv_comment": "Updated UI-R1-E-3B",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21620v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21620v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2303.18233v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2303.18233v3",
                "updated": "2025-05-14T11:22:49Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    11,
                    22,
                    49,
                    2,
                    134,
                    0
                ],
                "published": "2023-03-31T17:48:20Z",
                "published_parsed": [
                    2023,
                    3,
                    31,
                    17,
                    48,
                    20,
                    4,
                    90,
                    0
                ],
                "title": "Hypothesis testing on invariant subspaces of non-symmetric matrices with\n  applications to network statistics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hypothesis testing on invariant subspaces of non-symmetric matrices with\n  applications to network statistics"
                },
                "summary": "We extend the inference procedure for eigenvectors of Tyler (1981), which\nassumes symmetrizable matrices to generic invariant and singular subspaces of\nnon-diagonalisable matrices to test whether $\\nu \\in \\mathbb{R}^{p \\times r}$\nis an element of an invariant subspace of $M \\in \\mathbb{R}^{p \\times p}$. Our\nresults include a Wald test for full-vector hypotheses and a $t$-test for\ncoefficient-wise hypotheses. We employ perturbation expansions of invariant\nsubspaces from Sun (1991) and singular subspaces from Liu et al. (2007). Based\non the former, we extend the popular Davis-Kahan bound to estimations of its\nhigher-order polynomials and study how the bound simplifies for eigenspaces but\nattains complexity for generic invariant subspaces.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We extend the inference procedure for eigenvectors of Tyler (1981), which\nassumes symmetrizable matrices to generic invariant and singular subspaces of\nnon-diagonalisable matrices to test whether $\\nu \\in \\mathbb{R}^{p \\times r}$\nis an element of an invariant subspace of $M \\in \\mathbb{R}^{p \\times p}$. Our\nresults include a Wald test for full-vector hypotheses and a $t$-test for\ncoefficient-wise hypotheses. We employ perturbation expansions of invariant\nsubspaces from Sun (1991) and singular subspaces from Liu et al. (2007). Based\non the former, we extend the popular Davis-Kahan bound to estimations of its\nhigher-order polynomials and study how the bound simplifies for eigenspaces but\nattains complexity for generic invariant subspaces."
                },
                "authors": [
                    {
                        "name": "Jérôme R. Simons"
                    }
                ],
                "author_detail": {
                    "name": "Jérôme R. Simons"
                },
                "author": "Jérôme R. Simons",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2303.18233v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2303.18233v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09289v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09289v1",
                "updated": "2025-05-14T11:15:14Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    11,
                    15,
                    14,
                    2,
                    134,
                    0
                ],
                "published": "2025-05-14T11:15:14Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    11,
                    15,
                    14,
                    2,
                    134,
                    0
                ],
                "title": "Reproducibility Study of \"Cooperate or Collapse: Emergence of\n  Sustainable Cooperation in a Society of LLM Agents\"",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reproducibility Study of \"Cooperate or Collapse: Emergence of\n  Sustainable Cooperation in a Society of LLM Agents\""
                },
                "summary": "This study evaluates and extends the findings made by Piatti et al., who\nintroduced GovSim, a simulation framework designed to assess the cooperative\ndecision-making capabilities of large language models (LLMs) in\nresource-sharing scenarios. By replicating key experiments, we validate claims\nregarding the performance of large models, such as GPT-4-turbo, compared to\nsmaller models. The impact of the universalization principle is also examined,\nwith results showing that large models can achieve sustainable cooperation,\nwith or without the principle, while smaller models fail without it. In\naddition, we provide multiple extensions to explore the applicability of the\nframework to new settings. We evaluate additional models, such as DeepSeek-V3\nand GPT-4o-mini, to test whether cooperative behavior generalizes across\ndifferent architectures and model sizes. Furthermore, we introduce new\nsettings: we create a heterogeneous multi-agent environment, study a scenario\nusing Japanese instructions, and explore an \"inverse environment\" where agents\nmust cooperate to mitigate harmful resource distributions. Our results confirm\nthat the benchmark can be applied to new models, scenarios, and languages,\noffering valuable insights into the adaptability of LLMs in complex cooperative\ntasks. Moreover, the experiment involving heterogeneous multi-agent systems\ndemonstrates that high-performing models can influence lower-performing ones to\nadopt similar behaviors. This finding has significant implications for other\nagent-based applications, potentially enabling more efficient use of\ncomputational resources and contributing to the development of more effective\ncooperative AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study evaluates and extends the findings made by Piatti et al., who\nintroduced GovSim, a simulation framework designed to assess the cooperative\ndecision-making capabilities of large language models (LLMs) in\nresource-sharing scenarios. By replicating key experiments, we validate claims\nregarding the performance of large models, such as GPT-4-turbo, compared to\nsmaller models. The impact of the universalization principle is also examined,\nwith results showing that large models can achieve sustainable cooperation,\nwith or without the principle, while smaller models fail without it. In\naddition, we provide multiple extensions to explore the applicability of the\nframework to new settings. We evaluate additional models, such as DeepSeek-V3\nand GPT-4o-mini, to test whether cooperative behavior generalizes across\ndifferent architectures and model sizes. Furthermore, we introduce new\nsettings: we create a heterogeneous multi-agent environment, study a scenario\nusing Japanese instructions, and explore an \"inverse environment\" where agents\nmust cooperate to mitigate harmful resource distributions. Our results confirm\nthat the benchmark can be applied to new models, scenarios, and languages,\noffering valuable insights into the adaptability of LLMs in complex cooperative\ntasks. Moreover, the experiment involving heterogeneous multi-agent systems\ndemonstrates that high-performing models can influence lower-performing ones to\nadopt similar behaviors. This finding has significant implications for other\nagent-based applications, potentially enabling more efficient use of\ncomputational resources and contributing to the development of more effective\ncooperative AI systems."
                },
                "authors": [
                    {
                        "name": "Pedro M. P. Curvo"
                    },
                    {
                        "name": "Mara Dragomir"
                    },
                    {
                        "name": "Salvador Torpes"
                    },
                    {
                        "name": "Mohammadmahdi Rahimi"
                    }
                ],
                "author_detail": {
                    "name": "Mohammadmahdi Rahimi"
                },
                "author": "Mohammadmahdi Rahimi",
                "arxiv_comment": "11 Tables, 9 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09289v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09289v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16393v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16393v2",
                "updated": "2025-05-14T11:10:47Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    11,
                    10,
                    47,
                    2,
                    134,
                    0
                ],
                "published": "2025-01-26T14:59:47Z",
                "published_parsed": [
                    2025,
                    1,
                    26,
                    14,
                    59,
                    47,
                    6,
                    26,
                    0
                ],
                "title": "Improving Network Threat Detection by Knowledge Graph, Large Language\n  Model, and Imbalanced Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Network Threat Detection by Knowledge Graph, Large Language\n  Model, and Imbalanced Learning"
                },
                "summary": "Network threat detection has been challenging due to the complexities of\nattack activities and the limitation of historical threat data to learn from.\nTo help enhance the existing practices of using analytics, machine learning,\nand artificial intelligence methods to detect the network threats, we propose\nan integrated modelling framework, where Knowledge Graph is used to analyze the\nusers' activity patterns, Imbalanced Learning techniques are used to prune and\nweigh Knowledge Graph, and LLM is used to retrieve and interpret the users'\nactivities from Knowledge Graph. The proposed framework is applied to Agile\nThreat Detection through Online Sequential Learning. The preliminary results\nshow the improved threat capture rate by 3%-4% and the increased\ninterpretabilities of risk predictions based on the users' activities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Network threat detection has been challenging due to the complexities of\nattack activities and the limitation of historical threat data to learn from.\nTo help enhance the existing practices of using analytics, machine learning,\nand artificial intelligence methods to detect the network threats, we propose\nan integrated modelling framework, where Knowledge Graph is used to analyze the\nusers' activity patterns, Imbalanced Learning techniques are used to prune and\nweigh Knowledge Graph, and LLM is used to retrieve and interpret the users'\nactivities from Knowledge Graph. The proposed framework is applied to Agile\nThreat Detection through Online Sequential Learning. The preliminary results\nshow the improved threat capture rate by 3%-4% and the increased\ninterpretabilities of risk predictions based on the users' activities."
                },
                "authors": [
                    {
                        "name": "Lili Zhang"
                    },
                    {
                        "name": "Quanyan Zhu"
                    },
                    {
                        "name": "Herman Ray"
                    },
                    {
                        "name": "Ying Xie"
                    }
                ],
                "author_detail": {
                    "name": "Ying Xie"
                },
                "author": "Ying Xie",
                "arxiv_comment": "Accepted by \"Combining AI and OR/MS for Better Trustworthy Decision\n  Making\" Bridge Program co-organized by AAAI and INFORMS as poster and demo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16393v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16393v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09283v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09283v1",
                "updated": "2025-05-14T11:08:45Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    11,
                    8,
                    45,
                    2,
                    134,
                    0
                ],
                "published": "2025-05-14T11:08:45Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    11,
                    8,
                    45,
                    2,
                    134,
                    0
                ],
                "title": "A Note on Semantic Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Note on Semantic Diffusion"
                },
                "summary": "This paper provides an in-depth examination of the concept of semantic\ndiffusion as a complementary instrument to large language models (LLMs) for\ndesign applications. Conventional LLMs and diffusion models fail to induce a\nconvergent, iterative refinement process: each invocation of the diffusion\nmechanism spawns a new stochastic cycle, so successive outputs do not relate to\nprior ones and convergence toward a desired design is not guaranteed. The\nproposed hybrid framework - \"LLM + semantic diffusion\" - resolves this\nlimitation by enforcing an approximately convergent search procedure, thereby\nformally addressing the problem of localized design refinement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper provides an in-depth examination of the concept of semantic\ndiffusion as a complementary instrument to large language models (LLMs) for\ndesign applications. Conventional LLMs and diffusion models fail to induce a\nconvergent, iterative refinement process: each invocation of the diffusion\nmechanism spawns a new stochastic cycle, so successive outputs do not relate to\nprior ones and convergence toward a desired design is not guaranteed. The\nproposed hybrid framework - \"LLM + semantic diffusion\" - resolves this\nlimitation by enforcing an approximately convergent search procedure, thereby\nformally addressing the problem of localized design refinement."
                },
                "authors": [
                    {
                        "name": "Alexander P. Ryjov"
                    },
                    {
                        "name": "Alina A. Egorova"
                    }
                ],
                "author_detail": {
                    "name": "Alina A. Egorova"
                },
                "author": "Alina A. Egorova",
                "arxiv_comment": "8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09283v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09283v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "03E72, 68T37, 94D05",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.8; I.2.10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17058v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17058v2",
                "updated": "2025-05-14T11:00:39Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    11,
                    0,
                    39,
                    2,
                    134,
                    0
                ],
                "published": "2024-11-26T02:57:28Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    2,
                    57,
                    28,
                    1,
                    331,
                    0
                ],
                "title": "ThreatModeling-LLM: Automating Threat Modeling using Large Language\n  Models for Banking System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ThreatModeling-LLM: Automating Threat Modeling using Large Language\n  Models for Banking System"
                },
                "summary": "Threat modeling is a crucial component of cybersecurity, particularly for\nindustries such as banking, where the security of financial data is paramount.\nTraditional threat modeling approaches require expert intervention and manual\neffort, often leading to inefficiencies and human error. The advent of Large\nLanguage Models (LLMs) offers a promising avenue for automating these\nprocesses, enhancing both efficiency and efficacy. However, this transition is\nnot straightforward due to three main challenges: (1) the lack of publicly\navailable, domain-specific datasets, (2) the need for tailored models to handle\ncomplex banking system architectures, and (3) the requirement for real-time,\nadaptive mitigation strategies that align with compliance standards like NIST\n800-53. In this paper, we introduce ThreatModeling-LLM, a novel and adaptable\nframework that automates threat modeling for banking systems using LLMs.\nThreatModeling-LLM operates in three stages: 1) dataset creation, 2) prompt\nengineering and 3) model fine-tuning. We first generate a benchmark dataset\nusing Microsoft Threat Modeling Tool (TMT). Then, we apply Chain of Thought\n(CoT) and Optimization by PROmpting (OPRO) on the pre-trained LLMs to optimize\nthe initial prompt. Lastly, we fine-tune the LLM using Low-Rank Adaptation\n(LoRA) based on the benchmark dataset and the optimized prompt to improve the\nthreat identification and mitigation generation capabilities of pre-trained\nLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Threat modeling is a crucial component of cybersecurity, particularly for\nindustries such as banking, where the security of financial data is paramount.\nTraditional threat modeling approaches require expert intervention and manual\neffort, often leading to inefficiencies and human error. The advent of Large\nLanguage Models (LLMs) offers a promising avenue for automating these\nprocesses, enhancing both efficiency and efficacy. However, this transition is\nnot straightforward due to three main challenges: (1) the lack of publicly\navailable, domain-specific datasets, (2) the need for tailored models to handle\ncomplex banking system architectures, and (3) the requirement for real-time,\nadaptive mitigation strategies that align with compliance standards like NIST\n800-53. In this paper, we introduce ThreatModeling-LLM, a novel and adaptable\nframework that automates threat modeling for banking systems using LLMs.\nThreatModeling-LLM operates in three stages: 1) dataset creation, 2) prompt\nengineering and 3) model fine-tuning. We first generate a benchmark dataset\nusing Microsoft Threat Modeling Tool (TMT). Then, we apply Chain of Thought\n(CoT) and Optimization by PROmpting (OPRO) on the pre-trained LLMs to optimize\nthe initial prompt. Lastly, we fine-tune the LLM using Low-Rank Adaptation\n(LoRA) based on the benchmark dataset and the optimized prompt to improve the\nthreat identification and mitigation generation capabilities of pre-trained\nLLMs."
                },
                "authors": [
                    {
                        "name": "Tingmin Wu"
                    },
                    {
                        "name": "Shuiqiao Yang"
                    },
                    {
                        "name": "Shigang Liu"
                    },
                    {
                        "name": "David Nguyen"
                    },
                    {
                        "name": "Seung Jang"
                    },
                    {
                        "name": "Alsharif Abuadbba"
                    }
                ],
                "author_detail": {
                    "name": "Alsharif Abuadbba"
                },
                "author": "Alsharif Abuadbba",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17058v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17058v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16472v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16472v2",
                "updated": "2025-05-14T10:55:20Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    10,
                    55,
                    20,
                    2,
                    134,
                    0
                ],
                "published": "2025-04-23T07:32:43Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    7,
                    32,
                    43,
                    2,
                    113,
                    0
                ],
                "title": "Harden and Catch for Just-in-Time Assured LLM-Based Software Testing:\n  Open Research Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Harden and Catch for Just-in-Time Assured LLM-Based Software Testing:\n  Open Research Challenges"
                },
                "summary": "Despite decades of research and practice in automated software testing,\nseveral fundamental concepts remain ill-defined and under-explored, yet offer\nenormous potential real-world impact. We show that these concepts raise\nexciting new challenges in the context of Large Language Models for software\ntest generation. More specifically, we formally define and investigate the\nproperties of hardening and catching tests. A hardening test is one that seeks\nto protect against future regressions, while a catching test is one that\ncatches such a regression or a fault in new functionality introduced by a code\nchange. Hardening tests can be generated at any time and may become catching\ntests when a future regression is caught. We also define and motivate the\nCatching 'Just-in-Time' (JiTTest) Challenge, in which tests are generated\n'just-in-time' to catch new faults before they land into production. We show\nthat any solution to Catching JiTTest generation can also be repurposed to\ncatch latent faults in legacy code. We enumerate possible outcomes for\nhardening and catching tests and JiTTests, and discuss open research problems,\ndeployment options, and initial results from our work on automated LLM-based\nhardening at Meta. This paper was written to accompany the keynote by the\nauthors at the ACM International Conference on the Foundations of Software\nEngineering (FSE) 2025. Author order is alphabetical. The corresponding author\nis Mark Harman.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite decades of research and practice in automated software testing,\nseveral fundamental concepts remain ill-defined and under-explored, yet offer\nenormous potential real-world impact. We show that these concepts raise\nexciting new challenges in the context of Large Language Models for software\ntest generation. More specifically, we formally define and investigate the\nproperties of hardening and catching tests. A hardening test is one that seeks\nto protect against future regressions, while a catching test is one that\ncatches such a regression or a fault in new functionality introduced by a code\nchange. Hardening tests can be generated at any time and may become catching\ntests when a future regression is caught. We also define and motivate the\nCatching 'Just-in-Time' (JiTTest) Challenge, in which tests are generated\n'just-in-time' to catch new faults before they land into production. We show\nthat any solution to Catching JiTTest generation can also be repurposed to\ncatch latent faults in legacy code. We enumerate possible outcomes for\nhardening and catching tests and JiTTests, and discuss open research problems,\ndeployment options, and initial results from our work on automated LLM-based\nhardening at Meta. This paper was written to accompany the keynote by the\nauthors at the ACM International Conference on the Foundations of Software\nEngineering (FSE) 2025. Author order is alphabetical. The corresponding author\nis Mark Harman."
                },
                "authors": [
                    {
                        "name": "Mark Harman"
                    },
                    {
                        "name": "Peter O'Hearn"
                    },
                    {
                        "name": "Shubho Sengupta"
                    }
                ],
                "author_detail": {
                    "name": "Shubho Sengupta"
                },
                "author": "Shubho Sengupta",
                "arxiv_comment": "To Appear as keynote paper at FSE 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16472v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16472v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10272v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10272v2",
                "updated": "2025-05-14T10:53:06Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    10,
                    53,
                    6,
                    2,
                    134,
                    0
                ],
                "published": "2025-01-17T15:59:21Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    15,
                    59,
                    21,
                    4,
                    17,
                    0
                ],
                "title": "Data-driven approach for extracting tidal information from neutron star\n  binary mergers observed with the Einstein Telescope",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data-driven approach for extracting tidal information from neutron star\n  binary mergers observed with the Einstein Telescope"
                },
                "summary": "The recent breakthroughs regarding the detection of compact binary mergers\nvia gravitational waves opened up a new window to the Universe.\nGravitational-wave models have been essential to this success since they are\nnecessary to infer the properties of the compact binary system from the\nobservational data. Next-generation detectors, such as the Einstein Telescope,\nwill allow for more observations of binary neutron star mergers with higher\nprecision, making accurate waveform models crucial in describing these systems.\nIn this article, we propose a novel approach for constructing phenomenological\nwaveform models informed by observational data. Using mock data representing a\none-year operation of the Einstein Telescope as our baseline, we demonstrate\nhow the results improve as more events are included in the calibration. This\nmethod offers a new and complementary approach for developing sophisticated\ngravitational-wave models compared to classical techniques that employ\nanalytical computations and numerical-relativity simulations. Improved waveform\nmodels will then yield more accurate parameter estimation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent breakthroughs regarding the detection of compact binary mergers\nvia gravitational waves opened up a new window to the Universe.\nGravitational-wave models have been essential to this success since they are\nnecessary to infer the properties of the compact binary system from the\nobservational data. Next-generation detectors, such as the Einstein Telescope,\nwill allow for more observations of binary neutron star mergers with higher\nprecision, making accurate waveform models crucial in describing these systems.\nIn this article, we propose a novel approach for constructing phenomenological\nwaveform models informed by observational data. Using mock data representing a\none-year operation of the Einstein Telescope as our baseline, we demonstrate\nhow the results improve as more events are included in the calibration. This\nmethod offers a new and complementary approach for developing sophisticated\ngravitational-wave models compared to classical techniques that employ\nanalytical computations and numerical-relativity simulations. Improved waveform\nmodels will then yield more accurate parameter estimation."
                },
                "authors": [
                    {
                        "name": "Adrian Abac"
                    },
                    {
                        "name": "Anna Puecher"
                    },
                    {
                        "name": "Jonathan Gair"
                    },
                    {
                        "name": "Tim Dietrich"
                    }
                ],
                "author_detail": {
                    "name": "Tim Dietrich"
                },
                "author": "Tim Dietrich",
                "arxiv_comment": "version accepted to Physical Review Letters",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10272v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10272v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17599v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17599v2",
                "updated": "2025-05-14T10:25:11Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    10,
                    25,
                    11,
                    2,
                    134,
                    0
                ],
                "published": "2025-03-22T01:02:44Z",
                "published_parsed": [
                    2025,
                    3,
                    22,
                    1,
                    2,
                    44,
                    5,
                    81,
                    0
                ],
                "title": "Evaluating Clinical Competencies of Large Language Models with a General\n  Practice Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Clinical Competencies of Large Language Models with a General\n  Practice Benchmark"
                },
                "summary": "Large Language Models (LLMs) have demonstrated considerable potential in\ngeneral practice. However, existing benchmarks and evaluation frameworks\nprimarily depend on exam-style or simplified question-answer formats, lacking a\ncompetency-based structure aligned with the real-world clinical\nresponsibilities encountered in general practice. Consequently, the extent to\nwhich LLMs can reliably fulfill the duties of general practitioners (GPs)\nremains uncertain. In this work, we propose a novel evaluation framework to\nassess the capability of LLMs to function as GPs. Based on this framework, we\nintroduce a general practice benchmark (GPBench), whose data are meticulously\nannotated by domain experts in accordance with routine clinical practice\nstandards. We evaluate ten state-of-the-art LLMs and analyze their\ncompetencies. Our findings indicate that current LLMs are not yet ready for\ndeployment in such settings without human oversight, and further optimization\nspecifically tailored to the daily responsibilities of GPs is essential.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated considerable potential in\ngeneral practice. However, existing benchmarks and evaluation frameworks\nprimarily depend on exam-style or simplified question-answer formats, lacking a\ncompetency-based structure aligned with the real-world clinical\nresponsibilities encountered in general practice. Consequently, the extent to\nwhich LLMs can reliably fulfill the duties of general practitioners (GPs)\nremains uncertain. In this work, we propose a novel evaluation framework to\nassess the capability of LLMs to function as GPs. Based on this framework, we\nintroduce a general practice benchmark (GPBench), whose data are meticulously\nannotated by domain experts in accordance with routine clinical practice\nstandards. We evaluate ten state-of-the-art LLMs and analyze their\ncompetencies. Our findings indicate that current LLMs are not yet ready for\ndeployment in such settings without human oversight, and further optimization\nspecifically tailored to the daily responsibilities of GPs is essential."
                },
                "authors": [
                    {
                        "name": "Zheqing Li"
                    },
                    {
                        "name": "Yiying Yang"
                    },
                    {
                        "name": "Jiping Lang"
                    },
                    {
                        "name": "Wenhao Jiang"
                    },
                    {
                        "name": "Yuhang Zhao"
                    },
                    {
                        "name": "Shuang Li"
                    },
                    {
                        "name": "Dingqian Wang"
                    },
                    {
                        "name": "Zhu Lin"
                    },
                    {
                        "name": "Xuanna Li"
                    },
                    {
                        "name": "Yuze Tang"
                    },
                    {
                        "name": "Jiexian Qiu"
                    },
                    {
                        "name": "Xiaolin Lu"
                    },
                    {
                        "name": "Hongji Yu"
                    },
                    {
                        "name": "Shuang Chen"
                    },
                    {
                        "name": "Yuhua Bi"
                    },
                    {
                        "name": "Xiaofei Zeng"
                    },
                    {
                        "name": "Yixian Chen"
                    },
                    {
                        "name": "Junrong Chen"
                    },
                    {
                        "name": "Lin Yao"
                    }
                ],
                "author_detail": {
                    "name": "Lin Yao"
                },
                "author": "Lin Yao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17599v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17599v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09261v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09261v1",
                "updated": "2025-05-14T10:22:13Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    10,
                    22,
                    13,
                    2,
                    134,
                    0
                ],
                "published": "2025-05-14T10:22:13Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    10,
                    22,
                    13,
                    2,
                    134,
                    0
                ],
                "title": "Instantiating Standards: Enabling Standard-Driven Text TTP Extraction\n  with Evolvable Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instantiating Standards: Enabling Standard-Driven Text TTP Extraction\n  with Evolvable Memory"
                },
                "summary": "Extracting MITRE ATT\\&CK Tactics, Techniques, and Procedures (TTPs) from\nnatural language threat reports is crucial yet challenging. Existing methods\nprimarily focus on performance metrics using data-driven approaches, often\nneglecting mechanisms to ensure faithful adherence to the official standard.\nThis deficiency compromises reliability and consistency of TTP assignments,\ncreating intelligence silos and contradictory threat assessments across\norganizations. To address this, we introduce a novel framework that converts\nabstract standard definitions into actionable, contextualized knowledge. Our\nmethod utilizes Large Language Model (LLM) to generate, update, and apply this\nknowledge. This framework populates an evolvable memory with dual-layer\nsituational knowledge instances derived from labeled examples and official\ndefinitions. The first layer identifies situational contexts (e.g.,\n\"Communication with C2 using encoded subdomains\"), while the second layer\ncaptures distinctive features that differentiate similar techniques (e.g.,\ndistinguishing T1132 \"Data Encoding\" from T1071 \"Application Layer Protocol\"\nbased on whether the focus is on encoding methods or protocol usage). This\nstructured approach provides a transparent basis for explainable TTP\nassignments and enhanced human oversight, while also helping to standardize\nother TTP extraction systems. Experiments show our framework (using\nQwen2.5-32B) boosts Technique F1 scores by 11\\% over GPT-4o. Qualitative\nanalysis confirms superior standardization, enhanced transparency, and improved\nexplainability in real-world threat intelligence scenarios. To the best of our\nknowledge, this is the first work that uses the LLM to generate, update, and\napply the a new knowledge for TTP extraction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extracting MITRE ATT\\&CK Tactics, Techniques, and Procedures (TTPs) from\nnatural language threat reports is crucial yet challenging. Existing methods\nprimarily focus on performance metrics using data-driven approaches, often\nneglecting mechanisms to ensure faithful adherence to the official standard.\nThis deficiency compromises reliability and consistency of TTP assignments,\ncreating intelligence silos and contradictory threat assessments across\norganizations. To address this, we introduce a novel framework that converts\nabstract standard definitions into actionable, contextualized knowledge. Our\nmethod utilizes Large Language Model (LLM) to generate, update, and apply this\nknowledge. This framework populates an evolvable memory with dual-layer\nsituational knowledge instances derived from labeled examples and official\ndefinitions. The first layer identifies situational contexts (e.g.,\n\"Communication with C2 using encoded subdomains\"), while the second layer\ncaptures distinctive features that differentiate similar techniques (e.g.,\ndistinguishing T1132 \"Data Encoding\" from T1071 \"Application Layer Protocol\"\nbased on whether the focus is on encoding methods or protocol usage). This\nstructured approach provides a transparent basis for explainable TTP\nassignments and enhanced human oversight, while also helping to standardize\nother TTP extraction systems. Experiments show our framework (using\nQwen2.5-32B) boosts Technique F1 scores by 11\\% over GPT-4o. Qualitative\nanalysis confirms superior standardization, enhanced transparency, and improved\nexplainability in real-world threat intelligence scenarios. To the best of our\nknowledge, this is the first work that uses the LLM to generate, update, and\napply the a new knowledge for TTP extraction."
                },
                "authors": [
                    {
                        "name": "Cheng Meng"
                    },
                    {
                        "name": "ZhengWei Jiang"
                    },
                    {
                        "name": "QiuYun Wang"
                    },
                    {
                        "name": "XinYi Li"
                    },
                    {
                        "name": "ChunYan Ma"
                    },
                    {
                        "name": "FangMing Dong"
                    },
                    {
                        "name": "FangLi Ren"
                    },
                    {
                        "name": "BaoXu Liu"
                    }
                ],
                "author_detail": {
                    "name": "BaoXu Liu"
                },
                "author": "BaoXu Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09261v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09261v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.04526v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.04526v4",
                "updated": "2025-05-15T02:17:31Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    2,
                    17,
                    31,
                    3,
                    135,
                    0
                ],
                "published": "2024-10-06T15:41:26Z",
                "published_parsed": [
                    2024,
                    10,
                    6,
                    15,
                    41,
                    26,
                    6,
                    280,
                    0
                ],
                "title": "FAMMA: A Benchmark for Financial Domain Multilingual Multimodal Question\n  Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FAMMA: A Benchmark for Financial Domain Multilingual Multimodal Question\n  Answering"
                },
                "summary": "In this paper, we introduce FAMMA, an open-source benchmark for\n\\underline{f}in\\underline{a}ncial \\underline{m}ultilingual\n\\underline{m}ultimodal question \\underline{a}nswering (QA). Our benchmark aims\nto evaluate the abilities of large language models (LLMs) in answering complex\nreasoning questions that require advanced financial knowledge. The benchmark\nhas two versions: FAMMA-Basic consists of 1,945 questions extracted from\nuniversity textbooks and exams, along with human-annotated answers and\nrationales; FAMMA-LivePro consists of 103 novel questions created by human\ndomain experts, with answers and rationales held out from the public for a\ncontamination-free evaluation. These questions cover advanced knowledge of 8\nmajor subfields in finance (e.g., corporate finance, derivatives, and portfolio\nmanagement). Some are in Chinese or French, while a majority of them are in\nEnglish. Each question has some non-text data such as charts, diagrams, or\ntables. Our experiments reveal that FAMMA poses a significant challenge on\nLLMs, including reasoning models such as GPT-o1 and DeepSeek-R1. Additionally,\nwe curated 1,270 reasoning trajectories of DeepSeek-R1 on the FAMMA-Basic data,\nand fine-tuned a series of open-source Qwen models using this reasoning data.\nWe found that training a model on these reasoning trajectories can\nsignificantly improve its performance on FAMMA-LivePro. We released our\nleaderboard, data, code, and trained models at\nhttps://famma-bench.github.io/famma/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce FAMMA, an open-source benchmark for\n\\underline{f}in\\underline{a}ncial \\underline{m}ultilingual\n\\underline{m}ultimodal question \\underline{a}nswering (QA). Our benchmark aims\nto evaluate the abilities of large language models (LLMs) in answering complex\nreasoning questions that require advanced financial knowledge. The benchmark\nhas two versions: FAMMA-Basic consists of 1,945 questions extracted from\nuniversity textbooks and exams, along with human-annotated answers and\nrationales; FAMMA-LivePro consists of 103 novel questions created by human\ndomain experts, with answers and rationales held out from the public for a\ncontamination-free evaluation. These questions cover advanced knowledge of 8\nmajor subfields in finance (e.g., corporate finance, derivatives, and portfolio\nmanagement). Some are in Chinese or French, while a majority of them are in\nEnglish. Each question has some non-text data such as charts, diagrams, or\ntables. Our experiments reveal that FAMMA poses a significant challenge on\nLLMs, including reasoning models such as GPT-o1 and DeepSeek-R1. Additionally,\nwe curated 1,270 reasoning trajectories of DeepSeek-R1 on the FAMMA-Basic data,\nand fine-tuned a series of open-source Qwen models using this reasoning data.\nWe found that training a model on these reasoning trajectories can\nsignificantly improve its performance on FAMMA-LivePro. We released our\nleaderboard, data, code, and trained models at\nhttps://famma-bench.github.io/famma/."
                },
                "authors": [
                    {
                        "name": "Siqiao Xue"
                    },
                    {
                        "name": "Xiaojing Li"
                    },
                    {
                        "name": "Fan Zhou"
                    },
                    {
                        "name": "Qingyang Dai"
                    },
                    {
                        "name": "Zhixuan Chu"
                    },
                    {
                        "name": "Hongyuan Mei"
                    }
                ],
                "author_detail": {
                    "name": "Hongyuan Mei"
                },
                "author": "Hongyuan Mei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.04526v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.04526v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13881v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13881v3",
                "updated": "2025-05-14T10:12:27Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    10,
                    12,
                    27,
                    2,
                    134,
                    0
                ],
                "published": "2025-01-23T17:56:07Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    17,
                    56,
                    7,
                    3,
                    23,
                    0
                ],
                "title": "The machine learning platform for developers of large systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The machine learning platform for developers of large systems"
                },
                "summary": "The machine learning system in the form of Retrieval Augmented Generation\n(RAG) has developed steadily since about 2021. RAG could be observed as a\nversion of the knowledge transfer. In the studied case, the large computing\nsystems are observed as the application point of RAG, which includes large\nlanguage model (LLM), as a partner for the developing team. Such an approach\nhas advantages during the development process and further in exploitation time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The machine learning system in the form of Retrieval Augmented Generation\n(RAG) has developed steadily since about 2021. RAG could be observed as a\nversion of the knowledge transfer. In the studied case, the large computing\nsystems are observed as the application point of RAG, which includes large\nlanguage model (LLM), as a partner for the developing team. Such an approach\nhas advantages during the development process and further in exploitation time."
                },
                "authors": [
                    {
                        "name": "Alexey Naikov"
                    },
                    {
                        "name": "Anatoly Oreshkin"
                    },
                    {
                        "name": "Alexey Shvetsov"
                    },
                    {
                        "name": "Andrey Shevel"
                    }
                ],
                "author_detail": {
                    "name": "Andrey Shevel"
                },
                "author": "Andrey Shevel",
                "arxiv_comment": "5 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13881v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13881v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.gen-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.gen-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "J.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09256v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09256v1",
                "updated": "2025-05-14T10:11:35Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    10,
                    11,
                    35,
                    2,
                    134,
                    0
                ],
                "published": "2025-05-14T10:11:35Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    10,
                    11,
                    35,
                    2,
                    134,
                    0
                ],
                "title": "Test-Time Augmentation for Pose-invariant Face Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-Time Augmentation for Pose-invariant Face Recognition"
                },
                "summary": "The goal of this paper is to enhance face recognition performance by\naugmenting head poses during the testing phase. Existing methods often rely on\ntraining on frontalised images or learning pose-invariant representations, yet\nboth approaches typically require re-training and testing for each dataset,\ninvolving a substantial amount of effort. In contrast, this study proposes\nPose-TTA, a novel approach that aligns faces at inference time without\nadditional training. To achieve this, we employ a portrait animator that\ntransfers the source image identity into the pose of a driving image. Instead\nof frontalising a side-profile face -- which can introduce distortion --\nPose-TTA generates matching side-profile images for comparison, thereby\nreducing identity information loss. Furthermore, we propose a weighted feature\naggregation strategy to address any distortions or biases arising from the\nsynthetic data, thus enhancing the reliability of the augmented images.\nExtensive experiments on diverse datasets and with various pre-trained face\nrecognition models demonstrate that Pose-TTA consistently improves inference\nperformance. Moreover, our method is straightforward to integrate into existing\nface recognition pipelines, as it requires no retraining or fine-tuning of the\nunderlying recognition models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The goal of this paper is to enhance face recognition performance by\naugmenting head poses during the testing phase. Existing methods often rely on\ntraining on frontalised images or learning pose-invariant representations, yet\nboth approaches typically require re-training and testing for each dataset,\ninvolving a substantial amount of effort. In contrast, this study proposes\nPose-TTA, a novel approach that aligns faces at inference time without\nadditional training. To achieve this, we employ a portrait animator that\ntransfers the source image identity into the pose of a driving image. Instead\nof frontalising a side-profile face -- which can introduce distortion --\nPose-TTA generates matching side-profile images for comparison, thereby\nreducing identity information loss. Furthermore, we propose a weighted feature\naggregation strategy to address any distortions or biases arising from the\nsynthetic data, thus enhancing the reliability of the augmented images.\nExtensive experiments on diverse datasets and with various pre-trained face\nrecognition models demonstrate that Pose-TTA consistently improves inference\nperformance. Moreover, our method is straightforward to integrate into existing\nface recognition pipelines, as it requires no retraining or fine-tuning of the\nunderlying recognition models."
                },
                "authors": [
                    {
                        "name": "Jaemin Jung"
                    },
                    {
                        "name": "Youngjoon Jang"
                    },
                    {
                        "name": "Joon Son Chung"
                    }
                ],
                "author_detail": {
                    "name": "Joon Son Chung"
                },
                "author": "Joon Son Chung",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09256v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09256v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09254v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09254v1",
                "updated": "2025-05-14T10:02:25Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    10,
                    2,
                    25,
                    2,
                    134,
                    0
                ],
                "published": "2025-05-14T10:02:25Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    10,
                    2,
                    25,
                    2,
                    134,
                    0
                ],
                "title": "Moving towards informative and actionable social media research",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Moving towards informative and actionable social media research"
                },
                "summary": "Social media is nearly ubiquitous in modern life, and concerns have been\nraised about its putative societal impacts, ranging from undermining mental\nhealth and exacerbating polarization to fomenting violence and disrupting\ndemocracy. Despite extensive research, consensus on these effects remains\nelusive, with observational studies often highlighting concerns while\nrandomized controlled trials (RCTs) yield conflicting or null findings. This\nreview examines how the complexity inherent in social systems can account for\nsuch discrepancies, emphasizing that emergent societal and long-term outcomes\ncannot be readily inferred from individual-level effects. In complex systems,\nsuch as social networks, feedback loops, hysteresis, multi-scale dynamics, and\nnon-linearity limit the utility of approaches for assessing causality that are\notherwise robust in simpler contexts. Revisiting large-scale experiments, we\nexplore how null or conflicting findings may reflect these complexities rather\nthan a true absence of effects. Even in cases where the methods are\nappropriate, assessing the net impacts of social media provides little\nactionable insight given that eliminating social media is not a realistic\noption for whole populations. We argue that progress will require a\ncomplexity-minded approach focused on specific design choices of online\nplatforms that triangulates experimental, observational and theoretical\nmethods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Social media is nearly ubiquitous in modern life, and concerns have been\nraised about its putative societal impacts, ranging from undermining mental\nhealth and exacerbating polarization to fomenting violence and disrupting\ndemocracy. Despite extensive research, consensus on these effects remains\nelusive, with observational studies often highlighting concerns while\nrandomized controlled trials (RCTs) yield conflicting or null findings. This\nreview examines how the complexity inherent in social systems can account for\nsuch discrepancies, emphasizing that emergent societal and long-term outcomes\ncannot be readily inferred from individual-level effects. In complex systems,\nsuch as social networks, feedback loops, hysteresis, multi-scale dynamics, and\nnon-linearity limit the utility of approaches for assessing causality that are\notherwise robust in simpler contexts. Revisiting large-scale experiments, we\nexplore how null or conflicting findings may reflect these complexities rather\nthan a true absence of effects. Even in cases where the methods are\nappropriate, assessing the net impacts of social media provides little\nactionable insight given that eliminating social media is not a realistic\noption for whole populations. We argue that progress will require a\ncomplexity-minded approach focused on specific design choices of online\nplatforms that triangulates experimental, observational and theoretical\nmethods."
                },
                "authors": [
                    {
                        "name": "Joseph B. Bak-Coleman"
                    },
                    {
                        "name": "Stephan Lewandowsky"
                    },
                    {
                        "name": "Philipp Lorenz-Spreen"
                    },
                    {
                        "name": "Arvind Narayanan"
                    },
                    {
                        "name": "Amy Orben"
                    },
                    {
                        "name": "Lisa Oswald"
                    }
                ],
                "author_detail": {
                    "name": "Lisa Oswald"
                },
                "author": "Lisa Oswald",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09254v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09254v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nlin.AO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09247v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09247v1",
                "updated": "2025-05-14T09:36:16Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    9,
                    36,
                    16,
                    2,
                    134,
                    0
                ],
                "published": "2025-05-14T09:36:16Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    9,
                    36,
                    16,
                    2,
                    134,
                    0
                ],
                "title": "Semiparametric marginal promotion time cure model for clustered survival\n  data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semiparametric marginal promotion time cure model for clustered survival\n  data"
                },
                "summary": "Modeling clustered/correlated failure time data has been becoming\nincreasingly important in clinical trials and epidemiology studies. In this\npaper, we consider a semiparametric marginal promotion time cure model for\nclustered right-censored survival data with a cure fraction. We propose two\nestimation methods based on the generalized estimating equations and the\nquadratic inference functions and prove that the regression estimates from the\ntwo proposed methods are consistent and asymptotic normal and that the\nestimates from the quadratic inference functions are optimal. The simulation\nstudy shows that the estimates from both methods are more efficient than those\nfrom the existing method no matter whether the correlation structure is\ncorrectly specified. The estimates based on the quadratic inference functions\nachieve higher efficiency compared with those based on the generalized\nestimating equations under the same working correlation structure. An\napplication of the proposed methods is demonstrated with periodontal disease\ndata and new findings are revealed in the analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling clustered/correlated failure time data has been becoming\nincreasingly important in clinical trials and epidemiology studies. In this\npaper, we consider a semiparametric marginal promotion time cure model for\nclustered right-censored survival data with a cure fraction. We propose two\nestimation methods based on the generalized estimating equations and the\nquadratic inference functions and prove that the regression estimates from the\ntwo proposed methods are consistent and asymptotic normal and that the\nestimates from the quadratic inference functions are optimal. The simulation\nstudy shows that the estimates from both methods are more efficient than those\nfrom the existing method no matter whether the correlation structure is\ncorrectly specified. The estimates based on the quadratic inference functions\nachieve higher efficiency compared with those based on the generalized\nestimating equations under the same working correlation structure. An\napplication of the proposed methods is demonstrated with periodontal disease\ndata and new findings are revealed in the analysis."
                },
                "authors": [
                    {
                        "name": "Fei Xiao"
                    },
                    {
                        "name": "Yingwei Peng"
                    },
                    {
                        "name": "Dipankar Bandyopadhyayd"
                    },
                    {
                        "name": "Yi Niu"
                    }
                ],
                "author_detail": {
                    "name": "Yi Niu"
                },
                "author": "Yi Niu",
                "arxiv_comment": "27 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09247v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09247v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62N02 (Primary) 62H12 (Secondary)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09246v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09246v1",
                "updated": "2025-05-14T09:35:56Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    9,
                    35,
                    56,
                    2,
                    134,
                    0
                ],
                "published": "2025-05-14T09:35:56Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    9,
                    35,
                    56,
                    2,
                    134,
                    0
                ],
                "title": "Focus, Merge, Rank: Improved Question Answering Based on Semi-structured\n  Knowledge Bases",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Focus, Merge, Rank: Improved Question Answering Based on Semi-structured\n  Knowledge Bases"
                },
                "summary": "In many real-world settings, machine learning models and interactive systems\nhave access to both structured knowledge, e.g., knowledge graphs or tables, and\nunstructured content, e.g., natural language documents. However, most rely on\neither. Semi-Structured Knowledge Bases (SKBs) bridge this gap by linking\nunstructured content to nodes within structured data, thereby enabling new\nstrategies for knowledge access and use. In this work, we present\nFocusedRetriever, a modular SKB-based framework for multi-hop question\nanswering. It integrates components (VSS-based entity search, LLM-based\ngeneration of Cypher queries and pairwise re-ranking) in a way that enables it\nto outperform state-of-the-art methods across all three STaRK benchmark test\nsets, covering diverse domains and multiple performance metrics. The average\nfirst-hit rate exceeds that of the second-best method by 25.7%.\nFocusedRetriever leverages (1) the capacity of Large Language Models (LLMs) to\nextract relational facts and entity attributes from unstructured text, (2) node\nset joins to filter answer candidates based on these extracted triplets and\nconstraints, (3) vector similarity search to retrieve and rank relevant\nunstructured content, and (4) the contextual capabilities of LLMs to finally\nrank the top-k answers. For generality, we only incorporate base LLMs in\nFocusedRetriever in our evaluation. However, our analysis of intermediate\nresults highlights several opportunities for further upgrades including\nfinetuning. The source code is publicly available at\nhttps://github.com/kramerlab/FocusedRetriever .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In many real-world settings, machine learning models and interactive systems\nhave access to both structured knowledge, e.g., knowledge graphs or tables, and\nunstructured content, e.g., natural language documents. However, most rely on\neither. Semi-Structured Knowledge Bases (SKBs) bridge this gap by linking\nunstructured content to nodes within structured data, thereby enabling new\nstrategies for knowledge access and use. In this work, we present\nFocusedRetriever, a modular SKB-based framework for multi-hop question\nanswering. It integrates components (VSS-based entity search, LLM-based\ngeneration of Cypher queries and pairwise re-ranking) in a way that enables it\nto outperform state-of-the-art methods across all three STaRK benchmark test\nsets, covering diverse domains and multiple performance metrics. The average\nfirst-hit rate exceeds that of the second-best method by 25.7%.\nFocusedRetriever leverages (1) the capacity of Large Language Models (LLMs) to\nextract relational facts and entity attributes from unstructured text, (2) node\nset joins to filter answer candidates based on these extracted triplets and\nconstraints, (3) vector similarity search to retrieve and rank relevant\nunstructured content, and (4) the contextual capabilities of LLMs to finally\nrank the top-k answers. For generality, we only incorporate base LLMs in\nFocusedRetriever in our evaluation. However, our analysis of intermediate\nresults highlights several opportunities for further upgrades including\nfinetuning. The source code is publicly available at\nhttps://github.com/kramerlab/FocusedRetriever ."
                },
                "authors": [
                    {
                        "name": "Derian Boer"
                    },
                    {
                        "name": "Stephen Roth"
                    },
                    {
                        "name": "Stefan Kramer"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Kramer"
                },
                "author": "Stefan Kramer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09246v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09246v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09219v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09219v1",
                "updated": "2025-05-14T08:40:19Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    8,
                    40,
                    19,
                    2,
                    134,
                    0
                ],
                "published": "2025-05-14T08:40:19Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    8,
                    40,
                    19,
                    2,
                    134,
                    0
                ],
                "title": "Probing Self-Interacting Dark Matter via Gravitational-Wave Background\n  from Eccentric Supermassive Black Hole Mergers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probing Self-Interacting Dark Matter via Gravitational-Wave Background\n  from Eccentric Supermassive Black Hole Mergers"
                },
                "summary": "The nature of dark matter is still mysterious despite various astronomical\nevidence. As a possible candidate, self-interacting dark matter (SIDM) can\npotentially resolve some issues appearing in cold dark matter paradigm. Here we\ninvestigate how SIDM around supermassive black holes (SMBH) in galaxy centers\nmay form a density spike and imprint in the spectrum shape of stochastic\ngravitational-wave background from SMBH binaries (SMBHBs). Employing a refined\ndynamical friction formula and consistently evolving the orbital dynamics, we\ndemonstrate that current pulsar timing arrays (PTAs) data is sensitive to the\ncross section of SIDM with\n$\\sigma(v)/m_\\chi\\lesssim0.66\\,\\mathrm{cm}^2/\\mathrm{g}$, comparable to other\nastrophysical probes. We also highlight the importance of including the\neccentricity of SMBHBs in the parameter inference, which would affect the\nresults significantly. Our findings reveal the promising potential of PTAs\nobservations in probing the nature of dark matter.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The nature of dark matter is still mysterious despite various astronomical\nevidence. As a possible candidate, self-interacting dark matter (SIDM) can\npotentially resolve some issues appearing in cold dark matter paradigm. Here we\ninvestigate how SIDM around supermassive black holes (SMBH) in galaxy centers\nmay form a density spike and imprint in the spectrum shape of stochastic\ngravitational-wave background from SMBH binaries (SMBHBs). Employing a refined\ndynamical friction formula and consistently evolving the orbital dynamics, we\ndemonstrate that current pulsar timing arrays (PTAs) data is sensitive to the\ncross section of SIDM with\n$\\sigma(v)/m_\\chi\\lesssim0.66\\,\\mathrm{cm}^2/\\mathrm{g}$, comparable to other\nastrophysical probes. We also highlight the importance of including the\neccentricity of SMBHBs in the parameter inference, which would affect the\nresults significantly. Our findings reveal the promising potential of PTAs\nobservations in probing the nature of dark matter."
                },
                "authors": [
                    {
                        "name": "Mu-Chun Chen"
                    },
                    {
                        "name": "Yong Tang"
                    }
                ],
                "author_detail": {
                    "name": "Yong Tang"
                },
                "author": "Yong Tang",
                "arxiv_comment": "15 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09219v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09219v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09214v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09214v1",
                "updated": "2025-05-14T08:18:55Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    8,
                    18,
                    55,
                    2,
                    134,
                    0
                ],
                "published": "2025-05-14T08:18:55Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    8,
                    18,
                    55,
                    2,
                    134,
                    0
                ],
                "title": "The Larger the Merrier? Efficient Large AI Model Inference in Wireless\n  Edge Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Larger the Merrier? Efficient Large AI Model Inference in Wireless\n  Edge Networks"
                },
                "summary": "The growing demand for large artificial intelligence model (LAIM) services is\ndriving a paradigm shift from traditional cloud-based inference to edge-based\ninference for low-latency, privacy-preserving applications. In particular,\nedge-device co-inference, which partitions LAIMs between edge devices and\nservers, has emerged as a promising strategy for resource-efficient LAIM\nexecution in wireless networks. In this paper, we investigate a pruning-aware\nLAIM co-inference scheme, where a pre-trained LAIM is pruned and partitioned\ninto on-device and on-server sub-models for deployment. For analysis, we first\nprove that the LAIM output distortion is upper bounded by its parameter\ndistortion. Then, we derive a lower bound on parameter distortion via\nrate-distortion theory, analytically capturing the relationship between pruning\nratio and co-inference performance. Next, based on the analytical results, we\nformulate an LAIM co-inference distortion bound minimization problem by jointly\noptimizing the pruning ratio, transmit power, and computation frequency under\nsystem latency, energy, and available resource constraints. Moreover, we\npropose an efficient algorithm to tackle the considered highly non-convex\nproblem. Finally, extensive simulations demonstrate the effectiveness of the\nproposed design. In particular, model parameter distortion is shown to provide\na reliable bound on output distortion. Also, the proposed joint pruning ratio\nand resource management design achieves superior performance in balancing\ntrade-offs among inference performance, system latency, and energy consumption\ncompared with benchmark schemes, such as fully on-device and on-server\ninference. Moreover, the split point is shown to play a critical role in system\nperformance optimization under heterogeneous and resource-limited edge\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing demand for large artificial intelligence model (LAIM) services is\ndriving a paradigm shift from traditional cloud-based inference to edge-based\ninference for low-latency, privacy-preserving applications. In particular,\nedge-device co-inference, which partitions LAIMs between edge devices and\nservers, has emerged as a promising strategy for resource-efficient LAIM\nexecution in wireless networks. In this paper, we investigate a pruning-aware\nLAIM co-inference scheme, where a pre-trained LAIM is pruned and partitioned\ninto on-device and on-server sub-models for deployment. For analysis, we first\nprove that the LAIM output distortion is upper bounded by its parameter\ndistortion. Then, we derive a lower bound on parameter distortion via\nrate-distortion theory, analytically capturing the relationship between pruning\nratio and co-inference performance. Next, based on the analytical results, we\nformulate an LAIM co-inference distortion bound minimization problem by jointly\noptimizing the pruning ratio, transmit power, and computation frequency under\nsystem latency, energy, and available resource constraints. Moreover, we\npropose an efficient algorithm to tackle the considered highly non-convex\nproblem. Finally, extensive simulations demonstrate the effectiveness of the\nproposed design. In particular, model parameter distortion is shown to provide\na reliable bound on output distortion. Also, the proposed joint pruning ratio\nand resource management design achieves superior performance in balancing\ntrade-offs among inference performance, system latency, and energy consumption\ncompared with benchmark schemes, such as fully on-device and on-server\ninference. Moreover, the split point is shown to play a critical role in system\nperformance optimization under heterogeneous and resource-limited edge\nenvironments."
                },
                "authors": [
                    {
                        "name": "Zhonghao Lyu"
                    },
                    {
                        "name": "Ming Xiao"
                    },
                    {
                        "name": "Jie Xu"
                    },
                    {
                        "name": "Mikael Skoglund"
                    },
                    {
                        "name": "Marco Di Renzo"
                    }
                ],
                "author_detail": {
                    "name": "Marco Di Renzo"
                },
                "author": "Marco Di Renzo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09214v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09214v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09205v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09205v1",
                "updated": "2025-05-14T07:34:36Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    7,
                    34,
                    36,
                    2,
                    134,
                    0
                ],
                "published": "2025-05-14T07:34:36Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    7,
                    34,
                    36,
                    2,
                    134,
                    0
                ],
                "title": "HMamba: Hyperbolic Mamba for Sequential Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HMamba: Hyperbolic Mamba for Sequential Recommendation"
                },
                "summary": "Sequential recommendation systems have become a cornerstone of personalized\nservices, adept at modeling the temporal evolution of user preferences by\ncapturing dynamic interaction sequences. Existing approaches predominantly rely\non traditional models, including RNNs and Transformers. Despite their success\nin local pattern recognition, Transformer-based methods suffer from quadratic\ncomputational complexity and a tendency toward superficial attention patterns,\nlimiting their ability to infer enduring preference hierarchies in sequential\nrecommendation data. Recent advances in Mamba-based sequential models introduce\nlinear-time efficiency but remain constrained by Euclidean geometry, failing to\nleverage the intrinsic hyperbolic structure of recommendation data. To bridge\nthis gap, we propose Hyperbolic Mamba, a novel architecture that unifies the\nefficiency of Mamba's selective state space mechanism with hyperbolic\ngeometry's hierarchical representational power. Our framework introduces (1) a\nhyperbolic selective state space that maintains curvature-aware sequence\nmodeling and (2) stabilized Riemannian operations to enable scalable training.\nExperiments across four benchmarks demonstrate that Hyperbolic Mamba achieves\n3-11% improvement while retaining Mamba's linear-time efficiency, enabling\nreal-world deployment. This work establishes a new paradigm for efficient,\nhierarchy-aware sequential modeling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequential recommendation systems have become a cornerstone of personalized\nservices, adept at modeling the temporal evolution of user preferences by\ncapturing dynamic interaction sequences. Existing approaches predominantly rely\non traditional models, including RNNs and Transformers. Despite their success\nin local pattern recognition, Transformer-based methods suffer from quadratic\ncomputational complexity and a tendency toward superficial attention patterns,\nlimiting their ability to infer enduring preference hierarchies in sequential\nrecommendation data. Recent advances in Mamba-based sequential models introduce\nlinear-time efficiency but remain constrained by Euclidean geometry, failing to\nleverage the intrinsic hyperbolic structure of recommendation data. To bridge\nthis gap, we propose Hyperbolic Mamba, a novel architecture that unifies the\nefficiency of Mamba's selective state space mechanism with hyperbolic\ngeometry's hierarchical representational power. Our framework introduces (1) a\nhyperbolic selective state space that maintains curvature-aware sequence\nmodeling and (2) stabilized Riemannian operations to enable scalable training.\nExperiments across four benchmarks demonstrate that Hyperbolic Mamba achieves\n3-11% improvement while retaining Mamba's linear-time efficiency, enabling\nreal-world deployment. This work establishes a new paradigm for efficient,\nhierarchy-aware sequential modeling."
                },
                "authors": [
                    {
                        "name": "Qianru Zhang"
                    },
                    {
                        "name": "Honggang Wen"
                    },
                    {
                        "name": "Wei Yuan"
                    },
                    {
                        "name": "Crystal Chen"
                    },
                    {
                        "name": "Menglin Yang"
                    },
                    {
                        "name": "Siu-Ming Yiu"
                    },
                    {
                        "name": "Hongzhi Yin"
                    }
                ],
                "author_detail": {
                    "name": "Hongzhi Yin"
                },
                "author": "Hongzhi Yin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09205v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09205v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09197v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09197v1",
                "updated": "2025-05-14T07:18:06Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    7,
                    18,
                    6,
                    2,
                    134,
                    0
                ],
                "published": "2025-05-14T07:18:06Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    7,
                    18,
                    6,
                    2,
                    134,
                    0
                ],
                "title": "Generalizing imaging biomarker repeatability studies using Bayesian\n  inference: Applications in detecting heterogeneous treatment response in\n  whole-body diffusion-weighted MRI of metastatic prostate cancer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generalizing imaging biomarker repeatability studies using Bayesian\n  inference: Applications in detecting heterogeneous treatment response in\n  whole-body diffusion-weighted MRI of metastatic prostate cancer"
                },
                "summary": "The assessment of imaging biomarkers is critical for advancing precision\nmedicine and improving disease characterization. Despite the availability of\nmethods to derive disease heterogeneity metrics in imaging studies, a robust\nframework for evaluating measurement uncertainty remains underdeveloped. To\naddress this gap, we propose a novel Bayesian framework to assess the precision\nof disease heterogeneity measures in biomarker studies.\n  Our approach extends traditional methods for evaluating biomarker precision\nby providing greater flexibility in statistical assumptions and enabling the\nanalysis of biomarkers beyond univariate or multivariate normally-distributed\nvariables. Using Hamiltonian Monte Carlo sampling, the framework supports both,\nfor example, normally-distributed and Dirichlet-Multinomial distributed\nvariables, enabling the derivation of posterior distributions for biomarker\nparameters under diverse model assumptions. Designed to be broadly applicable\nacross various imaging modalities and biomarker types, the framework builds a\nfoundation for generalizing reproducible and objective biomarker evaluation.\n  To demonstrate utility, we apply the framework to whole-body\ndiffusion-weighted MRI (WBDWI) to assess heterogeneous therapeutic responses in\nmetastatic bone disease. Specifically, we analyze data from two patient studies\ninvestigating treatments for metastatic castrate-resistant prostate cancer\n(mCRPC). Our results reveal an approximately 70% response rate among individual\ntumors across both studies, objectively characterizing differential responses\nto systemic therapies and validating the clinical relevance of the proposed\nmethodology.\n  This Bayesian framework provides a powerful tool for advancing biomarker\nresearch across diverse imaging-based studies while offering valuable insights\ninto specific clinical applications, such as mCRPC treatment response.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The assessment of imaging biomarkers is critical for advancing precision\nmedicine and improving disease characterization. Despite the availability of\nmethods to derive disease heterogeneity metrics in imaging studies, a robust\nframework for evaluating measurement uncertainty remains underdeveloped. To\naddress this gap, we propose a novel Bayesian framework to assess the precision\nof disease heterogeneity measures in biomarker studies.\n  Our approach extends traditional methods for evaluating biomarker precision\nby providing greater flexibility in statistical assumptions and enabling the\nanalysis of biomarkers beyond univariate or multivariate normally-distributed\nvariables. Using Hamiltonian Monte Carlo sampling, the framework supports both,\nfor example, normally-distributed and Dirichlet-Multinomial distributed\nvariables, enabling the derivation of posterior distributions for biomarker\nparameters under diverse model assumptions. Designed to be broadly applicable\nacross various imaging modalities and biomarker types, the framework builds a\nfoundation for generalizing reproducible and objective biomarker evaluation.\n  To demonstrate utility, we apply the framework to whole-body\ndiffusion-weighted MRI (WBDWI) to assess heterogeneous therapeutic responses in\nmetastatic bone disease. Specifically, we analyze data from two patient studies\ninvestigating treatments for metastatic castrate-resistant prostate cancer\n(mCRPC). Our results reveal an approximately 70% response rate among individual\ntumors across both studies, objectively characterizing differential responses\nto systemic therapies and validating the clinical relevance of the proposed\nmethodology.\n  This Bayesian framework provides a powerful tool for advancing biomarker\nresearch across diverse imaging-based studies while offering valuable insights\ninto specific clinical applications, such as mCRPC treatment response."
                },
                "authors": [
                    {
                        "name": "Matthew D Blackledge"
                    },
                    {
                        "name": "Konstantinos Zormpas-Petridis"
                    },
                    {
                        "name": "Ricardo Donners"
                    },
                    {
                        "name": "Antonio Candito"
                    },
                    {
                        "name": "David J Collins"
                    },
                    {
                        "name": "Johann de Bono"
                    },
                    {
                        "name": "Chris Parker"
                    },
                    {
                        "name": "Dow-Mu Koh"
                    },
                    {
                        "name": "Nina Tunariu"
                    }
                ],
                "author_detail": {
                    "name": "Nina Tunariu"
                },
                "author": "Nina Tunariu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09197v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09197v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62P10 (Primary)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.4.7; I.4.9",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09176v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09176v1",
                "updated": "2025-05-14T06:13:26Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    6,
                    13,
                    26,
                    2,
                    134,
                    0
                ],
                "published": "2025-05-14T06:13:26Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    6,
                    13,
                    26,
                    2,
                    134,
                    0
                ],
                "title": "Late-time suppression of structure growth as a solution for the $S_8$\n  tension",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Late-time suppression of structure growth as a solution for the $S_8$\n  tension"
                },
                "summary": "The $S_8$ value inferred from the Subaru Hyper Suprime-Cam (HSC) Year 3\ncosmic shear data, under the assumption of the flat $\\Lambda$CDM model, is\n2-3$\\sigma$ lower than that inferred from observations of the early-time\nuniverse, such as cosmic microwave background (CMB) anisotropy data. Resolving\nthe $S_8$ tension requires a scenario in which structure formation on small\nscales is suppressed in the late universe. As potential solutions, we consider\nextended models both within and beyond the $\\Lambda$CDM model -- models that\nincorporate parameterized baryonic feedback effects, the effect of varying\nneutrino mass, and modified structure growth, each of which can lead to a\nsuppression of structure growth at lower redshifts, with its own distinct\nscale- and redshift-dependencies. In particular, we consider phenomenological\nmodified gravity models in which the suppression of structure growth is\ntriggered at lower redshifts, as dark energy ($\\Lambda$) begins to dominate the\nbackground expansion. We show that the modified growth factor models --\nespecially those featuring more rapid growth suppression at lower redshifts --\nprovide an improved fit to the combined datasets of the HSC-Y3 cosmic shear\ncorrelation functions, the Planck CMB, and the ACT DR6 CMB lensing, compared to\nthe fiducial $\\Lambda$CDM model and the models including the baryonic effects\nor the massive neutrino effect within the the $\\Lambda$CDM framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The $S_8$ value inferred from the Subaru Hyper Suprime-Cam (HSC) Year 3\ncosmic shear data, under the assumption of the flat $\\Lambda$CDM model, is\n2-3$\\sigma$ lower than that inferred from observations of the early-time\nuniverse, such as cosmic microwave background (CMB) anisotropy data. Resolving\nthe $S_8$ tension requires a scenario in which structure formation on small\nscales is suppressed in the late universe. As potential solutions, we consider\nextended models both within and beyond the $\\Lambda$CDM model -- models that\nincorporate parameterized baryonic feedback effects, the effect of varying\nneutrino mass, and modified structure growth, each of which can lead to a\nsuppression of structure growth at lower redshifts, with its own distinct\nscale- and redshift-dependencies. In particular, we consider phenomenological\nmodified gravity models in which the suppression of structure growth is\ntriggered at lower redshifts, as dark energy ($\\Lambda$) begins to dominate the\nbackground expansion. We show that the modified growth factor models --\nespecially those featuring more rapid growth suppression at lower redshifts --\nprovide an improved fit to the combined datasets of the HSC-Y3 cosmic shear\ncorrelation functions, the Planck CMB, and the ACT DR6 CMB lensing, compared to\nthe fiducial $\\Lambda$CDM model and the models including the baryonic effects\nor the massive neutrino effect within the the $\\Lambda$CDM framework."
                },
                "authors": [
                    {
                        "name": "Ryo Terasawa"
                    },
                    {
                        "name": "Masahiro Takada"
                    },
                    {
                        "name": "Toshiki Kurita"
                    },
                    {
                        "name": "Sunao Sugiyama"
                    }
                ],
                "author_detail": {
                    "name": "Sunao Sugiyama"
                },
                "author": "Sunao Sugiyama",
                "arxiv_comment": "13 pages, 7 figures. arXiv admin note: text overlap with\n  arXiv:2503.20396, arXiv:2403.20323",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09176v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09176v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08175v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08175v2",
                "updated": "2025-05-14T06:07:26Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    6,
                    7,
                    26,
                    2,
                    134,
                    0
                ],
                "published": "2025-05-13T02:25:47Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    2,
                    25,
                    47,
                    1,
                    133,
                    0
                ],
                "title": "Fast Text-to-Audio Generation with Adversarial Post-Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast Text-to-Audio Generation with Adversarial Post-Training"
                },
                "summary": "Text-to-audio systems, while increasingly performant, are slow at inference\ntime, thus making their latency unpractical for many creative applications. We\npresent Adversarial Relativistic-Contrastive (ARC) post-training, the first\nadversarial acceleration algorithm for diffusion/flow models not based on\ndistillation. While past adversarial post-training methods have struggled to\ncompare against their expensive distillation counterparts, ARC post-training is\na simple procedure that (1) extends a recent relativistic adversarial\nformulation to diffusion/flow post-training and (2) combines it with a novel\ncontrastive discriminator objective to encourage better prompt adherence. We\npair ARC post-training with a number optimizations to Stable Audio Open and\nbuild a model capable of generating $\\approx$12s of 44.1kHz stereo audio in\n$\\approx$75ms on an H100, and $\\approx$7s on a mobile edge-device, the fastest\ntext-to-audio model to our knowledge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-audio systems, while increasingly performant, are slow at inference\ntime, thus making their latency unpractical for many creative applications. We\npresent Adversarial Relativistic-Contrastive (ARC) post-training, the first\nadversarial acceleration algorithm for diffusion/flow models not based on\ndistillation. While past adversarial post-training methods have struggled to\ncompare against their expensive distillation counterparts, ARC post-training is\na simple procedure that (1) extends a recent relativistic adversarial\nformulation to diffusion/flow post-training and (2) combines it with a novel\ncontrastive discriminator objective to encourage better prompt adherence. We\npair ARC post-training with a number optimizations to Stable Audio Open and\nbuild a model capable of generating $\\approx$12s of 44.1kHz stereo audio in\n$\\approx$75ms on an H100, and $\\approx$7s on a mobile edge-device, the fastest\ntext-to-audio model to our knowledge."
                },
                "authors": [
                    {
                        "name": "Zachary Novack"
                    },
                    {
                        "name": "Zach Evans"
                    },
                    {
                        "name": "Zack Zukowski"
                    },
                    {
                        "name": "Josiah Taylor"
                    },
                    {
                        "name": "CJ Carr"
                    },
                    {
                        "name": "Julian Parker"
                    },
                    {
                        "name": "Adnan Al-Sinan"
                    },
                    {
                        "name": "Gian Marco Iodice"
                    },
                    {
                        "name": "Julian McAuley"
                    },
                    {
                        "name": "Taylor Berg-Kirkpatrick"
                    },
                    {
                        "name": "Jordi Pons"
                    }
                ],
                "author_detail": {
                    "name": "Jordi Pons"
                },
                "author": "Jordi Pons",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08175v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08175v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02247v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02247v3",
                "updated": "2025-05-14T06:06:20Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    6,
                    6,
                    20,
                    2,
                    134,
                    0
                ],
                "published": "2024-10-03T06:37:37Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    6,
                    37,
                    37,
                    3,
                    277,
                    0
                ],
                "title": "Theoretical Insights into Fine-Tuning Attention Mechanism:\n  Generalization and Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Theoretical Insights into Fine-Tuning Attention Mechanism:\n  Generalization and Optimization"
                },
                "summary": "Large Language Models (LLMs), built on Transformer architectures, exhibit\nremarkable generalization across a wide range of tasks. However, fine-tuning\nthese models for specific tasks remains resource-intensive due to their\nextensive parameterization. In this paper, we explore two remarkable phenomena\nrelated to the attention mechanism during the fine-tuning of LLMs (where\n$\\mathbf{W}_q$, $\\mathbf{W}_k$, and $\\mathbf{W}_v$ denote the weights of the\nquery, key, and value layers, respectively). The first phenomenon, termed\n\"Unequal Importance of Attention Matrices\", highlights the impact of\nfine-tuning different weight matrices. It shows that optimizing the\n$\\mathbf{W}_v$ matrix yields significantly better performance than optimizing\nthe $\\mathbf{W}_k$ matrix. Fine-tuning only the $\\mathbf{W}_q$ and\n$\\mathbf{W}_v$ matrices is computationally efficient while delivering results\ncomparable to, or even better than fine-tuning all three matrices\n($\\mathbf{W}_q$, $\\mathbf{W}_k$, and $\\mathbf{W}_v$). The second\nphenomenon,\"Attention Matrices with Customized Learning Rate Lead to Better\nConvergence\", emphasizes the importance of assigning distinct learning rates to\nthese matrices. Specifically, a higher learning rate for the $\\mathbf{W}_v$\nmatrix compared to $\\mathbf{W}_q$ and $\\mathbf{W}_k$ accelerates convergence\nand improves performance. Building on these insights, we propose a new strategy\nthat improves fine-tuning efficiency in terms of both storage and time.\nExperimental results on benchmark datasets validate the effectiveness of this\napproach, supporting our theoretical findings. Our analysis lays the\ntheoretical groundwork for configuring and improving algorithms in LLMs\nfine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), built on Transformer architectures, exhibit\nremarkable generalization across a wide range of tasks. However, fine-tuning\nthese models for specific tasks remains resource-intensive due to their\nextensive parameterization. In this paper, we explore two remarkable phenomena\nrelated to the attention mechanism during the fine-tuning of LLMs (where\n$\\mathbf{W}_q$, $\\mathbf{W}_k$, and $\\mathbf{W}_v$ denote the weights of the\nquery, key, and value layers, respectively). The first phenomenon, termed\n\"Unequal Importance of Attention Matrices\", highlights the impact of\nfine-tuning different weight matrices. It shows that optimizing the\n$\\mathbf{W}_v$ matrix yields significantly better performance than optimizing\nthe $\\mathbf{W}_k$ matrix. Fine-tuning only the $\\mathbf{W}_q$ and\n$\\mathbf{W}_v$ matrices is computationally efficient while delivering results\ncomparable to, or even better than fine-tuning all three matrices\n($\\mathbf{W}_q$, $\\mathbf{W}_k$, and $\\mathbf{W}_v$). The second\nphenomenon,\"Attention Matrices with Customized Learning Rate Lead to Better\nConvergence\", emphasizes the importance of assigning distinct learning rates to\nthese matrices. Specifically, a higher learning rate for the $\\mathbf{W}_v$\nmatrix compared to $\\mathbf{W}_q$ and $\\mathbf{W}_k$ accelerates convergence\nand improves performance. Building on these insights, we propose a new strategy\nthat improves fine-tuning efficiency in terms of both storage and time.\nExperimental results on benchmark datasets validate the effectiveness of this\napproach, supporting our theoretical findings. Our analysis lays the\ntheoretical groundwork for configuring and improving algorithms in LLMs\nfine-tuning."
                },
                "authors": [
                    {
                        "name": "Xinhao Yao"
                    },
                    {
                        "name": "Hongjin Qian"
                    },
                    {
                        "name": "Xiaolin Hu"
                    },
                    {
                        "name": "Gengze Xu"
                    },
                    {
                        "name": "Wei Liu"
                    },
                    {
                        "name": "Jian Luan"
                    },
                    {
                        "name": "Bin Wang"
                    },
                    {
                        "name": "Yong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Liu"
                },
                "author": "Yong Liu",
                "arxiv_comment": "IJCAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02247v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02247v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.01383v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.01383v3",
                "updated": "2025-05-14T06:05:53Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    6,
                    5,
                    53,
                    2,
                    134,
                    0
                ],
                "published": "2024-02-02T13:06:35Z",
                "published_parsed": [
                    2024,
                    2,
                    2,
                    13,
                    6,
                    35,
                    4,
                    33,
                    0
                ],
                "title": "LLM-based NLG Evaluation: Current Status and Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based NLG Evaluation: Current Status and Challenges"
                },
                "summary": "Evaluating natural language generation (NLG) is a vital but challenging\nproblem in natural language processing. Traditional evaluation metrics mainly\ncapturing content (e.g. n-gram) overlap between system outputs and references\nare far from satisfactory, and large language models (LLMs) such as ChatGPT\nhave demonstrated great potential in NLG evaluation in recent years. Various\nautomatic evaluation methods based on LLMs have been proposed, including\nmetrics derived from LLMs, prompting LLMs, fine-tuning LLMs, and human-LLM\ncollaborative evaluation. In this survey, we first give a taxonomy of LLM-based\nNLG evaluation methods, and discuss their pros and cons, respectively. Lastly,\nwe discuss several open problems in this area and point out future research\ndirections.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating natural language generation (NLG) is a vital but challenging\nproblem in natural language processing. Traditional evaluation metrics mainly\ncapturing content (e.g. n-gram) overlap between system outputs and references\nare far from satisfactory, and large language models (LLMs) such as ChatGPT\nhave demonstrated great potential in NLG evaluation in recent years. Various\nautomatic evaluation methods based on LLMs have been proposed, including\nmetrics derived from LLMs, prompting LLMs, fine-tuning LLMs, and human-LLM\ncollaborative evaluation. In this survey, we first give a taxonomy of LLM-based\nNLG evaluation methods, and discuss their pros and cons, respectively. Lastly,\nwe discuss several open problems in this area and point out future research\ndirections."
                },
                "authors": [
                    {
                        "name": "Mingqi Gao"
                    },
                    {
                        "name": "Xinyu Hu"
                    },
                    {
                        "name": "Jie Ruan"
                    },
                    {
                        "name": "Xiao Pu"
                    },
                    {
                        "name": "Xiaojun Wan"
                    }
                ],
                "author_detail": {
                    "name": "Xiaojun Wan"
                },
                "author": "Xiaojun Wan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.01383v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.01383v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04405v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04405v2",
                "updated": "2025-05-14T05:23:45Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    5,
                    23,
                    45,
                    2,
                    134,
                    0
                ],
                "published": "2025-02-06T09:08:12Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    9,
                    8,
                    12,
                    3,
                    37,
                    0
                ],
                "title": "FAS: Fast ANN-SNN Conversion for Spiking Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FAS: Fast ANN-SNN Conversion for Spiking Large Language Models"
                },
                "summary": "Spiking Large Language Models have been shown as a good alternative to LLMs\nin various scenarios. Existing methods for creating Spiking LLMs, i.e., direct\ntraining and ANN-SNN conversion, often suffer from performance degradation and\nrelatively high computational costs. To address these issues, we propose a\nnovel Fast ANN-SNN conversion strategy (FAS) that transforms LLMs into spiking\nLLMs in two stages. The first stage employs a full-parameter fine-tuning of\npre-trained models, so it does not need any direct training from scratch. The\nsecond stage introduces a coarse-to-fine calibration method to reduce\nconversion errors and improve accuracy. Experiments on both language and\nvision-language tasks across four different scales of LLMs demonstrate that FAS\ncan achieve state-of-the-art performance yet with significantly reduced\ninference latency and computational costs. Notably, FAS only takes eight\ntimesteps to achieve an accuracy of 3\\% higher than that of the OPT-7B model,\nwhile reducing energy consumption by 96.63\\%. The source code is available at\nhttps://github.com/lc783/FAS",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spiking Large Language Models have been shown as a good alternative to LLMs\nin various scenarios. Existing methods for creating Spiking LLMs, i.e., direct\ntraining and ANN-SNN conversion, often suffer from performance degradation and\nrelatively high computational costs. To address these issues, we propose a\nnovel Fast ANN-SNN conversion strategy (FAS) that transforms LLMs into spiking\nLLMs in two stages. The first stage employs a full-parameter fine-tuning of\npre-trained models, so it does not need any direct training from scratch. The\nsecond stage introduces a coarse-to-fine calibration method to reduce\nconversion errors and improve accuracy. Experiments on both language and\nvision-language tasks across four different scales of LLMs demonstrate that FAS\ncan achieve state-of-the-art performance yet with significantly reduced\ninference latency and computational costs. Notably, FAS only takes eight\ntimesteps to achieve an accuracy of 3\\% higher than that of the OPT-7B model,\nwhile reducing energy consumption by 96.63\\%. The source code is available at\nhttps://github.com/lc783/FAS"
                },
                "authors": [
                    {
                        "name": "Long Chen"
                    },
                    {
                        "name": "Xiaotian Song"
                    },
                    {
                        "name": "Andy Song"
                    },
                    {
                        "name": "BaDong Chen"
                    },
                    {
                        "name": "Jiancheng Lv"
                    },
                    {
                        "name": "Yanan Sun"
                    }
                ],
                "author_detail": {
                    "name": "Yanan Sun"
                },
                "author": "Yanan Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04405v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04405v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09144v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09144v1",
                "updated": "2025-05-14T05:03:09Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    5,
                    3,
                    9,
                    2,
                    134,
                    0
                ],
                "published": "2025-05-14T05:03:09Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    5,
                    3,
                    9,
                    2,
                    134,
                    0
                ],
                "title": "Latent Theory of Mind: A Decentralized Diffusion Architecture for\n  Cooperative Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latent Theory of Mind: A Decentralized Diffusion Architecture for\n  Cooperative Manipulation"
                },
                "summary": "We present Latent Theory of Mind (LatentToM), a decentralized diffusion\npolicy architecture for collaborative robot manipulation. Our policy allows\nmultiple manipulators with their own perception and computation to collaborate\nwith each other towards a common task goal with or without explicit\ncommunication. Our key innovation lies in allowing each agent to maintain two\nlatent representations: an ego embedding specific to the robot, and a consensus\nembedding trained to be common to both robots, despite their different sensor\nstreams and poses. We further let each robot train a decoder to infer the other\nrobot's ego embedding from their consensus embedding, akin to theory of mind in\nlatent space. Training occurs centrally, with all the policies' consensus\nencoders supervised by a loss inspired by sheaf theory, a mathematical theory\nfor clustering data on a topological manifold. Specifically, we introduce a\nfirst-order cohomology loss to enforce sheaf-consistent alignment of the\nconsensus embeddings. To preserve the expressiveness of the consensus\nembedding, we further propose structural constraints based on theory of mind\nand a directional consensus mechanism. Execution can be fully distributed,\nrequiring no explicit communication between policies. In which case, the\ninformation is exchanged implicitly through each robot's sensor stream by\nobserving the actions of the other robots and their effects on the scene.\nAlternatively, execution can leverage direct communication to share the robots'\nconsensus embeddings, where the embeddings are shared once during each\ninference step and are aligned using the sheaf Laplacian. In our hardware\nexperiments, LatentToM outperforms a naive decentralized diffusion baseline,\nand shows comparable performance with a state-of-the-art centralized diffusion\npolicy for bi-manual manipulation. Project website:\nhttps://stanfordmsl.github.io/LatentToM/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Latent Theory of Mind (LatentToM), a decentralized diffusion\npolicy architecture for collaborative robot manipulation. Our policy allows\nmultiple manipulators with their own perception and computation to collaborate\nwith each other towards a common task goal with or without explicit\ncommunication. Our key innovation lies in allowing each agent to maintain two\nlatent representations: an ego embedding specific to the robot, and a consensus\nembedding trained to be common to both robots, despite their different sensor\nstreams and poses. We further let each robot train a decoder to infer the other\nrobot's ego embedding from their consensus embedding, akin to theory of mind in\nlatent space. Training occurs centrally, with all the policies' consensus\nencoders supervised by a loss inspired by sheaf theory, a mathematical theory\nfor clustering data on a topological manifold. Specifically, we introduce a\nfirst-order cohomology loss to enforce sheaf-consistent alignment of the\nconsensus embeddings. To preserve the expressiveness of the consensus\nembedding, we further propose structural constraints based on theory of mind\nand a directional consensus mechanism. Execution can be fully distributed,\nrequiring no explicit communication between policies. In which case, the\ninformation is exchanged implicitly through each robot's sensor stream by\nobserving the actions of the other robots and their effects on the scene.\nAlternatively, execution can leverage direct communication to share the robots'\nconsensus embeddings, where the embeddings are shared once during each\ninference step and are aligned using the sheaf Laplacian. In our hardware\nexperiments, LatentToM outperforms a naive decentralized diffusion baseline,\nand shows comparable performance with a state-of-the-art centralized diffusion\npolicy for bi-manual manipulation. Project website:\nhttps://stanfordmsl.github.io/LatentToM/."
                },
                "authors": [
                    {
                        "name": "Chengyang He"
                    },
                    {
                        "name": "Gadiel Sznaier Camps"
                    },
                    {
                        "name": "Xu Liu"
                    },
                    {
                        "name": "Mac Schwager"
                    },
                    {
                        "name": "Guillaume Sartoretti"
                    }
                ],
                "author_detail": {
                    "name": "Guillaume Sartoretti"
                },
                "author": "Guillaume Sartoretti",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09144v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09144v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09141v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09141v1",
                "updated": "2025-05-14T04:50:00Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    4,
                    50,
                    0,
                    2,
                    134,
                    0
                ],
                "published": "2025-05-14T04:50:00Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    4,
                    50,
                    0,
                    2,
                    134,
                    0
                ],
                "title": "Sensing-Assisted Channel Prediction in Complex Wireless Environments: An\n  LLM-Based Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sensing-Assisted Channel Prediction in Complex Wireless Environments: An\n  LLM-Based Approach"
                },
                "summary": "This letter studies the sensing-assisted channel prediction for a\nmulti-antenna orthogonal frequency division multiplexing (OFDM) system\noperating in realistic and complex wireless environments. In this system,an\nintegrated sensing and communication (ISAC) transmitter leverages the\nmono-static sensing capability to facilitate the prediction of its bi-static\ncommunication channel, by exploiting the fact that the sensing and\ncommunication channels share the same physical environment involving shared\nscatterers. Specifically, we propose a novel large language model (LLM)-based\nchannel prediction approach,which adapts pre-trained text-based LLM to handle\nthe complex-matrix-form channel state information (CSI) data. This approach\nutilizes the LLM's strong ability to capture the intricate spatiotemporal\nrelationships between the multi-path sensing and communication channels, and\nthus efficiently predicts upcoming communication CSI based on historical\ncommunication and sensing CSI data. Experimental results show that the proposed\nLLM-based approach significantly outperforms conventional deep learning-based\nmethods and the benchmark scheme without sensing assistance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This letter studies the sensing-assisted channel prediction for a\nmulti-antenna orthogonal frequency division multiplexing (OFDM) system\noperating in realistic and complex wireless environments. In this system,an\nintegrated sensing and communication (ISAC) transmitter leverages the\nmono-static sensing capability to facilitate the prediction of its bi-static\ncommunication channel, by exploiting the fact that the sensing and\ncommunication channels share the same physical environment involving shared\nscatterers. Specifically, we propose a novel large language model (LLM)-based\nchannel prediction approach,which adapts pre-trained text-based LLM to handle\nthe complex-matrix-form channel state information (CSI) data. This approach\nutilizes the LLM's strong ability to capture the intricate spatiotemporal\nrelationships between the multi-path sensing and communication channels, and\nthus efficiently predicts upcoming communication CSI based on historical\ncommunication and sensing CSI data. Experimental results show that the proposed\nLLM-based approach significantly outperforms conventional deep learning-based\nmethods and the benchmark scheme without sensing assistance."
                },
                "authors": [
                    {
                        "name": "Junjie He"
                    },
                    {
                        "name": "Zixiang Ren"
                    },
                    {
                        "name": "Jianping Yao"
                    },
                    {
                        "name": "Han Hu"
                    },
                    {
                        "name": "Tony Xiao Han"
                    },
                    {
                        "name": "Jie Xu"
                    }
                ],
                "author_detail": {
                    "name": "Jie Xu"
                },
                "author": "Jie Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09141v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09141v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09142v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09142v1",
                "updated": "2025-05-14T04:50:00Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    4,
                    50,
                    0,
                    2,
                    134,
                    0
                ],
                "published": "2025-05-14T04:50:00Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    4,
                    50,
                    0,
                    2,
                    134,
                    0
                ],
                "title": "ELIS: Efficient LLM Iterative Scheduling System with Response Length\n  Predictor",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ELIS: Efficient LLM Iterative Scheduling System with Response Length\n  Predictor"
                },
                "summary": "We propose ELIS, a serving system for Large Language Models (LLMs) featuring\nan Iterative Shortest Remaining Time First (ISRTF) scheduler designed to\nefficiently manage inference tasks with the shortest remaining tokens. Current\nLLM serving systems often employ a first-come-first-served scheduling strategy,\nwhich can lead to the \"head-of-line blocking\" problem. To overcome this\nlimitation, it is necessary to predict LLM inference times and apply a shortest\njob first scheduling strategy. However, due to the auto-regressive nature of\nLLMs, predicting the inference latency is challenging. ELIS addresses this\nchallenge by training a response length predictor for LLMs using the BGE model,\nan encoder-based state-of-the-art model. Additionally, we have devised the\nISRTF scheduling strategy, an optimization of shortest remaining time first\ntailored to existing LLM iteration batching. To evaluate our work in an\nindustrial setting, we simulate streams of requests based on our study of\nreal-world user LLM serving trace records. Furthermore, we implemented ELIS as\na cloud-native scheduler system on Kubernetes to evaluate its performance in\nproduction environments. Our experimental results demonstrate that ISRTF\nreduces the average job completion time by up to 19.6%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose ELIS, a serving system for Large Language Models (LLMs) featuring\nan Iterative Shortest Remaining Time First (ISRTF) scheduler designed to\nefficiently manage inference tasks with the shortest remaining tokens. Current\nLLM serving systems often employ a first-come-first-served scheduling strategy,\nwhich can lead to the \"head-of-line blocking\" problem. To overcome this\nlimitation, it is necessary to predict LLM inference times and apply a shortest\njob first scheduling strategy. However, due to the auto-regressive nature of\nLLMs, predicting the inference latency is challenging. ELIS addresses this\nchallenge by training a response length predictor for LLMs using the BGE model,\nan encoder-based state-of-the-art model. Additionally, we have devised the\nISRTF scheduling strategy, an optimization of shortest remaining time first\ntailored to existing LLM iteration batching. To evaluate our work in an\nindustrial setting, we simulate streams of requests based on our study of\nreal-world user LLM serving trace records. Furthermore, we implemented ELIS as\na cloud-native scheduler system on Kubernetes to evaluate its performance in\nproduction environments. Our experimental results demonstrate that ISRTF\nreduces the average job completion time by up to 19.6%."
                },
                "authors": [
                    {
                        "name": "Seungbeom Choi"
                    },
                    {
                        "name": "Jeonghoe Goo"
                    },
                    {
                        "name": "Eunjoo Jeon"
                    },
                    {
                        "name": "Mingyu Yang"
                    },
                    {
                        "name": "Minsung Jang"
                    }
                ],
                "author_detail": {
                    "name": "Minsung Jang"
                },
                "author": "Minsung Jang",
                "arxiv_comment": "13 pages, 5 figures. Cloud-native LLM scheduling system with\n  latency-aware inference optimization",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09142v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09142v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02291v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02291v2",
                "updated": "2025-05-14T04:35:33Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    4,
                    35,
                    33,
                    2,
                    134,
                    0
                ],
                "published": "2025-05-04T23:20:40Z",
                "published_parsed": [
                    2025,
                    5,
                    4,
                    23,
                    20,
                    40,
                    6,
                    124,
                    0
                ],
                "title": "Dexterous Contact-Rich Manipulation via the Contact Trust Region",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dexterous Contact-Rich Manipulation via the Contact Trust Region"
                },
                "summary": "What is a good local description of contact dynamics for contact-rich\nmanipulation, and where can we trust this local description? While many\napproaches often rely on the Taylor approximation of dynamics with an\nellipsoidal trust region, we argue that such approaches are fundamentally\ninconsistent with the unilateral nature of contact. As a remedy, we present the\nContact Trust Region (CTR), which captures the unilateral nature of contact\nwhile remaining efficient for computation. With CTR, we first develop a\nModel-Predictive Control (MPC) algorithm capable of synthesizing local\ncontact-rich plans. Then, we extend this capability to plan globally by\nstitching together local MPC plans, enabling efficient and dexterous\ncontact-rich manipulation. To verify the performance of our method, we perform\ncomprehensive evaluations, both in high-fidelity simulation and on hardware, on\ntwo contact-rich systems: a planar IiwaBimanual system and a 3D AllegroHand\nsystem. On both systems, our method offers a significantly lower-compute\nalternative to existing RL-based approaches to contact-rich manipulation. In\nparticular, our Allegro in-hand manipulation policy, in the form of a roadmap,\ntakes fewer than 10 minutes to build offline on a standard laptop using just\nits CPU, with online inference taking just a few seconds. Experiment data,\nvideo and code are available at ctr.theaiinstitute.com.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What is a good local description of contact dynamics for contact-rich\nmanipulation, and where can we trust this local description? While many\napproaches often rely on the Taylor approximation of dynamics with an\nellipsoidal trust region, we argue that such approaches are fundamentally\ninconsistent with the unilateral nature of contact. As a remedy, we present the\nContact Trust Region (CTR), which captures the unilateral nature of contact\nwhile remaining efficient for computation. With CTR, we first develop a\nModel-Predictive Control (MPC) algorithm capable of synthesizing local\ncontact-rich plans. Then, we extend this capability to plan globally by\nstitching together local MPC plans, enabling efficient and dexterous\ncontact-rich manipulation. To verify the performance of our method, we perform\ncomprehensive evaluations, both in high-fidelity simulation and on hardware, on\ntwo contact-rich systems: a planar IiwaBimanual system and a 3D AllegroHand\nsystem. On both systems, our method offers a significantly lower-compute\nalternative to existing RL-based approaches to contact-rich manipulation. In\nparticular, our Allegro in-hand manipulation policy, in the form of a roadmap,\ntakes fewer than 10 minutes to build offline on a standard laptop using just\nits CPU, with online inference taking just a few seconds. Experiment data,\nvideo and code are available at ctr.theaiinstitute.com."
                },
                "authors": [
                    {
                        "name": "H. J. Terry Suh"
                    },
                    {
                        "name": "Tao Pang"
                    },
                    {
                        "name": "Tong Zhao"
                    },
                    {
                        "name": "Russ Tedrake"
                    }
                ],
                "author_detail": {
                    "name": "Russ Tedrake"
                },
                "author": "Russ Tedrake",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02291v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02291v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18599v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18599v2",
                "updated": "2025-05-14T04:22:24Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    4,
                    22,
                    24,
                    2,
                    134,
                    0
                ],
                "published": "2025-03-24T11:56:50Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    11,
                    56,
                    50,
                    0,
                    83,
                    0
                ],
                "title": "Oaken: Fast and Efficient LLM Serving with Online-Offline Hybrid KV\n  Cache Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Oaken: Fast and Efficient LLM Serving with Online-Offline Hybrid KV\n  Cache Quantization"
                },
                "summary": "Modern Large Language Model serving system batches multiple requests to\nachieve high throughput, while batching attention operations is challenging,\nrendering memory bandwidth a critical bottleneck. The community relies on\nhigh-end GPUs with multiple high-bandwidth memory channels. Unfortunately,\nHBM's high bandwidth often comes at the expense of limited memory capacity,\nwhich reduces core utilization and increases costs. Recent advancements\nenabling longer contexts for LLMs have substantially increased the key-value\ncache size, further intensifying the pressures on memory capacity. The\nliterature has explored KV cache quantization techniques, which commonly use\nlow bitwidth for most values, selectively using higher bitwidth for outlier\nvalues. While this approach helps achieve high accuracy and low bitwidth\nsimultaneously, it comes with the limitation that cost for online outlier\ndetection is excessively high, negating the advantages. We propose Oaken, an\nacceleration solution that achieves high accuracy and high performance\nsimultaneously through co-designing algorithm and hardware. To effectively find\na sweet spot in the accuracy-performance trade-off space of KV cache\nquantization, Oaken employs an online-offline hybrid approach, setting outlier\nthresholds offline, which are then used to determine the quantization scale\nonline. To translate the proposed algorithmic technique into tangible\nperformance gains, Oaken also comes with custom quantization engines and memory\nmanagement units that can be integrated with any LLM accelerators. We built an\nOaken accelerator on top of an LLM accelerator, LPU, and conducted a\ncomprehensive evaluation. Our experiments show that for a batch size of 256,\nOaken achieves up to 1.58x throughput improvement over NVIDIA A100 GPU,\nincurring a minimal accuracy loss of only 0.54\\% on average, compared to\nstate-of-the-art KV cache quantization techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern Large Language Model serving system batches multiple requests to\nachieve high throughput, while batching attention operations is challenging,\nrendering memory bandwidth a critical bottleneck. The community relies on\nhigh-end GPUs with multiple high-bandwidth memory channels. Unfortunately,\nHBM's high bandwidth often comes at the expense of limited memory capacity,\nwhich reduces core utilization and increases costs. Recent advancements\nenabling longer contexts for LLMs have substantially increased the key-value\ncache size, further intensifying the pressures on memory capacity. The\nliterature has explored KV cache quantization techniques, which commonly use\nlow bitwidth for most values, selectively using higher bitwidth for outlier\nvalues. While this approach helps achieve high accuracy and low bitwidth\nsimultaneously, it comes with the limitation that cost for online outlier\ndetection is excessively high, negating the advantages. We propose Oaken, an\nacceleration solution that achieves high accuracy and high performance\nsimultaneously through co-designing algorithm and hardware. To effectively find\na sweet spot in the accuracy-performance trade-off space of KV cache\nquantization, Oaken employs an online-offline hybrid approach, setting outlier\nthresholds offline, which are then used to determine the quantization scale\nonline. To translate the proposed algorithmic technique into tangible\nperformance gains, Oaken also comes with custom quantization engines and memory\nmanagement units that can be integrated with any LLM accelerators. We built an\nOaken accelerator on top of an LLM accelerator, LPU, and conducted a\ncomprehensive evaluation. Our experiments show that for a batch size of 256,\nOaken achieves up to 1.58x throughput improvement over NVIDIA A100 GPU,\nincurring a minimal accuracy loss of only 0.54\\% on average, compared to\nstate-of-the-art KV cache quantization techniques."
                },
                "authors": [
                    {
                        "name": "Minsu Kim"
                    },
                    {
                        "name": "Seongmin Hong"
                    },
                    {
                        "name": "RyeoWook Ko"
                    },
                    {
                        "name": "Soongyu Choi"
                    },
                    {
                        "name": "Hunjong Lee"
                    },
                    {
                        "name": "Junsoo Kim"
                    },
                    {
                        "name": "Joo-Young Kim"
                    },
                    {
                        "name": "Jongse Park"
                    }
                ],
                "author_detail": {
                    "name": "Jongse Park"
                },
                "author": "Jongse Park",
                "arxiv_doi": "10.1145/3695053.3731019",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3695053.3731019",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.18599v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18599v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "16 pages, 14 figures, and 4 tables",
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09192v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09192v4",
                "updated": "2025-05-15T06:21:11Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    6,
                    21,
                    11,
                    3,
                    135,
                    0
                ],
                "published": "2025-04-12T12:17:20Z",
                "published_parsed": [
                    2025,
                    4,
                    12,
                    12,
                    17,
                    20,
                    5,
                    102,
                    0
                ],
                "title": "Towards More Efficient, Robust, Instance-adaptive, and Generalizable\n  Sequential Decision making",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards More Efficient, Robust, Instance-adaptive, and Generalizable\n  Sequential Decision making"
                },
                "summary": "The primary goal of my Ph.D. study is to develop provably efficient and\npractical algorithms for data-driven sequential decision-making under\nuncertainty. My work focuses on reinforcement learning (RL), multi-armed\nbandits, and their applications, including recommendation systems, computer\nnetworks, video analytics, and large language models (LLMs). Sequential\ndecision-making methods, such as bandits and RL, have demonstrated remarkable\nsuccess - ranging from outperforming human players in complex games like Atari\nand Go to advancing robotics, recommendation systems, and fine-tuning LLMs.\nDespite these successes, many established algorithms rely on idealized models\nthat can fail under model misspecifications or adversarial perturbations,\nparticularly in settings where accurate prior knowledge of the underlying model\nclass is unavailable or where malicious users operate within dynamic systems.\nThese challenges are pervasive in real-world applications, where robust and\nadaptive solutions are critical. Furthermore, while worst-case guarantees\nprovide theoretical reliability, they often fail to capture instance-dependent\nperformance, which can lead to more efficient and practical solutions. Another\nkey challenge lies in generalizing to new, unseen environments, a crucial\nrequirement for deploying these methods in dynamic and unpredictable settings.\nTo address these limitations, my research aims to develop more efficient,\nrobust, instance-adaptive, and generalizable sequential decision-making\nalgorithms for both reinforcement learning and bandits. Towards this end, I\nfocus on developing more efficient, robust, instance-adaptive, and\ngeneralizable for both general reinforcement learning (RL) and bandits.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The primary goal of my Ph.D. study is to develop provably efficient and\npractical algorithms for data-driven sequential decision-making under\nuncertainty. My work focuses on reinforcement learning (RL), multi-armed\nbandits, and their applications, including recommendation systems, computer\nnetworks, video analytics, and large language models (LLMs). Sequential\ndecision-making methods, such as bandits and RL, have demonstrated remarkable\nsuccess - ranging from outperforming human players in complex games like Atari\nand Go to advancing robotics, recommendation systems, and fine-tuning LLMs.\nDespite these successes, many established algorithms rely on idealized models\nthat can fail under model misspecifications or adversarial perturbations,\nparticularly in settings where accurate prior knowledge of the underlying model\nclass is unavailable or where malicious users operate within dynamic systems.\nThese challenges are pervasive in real-world applications, where robust and\nadaptive solutions are critical. Furthermore, while worst-case guarantees\nprovide theoretical reliability, they often fail to capture instance-dependent\nperformance, which can lead to more efficient and practical solutions. Another\nkey challenge lies in generalizing to new, unseen environments, a crucial\nrequirement for deploying these methods in dynamic and unpredictable settings.\nTo address these limitations, my research aims to develop more efficient,\nrobust, instance-adaptive, and generalizable sequential decision-making\nalgorithms for both reinforcement learning and bandits. Towards this end, I\nfocus on developing more efficient, robust, instance-adaptive, and\ngeneralizable for both general reinforcement learning (RL) and bandits."
                },
                "authors": [
                    {
                        "name": "Zhiyong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyong Wang"
                },
                "author": "Zhiyong Wang",
                "arxiv_comment": "Ph.D. Thesis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09192v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09192v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.07773v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.07773v2",
                "updated": "2025-05-14T04:15:06Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    4,
                    15,
                    6,
                    2,
                    134,
                    0
                ],
                "published": "2025-05-12T17:23:34Z",
                "published_parsed": [
                    2025,
                    5,
                    12,
                    17,
                    23,
                    34,
                    0,
                    132,
                    0
                ],
                "title": "Agent RL Scaling Law: Agent RL with Spontaneous Code Execution for\n  Mathematical Problem Solving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agent RL Scaling Law: Agent RL with Spontaneous Code Execution for\n  Mathematical Problem Solving"
                },
                "summary": "Large Language Models (LLMs) often struggle with mathematical reasoning tasks\nrequiring precise, verifiable computation. While Reinforcement Learning (RL)\nfrom outcome-based rewards enhances text-based reasoning, understanding how\nagents autonomously learn to leverage external tools like code execution\nremains crucial. We investigate RL from outcome-based rewards for\nTool-Integrated Reasoning, ZeroTIR, training base LLMs to spontaneously\ngenerate and execute Python code for mathematical problems without supervised\ntool-use examples. Our central contribution is we demonstrate that as RL\ntraining progresses, key metrics scale predictably. Specifically, we observe\nstrong positive correlations where increased training steps lead to increases\nin the spontaneous code execution frequency, the average response length, and,\ncritically, the final task accuracy. This suggests a quantifiable relationship\nbetween computational effort invested in training and the emergence of\neffective, tool-augmented reasoning strategies. We implement a robust framework\nfeaturing a decoupled code execution environment and validate our findings\nacross standard RL algorithms and frameworks. Experiments show ZeroTIR\nsignificantly surpasses non-tool ZeroRL baselines on challenging math\nbenchmarks. Our findings provide a foundational understanding of how autonomous\ntool use is acquired and scales within Agent RL, offering a reproducible\nbenchmark for future studies. Code is released at\n\\href{https://github.com/yyht/openrlhf_async_pipline}{https://github.com/yyht/openrlhf\\_async\\_pipline}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) often struggle with mathematical reasoning tasks\nrequiring precise, verifiable computation. While Reinforcement Learning (RL)\nfrom outcome-based rewards enhances text-based reasoning, understanding how\nagents autonomously learn to leverage external tools like code execution\nremains crucial. We investigate RL from outcome-based rewards for\nTool-Integrated Reasoning, ZeroTIR, training base LLMs to spontaneously\ngenerate and execute Python code for mathematical problems without supervised\ntool-use examples. Our central contribution is we demonstrate that as RL\ntraining progresses, key metrics scale predictably. Specifically, we observe\nstrong positive correlations where increased training steps lead to increases\nin the spontaneous code execution frequency, the average response length, and,\ncritically, the final task accuracy. This suggests a quantifiable relationship\nbetween computational effort invested in training and the emergence of\neffective, tool-augmented reasoning strategies. We implement a robust framework\nfeaturing a decoupled code execution environment and validate our findings\nacross standard RL algorithms and frameworks. Experiments show ZeroTIR\nsignificantly surpasses non-tool ZeroRL baselines on challenging math\nbenchmarks. Our findings provide a foundational understanding of how autonomous\ntool use is acquired and scales within Agent RL, offering a reproducible\nbenchmark for future studies. Code is released at\n\\href{https://github.com/yyht/openrlhf_async_pipline}{https://github.com/yyht/openrlhf\\_async\\_pipline}."
                },
                "authors": [
                    {
                        "name": "Xinji Mai"
                    },
                    {
                        "name": "Haotian Xu"
                    },
                    {
                        "name": "Xing W"
                    },
                    {
                        "name": "Weinong Wang"
                    },
                    {
                        "name": "Yingying Zhang"
                    },
                    {
                        "name": "Wenqiang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wenqiang Zhang"
                },
                "author": "Wenqiang Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.07773v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.07773v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09108v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09108v1",
                "updated": "2025-05-14T03:33:46Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    3,
                    33,
                    46,
                    2,
                    134,
                    0
                ],
                "published": "2025-05-14T03:33:46Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    3,
                    33,
                    46,
                    2,
                    134,
                    0
                ],
                "title": "Air-Ground Collaboration for Language-Specified Missions in Unknown\n  Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Air-Ground Collaboration for Language-Specified Missions in Unknown\n  Environments"
                },
                "summary": "As autonomous robotic systems become increasingly mature, users will want to\nspecify missions at the level of intent rather than in low-level detail.\nLanguage is an expressive and intuitive medium for such mission specification.\nHowever, realizing language-guided robotic teams requires overcoming\nsignificant technical hurdles. Interpreting and realizing language-specified\nmissions requires advanced semantic reasoning. Successful heterogeneous robots\nmust effectively coordinate actions and share information across varying\nviewpoints. Additionally, communication between robots is typically\nintermittent, necessitating robust strategies that leverage communication\nopportunities to maintain coordination and achieve mission objectives. In this\nwork, we present a first-of-its-kind system where an unmanned aerial vehicle\n(UAV) and an unmanned ground vehicle (UGV) are able to collaboratively\naccomplish missions specified in natural language while reacting to changes in\nspecification on the fly. We leverage a Large Language Model (LLM)-enabled\nplanner to reason over semantic-metric maps that are built online and\nopportunistically shared between an aerial and a ground robot. We consider\ntask-driven navigation in urban and rural areas. Our system must infer\nmission-relevant semantics and actively acquire information via semantic\nmapping. In both ground and air-ground teaming experiments, we demonstrate our\nsystem on seven different natural-language specifications at up to\nkilometer-scale navigation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As autonomous robotic systems become increasingly mature, users will want to\nspecify missions at the level of intent rather than in low-level detail.\nLanguage is an expressive and intuitive medium for such mission specification.\nHowever, realizing language-guided robotic teams requires overcoming\nsignificant technical hurdles. Interpreting and realizing language-specified\nmissions requires advanced semantic reasoning. Successful heterogeneous robots\nmust effectively coordinate actions and share information across varying\nviewpoints. Additionally, communication between robots is typically\nintermittent, necessitating robust strategies that leverage communication\nopportunities to maintain coordination and achieve mission objectives. In this\nwork, we present a first-of-its-kind system where an unmanned aerial vehicle\n(UAV) and an unmanned ground vehicle (UGV) are able to collaboratively\naccomplish missions specified in natural language while reacting to changes in\nspecification on the fly. We leverage a Large Language Model (LLM)-enabled\nplanner to reason over semantic-metric maps that are built online and\nopportunistically shared between an aerial and a ground robot. We consider\ntask-driven navigation in urban and rural areas. Our system must infer\nmission-relevant semantics and actively acquire information via semantic\nmapping. In both ground and air-ground teaming experiments, we demonstrate our\nsystem on seven different natural-language specifications at up to\nkilometer-scale navigation."
                },
                "authors": [
                    {
                        "name": "Fernando Cladera"
                    },
                    {
                        "name": "Zachary Ravichandran"
                    },
                    {
                        "name": "Jason Hughes"
                    },
                    {
                        "name": "Varun Murali"
                    },
                    {
                        "name": "Carlos Nieto-Granda"
                    },
                    {
                        "name": "M. Ani Hsieh"
                    },
                    {
                        "name": "George J. Pappas"
                    },
                    {
                        "name": "Camillo J. Taylor"
                    },
                    {
                        "name": "Vijay Kumar"
                    }
                ],
                "author_detail": {
                    "name": "Vijay Kumar"
                },
                "author": "Vijay Kumar",
                "arxiv_comment": "19 pages, 24 figures, 7 tables. Submitted to T-FR",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09108v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09108v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09089v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09089v2",
                "updated": "2025-05-15T00:55:20Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    0,
                    55,
                    20,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-14T02:51:10Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    2,
                    51,
                    10,
                    2,
                    134,
                    0
                ],
                "title": "Generating time-consistent dynamics with discriminator-guided image\n  diffusion models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating time-consistent dynamics with discriminator-guided image\n  diffusion models"
                },
                "summary": "Realistic temporal dynamics are crucial for many video generation, processing\nand modelling applications, e.g. in computational fluid dynamics, weather\nprediction, or long-term climate simulations. Video diffusion models (VDMs) are\nthe current state-of-the-art method for generating highly realistic dynamics.\nHowever, training VDMs from scratch can be challenging and requires large\ncomputational resources, limiting their wider application. Here, we propose a\ntime-consistency discriminator that enables pretrained image diffusion models\nto generate realistic spatiotemporal dynamics. The discriminator guides the\nsampling inference process and does not require extensions or finetuning of the\nimage diffusion model. We compare our approach against a VDM trained from\nscratch on an idealized turbulence simulation and a real-world global\nprecipitation dataset. Our approach performs equally well in terms of temporal\nconsistency, shows improved uncertainty calibration and lower biases compared\nto the VDM, and achieves stable centennial-scale climate simulations at daily\ntime steps.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Realistic temporal dynamics are crucial for many video generation, processing\nand modelling applications, e.g. in computational fluid dynamics, weather\nprediction, or long-term climate simulations. Video diffusion models (VDMs) are\nthe current state-of-the-art method for generating highly realistic dynamics.\nHowever, training VDMs from scratch can be challenging and requires large\ncomputational resources, limiting their wider application. Here, we propose a\ntime-consistency discriminator that enables pretrained image diffusion models\nto generate realistic spatiotemporal dynamics. The discriminator guides the\nsampling inference process and does not require extensions or finetuning of the\nimage diffusion model. We compare our approach against a VDM trained from\nscratch on an idealized turbulence simulation and a real-world global\nprecipitation dataset. Our approach performs equally well in terms of temporal\nconsistency, shows improved uncertainty calibration and lower biases compared\nto the VDM, and achieves stable centennial-scale climate simulations at daily\ntime steps."
                },
                "authors": [
                    {
                        "name": "Philipp Hess"
                    },
                    {
                        "name": "Maximilian Gelbrecht"
                    },
                    {
                        "name": "Christof Schötz"
                    },
                    {
                        "name": "Michael Aich"
                    },
                    {
                        "name": "Yu Huang"
                    },
                    {
                        "name": "Shangshang Yang"
                    },
                    {
                        "name": "Niklas Boers"
                    }
                ],
                "author_detail": {
                    "name": "Niklas Boers"
                },
                "author": "Niklas Boers",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09089v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09089v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09087v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09087v1",
                "updated": "2025-05-14T02:40:13Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    2,
                    40,
                    13,
                    2,
                    134,
                    0
                ],
                "published": "2025-05-14T02:40:13Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    2,
                    40,
                    13,
                    2,
                    134,
                    0
                ],
                "title": "A Comparative Review of RNA Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Comparative Review of RNA Language Models"
                },
                "summary": "Given usefulness of protein language models (LMs) in structure and functional\ninference, RNA LMs have received increased attentions in the last few years.\nHowever, these RNA models are often not compared against the same standard.\nHere, we divided RNA LMs into three classes (pretrained on multiple RNA types\n(especially noncoding RNAs), specific-purpose RNAs, and LMs that unify RNA with\nDNA or proteins or both) and compared 13 RNA LMs along with 3 DNA and 1 protein\nLMs as controls in zero-shot prediction of RNA secondary structure and\nfunctional classification. Results shows that the models doing well on\nsecondary structure prediction often perform worse in function classification\nor vice versa, suggesting that more balanced unsupervised training is needed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Given usefulness of protein language models (LMs) in structure and functional\ninference, RNA LMs have received increased attentions in the last few years.\nHowever, these RNA models are often not compared against the same standard.\nHere, we divided RNA LMs into three classes (pretrained on multiple RNA types\n(especially noncoding RNAs), specific-purpose RNAs, and LMs that unify RNA with\nDNA or proteins or both) and compared 13 RNA LMs along with 3 DNA and 1 protein\nLMs as controls in zero-shot prediction of RNA secondary structure and\nfunctional classification. Results shows that the models doing well on\nsecondary structure prediction often perform worse in function classification\nor vice versa, suggesting that more balanced unsupervised training is needed."
                },
                "authors": [
                    {
                        "name": "He Wang"
                    },
                    {
                        "name": "Yikun Zhang"
                    },
                    {
                        "name": "Jie Chen"
                    },
                    {
                        "name": "Jian Zhan"
                    },
                    {
                        "name": "Yaoqi Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Yaoqi Zhou"
                },
                "author": "Yaoqi Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09087v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09087v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.BM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.BM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09082v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09082v1",
                "updated": "2025-05-14T02:35:47Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    2,
                    35,
                    47,
                    2,
                    134,
                    0
                ],
                "published": "2025-05-14T02:35:47Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    2,
                    35,
                    47,
                    2,
                    134,
                    0
                ],
                "title": "CEC-Zero: Chinese Error Correction Solution Based on LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CEC-Zero: Chinese Error Correction Solution Based on LLM"
                },
                "summary": "Recent advancements in large language models (LLMs) demonstrate exceptional\nChinese text processing capabilities, particularly in Chinese Spelling\nCorrection (CSC). While LLMs outperform traditional BERT-based models in\naccuracy and robustness, challenges persist in reliability and generalization.\nThis paper proposes CEC-Zero, a novel reinforcement learning (RL) framework\nenabling LLMs to self-correct through autonomous error strategy learning\nwithout external supervision. By integrating RL with LLMs' generative power,\nthe method eliminates dependency on annotated data or auxiliary models.\nExperiments reveal RL-enhanced LLMs achieve industry-viable accuracy and\nsuperior cross-domain generalization, offering a scalable solution for\nreliability optimization in Chinese NLP applications. This breakthrough\nfacilitates LLM deployment in practical Chinese text correction scenarios while\nestablishing a new paradigm for self-improving language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) demonstrate exceptional\nChinese text processing capabilities, particularly in Chinese Spelling\nCorrection (CSC). While LLMs outperform traditional BERT-based models in\naccuracy and robustness, challenges persist in reliability and generalization.\nThis paper proposes CEC-Zero, a novel reinforcement learning (RL) framework\nenabling LLMs to self-correct through autonomous error strategy learning\nwithout external supervision. By integrating RL with LLMs' generative power,\nthe method eliminates dependency on annotated data or auxiliary models.\nExperiments reveal RL-enhanced LLMs achieve industry-viable accuracy and\nsuperior cross-domain generalization, offering a scalable solution for\nreliability optimization in Chinese NLP applications. This breakthrough\nfacilitates LLM deployment in practical Chinese text correction scenarios while\nestablishing a new paradigm for self-improving language models."
                },
                "authors": [
                    {
                        "name": "Sophie Zhang"
                    },
                    {
                        "name": "Zhiming Lin"
                    }
                ],
                "author_detail": {
                    "name": "Zhiming Lin"
                },
                "author": "Zhiming Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09082v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09082v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03146v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03146v2",
                "updated": "2025-05-14T02:31:31Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    2,
                    31,
                    31,
                    2,
                    134,
                    0
                ],
                "published": "2025-03-05T03:41:57Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    3,
                    41,
                    57,
                    2,
                    64,
                    0
                ],
                "title": "PriFFT: Privacy-preserving Federated Fine-tuning of Large Language\n  Models via Hybrid Secret Sharing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PriFFT: Privacy-preserving Federated Fine-tuning of Large Language\n  Models via Hybrid Secret Sharing"
                },
                "summary": "Fine-tuning large language models (LLMs) raises privacy concerns due to the\nrisk of exposing sensitive training data. Federated learning (FL) mitigates\nthis risk by keeping training samples on local devices, while facing the\nfollowing problems in privacy-preserving federated fine-tuning. (i) Recent\nstudies show that adversaries can still infer private information in FL. (ii)\nLLM parameters are shared publicly during federated fine-tuning, while\ndevelopers are often reluctant to disclose these parameters, posing further\nsecurity challenges. (iii) Existing works focus on secure inference of LLMs but\ndo not consider privacy-preserving fine-tuning. Inspired by the above problems,\nwe propose PriFFT, a privacy-preserving federated fine-tuning mechanism, to\nprotect both the model parameters and users' privacy. Due to considerable LLM\nparameters, we present hybrid secret sharing combining arithmetic secret\nsharing (ASS) and function secret sharing (FSS) to build secure operations and\nimplement secure layers and activation for privacy-preserving fine-tuning. To\nimprove the efficiency of privacy-preserving federated fine-tuning of LLMs, we\noptimize several secure computation protocols based on FSS, including\nreciprocal calculation, tensor products, natural exponentiation, softmax,\nsigmoid, hyperbolic tangent, and dropout. The hybrid secret sharing enables\nPriFFT to apply our optimized FSS protocols while combining ASS protocols to\nsupport complex computation without extra communication. The optimized\nprotocols reduce execution time up to 62.5% and communication overhead up to\n70.7% compared to existing protocols. Besides, PriFFT reduces execution time\nand communication overhead in privacy-preserving fine-tuning up to 59.1%$ and\n77.0%$ without accuracy drop compared to the existing secret sharing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning large language models (LLMs) raises privacy concerns due to the\nrisk of exposing sensitive training data. Federated learning (FL) mitigates\nthis risk by keeping training samples on local devices, while facing the\nfollowing problems in privacy-preserving federated fine-tuning. (i) Recent\nstudies show that adversaries can still infer private information in FL. (ii)\nLLM parameters are shared publicly during federated fine-tuning, while\ndevelopers are often reluctant to disclose these parameters, posing further\nsecurity challenges. (iii) Existing works focus on secure inference of LLMs but\ndo not consider privacy-preserving fine-tuning. Inspired by the above problems,\nwe propose PriFFT, a privacy-preserving federated fine-tuning mechanism, to\nprotect both the model parameters and users' privacy. Due to considerable LLM\nparameters, we present hybrid secret sharing combining arithmetic secret\nsharing (ASS) and function secret sharing (FSS) to build secure operations and\nimplement secure layers and activation for privacy-preserving fine-tuning. To\nimprove the efficiency of privacy-preserving federated fine-tuning of LLMs, we\noptimize several secure computation protocols based on FSS, including\nreciprocal calculation, tensor products, natural exponentiation, softmax,\nsigmoid, hyperbolic tangent, and dropout. The hybrid secret sharing enables\nPriFFT to apply our optimized FSS protocols while combining ASS protocols to\nsupport complex computation without extra communication. The optimized\nprotocols reduce execution time up to 62.5% and communication overhead up to\n70.7% compared to existing protocols. Besides, PriFFT reduces execution time\nand communication overhead in privacy-preserving fine-tuning up to 59.1%$ and\n77.0%$ without accuracy drop compared to the existing secret sharing methods."
                },
                "authors": [
                    {
                        "name": "Zhichao You"
                    },
                    {
                        "name": "Xuewen Dong"
                    },
                    {
                        "name": "Ke Cheng"
                    },
                    {
                        "name": "Xutong Mu"
                    },
                    {
                        "name": "Jiaxuan Fu"
                    },
                    {
                        "name": "Shiyang Ma"
                    },
                    {
                        "name": "Qiang Qu"
                    },
                    {
                        "name": "Yulong Shen"
                    }
                ],
                "author_detail": {
                    "name": "Yulong Shen"
                },
                "author": "Yulong Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03146v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03146v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09081v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09081v1",
                "updated": "2025-05-14T02:29:46Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    2,
                    29,
                    46,
                    2,
                    134,
                    0
                ],
                "published": "2025-05-14T02:29:46Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    2,
                    29,
                    46,
                    2,
                    134,
                    0
                ],
                "title": "SALM: A Multi-Agent Framework for Language Model-Driven Social Network\n  Simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SALM: A Multi-Agent Framework for Language Model-Driven Social Network\n  Simulation"
                },
                "summary": "Contemporary approaches to agent-based modeling (ABM) of social systems have\ntraditionally emphasized rule-based behaviors, limiting their ability to\ncapture nuanced dynamics by moving beyond predefined rules and leveraging\ncontextual understanding from LMs of human social interaction. This paper\npresents SALM (Social Agent LM Framework), a novel approach for integrating\nlanguage models (LMs) into social network simulation that achieves\nunprecedented temporal stability in multi-agent scenarios. Our primary\ncontributions include: (1) a hierarchical prompting architecture enabling\nstable simulation beyond 4,000 timesteps while reducing token usage by 73%, (2)\nan attention-based memory system achieving 80% cache hit rates (95% CI [78%,\n82%]) with sub-linear memory growth of 9.5%, and (3) formal bounds on\npersonality stability. Through extensive validation against SNAP ego networks,\nwe demonstrate the first LLM-based framework capable of modeling long-term\nsocial phenomena while maintaining empirically validated behavioral fidelity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contemporary approaches to agent-based modeling (ABM) of social systems have\ntraditionally emphasized rule-based behaviors, limiting their ability to\ncapture nuanced dynamics by moving beyond predefined rules and leveraging\ncontextual understanding from LMs of human social interaction. This paper\npresents SALM (Social Agent LM Framework), a novel approach for integrating\nlanguage models (LMs) into social network simulation that achieves\nunprecedented temporal stability in multi-agent scenarios. Our primary\ncontributions include: (1) a hierarchical prompting architecture enabling\nstable simulation beyond 4,000 timesteps while reducing token usage by 73%, (2)\nan attention-based memory system achieving 80% cache hit rates (95% CI [78%,\n82%]) with sub-linear memory growth of 9.5%, and (3) formal bounds on\npersonality stability. Through extensive validation against SNAP ego networks,\nwe demonstrate the first LLM-based framework capable of modeling long-term\nsocial phenomena while maintaining empirically validated behavioral fidelity."
                },
                "authors": [
                    {
                        "name": "Gaurav Koley"
                    }
                ],
                "author_detail": {
                    "name": "Gaurav Koley"
                },
                "author": "Gaurav Koley",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09081v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09081v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09116v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09116v2",
                "updated": "2025-05-14T02:29:41Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    2,
                    29,
                    41,
                    2,
                    134,
                    0
                ],
                "published": "2024-11-14T01:29:36Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    1,
                    29,
                    36,
                    3,
                    319,
                    0
                ],
                "title": "P-MMEval: A Parallel Multilingual Multitask Benchmark for Consistent\n  Evaluation of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "P-MMEval: A Parallel Multilingual Multitask Benchmark for Consistent\n  Evaluation of LLMs"
                },
                "summary": "Recent advancements in large language models (LLMs) showcase varied\nmultilingual capabilities across tasks like translation, code generation, and\nreasoning. Previous assessments often limited their scope to fundamental\nnatural language processing (NLP) or isolated capability-specific tasks. To\nalleviate this drawback, we aim to present a comprehensive multilingual\nmultitask benchmark. First, we introduce P-MMEval, a large-scale benchmark\ncovering effective fundamental and capability-specialized datasets.\nFurthermore, P-MMEval delivers consistent language coverage across various\ndatasets and provides parallel samples. Finally, we conduct extensive\nexperiments on representative multilingual model series to compare performances\nacross models and tasks, explore the relationship between multilingual\nperformances and factors such as tasks, model sizes, languages, and prompts,\nand examine the effectiveness of knowledge transfer from English to other\nlanguages. The resulting insights are intended to offer valuable guidance for\nfuture research. The dataset is available at\nhttps://huggingface.co/datasets/Qwen/P-MMEval.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) showcase varied\nmultilingual capabilities across tasks like translation, code generation, and\nreasoning. Previous assessments often limited their scope to fundamental\nnatural language processing (NLP) or isolated capability-specific tasks. To\nalleviate this drawback, we aim to present a comprehensive multilingual\nmultitask benchmark. First, we introduce P-MMEval, a large-scale benchmark\ncovering effective fundamental and capability-specialized datasets.\nFurthermore, P-MMEval delivers consistent language coverage across various\ndatasets and provides parallel samples. Finally, we conduct extensive\nexperiments on representative multilingual model series to compare performances\nacross models and tasks, explore the relationship between multilingual\nperformances and factors such as tasks, model sizes, languages, and prompts,\nand examine the effectiveness of knowledge transfer from English to other\nlanguages. The resulting insights are intended to offer valuable guidance for\nfuture research. The dataset is available at\nhttps://huggingface.co/datasets/Qwen/P-MMEval."
                },
                "authors": [
                    {
                        "name": "Yidan Zhang"
                    },
                    {
                        "name": "Yu Wan"
                    },
                    {
                        "name": "Boyi Deng"
                    },
                    {
                        "name": "Baosong Yang"
                    },
                    {
                        "name": "Haoran Wei"
                    },
                    {
                        "name": "Fei Huang"
                    },
                    {
                        "name": "Bowen Yu"
                    },
                    {
                        "name": "Junyang Lin"
                    },
                    {
                        "name": "Fei Huang"
                    },
                    {
                        "name": "Jingren Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jingren Zhou"
                },
                "author": "Jingren Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09116v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09116v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09073v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09073v1",
                "updated": "2025-05-14T02:17:53Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    2,
                    17,
                    53,
                    2,
                    134,
                    0
                ],
                "published": "2025-05-14T02:17:53Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    2,
                    17,
                    53,
                    2,
                    134,
                    0
                ],
                "title": "2D-3D Attention and Entropy for Pose Robust 2D Facial Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "2D-3D Attention and Entropy for Pose Robust 2D Facial Recognition"
                },
                "summary": "Despite recent advances in facial recognition, there remains a fundamental\nissue concerning degradations in performance due to substantial perspective\n(pose) differences between enrollment and query (probe) imagery. Therefore, we\npropose a novel domain adaptive framework to facilitate improved performances\nacross large discrepancies in pose by enabling image-based (2D) representations\nto infer properties of inherently pose invariant point cloud (3D)\nrepresentations. Specifically, our proposed framework achieves better pose\ninvariance by using (1) a shared (joint) attention mapping to emphasize common\npatterns that are most correlated between 2D facial images and 3D facial data\nand (2) a joint entropy regularizing loss to promote better\nconsistency$\\unicode{x2014}$enhancing correlations among the intersecting 2D\nand 3D representations$\\unicode{x2014}$by leveraging both attention maps. This\nframework is evaluated on FaceScape and ARL-VTF datasets, where it outperforms\ncompetitive methods by achieving profile (90$\\unicode{x00b0}$$\\unicode{x002b}$)\nTAR @ 1$\\unicode{x0025}$ FAR improvements of at least 7.1$\\unicode{x0025}$ and\n1.57$\\unicode{x0025}$, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite recent advances in facial recognition, there remains a fundamental\nissue concerning degradations in performance due to substantial perspective\n(pose) differences between enrollment and query (probe) imagery. Therefore, we\npropose a novel domain adaptive framework to facilitate improved performances\nacross large discrepancies in pose by enabling image-based (2D) representations\nto infer properties of inherently pose invariant point cloud (3D)\nrepresentations. Specifically, our proposed framework achieves better pose\ninvariance by using (1) a shared (joint) attention mapping to emphasize common\npatterns that are most correlated between 2D facial images and 3D facial data\nand (2) a joint entropy regularizing loss to promote better\nconsistency$\\unicode{x2014}$enhancing correlations among the intersecting 2D\nand 3D representations$\\unicode{x2014}$by leveraging both attention maps. This\nframework is evaluated on FaceScape and ARL-VTF datasets, where it outperforms\ncompetitive methods by achieving profile (90$\\unicode{x00b0}$$\\unicode{x002b}$)\nTAR @ 1$\\unicode{x0025}$ FAR improvements of at least 7.1$\\unicode{x0025}$ and\n1.57$\\unicode{x0025}$, respectively."
                },
                "authors": [
                    {
                        "name": "J. Brennan Peace"
                    },
                    {
                        "name": "Shuowen Hu"
                    },
                    {
                        "name": "Benjamin S. Riggan"
                    }
                ],
                "author_detail": {
                    "name": "Benjamin S. Riggan"
                },
                "author": "Benjamin S. Riggan",
                "arxiv_comment": "To appear at the IEEE International Conference on Automatic Face and\n  Gesture 2025 (FG2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09073v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09073v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11197v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11197v4",
                "updated": "2025-05-14T02:12:43Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    2,
                    12,
                    43,
                    2,
                    134,
                    0
                ],
                "published": "2025-03-14T08:43:53Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    8,
                    43,
                    53,
                    4,
                    73,
                    0
                ],
                "title": "Reinforcement Learning Outperforms Supervised Fine-Tuning: A Case Study\n  on Audio Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning Outperforms Supervised Fine-Tuning: A Case Study\n  on Audio Question Answering"
                },
                "summary": "Recently, reinforcement learning (RL) has been shown to greatly enhance the\nreasoning capabilities of large language models (LLMs), and RL-based approaches\nhave been progressively applied to visual multimodal tasks. However, the audio\nmodality has largely been overlooked in these developments. Thus, we conduct a\nseries of RL explorations in audio understanding and reasoning, specifically\nfocusing on the audio question answering (AQA) task. We leverage the group\nrelative policy optimization (GRPO) algorithm to Qwen2-Audio-7B-Instruct, and\nour experiments demonstrated state-of-the-art performance on the MMAU Test-mini\nbenchmark, achieving an accuracy rate of 64.5%. The main findings in this\ntechnical report are as follows: 1) The GRPO algorithm can be effectively\napplied to large audio language models (LALMs), even when the model has only\n8.2B parameters; 2) With only 38k post-training samples, RL significantly\noutperforms supervised fine-tuning (SFT), indicating that RL-based approaches\ncan be effective without large datasets; 3) The explicit reasoning process has\nnot shown significant benefits for AQA tasks, and how to efficiently utilize\ndeep thinking remains an open question for further research; 4) LALMs still lag\nfar behind humans auditory-language reasoning, suggesting that the RL-based\napproaches warrant further exploration. Our project is available at\nhttps://github.com/xiaomi-research/r1-aqa and\nhttps://huggingface.co/mispeech/r1-aqa.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, reinforcement learning (RL) has been shown to greatly enhance the\nreasoning capabilities of large language models (LLMs), and RL-based approaches\nhave been progressively applied to visual multimodal tasks. However, the audio\nmodality has largely been overlooked in these developments. Thus, we conduct a\nseries of RL explorations in audio understanding and reasoning, specifically\nfocusing on the audio question answering (AQA) task. We leverage the group\nrelative policy optimization (GRPO) algorithm to Qwen2-Audio-7B-Instruct, and\nour experiments demonstrated state-of-the-art performance on the MMAU Test-mini\nbenchmark, achieving an accuracy rate of 64.5%. The main findings in this\ntechnical report are as follows: 1) The GRPO algorithm can be effectively\napplied to large audio language models (LALMs), even when the model has only\n8.2B parameters; 2) With only 38k post-training samples, RL significantly\noutperforms supervised fine-tuning (SFT), indicating that RL-based approaches\ncan be effective without large datasets; 3) The explicit reasoning process has\nnot shown significant benefits for AQA tasks, and how to efficiently utilize\ndeep thinking remains an open question for further research; 4) LALMs still lag\nfar behind humans auditory-language reasoning, suggesting that the RL-based\napproaches warrant further exploration. Our project is available at\nhttps://github.com/xiaomi-research/r1-aqa and\nhttps://huggingface.co/mispeech/r1-aqa."
                },
                "authors": [
                    {
                        "name": "Gang Li"
                    },
                    {
                        "name": "Jizhong Liu"
                    },
                    {
                        "name": "Heinrich Dinkel"
                    },
                    {
                        "name": "Yadong Niu"
                    },
                    {
                        "name": "Junbo Zhang"
                    },
                    {
                        "name": "Jian Luan"
                    }
                ],
                "author_detail": {
                    "name": "Jian Luan"
                },
                "author": "Jian Luan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11197v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11197v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08699v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08699v2",
                "updated": "2025-05-14T02:10:29Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    2,
                    10,
                    29,
                    2,
                    134,
                    0
                ],
                "published": "2025-05-13T15:58:57Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    15,
                    58,
                    57,
                    1,
                    133,
                    0
                ],
                "title": "Granite-speech: open-source speech-aware LLMs with strong English ASR\n  capabilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Granite-speech: open-source speech-aware LLMs with strong English ASR\n  capabilities"
                },
                "summary": "Granite-speech LLMs are compact and efficient speech language models\nspecifically designed for English ASR and automatic speech translation (AST).\nThe models were trained by modality aligning the 2B and 8B parameter variants\nof granite-3.3-instruct to speech on publicly available open-source corpora\ncontaining audio inputs and text targets consisting of either human transcripts\nfor ASR or automatically generated translations for AST. Comprehensive\nbenchmarking shows that on English ASR, which was our primary focus, they\noutperform several competitors' models that were trained on orders of magnitude\nmore proprietary data, and they keep pace on English-to-X AST for major\nEuropean languages, Japanese, and Chinese. The speech-specific components are:\na conformer acoustic encoder using block attention and self-conditioning\ntrained with connectionist temporal classification, a windowed\nquery-transformer speech modality adapter used to do temporal downsampling of\nthe acoustic embeddings and map them to the LLM text embedding space, and LoRA\nadapters to further fine-tune the text LLM. Granite-speech-3.3 operates in two\nmodes: in speech mode, it performs ASR and AST by activating the encoder,\nprojector, and LoRA adapters; in text mode, it calls the underlying\ngranite-3.3-instruct model directly (without LoRA), essentially preserving all\nthe text LLM capabilities and safety. Both models are freely available on\nHuggingFace (https://huggingface.co/ibm-granite/granite-speech-3.3-2b and\nhttps://huggingface.co/ibm-granite/granite-speech-3.3-8b) and can be used for\nboth research and commercial purposes under a permissive Apache 2.0 license.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Granite-speech LLMs are compact and efficient speech language models\nspecifically designed for English ASR and automatic speech translation (AST).\nThe models were trained by modality aligning the 2B and 8B parameter variants\nof granite-3.3-instruct to speech on publicly available open-source corpora\ncontaining audio inputs and text targets consisting of either human transcripts\nfor ASR or automatically generated translations for AST. Comprehensive\nbenchmarking shows that on English ASR, which was our primary focus, they\noutperform several competitors' models that were trained on orders of magnitude\nmore proprietary data, and they keep pace on English-to-X AST for major\nEuropean languages, Japanese, and Chinese. The speech-specific components are:\na conformer acoustic encoder using block attention and self-conditioning\ntrained with connectionist temporal classification, a windowed\nquery-transformer speech modality adapter used to do temporal downsampling of\nthe acoustic embeddings and map them to the LLM text embedding space, and LoRA\nadapters to further fine-tune the text LLM. Granite-speech-3.3 operates in two\nmodes: in speech mode, it performs ASR and AST by activating the encoder,\nprojector, and LoRA adapters; in text mode, it calls the underlying\ngranite-3.3-instruct model directly (without LoRA), essentially preserving all\nthe text LLM capabilities and safety. Both models are freely available on\nHuggingFace (https://huggingface.co/ibm-granite/granite-speech-3.3-2b and\nhttps://huggingface.co/ibm-granite/granite-speech-3.3-8b) and can be used for\nboth research and commercial purposes under a permissive Apache 2.0 license."
                },
                "authors": [
                    {
                        "name": "George Saon"
                    },
                    {
                        "name": "Avihu Dekel"
                    },
                    {
                        "name": "Alexander Brooks"
                    },
                    {
                        "name": "Tohru Nagano"
                    },
                    {
                        "name": "Abraham Daniels"
                    },
                    {
                        "name": "Aharon Satt"
                    },
                    {
                        "name": "Ashish Mittal"
                    },
                    {
                        "name": "Brian Kingsbury"
                    },
                    {
                        "name": "David Haws"
                    },
                    {
                        "name": "Edmilson Morais"
                    },
                    {
                        "name": "Gakuto Kurata"
                    },
                    {
                        "name": "Hagai Aronowitz"
                    },
                    {
                        "name": "Ibrahim Ibrahim"
                    },
                    {
                        "name": "Jeff Kuo"
                    },
                    {
                        "name": "Kate Soule"
                    },
                    {
                        "name": "Luis Lastras"
                    },
                    {
                        "name": "Masayuki Suzuki"
                    },
                    {
                        "name": "Ron Hoory"
                    },
                    {
                        "name": "Samuel Thomas"
                    },
                    {
                        "name": "Sashi Novitasari"
                    },
                    {
                        "name": "Takashi Fukuda"
                    },
                    {
                        "name": "Vishal Sunder"
                    },
                    {
                        "name": "Xiaodong Cui"
                    },
                    {
                        "name": "Zvi Kons"
                    }
                ],
                "author_detail": {
                    "name": "Zvi Kons"
                },
                "author": "Zvi Kons",
                "arxiv_comment": "7 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08699v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08699v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04717v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04717v4",
                "updated": "2025-05-14T01:48:30Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    1,
                    48,
                    30,
                    2,
                    134,
                    0
                ],
                "published": "2025-04-07T04:00:08Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    4,
                    0,
                    8,
                    0,
                    97,
                    0
                ],
                "title": "Beyond Single-Turn: A Survey on Multi-Turn Interactions with Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Single-Turn: A Survey on Multi-Turn Interactions with Large\n  Language Models"
                },
                "summary": "Recent advancements in large language models (LLMs) have revolutionized their\nability to handle single-turn tasks, yet real-world applications demand\nsophisticated multi-turn interactions. This survey provides a comprehensive\nreview of recent advancements in evaluating and enhancing multi-turn\ninteractions in LLMs. Focusing on task-specific scenarios, from instruction\nfollowing in diverse domains such as math and coding to complex conversational\nengagements in roleplay, healthcare, education, and even adversarial jailbreak\nsettings, we systematically examine the challenges of maintaining context,\ncoherence, fairness, and responsiveness over prolonged dialogues. The paper\norganizes current benchmarks and datasets into coherent categories that reflect\nthe evolving landscape of multi-turn dialogue evaluation. In addition, we\nreview a range of enhancement methodologies under multi-turn settings,\nincluding model-centric strategies (contextual learning, supervised\nfine-tuning, reinforcement learning, and new architectures), external\nintegration approaches (memory-augmented, retrieval-based methods, and\nknowledge graph), and agent-based techniques for collaborative interactions.\nFinally, we discuss open challenges and propose future directions for research\nto further advance the robustness and effectiveness of multi-turn interactions\nin LLMs. Related resources and papers are available at\nhttps://github.com/yubol-cmu/Awesome-Multi-Turn-LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have revolutionized their\nability to handle single-turn tasks, yet real-world applications demand\nsophisticated multi-turn interactions. This survey provides a comprehensive\nreview of recent advancements in evaluating and enhancing multi-turn\ninteractions in LLMs. Focusing on task-specific scenarios, from instruction\nfollowing in diverse domains such as math and coding to complex conversational\nengagements in roleplay, healthcare, education, and even adversarial jailbreak\nsettings, we systematically examine the challenges of maintaining context,\ncoherence, fairness, and responsiveness over prolonged dialogues. The paper\norganizes current benchmarks and datasets into coherent categories that reflect\nthe evolving landscape of multi-turn dialogue evaluation. In addition, we\nreview a range of enhancement methodologies under multi-turn settings,\nincluding model-centric strategies (contextual learning, supervised\nfine-tuning, reinforcement learning, and new architectures), external\nintegration approaches (memory-augmented, retrieval-based methods, and\nknowledge graph), and agent-based techniques for collaborative interactions.\nFinally, we discuss open challenges and propose future directions for research\nto further advance the robustness and effectiveness of multi-turn interactions\nin LLMs. Related resources and papers are available at\nhttps://github.com/yubol-cmu/Awesome-Multi-Turn-LLMs."
                },
                "authors": [
                    {
                        "name": "Yubo Li"
                    },
                    {
                        "name": "Xiaobin Shen"
                    },
                    {
                        "name": "Xinyu Yao"
                    },
                    {
                        "name": "Xueying Ding"
                    },
                    {
                        "name": "Yidi Miao"
                    },
                    {
                        "name": "Ramayya Krishnan"
                    },
                    {
                        "name": "Rema Padman"
                    }
                ],
                "author_detail": {
                    "name": "Rema Padman"
                },
                "author": "Rema Padman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04717v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04717v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08167v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08167v2",
                "updated": "2025-05-14T01:35:33Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    1,
                    35,
                    33,
                    2,
                    134,
                    0
                ],
                "published": "2025-05-13T02:05:25Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    2,
                    5,
                    25,
                    1,
                    133,
                    0
                ],
                "title": "Fusing Bidirectional Chains of Thought and Reward Mechanisms A Method\n  for Enhancing Question-Answering Capabilities of Large Language Models for\n  Chinese Intangible Cultural Heritage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fusing Bidirectional Chains of Thought and Reward Mechanisms A Method\n  for Enhancing Question-Answering Capabilities of Large Language Models for\n  Chinese Intangible Cultural Heritage"
                },
                "summary": "The rapid development of large language models (LLMs) has provided\nsignificant support and opportunities for the advancement of domain-specific\nLLMs. However, fine-tuning these large models using Intangible Cultural\nHeritage (ICH) data inevitably faces challenges such as bias, incorrect\nknowledge inheritance, and catastrophic forgetting. To address these issues, we\npropose a novel training method that integrates a bidirectional chains of\nthought and a reward mechanism. This method is built upon ICH-Qwen, a large\nlanguage model specifically designed for the field of intangible cultural\nheritage. The proposed method enables the model to not only perform forward\nreasoning but also enhances the accuracy of the generated answers by utilizing\nreverse questioning and reverse reasoning to activate the model's latent\nknowledge. Additionally, a reward mechanism is introduced during training to\noptimize the decision-making process. This mechanism improves the quality of\nthe model's outputs through structural and content evaluations with different\nweighting schemes. We conduct comparative experiments on ICH-Qwen, with results\ndemonstrating that our method outperforms 0-shot, step-by-step reasoning,\nknowledge distillation, and question augmentation methods in terms of accuracy,\nBleu-4, and Rouge-L scores on the question-answering task. Furthermore, the\npaper highlights the effectiveness of combining the bidirectional chains of\nthought and reward mechanism through ablation experiments. In addition, a\nseries of generalizability experiments are conducted, with results showing that\nthe proposed method yields improvements on various domain-specific datasets and\nadvanced models in areas such as Finance, Wikidata, and StrategyQA. This\ndemonstrates that the method is adaptable to multiple domains and provides a\nvaluable approach for model training in future applications across diverse\nfields.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development of large language models (LLMs) has provided\nsignificant support and opportunities for the advancement of domain-specific\nLLMs. However, fine-tuning these large models using Intangible Cultural\nHeritage (ICH) data inevitably faces challenges such as bias, incorrect\nknowledge inheritance, and catastrophic forgetting. To address these issues, we\npropose a novel training method that integrates a bidirectional chains of\nthought and a reward mechanism. This method is built upon ICH-Qwen, a large\nlanguage model specifically designed for the field of intangible cultural\nheritage. The proposed method enables the model to not only perform forward\nreasoning but also enhances the accuracy of the generated answers by utilizing\nreverse questioning and reverse reasoning to activate the model's latent\nknowledge. Additionally, a reward mechanism is introduced during training to\noptimize the decision-making process. This mechanism improves the quality of\nthe model's outputs through structural and content evaluations with different\nweighting schemes. We conduct comparative experiments on ICH-Qwen, with results\ndemonstrating that our method outperforms 0-shot, step-by-step reasoning,\nknowledge distillation, and question augmentation methods in terms of accuracy,\nBleu-4, and Rouge-L scores on the question-answering task. Furthermore, the\npaper highlights the effectiveness of combining the bidirectional chains of\nthought and reward mechanism through ablation experiments. In addition, a\nseries of generalizability experiments are conducted, with results showing that\nthe proposed method yields improvements on various domain-specific datasets and\nadvanced models in areas such as Finance, Wikidata, and StrategyQA. This\ndemonstrates that the method is adaptable to multiple domains and provides a\nvaluable approach for model training in future applications across diverse\nfields."
                },
                "authors": [
                    {
                        "name": "Ruilin Liu"
                    },
                    {
                        "name": "Zhixiao Zhao"
                    },
                    {
                        "name": "Jieqiong Li"
                    },
                    {
                        "name": "Chang Liu"
                    },
                    {
                        "name": "Dongbo Wang"
                    }
                ],
                "author_detail": {
                    "name": "Dongbo Wang"
                },
                "author": "Dongbo Wang",
                "arxiv_comment": "22 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08167v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08167v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09056v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09056v1",
                "updated": "2025-05-14T01:21:46Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    1,
                    21,
                    46,
                    2,
                    134,
                    0
                ],
                "published": "2025-05-14T01:21:46Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    1,
                    21,
                    46,
                    2,
                    134,
                    0
                ],
                "title": "A Comprehensive Analysis of Large Language Model Outputs: Similarity,\n  Diversity, and Bias",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Comprehensive Analysis of Large Language Model Outputs: Similarity,\n  Diversity, and Bias"
                },
                "summary": "Large Language Models (LLMs) represent a major step toward artificial general\nintelligence, significantly advancing our ability to interact with technology.\nWhile LLMs perform well on Natural Language Processing tasks -- such as\ntranslation, generation, code writing, and summarization -- questions remain\nabout their output similarity, variability, and ethical implications. For\ninstance, how similar are texts generated by the same model? How does this\ncompare across different models? And which models best uphold ethical\nstandards? To investigate, we used 5{,}000 prompts spanning diverse tasks like\ngeneration, explanation, and rewriting. This resulted in approximately 3\nmillion texts from 12 LLMs, including proprietary and open-source systems from\nOpenAI, Google, Microsoft, Meta, and Mistral. Key findings include: (1) outputs\nfrom the same LLM are more similar to each other than to human-written texts;\n(2) models like WizardLM-2-8x22b generate highly similar outputs, while GPT-4\nproduces more varied responses; (3) LLM writing styles differ significantly,\nwith Llama 3 and Mistral showing higher similarity, and GPT-4 standing out for\ndistinctiveness; (4) differences in vocabulary and tone underscore the\nlinguistic uniqueness of LLM-generated content; (5) some LLMs demonstrate\ngreater gender balance and reduced bias. These results offer new insights into\nthe behavior and diversity of LLM outputs, helping guide future development and\nethical evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) represent a major step toward artificial general\nintelligence, significantly advancing our ability to interact with technology.\nWhile LLMs perform well on Natural Language Processing tasks -- such as\ntranslation, generation, code writing, and summarization -- questions remain\nabout their output similarity, variability, and ethical implications. For\ninstance, how similar are texts generated by the same model? How does this\ncompare across different models? And which models best uphold ethical\nstandards? To investigate, we used 5{,}000 prompts spanning diverse tasks like\ngeneration, explanation, and rewriting. This resulted in approximately 3\nmillion texts from 12 LLMs, including proprietary and open-source systems from\nOpenAI, Google, Microsoft, Meta, and Mistral. Key findings include: (1) outputs\nfrom the same LLM are more similar to each other than to human-written texts;\n(2) models like WizardLM-2-8x22b generate highly similar outputs, while GPT-4\nproduces more varied responses; (3) LLM writing styles differ significantly,\nwith Llama 3 and Mistral showing higher similarity, and GPT-4 standing out for\ndistinctiveness; (4) differences in vocabulary and tone underscore the\nlinguistic uniqueness of LLM-generated content; (5) some LLMs demonstrate\ngreater gender balance and reduced bias. These results offer new insights into\nthe behavior and diversity of LLM outputs, helping guide future development and\nethical evaluation."
                },
                "authors": [
                    {
                        "name": "Brandon Smith"
                    },
                    {
                        "name": "Mohamed Reda Bouadjenek"
                    },
                    {
                        "name": "Tahsin Alamgir Kheya"
                    },
                    {
                        "name": "Phillip Dawson"
                    },
                    {
                        "name": "Sunil Aryal"
                    }
                ],
                "author_detail": {
                    "name": "Sunil Aryal"
                },
                "author": "Sunil Aryal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09056v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09056v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09048v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09048v1",
                "updated": "2025-05-14T01:04:45Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    1,
                    4,
                    45,
                    2,
                    134,
                    0
                ],
                "published": "2025-05-14T01:04:45Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    1,
                    4,
                    45,
                    2,
                    134,
                    0
                ],
                "title": "Modeling Interdependent Cybersecurity Threats Using Bayesian Networks: A\n  Case Study on In-Vehicle Infotainment Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling Interdependent Cybersecurity Threats Using Bayesian Networks: A\n  Case Study on In-Vehicle Infotainment Systems"
                },
                "summary": "Cybersecurity threats are increasingly marked by interdependence,\nuncertainty, and evolving complexity challenges that traditional assessment\nmethods such as CVSS, STRIDE, and attack trees fail to adequately capture. This\npaper reviews the application of Bayesian Networks (BNs) in cybersecurity risk\nmodeling, highlighting their capacity to represent probabilistic dependencies,\nintegrate diverse threat indicators, and support reasoning under uncertainty. A\nstructured case study is presented in which a STRIDE-based attack tree for an\nautomotive In-Vehicle Infotainment (IVI) system is transformed into a Bayesian\nNetwork. Logical relationships are encoded using Conditional Probability Tables\n(CPTs), and threat likelihoods are derived from normalized DREAD scores. The\nmodel enables not only probabilistic inference of system compromise likelihood\nbut also supports causal analysis using do-calculus and local sensitivity\nanalysis to identify high-impact vulnerabilities. These analyses provide\ninsight into the most influential nodes within the threat propagation chain,\ninforming targeted mitigation strategies. While demonstrating the potential of\nBNs for dynamic and context-aware risk assessment, the study also outlines\nlimitations related to scalability, reliance on expert input, static structure\nassumptions, and limited temporal modeling. The paper concludes by advocating\nfor future enhancements through Dynamic Bayesian Networks, structure learning,\nand adaptive inference to better support real-time cybersecurity\ndecision-making in complex environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cybersecurity threats are increasingly marked by interdependence,\nuncertainty, and evolving complexity challenges that traditional assessment\nmethods such as CVSS, STRIDE, and attack trees fail to adequately capture. This\npaper reviews the application of Bayesian Networks (BNs) in cybersecurity risk\nmodeling, highlighting their capacity to represent probabilistic dependencies,\nintegrate diverse threat indicators, and support reasoning under uncertainty. A\nstructured case study is presented in which a STRIDE-based attack tree for an\nautomotive In-Vehicle Infotainment (IVI) system is transformed into a Bayesian\nNetwork. Logical relationships are encoded using Conditional Probability Tables\n(CPTs), and threat likelihoods are derived from normalized DREAD scores. The\nmodel enables not only probabilistic inference of system compromise likelihood\nbut also supports causal analysis using do-calculus and local sensitivity\nanalysis to identify high-impact vulnerabilities. These analyses provide\ninsight into the most influential nodes within the threat propagation chain,\ninforming targeted mitigation strategies. While demonstrating the potential of\nBNs for dynamic and context-aware risk assessment, the study also outlines\nlimitations related to scalability, reliance on expert input, static structure\nassumptions, and limited temporal modeling. The paper concludes by advocating\nfor future enhancements through Dynamic Bayesian Networks, structure learning,\nand adaptive inference to better support real-time cybersecurity\ndecision-making in complex environments."
                },
                "authors": [
                    {
                        "name": "Sangita Sridar"
                    }
                ],
                "author_detail": {
                    "name": "Sangita Sridar"
                },
                "author": "Sangita Sridar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09048v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09048v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.07089v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.07089v2",
                "updated": "2025-05-14T00:44:05Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    0,
                    44,
                    5,
                    2,
                    134,
                    0
                ],
                "published": "2025-05-11T18:38:00Z",
                "published_parsed": [
                    2025,
                    5,
                    11,
                    18,
                    38,
                    0,
                    6,
                    131,
                    0
                ],
                "title": "RefPentester: A Knowledge-Informed Self-Reflective Penetration Testing\n  Framework Based on Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RefPentester: A Knowledge-Informed Self-Reflective Penetration Testing\n  Framework Based on Large Language Models"
                },
                "summary": "Automated penetration testing (AutoPT) powered by large language models\n(LLMs) has gained attention for its ability to automate ethical hacking\nprocesses and identify vulnerabilities in target systems by leveraging the\nintrinsic knowledge of LLMs. However, existing LLM-based AutoPT frameworks\noften underperform compared to human experts in challenging tasks for several\nreasons: the imbalanced knowledge used in LLM training, short-sighted planning\nin the planning process, and hallucinations during command generation. In\naddition, the penetration testing (PT) process, with its trial-and-error\nnature, is limited by existing frameworks that lack mechanisms to learn from\nprevious failed operations, restricting adaptive improvement of PT strategies.\nTo address these limitations, we propose a knowledge-informed self-reflective\nPT framework powered by LLMs, called RefPentester, which is an AutoPT framework\ndesigned to assist human operators in identifying the current stage of the PT\nprocess, selecting appropriate tactic and technique for the stage, choosing\nsuggested action, providing step-by-step operational guidance, and learning\nfrom previous failed operations. We also modeled the PT process as a\nseven-state Stage Machine to integrate the proposed framework effectively. The\nevaluation shows that RefPentester can successfully reveal credentials on Hack\nThe Box's Sau machine, outperforming the baseline GPT-4o model by 16.7%. Across\nPT stages, RefPentester also demonstrates superior success rates on PT stage\ntransitions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated penetration testing (AutoPT) powered by large language models\n(LLMs) has gained attention for its ability to automate ethical hacking\nprocesses and identify vulnerabilities in target systems by leveraging the\nintrinsic knowledge of LLMs. However, existing LLM-based AutoPT frameworks\noften underperform compared to human experts in challenging tasks for several\nreasons: the imbalanced knowledge used in LLM training, short-sighted planning\nin the planning process, and hallucinations during command generation. In\naddition, the penetration testing (PT) process, with its trial-and-error\nnature, is limited by existing frameworks that lack mechanisms to learn from\nprevious failed operations, restricting adaptive improvement of PT strategies.\nTo address these limitations, we propose a knowledge-informed self-reflective\nPT framework powered by LLMs, called RefPentester, which is an AutoPT framework\ndesigned to assist human operators in identifying the current stage of the PT\nprocess, selecting appropriate tactic and technique for the stage, choosing\nsuggested action, providing step-by-step operational guidance, and learning\nfrom previous failed operations. We also modeled the PT process as a\nseven-state Stage Machine to integrate the proposed framework effectively. The\nevaluation shows that RefPentester can successfully reveal credentials on Hack\nThe Box's Sau machine, outperforming the baseline GPT-4o model by 16.7%. Across\nPT stages, RefPentester also demonstrates superior success rates on PT stage\ntransitions."
                },
                "authors": [
                    {
                        "name": "Hanzheng Dai"
                    },
                    {
                        "name": "Yuanliang Li"
                    },
                    {
                        "name": "Zhibo Zhang"
                    },
                    {
                        "name": "Jun Yan"
                    }
                ],
                "author_detail": {
                    "name": "Jun Yan"
                },
                "author": "Jun Yan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.07089v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.07089v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09040v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09040v1",
                "updated": "2025-05-14T00:41:44Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    0,
                    41,
                    44,
                    2,
                    134,
                    0
                ],
                "published": "2025-05-14T00:41:44Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    0,
                    41,
                    44,
                    2,
                    134,
                    0
                ],
                "title": "RT-cache: Efficient Robot Trajectory Retrieval System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RT-cache: Efficient Robot Trajectory Retrieval System"
                },
                "summary": "This paper introduces RT-cache, a novel trajectorymemory pipeline that\naccelerates real-world robot inference by leveraging big-data retrieval and\nlearning from experience. While modern Vision-Language-Action (VLA) models can\nhandle diverse robotic tasks, they often incur high per-step inference costs,\nresulting in significant latency, sometimes minutes per task. In contrast,\nRT-cache stores a large-scale Memory of previously successful robot\ntrajectories and retrieves relevant multistep motion snippets, drastically\nreducing inference overhead. By integrating a Memory Builder with a Trajectory\nRetrieval, we develop an efficient retrieval process that remains tractable\neven for extremely large datasets. RT-cache flexibly accumulates real-world\nexperiences and replays them whenever the current scene matches past states,\nadapting quickly to new or unseen environments with only a few additional\nsamples. Experiments on the Open-X Embodiment Dataset and other real-world data\ndemonstrate that RT-cache completes tasks both faster and more successfully\nthan a baseline lacking retrieval, suggesting a practical, data-driven solution\nfor real-time manipulation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces RT-cache, a novel trajectorymemory pipeline that\naccelerates real-world robot inference by leveraging big-data retrieval and\nlearning from experience. While modern Vision-Language-Action (VLA) models can\nhandle diverse robotic tasks, they often incur high per-step inference costs,\nresulting in significant latency, sometimes minutes per task. In contrast,\nRT-cache stores a large-scale Memory of previously successful robot\ntrajectories and retrieves relevant multistep motion snippets, drastically\nreducing inference overhead. By integrating a Memory Builder with a Trajectory\nRetrieval, we develop an efficient retrieval process that remains tractable\neven for extremely large datasets. RT-cache flexibly accumulates real-world\nexperiences and replays them whenever the current scene matches past states,\nadapting quickly to new or unseen environments with only a few additional\nsamples. Experiments on the Open-X Embodiment Dataset and other real-world data\ndemonstrate that RT-cache completes tasks both faster and more successfully\nthan a baseline lacking retrieval, suggesting a practical, data-driven solution\nfor real-time manipulation."
                },
                "authors": [
                    {
                        "name": "Owen Kwon"
                    },
                    {
                        "name": "Abraham George"
                    },
                    {
                        "name": "Alison Bartsch"
                    },
                    {
                        "name": "Amir Barati Farimani"
                    }
                ],
                "author_detail": {
                    "name": "Amir Barati Farimani"
                },
                "author": "Amir Barati Farimani",
                "arxiv_comment": "9 pages, 5 figures. Submitted to an IEEE robotics conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09040v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09040v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2505.09610v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09610v1",
                "updated": "2025-05-14T17:58:40Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    17,
                    58,
                    40,
                    2,
                    134,
                    0
                ],
                "published": "2025-05-14T17:58:40Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    17,
                    58,
                    40,
                    2,
                    134,
                    0
                ],
                "title": "Customizing a Large Language Model for VHDL Design of High-Performance\n  Microprocessors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Customizing a Large Language Model for VHDL Design of High-Performance\n  Microprocessors"
                },
                "summary": "The use of Large Language Models (LLMs) in hardware design has taken off in\nrecent years, principally through its incorporation in tools that increase chip\ndesigner productivity. There has been considerable discussion about the use of\nLLMs in RTL specifications of chip designs, for which the two most popular\nlanguages are Verilog and VHDL. LLMs and their use in Verilog design has\nreceived significant attention due to the higher popularity of the language,\nbut little attention so far has been given to VHDL despite its continued\npopularity in the industry. There has also been little discussion about the\nunique needs of organizations that engage in high-performance processor design,\nand techniques to deploy AI solutions in these settings. In this paper, we\ndescribe our journey in developing a Large Language Model (LLM) specifically\nfor the purpose of explaining VHDL code, a task that has particular importance\nin an organization with decades of experience and assets in high-performance\nprocessor design. We show how we developed test sets specific to our needs and\nused them for evaluating models as we performed extended pretraining (EPT) of a\nbase LLM. Expert evaluation of the code explanations produced by the EPT model\nincreased to 69% compared to a base model rating of 43%. We further show how we\ndeveloped an LLM-as-a-judge to gauge models similar to expert evaluators. This\nled us to deriving and evaluating a host of new models, including an\ninstruction-tuned version of the EPT model with an expected expert evaluator\nrating of 71%. Our experiments also indicate that with the potential use of\nnewer base models, this rating can be pushed to 85% and beyond. We conclude\nwith a discussion on further improving the quality of hardware design LLMs\nusing exciting new developments in the Generative AI world.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The use of Large Language Models (LLMs) in hardware design has taken off in\nrecent years, principally through its incorporation in tools that increase chip\ndesigner productivity. There has been considerable discussion about the use of\nLLMs in RTL specifications of chip designs, for which the two most popular\nlanguages are Verilog and VHDL. LLMs and their use in Verilog design has\nreceived significant attention due to the higher popularity of the language,\nbut little attention so far has been given to VHDL despite its continued\npopularity in the industry. There has also been little discussion about the\nunique needs of organizations that engage in high-performance processor design,\nand techniques to deploy AI solutions in these settings. In this paper, we\ndescribe our journey in developing a Large Language Model (LLM) specifically\nfor the purpose of explaining VHDL code, a task that has particular importance\nin an organization with decades of experience and assets in high-performance\nprocessor design. We show how we developed test sets specific to our needs and\nused them for evaluating models as we performed extended pretraining (EPT) of a\nbase LLM. Expert evaluation of the code explanations produced by the EPT model\nincreased to 69% compared to a base model rating of 43%. We further show how we\ndeveloped an LLM-as-a-judge to gauge models similar to expert evaluators. This\nled us to deriving and evaluating a host of new models, including an\ninstruction-tuned version of the EPT model with an expected expert evaluator\nrating of 71%. Our experiments also indicate that with the potential use of\nnewer base models, this rating can be pushed to 85% and beyond. We conclude\nwith a discussion on further improving the quality of hardware design LLMs\nusing exciting new developments in the Generative AI world."
                },
                "authors": [
                    {
                        "name": "Nicolas Dupuis"
                    },
                    {
                        "name": "Ravi Nair"
                    },
                    {
                        "name": "Shyam Ramji"
                    },
                    {
                        "name": "Sean McClintock"
                    },
                    {
                        "name": "Nishant Chauhan"
                    },
                    {
                        "name": "Priyanka Nagpal"
                    },
                    {
                        "name": "Bart Blaner"
                    },
                    {
                        "name": "Ken Valk"
                    },
                    {
                        "name": "Leon Stok"
                    },
                    {
                        "name": "Ruchir Puri"
                    }
                ],
                "author_detail": {
                    "name": "Ruchir Puri"
                },
                "author": "Ruchir Puri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09610v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09610v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09606v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09606v1",
                "updated": "2025-05-14T17:57:21Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    17,
                    57,
                    21,
                    2,
                    134,
                    0
                ],
                "published": "2025-05-14T17:57:21Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    17,
                    57,
                    21,
                    2,
                    134,
                    0
                ],
                "title": "Comparative Analysis of GFN Methods in Geometry Optimization of Small\n  Organic Semiconductor Molecules: A DFT Benchmarking Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comparative Analysis of GFN Methods in Geometry Optimization of Small\n  Organic Semiconductor Molecules: A DFT Benchmarking Study"
                },
                "summary": "This study benchmarks the geometric, frequency, non-covalent (GFN)\nsemi-empirical methods, GFN1-xTB, GFN2-xTB, GFN0-xTB, and GFN-FF, against\ndensity functional theory (DFT) for the geometry optimization of small organic\nsemiconductor molecules. Two datasets are evaluated: a QM9-derived subset of\nsmall organic molecules and the Harvard Clean Energy Project (CEP) database of\nextended $\\pi$-systems relevant to organic photovoltaics. Structural agreement\nis quantified using heavy-atom RMSD, equilibrium rotational constants, bond\nlengths, angles, and HOMO-LUMO energy gaps. Computational efficiency is\nassessed via CPU time and scaling behavior. GFN1-xTB and GFN2-xTB demonstrate\nthe highest structural fidelity, while GFN-FF offers an optimal balance between\naccuracy and speed, particularly for larger systems. The results indicate that\nGFN-based methods are suitable for high-throughput molecular screening of small\norganic semiconductors, the choice of the method depending on the accuracy-cost\ntrade-offs. The findings support the deployment of GFN approaches in\ncomputational pipelines for the discovery of organic electronics and materials,\nproviding information on their strengths and limitations relative to\nestablished DFT methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study benchmarks the geometric, frequency, non-covalent (GFN)\nsemi-empirical methods, GFN1-xTB, GFN2-xTB, GFN0-xTB, and GFN-FF, against\ndensity functional theory (DFT) for the geometry optimization of small organic\nsemiconductor molecules. Two datasets are evaluated: a QM9-derived subset of\nsmall organic molecules and the Harvard Clean Energy Project (CEP) database of\nextended $\\pi$-systems relevant to organic photovoltaics. Structural agreement\nis quantified using heavy-atom RMSD, equilibrium rotational constants, bond\nlengths, angles, and HOMO-LUMO energy gaps. Computational efficiency is\nassessed via CPU time and scaling behavior. GFN1-xTB and GFN2-xTB demonstrate\nthe highest structural fidelity, while GFN-FF offers an optimal balance between\naccuracy and speed, particularly for larger systems. The results indicate that\nGFN-based methods are suitable for high-throughput molecular screening of small\norganic semiconductors, the choice of the method depending on the accuracy-cost\ntrade-offs. The findings support the deployment of GFN approaches in\ncomputational pipelines for the discovery of organic electronics and materials,\nproviding information on their strengths and limitations relative to\nestablished DFT methods."
                },
                "authors": [
                    {
                        "name": "Steve Cabrel Teguia Kouam"
                    },
                    {
                        "name": "Jean-Pierre Tchapet Njafa"
                    },
                    {
                        "name": "Raoult Dabou Teukam"
                    },
                    {
                        "name": "Patrick Mvoto Kongo"
                    },
                    {
                        "name": "Jean-Pierre Nguenang"
                    },
                    {
                        "name": "Serge Guy Nana Engo"
                    }
                ],
                "author_detail": {
                    "name": "Serge Guy Nana Engo"
                },
                "author": "Serge Guy Nana Engo",
                "arxiv_comment": "The PDF contains the main text and the supplementary materials,\n  including 39 pages, 23 figures, 7 tables, and to be published in \"Chemical\n  Physics\"",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09606v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09606v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.chem-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09602v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09602v1",
                "updated": "2025-05-14T17:52:10Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    17,
                    52,
                    10,
                    2,
                    134,
                    0
                ],
                "published": "2025-05-14T17:52:10Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    17,
                    52,
                    10,
                    2,
                    134,
                    0
                ],
                "title": "Adversarial Suffix Filtering: a Defense Pipeline for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adversarial Suffix Filtering: a Defense Pipeline for LLMs"
                },
                "summary": "Large Language Models (LLMs) are increasingly embedded in autonomous systems\nand public-facing environments, yet they remain susceptible to jailbreak\nvulnerabilities that may undermine their security and trustworthiness.\nAdversarial suffixes are considered to be the current state-of-the-art\njailbreak, consistently outperforming simpler methods and frequently succeeding\neven in black-box settings. Existing defenses rely on access to the internal\narchitecture of models limiting diverse deployment, increase memory and\ncomputation footprints dramatically, or can be bypassed with simple prompt\nengineering methods. We introduce $\\textbf{Adversarial Suffix Filtering}$\n(ASF), a lightweight novel model-agnostic defensive pipeline designed to\nprotect LLMs against adversarial suffix attacks. ASF functions as an input\npreprocessor and sanitizer that detects and filters adversarially crafted\nsuffixes in prompts, effectively neutralizing malicious injections. We\ndemonstrate that ASF provides comprehensive defense capabilities across both\nblack-box and white-box attack settings, reducing the attack efficacy of\nstate-of-the-art adversarial suffix generation methods to below 4%, while only\nminimally affecting the target model's capabilities in non-adversarial\nscenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly embedded in autonomous systems\nand public-facing environments, yet they remain susceptible to jailbreak\nvulnerabilities that may undermine their security and trustworthiness.\nAdversarial suffixes are considered to be the current state-of-the-art\njailbreak, consistently outperforming simpler methods and frequently succeeding\neven in black-box settings. Existing defenses rely on access to the internal\narchitecture of models limiting diverse deployment, increase memory and\ncomputation footprints dramatically, or can be bypassed with simple prompt\nengineering methods. We introduce $\\textbf{Adversarial Suffix Filtering}$\n(ASF), a lightweight novel model-agnostic defensive pipeline designed to\nprotect LLMs against adversarial suffix attacks. ASF functions as an input\npreprocessor and sanitizer that detects and filters adversarially crafted\nsuffixes in prompts, effectively neutralizing malicious injections. We\ndemonstrate that ASF provides comprehensive defense capabilities across both\nblack-box and white-box attack settings, reducing the attack efficacy of\nstate-of-the-art adversarial suffix generation methods to below 4%, while only\nminimally affecting the target model's capabilities in non-adversarial\nscenarios."
                },
                "authors": [
                    {
                        "name": "David Khachaturov"
                    },
                    {
                        "name": "Robert Mullins"
                    }
                ],
                "author_detail": {
                    "name": "Robert Mullins"
                },
                "author": "Robert Mullins",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09602v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09602v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09598v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09598v1",
                "updated": "2025-05-14T17:47:00Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    17,
                    47,
                    0,
                    2,
                    134,
                    0
                ],
                "published": "2025-05-14T17:47:00Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    17,
                    47,
                    0,
                    2,
                    134,
                    0
                ],
                "title": "How Hungry is AI? Benchmarking Energy, Water, and Carbon Footprint of\n  LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Hungry is AI? Benchmarking Energy, Water, and Carbon Footprint of\n  LLM Inference"
                },
                "summary": "As large language models (LLMs) spread across industries, understanding their\nenvironmental footprint at the inference level is no longer optional; it is\nessential. However, most existing studies exclude proprietary models, overlook\ninfrastructural variability and overhead, or focus solely on training, even as\ninference increasingly dominates AI's environmental impact. To bridge this gap,\nthis paper introduces a novel infrastructure-aware benchmarking framework for\nquantifying the environmental footprint of LLM inference across 30\nstate-of-the-art models as deployed in commercial data centers. Our framework\ncombines public API performance data with region-specific environmental\nmultipliers and statistical inference of hardware configurations. We\nadditionally utilize cross-efficiency Data Envelopment Analysis (DEA) to rank\nmodels by performance relative to environmental cost. Our results show that o3\nand DeepSeek-R1 emerge as the most energy-intensive models, consuming over 33\nWh per long prompt, more than 70 times the consumption of GPT-4.1 nano, and\nthat Claude-3.7 Sonnet ranks highest in eco-efficiency. While a single short\nGPT-4o query consumes 0.43 Wh, scaling this to 700 million queries/day results\nin substantial annual environmental impacts. These include electricity use\ncomparable to 35,000 U.S. homes, freshwater evaporation matching the annual\ndrinking needs of 1.2 million people, and carbon emissions requiring a\nChicago-sized forest to offset. These findings illustrate a growing paradox:\nalthough individual queries are efficient, their global scale drives\ndisproportionate resource consumption. Our study provides a standardized,\nempirically grounded methodology for benchmarking the sustainability of LLM\ndeployments, laying a foundation for future environmental accountability in AI\ndevelopment and sustainability standards.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) spread across industries, understanding their\nenvironmental footprint at the inference level is no longer optional; it is\nessential. However, most existing studies exclude proprietary models, overlook\ninfrastructural variability and overhead, or focus solely on training, even as\ninference increasingly dominates AI's environmental impact. To bridge this gap,\nthis paper introduces a novel infrastructure-aware benchmarking framework for\nquantifying the environmental footprint of LLM inference across 30\nstate-of-the-art models as deployed in commercial data centers. Our framework\ncombines public API performance data with region-specific environmental\nmultipliers and statistical inference of hardware configurations. We\nadditionally utilize cross-efficiency Data Envelopment Analysis (DEA) to rank\nmodels by performance relative to environmental cost. Our results show that o3\nand DeepSeek-R1 emerge as the most energy-intensive models, consuming over 33\nWh per long prompt, more than 70 times the consumption of GPT-4.1 nano, and\nthat Claude-3.7 Sonnet ranks highest in eco-efficiency. While a single short\nGPT-4o query consumes 0.43 Wh, scaling this to 700 million queries/day results\nin substantial annual environmental impacts. These include electricity use\ncomparable to 35,000 U.S. homes, freshwater evaporation matching the annual\ndrinking needs of 1.2 million people, and carbon emissions requiring a\nChicago-sized forest to offset. These findings illustrate a growing paradox:\nalthough individual queries are efficient, their global scale drives\ndisproportionate resource consumption. Our study provides a standardized,\nempirically grounded methodology for benchmarking the sustainability of LLM\ndeployments, laying a foundation for future environmental accountability in AI\ndevelopment and sustainability standards."
                },
                "authors": [
                    {
                        "name": "Nidhal Jegham"
                    },
                    {
                        "name": "Marwen Abdelatti"
                    },
                    {
                        "name": "Lassad Elmoubarki"
                    },
                    {
                        "name": "Abdeltawab Hendawi"
                    }
                ],
                "author_detail": {
                    "name": "Abdeltawab Hendawi"
                },
                "author": "Abdeltawab Hendawi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09598v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09598v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09595v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09595v1",
                "updated": "2025-05-14T17:43:40Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    17,
                    43,
                    40,
                    2,
                    134,
                    0
                ],
                "published": "2025-05-14T17:43:40Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    17,
                    43,
                    40,
                    2,
                    134,
                    0
                ],
                "title": "WorldView-Bench: A Benchmark for Evaluating Global Cultural Perspectives\n  in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WorldView-Bench: A Benchmark for Evaluating Global Cultural Perspectives\n  in Large Language Models"
                },
                "summary": "Large Language Models (LLMs) are predominantly trained and aligned in ways\nthat reinforce Western-centric epistemologies and socio-cultural norms, leading\nto cultural homogenization and limiting their ability to reflect global\ncivilizational plurality. Existing benchmarking frameworks fail to adequately\ncapture this bias, as they rely on rigid, closed-form assessments that overlook\nthe complexity of cultural inclusivity. To address this, we introduce\nWorldView-Bench, a benchmark designed to evaluate Global Cultural Inclusivity\n(GCI) in LLMs by analyzing their ability to accommodate diverse worldviews. Our\napproach is grounded in the Multiplex Worldview proposed by Senturk et al.,\nwhich distinguishes between Uniplex models, reinforcing cultural\nhomogenization, and Multiplex models, which integrate diverse perspectives.\nWorldView-Bench measures Cultural Polarization, the exclusion of alternative\nperspectives, through free-form generative evaluation rather than conventional\ncategorical benchmarks. We implement applied multiplexity through two\nintervention strategies: (1) Contextually-Implemented Multiplex LLMs, where\nsystem prompts embed multiplexity principles, and (2) Multi-Agent System\n(MAS)-Implemented Multiplex LLMs, where multiple LLM agents representing\ndistinct cultural perspectives collaboratively generate responses. Our results\ndemonstrate a significant increase in Perspectives Distribution Score (PDS)\nentropy from 13% at baseline to 94% with MAS-Implemented Multiplex LLMs,\nalongside a shift toward positive sentiment (67.7%) and enhanced cultural\nbalance. These findings highlight the potential of multiplex-aware AI\nevaluation in mitigating cultural bias in LLMs, paving the way for more\ninclusive and ethically aligned AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are predominantly trained and aligned in ways\nthat reinforce Western-centric epistemologies and socio-cultural norms, leading\nto cultural homogenization and limiting their ability to reflect global\ncivilizational plurality. Existing benchmarking frameworks fail to adequately\ncapture this bias, as they rely on rigid, closed-form assessments that overlook\nthe complexity of cultural inclusivity. To address this, we introduce\nWorldView-Bench, a benchmark designed to evaluate Global Cultural Inclusivity\n(GCI) in LLMs by analyzing their ability to accommodate diverse worldviews. Our\napproach is grounded in the Multiplex Worldview proposed by Senturk et al.,\nwhich distinguishes between Uniplex models, reinforcing cultural\nhomogenization, and Multiplex models, which integrate diverse perspectives.\nWorldView-Bench measures Cultural Polarization, the exclusion of alternative\nperspectives, through free-form generative evaluation rather than conventional\ncategorical benchmarks. We implement applied multiplexity through two\nintervention strategies: (1) Contextually-Implemented Multiplex LLMs, where\nsystem prompts embed multiplexity principles, and (2) Multi-Agent System\n(MAS)-Implemented Multiplex LLMs, where multiple LLM agents representing\ndistinct cultural perspectives collaboratively generate responses. Our results\ndemonstrate a significant increase in Perspectives Distribution Score (PDS)\nentropy from 13% at baseline to 94% with MAS-Implemented Multiplex LLMs,\nalongside a shift toward positive sentiment (67.7%) and enhanced cultural\nbalance. These findings highlight the potential of multiplex-aware AI\nevaluation in mitigating cultural bias in LLMs, paving the way for more\ninclusive and ethically aligned AI systems."
                },
                "authors": [
                    {
                        "name": "Abdullah Mushtaq"
                    },
                    {
                        "name": "Imran Taj"
                    },
                    {
                        "name": "Rafay Naeem"
                    },
                    {
                        "name": "Ibrahim Ghaznavi"
                    },
                    {
                        "name": "Junaid Qadir"
                    }
                ],
                "author_detail": {
                    "name": "Junaid Qadir"
                },
                "author": "Junaid Qadir",
                "arxiv_comment": "Preprint. Submitted to the Journal of Artificial Intelligence\n  Research (JAIR) on April 29, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09595v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09595v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09583v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09583v2",
                "updated": "2025-05-15T14:03:10Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    14,
                    3,
                    10,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-14T17:31:16Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    17,
                    31,
                    16,
                    2,
                    134,
                    0
                ],
                "title": "Beyond Likes: How Normative Feedback Complements Engagement Signals on\n  Social Media",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Likes: How Normative Feedback Complements Engagement Signals on\n  Social Media"
                },
                "summary": "Many online platforms incorporate engagement signals--such as likes and\nupvotes--into their content ranking systems and interface design. These signals\nare designed to boost user engagement. However, they can unintentionally\nelevate content that is less inclusive and may not support normatively\ndesirable behavior. This issue becomes especially concerning when toxic content\ncorrelates strongly with popularity indicators such as likes and upvotes. In\nthis study, we propose structured prosocial feedback as a complementary signal\nto likes and upvotes--one that highlights content quality based on normative\ncriteria to help address the limitations of conventional engagement signals. We\nbegin by designing and implementing a machine learning feedback system powered\nby a large language model (LLM), which evaluates user comments based on\nprinciples of positive psychology, such as individual well-being, constructive\nsocial media use, and character strengths. We then conduct a pre-registered\nuser study to examine how existing peer-based and the new expert-based feedback\ninteract to shape users' selection of comments in a social media setting.\nResults show that peer feedback increases conformity to popularity cues, while\nexpert feedback shifts preferences toward normatively higher-quality content.\nMoreover, incorporating expert feedback alongside peer evaluations improves\nalignment with expert assessments and contributes to a less toxic community\nenvironment. This illustrates the added value of normative cues--such as expert\nscores generated by LLMs using psychological rubrics--and underscores the\npotential benefits of incorporating such signals into platform feedback systems\nto foster healthier online environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many online platforms incorporate engagement signals--such as likes and\nupvotes--into their content ranking systems and interface design. These signals\nare designed to boost user engagement. However, they can unintentionally\nelevate content that is less inclusive and may not support normatively\ndesirable behavior. This issue becomes especially concerning when toxic content\ncorrelates strongly with popularity indicators such as likes and upvotes. In\nthis study, we propose structured prosocial feedback as a complementary signal\nto likes and upvotes--one that highlights content quality based on normative\ncriteria to help address the limitations of conventional engagement signals. We\nbegin by designing and implementing a machine learning feedback system powered\nby a large language model (LLM), which evaluates user comments based on\nprinciples of positive psychology, such as individual well-being, constructive\nsocial media use, and character strengths. We then conduct a pre-registered\nuser study to examine how existing peer-based and the new expert-based feedback\ninteract to shape users' selection of comments in a social media setting.\nResults show that peer feedback increases conformity to popularity cues, while\nexpert feedback shifts preferences toward normatively higher-quality content.\nMoreover, incorporating expert feedback alongside peer evaluations improves\nalignment with expert assessments and contributes to a less toxic community\nenvironment. This illustrates the added value of normative cues--such as expert\nscores generated by LLMs using psychological rubrics--and underscores the\npotential benefits of incorporating such signals into platform feedback systems\nto foster healthier online environments."
                },
                "authors": [
                    {
                        "name": "Yuchen Wu"
                    },
                    {
                        "name": "Mingduo Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Mingduo Zhao"
                },
                "author": "Mingduo Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09583v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09583v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09576v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09576v1",
                "updated": "2025-05-14T17:29:19Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    17,
                    29,
                    19,
                    2,
                    134,
                    0
                ],
                "published": "2025-05-14T17:29:19Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    17,
                    29,
                    19,
                    2,
                    134,
                    0
                ],
                "title": "Ethics and Persuasion in Reinforcement Learning from Human Feedback: A\n  Procedural Rhetorical Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ethics and Persuasion in Reinforcement Learning from Human Feedback: A\n  Procedural Rhetorical Approach"
                },
                "summary": "Since 2022, versions of generative AI chatbots such as ChatGPT and Claude\nhave been trained using a specialized technique called Reinforcement Learning\nfrom Human Feedback (RLHF) to fine-tune language model output using feedback\nfrom human annotators. As a result, the integration of RLHF has greatly\nenhanced the outputs of these large language models (LLMs) and made the\ninteractions and responses appear more \"human-like\" than those of previous\nversions using only supervised learning. The increasing convergence of human\nand machine-written text has potentially severe ethical, sociotechnical, and\npedagogical implications relating to transparency, trust, bias, and\ninterpersonal relations. To highlight these implications, this paper presents a\nrhetorical analysis of some of the central procedures and processes currently\nbeing reshaped by RLHF-enhanced generative AI chatbots: upholding language\nconventions, information seeking practices, and expectations for social\nrelationships. Rhetorical investigations of generative AI and LLMs have, to\nthis point, focused largely on the persuasiveness of the content generated.\nUsing Ian Bogost's concept of procedural rhetoric, this paper shifts the site\nof rhetorical investigation from content analysis to the underlying mechanisms\nof persuasion built into RLHF-enhanced LLMs. In doing so, this theoretical\ninvestigation opens a new direction for further inquiry in AI ethics that\nconsiders how procedures rerouted through AI-driven technologies might\nreinforce hegemonic language use, perpetuate biases, decontextualize learning,\nand encroach upon human relationships. It will therefore be of interest to\neducators, researchers, scholars, and the growing number of users of generative\nAI chatbots.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Since 2022, versions of generative AI chatbots such as ChatGPT and Claude\nhave been trained using a specialized technique called Reinforcement Learning\nfrom Human Feedback (RLHF) to fine-tune language model output using feedback\nfrom human annotators. As a result, the integration of RLHF has greatly\nenhanced the outputs of these large language models (LLMs) and made the\ninteractions and responses appear more \"human-like\" than those of previous\nversions using only supervised learning. The increasing convergence of human\nand machine-written text has potentially severe ethical, sociotechnical, and\npedagogical implications relating to transparency, trust, bias, and\ninterpersonal relations. To highlight these implications, this paper presents a\nrhetorical analysis of some of the central procedures and processes currently\nbeing reshaped by RLHF-enhanced generative AI chatbots: upholding language\nconventions, information seeking practices, and expectations for social\nrelationships. Rhetorical investigations of generative AI and LLMs have, to\nthis point, focused largely on the persuasiveness of the content generated.\nUsing Ian Bogost's concept of procedural rhetoric, this paper shifts the site\nof rhetorical investigation from content analysis to the underlying mechanisms\nof persuasion built into RLHF-enhanced LLMs. In doing so, this theoretical\ninvestigation opens a new direction for further inquiry in AI ethics that\nconsiders how procedures rerouted through AI-driven technologies might\nreinforce hegemonic language use, perpetuate biases, decontextualize learning,\nand encroach upon human relationships. It will therefore be of interest to\neducators, researchers, scholars, and the growing number of users of generative\nAI chatbots."
                },
                "authors": [
                    {
                        "name": "Shannon Lodoen"
                    },
                    {
                        "name": "Alexi Orchard"
                    }
                ],
                "author_detail": {
                    "name": "Alexi Orchard"
                },
                "author": "Alexi Orchard",
                "arxiv_comment": "10 pages, 1 figure, Accepted version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09576v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09576v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15507v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15507v3",
                "updated": "2025-05-14T17:25:36Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    17,
                    25,
                    36,
                    2,
                    134,
                    0
                ],
                "published": "2025-02-21T15:04:48Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    15,
                    4,
                    48,
                    4,
                    52,
                    0
                ],
                "title": "Activation Steering in Neural Theorem Provers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Activation Steering in Neural Theorem Provers"
                },
                "summary": "Large Language Models (LLMs) have shown promise in proving formal theorems\nusing proof assistants like Lean. However, current state of the art language\nmodels struggles to predict next step in proofs leading practitioners to use\ndifferent sampling techniques to improve LLMs capabilities. We observe that the\nLLM is capable of predicting the correct tactic; however, it faces challenges\nin ranking it appropriately within the set of candidate tactics, affecting the\noverall selection process. To overcome this hurdle, we use activation steering\nto guide LLMs responses to improve the generations at the time of inference.\nOur results suggest that activation steering offers a promising lightweight\nalternative to specialized fine-tuning for enhancing theorem proving\ncapabilities in LLMs, particularly valuable in resource-constrained\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown promise in proving formal theorems\nusing proof assistants like Lean. However, current state of the art language\nmodels struggles to predict next step in proofs leading practitioners to use\ndifferent sampling techniques to improve LLMs capabilities. We observe that the\nLLM is capable of predicting the correct tactic; however, it faces challenges\nin ranking it appropriately within the set of candidate tactics, affecting the\noverall selection process. To overcome this hurdle, we use activation steering\nto guide LLMs responses to improve the generations at the time of inference.\nOur results suggest that activation steering offers a promising lightweight\nalternative to specialized fine-tuning for enhancing theorem proving\ncapabilities in LLMs, particularly valuable in resource-constrained\nenvironments."
                },
                "authors": [
                    {
                        "name": "Shashank Kirtania"
                    }
                ],
                "author_detail": {
                    "name": "Shashank Kirtania"
                },
                "author": "Shashank Kirtania",
                "arxiv_comment": "incorrect explanation for a concept, need to revise and update!",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15507v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15507v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09569v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09569v1",
                "updated": "2025-05-14T17:11:23Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    17,
                    11,
                    23,
                    2,
                    134,
                    0
                ],
                "published": "2025-05-14T17:11:23Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    17,
                    11,
                    23,
                    2,
                    134,
                    0
                ],
                "title": "MIGRATION-BENCH: Repository-Level Code Migration Benchmark from Java 8",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MIGRATION-BENCH: Repository-Level Code Migration Benchmark from Java 8"
                },
                "summary": "With the rapid advancement of powerful large language models (LLMs) in recent\nyears, a wide range of software engineering tasks can now be addressed using\nLLMs, significantly enhancing productivity and scalability. Numerous benchmark\ndatasets have been developed to evaluate the coding capabilities of these\nmodels, while they primarily focus on problem-solving and issue-resolution\ntasks. In contrast, we introduce a new coding benchmark MIGRATION-BENCH with a\ndistinct focus: code migration. MIGRATION-BENCH aims to serve as a\ncomprehensive benchmark for migration from Java 8 to the latest long-term\nsupport (LTS) versions (Java 17, 21), MIGRATION-BENCH includes a full dataset\nand its subset selected with $5,102$ and $300$ repositories respectively.\nSelected is a representative subset curated for complexity and difficulty,\noffering a versatile resource to support research in the field of code\nmigration. Additionally, we provide a comprehensive evaluation framework to\nfacilitate rigorous and standardized assessment of LLMs on this challenging\ntask. We further propose SD-Feedback and demonstrate that LLMs can effectively\ntackle repository-level code migration to Java 17. For the selected subset with\nClaude-3.5-Sonnet-v2, SD-Feedback achieves 62.33% and 27.00% success rate\n(pass@1) for minimal and maximal migration respectively. The benchmark dataset\nand source code are available at:\nhttps://huggingface.co/collections/AmazonScience and\nhttps://github.com/amazon-science/self_debug respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid advancement of powerful large language models (LLMs) in recent\nyears, a wide range of software engineering tasks can now be addressed using\nLLMs, significantly enhancing productivity and scalability. Numerous benchmark\ndatasets have been developed to evaluate the coding capabilities of these\nmodels, while they primarily focus on problem-solving and issue-resolution\ntasks. In contrast, we introduce a new coding benchmark MIGRATION-BENCH with a\ndistinct focus: code migration. MIGRATION-BENCH aims to serve as a\ncomprehensive benchmark for migration from Java 8 to the latest long-term\nsupport (LTS) versions (Java 17, 21), MIGRATION-BENCH includes a full dataset\nand its subset selected with $5,102$ and $300$ repositories respectively.\nSelected is a representative subset curated for complexity and difficulty,\noffering a versatile resource to support research in the field of code\nmigration. Additionally, we provide a comprehensive evaluation framework to\nfacilitate rigorous and standardized assessment of LLMs on this challenging\ntask. We further propose SD-Feedback and demonstrate that LLMs can effectively\ntackle repository-level code migration to Java 17. For the selected subset with\nClaude-3.5-Sonnet-v2, SD-Feedback achieves 62.33% and 27.00% success rate\n(pass@1) for minimal and maximal migration respectively. The benchmark dataset\nand source code are available at:\nhttps://huggingface.co/collections/AmazonScience and\nhttps://github.com/amazon-science/self_debug respectively."
                },
                "authors": [
                    {
                        "name": "Linbo Liu"
                    },
                    {
                        "name": "Xinle Liu"
                    },
                    {
                        "name": "Qiang Zhou"
                    },
                    {
                        "name": "Lin Chen"
                    },
                    {
                        "name": "Yihan Liu"
                    },
                    {
                        "name": "Hoan Nguyen"
                    },
                    {
                        "name": "Behrooz Omidvar-Tehrani"
                    },
                    {
                        "name": "Xi Shen"
                    },
                    {
                        "name": "Jun Huan"
                    },
                    {
                        "name": "Omer Tripp"
                    },
                    {
                        "name": "Anoop Deoras"
                    }
                ],
                "author_detail": {
                    "name": "Anoop Deoras"
                },
                "author": "Anoop Deoras",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09569v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09569v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01618v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01618v2",
                "updated": "2025-05-14T17:09:58Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    17,
                    9,
                    58,
                    2,
                    134,
                    0
                ],
                "published": "2025-05-02T22:45:14Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    22,
                    45,
                    14,
                    4,
                    122,
                    0
                ],
                "title": "Don't be lazy: CompleteP enables compute-efficient deep transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Don't be lazy: CompleteP enables compute-efficient deep transformers"
                },
                "summary": "We study compute efficiency of LLM training when using different\nparameterizations, i.e., rules for adjusting model and optimizer\nhyperparameters (HPs) as model size changes. Some parameterizations fail to\ntransfer optimal base HPs (such as learning rate) across changes in model\ndepth, requiring practitioners to either re-tune these HPs as they scale up\n(expensive), or accept sub-optimal training when re-tuning is prohibitive. Even\nwhen they achieve HP transfer, we develop theory to show parameterizations may\nstill exist in the lazy learning regime where layers learn only features close\nto their linearization, preventing effective use of depth and nonlinearity.\nFinally, we identify and adopt the parameterization we call CompleteP that\nachieves both depth-wise HP transfer and non-lazy learning in all layers.\nCompleteP enables a wider range of model width/depth ratios to remain\ncompute-efficient, unlocking shapes better suited for different hardware\nsettings and operational contexts. Moreover, CompleteP enables 12-34% compute\nefficiency improvements over the prior state-of-the-art.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study compute efficiency of LLM training when using different\nparameterizations, i.e., rules for adjusting model and optimizer\nhyperparameters (HPs) as model size changes. Some parameterizations fail to\ntransfer optimal base HPs (such as learning rate) across changes in model\ndepth, requiring practitioners to either re-tune these HPs as they scale up\n(expensive), or accept sub-optimal training when re-tuning is prohibitive. Even\nwhen they achieve HP transfer, we develop theory to show parameterizations may\nstill exist in the lazy learning regime where layers learn only features close\nto their linearization, preventing effective use of depth and nonlinearity.\nFinally, we identify and adopt the parameterization we call CompleteP that\nachieves both depth-wise HP transfer and non-lazy learning in all layers.\nCompleteP enables a wider range of model width/depth ratios to remain\ncompute-efficient, unlocking shapes better suited for different hardware\nsettings and operational contexts. Moreover, CompleteP enables 12-34% compute\nefficiency improvements over the prior state-of-the-art."
                },
                "authors": [
                    {
                        "name": "Nolan Dey"
                    },
                    {
                        "name": "Bin Claire Zhang"
                    },
                    {
                        "name": "Lorenzo Noci"
                    },
                    {
                        "name": "Mufan Li"
                    },
                    {
                        "name": "Blake Bordelon"
                    },
                    {
                        "name": "Shane Bergsma"
                    },
                    {
                        "name": "Cengiz Pehlevan"
                    },
                    {
                        "name": "Boris Hanin"
                    },
                    {
                        "name": "Joel Hestness"
                    }
                ],
                "author_detail": {
                    "name": "Joel Hestness"
                },
                "author": "Joel Hestness",
                "arxiv_comment": "10 main pages, 16 appendix pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01618v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01618v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21813v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21813v3",
                "updated": "2025-05-14T16:57:13Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    16,
                    57,
                    13,
                    2,
                    134,
                    0
                ],
                "published": "2025-03-25T18:20:04Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    18,
                    20,
                    4,
                    1,
                    84,
                    0
                ],
                "title": "OAEI-LLM-T: A TBox Benchmark Dataset for Understanding Large Language\n  Model Hallucinations in Ontology Matching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OAEI-LLM-T: A TBox Benchmark Dataset for Understanding Large Language\n  Model Hallucinations in Ontology Matching"
                },
                "summary": "Hallucinations are often inevitable in downstream tasks using large language\nmodels (LLMs). To tackle the substantial challenge of addressing hallucinations\nfor LLM-based ontology matching (OM) systems, we introduce a new benchmark\ndataset OAEI-LLM-T. The dataset evolves from seven TBox datasets in the\nOntology Alignment Evaluation Initiative (OAEI), capturing hallucinations of\nten different LLMs performing OM tasks. These OM-specific hallucinations are\norganised into two primary categories and six sub-categories. We showcase the\nusefulness of the dataset in constructing an LLM leaderboard for OM tasks and\nfor fine-tuning LLMs used in OM tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hallucinations are often inevitable in downstream tasks using large language\nmodels (LLMs). To tackle the substantial challenge of addressing hallucinations\nfor LLM-based ontology matching (OM) systems, we introduce a new benchmark\ndataset OAEI-LLM-T. The dataset evolves from seven TBox datasets in the\nOntology Alignment Evaluation Initiative (OAEI), capturing hallucinations of\nten different LLMs performing OM tasks. These OM-specific hallucinations are\norganised into two primary categories and six sub-categories. We showcase the\nusefulness of the dataset in constructing an LLM leaderboard for OM tasks and\nfor fine-tuning LLMs used in OM tasks."
                },
                "authors": [
                    {
                        "name": "Zhangcheng Qiang"
                    },
                    {
                        "name": "Kerry Taylor"
                    },
                    {
                        "name": "Weiqing Wang"
                    },
                    {
                        "name": "Jing Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Jing Jiang"
                },
                "author": "Jing Jiang",
                "arxiv_comment": "14 pages, 4 figures, 4 tables, 2 prompt templates",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21813v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21813v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.06738v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.06738v2",
                "updated": "2025-05-14T16:04:57Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    16,
                    4,
                    57,
                    2,
                    134,
                    0
                ],
                "published": "2025-05-10T19:06:37Z",
                "published_parsed": [
                    2025,
                    5,
                    10,
                    19,
                    6,
                    37,
                    5,
                    130,
                    0
                ],
                "title": "I Know What You Said: Unveiling Hardware Cache Side-Channels in Local\n  Large Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "I Know What You Said: Unveiling Hardware Cache Side-Channels in Local\n  Large Language Model Inference"
                },
                "summary": "Large Language Models (LLMs) that can be deployed locally have recently\ngained popularity for privacy-sensitive tasks, with companies such as Meta,\nGoogle, and Intel playing significant roles in their development. However, the\nsecurity of local LLMs through the lens of hardware cache side-channels remains\nunexplored. In this paper, we unveil novel side-channel vulnerabilities in\nlocal LLM inference: token value and token position leakage, which can expose\nboth the victim's input and output text, thereby compromising user privacy.\nSpecifically, we found that adversaries can infer the token values from the\ncache access patterns of the token embedding operation, and deduce the token\npositions from the timing of autoregressive decoding phases. To demonstrate the\npotential of these leaks, we design a novel eavesdropping attack framework\ntargeting both open-source and proprietary LLM inference systems. The attack\nframework does not directly interact with the victim's LLM and can be executed\nwithout privilege.\n  We evaluate the attack on a range of practical local LLM deployments (e.g.,\nLlama, Falcon, and Gemma), and the results show that our attack achieves\npromising accuracy. The restored output and input text have an average edit\ndistance of 5.2% and 17.3% to the ground truth, respectively. Furthermore, the\nreconstructed texts achieve average cosine similarity scores of 98.7% (input)\nand 98.0% (output).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) that can be deployed locally have recently\ngained popularity for privacy-sensitive tasks, with companies such as Meta,\nGoogle, and Intel playing significant roles in their development. However, the\nsecurity of local LLMs through the lens of hardware cache side-channels remains\nunexplored. In this paper, we unveil novel side-channel vulnerabilities in\nlocal LLM inference: token value and token position leakage, which can expose\nboth the victim's input and output text, thereby compromising user privacy.\nSpecifically, we found that adversaries can infer the token values from the\ncache access patterns of the token embedding operation, and deduce the token\npositions from the timing of autoregressive decoding phases. To demonstrate the\npotential of these leaks, we design a novel eavesdropping attack framework\ntargeting both open-source and proprietary LLM inference systems. The attack\nframework does not directly interact with the victim's LLM and can be executed\nwithout privilege.\n  We evaluate the attack on a range of practical local LLM deployments (e.g.,\nLlama, Falcon, and Gemma), and the results show that our attack achieves\npromising accuracy. The restored output and input text have an average edit\ndistance of 5.2% and 17.3% to the ground truth, respectively. Furthermore, the\nreconstructed texts achieve average cosine similarity scores of 98.7% (input)\nand 98.0% (output)."
                },
                "authors": [
                    {
                        "name": "Zibo Gao"
                    },
                    {
                        "name": "Junjie Hu"
                    },
                    {
                        "name": "Feng Guo"
                    },
                    {
                        "name": "Yixin Zhang"
                    },
                    {
                        "name": "Yinglong Han"
                    },
                    {
                        "name": "Siyuan Liu"
                    },
                    {
                        "name": "Haiyang Li"
                    },
                    {
                        "name": "Zhiqiang Lv"
                    }
                ],
                "author_detail": {
                    "name": "Zhiqiang Lv"
                },
                "author": "Zhiqiang Lv",
                "arxiv_comment": "Submitted for review in January 22, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.06738v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.06738v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "K.6.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18769v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18769v5",
                "updated": "2025-05-14T16:01:10Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    16,
                    1,
                    10,
                    2,
                    134,
                    0
                ],
                "published": "2024-09-27T14:14:16Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    14,
                    14,
                    16,
                    4,
                    271,
                    0
                ],
                "title": "State-of-the-Art Periorbital Distance Prediction and Disease\n  Classification Using Periorbital Features",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "State-of-the-Art Periorbital Distance Prediction and Disease\n  Classification Using Periorbital Features"
                },
                "summary": "Periorbital distances are critical markers for diagnosing and monitoring a\nrange of oculoplastic and craniofacial conditions. Manual measurement, however,\nis subjective and prone to intergrader variability. Automated methods have been\ndeveloped but remain limited by standardized imaging requirements, small\ndatasets, and a narrow focus on individual measurements. We developed a\nsegmentation pipeline trained on a domain-specific dataset of healthy eyes and\ncompared its performance against the Segment Anything Model (SAM) and the prior\nbenchmark, PeriorbitAI. Segmentation accuracy was evaluated across multiple\ndisease classes and imaging conditions. We further investigated the use of\npredicted periorbital distances as features for disease classification under\nin-distribution (ID) and out-of-distribution (OOD) settings, comparing shallow\nclassifiers, CNNs, and fusion models. Our segmentation model achieved\nstate-of-the-art accuracy across all datasets, with error rates within\nintergrader variability and superior performance relative to SAM and\nPeriorbitAI. In classification tasks, models trained on periorbital distances\nmatched CNN performance on ID data (77--78\\% accuracy) and substantially\noutperformed CNNs under OOD conditions (63--68\\% accuracy vs. 14\\%). Fusion\nmodels achieved the highest ID accuracy (80\\%) but were sensitive to degraded\nCNN features under OOD shifts. Segmentation-derived periorbital distances\nprovide robust, explainable features for disease classification and generalize\nbetter under domain shift than CNN image classifiers. These results establish a\nnew benchmark for periorbital distance prediction and highlight the potential\nof anatomy-based AI pipelines for real-world deployment in oculoplastic and\ncraniofacial care.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Periorbital distances are critical markers for diagnosing and monitoring a\nrange of oculoplastic and craniofacial conditions. Manual measurement, however,\nis subjective and prone to intergrader variability. Automated methods have been\ndeveloped but remain limited by standardized imaging requirements, small\ndatasets, and a narrow focus on individual measurements. We developed a\nsegmentation pipeline trained on a domain-specific dataset of healthy eyes and\ncompared its performance against the Segment Anything Model (SAM) and the prior\nbenchmark, PeriorbitAI. Segmentation accuracy was evaluated across multiple\ndisease classes and imaging conditions. We further investigated the use of\npredicted periorbital distances as features for disease classification under\nin-distribution (ID) and out-of-distribution (OOD) settings, comparing shallow\nclassifiers, CNNs, and fusion models. Our segmentation model achieved\nstate-of-the-art accuracy across all datasets, with error rates within\nintergrader variability and superior performance relative to SAM and\nPeriorbitAI. In classification tasks, models trained on periorbital distances\nmatched CNN performance on ID data (77--78\\% accuracy) and substantially\noutperformed CNNs under OOD conditions (63--68\\% accuracy vs. 14\\%). Fusion\nmodels achieved the highest ID accuracy (80\\%) but were sensitive to degraded\nCNN features under OOD shifts. Segmentation-derived periorbital distances\nprovide robust, explainable features for disease classification and generalize\nbetter under domain shift than CNN image classifiers. These results establish a\nnew benchmark for periorbital distance prediction and highlight the potential\nof anatomy-based AI pipelines for real-world deployment in oculoplastic and\ncraniofacial care."
                },
                "authors": [
                    {
                        "name": "George R. Nahass"
                    },
                    {
                        "name": "Sasha Hubschman"
                    },
                    {
                        "name": "Jeffrey C. Peterson"
                    },
                    {
                        "name": "Ghasem Yazdanpanah"
                    },
                    {
                        "name": "Nicholas Tomaras"
                    },
                    {
                        "name": "Madison Cheung"
                    },
                    {
                        "name": "Alex Palacios"
                    },
                    {
                        "name": "Kevin Heinze"
                    },
                    {
                        "name": "Chad A. Purnell"
                    },
                    {
                        "name": "Pete Setabutr"
                    },
                    {
                        "name": "Ann Q. Tran"
                    },
                    {
                        "name": "Darvin Yi"
                    }
                ],
                "author_detail": {
                    "name": "Darvin Yi"
                },
                "author": "Darvin Yi",
                "arxiv_comment": "25 pages, 12 figures, 16 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18769v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18769v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09500v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09500v1",
                "updated": "2025-05-14T15:50:45Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    15,
                    50,
                    45,
                    2,
                    134,
                    0
                ],
                "published": "2025-05-14T15:50:45Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    15,
                    50,
                    45,
                    2,
                    134,
                    0
                ],
                "title": "Layered Unlearning for Adversarial Relearning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Layered Unlearning for Adversarial Relearning"
                },
                "summary": "Our goal is to understand how post-training methods, such as fine-tuning,\nalignment, and unlearning, modify language model behavior and representations.\nWe are particularly interested in the brittle nature of these modifications\nthat makes them easy to bypass through prompt engineering or relearning. Recent\nresults suggest that post-training induces shallow context-dependent\n``circuits'' that suppress specific response patterns. This could be one\nexplanation for the brittleness of post-training. To test this hypothesis, we\ndesign an unlearning algorithm, Layered Unlearning (LU), that creates distinct\ninhibitory mechanisms for a growing subset of the data. By unlearning the first\n$i$ folds while retaining the remaining $k - i$ at the $i$th of $k$ stages, LU\nlimits the ability of relearning on a subset of data to recover the full\ndataset. We evaluate LU through a combination of synthetic and large language\nmodel (LLM) experiments. We find that LU improves robustness to adversarial\nrelearning for several different unlearning methods. Our results contribute to\nthe state-of-the-art of machine unlearning and provide insight into the effect\nof post-training updates.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Our goal is to understand how post-training methods, such as fine-tuning,\nalignment, and unlearning, modify language model behavior and representations.\nWe are particularly interested in the brittle nature of these modifications\nthat makes them easy to bypass through prompt engineering or relearning. Recent\nresults suggest that post-training induces shallow context-dependent\n``circuits'' that suppress specific response patterns. This could be one\nexplanation for the brittleness of post-training. To test this hypothesis, we\ndesign an unlearning algorithm, Layered Unlearning (LU), that creates distinct\ninhibitory mechanisms for a growing subset of the data. By unlearning the first\n$i$ folds while retaining the remaining $k - i$ at the $i$th of $k$ stages, LU\nlimits the ability of relearning on a subset of data to recover the full\ndataset. We evaluate LU through a combination of synthetic and large language\nmodel (LLM) experiments. We find that LU improves robustness to adversarial\nrelearning for several different unlearning methods. Our results contribute to\nthe state-of-the-art of machine unlearning and provide insight into the effect\nof post-training updates."
                },
                "authors": [
                    {
                        "name": "Timothy Qian"
                    },
                    {
                        "name": "Vinith Suriyakumar"
                    },
                    {
                        "name": "Ashia Wilson"
                    },
                    {
                        "name": "Dylan Hadfield-Menell"
                    }
                ],
                "author_detail": {
                    "name": "Dylan Hadfield-Menell"
                },
                "author": "Dylan Hadfield-Menell",
                "arxiv_comment": "37 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09500v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09500v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09498v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09498v1",
                "updated": "2025-05-14T15:45:17Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    15,
                    45,
                    17,
                    2,
                    134,
                    0
                ],
                "published": "2025-05-14T15:45:17Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    15,
                    45,
                    17,
                    2,
                    134,
                    0
                ],
                "title": "Flash-VL 2B: Optimizing Vision-Language Model Performance for Ultra-Low\n  Latency and High Throughput",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flash-VL 2B: Optimizing Vision-Language Model Performance for Ultra-Low\n  Latency and High Throughput"
                },
                "summary": "In this paper, we introduce Flash-VL 2B, a novel approach to optimizing\nVision-Language Models (VLMs) for real-time applications, targeting ultra-low\nlatency and high throughput without sacrificing accuracy. Leveraging advanced\narchitectural enhancements and efficient computational strategies, Flash-VL 2B\nis designed to maximize throughput by reducing processing time while\nmaintaining competitive performance across multiple vision-language benchmarks.\nOur approach includes tailored architectural choices, token compression\nmechanisms, data curation, training schemes, and a novel image processing\ntechnique called implicit semantic stitching that effectively balances\ncomputational load and model performance. Through extensive evaluations on 11\nstandard VLM benchmarks, we demonstrate that Flash-VL 2B achieves\nstate-of-the-art results in both speed and accuracy, making it a promising\nsolution for deployment in resource-constrained environments and large-scale\nreal-time applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce Flash-VL 2B, a novel approach to optimizing\nVision-Language Models (VLMs) for real-time applications, targeting ultra-low\nlatency and high throughput without sacrificing accuracy. Leveraging advanced\narchitectural enhancements and efficient computational strategies, Flash-VL 2B\nis designed to maximize throughput by reducing processing time while\nmaintaining competitive performance across multiple vision-language benchmarks.\nOur approach includes tailored architectural choices, token compression\nmechanisms, data curation, training schemes, and a novel image processing\ntechnique called implicit semantic stitching that effectively balances\ncomputational load and model performance. Through extensive evaluations on 11\nstandard VLM benchmarks, we demonstrate that Flash-VL 2B achieves\nstate-of-the-art results in both speed and accuracy, making it a promising\nsolution for deployment in resource-constrained environments and large-scale\nreal-time applications."
                },
                "authors": [
                    {
                        "name": "Bo Zhang"
                    },
                    {
                        "name": "Shuo Li"
                    },
                    {
                        "name": "Runhe Tian"
                    },
                    {
                        "name": "Yang Yang"
                    },
                    {
                        "name": "Jixin Tang"
                    },
                    {
                        "name": "Jinhao Zhou"
                    },
                    {
                        "name": "Lin Ma"
                    }
                ],
                "author_detail": {
                    "name": "Lin Ma"
                },
                "author": "Lin Ma",
                "arxiv_comment": "18 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09498v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09498v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09478v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09478v1",
                "updated": "2025-05-14T15:29:15Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    15,
                    29,
                    15,
                    2,
                    134,
                    0
                ],
                "published": "2025-05-14T15:29:15Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    15,
                    29,
                    15,
                    2,
                    134,
                    0
                ],
                "title": "Card Sorting Simulator: Augmenting Design of Logical Information\n  Architectures with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Card Sorting Simulator: Augmenting Design of Logical Information\n  Architectures with Large Language Models"
                },
                "summary": "Card sorting is a common ideation technique that elicits information on\nusers' mental organization of content and functionality by having them sort\nitems into categories. For more robust card sorting research, digital card\nsorting tools could benefit from providing quick automated feedback. Our\nobjective of this research is to advance toward an instrument that applies\nartificial intelligence (AI) to augment card sorting. For this purpose, we\ndevelop the Card Sorting Simulator, a prototype tool that leverages Large\nLanguage Models (LLMs) to generate informative categorizations of cards. To\nilluminate how aligned the simulation is with card sorting by actual\nparticipants, and to inform the instrument's design decisions, we conducted a\ngeneralizability-focused comparative study. We obtained 28 pre-existing card\nsorting studies from real practitioners, comprising 1,399 participants, along\nwith diverse contents and origins. With this dataset, we conducted a\ncomprehensive and nuanced analysis of the agreement between actual card sorting\nresults (clusterings of cards) and synthetic clusterings across a multitude of\nLLMs and prompt designs. Mutual information scores indicate a good degree of\nagreement to real result clustering, although similarity matrices also\ndemonstrate inconsistencies from mental models, which can be attributed to\ntheir top-down nature. Furthermore, the number of cards or complexity of their\nlabels impact the accuracy of its simulation. These findings bolster the case\nfor AI augmentation in card sorting research as a source of meaningful\npreliminary feedback and highlight the need for further study for the\ndevelopment and validation of intelligent user research tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Card sorting is a common ideation technique that elicits information on\nusers' mental organization of content and functionality by having them sort\nitems into categories. For more robust card sorting research, digital card\nsorting tools could benefit from providing quick automated feedback. Our\nobjective of this research is to advance toward an instrument that applies\nartificial intelligence (AI) to augment card sorting. For this purpose, we\ndevelop the Card Sorting Simulator, a prototype tool that leverages Large\nLanguage Models (LLMs) to generate informative categorizations of cards. To\nilluminate how aligned the simulation is with card sorting by actual\nparticipants, and to inform the instrument's design decisions, we conducted a\ngeneralizability-focused comparative study. We obtained 28 pre-existing card\nsorting studies from real practitioners, comprising 1,399 participants, along\nwith diverse contents and origins. With this dataset, we conducted a\ncomprehensive and nuanced analysis of the agreement between actual card sorting\nresults (clusterings of cards) and synthetic clusterings across a multitude of\nLLMs and prompt designs. Mutual information scores indicate a good degree of\nagreement to real result clustering, although similarity matrices also\ndemonstrate inconsistencies from mental models, which can be attributed to\ntheir top-down nature. Furthermore, the number of cards or complexity of their\nlabels impact the accuracy of its simulation. These findings bolster the case\nfor AI augmentation in card sorting research as a source of meaningful\npreliminary feedback and highlight the need for further study for the\ndevelopment and validation of intelligent user research tools."
                },
                "authors": [
                    {
                        "name": "Eduard Kuric"
                    },
                    {
                        "name": "Peter Demcak"
                    },
                    {
                        "name": "Matus Krajcovic"
                    }
                ],
                "author_detail": {
                    "name": "Matus Krajcovic"
                },
                "author": "Matus Krajcovic",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09478v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09478v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09477v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09477v1",
                "updated": "2025-05-14T15:28:43Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    15,
                    28,
                    43,
                    2,
                    134,
                    0
                ],
                "published": "2025-05-14T15:28:43Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    15,
                    28,
                    43,
                    2,
                    134,
                    0
                ],
                "title": "Deploying Foundation Model-Enabled Air and Ground Robots in the Field:\n  Challenges and Opportunities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying Foundation Model-Enabled Air and Ground Robots in the Field:\n  Challenges and Opportunities"
                },
                "summary": "The integration of foundation models (FMs) into robotics has enabled robots\nto understand natural language and reason about the semantics in their\nenvironments. However, existing FM-enabled robots primary operate in\nclosed-world settings, where the robot is given a full prior map or has a full\nview of its workspace. This paper addresses the deployment of FM-enabled robots\nin the field, where missions often require a robot to operate in large-scale\nand unstructured environments. To effectively accomplish these missions, robots\nmust actively explore their environments, navigate obstacle-cluttered terrain,\nhandle unexpected sensor inputs, and operate with compute constraints. We\ndiscuss recent deployments of SPINE, our LLM-enabled autonomy framework, in\nfield robotic settings. To the best of our knowledge, we present the first\ndemonstration of large-scale LLM-enabled robot planning in unstructured\nenvironments with several kilometers of missions. SPINE is agnostic to a\nparticular LLM, which allows us to distill small language models capable of\nrunning onboard size, weight and power (SWaP) limited platforms. Via\npreliminary model distillation work, we then present the first language-driven\nUAV planner using on-device language models. We conclude our paper by proposing\nseveral promising directions for future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of foundation models (FMs) into robotics has enabled robots\nto understand natural language and reason about the semantics in their\nenvironments. However, existing FM-enabled robots primary operate in\nclosed-world settings, where the robot is given a full prior map or has a full\nview of its workspace. This paper addresses the deployment of FM-enabled robots\nin the field, where missions often require a robot to operate in large-scale\nand unstructured environments. To effectively accomplish these missions, robots\nmust actively explore their environments, navigate obstacle-cluttered terrain,\nhandle unexpected sensor inputs, and operate with compute constraints. We\ndiscuss recent deployments of SPINE, our LLM-enabled autonomy framework, in\nfield robotic settings. To the best of our knowledge, we present the first\ndemonstration of large-scale LLM-enabled robot planning in unstructured\nenvironments with several kilometers of missions. SPINE is agnostic to a\nparticular LLM, which allows us to distill small language models capable of\nrunning onboard size, weight and power (SWaP) limited platforms. Via\npreliminary model distillation work, we then present the first language-driven\nUAV planner using on-device language models. We conclude our paper by proposing\nseveral promising directions for future research."
                },
                "authors": [
                    {
                        "name": "Zachary Ravichandran"
                    },
                    {
                        "name": "Fernando Cladera"
                    },
                    {
                        "name": "Jason Hughes"
                    },
                    {
                        "name": "Varun Murali"
                    },
                    {
                        "name": "M. Ani Hsieh"
                    },
                    {
                        "name": "George J. Pappas"
                    },
                    {
                        "name": "Camillo J. Taylor"
                    },
                    {
                        "name": "Vijay Kumar"
                    }
                ],
                "author_detail": {
                    "name": "Vijay Kumar"
                },
                "author": "Vijay Kumar",
                "arxiv_comment": "Accepted to the IEEE ICRA Workshop on Field Robotics 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09477v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09477v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11523v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11523v2",
                "updated": "2025-05-14T15:19:54Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    15,
                    19,
                    54,
                    2,
                    134,
                    0
                ],
                "published": "2024-12-16T07:59:23Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    7,
                    59,
                    23,
                    0,
                    351,
                    0
                ],
                "title": "ON as ALC: Active Loop Closing Object Goal Navigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ON as ALC: Active Loop Closing Object Goal Navigation"
                },
                "summary": "In simultaneous localization and mapping, active loop closing (ALC) is an\nactive vision problem that aims to visually guide a robot to maximize the\nchances of revisiting previously visited points, thereby resetting the drift\nerrors accumulated in the incrementally built map during travel. However,\ncurrent mainstream navigation strategies that leverage such incomplete maps as\nworkspace prior knowledge often fail in modern long-term autonomy long-distance\ntravel scenarios where map accumulation errors become significant. To address\nthese limitations of map-based navigation, this paper is the first to explore\nmapless navigation in the embodied AI field, in particular, to utilize\nobject-goal navigation (commonly abbreviated as ON, ObjNav, or OGN) techniques\nthat efficiently explore target objects without using such a prior map.\nSpecifically, in this work, we start from an off-the-shelf mapless ON planner,\nextend it to utilize a prior map, and further show that the performance in\nlong-distance ALC (LD-ALC) can be maximized by minimizing ``ALC loss\" and ``ON\nloss\". This study highlights a simple and effective approach, called ALC-ON\n(ALCON), to accelerate the progress of challenging long-distance ALC technology\nby leveraging the growing frontier-guided, data-driven, and LLM-guided ON\ntechnologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In simultaneous localization and mapping, active loop closing (ALC) is an\nactive vision problem that aims to visually guide a robot to maximize the\nchances of revisiting previously visited points, thereby resetting the drift\nerrors accumulated in the incrementally built map during travel. However,\ncurrent mainstream navigation strategies that leverage such incomplete maps as\nworkspace prior knowledge often fail in modern long-term autonomy long-distance\ntravel scenarios where map accumulation errors become significant. To address\nthese limitations of map-based navigation, this paper is the first to explore\nmapless navigation in the embodied AI field, in particular, to utilize\nobject-goal navigation (commonly abbreviated as ON, ObjNav, or OGN) techniques\nthat efficiently explore target objects without using such a prior map.\nSpecifically, in this work, we start from an off-the-shelf mapless ON planner,\nextend it to utilize a prior map, and further show that the performance in\nlong-distance ALC (LD-ALC) can be maximized by minimizing ``ALC loss\" and ``ON\nloss\". This study highlights a simple and effective approach, called ALC-ON\n(ALCON), to accelerate the progress of challenging long-distance ALC technology\nby leveraging the growing frontier-guided, data-driven, and LLM-guided ON\ntechnologies."
                },
                "authors": [
                    {
                        "name": "Daiki Iwata"
                    },
                    {
                        "name": "Kanji Tanaka"
                    },
                    {
                        "name": "Shoya Miyazaki"
                    },
                    {
                        "name": "Kouki Terashima"
                    }
                ],
                "author_detail": {
                    "name": "Kouki Terashima"
                },
                "author": "Kouki Terashima",
                "arxiv_comment": "Draft version of a conference paper with 7 pages, 5 figures, and 1\n  table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11523v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11523v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09439v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09439v1",
                "updated": "2025-05-14T14:47:16Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    14,
                    47,
                    16,
                    2,
                    134,
                    0
                ],
                "published": "2025-05-14T14:47:16Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    14,
                    47,
                    16,
                    2,
                    134,
                    0
                ],
                "title": "Omni-R1: Do You Really Need Audio to Fine-Tune Your Audio LLM?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Omni-R1: Do You Really Need Audio to Fine-Tune Your Audio LLM?"
                },
                "summary": "We propose Omni-R1 which fine-tunes a recent multi-modal LLM, Qwen2.5-Omni,\non an audio question answering dataset with the reinforcement learning method\nGRPO. This leads to new State-of-the-Art performance on the recent MMAU\nbenchmark. Omni-R1 achieves the highest accuracies on the sounds, music,\nspeech, and overall average categories, both on the Test-mini and Test-full\nsplits. To understand the performance improvement, we tested models both with\nand without audio and found that much of the performance improvement from GRPO\ncould be attributed to better text-based reasoning. We also made a surprising\ndiscovery that fine-tuning without audio on a text-only dataset was effective\nat improving the audio-based performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose Omni-R1 which fine-tunes a recent multi-modal LLM, Qwen2.5-Omni,\non an audio question answering dataset with the reinforcement learning method\nGRPO. This leads to new State-of-the-Art performance on the recent MMAU\nbenchmark. Omni-R1 achieves the highest accuracies on the sounds, music,\nspeech, and overall average categories, both on the Test-mini and Test-full\nsplits. To understand the performance improvement, we tested models both with\nand without audio and found that much of the performance improvement from GRPO\ncould be attributed to better text-based reasoning. We also made a surprising\ndiscovery that fine-tuning without audio on a text-only dataset was effective\nat improving the audio-based performance."
                },
                "authors": [
                    {
                        "name": "Andrew Rouditchenko"
                    },
                    {
                        "name": "Saurabhchand Bhati"
                    },
                    {
                        "name": "Edson Araujo"
                    },
                    {
                        "name": "Samuel Thomas"
                    },
                    {
                        "name": "Hilde Kuehne"
                    },
                    {
                        "name": "Rogerio Feris"
                    },
                    {
                        "name": "James Glass"
                    }
                ],
                "author_detail": {
                    "name": "James Glass"
                },
                "author": "James Glass",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09439v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09439v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09438v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09438v1",
                "updated": "2025-05-14T14:46:32Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    14,
                    46,
                    32,
                    2,
                    134,
                    0
                ],
                "published": "2025-05-14T14:46:32Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    14,
                    46,
                    32,
                    2,
                    134,
                    0
                ],
                "title": "Evaluating GPT- and Reasoning-based Large Language Models on Physics\n  Olympiad Problems: Surpassing Human Performance and Implications for\n  Educational Assessment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating GPT- and Reasoning-based Large Language Models on Physics\n  Olympiad Problems: Surpassing Human Performance and Implications for\n  Educational Assessment"
                },
                "summary": "Large language models (LLMs) are now widely accessible, reaching learners at\nall educational levels. This development has raised concerns that their use may\ncircumvent essential learning processes and compromise the integrity of\nestablished assessment formats. In physics education, where problem solving\nplays a central role in instruction and assessment, it is therefore essential\nto understand the physics-specific problem-solving capabilities of LLMs. Such\nunderstanding is key to informing responsible and pedagogically sound\napproaches to integrating LLMs into instruction and assessment. This study\ntherefore compares the problem-solving performance of a general-purpose LLM\n(GPT-4o, using varying prompting techniques) and a reasoning-optimized model\n(o1-preview) with that of participants of the German Physics Olympiad, based on\na set of well-defined Olympiad problems. In addition to evaluating the\ncorrectness of the generated solutions, the study analyzes characteristic\nstrengths and limitations of LLM-generated solutions. The findings of this\nstudy indicate that both tested LLMs (GPT-4o and o1-preview) demonstrate\nadvanced problem-solving capabilities on Olympiad-type physics problems, on\naverage outperforming the human participants. Prompting techniques had little\neffect on GPT-4o's performance, while o1-preview almost consistently\noutperformed both GPT-4o and the human benchmark. Based on these findings, the\nstudy discusses implications for the design of summative and formative\nassessment in physics education, including how to uphold assessment integrity\nand support students in critically engaging with LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are now widely accessible, reaching learners at\nall educational levels. This development has raised concerns that their use may\ncircumvent essential learning processes and compromise the integrity of\nestablished assessment formats. In physics education, where problem solving\nplays a central role in instruction and assessment, it is therefore essential\nto understand the physics-specific problem-solving capabilities of LLMs. Such\nunderstanding is key to informing responsible and pedagogically sound\napproaches to integrating LLMs into instruction and assessment. This study\ntherefore compares the problem-solving performance of a general-purpose LLM\n(GPT-4o, using varying prompting techniques) and a reasoning-optimized model\n(o1-preview) with that of participants of the German Physics Olympiad, based on\na set of well-defined Olympiad problems. In addition to evaluating the\ncorrectness of the generated solutions, the study analyzes characteristic\nstrengths and limitations of LLM-generated solutions. The findings of this\nstudy indicate that both tested LLMs (GPT-4o and o1-preview) demonstrate\nadvanced problem-solving capabilities on Olympiad-type physics problems, on\naverage outperforming the human participants. Prompting techniques had little\neffect on GPT-4o's performance, while o1-preview almost consistently\noutperformed both GPT-4o and the human benchmark. Based on these findings, the\nstudy discusses implications for the design of summative and formative\nassessment in physics education, including how to uphold assessment integrity\nand support students in critically engaging with LLMs."
                },
                "authors": [
                    {
                        "name": "Paul Tschisgale"
                    },
                    {
                        "name": "Holger Maus"
                    },
                    {
                        "name": "Fabian Kieser"
                    },
                    {
                        "name": "Ben Kroehs"
                    },
                    {
                        "name": "Stefan Petersen"
                    },
                    {
                        "name": "Peter Wulff"
                    }
                ],
                "author_detail": {
                    "name": "Peter Wulff"
                },
                "author": "Peter Wulff",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09438v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09438v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ed-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ed-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09436v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09436v1",
                "updated": "2025-05-14T14:44:30Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    14,
                    44,
                    30,
                    2,
                    134,
                    0
                ],
                "published": "2025-05-14T14:44:30Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    14,
                    44,
                    30,
                    2,
                    134,
                    0
                ],
                "title": "CXMArena: Unified Dataset to benchmark performance in realistic CXM\n  Scenarios",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CXMArena: Unified Dataset to benchmark performance in realistic CXM\n  Scenarios"
                },
                "summary": "Large Language Models (LLMs) hold immense potential for revolutionizing\nCustomer Experience Management (CXM), particularly in contact center\noperations. However, evaluating their practical utility in complex operational\nenvironments is hindered by data scarcity (due to privacy concerns) and the\nlimitations of current benchmarks. Existing benchmarks often lack realism,\nfailing to incorporate deep knowledge base (KB) integration, real-world noise,\nor critical operational tasks beyond conversational fluency. To bridge this\ngap, we introduce CXMArena, a novel, large-scale synthetic benchmark dataset\nspecifically designed for evaluating AI in operational CXM contexts. Given the\ndiversity in possible contact center features, we have developed a scalable\nLLM-powered pipeline that simulates the brand's CXM entities that form the\nfoundation of our datasets-such as knowledge articles including product\nspecifications, issue taxonomies, and contact center conversations. The\nentities closely represent real-world distribution because of controlled noise\ninjection (informed by domain experts) and rigorous automated validation.\nBuilding on this, we release CXMArena, which provides dedicated benchmarks\ntargeting five important operational tasks: Knowledge Base Refinement, Intent\nPrediction, Agent Quality Adherence, Article Search, and Multi-turn RAG with\nIntegrated Tools. Our baseline experiments underscore the benchmark's\ndifficulty: even state of the art embedding and generation models achieve only\n68% accuracy on article search, while standard embedding methods yield a low F1\nscore of 0.3 for knowledge base refinement, highlighting significant challenges\nfor current models necessitating complex pipelines and solutions over\nconventional techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) hold immense potential for revolutionizing\nCustomer Experience Management (CXM), particularly in contact center\noperations. However, evaluating their practical utility in complex operational\nenvironments is hindered by data scarcity (due to privacy concerns) and the\nlimitations of current benchmarks. Existing benchmarks often lack realism,\nfailing to incorporate deep knowledge base (KB) integration, real-world noise,\nor critical operational tasks beyond conversational fluency. To bridge this\ngap, we introduce CXMArena, a novel, large-scale synthetic benchmark dataset\nspecifically designed for evaluating AI in operational CXM contexts. Given the\ndiversity in possible contact center features, we have developed a scalable\nLLM-powered pipeline that simulates the brand's CXM entities that form the\nfoundation of our datasets-such as knowledge articles including product\nspecifications, issue taxonomies, and contact center conversations. The\nentities closely represent real-world distribution because of controlled noise\ninjection (informed by domain experts) and rigorous automated validation.\nBuilding on this, we release CXMArena, which provides dedicated benchmarks\ntargeting five important operational tasks: Knowledge Base Refinement, Intent\nPrediction, Agent Quality Adherence, Article Search, and Multi-turn RAG with\nIntegrated Tools. Our baseline experiments underscore the benchmark's\ndifficulty: even state of the art embedding and generation models achieve only\n68% accuracy on article search, while standard embedding methods yield a low F1\nscore of 0.3 for knowledge base refinement, highlighting significant challenges\nfor current models necessitating complex pipelines and solutions over\nconventional techniques."
                },
                "authors": [
                    {
                        "name": "Raghav Garg"
                    },
                    {
                        "name": "Kapil Sharma"
                    },
                    {
                        "name": "Karan Gupta"
                    }
                ],
                "author_detail": {
                    "name": "Karan Gupta"
                },
                "author": "Karan Gupta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09436v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09436v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09427v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09427v2",
                "updated": "2025-05-15T07:22:20Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    7,
                    22,
                    20,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-14T14:28:24Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    14,
                    28,
                    24,
                    2,
                    134,
                    0
                ],
                "title": "SafePath: Conformal Prediction for Safe LLM-Based Autonomous Navigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SafePath: Conformal Prediction for Safe LLM-Based Autonomous Navigation"
                },
                "summary": "Large Language Models (LLMs) show growing promise in autonomous driving by\nreasoning over complex traffic scenarios to generate path plans. However, their\ntendencies toward overconfidence, and hallucinations raise critical safety\nconcerns. We introduce SafePath, a modular framework that augments LLM-based\npath planning with formal safety guarantees using conformal prediction.\nSafePath operates in three stages. In the first stage, we use an LLM that\ngenerates a set of diverse candidate paths, exploring possible trajectories\nbased on agent behaviors and environmental cues. In the second stage, SafePath\nfilters out high-risk trajectories while guaranteeing that at least one safe\noption is included with a user-defined probability, through a multiple-choice\nquestion-answering formulation that integrates conformal prediction. In the\nfinal stage, our approach selects the path with the lowest expected collision\nrisk when uncertainty is low or delegates control to a human when uncertainty\nis high. We theoretically prove that SafePath guarantees a safe trajectory with\na user-defined probability, and we show how its human delegation rate can be\ntuned to balance autonomy and safety. Extensive experiments on nuScenes and\nHighway-env show that SafePath reduces planning uncertainty by 77\\% and\ncollision rates by up to 70\\%, demonstrating effectiveness in making LLM-driven\npath planning more safer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) show growing promise in autonomous driving by\nreasoning over complex traffic scenarios to generate path plans. However, their\ntendencies toward overconfidence, and hallucinations raise critical safety\nconcerns. We introduce SafePath, a modular framework that augments LLM-based\npath planning with formal safety guarantees using conformal prediction.\nSafePath operates in three stages. In the first stage, we use an LLM that\ngenerates a set of diverse candidate paths, exploring possible trajectories\nbased on agent behaviors and environmental cues. In the second stage, SafePath\nfilters out high-risk trajectories while guaranteeing that at least one safe\noption is included with a user-defined probability, through a multiple-choice\nquestion-answering formulation that integrates conformal prediction. In the\nfinal stage, our approach selects the path with the lowest expected collision\nrisk when uncertainty is low or delegates control to a human when uncertainty\nis high. We theoretically prove that SafePath guarantees a safe trajectory with\na user-defined probability, and we show how its human delegation rate can be\ntuned to balance autonomy and safety. Extensive experiments on nuScenes and\nHighway-env show that SafePath reduces planning uncertainty by 77\\% and\ncollision rates by up to 70\\%, demonstrating effectiveness in making LLM-driven\npath planning more safer."
                },
                "authors": [
                    {
                        "name": "Achref Doula"
                    },
                    {
                        "name": "Max Mühlhäuser"
                    },
                    {
                        "name": "Alejandro Sanchez Guinea"
                    }
                ],
                "author_detail": {
                    "name": "Alejandro Sanchez Guinea"
                },
                "author": "Alejandro Sanchez Guinea",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09427v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09427v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24293v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24293v2",
                "updated": "2025-05-14T13:59:02Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    13,
                    59,
                    2,
                    2,
                    134,
                    0
                ],
                "published": "2025-03-31T16:41:16Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    16,
                    41,
                    16,
                    0,
                    90,
                    0
                ],
                "title": "Is analogy enough to draw novel adjective-noun inferences?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is analogy enough to draw novel adjective-noun inferences?"
                },
                "summary": "Recent work (Ross et al., 2025, 2024) has argued that the ability of humans\nand LLMs respectively to generalize to novel adjective-noun combinations shows\nthat they each have access to a compositional mechanism to determine the\nphrase's meaning and derive inferences. We study whether these inferences can\ninstead be derived by analogy to known inferences, without need for\ncomposition. We investigate this by (1) building a model of analogical\nreasoning using similarity over lexical items, and (2) asking human\nparticipants to reason by analogy. While we find that this strategy works well\nfor a large proportion of the dataset of Ross et al. (2025), there are novel\ncombinations for which both humans and LLMs derive convergent inferences but\nwhich are not well handled by analogy. We thus conclude that the mechanism\nhumans and LLMs use to generalize in these cases cannot be fully reduced to\nanalogy, and likely involves composition.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent work (Ross et al., 2025, 2024) has argued that the ability of humans\nand LLMs respectively to generalize to novel adjective-noun combinations shows\nthat they each have access to a compositional mechanism to determine the\nphrase's meaning and derive inferences. We study whether these inferences can\ninstead be derived by analogy to known inferences, without need for\ncomposition. We investigate this by (1) building a model of analogical\nreasoning using similarity over lexical items, and (2) asking human\nparticipants to reason by analogy. While we find that this strategy works well\nfor a large proportion of the dataset of Ross et al. (2025), there are novel\ncombinations for which both humans and LLMs derive convergent inferences but\nwhich are not well handled by analogy. We thus conclude that the mechanism\nhumans and LLMs use to generalize in these cases cannot be fully reduced to\nanalogy, and likely involves composition."
                },
                "authors": [
                    {
                        "name": "Hayley Ross"
                    },
                    {
                        "name": "Kathryn Davidson"
                    },
                    {
                        "name": "Najoung Kim"
                    }
                ],
                "author_detail": {
                    "name": "Najoung Kim"
                },
                "author": "Najoung Kim",
                "arxiv_comment": "9 pages (17 pages with appendix). Accepted to SCiL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24293v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24293v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09396v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09396v1",
                "updated": "2025-05-14T13:51:24Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    13,
                    51,
                    24,
                    2,
                    134,
                    0
                ],
                "published": "2025-05-14T13:51:24Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    13,
                    51,
                    24,
                    2,
                    134,
                    0
                ],
                "title": "The Influence of Human-inspired Agentic Sophistication in LLM-driven\n  Strategic Reasoners",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Influence of Human-inspired Agentic Sophistication in LLM-driven\n  Strategic Reasoners"
                },
                "summary": "The rapid rise of large language models (LLMs) has shifted artificial\nintelligence (AI) research toward agentic systems, motivating the use of weaker\nand more flexible notions of agency. However, this shift raises key questions\nabout the extent to which LLM-based agents replicate human strategic reasoning,\nparticularly in game-theoretic settings. In this context, we examine the role\nof agentic sophistication in shaping artificial reasoners' performance by\nevaluating three agent designs: a simple game-theoretic model, an unstructured\nLLM-as-agent model, and an LLM integrated into a traditional agentic framework.\nUsing guessing games as a testbed, we benchmarked these agents against human\nparticipants across general reasoning patterns and individual role-based\nobjectives. Furthermore, we introduced obfuscated game scenarios to assess\nagents' ability to generalise beyond training distributions. Our analysis,\ncovering over 2000 reasoning samples across 25 agent configurations, shows that\nhuman-inspired cognitive structures can enhance LLM agents' alignment with\nhuman strategic behaviour. Still, the relationship between agentic design\ncomplexity and human-likeness is non-linear, highlighting a critical dependence\non underlying LLM capabilities and suggesting limits to simple architectural\naugmentation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid rise of large language models (LLMs) has shifted artificial\nintelligence (AI) research toward agentic systems, motivating the use of weaker\nand more flexible notions of agency. However, this shift raises key questions\nabout the extent to which LLM-based agents replicate human strategic reasoning,\nparticularly in game-theoretic settings. In this context, we examine the role\nof agentic sophistication in shaping artificial reasoners' performance by\nevaluating three agent designs: a simple game-theoretic model, an unstructured\nLLM-as-agent model, and an LLM integrated into a traditional agentic framework.\nUsing guessing games as a testbed, we benchmarked these agents against human\nparticipants across general reasoning patterns and individual role-based\nobjectives. Furthermore, we introduced obfuscated game scenarios to assess\nagents' ability to generalise beyond training distributions. Our analysis,\ncovering over 2000 reasoning samples across 25 agent configurations, shows that\nhuman-inspired cognitive structures can enhance LLM agents' alignment with\nhuman strategic behaviour. Still, the relationship between agentic design\ncomplexity and human-likeness is non-linear, highlighting a critical dependence\non underlying LLM capabilities and suggesting limits to simple architectural\naugmentation."
                },
                "authors": [
                    {
                        "name": "Vince Trencsenyi"
                    },
                    {
                        "name": "Agnieszka Mensfelt"
                    },
                    {
                        "name": "Kostas Stathis"
                    }
                ],
                "author_detail": {
                    "name": "Kostas Stathis"
                },
                "author": "Kostas Stathis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09396v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09396v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09395v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09395v1",
                "updated": "2025-05-14T13:50:44Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    13,
                    50,
                    44,
                    2,
                    134,
                    0
                ],
                "published": "2025-05-14T13:50:44Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    13,
                    50,
                    44,
                    2,
                    134,
                    0
                ],
                "title": "Quantum-Enhanced Parameter-Efficient Learning for Typhoon Trajectory\n  Forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum-Enhanced Parameter-Efficient Learning for Typhoon Trajectory\n  Forecasting"
                },
                "summary": "Typhoon trajectory forecasting is essential for disaster preparedness but\nremains computationally demanding due to the complexity of atmospheric dynamics\nand the resource requirements of deep learning models. Quantum-Train (QT), a\nhybrid quantum-classical framework that leverages quantum neural networks\n(QNNs) to generate trainable parameters exclusively during training,\neliminating the need for quantum hardware at inference time. Building on QT's\nsuccess across multiple domains, including image classification, reinforcement\nlearning, flood prediction, and large language model (LLM) fine-tuning, we\nintroduce Quantum Parameter Adaptation (QPA) for efficient typhoon forecasting\nmodel learning. Integrated with an Attention-based Multi-ConvGRU model, QPA\nenables parameter-efficient training while maintaining predictive accuracy.\nThis work represents the first application of quantum machine learning (QML) to\nlarge-scale typhoon trajectory prediction, offering a scalable and\nenergy-efficient approach to climate modeling. Our results demonstrate that QPA\nsignificantly reduces the number of trainable parameters while preserving\nperformance, making high-performance forecasting more accessible and\nsustainable through hybrid quantum-classical learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Typhoon trajectory forecasting is essential for disaster preparedness but\nremains computationally demanding due to the complexity of atmospheric dynamics\nand the resource requirements of deep learning models. Quantum-Train (QT), a\nhybrid quantum-classical framework that leverages quantum neural networks\n(QNNs) to generate trainable parameters exclusively during training,\neliminating the need for quantum hardware at inference time. Building on QT's\nsuccess across multiple domains, including image classification, reinforcement\nlearning, flood prediction, and large language model (LLM) fine-tuning, we\nintroduce Quantum Parameter Adaptation (QPA) for efficient typhoon forecasting\nmodel learning. Integrated with an Attention-based Multi-ConvGRU model, QPA\nenables parameter-efficient training while maintaining predictive accuracy.\nThis work represents the first application of quantum machine learning (QML) to\nlarge-scale typhoon trajectory prediction, offering a scalable and\nenergy-efficient approach to climate modeling. Our results demonstrate that QPA\nsignificantly reduces the number of trainable parameters while preserving\nperformance, making high-performance forecasting more accessible and\nsustainable through hybrid quantum-classical learning."
                },
                "authors": [
                    {
                        "name": "Chen-Yu Liu"
                    },
                    {
                        "name": "Kuan-Cheng Chen"
                    },
                    {
                        "name": "Yi-Chien Chen"
                    },
                    {
                        "name": "Samuel Yen-Chi Chen"
                    },
                    {
                        "name": "Wei-Hao Huang"
                    },
                    {
                        "name": "Wei-Jia Huang"
                    },
                    {
                        "name": "Yen-Jui Chang"
                    }
                ],
                "author_detail": {
                    "name": "Yen-Jui Chang"
                },
                "author": "Yen-Jui Chang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09395v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09395v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09388v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09388v1",
                "updated": "2025-05-14T13:41:34Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    13,
                    41,
                    34,
                    2,
                    134,
                    0
                ],
                "published": "2025-05-14T13:41:34Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    13,
                    41,
                    34,
                    2,
                    134,
                    0
                ],
                "title": "Qwen3 Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Qwen3 Technical Report"
                },
                "summary": "In this work, we present Qwen3, the latest version of the Qwen model family.\nQwen3 comprises a series of large language models (LLMs) designed to advance\nperformance, efficiency, and multilingual capabilities. The Qwen3 series\nincludes models of both dense and Mixture-of-Expert (MoE) architectures, with\nparameter scales ranging from 0.6 to 235 billion. A key innovation in Qwen3 is\nthe integration of thinking mode (for complex, multi-step reasoning) and\nnon-thinking mode (for rapid, context-driven responses) into a unified\nframework. This eliminates the need to switch between different models--such as\nchat-optimized models (e.g., GPT-4o) and dedicated reasoning models (e.g.,\nQwQ-32B)--and enables dynamic mode switching based on user queries or chat\ntemplates. Meanwhile, Qwen3 introduces a thinking budget mechanism, allowing\nusers to allocate computational resources adaptively during inference, thereby\nbalancing latency and performance based on task complexity. Moreover, by\nleveraging the knowledge from the flagship models, we significantly reduce the\ncomputational resources required to build smaller-scale models, while ensuring\ntheir highly competitive performance. Empirical evaluations demonstrate that\nQwen3 achieves state-of-the-art results across diverse benchmarks, including\ntasks in code generation, mathematical reasoning, agent tasks, etc.,\ncompetitive against larger MoE models and proprietary models. Compared to its\npredecessor Qwen2.5, Qwen3 expands multilingual support from 29 to 119\nlanguages and dialects, enhancing global accessibility through improved\ncross-lingual understanding and generation capabilities. To facilitate\nreproducibility and community-driven research and development, all Qwen3 models\nare publicly accessible under Apache 2.0.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we present Qwen3, the latest version of the Qwen model family.\nQwen3 comprises a series of large language models (LLMs) designed to advance\nperformance, efficiency, and multilingual capabilities. The Qwen3 series\nincludes models of both dense and Mixture-of-Expert (MoE) architectures, with\nparameter scales ranging from 0.6 to 235 billion. A key innovation in Qwen3 is\nthe integration of thinking mode (for complex, multi-step reasoning) and\nnon-thinking mode (for rapid, context-driven responses) into a unified\nframework. This eliminates the need to switch between different models--such as\nchat-optimized models (e.g., GPT-4o) and dedicated reasoning models (e.g.,\nQwQ-32B)--and enables dynamic mode switching based on user queries or chat\ntemplates. Meanwhile, Qwen3 introduces a thinking budget mechanism, allowing\nusers to allocate computational resources adaptively during inference, thereby\nbalancing latency and performance based on task complexity. Moreover, by\nleveraging the knowledge from the flagship models, we significantly reduce the\ncomputational resources required to build smaller-scale models, while ensuring\ntheir highly competitive performance. Empirical evaluations demonstrate that\nQwen3 achieves state-of-the-art results across diverse benchmarks, including\ntasks in code generation, mathematical reasoning, agent tasks, etc.,\ncompetitive against larger MoE models and proprietary models. Compared to its\npredecessor Qwen2.5, Qwen3 expands multilingual support from 29 to 119\nlanguages and dialects, enhancing global accessibility through improved\ncross-lingual understanding and generation capabilities. To facilitate\nreproducibility and community-driven research and development, all Qwen3 models\nare publicly accessible under Apache 2.0."
                },
                "authors": [
                    {
                        "name": "An Yang"
                    },
                    {
                        "name": "Anfeng Li"
                    },
                    {
                        "name": "Baosong Yang"
                    },
                    {
                        "name": "Beichen Zhang"
                    },
                    {
                        "name": "Binyuan Hui"
                    },
                    {
                        "name": "Bo Zheng"
                    },
                    {
                        "name": "Bowen Yu"
                    },
                    {
                        "name": "Chang Gao"
                    },
                    {
                        "name": "Chengen Huang"
                    },
                    {
                        "name": "Chenxu Lv"
                    },
                    {
                        "name": "Chujie Zheng"
                    },
                    {
                        "name": "Dayiheng Liu"
                    },
                    {
                        "name": "Fan Zhou"
                    },
                    {
                        "name": "Fei Huang"
                    },
                    {
                        "name": "Feng Hu"
                    },
                    {
                        "name": "Hao Ge"
                    },
                    {
                        "name": "Haoran Wei"
                    },
                    {
                        "name": "Huan Lin"
                    },
                    {
                        "name": "Jialong Tang"
                    },
                    {
                        "name": "Jian Yang"
                    },
                    {
                        "name": "Jianhong Tu"
                    },
                    {
                        "name": "Jianwei Zhang"
                    },
                    {
                        "name": "Jianxin Yang"
                    },
                    {
                        "name": "Jiaxi Yang"
                    },
                    {
                        "name": "Jing Zhou"
                    },
                    {
                        "name": "Jingren Zhou"
                    },
                    {
                        "name": "Junyang Lin"
                    },
                    {
                        "name": "Kai Dang"
                    },
                    {
                        "name": "Keqin Bao"
                    },
                    {
                        "name": "Kexin Yang"
                    },
                    {
                        "name": "Le Yu"
                    },
                    {
                        "name": "Lianghao Deng"
                    },
                    {
                        "name": "Mei Li"
                    },
                    {
                        "name": "Mingfeng Xue"
                    },
                    {
                        "name": "Mingze Li"
                    },
                    {
                        "name": "Pei Zhang"
                    },
                    {
                        "name": "Peng Wang"
                    },
                    {
                        "name": "Qin Zhu"
                    },
                    {
                        "name": "Rui Men"
                    },
                    {
                        "name": "Ruize Gao"
                    },
                    {
                        "name": "Shixuan Liu"
                    },
                    {
                        "name": "Shuang Luo"
                    },
                    {
                        "name": "Tianhao Li"
                    },
                    {
                        "name": "Tianyi Tang"
                    },
                    {
                        "name": "Wenbiao Yin"
                    },
                    {
                        "name": "Xingzhang Ren"
                    },
                    {
                        "name": "Xinyu Wang"
                    },
                    {
                        "name": "Xinyu Zhang"
                    },
                    {
                        "name": "Xuancheng Ren"
                    },
                    {
                        "name": "Yang Fan"
                    },
                    {
                        "name": "Yang Su"
                    },
                    {
                        "name": "Yichang Zhang"
                    },
                    {
                        "name": "Yinger Zhang"
                    },
                    {
                        "name": "Yu Wan"
                    },
                    {
                        "name": "Yuqiong Liu"
                    },
                    {
                        "name": "Zekun Wang"
                    },
                    {
                        "name": "Zeyu Cui"
                    },
                    {
                        "name": "Zhenru Zhang"
                    },
                    {
                        "name": "Zhipeng Zhou"
                    },
                    {
                        "name": "Zihan Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Zihan Qiu"
                },
                "author": "Zihan Qiu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09388v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09388v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09386v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09386v1",
                "updated": "2025-05-14T13:38:53Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    13,
                    38,
                    53,
                    2,
                    134,
                    0
                ],
                "published": "2025-05-14T13:38:53Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    13,
                    38,
                    53,
                    2,
                    134,
                    0
                ],
                "title": "Instant AoI Optimization through Relay Location Selection in Disaster\n  Multi-hop Communication",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instant AoI Optimization through Relay Location Selection in Disaster\n  Multi-hop Communication"
                },
                "summary": "Meteorological disasters such as typhoons, forest fires, and floods can\ndamage the communication infrastructures, which will further disable the\ncommunication capabilities of cellular networks. The multi-hop wireless\ncommunication based on IoT devices (e.g., rescue robots, UAVs, and mobile\ndevices) becomes an available and rapidly deployable communication approach for\nsearch and rescue operations. However, Age of Information (AoI), an emerging\nnetwork performance metric, has not been comprehensively investigated in this\nmulti-hop model. In this paper, we first construct a UAV-relayed wireless\nnetwork model and formulate the end-to-end instant AoI. Then we derive the\noptimal location of the relay UAV to achieve the minimum instant AoI by\nmathematical analysis. Simulations show that the derived relay location can\nalways guarantee the optimal AoI and outperform other schemes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Meteorological disasters such as typhoons, forest fires, and floods can\ndamage the communication infrastructures, which will further disable the\ncommunication capabilities of cellular networks. The multi-hop wireless\ncommunication based on IoT devices (e.g., rescue robots, UAVs, and mobile\ndevices) becomes an available and rapidly deployable communication approach for\nsearch and rescue operations. However, Age of Information (AoI), an emerging\nnetwork performance metric, has not been comprehensively investigated in this\nmulti-hop model. In this paper, we first construct a UAV-relayed wireless\nnetwork model and formulate the end-to-end instant AoI. Then we derive the\noptimal location of the relay UAV to achieve the minimum instant AoI by\nmathematical analysis. Simulations show that the derived relay location can\nalways guarantee the optimal AoI and outperform other schemes."
                },
                "authors": [
                    {
                        "name": "Yang Gao"
                    },
                    {
                        "name": "Zezhi Zeng"
                    }
                ],
                "author_detail": {
                    "name": "Zezhi Zeng"
                },
                "author": "Zezhi Zeng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09386v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09386v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09380v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09380v1",
                "updated": "2025-05-14T13:33:38Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    13,
                    33,
                    38,
                    2,
                    134,
                    0
                ],
                "published": "2025-05-14T13:33:38Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    13,
                    33,
                    38,
                    2,
                    134,
                    0
                ],
                "title": "Examining Deployment and Refinement of the VIOLA-AI Intracranial\n  Hemorrhage Model Using an Interactive NeoMedSys Platform",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Examining Deployment and Refinement of the VIOLA-AI Intracranial\n  Hemorrhage Model Using an Interactive NeoMedSys Platform"
                },
                "summary": "Background: There are many challenges and opportunities in the clinical\ndeployment of AI tools in radiology. The current study describes a radiology\nsoftware platform called NeoMedSys that can enable efficient deployment and\nrefinements of AI models. We evaluated the feasibility and effectiveness of\nrunning NeoMedSys for three months in real-world clinical settings and focused\non improvement performance of an in-house developed AI model (VIOLA-AI)\ndesigned for intracranial hemorrhage (ICH) detection.\n  Methods: NeoMedSys integrates tools for deploying, testing, and optimizing AI\nmodels with a web-based medical image viewer, annotation system, and\nhospital-wide radiology information systems. A pragmatic investigation was\ndeployed using clinical cases of patients presenting to the largest Emergency\nDepartment in Norway (site-1) with suspected traumatic brain injury (TBI) or\npatients with suspected stroke (site-2). We assessed ICH classification\nperformance as VIOLA-AI encountered new data and underwent pre-planned model\nretraining. Performance metrics included sensitivity, specificity, accuracy,\nand the area under the receiver operating characteristic curve (AUC).\n  Results: NeoMedSys facilitated iterative improvements in the AI model,\nsignificantly enhancing its diagnostic accuracy. Automated bleed detection and\nsegmentation were reviewed in near real-time to facilitate re-training\nVIOLA-AI. The iterative refinement process yielded a marked improvement in\nclassification sensitivity, rising to 90.3% (from 79.2%), and specificity that\nreached 89.3% (from 80.7%). The bleed detection ROC analysis for the entire\nsample demonstrated a high area-under-the-curve (AUC) of 0.949 (from 0.873).\nModel refinement stages were associated with notable gains, highlighting the\nvalue of real-time radiologist feedback.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Background: There are many challenges and opportunities in the clinical\ndeployment of AI tools in radiology. The current study describes a radiology\nsoftware platform called NeoMedSys that can enable efficient deployment and\nrefinements of AI models. We evaluated the feasibility and effectiveness of\nrunning NeoMedSys for three months in real-world clinical settings and focused\non improvement performance of an in-house developed AI model (VIOLA-AI)\ndesigned for intracranial hemorrhage (ICH) detection.\n  Methods: NeoMedSys integrates tools for deploying, testing, and optimizing AI\nmodels with a web-based medical image viewer, annotation system, and\nhospital-wide radiology information systems. A pragmatic investigation was\ndeployed using clinical cases of patients presenting to the largest Emergency\nDepartment in Norway (site-1) with suspected traumatic brain injury (TBI) or\npatients with suspected stroke (site-2). We assessed ICH classification\nperformance as VIOLA-AI encountered new data and underwent pre-planned model\nretraining. Performance metrics included sensitivity, specificity, accuracy,\nand the area under the receiver operating characteristic curve (AUC).\n  Results: NeoMedSys facilitated iterative improvements in the AI model,\nsignificantly enhancing its diagnostic accuracy. Automated bleed detection and\nsegmentation were reviewed in near real-time to facilitate re-training\nVIOLA-AI. The iterative refinement process yielded a marked improvement in\nclassification sensitivity, rising to 90.3% (from 79.2%), and specificity that\nreached 89.3% (from 80.7%). The bleed detection ROC analysis for the entire\nsample demonstrated a high area-under-the-curve (AUC) of 0.949 (from 0.873).\nModel refinement stages were associated with notable gains, highlighting the\nvalue of real-time radiologist feedback."
                },
                "authors": [
                    {
                        "name": "Qinghui Liu"
                    },
                    {
                        "name": "Jon Nesvold"
                    },
                    {
                        "name": "Hanna Raaum"
                    },
                    {
                        "name": "Elakkyen Murugesu"
                    },
                    {
                        "name": "Martin Røvang"
                    },
                    {
                        "name": "Bradley J Maclntosh"
                    },
                    {
                        "name": "Atle Bjørnerud"
                    },
                    {
                        "name": "Karoline Skogen"
                    }
                ],
                "author_detail": {
                    "name": "Karoline Skogen"
                },
                "author": "Karoline Skogen",
                "arxiv_comment": "19 pages, 11 figures, on submission to BMC Methods",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09380v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09380v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21227v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21227v2",
                "updated": "2025-05-14T13:30:48Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    13,
                    30,
                    48,
                    2,
                    134,
                    0
                ],
                "published": "2025-04-29T23:41:37Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    23,
                    41,
                    37,
                    1,
                    119,
                    0
                ],
                "title": "Gradient Attention Map Based Verification of Deep Convolutional Neural\n  Networks with Application to X-ray Image Datasets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gradient Attention Map Based Verification of Deep Convolutional Neural\n  Networks with Application to X-ray Image Datasets"
                },
                "summary": "Deep learning models have great potential in medical imaging, including\northodontics and skeletal maturity assessment. However, applying a model to\ndata different from its training set can lead to unreliable predictions that\nmay impact patient care. To address this, we propose a comprehensive\nverification framework that evaluates model suitability through multiple\ncomplementary strategies. First, we introduce a Gradient Attention Map\n(GAM)-based approach that analyzes attention patterns using Grad-CAM and\ncompares them via similarity metrics such as IoU, Dice Similarity, SSIM, Cosine\nSimilarity, Pearson Correlation, KL Divergence, and Wasserstein Distance.\nSecond, we extend verification to early convolutional feature maps, capturing\nstructural mis-alignments missed by attention alone. Finally, we incorporate an\nadditional garbage class into the classification model to explicitly reject\nout-of-distribution inputs. Experimental results demonstrate that these\ncombined methods effectively identify unsuitable models and inputs, promoting\nsafer and more reliable deployment of deep learning in medical imaging.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning models have great potential in medical imaging, including\northodontics and skeletal maturity assessment. However, applying a model to\ndata different from its training set can lead to unreliable predictions that\nmay impact patient care. To address this, we propose a comprehensive\nverification framework that evaluates model suitability through multiple\ncomplementary strategies. First, we introduce a Gradient Attention Map\n(GAM)-based approach that analyzes attention patterns using Grad-CAM and\ncompares them via similarity metrics such as IoU, Dice Similarity, SSIM, Cosine\nSimilarity, Pearson Correlation, KL Divergence, and Wasserstein Distance.\nSecond, we extend verification to early convolutional feature maps, capturing\nstructural mis-alignments missed by attention alone. Finally, we incorporate an\nadditional garbage class into the classification model to explicitly reject\nout-of-distribution inputs. Experimental results demonstrate that these\ncombined methods effectively identify unsuitable models and inputs, promoting\nsafer and more reliable deployment of deep learning in medical imaging."
                },
                "authors": [
                    {
                        "name": "Omid Halimi Milani"
                    },
                    {
                        "name": "Amanda Nikho"
                    },
                    {
                        "name": "Lauren Mills"
                    },
                    {
                        "name": "Marouane Tliba"
                    },
                    {
                        "name": "Ahmet Enis Cetin"
                    },
                    {
                        "name": "Mohammed H. Elnagar"
                    }
                ],
                "author_detail": {
                    "name": "Mohammed H. Elnagar"
                },
                "author": "Mohammed H. Elnagar",
                "arxiv_comment": "13 pages, 7 figures, accepted at IEEE VLSI Test Symposium (VTS) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21227v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21227v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.07634v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.07634v2",
                "updated": "2025-05-14T12:56:45Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    12,
                    56,
                    45,
                    2,
                    134,
                    0
                ],
                "published": "2025-05-12T15:05:34Z",
                "published_parsed": [
                    2025,
                    5,
                    12,
                    15,
                    5,
                    34,
                    0,
                    132,
                    0
                ],
                "title": "Neural Brain: A Neuroscience-inspired Framework for Embodied Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Brain: A Neuroscience-inspired Framework for Embodied Agents"
                },
                "summary": "The rapid evolution of artificial intelligence (AI) has shifted from static,\ndata-driven models to dynamic systems capable of perceiving and interacting\nwith real-world environments. Despite advancements in pattern recognition and\nsymbolic reasoning, current AI systems, such as large language models, remain\ndisembodied, unable to physically engage with the world. This limitation has\ndriven the rise of embodied AI, where autonomous agents, such as humanoid\nrobots, must navigate and manipulate unstructured environments with human-like\nadaptability. At the core of this challenge lies the concept of Neural Brain, a\ncentral intelligence system designed to drive embodied agents with human-like\nadaptability. A Neural Brain must seamlessly integrate multimodal sensing and\nperception with cognitive capabilities. Achieving this also requires an\nadaptive memory system and energy-efficient hardware-software co-design,\nenabling real-time action in dynamic environments. This paper introduces a\nunified framework for the Neural Brain of embodied agents, addressing two\nfundamental challenges: (1) defining the core components of Neural Brain and\n(2) bridging the gap between static AI models and the dynamic adaptability\nrequired for real-world deployment. To this end, we propose a biologically\ninspired architecture that integrates multimodal active sensing,\nperception-cognition-action function, neuroplasticity-based memory storage and\nupdating, and neuromorphic hardware/software optimization. Furthermore, we also\nreview the latest research on embodied agents across these four aspects and\nanalyze the gap between current AI systems and human intelligence. By\nsynthesizing insights from neuroscience, we outline a roadmap towards the\ndevelopment of generalizable, autonomous agents capable of human-level\nintelligence in real-world scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid evolution of artificial intelligence (AI) has shifted from static,\ndata-driven models to dynamic systems capable of perceiving and interacting\nwith real-world environments. Despite advancements in pattern recognition and\nsymbolic reasoning, current AI systems, such as large language models, remain\ndisembodied, unable to physically engage with the world. This limitation has\ndriven the rise of embodied AI, where autonomous agents, such as humanoid\nrobots, must navigate and manipulate unstructured environments with human-like\nadaptability. At the core of this challenge lies the concept of Neural Brain, a\ncentral intelligence system designed to drive embodied agents with human-like\nadaptability. A Neural Brain must seamlessly integrate multimodal sensing and\nperception with cognitive capabilities. Achieving this also requires an\nadaptive memory system and energy-efficient hardware-software co-design,\nenabling real-time action in dynamic environments. This paper introduces a\nunified framework for the Neural Brain of embodied agents, addressing two\nfundamental challenges: (1) defining the core components of Neural Brain and\n(2) bridging the gap between static AI models and the dynamic adaptability\nrequired for real-world deployment. To this end, we propose a biologically\ninspired architecture that integrates multimodal active sensing,\nperception-cognition-action function, neuroplasticity-based memory storage and\nupdating, and neuromorphic hardware/software optimization. Furthermore, we also\nreview the latest research on embodied agents across these four aspects and\nanalyze the gap between current AI systems and human intelligence. By\nsynthesizing insights from neuroscience, we outline a roadmap towards the\ndevelopment of generalizable, autonomous agents capable of human-level\nintelligence in real-world scenarios."
                },
                "authors": [
                    {
                        "name": "Jian Liu"
                    },
                    {
                        "name": "Xiongtao Shi"
                    },
                    {
                        "name": "Thai Duy Nguyen"
                    },
                    {
                        "name": "Haitian Zhang"
                    },
                    {
                        "name": "Tianxiang Zhang"
                    },
                    {
                        "name": "Wei Sun"
                    },
                    {
                        "name": "Yanjie Li"
                    },
                    {
                        "name": "Athanasios V. Vasilakos"
                    },
                    {
                        "name": "Giovanni Iacca"
                    },
                    {
                        "name": "Arshad Ali Khan"
                    },
                    {
                        "name": "Arvind Kumar"
                    },
                    {
                        "name": "Jae Won Cho"
                    },
                    {
                        "name": "Ajmal Mian"
                    },
                    {
                        "name": "Lihua Xie"
                    },
                    {
                        "name": "Erik Cambria"
                    },
                    {
                        "name": "Lin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Lin Wang"
                },
                "author": "Lin Wang",
                "arxiv_comment": "51 pages, 17 figures, 9 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.07634v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.07634v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09343v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09343v1",
                "updated": "2025-05-14T12:39:03Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    12,
                    39,
                    3,
                    2,
                    134,
                    0
                ],
                "published": "2025-05-14T12:39:03Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    12,
                    39,
                    3,
                    2,
                    134,
                    0
                ],
                "title": "Insights into DeepSeek-V3: Scaling Challenges and Reflections on\n  Hardware for AI Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Insights into DeepSeek-V3: Scaling Challenges and Reflections on\n  Hardware for AI Architectures"
                },
                "summary": "The rapid scaling of large language models (LLMs) has unveiled critical\nlimitations in current hardware architectures, including constraints in memory\ncapacity, computational efficiency, and interconnection bandwidth. DeepSeek-V3,\ntrained on 2,048 NVIDIA H800 GPUs, demonstrates how hardware-aware model\nco-design can effectively address these challenges, enabling cost-efficient\ntraining and inference at scale. This paper presents an in-depth analysis of\nthe DeepSeek-V3/R1 model architecture and its AI infrastructure, highlighting\nkey innovations such as Multi-head Latent Attention (MLA) for enhanced memory\nefficiency, Mixture of Experts (MoE) architectures for optimized\ncomputation-communication trade-offs, FP8 mixed-precision training to unlock\nthe full potential of hardware capabilities, and a Multi-Plane Network Topology\nto minimize cluster-level network overhead. Building on the hardware\nbottlenecks encountered during DeepSeek-V3's development, we engage in a\nbroader discussion with academic and industry peers on potential future\nhardware directions, including precise low-precision computation units,\nscale-up and scale-out convergence, and innovations in low-latency\ncommunication fabrics. These insights underscore the critical role of hardware\nand model co-design in meeting the escalating demands of AI workloads, offering\na practical blueprint for innovation in next-generation AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid scaling of large language models (LLMs) has unveiled critical\nlimitations in current hardware architectures, including constraints in memory\ncapacity, computational efficiency, and interconnection bandwidth. DeepSeek-V3,\ntrained on 2,048 NVIDIA H800 GPUs, demonstrates how hardware-aware model\nco-design can effectively address these challenges, enabling cost-efficient\ntraining and inference at scale. This paper presents an in-depth analysis of\nthe DeepSeek-V3/R1 model architecture and its AI infrastructure, highlighting\nkey innovations such as Multi-head Latent Attention (MLA) for enhanced memory\nefficiency, Mixture of Experts (MoE) architectures for optimized\ncomputation-communication trade-offs, FP8 mixed-precision training to unlock\nthe full potential of hardware capabilities, and a Multi-Plane Network Topology\nto minimize cluster-level network overhead. Building on the hardware\nbottlenecks encountered during DeepSeek-V3's development, we engage in a\nbroader discussion with academic and industry peers on potential future\nhardware directions, including precise low-precision computation units,\nscale-up and scale-out convergence, and innovations in low-latency\ncommunication fabrics. These insights underscore the critical role of hardware\nand model co-design in meeting the escalating demands of AI workloads, offering\na practical blueprint for innovation in next-generation AI systems."
                },
                "authors": [
                    {
                        "name": "Chenggang Zhao"
                    },
                    {
                        "name": "Chengqi Deng"
                    },
                    {
                        "name": "Chong Ruan"
                    },
                    {
                        "name": "Damai Dai"
                    },
                    {
                        "name": "Huazuo Gao"
                    },
                    {
                        "name": "Jiashi Li"
                    },
                    {
                        "name": "Liyue Zhang"
                    },
                    {
                        "name": "Panpan Huang"
                    },
                    {
                        "name": "Shangyan Zhou"
                    },
                    {
                        "name": "Shirong Ma"
                    },
                    {
                        "name": "Wenfeng Liang"
                    },
                    {
                        "name": "Ying He"
                    },
                    {
                        "name": "Yuqing Wang"
                    },
                    {
                        "name": "Yuxuan Liu"
                    },
                    {
                        "name": "Y. X. Wei"
                    }
                ],
                "author_detail": {
                    "name": "Y. X. Wei"
                },
                "author": "Y. X. Wei",
                "arxiv_comment": "This is the author's version of the work. It is posted here for your\n  personal use. Not for redistribution. The definitive version will appear as\n  part of the Industry Track in Proceedings of the 52nd Annual International\n  Symposium on Computer Architecture (ISCA '25)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09343v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09343v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09339v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09339v1",
                "updated": "2025-05-14T12:34:58Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    12,
                    34,
                    58,
                    2,
                    134,
                    0
                ],
                "published": "2025-05-14T12:34:58Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    12,
                    34,
                    58,
                    2,
                    134,
                    0
                ],
                "title": "RAG-Enabled Intent Reasoning for Application-Network Interaction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAG-Enabled Intent Reasoning for Application-Network Interaction"
                },
                "summary": "Intent-based network (IBN) is a promising solution to automate network\noperation and management. IBN aims to offer human-tailored network interaction,\nallowing the network to communicate in a way that aligns with the network\nusers' language, rather than requiring the network users to understand the\ntechnical language of the network/devices. Nowadays, different applications\ninteract with the network, each with its own specialized needs and domain\nlanguage. Creating semantic languages (i.e., ontology-based languages) and\nassociating them with each application to facilitate intent translation lacks\ntechnical expertise and is neither practical nor scalable. To tackle the\naforementioned problem, we propose a context-aware AI framework that utilizes\nmachine reasoning (MR), retrieval augmented generation (RAG), and generative AI\ntechnologies to interpret intents from different applications and generate\nstructured network intents. The proposed framework allows for\ngeneralized/domain-specific intent expression and overcomes the drawbacks of\nlarge language models (LLMs) and vanilla-RAG framework. The experimental\nresults show that our proposed intent-RAG framework outperforms the LLM and\nvanilla-RAG framework in intent translation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intent-based network (IBN) is a promising solution to automate network\noperation and management. IBN aims to offer human-tailored network interaction,\nallowing the network to communicate in a way that aligns with the network\nusers' language, rather than requiring the network users to understand the\ntechnical language of the network/devices. Nowadays, different applications\ninteract with the network, each with its own specialized needs and domain\nlanguage. Creating semantic languages (i.e., ontology-based languages) and\nassociating them with each application to facilitate intent translation lacks\ntechnical expertise and is neither practical nor scalable. To tackle the\naforementioned problem, we propose a context-aware AI framework that utilizes\nmachine reasoning (MR), retrieval augmented generation (RAG), and generative AI\ntechnologies to interpret intents from different applications and generate\nstructured network intents. The proposed framework allows for\ngeneralized/domain-specific intent expression and overcomes the drawbacks of\nlarge language models (LLMs) and vanilla-RAG framework. The experimental\nresults show that our proposed intent-RAG framework outperforms the LLM and\nvanilla-RAG framework in intent translation."
                },
                "authors": [
                    {
                        "name": "Salwa Mostafa"
                    },
                    {
                        "name": "Mohamed K. Abdel-Aziz"
                    },
                    {
                        "name": "Mohammed S. Elbamby"
                    },
                    {
                        "name": "Mehdi Bennis"
                    }
                ],
                "author_detail": {
                    "name": "Mehdi Bennis"
                },
                "author": "Mehdi Bennis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09339v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09339v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09338v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09338v1",
                "updated": "2025-05-14T12:33:05Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    12,
                    33,
                    5,
                    2,
                    134,
                    0
                ],
                "published": "2025-05-14T12:33:05Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    12,
                    33,
                    5,
                    2,
                    134,
                    0
                ],
                "title": "Llama See, Llama Do: A Mechanistic Perspective on Contextual Entrainment\n  and Distraction in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Llama See, Llama Do: A Mechanistic Perspective on Contextual Entrainment\n  and Distraction in LLMs"
                },
                "summary": "We observe a novel phenomenon, contextual entrainment, across a wide range of\nlanguage models (LMs) and prompt settings, providing a new mechanistic\nperspective on how LMs become distracted by ``irrelevant'' contextual\ninformation in the input prompt. Specifically, LMs assign significantly higher\nlogits (or probabilities) to any tokens that have previously appeared in the\ncontext prompt, even for random tokens. This suggests that contextual\nentrainment is a mechanistic phenomenon, occurring independently of the\nrelevance or semantic relation of the tokens to the question or the rest of the\nsentence. We find statistically significant evidence that the magnitude of\ncontextual entrainment is influenced by semantic factors. Counterfactual\nprompts have a greater effect compared to factual ones, suggesting that while\ncontextual entrainment is a mechanistic phenomenon, it is modulated by semantic\nfactors.\n  We hypothesise that there is a circuit of attention heads -- the entrainment\nheads -- that corresponds to the contextual entrainment phenomenon. Using a\nnovel entrainment head discovery method based on differentiable masking, we\nidentify these heads across various settings. When we ``turn off'' these heads,\ni.e., set their outputs to zero, the effect of contextual entrainment is\nsignificantly attenuated, causing the model to generate output that capitulates\nto what it would produce if no distracting context were provided. Our discovery\nof contextual entrainment, along with our investigation into LM distraction via\nthe entrainment heads, marks a key step towards the mechanistic analysis and\nmitigation of the distraction problem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We observe a novel phenomenon, contextual entrainment, across a wide range of\nlanguage models (LMs) and prompt settings, providing a new mechanistic\nperspective on how LMs become distracted by ``irrelevant'' contextual\ninformation in the input prompt. Specifically, LMs assign significantly higher\nlogits (or probabilities) to any tokens that have previously appeared in the\ncontext prompt, even for random tokens. This suggests that contextual\nentrainment is a mechanistic phenomenon, occurring independently of the\nrelevance or semantic relation of the tokens to the question or the rest of the\nsentence. We find statistically significant evidence that the magnitude of\ncontextual entrainment is influenced by semantic factors. Counterfactual\nprompts have a greater effect compared to factual ones, suggesting that while\ncontextual entrainment is a mechanistic phenomenon, it is modulated by semantic\nfactors.\n  We hypothesise that there is a circuit of attention heads -- the entrainment\nheads -- that corresponds to the contextual entrainment phenomenon. Using a\nnovel entrainment head discovery method based on differentiable masking, we\nidentify these heads across various settings. When we ``turn off'' these heads,\ni.e., set their outputs to zero, the effect of contextual entrainment is\nsignificantly attenuated, causing the model to generate output that capitulates\nto what it would produce if no distracting context were provided. Our discovery\nof contextual entrainment, along with our investigation into LM distraction via\nthe entrainment heads, marks a key step towards the mechanistic analysis and\nmitigation of the distraction problem."
                },
                "authors": [
                    {
                        "name": "Jingcheng Niu"
                    },
                    {
                        "name": "Xingdi Yuan"
                    },
                    {
                        "name": "Tong Wang"
                    },
                    {
                        "name": "Hamidreza Saghir"
                    },
                    {
                        "name": "Amir H. Abdi"
                    }
                ],
                "author_detail": {
                    "name": "Amir H. Abdi"
                },
                "author": "Amir H. Abdi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09338v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09338v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03343v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03343v2",
                "updated": "2025-05-14T12:32:17Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    12,
                    32,
                    17,
                    2,
                    134,
                    0
                ],
                "published": "2024-11-02T17:29:47Z",
                "published_parsed": [
                    2024,
                    11,
                    2,
                    17,
                    29,
                    47,
                    5,
                    307,
                    0
                ],
                "title": "What Features in Prompts Jailbreak LLMs? Investigating the Mechanisms\n  Behind Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What Features in Prompts Jailbreak LLMs? Investigating the Mechanisms\n  Behind Attacks"
                },
                "summary": "Jailbreaks have been a central focus of research regarding the safety and\nreliability of large language models (LLMs), yet the mechanisms underlying\nthese attacks remain poorly understood. While previous studies have\npredominantly relied on linear methods to detect jailbreak attempts and model\nrefusals, we take a different approach by examining both linear and non-linear\nfeatures in prompts that lead to successful jailbreaks. First, we introduce a\nnovel dataset comprising 10,800 jailbreak attempts spanning 35 diverse attack\nmethods. Leveraging this dataset, we train probes to classify successful from\nunsuccessful jailbreaks using the latent representations corresponding to\nprompt tokens. Notably, we find that even when probes achieve high accuracy in\npredicting the success of jailbreaks, their performance often fails to\ngeneralize to unseen attack methods. This reveals that different jailbreaking\nstrategies exploit different non-linear, non-universal features. Next, we\ndemonstrate that non-linear probes provide a powerful tool for steering model\nbehavior. Specifically, we use these probes to guide targeted latent space\nperturbations, enabling us to effectively modulate the model's robustness\nagainst jailbreaks. Overall, our findings challenge the assumption that\njailbreaks can be fully understood through linear or simple universal prompt\nfeatures alone, highlighting the importance of a nuanced understanding of the\nmechanisms behind LLM vulnerabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Jailbreaks have been a central focus of research regarding the safety and\nreliability of large language models (LLMs), yet the mechanisms underlying\nthese attacks remain poorly understood. While previous studies have\npredominantly relied on linear methods to detect jailbreak attempts and model\nrefusals, we take a different approach by examining both linear and non-linear\nfeatures in prompts that lead to successful jailbreaks. First, we introduce a\nnovel dataset comprising 10,800 jailbreak attempts spanning 35 diverse attack\nmethods. Leveraging this dataset, we train probes to classify successful from\nunsuccessful jailbreaks using the latent representations corresponding to\nprompt tokens. Notably, we find that even when probes achieve high accuracy in\npredicting the success of jailbreaks, their performance often fails to\ngeneralize to unseen attack methods. This reveals that different jailbreaking\nstrategies exploit different non-linear, non-universal features. Next, we\ndemonstrate that non-linear probes provide a powerful tool for steering model\nbehavior. Specifically, we use these probes to guide targeted latent space\nperturbations, enabling us to effectively modulate the model's robustness\nagainst jailbreaks. Overall, our findings challenge the assumption that\njailbreaks can be fully understood through linear or simple universal prompt\nfeatures alone, highlighting the importance of a nuanced understanding of the\nmechanisms behind LLM vulnerabilities."
                },
                "authors": [
                    {
                        "name": "Nathalie Kirch"
                    },
                    {
                        "name": "Constantin Weisser"
                    },
                    {
                        "name": "Severin Field"
                    },
                    {
                        "name": "Helen Yannakoudakis"
                    },
                    {
                        "name": "Stephen Casper"
                    }
                ],
                "author_detail": {
                    "name": "Stephen Casper"
                },
                "author": "Stephen Casper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03343v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03343v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09319v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09319v1",
                "updated": "2025-05-14T12:16:40Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    12,
                    16,
                    40,
                    2,
                    134,
                    0
                ],
                "published": "2025-05-14T12:16:40Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    12,
                    16,
                    40,
                    2,
                    134,
                    0
                ],
                "title": "Statistical Modeling and Uncertainty Estimation of LLM Inference Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Statistical Modeling and Uncertainty Estimation of LLM Inference Systems"
                },
                "summary": "Large Language Model (LLM) inference systems present significant challenges\nin statistical performance characterization due to dynamic workload variations,\ndiverse hardware architectures, and complex interactions between model size,\nbatch processing, and throughput requirements. Accurate statistical\ncharacterization enables better workload scheduling, adaptive resource\nprovisioning, and cost-aware inference optimization, making it crucial for\nimproving efficiency in large-scale AI deployments. Traditional analytical\nmodels provide explainability but cannot cover the vast diversity of real-world\nworkloads, making it impossible to benchmark every scenario in advance. Machine\nlearning (ML) approaches effectively predict performance for non-benchmarked\ncases but struggle when extrapolating beyond their observed training space. To\naddress these limitations for LLM inference systems, we propose an Analytical\nwith Learning Augmentation (ALA) framework that bridges analytical modeling\nwith \\ml for robust statistical prediction and uncertainty estimation in LLM\ninference workloads. Our method employs an analytical throughput model with\nparameters estimated for benchmarked workloads, then extends to unobserved\nconfigurations using \\ml predictions. We enhance this with simulated annealing\nto exploit subsets of the workload data point combinations and develop an error\npredictor. Finally, we quantify uncertainty based on vector space similarity\nbetween new and observed workloads to ensure robust generalization. Through\nextensive experimentation on diverse LLM inference workloads, we demonstrate\nthat our framework achieves low median errors while maintaining adaptability to\nnew inference scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) inference systems present significant challenges\nin statistical performance characterization due to dynamic workload variations,\ndiverse hardware architectures, and complex interactions between model size,\nbatch processing, and throughput requirements. Accurate statistical\ncharacterization enables better workload scheduling, adaptive resource\nprovisioning, and cost-aware inference optimization, making it crucial for\nimproving efficiency in large-scale AI deployments. Traditional analytical\nmodels provide explainability but cannot cover the vast diversity of real-world\nworkloads, making it impossible to benchmark every scenario in advance. Machine\nlearning (ML) approaches effectively predict performance for non-benchmarked\ncases but struggle when extrapolating beyond their observed training space. To\naddress these limitations for LLM inference systems, we propose an Analytical\nwith Learning Augmentation (ALA) framework that bridges analytical modeling\nwith \\ml for robust statistical prediction and uncertainty estimation in LLM\ninference workloads. Our method employs an analytical throughput model with\nparameters estimated for benchmarked workloads, then extends to unobserved\nconfigurations using \\ml predictions. We enhance this with simulated annealing\nto exploit subsets of the workload data point combinations and develop an error\npredictor. Finally, we quantify uncertainty based on vector space similarity\nbetween new and observed workloads to ensure robust generalization. Through\nextensive experimentation on diverse LLM inference workloads, we demonstrate\nthat our framework achieves low median errors while maintaining adaptability to\nnew inference scenarios."
                },
                "authors": [
                    {
                        "name": "Kaustabha Ray"
                    },
                    {
                        "name": "Nelson Mimura Gonzalez"
                    },
                    {
                        "name": "Bruno Wassermann"
                    },
                    {
                        "name": "Rachel Tzoref-Brill"
                    },
                    {
                        "name": "Dean H. Lorenz"
                    }
                ],
                "author_detail": {
                    "name": "Dean H. Lorenz"
                },
                "author": "Dean H. Lorenz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09319v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09319v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09316v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09316v1",
                "updated": "2025-05-14T12:13:38Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    12,
                    13,
                    38,
                    2,
                    134,
                    0
                ],
                "published": "2025-05-14T12:13:38Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    12,
                    13,
                    38,
                    2,
                    134,
                    0
                ],
                "title": "Scent of Knowledge: Optimizing Search-Enhanced Reasoning with\n  Information Foraging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scent of Knowledge: Optimizing Search-Enhanced Reasoning with\n  Information Foraging"
                },
                "summary": "Augmenting large language models (LLMs) with external retrieval has become a\nstandard method to address their inherent knowledge cutoff limitations.\nHowever, traditional retrieval-augmented generation methods employ static,\npre-inference retrieval strategies, making them inadequate for complex tasks\ninvolving ambiguous, multi-step, or evolving information needs. Recent advances\nin test-time scaling techniques have demonstrated significant potential in\nenabling LLMs to dynamically interact with external tools, motivating the shift\ntoward adaptive inference-time retrieval. Inspired by Information Foraging\nTheory (IFT), we propose InForage, a reinforcement learning framework that\nformalizes retrieval-augmented reasoning as a dynamic information-seeking\nprocess. Unlike existing approaches, InForage explicitly rewards intermediate\nretrieval quality, encouraging LLMs to iteratively gather and integrate\ninformation through adaptive search behaviors. To facilitate training, we\nconstruct a human-guided dataset capturing iterative search and reasoning\ntrajectories for complex, real-world web tasks. Extensive evaluations across\ngeneral question answering, multi-hop reasoning tasks, and a newly developed\nreal-time web QA dataset demonstrate InForage's superior performance over\nbaseline methods. These results highlight InForage's effectiveness in building\nrobust, adaptive, and efficient reasoning agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Augmenting large language models (LLMs) with external retrieval has become a\nstandard method to address their inherent knowledge cutoff limitations.\nHowever, traditional retrieval-augmented generation methods employ static,\npre-inference retrieval strategies, making them inadequate for complex tasks\ninvolving ambiguous, multi-step, or evolving information needs. Recent advances\nin test-time scaling techniques have demonstrated significant potential in\nenabling LLMs to dynamically interact with external tools, motivating the shift\ntoward adaptive inference-time retrieval. Inspired by Information Foraging\nTheory (IFT), we propose InForage, a reinforcement learning framework that\nformalizes retrieval-augmented reasoning as a dynamic information-seeking\nprocess. Unlike existing approaches, InForage explicitly rewards intermediate\nretrieval quality, encouraging LLMs to iteratively gather and integrate\ninformation through adaptive search behaviors. To facilitate training, we\nconstruct a human-guided dataset capturing iterative search and reasoning\ntrajectories for complex, real-world web tasks. Extensive evaluations across\ngeneral question answering, multi-hop reasoning tasks, and a newly developed\nreal-time web QA dataset demonstrate InForage's superior performance over\nbaseline methods. These results highlight InForage's effectiveness in building\nrobust, adaptive, and efficient reasoning agents."
                },
                "authors": [
                    {
                        "name": "Hongjin Qian"
                    },
                    {
                        "name": "Zheng Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zheng Liu"
                },
                "author": "Zheng Liu",
                "arxiv_comment": "16 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09316v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09316v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21620v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21620v4",
                "updated": "2025-05-14T11:56:07Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    11,
                    56,
                    7,
                    2,
                    134,
                    0
                ],
                "published": "2025-03-27T15:39:30Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    15,
                    39,
                    30,
                    3,
                    86,
                    0
                ],
                "title": "UI-R1: Enhancing Efficient Action Prediction of GUI Agents by\n  Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UI-R1: Enhancing Efficient Action Prediction of GUI Agents by\n  Reinforcement Learning"
                },
                "summary": "The recent DeepSeek-R1 has showcased the emergence of reasoning capabilities\nin LLMs through reinforcement learning (RL) with rule-based rewards. Despite\nits success in language models, its application in multi-modal domains,\nparticularly in graphic user interface (GUI) agent tasks, remains\nunder-explored. To address this issue, we propose UI-R1, the first framework to\nexplore how rule-based RL can enhance the reasoning capabilities of multimodal\nlarge language models (MLLMs) for GUI action prediction tasks. Specifically,\nUI-R1 introduces a novel rule-based action reward, enabling model optimization\nvia policy-based algorithms such as Group Relative Policy Optimization (GRPO).\nFor efficient training, we curate a small yet high-quality dataset of 136\nchallenging tasks, encompassing five common action types on mobile devices.\nExperimental results demonstrate that our proposed UI-R1-3B achieves\nsignificant improvements over the base model (i.e. Qwen2.5-VL-3B) on both\nin-domain (ID) and out-of-domain (OOD) tasks, with average accuracy gains of\n22.1% on ScreenSpot, 6.0% on ScreenSpot-Pro, and 12.7% on ANDROIDCONTROL.\nFurthermore, UI-R1-3B delivers competitive performance compared to larger\nmodels (e.g., OS-Atlas-7B) trained via supervised fine-tuning (SFT) on 76K\nsamples. We additionally develop an optimized version, UI-R1-E-3B, which\nsignificantly improves both grounding efficiency and accuracy. These results\nunderscore the potential of rule-based reinforcement learning to advance GUI\nunderstanding and control, paving the way for future research in this domain.\nCode website: https://github.com/lll6gg/UI-R1.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent DeepSeek-R1 has showcased the emergence of reasoning capabilities\nin LLMs through reinforcement learning (RL) with rule-based rewards. Despite\nits success in language models, its application in multi-modal domains,\nparticularly in graphic user interface (GUI) agent tasks, remains\nunder-explored. To address this issue, we propose UI-R1, the first framework to\nexplore how rule-based RL can enhance the reasoning capabilities of multimodal\nlarge language models (MLLMs) for GUI action prediction tasks. Specifically,\nUI-R1 introduces a novel rule-based action reward, enabling model optimization\nvia policy-based algorithms such as Group Relative Policy Optimization (GRPO).\nFor efficient training, we curate a small yet high-quality dataset of 136\nchallenging tasks, encompassing five common action types on mobile devices.\nExperimental results demonstrate that our proposed UI-R1-3B achieves\nsignificant improvements over the base model (i.e. Qwen2.5-VL-3B) on both\nin-domain (ID) and out-of-domain (OOD) tasks, with average accuracy gains of\n22.1% on ScreenSpot, 6.0% on ScreenSpot-Pro, and 12.7% on ANDROIDCONTROL.\nFurthermore, UI-R1-3B delivers competitive performance compared to larger\nmodels (e.g., OS-Atlas-7B) trained via supervised fine-tuning (SFT) on 76K\nsamples. We additionally develop an optimized version, UI-R1-E-3B, which\nsignificantly improves both grounding efficiency and accuracy. These results\nunderscore the potential of rule-based reinforcement learning to advance GUI\nunderstanding and control, paving the way for future research in this domain.\nCode website: https://github.com/lll6gg/UI-R1."
                },
                "authors": [
                    {
                        "name": "Zhengxi Lu"
                    },
                    {
                        "name": "Yuxiang Chai"
                    },
                    {
                        "name": "Yaxuan Guo"
                    },
                    {
                        "name": "Xi Yin"
                    },
                    {
                        "name": "Liang Liu"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Han Xiao"
                    },
                    {
                        "name": "Shuai Ren"
                    },
                    {
                        "name": "Guanjing Xiong"
                    },
                    {
                        "name": "Hongsheng Li"
                    }
                ],
                "author_detail": {
                    "name": "Hongsheng Li"
                },
                "author": "Hongsheng Li",
                "arxiv_comment": "Updated UI-R1-E-3B",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21620v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21620v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09304v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09304v1",
                "updated": "2025-05-14T11:39:47Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    11,
                    39,
                    47,
                    2,
                    134,
                    0
                ],
                "published": "2025-05-14T11:39:47Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    11,
                    39,
                    47,
                    2,
                    134,
                    0
                ],
                "title": "Adaptive Noise Resilient Keyword Spotting Using One-Shot Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Noise Resilient Keyword Spotting Using One-Shot Learning"
                },
                "summary": "Keyword spotting (KWS) is a key component of smart devices, enabling\nefficient and intuitive audio interaction. However, standard KWS systems\ndeployed on embedded devices often suffer performance degradation under\nreal-world operating conditions. Resilient KWS systems address this issue by\nenabling dynamic adaptation, with applications such as adding or replacing\nkeywords, adjusting to specific users, and improving noise robustness. However,\ndeploying resilient, standalone KWS systems with low latency on\nresource-constrained devices remains challenging due to limited memory and\ncomputational resources. This study proposes a low computational approach for\ncontinuous noise adaptation of pretrained neural networks used for KWS\nclassification, requiring only 1-shot learning and one epoch. The proposed\nmethod was assessed using two pretrained models and three real-world noise\nsources at signal-to-noise ratios (SNRs) ranging from 24 to -3 dB. The adapted\nmodels consistently outperformed the pretrained models across all scenarios,\nespecially at SNR $\\leq$ 18 dB, achieving accuracy improvements of 4.9% to\n46.0%. These results highlight the efficacy of the proposed methodology while\nbeing lightweight enough for deployment on resource-constrained devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Keyword spotting (KWS) is a key component of smart devices, enabling\nefficient and intuitive audio interaction. However, standard KWS systems\ndeployed on embedded devices often suffer performance degradation under\nreal-world operating conditions. Resilient KWS systems address this issue by\nenabling dynamic adaptation, with applications such as adding or replacing\nkeywords, adjusting to specific users, and improving noise robustness. However,\ndeploying resilient, standalone KWS systems with low latency on\nresource-constrained devices remains challenging due to limited memory and\ncomputational resources. This study proposes a low computational approach for\ncontinuous noise adaptation of pretrained neural networks used for KWS\nclassification, requiring only 1-shot learning and one epoch. The proposed\nmethod was assessed using two pretrained models and three real-world noise\nsources at signal-to-noise ratios (SNRs) ranging from 24 to -3 dB. The adapted\nmodels consistently outperformed the pretrained models across all scenarios,\nespecially at SNR $\\leq$ 18 dB, achieving accuracy improvements of 4.9% to\n46.0%. These results highlight the efficacy of the proposed methodology while\nbeing lightweight enough for deployment on resource-constrained devices."
                },
                "authors": [
                    {
                        "name": "Luciano Sebastian Martinez-Rau"
                    },
                    {
                        "name": "Quynh Nguyen Phuong Vu"
                    },
                    {
                        "name": "Yuxuan Zhang"
                    },
                    {
                        "name": "Bengt Oelmann"
                    },
                    {
                        "name": "Sebastian Bader"
                    }
                ],
                "author_detail": {
                    "name": "Sebastian Bader"
                },
                "author": "Sebastian Bader",
                "arxiv_comment": "Preprint submitted to the IEEE 11th World Forum on Internet of Things",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09304v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09304v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09289v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09289v1",
                "updated": "2025-05-14T11:15:14Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    11,
                    15,
                    14,
                    2,
                    134,
                    0
                ],
                "published": "2025-05-14T11:15:14Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    11,
                    15,
                    14,
                    2,
                    134,
                    0
                ],
                "title": "Reproducibility Study of \"Cooperate or Collapse: Emergence of\n  Sustainable Cooperation in a Society of LLM Agents\"",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reproducibility Study of \"Cooperate or Collapse: Emergence of\n  Sustainable Cooperation in a Society of LLM Agents\""
                },
                "summary": "This study evaluates and extends the findings made by Piatti et al., who\nintroduced GovSim, a simulation framework designed to assess the cooperative\ndecision-making capabilities of large language models (LLMs) in\nresource-sharing scenarios. By replicating key experiments, we validate claims\nregarding the performance of large models, such as GPT-4-turbo, compared to\nsmaller models. The impact of the universalization principle is also examined,\nwith results showing that large models can achieve sustainable cooperation,\nwith or without the principle, while smaller models fail without it. In\naddition, we provide multiple extensions to explore the applicability of the\nframework to new settings. We evaluate additional models, such as DeepSeek-V3\nand GPT-4o-mini, to test whether cooperative behavior generalizes across\ndifferent architectures and model sizes. Furthermore, we introduce new\nsettings: we create a heterogeneous multi-agent environment, study a scenario\nusing Japanese instructions, and explore an \"inverse environment\" where agents\nmust cooperate to mitigate harmful resource distributions. Our results confirm\nthat the benchmark can be applied to new models, scenarios, and languages,\noffering valuable insights into the adaptability of LLMs in complex cooperative\ntasks. Moreover, the experiment involving heterogeneous multi-agent systems\ndemonstrates that high-performing models can influence lower-performing ones to\nadopt similar behaviors. This finding has significant implications for other\nagent-based applications, potentially enabling more efficient use of\ncomputational resources and contributing to the development of more effective\ncooperative AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study evaluates and extends the findings made by Piatti et al., who\nintroduced GovSim, a simulation framework designed to assess the cooperative\ndecision-making capabilities of large language models (LLMs) in\nresource-sharing scenarios. By replicating key experiments, we validate claims\nregarding the performance of large models, such as GPT-4-turbo, compared to\nsmaller models. The impact of the universalization principle is also examined,\nwith results showing that large models can achieve sustainable cooperation,\nwith or without the principle, while smaller models fail without it. In\naddition, we provide multiple extensions to explore the applicability of the\nframework to new settings. We evaluate additional models, such as DeepSeek-V3\nand GPT-4o-mini, to test whether cooperative behavior generalizes across\ndifferent architectures and model sizes. Furthermore, we introduce new\nsettings: we create a heterogeneous multi-agent environment, study a scenario\nusing Japanese instructions, and explore an \"inverse environment\" where agents\nmust cooperate to mitigate harmful resource distributions. Our results confirm\nthat the benchmark can be applied to new models, scenarios, and languages,\noffering valuable insights into the adaptability of LLMs in complex cooperative\ntasks. Moreover, the experiment involving heterogeneous multi-agent systems\ndemonstrates that high-performing models can influence lower-performing ones to\nadopt similar behaviors. This finding has significant implications for other\nagent-based applications, potentially enabling more efficient use of\ncomputational resources and contributing to the development of more effective\ncooperative AI systems."
                },
                "authors": [
                    {
                        "name": "Pedro M. P. Curvo"
                    },
                    {
                        "name": "Mara Dragomir"
                    },
                    {
                        "name": "Salvador Torpes"
                    },
                    {
                        "name": "Mohammadmahdi Rahimi"
                    }
                ],
                "author_detail": {
                    "name": "Mohammadmahdi Rahimi"
                },
                "author": "Mohammadmahdi Rahimi",
                "arxiv_comment": "11 Tables, 9 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09289v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09289v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16393v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16393v2",
                "updated": "2025-05-14T11:10:47Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    11,
                    10,
                    47,
                    2,
                    134,
                    0
                ],
                "published": "2025-01-26T14:59:47Z",
                "published_parsed": [
                    2025,
                    1,
                    26,
                    14,
                    59,
                    47,
                    6,
                    26,
                    0
                ],
                "title": "Improving Network Threat Detection by Knowledge Graph, Large Language\n  Model, and Imbalanced Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Network Threat Detection by Knowledge Graph, Large Language\n  Model, and Imbalanced Learning"
                },
                "summary": "Network threat detection has been challenging due to the complexities of\nattack activities and the limitation of historical threat data to learn from.\nTo help enhance the existing practices of using analytics, machine learning,\nand artificial intelligence methods to detect the network threats, we propose\nan integrated modelling framework, where Knowledge Graph is used to analyze the\nusers' activity patterns, Imbalanced Learning techniques are used to prune and\nweigh Knowledge Graph, and LLM is used to retrieve and interpret the users'\nactivities from Knowledge Graph. The proposed framework is applied to Agile\nThreat Detection through Online Sequential Learning. The preliminary results\nshow the improved threat capture rate by 3%-4% and the increased\ninterpretabilities of risk predictions based on the users' activities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Network threat detection has been challenging due to the complexities of\nattack activities and the limitation of historical threat data to learn from.\nTo help enhance the existing practices of using analytics, machine learning,\nand artificial intelligence methods to detect the network threats, we propose\nan integrated modelling framework, where Knowledge Graph is used to analyze the\nusers' activity patterns, Imbalanced Learning techniques are used to prune and\nweigh Knowledge Graph, and LLM is used to retrieve and interpret the users'\nactivities from Knowledge Graph. The proposed framework is applied to Agile\nThreat Detection through Online Sequential Learning. The preliminary results\nshow the improved threat capture rate by 3%-4% and the increased\ninterpretabilities of risk predictions based on the users' activities."
                },
                "authors": [
                    {
                        "name": "Lili Zhang"
                    },
                    {
                        "name": "Quanyan Zhu"
                    },
                    {
                        "name": "Herman Ray"
                    },
                    {
                        "name": "Ying Xie"
                    }
                ],
                "author_detail": {
                    "name": "Ying Xie"
                },
                "author": "Ying Xie",
                "arxiv_comment": "Accepted by \"Combining AI and OR/MS for Better Trustworthy Decision\n  Making\" Bridge Program co-organized by AAAI and INFORMS as poster and demo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16393v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16393v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09283v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09283v1",
                "updated": "2025-05-14T11:08:45Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    11,
                    8,
                    45,
                    2,
                    134,
                    0
                ],
                "published": "2025-05-14T11:08:45Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    11,
                    8,
                    45,
                    2,
                    134,
                    0
                ],
                "title": "A Note on Semantic Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Note on Semantic Diffusion"
                },
                "summary": "This paper provides an in-depth examination of the concept of semantic\ndiffusion as a complementary instrument to large language models (LLMs) for\ndesign applications. Conventional LLMs and diffusion models fail to induce a\nconvergent, iterative refinement process: each invocation of the diffusion\nmechanism spawns a new stochastic cycle, so successive outputs do not relate to\nprior ones and convergence toward a desired design is not guaranteed. The\nproposed hybrid framework - \"LLM + semantic diffusion\" - resolves this\nlimitation by enforcing an approximately convergent search procedure, thereby\nformally addressing the problem of localized design refinement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper provides an in-depth examination of the concept of semantic\ndiffusion as a complementary instrument to large language models (LLMs) for\ndesign applications. Conventional LLMs and diffusion models fail to induce a\nconvergent, iterative refinement process: each invocation of the diffusion\nmechanism spawns a new stochastic cycle, so successive outputs do not relate to\nprior ones and convergence toward a desired design is not guaranteed. The\nproposed hybrid framework - \"LLM + semantic diffusion\" - resolves this\nlimitation by enforcing an approximately convergent search procedure, thereby\nformally addressing the problem of localized design refinement."
                },
                "authors": [
                    {
                        "name": "Alexander P. Ryjov"
                    },
                    {
                        "name": "Alina A. Egorova"
                    }
                ],
                "author_detail": {
                    "name": "Alina A. Egorova"
                },
                "author": "Alina A. Egorova",
                "arxiv_comment": "8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09283v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09283v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "03E72, 68T37, 94D05",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.8; I.2.10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17058v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17058v2",
                "updated": "2025-05-14T11:00:39Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    11,
                    0,
                    39,
                    2,
                    134,
                    0
                ],
                "published": "2024-11-26T02:57:28Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    2,
                    57,
                    28,
                    1,
                    331,
                    0
                ],
                "title": "ThreatModeling-LLM: Automating Threat Modeling using Large Language\n  Models for Banking System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ThreatModeling-LLM: Automating Threat Modeling using Large Language\n  Models for Banking System"
                },
                "summary": "Threat modeling is a crucial component of cybersecurity, particularly for\nindustries such as banking, where the security of financial data is paramount.\nTraditional threat modeling approaches require expert intervention and manual\neffort, often leading to inefficiencies and human error. The advent of Large\nLanguage Models (LLMs) offers a promising avenue for automating these\nprocesses, enhancing both efficiency and efficacy. However, this transition is\nnot straightforward due to three main challenges: (1) the lack of publicly\navailable, domain-specific datasets, (2) the need for tailored models to handle\ncomplex banking system architectures, and (3) the requirement for real-time,\nadaptive mitigation strategies that align with compliance standards like NIST\n800-53. In this paper, we introduce ThreatModeling-LLM, a novel and adaptable\nframework that automates threat modeling for banking systems using LLMs.\nThreatModeling-LLM operates in three stages: 1) dataset creation, 2) prompt\nengineering and 3) model fine-tuning. We first generate a benchmark dataset\nusing Microsoft Threat Modeling Tool (TMT). Then, we apply Chain of Thought\n(CoT) and Optimization by PROmpting (OPRO) on the pre-trained LLMs to optimize\nthe initial prompt. Lastly, we fine-tune the LLM using Low-Rank Adaptation\n(LoRA) based on the benchmark dataset and the optimized prompt to improve the\nthreat identification and mitigation generation capabilities of pre-trained\nLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Threat modeling is a crucial component of cybersecurity, particularly for\nindustries such as banking, where the security of financial data is paramount.\nTraditional threat modeling approaches require expert intervention and manual\neffort, often leading to inefficiencies and human error. The advent of Large\nLanguage Models (LLMs) offers a promising avenue for automating these\nprocesses, enhancing both efficiency and efficacy. However, this transition is\nnot straightforward due to three main challenges: (1) the lack of publicly\navailable, domain-specific datasets, (2) the need for tailored models to handle\ncomplex banking system architectures, and (3) the requirement for real-time,\nadaptive mitigation strategies that align with compliance standards like NIST\n800-53. In this paper, we introduce ThreatModeling-LLM, a novel and adaptable\nframework that automates threat modeling for banking systems using LLMs.\nThreatModeling-LLM operates in three stages: 1) dataset creation, 2) prompt\nengineering and 3) model fine-tuning. We first generate a benchmark dataset\nusing Microsoft Threat Modeling Tool (TMT). Then, we apply Chain of Thought\n(CoT) and Optimization by PROmpting (OPRO) on the pre-trained LLMs to optimize\nthe initial prompt. Lastly, we fine-tune the LLM using Low-Rank Adaptation\n(LoRA) based on the benchmark dataset and the optimized prompt to improve the\nthreat identification and mitigation generation capabilities of pre-trained\nLLMs."
                },
                "authors": [
                    {
                        "name": "Tingmin Wu"
                    },
                    {
                        "name": "Shuiqiao Yang"
                    },
                    {
                        "name": "Shigang Liu"
                    },
                    {
                        "name": "David Nguyen"
                    },
                    {
                        "name": "Seung Jang"
                    },
                    {
                        "name": "Alsharif Abuadbba"
                    }
                ],
                "author_detail": {
                    "name": "Alsharif Abuadbba"
                },
                "author": "Alsharif Abuadbba",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17058v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17058v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16472v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16472v2",
                "updated": "2025-05-14T10:55:20Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    10,
                    55,
                    20,
                    2,
                    134,
                    0
                ],
                "published": "2025-04-23T07:32:43Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    7,
                    32,
                    43,
                    2,
                    113,
                    0
                ],
                "title": "Harden and Catch for Just-in-Time Assured LLM-Based Software Testing:\n  Open Research Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Harden and Catch for Just-in-Time Assured LLM-Based Software Testing:\n  Open Research Challenges"
                },
                "summary": "Despite decades of research and practice in automated software testing,\nseveral fundamental concepts remain ill-defined and under-explored, yet offer\nenormous potential real-world impact. We show that these concepts raise\nexciting new challenges in the context of Large Language Models for software\ntest generation. More specifically, we formally define and investigate the\nproperties of hardening and catching tests. A hardening test is one that seeks\nto protect against future regressions, while a catching test is one that\ncatches such a regression or a fault in new functionality introduced by a code\nchange. Hardening tests can be generated at any time and may become catching\ntests when a future regression is caught. We also define and motivate the\nCatching 'Just-in-Time' (JiTTest) Challenge, in which tests are generated\n'just-in-time' to catch new faults before they land into production. We show\nthat any solution to Catching JiTTest generation can also be repurposed to\ncatch latent faults in legacy code. We enumerate possible outcomes for\nhardening and catching tests and JiTTests, and discuss open research problems,\ndeployment options, and initial results from our work on automated LLM-based\nhardening at Meta. This paper was written to accompany the keynote by the\nauthors at the ACM International Conference on the Foundations of Software\nEngineering (FSE) 2025. Author order is alphabetical. The corresponding author\nis Mark Harman.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite decades of research and practice in automated software testing,\nseveral fundamental concepts remain ill-defined and under-explored, yet offer\nenormous potential real-world impact. We show that these concepts raise\nexciting new challenges in the context of Large Language Models for software\ntest generation. More specifically, we formally define and investigate the\nproperties of hardening and catching tests. A hardening test is one that seeks\nto protect against future regressions, while a catching test is one that\ncatches such a regression or a fault in new functionality introduced by a code\nchange. Hardening tests can be generated at any time and may become catching\ntests when a future regression is caught. We also define and motivate the\nCatching 'Just-in-Time' (JiTTest) Challenge, in which tests are generated\n'just-in-time' to catch new faults before they land into production. We show\nthat any solution to Catching JiTTest generation can also be repurposed to\ncatch latent faults in legacy code. We enumerate possible outcomes for\nhardening and catching tests and JiTTests, and discuss open research problems,\ndeployment options, and initial results from our work on automated LLM-based\nhardening at Meta. This paper was written to accompany the keynote by the\nauthors at the ACM International Conference on the Foundations of Software\nEngineering (FSE) 2025. Author order is alphabetical. The corresponding author\nis Mark Harman."
                },
                "authors": [
                    {
                        "name": "Mark Harman"
                    },
                    {
                        "name": "Peter O'Hearn"
                    },
                    {
                        "name": "Shubho Sengupta"
                    }
                ],
                "author_detail": {
                    "name": "Shubho Sengupta"
                },
                "author": "Shubho Sengupta",
                "arxiv_comment": "To Appear as keynote paper at FSE 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16472v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16472v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17599v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17599v2",
                "updated": "2025-05-14T10:25:11Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    10,
                    25,
                    11,
                    2,
                    134,
                    0
                ],
                "published": "2025-03-22T01:02:44Z",
                "published_parsed": [
                    2025,
                    3,
                    22,
                    1,
                    2,
                    44,
                    5,
                    81,
                    0
                ],
                "title": "Evaluating Clinical Competencies of Large Language Models with a General\n  Practice Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Clinical Competencies of Large Language Models with a General\n  Practice Benchmark"
                },
                "summary": "Large Language Models (LLMs) have demonstrated considerable potential in\ngeneral practice. However, existing benchmarks and evaluation frameworks\nprimarily depend on exam-style or simplified question-answer formats, lacking a\ncompetency-based structure aligned with the real-world clinical\nresponsibilities encountered in general practice. Consequently, the extent to\nwhich LLMs can reliably fulfill the duties of general practitioners (GPs)\nremains uncertain. In this work, we propose a novel evaluation framework to\nassess the capability of LLMs to function as GPs. Based on this framework, we\nintroduce a general practice benchmark (GPBench), whose data are meticulously\nannotated by domain experts in accordance with routine clinical practice\nstandards. We evaluate ten state-of-the-art LLMs and analyze their\ncompetencies. Our findings indicate that current LLMs are not yet ready for\ndeployment in such settings without human oversight, and further optimization\nspecifically tailored to the daily responsibilities of GPs is essential.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated considerable potential in\ngeneral practice. However, existing benchmarks and evaluation frameworks\nprimarily depend on exam-style or simplified question-answer formats, lacking a\ncompetency-based structure aligned with the real-world clinical\nresponsibilities encountered in general practice. Consequently, the extent to\nwhich LLMs can reliably fulfill the duties of general practitioners (GPs)\nremains uncertain. In this work, we propose a novel evaluation framework to\nassess the capability of LLMs to function as GPs. Based on this framework, we\nintroduce a general practice benchmark (GPBench), whose data are meticulously\nannotated by domain experts in accordance with routine clinical practice\nstandards. We evaluate ten state-of-the-art LLMs and analyze their\ncompetencies. Our findings indicate that current LLMs are not yet ready for\ndeployment in such settings without human oversight, and further optimization\nspecifically tailored to the daily responsibilities of GPs is essential."
                },
                "authors": [
                    {
                        "name": "Zheqing Li"
                    },
                    {
                        "name": "Yiying Yang"
                    },
                    {
                        "name": "Jiping Lang"
                    },
                    {
                        "name": "Wenhao Jiang"
                    },
                    {
                        "name": "Yuhang Zhao"
                    },
                    {
                        "name": "Shuang Li"
                    },
                    {
                        "name": "Dingqian Wang"
                    },
                    {
                        "name": "Zhu Lin"
                    },
                    {
                        "name": "Xuanna Li"
                    },
                    {
                        "name": "Yuze Tang"
                    },
                    {
                        "name": "Jiexian Qiu"
                    },
                    {
                        "name": "Xiaolin Lu"
                    },
                    {
                        "name": "Hongji Yu"
                    },
                    {
                        "name": "Shuang Chen"
                    },
                    {
                        "name": "Yuhua Bi"
                    },
                    {
                        "name": "Xiaofei Zeng"
                    },
                    {
                        "name": "Yixian Chen"
                    },
                    {
                        "name": "Junrong Chen"
                    },
                    {
                        "name": "Lin Yao"
                    }
                ],
                "author_detail": {
                    "name": "Lin Yao"
                },
                "author": "Lin Yao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17599v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17599v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09261v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09261v1",
                "updated": "2025-05-14T10:22:13Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    10,
                    22,
                    13,
                    2,
                    134,
                    0
                ],
                "published": "2025-05-14T10:22:13Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    10,
                    22,
                    13,
                    2,
                    134,
                    0
                ],
                "title": "Instantiating Standards: Enabling Standard-Driven Text TTP Extraction\n  with Evolvable Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instantiating Standards: Enabling Standard-Driven Text TTP Extraction\n  with Evolvable Memory"
                },
                "summary": "Extracting MITRE ATT\\&CK Tactics, Techniques, and Procedures (TTPs) from\nnatural language threat reports is crucial yet challenging. Existing methods\nprimarily focus on performance metrics using data-driven approaches, often\nneglecting mechanisms to ensure faithful adherence to the official standard.\nThis deficiency compromises reliability and consistency of TTP assignments,\ncreating intelligence silos and contradictory threat assessments across\norganizations. To address this, we introduce a novel framework that converts\nabstract standard definitions into actionable, contextualized knowledge. Our\nmethod utilizes Large Language Model (LLM) to generate, update, and apply this\nknowledge. This framework populates an evolvable memory with dual-layer\nsituational knowledge instances derived from labeled examples and official\ndefinitions. The first layer identifies situational contexts (e.g.,\n\"Communication with C2 using encoded subdomains\"), while the second layer\ncaptures distinctive features that differentiate similar techniques (e.g.,\ndistinguishing T1132 \"Data Encoding\" from T1071 \"Application Layer Protocol\"\nbased on whether the focus is on encoding methods or protocol usage). This\nstructured approach provides a transparent basis for explainable TTP\nassignments and enhanced human oversight, while also helping to standardize\nother TTP extraction systems. Experiments show our framework (using\nQwen2.5-32B) boosts Technique F1 scores by 11\\% over GPT-4o. Qualitative\nanalysis confirms superior standardization, enhanced transparency, and improved\nexplainability in real-world threat intelligence scenarios. To the best of our\nknowledge, this is the first work that uses the LLM to generate, update, and\napply the a new knowledge for TTP extraction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extracting MITRE ATT\\&CK Tactics, Techniques, and Procedures (TTPs) from\nnatural language threat reports is crucial yet challenging. Existing methods\nprimarily focus on performance metrics using data-driven approaches, often\nneglecting mechanisms to ensure faithful adherence to the official standard.\nThis deficiency compromises reliability and consistency of TTP assignments,\ncreating intelligence silos and contradictory threat assessments across\norganizations. To address this, we introduce a novel framework that converts\nabstract standard definitions into actionable, contextualized knowledge. Our\nmethod utilizes Large Language Model (LLM) to generate, update, and apply this\nknowledge. This framework populates an evolvable memory with dual-layer\nsituational knowledge instances derived from labeled examples and official\ndefinitions. The first layer identifies situational contexts (e.g.,\n\"Communication with C2 using encoded subdomains\"), while the second layer\ncaptures distinctive features that differentiate similar techniques (e.g.,\ndistinguishing T1132 \"Data Encoding\" from T1071 \"Application Layer Protocol\"\nbased on whether the focus is on encoding methods or protocol usage). This\nstructured approach provides a transparent basis for explainable TTP\nassignments and enhanced human oversight, while also helping to standardize\nother TTP extraction systems. Experiments show our framework (using\nQwen2.5-32B) boosts Technique F1 scores by 11\\% over GPT-4o. Qualitative\nanalysis confirms superior standardization, enhanced transparency, and improved\nexplainability in real-world threat intelligence scenarios. To the best of our\nknowledge, this is the first work that uses the LLM to generate, update, and\napply the a new knowledge for TTP extraction."
                },
                "authors": [
                    {
                        "name": "Cheng Meng"
                    },
                    {
                        "name": "ZhengWei Jiang"
                    },
                    {
                        "name": "QiuYun Wang"
                    },
                    {
                        "name": "XinYi Li"
                    },
                    {
                        "name": "ChunYan Ma"
                    },
                    {
                        "name": "FangMing Dong"
                    },
                    {
                        "name": "FangLi Ren"
                    },
                    {
                        "name": "BaoXu Liu"
                    }
                ],
                "author_detail": {
                    "name": "BaoXu Liu"
                },
                "author": "BaoXu Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09261v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09261v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.04526v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.04526v4",
                "updated": "2025-05-15T02:17:31Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    2,
                    17,
                    31,
                    3,
                    135,
                    0
                ],
                "published": "2024-10-06T15:41:26Z",
                "published_parsed": [
                    2024,
                    10,
                    6,
                    15,
                    41,
                    26,
                    6,
                    280,
                    0
                ],
                "title": "FAMMA: A Benchmark for Financial Domain Multilingual Multimodal Question\n  Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FAMMA: A Benchmark for Financial Domain Multilingual Multimodal Question\n  Answering"
                },
                "summary": "In this paper, we introduce FAMMA, an open-source benchmark for\n\\underline{f}in\\underline{a}ncial \\underline{m}ultilingual\n\\underline{m}ultimodal question \\underline{a}nswering (QA). Our benchmark aims\nto evaluate the abilities of large language models (LLMs) in answering complex\nreasoning questions that require advanced financial knowledge. The benchmark\nhas two versions: FAMMA-Basic consists of 1,945 questions extracted from\nuniversity textbooks and exams, along with human-annotated answers and\nrationales; FAMMA-LivePro consists of 103 novel questions created by human\ndomain experts, with answers and rationales held out from the public for a\ncontamination-free evaluation. These questions cover advanced knowledge of 8\nmajor subfields in finance (e.g., corporate finance, derivatives, and portfolio\nmanagement). Some are in Chinese or French, while a majority of them are in\nEnglish. Each question has some non-text data such as charts, diagrams, or\ntables. Our experiments reveal that FAMMA poses a significant challenge on\nLLMs, including reasoning models such as GPT-o1 and DeepSeek-R1. Additionally,\nwe curated 1,270 reasoning trajectories of DeepSeek-R1 on the FAMMA-Basic data,\nand fine-tuned a series of open-source Qwen models using this reasoning data.\nWe found that training a model on these reasoning trajectories can\nsignificantly improve its performance on FAMMA-LivePro. We released our\nleaderboard, data, code, and trained models at\nhttps://famma-bench.github.io/famma/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce FAMMA, an open-source benchmark for\n\\underline{f}in\\underline{a}ncial \\underline{m}ultilingual\n\\underline{m}ultimodal question \\underline{a}nswering (QA). Our benchmark aims\nto evaluate the abilities of large language models (LLMs) in answering complex\nreasoning questions that require advanced financial knowledge. The benchmark\nhas two versions: FAMMA-Basic consists of 1,945 questions extracted from\nuniversity textbooks and exams, along with human-annotated answers and\nrationales; FAMMA-LivePro consists of 103 novel questions created by human\ndomain experts, with answers and rationales held out from the public for a\ncontamination-free evaluation. These questions cover advanced knowledge of 8\nmajor subfields in finance (e.g., corporate finance, derivatives, and portfolio\nmanagement). Some are in Chinese or French, while a majority of them are in\nEnglish. Each question has some non-text data such as charts, diagrams, or\ntables. Our experiments reveal that FAMMA poses a significant challenge on\nLLMs, including reasoning models such as GPT-o1 and DeepSeek-R1. Additionally,\nwe curated 1,270 reasoning trajectories of DeepSeek-R1 on the FAMMA-Basic data,\nand fine-tuned a series of open-source Qwen models using this reasoning data.\nWe found that training a model on these reasoning trajectories can\nsignificantly improve its performance on FAMMA-LivePro. We released our\nleaderboard, data, code, and trained models at\nhttps://famma-bench.github.io/famma/."
                },
                "authors": [
                    {
                        "name": "Siqiao Xue"
                    },
                    {
                        "name": "Xiaojing Li"
                    },
                    {
                        "name": "Fan Zhou"
                    },
                    {
                        "name": "Qingyang Dai"
                    },
                    {
                        "name": "Zhixuan Chu"
                    },
                    {
                        "name": "Hongyuan Mei"
                    }
                ],
                "author_detail": {
                    "name": "Hongyuan Mei"
                },
                "author": "Hongyuan Mei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.04526v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.04526v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13881v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13881v3",
                "updated": "2025-05-14T10:12:27Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    10,
                    12,
                    27,
                    2,
                    134,
                    0
                ],
                "published": "2025-01-23T17:56:07Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    17,
                    56,
                    7,
                    3,
                    23,
                    0
                ],
                "title": "The machine learning platform for developers of large systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The machine learning platform for developers of large systems"
                },
                "summary": "The machine learning system in the form of Retrieval Augmented Generation\n(RAG) has developed steadily since about 2021. RAG could be observed as a\nversion of the knowledge transfer. In the studied case, the large computing\nsystems are observed as the application point of RAG, which includes large\nlanguage model (LLM), as a partner for the developing team. Such an approach\nhas advantages during the development process and further in exploitation time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The machine learning system in the form of Retrieval Augmented Generation\n(RAG) has developed steadily since about 2021. RAG could be observed as a\nversion of the knowledge transfer. In the studied case, the large computing\nsystems are observed as the application point of RAG, which includes large\nlanguage model (LLM), as a partner for the developing team. Such an approach\nhas advantages during the development process and further in exploitation time."
                },
                "authors": [
                    {
                        "name": "Alexey Naikov"
                    },
                    {
                        "name": "Anatoly Oreshkin"
                    },
                    {
                        "name": "Alexey Shvetsov"
                    },
                    {
                        "name": "Andrey Shevel"
                    }
                ],
                "author_detail": {
                    "name": "Andrey Shevel"
                },
                "author": "Andrey Shevel",
                "arxiv_comment": "5 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13881v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13881v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.gen-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.gen-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "J.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09246v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09246v1",
                "updated": "2025-05-14T09:35:56Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    9,
                    35,
                    56,
                    2,
                    134,
                    0
                ],
                "published": "2025-05-14T09:35:56Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    9,
                    35,
                    56,
                    2,
                    134,
                    0
                ],
                "title": "Focus, Merge, Rank: Improved Question Answering Based on Semi-structured\n  Knowledge Bases",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Focus, Merge, Rank: Improved Question Answering Based on Semi-structured\n  Knowledge Bases"
                },
                "summary": "In many real-world settings, machine learning models and interactive systems\nhave access to both structured knowledge, e.g., knowledge graphs or tables, and\nunstructured content, e.g., natural language documents. However, most rely on\neither. Semi-Structured Knowledge Bases (SKBs) bridge this gap by linking\nunstructured content to nodes within structured data, thereby enabling new\nstrategies for knowledge access and use. In this work, we present\nFocusedRetriever, a modular SKB-based framework for multi-hop question\nanswering. It integrates components (VSS-based entity search, LLM-based\ngeneration of Cypher queries and pairwise re-ranking) in a way that enables it\nto outperform state-of-the-art methods across all three STaRK benchmark test\nsets, covering diverse domains and multiple performance metrics. The average\nfirst-hit rate exceeds that of the second-best method by 25.7%.\nFocusedRetriever leverages (1) the capacity of Large Language Models (LLMs) to\nextract relational facts and entity attributes from unstructured text, (2) node\nset joins to filter answer candidates based on these extracted triplets and\nconstraints, (3) vector similarity search to retrieve and rank relevant\nunstructured content, and (4) the contextual capabilities of LLMs to finally\nrank the top-k answers. For generality, we only incorporate base LLMs in\nFocusedRetriever in our evaluation. However, our analysis of intermediate\nresults highlights several opportunities for further upgrades including\nfinetuning. The source code is publicly available at\nhttps://github.com/kramerlab/FocusedRetriever .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In many real-world settings, machine learning models and interactive systems\nhave access to both structured knowledge, e.g., knowledge graphs or tables, and\nunstructured content, e.g., natural language documents. However, most rely on\neither. Semi-Structured Knowledge Bases (SKBs) bridge this gap by linking\nunstructured content to nodes within structured data, thereby enabling new\nstrategies for knowledge access and use. In this work, we present\nFocusedRetriever, a modular SKB-based framework for multi-hop question\nanswering. It integrates components (VSS-based entity search, LLM-based\ngeneration of Cypher queries and pairwise re-ranking) in a way that enables it\nto outperform state-of-the-art methods across all three STaRK benchmark test\nsets, covering diverse domains and multiple performance metrics. The average\nfirst-hit rate exceeds that of the second-best method by 25.7%.\nFocusedRetriever leverages (1) the capacity of Large Language Models (LLMs) to\nextract relational facts and entity attributes from unstructured text, (2) node\nset joins to filter answer candidates based on these extracted triplets and\nconstraints, (3) vector similarity search to retrieve and rank relevant\nunstructured content, and (4) the contextual capabilities of LLMs to finally\nrank the top-k answers. For generality, we only incorporate base LLMs in\nFocusedRetriever in our evaluation. However, our analysis of intermediate\nresults highlights several opportunities for further upgrades including\nfinetuning. The source code is publicly available at\nhttps://github.com/kramerlab/FocusedRetriever ."
                },
                "authors": [
                    {
                        "name": "Derian Boer"
                    },
                    {
                        "name": "Stephen Roth"
                    },
                    {
                        "name": "Stefan Kramer"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Kramer"
                },
                "author": "Stefan Kramer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09246v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09246v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09239v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09239v1",
                "updated": "2025-05-14T09:27:09Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    9,
                    27,
                    9,
                    2,
                    134,
                    0
                ],
                "published": "2025-05-14T09:27:09Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    9,
                    27,
                    9,
                    2,
                    134,
                    0
                ],
                "title": "Stable and Convexified Information Bottleneck Optimization via Symbolic\n  Continuation and Entropy-Regularized Trajectories",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stable and Convexified Information Bottleneck Optimization via Symbolic\n  Continuation and Entropy-Regularized Trajectories"
                },
                "summary": "The Information Bottleneck (IB) method frequently suffers from unstable\noptimization, characterized by abrupt representation shifts near critical\npoints of the IB trade-off parameter, beta. In this paper, I introduce a novel\napproach to achieve stable and convex IB optimization through symbolic\ncontinuation and entropy-regularized trajectories. I analytically prove\nconvexity and uniqueness of the IB solution path when an entropy regularization\nterm is included, and demonstrate how this stabilizes representation learning\nacross a wide range of \\b{eta} values. Additionally, I provide extensive\nsensitivity analyses around critical points (beta) with statistically robust\nuncertainty quantification (95% confidence intervals). The open-source\nimplementation, experimental results, and reproducibility framework included in\nthis work offer a clear path for practical deployment and future extension of\nmy proposed method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Information Bottleneck (IB) method frequently suffers from unstable\noptimization, characterized by abrupt representation shifts near critical\npoints of the IB trade-off parameter, beta. In this paper, I introduce a novel\napproach to achieve stable and convex IB optimization through symbolic\ncontinuation and entropy-regularized trajectories. I analytically prove\nconvexity and uniqueness of the IB solution path when an entropy regularization\nterm is included, and demonstrate how this stabilizes representation learning\nacross a wide range of \\b{eta} values. Additionally, I provide extensive\nsensitivity analyses around critical points (beta) with statistically robust\nuncertainty quantification (95% confidence intervals). The open-source\nimplementation, experimental results, and reproducibility framework included in\nthis work offer a clear path for practical deployment and future extension of\nmy proposed method."
                },
                "authors": [
                    {
                        "name": "Faruk Alpay"
                    }
                ],
                "author_detail": {
                    "name": "Faruk Alpay"
                },
                "author": "Faruk Alpay",
                "arxiv_comment": "23 pages, 11 figures, includes analytical proofs, sensitivity\n  analysis (95% CI), and JAX-based open-source implementation available at:\n  https://github.com/farukalpay/information-bottleneck-beta-optimization",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09239v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09239v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T05, 90C25, 94A15",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.6; G.1.6; H.1.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09214v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09214v1",
                "updated": "2025-05-14T08:18:55Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    8,
                    18,
                    55,
                    2,
                    134,
                    0
                ],
                "published": "2025-05-14T08:18:55Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    8,
                    18,
                    55,
                    2,
                    134,
                    0
                ],
                "title": "The Larger the Merrier? Efficient Large AI Model Inference in Wireless\n  Edge Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Larger the Merrier? Efficient Large AI Model Inference in Wireless\n  Edge Networks"
                },
                "summary": "The growing demand for large artificial intelligence model (LAIM) services is\ndriving a paradigm shift from traditional cloud-based inference to edge-based\ninference for low-latency, privacy-preserving applications. In particular,\nedge-device co-inference, which partitions LAIMs between edge devices and\nservers, has emerged as a promising strategy for resource-efficient LAIM\nexecution in wireless networks. In this paper, we investigate a pruning-aware\nLAIM co-inference scheme, where a pre-trained LAIM is pruned and partitioned\ninto on-device and on-server sub-models for deployment. For analysis, we first\nprove that the LAIM output distortion is upper bounded by its parameter\ndistortion. Then, we derive a lower bound on parameter distortion via\nrate-distortion theory, analytically capturing the relationship between pruning\nratio and co-inference performance. Next, based on the analytical results, we\nformulate an LAIM co-inference distortion bound minimization problem by jointly\noptimizing the pruning ratio, transmit power, and computation frequency under\nsystem latency, energy, and available resource constraints. Moreover, we\npropose an efficient algorithm to tackle the considered highly non-convex\nproblem. Finally, extensive simulations demonstrate the effectiveness of the\nproposed design. In particular, model parameter distortion is shown to provide\na reliable bound on output distortion. Also, the proposed joint pruning ratio\nand resource management design achieves superior performance in balancing\ntrade-offs among inference performance, system latency, and energy consumption\ncompared with benchmark schemes, such as fully on-device and on-server\ninference. Moreover, the split point is shown to play a critical role in system\nperformance optimization under heterogeneous and resource-limited edge\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing demand for large artificial intelligence model (LAIM) services is\ndriving a paradigm shift from traditional cloud-based inference to edge-based\ninference for low-latency, privacy-preserving applications. In particular,\nedge-device co-inference, which partitions LAIMs between edge devices and\nservers, has emerged as a promising strategy for resource-efficient LAIM\nexecution in wireless networks. In this paper, we investigate a pruning-aware\nLAIM co-inference scheme, where a pre-trained LAIM is pruned and partitioned\ninto on-device and on-server sub-models for deployment. For analysis, we first\nprove that the LAIM output distortion is upper bounded by its parameter\ndistortion. Then, we derive a lower bound on parameter distortion via\nrate-distortion theory, analytically capturing the relationship between pruning\nratio and co-inference performance. Next, based on the analytical results, we\nformulate an LAIM co-inference distortion bound minimization problem by jointly\noptimizing the pruning ratio, transmit power, and computation frequency under\nsystem latency, energy, and available resource constraints. Moreover, we\npropose an efficient algorithm to tackle the considered highly non-convex\nproblem. Finally, extensive simulations demonstrate the effectiveness of the\nproposed design. In particular, model parameter distortion is shown to provide\na reliable bound on output distortion. Also, the proposed joint pruning ratio\nand resource management design achieves superior performance in balancing\ntrade-offs among inference performance, system latency, and energy consumption\ncompared with benchmark schemes, such as fully on-device and on-server\ninference. Moreover, the split point is shown to play a critical role in system\nperformance optimization under heterogeneous and resource-limited edge\nenvironments."
                },
                "authors": [
                    {
                        "name": "Zhonghao Lyu"
                    },
                    {
                        "name": "Ming Xiao"
                    },
                    {
                        "name": "Jie Xu"
                    },
                    {
                        "name": "Mikael Skoglund"
                    },
                    {
                        "name": "Marco Di Renzo"
                    }
                ],
                "author_detail": {
                    "name": "Marco Di Renzo"
                },
                "author": "Marco Di Renzo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09214v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09214v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09210v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09210v1",
                "updated": "2025-05-14T08:04:02Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    8,
                    4,
                    2,
                    2,
                    134,
                    0
                ],
                "published": "2025-05-14T08:04:02Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    8,
                    4,
                    2,
                    2,
                    134,
                    0
                ],
                "title": "Harnessing self-sensitized scintillation by supramolecular engineering\n  of CsPbBr3 nanocrystals in dense mesoporous template nanospheres",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Harnessing self-sensitized scintillation by supramolecular engineering\n  of CsPbBr3 nanocrystals in dense mesoporous template nanospheres"
                },
                "summary": "Perovskite-based nanoscintillators, such as CsPbBr3 nanocrystals (NCs), are\nemerging as promising candidates for ionizing radiation detection, thanks to\ntheir high emission efficiency, rapid response, and facile synthesis. However,\ntheir nanoscale dimensions - smaller than the mean free path of secondary\ncarriers - and relatively low emitter density per unit volume, limited by their\nhigh molecular weight and reabsorption losses, restrict efficient secondary\ncarrier conversion and hamper their practical deployment. In this work, we\nintroduce a strategy to enhance scintillation performance by organizing NCs\ninto densely packed domains within porous SiO2 mesospheres (MSNs). This\nengineered architecture achieves up to a 40-fold increase in radioluminescence\nintensity compared to colloidal NCs, driven by improved retention and\nconversion of secondary charges, as corroborated by electron release\nmeasurements. This approach offers a promising pathway toward developing\nnext-generation nanoscintillators with enhanced performance, with potential\napplications in high-energy physics, medical imaging, and space technologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Perovskite-based nanoscintillators, such as CsPbBr3 nanocrystals (NCs), are\nemerging as promising candidates for ionizing radiation detection, thanks to\ntheir high emission efficiency, rapid response, and facile synthesis. However,\ntheir nanoscale dimensions - smaller than the mean free path of secondary\ncarriers - and relatively low emitter density per unit volume, limited by their\nhigh molecular weight and reabsorption losses, restrict efficient secondary\ncarrier conversion and hamper their practical deployment. In this work, we\nintroduce a strategy to enhance scintillation performance by organizing NCs\ninto densely packed domains within porous SiO2 mesospheres (MSNs). This\nengineered architecture achieves up to a 40-fold increase in radioluminescence\nintensity compared to colloidal NCs, driven by improved retention and\nconversion of secondary charges, as corroborated by electron release\nmeasurements. This approach offers a promising pathway toward developing\nnext-generation nanoscintillators with enhanced performance, with potential\napplications in high-energy physics, medical imaging, and space technologies."
                },
                "authors": [
                    {
                        "name": "Xiaohe Zhou"
                    },
                    {
                        "name": "Matteo L. Zaffalon"
                    },
                    {
                        "name": "Emanuele Mazzola"
                    },
                    {
                        "name": "Andrea Fratelli"
                    },
                    {
                        "name": "Francesco Carulli"
                    },
                    {
                        "name": "Chenger Wang"
                    },
                    {
                        "name": "Mengda He"
                    },
                    {
                        "name": "Francesco Bruni"
                    },
                    {
                        "name": "Saptarshi Chakraborty"
                    },
                    {
                        "name": "Leonardo Poletti"
                    },
                    {
                        "name": "Francesca Rossi"
                    },
                    {
                        "name": "Luca Gironi"
                    },
                    {
                        "name": "Francesco Meinardi"
                    },
                    {
                        "name": "Liang Li"
                    },
                    {
                        "name": "Sergio Brovelli"
                    }
                ],
                "author_detail": {
                    "name": "Sergio Brovelli"
                },
                "arxiv_affiliation": "INFN - Sezione di Milano - Bicocca, Milano, Italy",
                "author": "Sergio Brovelli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09210v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09210v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09205v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09205v1",
                "updated": "2025-05-14T07:34:36Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    7,
                    34,
                    36,
                    2,
                    134,
                    0
                ],
                "published": "2025-05-14T07:34:36Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    7,
                    34,
                    36,
                    2,
                    134,
                    0
                ],
                "title": "HMamba: Hyperbolic Mamba for Sequential Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HMamba: Hyperbolic Mamba for Sequential Recommendation"
                },
                "summary": "Sequential recommendation systems have become a cornerstone of personalized\nservices, adept at modeling the temporal evolution of user preferences by\ncapturing dynamic interaction sequences. Existing approaches predominantly rely\non traditional models, including RNNs and Transformers. Despite their success\nin local pattern recognition, Transformer-based methods suffer from quadratic\ncomputational complexity and a tendency toward superficial attention patterns,\nlimiting their ability to infer enduring preference hierarchies in sequential\nrecommendation data. Recent advances in Mamba-based sequential models introduce\nlinear-time efficiency but remain constrained by Euclidean geometry, failing to\nleverage the intrinsic hyperbolic structure of recommendation data. To bridge\nthis gap, we propose Hyperbolic Mamba, a novel architecture that unifies the\nefficiency of Mamba's selective state space mechanism with hyperbolic\ngeometry's hierarchical representational power. Our framework introduces (1) a\nhyperbolic selective state space that maintains curvature-aware sequence\nmodeling and (2) stabilized Riemannian operations to enable scalable training.\nExperiments across four benchmarks demonstrate that Hyperbolic Mamba achieves\n3-11% improvement while retaining Mamba's linear-time efficiency, enabling\nreal-world deployment. This work establishes a new paradigm for efficient,\nhierarchy-aware sequential modeling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequential recommendation systems have become a cornerstone of personalized\nservices, adept at modeling the temporal evolution of user preferences by\ncapturing dynamic interaction sequences. Existing approaches predominantly rely\non traditional models, including RNNs and Transformers. Despite their success\nin local pattern recognition, Transformer-based methods suffer from quadratic\ncomputational complexity and a tendency toward superficial attention patterns,\nlimiting their ability to infer enduring preference hierarchies in sequential\nrecommendation data. Recent advances in Mamba-based sequential models introduce\nlinear-time efficiency but remain constrained by Euclidean geometry, failing to\nleverage the intrinsic hyperbolic structure of recommendation data. To bridge\nthis gap, we propose Hyperbolic Mamba, a novel architecture that unifies the\nefficiency of Mamba's selective state space mechanism with hyperbolic\ngeometry's hierarchical representational power. Our framework introduces (1) a\nhyperbolic selective state space that maintains curvature-aware sequence\nmodeling and (2) stabilized Riemannian operations to enable scalable training.\nExperiments across four benchmarks demonstrate that Hyperbolic Mamba achieves\n3-11% improvement while retaining Mamba's linear-time efficiency, enabling\nreal-world deployment. This work establishes a new paradigm for efficient,\nhierarchy-aware sequential modeling."
                },
                "authors": [
                    {
                        "name": "Qianru Zhang"
                    },
                    {
                        "name": "Honggang Wen"
                    },
                    {
                        "name": "Wei Yuan"
                    },
                    {
                        "name": "Crystal Chen"
                    },
                    {
                        "name": "Menglin Yang"
                    },
                    {
                        "name": "Siu-Ming Yiu"
                    },
                    {
                        "name": "Hongzhi Yin"
                    }
                ],
                "author_detail": {
                    "name": "Hongzhi Yin"
                },
                "author": "Hongzhi Yin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09205v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09205v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09188v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09188v1",
                "updated": "2025-05-14T06:39:01Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    6,
                    39,
                    1,
                    2,
                    134,
                    0
                ],
                "published": "2025-05-14T06:39:01Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    6,
                    39,
                    1,
                    2,
                    134,
                    0
                ],
                "title": "Zero-shot Quantization: A Comprehensive Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-shot Quantization: A Comprehensive Survey"
                },
                "summary": "Network quantization has proven to be a powerful approach to reduce the\nmemory and computational demands of deep learning models for deployment on\nresource-constrained devices. However, traditional quantization methods often\nrely on access to training data, which is impractical in many real-world\nscenarios due to privacy, security, or regulatory constraints. Zero-shot\nQuantization (ZSQ) emerges as a promising solution, achieving quantization\nwithout requiring any real data. In this paper, we provide a comprehensive\noverview of ZSQ methods and their recent advancements. First, we provide a\nformal definition of the ZSQ problem and highlight the key challenges. Then, we\ncategorize the existing ZSQ methods into classes based on data generation\nstrategies, and analyze their motivations, core ideas, and key takeaways.\nLastly, we suggest future research directions to address the remaining\nlimitations and advance the field of ZSQ. To the best of our knowledge, this\npaper is the first in-depth survey on ZSQ.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Network quantization has proven to be a powerful approach to reduce the\nmemory and computational demands of deep learning models for deployment on\nresource-constrained devices. However, traditional quantization methods often\nrely on access to training data, which is impractical in many real-world\nscenarios due to privacy, security, or regulatory constraints. Zero-shot\nQuantization (ZSQ) emerges as a promising solution, achieving quantization\nwithout requiring any real data. In this paper, we provide a comprehensive\noverview of ZSQ methods and their recent advancements. First, we provide a\nformal definition of the ZSQ problem and highlight the key challenges. Then, we\ncategorize the existing ZSQ methods into classes based on data generation\nstrategies, and analyze their motivations, core ideas, and key takeaways.\nLastly, we suggest future research directions to address the remaining\nlimitations and advance the field of ZSQ. To the best of our knowledge, this\npaper is the first in-depth survey on ZSQ."
                },
                "authors": [
                    {
                        "name": "Minjun Kim"
                    },
                    {
                        "name": "Jaehyeon Choi"
                    },
                    {
                        "name": "Jongkeun Lee"
                    },
                    {
                        "name": "Wonjin Cho"
                    },
                    {
                        "name": "U Kang"
                    }
                ],
                "author_detail": {
                    "name": "U Kang"
                },
                "author": "U Kang",
                "arxiv_comment": "IJCAI 2025 Survey Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09188v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09188v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09178v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09178v2",
                "updated": "2025-05-15T12:49:27Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    12,
                    49,
                    27,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-14T06:21:27Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    6,
                    21,
                    27,
                    2,
                    134,
                    0
                ],
                "title": "UniCAD: Efficient and Extendable Architecture for Multi-Task\n  Computer-Aided Diagnosis System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UniCAD: Efficient and Extendable Architecture for Multi-Task\n  Computer-Aided Diagnosis System"
                },
                "summary": "The growing complexity and scale of visual model pre-training have made\ndeveloping and deploying multi-task computer-aided diagnosis (CAD) systems\nincreasingly challenging and resource-intensive. Furthermore, the medical\nimaging community lacks an open-source CAD platform to enable the rapid\ncreation of efficient and extendable diagnostic models. To address these\nissues, we propose UniCAD, a unified architecture that leverages the robust\ncapabilities of pre-trained vision foundation models to seamlessly handle both\n2D and 3D medical images while requiring only minimal task-specific parameters.\nUniCAD introduces two key innovations: (1) Efficiency: A low-rank adaptation\nstrategy is employed to adapt a pre-trained visual model to the medical image\ndomain, achieving performance on par with fully fine-tuned counterparts while\nintroducing only 0.17% trainable parameters. (2) Plug-and-Play: A modular\narchitecture that combines a frozen foundation model with multiple\nplug-and-play experts, enabling diverse tasks and seamless functionality\nexpansion. Building on this unified CAD architecture, we establish an\nopen-source platform where researchers can share and access lightweight CAD\nexperts, fostering a more equitable and efficient research ecosystem.\nComprehensive experiments across 12 diverse medical datasets demonstrate that\nUniCAD consistently outperforms existing methods in both accuracy and\ndeployment efficiency. The source code and project page are available at\nhttps://mii-laboratory.github.io/UniCAD/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing complexity and scale of visual model pre-training have made\ndeveloping and deploying multi-task computer-aided diagnosis (CAD) systems\nincreasingly challenging and resource-intensive. Furthermore, the medical\nimaging community lacks an open-source CAD platform to enable the rapid\ncreation of efficient and extendable diagnostic models. To address these\nissues, we propose UniCAD, a unified architecture that leverages the robust\ncapabilities of pre-trained vision foundation models to seamlessly handle both\n2D and 3D medical images while requiring only minimal task-specific parameters.\nUniCAD introduces two key innovations: (1) Efficiency: A low-rank adaptation\nstrategy is employed to adapt a pre-trained visual model to the medical image\ndomain, achieving performance on par with fully fine-tuned counterparts while\nintroducing only 0.17% trainable parameters. (2) Plug-and-Play: A modular\narchitecture that combines a frozen foundation model with multiple\nplug-and-play experts, enabling diverse tasks and seamless functionality\nexpansion. Building on this unified CAD architecture, we establish an\nopen-source platform where researchers can share and access lightweight CAD\nexperts, fostering a more equitable and efficient research ecosystem.\nComprehensive experiments across 12 diverse medical datasets demonstrate that\nUniCAD consistently outperforms existing methods in both accuracy and\ndeployment efficiency. The source code and project page are available at\nhttps://mii-laboratory.github.io/UniCAD/."
                },
                "authors": [
                    {
                        "name": "Yitao Zhu"
                    },
                    {
                        "name": "Yuan Yin"
                    },
                    {
                        "name": "Zhenrong Shen"
                    },
                    {
                        "name": "Zihao Zhao"
                    },
                    {
                        "name": "Haiyu Song"
                    },
                    {
                        "name": "Sheng Wang"
                    },
                    {
                        "name": "Dinggang Shen"
                    },
                    {
                        "name": "Qian Wang"
                    }
                ],
                "author_detail": {
                    "name": "Qian Wang"
                },
                "author": "Qian Wang",
                "arxiv_comment": "14 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09178v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09178v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09170v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09170v1",
                "updated": "2025-05-14T06:07:59Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    6,
                    7,
                    59,
                    2,
                    134,
                    0
                ],
                "published": "2025-05-14T06:07:59Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    6,
                    7,
                    59,
                    2,
                    134,
                    0
                ],
                "title": "Modeling Indoor PM$_{2.5}$ Exposure During Retrofits: Plastic Film\n  Barriers and a Quadratic Baseline Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling Indoor PM$_{2.5}$ Exposure During Retrofits: Plastic Film\n  Barriers and a Quadratic Baseline Approach"
                },
                "summary": "Temporary plastic film barriers are widely used to separate occupied rooms\nfrom exterior renovation zones, yet their effect on indoor particulate exposure\nis poorly quantified. We monitored PM$_{2.5}$ in a Tampa, Florida, apartment\nfor 48 days with a low-cost optical sensor (Temtop LKC-1000S+), spanning\npre-barrier, barrier-on, and post-barrier periods. A quadratic baseline was\nfitted to \"background\" minutes devoid of identifiable indoor sources, allowing\nexcess concentrations ($\\Delta$PM) to be partitioned into facade work, cooking,\nand passive accumulation without outdoor co-monitoring. The barrier prevented\nlarge construction spikes indoors but curtailed natural ventilation, doubling\nthe mean baseline from 1.9 to 3.9 $\\mu$g m$^{-3}$. During this stage, passive\nbuild-up accounted for $45\\,\\%$ of the daily excess dose, with facade work and\ncooking contributing $31\\,\\%$ and $24\\,\\%$, respectively. Once the new window\nwas installed and evening airing resumed, the baseline fell to 0.8 $\\mu$g\nm$^{-3}$, the lowest of the campaign. Our findings highlight the trade-off\nbetween dust shielding and background elevation and demonstrate that simple\npolynomial fitting bolsters low-cost IAQ diagnostics in mechanically\nunventilated dwellings. The framework is readily transferable to other\nretrofits; future studies should pair indoor sensing with outdoor references\nand multi-room deployments to refine infiltration estimates.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Temporary plastic film barriers are widely used to separate occupied rooms\nfrom exterior renovation zones, yet their effect on indoor particulate exposure\nis poorly quantified. We monitored PM$_{2.5}$ in a Tampa, Florida, apartment\nfor 48 days with a low-cost optical sensor (Temtop LKC-1000S+), spanning\npre-barrier, barrier-on, and post-barrier periods. A quadratic baseline was\nfitted to \"background\" minutes devoid of identifiable indoor sources, allowing\nexcess concentrations ($\\Delta$PM) to be partitioned into facade work, cooking,\nand passive accumulation without outdoor co-monitoring. The barrier prevented\nlarge construction spikes indoors but curtailed natural ventilation, doubling\nthe mean baseline from 1.9 to 3.9 $\\mu$g m$^{-3}$. During this stage, passive\nbuild-up accounted for $45\\,\\%$ of the daily excess dose, with facade work and\ncooking contributing $31\\,\\%$ and $24\\,\\%$, respectively. Once the new window\nwas installed and evening airing resumed, the baseline fell to 0.8 $\\mu$g\nm$^{-3}$, the lowest of the campaign. Our findings highlight the trade-off\nbetween dust shielding and background elevation and demonstrate that simple\npolynomial fitting bolsters low-cost IAQ diagnostics in mechanically\nunventilated dwellings. The framework is readily transferable to other\nretrofits; future studies should pair indoor sensing with outdoor references\nand multi-room deployments to refine infiltration estimates."
                },
                "authors": [
                    {
                        "name": "Rostyslav Sipakov"
                    },
                    {
                        "name": "Olena Voloshkina"
                    }
                ],
                "author_detail": {
                    "name": "Olena Voloshkina"
                },
                "author": "Olena Voloshkina",
                "arxiv_comment": "Case study (9 pp). Focus on the plastic film barrier and its impact\n  on indoor PM$_{2.5}$. Not peer-reviewed; full version in preparation",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09170v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09170v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.med-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ao-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02247v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02247v3",
                "updated": "2025-05-14T06:06:20Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    6,
                    6,
                    20,
                    2,
                    134,
                    0
                ],
                "published": "2024-10-03T06:37:37Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    6,
                    37,
                    37,
                    3,
                    277,
                    0
                ],
                "title": "Theoretical Insights into Fine-Tuning Attention Mechanism:\n  Generalization and Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Theoretical Insights into Fine-Tuning Attention Mechanism:\n  Generalization and Optimization"
                },
                "summary": "Large Language Models (LLMs), built on Transformer architectures, exhibit\nremarkable generalization across a wide range of tasks. However, fine-tuning\nthese models for specific tasks remains resource-intensive due to their\nextensive parameterization. In this paper, we explore two remarkable phenomena\nrelated to the attention mechanism during the fine-tuning of LLMs (where\n$\\mathbf{W}_q$, $\\mathbf{W}_k$, and $\\mathbf{W}_v$ denote the weights of the\nquery, key, and value layers, respectively). The first phenomenon, termed\n\"Unequal Importance of Attention Matrices\", highlights the impact of\nfine-tuning different weight matrices. It shows that optimizing the\n$\\mathbf{W}_v$ matrix yields significantly better performance than optimizing\nthe $\\mathbf{W}_k$ matrix. Fine-tuning only the $\\mathbf{W}_q$ and\n$\\mathbf{W}_v$ matrices is computationally efficient while delivering results\ncomparable to, or even better than fine-tuning all three matrices\n($\\mathbf{W}_q$, $\\mathbf{W}_k$, and $\\mathbf{W}_v$). The second\nphenomenon,\"Attention Matrices with Customized Learning Rate Lead to Better\nConvergence\", emphasizes the importance of assigning distinct learning rates to\nthese matrices. Specifically, a higher learning rate for the $\\mathbf{W}_v$\nmatrix compared to $\\mathbf{W}_q$ and $\\mathbf{W}_k$ accelerates convergence\nand improves performance. Building on these insights, we propose a new strategy\nthat improves fine-tuning efficiency in terms of both storage and time.\nExperimental results on benchmark datasets validate the effectiveness of this\napproach, supporting our theoretical findings. Our analysis lays the\ntheoretical groundwork for configuring and improving algorithms in LLMs\nfine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), built on Transformer architectures, exhibit\nremarkable generalization across a wide range of tasks. However, fine-tuning\nthese models for specific tasks remains resource-intensive due to their\nextensive parameterization. In this paper, we explore two remarkable phenomena\nrelated to the attention mechanism during the fine-tuning of LLMs (where\n$\\mathbf{W}_q$, $\\mathbf{W}_k$, and $\\mathbf{W}_v$ denote the weights of the\nquery, key, and value layers, respectively). The first phenomenon, termed\n\"Unequal Importance of Attention Matrices\", highlights the impact of\nfine-tuning different weight matrices. It shows that optimizing the\n$\\mathbf{W}_v$ matrix yields significantly better performance than optimizing\nthe $\\mathbf{W}_k$ matrix. Fine-tuning only the $\\mathbf{W}_q$ and\n$\\mathbf{W}_v$ matrices is computationally efficient while delivering results\ncomparable to, or even better than fine-tuning all three matrices\n($\\mathbf{W}_q$, $\\mathbf{W}_k$, and $\\mathbf{W}_v$). The second\nphenomenon,\"Attention Matrices with Customized Learning Rate Lead to Better\nConvergence\", emphasizes the importance of assigning distinct learning rates to\nthese matrices. Specifically, a higher learning rate for the $\\mathbf{W}_v$\nmatrix compared to $\\mathbf{W}_q$ and $\\mathbf{W}_k$ accelerates convergence\nand improves performance. Building on these insights, we propose a new strategy\nthat improves fine-tuning efficiency in terms of both storage and time.\nExperimental results on benchmark datasets validate the effectiveness of this\napproach, supporting our theoretical findings. Our analysis lays the\ntheoretical groundwork for configuring and improving algorithms in LLMs\nfine-tuning."
                },
                "authors": [
                    {
                        "name": "Xinhao Yao"
                    },
                    {
                        "name": "Hongjin Qian"
                    },
                    {
                        "name": "Xiaolin Hu"
                    },
                    {
                        "name": "Gengze Xu"
                    },
                    {
                        "name": "Wei Liu"
                    },
                    {
                        "name": "Jian Luan"
                    },
                    {
                        "name": "Bin Wang"
                    },
                    {
                        "name": "Yong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Liu"
                },
                "author": "Yong Liu",
                "arxiv_comment": "IJCAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02247v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02247v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.01383v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.01383v3",
                "updated": "2025-05-14T06:05:53Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    6,
                    5,
                    53,
                    2,
                    134,
                    0
                ],
                "published": "2024-02-02T13:06:35Z",
                "published_parsed": [
                    2024,
                    2,
                    2,
                    13,
                    6,
                    35,
                    4,
                    33,
                    0
                ],
                "title": "LLM-based NLG Evaluation: Current Status and Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based NLG Evaluation: Current Status and Challenges"
                },
                "summary": "Evaluating natural language generation (NLG) is a vital but challenging\nproblem in natural language processing. Traditional evaluation metrics mainly\ncapturing content (e.g. n-gram) overlap between system outputs and references\nare far from satisfactory, and large language models (LLMs) such as ChatGPT\nhave demonstrated great potential in NLG evaluation in recent years. Various\nautomatic evaluation methods based on LLMs have been proposed, including\nmetrics derived from LLMs, prompting LLMs, fine-tuning LLMs, and human-LLM\ncollaborative evaluation. In this survey, we first give a taxonomy of LLM-based\nNLG evaluation methods, and discuss their pros and cons, respectively. Lastly,\nwe discuss several open problems in this area and point out future research\ndirections.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating natural language generation (NLG) is a vital but challenging\nproblem in natural language processing. Traditional evaluation metrics mainly\ncapturing content (e.g. n-gram) overlap between system outputs and references\nare far from satisfactory, and large language models (LLMs) such as ChatGPT\nhave demonstrated great potential in NLG evaluation in recent years. Various\nautomatic evaluation methods based on LLMs have been proposed, including\nmetrics derived from LLMs, prompting LLMs, fine-tuning LLMs, and human-LLM\ncollaborative evaluation. In this survey, we first give a taxonomy of LLM-based\nNLG evaluation methods, and discuss their pros and cons, respectively. Lastly,\nwe discuss several open problems in this area and point out future research\ndirections."
                },
                "authors": [
                    {
                        "name": "Mingqi Gao"
                    },
                    {
                        "name": "Xinyu Hu"
                    },
                    {
                        "name": "Jie Ruan"
                    },
                    {
                        "name": "Xiao Pu"
                    },
                    {
                        "name": "Xiaojun Wan"
                    }
                ],
                "author_detail": {
                    "name": "Xiaojun Wan"
                },
                "author": "Xiaojun Wan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.01383v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.01383v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04405v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04405v2",
                "updated": "2025-05-14T05:23:45Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    5,
                    23,
                    45,
                    2,
                    134,
                    0
                ],
                "published": "2025-02-06T09:08:12Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    9,
                    8,
                    12,
                    3,
                    37,
                    0
                ],
                "title": "FAS: Fast ANN-SNN Conversion for Spiking Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FAS: Fast ANN-SNN Conversion for Spiking Large Language Models"
                },
                "summary": "Spiking Large Language Models have been shown as a good alternative to LLMs\nin various scenarios. Existing methods for creating Spiking LLMs, i.e., direct\ntraining and ANN-SNN conversion, often suffer from performance degradation and\nrelatively high computational costs. To address these issues, we propose a\nnovel Fast ANN-SNN conversion strategy (FAS) that transforms LLMs into spiking\nLLMs in two stages. The first stage employs a full-parameter fine-tuning of\npre-trained models, so it does not need any direct training from scratch. The\nsecond stage introduces a coarse-to-fine calibration method to reduce\nconversion errors and improve accuracy. Experiments on both language and\nvision-language tasks across four different scales of LLMs demonstrate that FAS\ncan achieve state-of-the-art performance yet with significantly reduced\ninference latency and computational costs. Notably, FAS only takes eight\ntimesteps to achieve an accuracy of 3\\% higher than that of the OPT-7B model,\nwhile reducing energy consumption by 96.63\\%. The source code is available at\nhttps://github.com/lc783/FAS",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spiking Large Language Models have been shown as a good alternative to LLMs\nin various scenarios. Existing methods for creating Spiking LLMs, i.e., direct\ntraining and ANN-SNN conversion, often suffer from performance degradation and\nrelatively high computational costs. To address these issues, we propose a\nnovel Fast ANN-SNN conversion strategy (FAS) that transforms LLMs into spiking\nLLMs in two stages. The first stage employs a full-parameter fine-tuning of\npre-trained models, so it does not need any direct training from scratch. The\nsecond stage introduces a coarse-to-fine calibration method to reduce\nconversion errors and improve accuracy. Experiments on both language and\nvision-language tasks across four different scales of LLMs demonstrate that FAS\ncan achieve state-of-the-art performance yet with significantly reduced\ninference latency and computational costs. Notably, FAS only takes eight\ntimesteps to achieve an accuracy of 3\\% higher than that of the OPT-7B model,\nwhile reducing energy consumption by 96.63\\%. The source code is available at\nhttps://github.com/lc783/FAS"
                },
                "authors": [
                    {
                        "name": "Long Chen"
                    },
                    {
                        "name": "Xiaotian Song"
                    },
                    {
                        "name": "Andy Song"
                    },
                    {
                        "name": "BaDong Chen"
                    },
                    {
                        "name": "Jiancheng Lv"
                    },
                    {
                        "name": "Yanan Sun"
                    }
                ],
                "author_detail": {
                    "name": "Yanan Sun"
                },
                "author": "Yanan Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04405v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04405v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09141v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09141v1",
                "updated": "2025-05-14T04:50:00Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    4,
                    50,
                    0,
                    2,
                    134,
                    0
                ],
                "published": "2025-05-14T04:50:00Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    4,
                    50,
                    0,
                    2,
                    134,
                    0
                ],
                "title": "Sensing-Assisted Channel Prediction in Complex Wireless Environments: An\n  LLM-Based Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sensing-Assisted Channel Prediction in Complex Wireless Environments: An\n  LLM-Based Approach"
                },
                "summary": "This letter studies the sensing-assisted channel prediction for a\nmulti-antenna orthogonal frequency division multiplexing (OFDM) system\noperating in realistic and complex wireless environments. In this system,an\nintegrated sensing and communication (ISAC) transmitter leverages the\nmono-static sensing capability to facilitate the prediction of its bi-static\ncommunication channel, by exploiting the fact that the sensing and\ncommunication channels share the same physical environment involving shared\nscatterers. Specifically, we propose a novel large language model (LLM)-based\nchannel prediction approach,which adapts pre-trained text-based LLM to handle\nthe complex-matrix-form channel state information (CSI) data. This approach\nutilizes the LLM's strong ability to capture the intricate spatiotemporal\nrelationships between the multi-path sensing and communication channels, and\nthus efficiently predicts upcoming communication CSI based on historical\ncommunication and sensing CSI data. Experimental results show that the proposed\nLLM-based approach significantly outperforms conventional deep learning-based\nmethods and the benchmark scheme without sensing assistance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This letter studies the sensing-assisted channel prediction for a\nmulti-antenna orthogonal frequency division multiplexing (OFDM) system\noperating in realistic and complex wireless environments. In this system,an\nintegrated sensing and communication (ISAC) transmitter leverages the\nmono-static sensing capability to facilitate the prediction of its bi-static\ncommunication channel, by exploiting the fact that the sensing and\ncommunication channels share the same physical environment involving shared\nscatterers. Specifically, we propose a novel large language model (LLM)-based\nchannel prediction approach,which adapts pre-trained text-based LLM to handle\nthe complex-matrix-form channel state information (CSI) data. This approach\nutilizes the LLM's strong ability to capture the intricate spatiotemporal\nrelationships between the multi-path sensing and communication channels, and\nthus efficiently predicts upcoming communication CSI based on historical\ncommunication and sensing CSI data. Experimental results show that the proposed\nLLM-based approach significantly outperforms conventional deep learning-based\nmethods and the benchmark scheme without sensing assistance."
                },
                "authors": [
                    {
                        "name": "Junjie He"
                    },
                    {
                        "name": "Zixiang Ren"
                    },
                    {
                        "name": "Jianping Yao"
                    },
                    {
                        "name": "Han Hu"
                    },
                    {
                        "name": "Tony Xiao Han"
                    },
                    {
                        "name": "Jie Xu"
                    }
                ],
                "author_detail": {
                    "name": "Jie Xu"
                },
                "author": "Jie Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09141v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09141v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09142v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09142v1",
                "updated": "2025-05-14T04:50:00Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    4,
                    50,
                    0,
                    2,
                    134,
                    0
                ],
                "published": "2025-05-14T04:50:00Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    4,
                    50,
                    0,
                    2,
                    134,
                    0
                ],
                "title": "ELIS: Efficient LLM Iterative Scheduling System with Response Length\n  Predictor",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ELIS: Efficient LLM Iterative Scheduling System with Response Length\n  Predictor"
                },
                "summary": "We propose ELIS, a serving system for Large Language Models (LLMs) featuring\nan Iterative Shortest Remaining Time First (ISRTF) scheduler designed to\nefficiently manage inference tasks with the shortest remaining tokens. Current\nLLM serving systems often employ a first-come-first-served scheduling strategy,\nwhich can lead to the \"head-of-line blocking\" problem. To overcome this\nlimitation, it is necessary to predict LLM inference times and apply a shortest\njob first scheduling strategy. However, due to the auto-regressive nature of\nLLMs, predicting the inference latency is challenging. ELIS addresses this\nchallenge by training a response length predictor for LLMs using the BGE model,\nan encoder-based state-of-the-art model. Additionally, we have devised the\nISRTF scheduling strategy, an optimization of shortest remaining time first\ntailored to existing LLM iteration batching. To evaluate our work in an\nindustrial setting, we simulate streams of requests based on our study of\nreal-world user LLM serving trace records. Furthermore, we implemented ELIS as\na cloud-native scheduler system on Kubernetes to evaluate its performance in\nproduction environments. Our experimental results demonstrate that ISRTF\nreduces the average job completion time by up to 19.6%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose ELIS, a serving system for Large Language Models (LLMs) featuring\nan Iterative Shortest Remaining Time First (ISRTF) scheduler designed to\nefficiently manage inference tasks with the shortest remaining tokens. Current\nLLM serving systems often employ a first-come-first-served scheduling strategy,\nwhich can lead to the \"head-of-line blocking\" problem. To overcome this\nlimitation, it is necessary to predict LLM inference times and apply a shortest\njob first scheduling strategy. However, due to the auto-regressive nature of\nLLMs, predicting the inference latency is challenging. ELIS addresses this\nchallenge by training a response length predictor for LLMs using the BGE model,\nan encoder-based state-of-the-art model. Additionally, we have devised the\nISRTF scheduling strategy, an optimization of shortest remaining time first\ntailored to existing LLM iteration batching. To evaluate our work in an\nindustrial setting, we simulate streams of requests based on our study of\nreal-world user LLM serving trace records. Furthermore, we implemented ELIS as\na cloud-native scheduler system on Kubernetes to evaluate its performance in\nproduction environments. Our experimental results demonstrate that ISRTF\nreduces the average job completion time by up to 19.6%."
                },
                "authors": [
                    {
                        "name": "Seungbeom Choi"
                    },
                    {
                        "name": "Jeonghoe Goo"
                    },
                    {
                        "name": "Eunjoo Jeon"
                    },
                    {
                        "name": "Mingyu Yang"
                    },
                    {
                        "name": "Minsung Jang"
                    }
                ],
                "author_detail": {
                    "name": "Minsung Jang"
                },
                "author": "Minsung Jang",
                "arxiv_comment": "13 pages, 5 figures. Cloud-native LLM scheduling system with\n  latency-aware inference optimization",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09142v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09142v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09138v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09138v1",
                "updated": "2025-05-14T04:42:15Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    4,
                    42,
                    15,
                    2,
                    134,
                    0
                ],
                "published": "2025-05-14T04:42:15Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    4,
                    42,
                    15,
                    2,
                    134,
                    0
                ],
                "title": "The VIIRS-DNB radiance product is insufficient to assess the effect of\n  \"cool pavement\" materials on nighttime radiances of treated areas",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The VIIRS-DNB radiance product is insufficient to assess the effect of\n  \"cool pavement\" materials on nighttime radiances of treated areas"
                },
                "summary": "In warmer climates, cities face the prospect of higher air temperatures in\nsummer compared to historical averages due to the urban heat island effect. An\napproach intended to address this problem is the application of \"cool pavement\"\ntreatments (CPT) to city streets to make them more reflective to sunlight.\nRaising the albedo of roadways for this purpose may also have the effect of\nincreasing the amount of street light that is reflected into the night sky. The\nsimplest hypothesis explaining the relationship between CPT application and\nupward radiance is that CPT applied to road surfaces in areas where street\nlighting is dominant should increase the upward radiance of neighborhoods where\nthe treatments are applied. A simple model predicted radiance increases of 2-6%\nimmediately after CPT application. To test the hypothesis and model\npredictions, we looked for radiance changes coinciding with the application of\nCPT in residential neighborhoods of Phoenix, U.S., since 2020. We obtained time\nseries radiance measurements from Visible Infrared Imaging Radiometer Suite\nDay-Night Band (VIIRS-DNB) data from Radiance Light Trends for Phoenix\nneighborhoods receiving CPT and nearby \"control\" neighborhoods not receiving\nCPT. At the 95% confidence level, we found that any increases in nighttime\nradiances from treated neighborhoods did not exceed about 14%. As a\nconsequence, we cannot rule out either the expected radiance increases from our\nmodel or the possibility that CPT application yielded no change in radiance. We\ntherefore cannot draw robust conclusions about the potential influence of CPT\ndeployment on skyglow given the limitations of the DNB as a data source.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In warmer climates, cities face the prospect of higher air temperatures in\nsummer compared to historical averages due to the urban heat island effect. An\napproach intended to address this problem is the application of \"cool pavement\"\ntreatments (CPT) to city streets to make them more reflective to sunlight.\nRaising the albedo of roadways for this purpose may also have the effect of\nincreasing the amount of street light that is reflected into the night sky. The\nsimplest hypothesis explaining the relationship between CPT application and\nupward radiance is that CPT applied to road surfaces in areas where street\nlighting is dominant should increase the upward radiance of neighborhoods where\nthe treatments are applied. A simple model predicted radiance increases of 2-6%\nimmediately after CPT application. To test the hypothesis and model\npredictions, we looked for radiance changes coinciding with the application of\nCPT in residential neighborhoods of Phoenix, U.S., since 2020. We obtained time\nseries radiance measurements from Visible Infrared Imaging Radiometer Suite\nDay-Night Band (VIIRS-DNB) data from Radiance Light Trends for Phoenix\nneighborhoods receiving CPT and nearby \"control\" neighborhoods not receiving\nCPT. At the 95% confidence level, we found that any increases in nighttime\nradiances from treated neighborhoods did not exceed about 14%. As a\nconsequence, we cannot rule out either the expected radiance increases from our\nmodel or the possibility that CPT application yielded no change in radiance. We\ntherefore cannot draw robust conclusions about the potential influence of CPT\ndeployment on skyglow given the limitations of the DNB as a data source."
                },
                "authors": [
                    {
                        "name": "John C. Barentine"
                    }
                ],
                "author_detail": {
                    "name": "John C. Barentine"
                },
                "author": "John C. Barentine",
                "arxiv_comment": "Accepted for publication in Journal of Quantitative Spectroscopy and\n  Radiative Transfer (MS no. JQSRT-D-24-00539R1)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09138v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09138v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ao-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ao-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09129v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09129v1",
                "updated": "2025-05-14T04:24:37Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    4,
                    24,
                    37,
                    2,
                    134,
                    0
                ],
                "published": "2025-05-14T04:24:37Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    4,
                    24,
                    37,
                    2,
                    134,
                    0
                ],
                "title": "WSCIF: A Weakly-Supervised Color Intelligence Framework for Tactical\n  Anomaly Detection in Surveillance Keyframes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WSCIF: A Weakly-Supervised Color Intelligence Framework for Tactical\n  Anomaly Detection in Surveillance Keyframes"
                },
                "summary": "The deployment of traditional deep learning models in high-risk security\ntasks in an unlabeled, data-non-exploitable video intelligence environment\nfaces significant challenges. In this paper, we propose a lightweight anomaly\ndetection framework based on color features for surveillance video clips in a\nhigh sensitivity tactical mission, aiming to quickly identify and interpret\npotential threat events under resource-constrained and data-sensitive\nconditions. The method fuses unsupervised KMeans clustering with RGB channel\nhistogram modeling to achieve composite detection of structural anomalies and\ncolor mutation signals in key frames. The experiment takes an operation\nsurveillance video occurring in an African country as a research sample, and\nsuccessfully identifies multiple highly anomalous frames related to high-energy\nlight sources, target presence, and reflective interference under the condition\nof no access to the original data. The results show that this method can be\neffectively used for tactical assassination warning, suspicious object\nscreening and environmental drastic change monitoring with strong deployability\nand tactical interpretation value. The study emphasizes the importance of color\nfeatures as low semantic battlefield signal carriers, and its battlefield\nintelligent perception capability will be further extended by combining graph\nneural networks and temporal modeling in the future.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of traditional deep learning models in high-risk security\ntasks in an unlabeled, data-non-exploitable video intelligence environment\nfaces significant challenges. In this paper, we propose a lightweight anomaly\ndetection framework based on color features for surveillance video clips in a\nhigh sensitivity tactical mission, aiming to quickly identify and interpret\npotential threat events under resource-constrained and data-sensitive\nconditions. The method fuses unsupervised KMeans clustering with RGB channel\nhistogram modeling to achieve composite detection of structural anomalies and\ncolor mutation signals in key frames. The experiment takes an operation\nsurveillance video occurring in an African country as a research sample, and\nsuccessfully identifies multiple highly anomalous frames related to high-energy\nlight sources, target presence, and reflective interference under the condition\nof no access to the original data. The results show that this method can be\neffectively used for tactical assassination warning, suspicious object\nscreening and environmental drastic change monitoring with strong deployability\nand tactical interpretation value. The study emphasizes the importance of color\nfeatures as low semantic battlefield signal carriers, and its battlefield\nintelligent perception capability will be further extended by combining graph\nneural networks and temporal modeling in the future."
                },
                "authors": [
                    {
                        "name": "Wei Meng"
                    }
                ],
                "author_detail": {
                    "name": "Wei Meng"
                },
                "author": "Wei Meng",
                "arxiv_comment": "17 pages, 3 figures, 3 tables. The paper proposes a lightweight\n  weakly-supervised color intelligence model for tactical video anomaly\n  detection, tested on anonymized African surveillance data",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09129v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09129v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "es: 68T10, 68T05, 62H35, 68U10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.4.9; I.5.1; I.2.10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18599v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18599v2",
                "updated": "2025-05-14T04:22:24Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    4,
                    22,
                    24,
                    2,
                    134,
                    0
                ],
                "published": "2025-03-24T11:56:50Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    11,
                    56,
                    50,
                    0,
                    83,
                    0
                ],
                "title": "Oaken: Fast and Efficient LLM Serving with Online-Offline Hybrid KV\n  Cache Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Oaken: Fast and Efficient LLM Serving with Online-Offline Hybrid KV\n  Cache Quantization"
                },
                "summary": "Modern Large Language Model serving system batches multiple requests to\nachieve high throughput, while batching attention operations is challenging,\nrendering memory bandwidth a critical bottleneck. The community relies on\nhigh-end GPUs with multiple high-bandwidth memory channels. Unfortunately,\nHBM's high bandwidth often comes at the expense of limited memory capacity,\nwhich reduces core utilization and increases costs. Recent advancements\nenabling longer contexts for LLMs have substantially increased the key-value\ncache size, further intensifying the pressures on memory capacity. The\nliterature has explored KV cache quantization techniques, which commonly use\nlow bitwidth for most values, selectively using higher bitwidth for outlier\nvalues. While this approach helps achieve high accuracy and low bitwidth\nsimultaneously, it comes with the limitation that cost for online outlier\ndetection is excessively high, negating the advantages. We propose Oaken, an\nacceleration solution that achieves high accuracy and high performance\nsimultaneously through co-designing algorithm and hardware. To effectively find\na sweet spot in the accuracy-performance trade-off space of KV cache\nquantization, Oaken employs an online-offline hybrid approach, setting outlier\nthresholds offline, which are then used to determine the quantization scale\nonline. To translate the proposed algorithmic technique into tangible\nperformance gains, Oaken also comes with custom quantization engines and memory\nmanagement units that can be integrated with any LLM accelerators. We built an\nOaken accelerator on top of an LLM accelerator, LPU, and conducted a\ncomprehensive evaluation. Our experiments show that for a batch size of 256,\nOaken achieves up to 1.58x throughput improvement over NVIDIA A100 GPU,\nincurring a minimal accuracy loss of only 0.54\\% on average, compared to\nstate-of-the-art KV cache quantization techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern Large Language Model serving system batches multiple requests to\nachieve high throughput, while batching attention operations is challenging,\nrendering memory bandwidth a critical bottleneck. The community relies on\nhigh-end GPUs with multiple high-bandwidth memory channels. Unfortunately,\nHBM's high bandwidth often comes at the expense of limited memory capacity,\nwhich reduces core utilization and increases costs. Recent advancements\nenabling longer contexts for LLMs have substantially increased the key-value\ncache size, further intensifying the pressures on memory capacity. The\nliterature has explored KV cache quantization techniques, which commonly use\nlow bitwidth for most values, selectively using higher bitwidth for outlier\nvalues. While this approach helps achieve high accuracy and low bitwidth\nsimultaneously, it comes with the limitation that cost for online outlier\ndetection is excessively high, negating the advantages. We propose Oaken, an\nacceleration solution that achieves high accuracy and high performance\nsimultaneously through co-designing algorithm and hardware. To effectively find\na sweet spot in the accuracy-performance trade-off space of KV cache\nquantization, Oaken employs an online-offline hybrid approach, setting outlier\nthresholds offline, which are then used to determine the quantization scale\nonline. To translate the proposed algorithmic technique into tangible\nperformance gains, Oaken also comes with custom quantization engines and memory\nmanagement units that can be integrated with any LLM accelerators. We built an\nOaken accelerator on top of an LLM accelerator, LPU, and conducted a\ncomprehensive evaluation. Our experiments show that for a batch size of 256,\nOaken achieves up to 1.58x throughput improvement over NVIDIA A100 GPU,\nincurring a minimal accuracy loss of only 0.54\\% on average, compared to\nstate-of-the-art KV cache quantization techniques."
                },
                "authors": [
                    {
                        "name": "Minsu Kim"
                    },
                    {
                        "name": "Seongmin Hong"
                    },
                    {
                        "name": "RyeoWook Ko"
                    },
                    {
                        "name": "Soongyu Choi"
                    },
                    {
                        "name": "Hunjong Lee"
                    },
                    {
                        "name": "Junsoo Kim"
                    },
                    {
                        "name": "Joo-Young Kim"
                    },
                    {
                        "name": "Jongse Park"
                    }
                ],
                "author_detail": {
                    "name": "Jongse Park"
                },
                "author": "Jongse Park",
                "arxiv_doi": "10.1145/3695053.3731019",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3695053.3731019",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.18599v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18599v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "16 pages, 14 figures, and 4 tables",
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09192v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09192v4",
                "updated": "2025-05-15T06:21:11Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    6,
                    21,
                    11,
                    3,
                    135,
                    0
                ],
                "published": "2025-04-12T12:17:20Z",
                "published_parsed": [
                    2025,
                    4,
                    12,
                    12,
                    17,
                    20,
                    5,
                    102,
                    0
                ],
                "title": "Towards More Efficient, Robust, Instance-adaptive, and Generalizable\n  Sequential Decision making",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards More Efficient, Robust, Instance-adaptive, and Generalizable\n  Sequential Decision making"
                },
                "summary": "The primary goal of my Ph.D. study is to develop provably efficient and\npractical algorithms for data-driven sequential decision-making under\nuncertainty. My work focuses on reinforcement learning (RL), multi-armed\nbandits, and their applications, including recommendation systems, computer\nnetworks, video analytics, and large language models (LLMs). Sequential\ndecision-making methods, such as bandits and RL, have demonstrated remarkable\nsuccess - ranging from outperforming human players in complex games like Atari\nand Go to advancing robotics, recommendation systems, and fine-tuning LLMs.\nDespite these successes, many established algorithms rely on idealized models\nthat can fail under model misspecifications or adversarial perturbations,\nparticularly in settings where accurate prior knowledge of the underlying model\nclass is unavailable or where malicious users operate within dynamic systems.\nThese challenges are pervasive in real-world applications, where robust and\nadaptive solutions are critical. Furthermore, while worst-case guarantees\nprovide theoretical reliability, they often fail to capture instance-dependent\nperformance, which can lead to more efficient and practical solutions. Another\nkey challenge lies in generalizing to new, unseen environments, a crucial\nrequirement for deploying these methods in dynamic and unpredictable settings.\nTo address these limitations, my research aims to develop more efficient,\nrobust, instance-adaptive, and generalizable sequential decision-making\nalgorithms for both reinforcement learning and bandits. Towards this end, I\nfocus on developing more efficient, robust, instance-adaptive, and\ngeneralizable for both general reinforcement learning (RL) and bandits.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The primary goal of my Ph.D. study is to develop provably efficient and\npractical algorithms for data-driven sequential decision-making under\nuncertainty. My work focuses on reinforcement learning (RL), multi-armed\nbandits, and their applications, including recommendation systems, computer\nnetworks, video analytics, and large language models (LLMs). Sequential\ndecision-making methods, such as bandits and RL, have demonstrated remarkable\nsuccess - ranging from outperforming human players in complex games like Atari\nand Go to advancing robotics, recommendation systems, and fine-tuning LLMs.\nDespite these successes, many established algorithms rely on idealized models\nthat can fail under model misspecifications or adversarial perturbations,\nparticularly in settings where accurate prior knowledge of the underlying model\nclass is unavailable or where malicious users operate within dynamic systems.\nThese challenges are pervasive in real-world applications, where robust and\nadaptive solutions are critical. Furthermore, while worst-case guarantees\nprovide theoretical reliability, they often fail to capture instance-dependent\nperformance, which can lead to more efficient and practical solutions. Another\nkey challenge lies in generalizing to new, unseen environments, a crucial\nrequirement for deploying these methods in dynamic and unpredictable settings.\nTo address these limitations, my research aims to develop more efficient,\nrobust, instance-adaptive, and generalizable sequential decision-making\nalgorithms for both reinforcement learning and bandits. Towards this end, I\nfocus on developing more efficient, robust, instance-adaptive, and\ngeneralizable for both general reinforcement learning (RL) and bandits."
                },
                "authors": [
                    {
                        "name": "Zhiyong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyong Wang"
                },
                "author": "Zhiyong Wang",
                "arxiv_comment": "Ph.D. Thesis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09192v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09192v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.07773v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.07773v2",
                "updated": "2025-05-14T04:15:06Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    4,
                    15,
                    6,
                    2,
                    134,
                    0
                ],
                "published": "2025-05-12T17:23:34Z",
                "published_parsed": [
                    2025,
                    5,
                    12,
                    17,
                    23,
                    34,
                    0,
                    132,
                    0
                ],
                "title": "Agent RL Scaling Law: Agent RL with Spontaneous Code Execution for\n  Mathematical Problem Solving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agent RL Scaling Law: Agent RL with Spontaneous Code Execution for\n  Mathematical Problem Solving"
                },
                "summary": "Large Language Models (LLMs) often struggle with mathematical reasoning tasks\nrequiring precise, verifiable computation. While Reinforcement Learning (RL)\nfrom outcome-based rewards enhances text-based reasoning, understanding how\nagents autonomously learn to leverage external tools like code execution\nremains crucial. We investigate RL from outcome-based rewards for\nTool-Integrated Reasoning, ZeroTIR, training base LLMs to spontaneously\ngenerate and execute Python code for mathematical problems without supervised\ntool-use examples. Our central contribution is we demonstrate that as RL\ntraining progresses, key metrics scale predictably. Specifically, we observe\nstrong positive correlations where increased training steps lead to increases\nin the spontaneous code execution frequency, the average response length, and,\ncritically, the final task accuracy. This suggests a quantifiable relationship\nbetween computational effort invested in training and the emergence of\neffective, tool-augmented reasoning strategies. We implement a robust framework\nfeaturing a decoupled code execution environment and validate our findings\nacross standard RL algorithms and frameworks. Experiments show ZeroTIR\nsignificantly surpasses non-tool ZeroRL baselines on challenging math\nbenchmarks. Our findings provide a foundational understanding of how autonomous\ntool use is acquired and scales within Agent RL, offering a reproducible\nbenchmark for future studies. Code is released at\n\\href{https://github.com/yyht/openrlhf_async_pipline}{https://github.com/yyht/openrlhf\\_async\\_pipline}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) often struggle with mathematical reasoning tasks\nrequiring precise, verifiable computation. While Reinforcement Learning (RL)\nfrom outcome-based rewards enhances text-based reasoning, understanding how\nagents autonomously learn to leverage external tools like code execution\nremains crucial. We investigate RL from outcome-based rewards for\nTool-Integrated Reasoning, ZeroTIR, training base LLMs to spontaneously\ngenerate and execute Python code for mathematical problems without supervised\ntool-use examples. Our central contribution is we demonstrate that as RL\ntraining progresses, key metrics scale predictably. Specifically, we observe\nstrong positive correlations where increased training steps lead to increases\nin the spontaneous code execution frequency, the average response length, and,\ncritically, the final task accuracy. This suggests a quantifiable relationship\nbetween computational effort invested in training and the emergence of\neffective, tool-augmented reasoning strategies. We implement a robust framework\nfeaturing a decoupled code execution environment and validate our findings\nacross standard RL algorithms and frameworks. Experiments show ZeroTIR\nsignificantly surpasses non-tool ZeroRL baselines on challenging math\nbenchmarks. Our findings provide a foundational understanding of how autonomous\ntool use is acquired and scales within Agent RL, offering a reproducible\nbenchmark for future studies. Code is released at\n\\href{https://github.com/yyht/openrlhf_async_pipline}{https://github.com/yyht/openrlhf\\_async\\_pipline}."
                },
                "authors": [
                    {
                        "name": "Xinji Mai"
                    },
                    {
                        "name": "Haotian Xu"
                    },
                    {
                        "name": "Xing W"
                    },
                    {
                        "name": "Weinong Wang"
                    },
                    {
                        "name": "Yingying Zhang"
                    },
                    {
                        "name": "Wenqiang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wenqiang Zhang"
                },
                "author": "Wenqiang Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.07773v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.07773v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09108v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09108v1",
                "updated": "2025-05-14T03:33:46Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    3,
                    33,
                    46,
                    2,
                    134,
                    0
                ],
                "published": "2025-05-14T03:33:46Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    3,
                    33,
                    46,
                    2,
                    134,
                    0
                ],
                "title": "Air-Ground Collaboration for Language-Specified Missions in Unknown\n  Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Air-Ground Collaboration for Language-Specified Missions in Unknown\n  Environments"
                },
                "summary": "As autonomous robotic systems become increasingly mature, users will want to\nspecify missions at the level of intent rather than in low-level detail.\nLanguage is an expressive and intuitive medium for such mission specification.\nHowever, realizing language-guided robotic teams requires overcoming\nsignificant technical hurdles. Interpreting and realizing language-specified\nmissions requires advanced semantic reasoning. Successful heterogeneous robots\nmust effectively coordinate actions and share information across varying\nviewpoints. Additionally, communication between robots is typically\nintermittent, necessitating robust strategies that leverage communication\nopportunities to maintain coordination and achieve mission objectives. In this\nwork, we present a first-of-its-kind system where an unmanned aerial vehicle\n(UAV) and an unmanned ground vehicle (UGV) are able to collaboratively\naccomplish missions specified in natural language while reacting to changes in\nspecification on the fly. We leverage a Large Language Model (LLM)-enabled\nplanner to reason over semantic-metric maps that are built online and\nopportunistically shared between an aerial and a ground robot. We consider\ntask-driven navigation in urban and rural areas. Our system must infer\nmission-relevant semantics and actively acquire information via semantic\nmapping. In both ground and air-ground teaming experiments, we demonstrate our\nsystem on seven different natural-language specifications at up to\nkilometer-scale navigation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As autonomous robotic systems become increasingly mature, users will want to\nspecify missions at the level of intent rather than in low-level detail.\nLanguage is an expressive and intuitive medium for such mission specification.\nHowever, realizing language-guided robotic teams requires overcoming\nsignificant technical hurdles. Interpreting and realizing language-specified\nmissions requires advanced semantic reasoning. Successful heterogeneous robots\nmust effectively coordinate actions and share information across varying\nviewpoints. Additionally, communication between robots is typically\nintermittent, necessitating robust strategies that leverage communication\nopportunities to maintain coordination and achieve mission objectives. In this\nwork, we present a first-of-its-kind system where an unmanned aerial vehicle\n(UAV) and an unmanned ground vehicle (UGV) are able to collaboratively\naccomplish missions specified in natural language while reacting to changes in\nspecification on the fly. We leverage a Large Language Model (LLM)-enabled\nplanner to reason over semantic-metric maps that are built online and\nopportunistically shared between an aerial and a ground robot. We consider\ntask-driven navigation in urban and rural areas. Our system must infer\nmission-relevant semantics and actively acquire information via semantic\nmapping. In both ground and air-ground teaming experiments, we demonstrate our\nsystem on seven different natural-language specifications at up to\nkilometer-scale navigation."
                },
                "authors": [
                    {
                        "name": "Fernando Cladera"
                    },
                    {
                        "name": "Zachary Ravichandran"
                    },
                    {
                        "name": "Jason Hughes"
                    },
                    {
                        "name": "Varun Murali"
                    },
                    {
                        "name": "Carlos Nieto-Granda"
                    },
                    {
                        "name": "M. Ani Hsieh"
                    },
                    {
                        "name": "George J. Pappas"
                    },
                    {
                        "name": "Camillo J. Taylor"
                    },
                    {
                        "name": "Vijay Kumar"
                    }
                ],
                "author_detail": {
                    "name": "Vijay Kumar"
                },
                "author": "Vijay Kumar",
                "arxiv_comment": "19 pages, 24 figures, 7 tables. Submitted to T-FR",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09108v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09108v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09082v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09082v1",
                "updated": "2025-05-14T02:35:47Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    2,
                    35,
                    47,
                    2,
                    134,
                    0
                ],
                "published": "2025-05-14T02:35:47Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    2,
                    35,
                    47,
                    2,
                    134,
                    0
                ],
                "title": "CEC-Zero: Chinese Error Correction Solution Based on LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CEC-Zero: Chinese Error Correction Solution Based on LLM"
                },
                "summary": "Recent advancements in large language models (LLMs) demonstrate exceptional\nChinese text processing capabilities, particularly in Chinese Spelling\nCorrection (CSC). While LLMs outperform traditional BERT-based models in\naccuracy and robustness, challenges persist in reliability and generalization.\nThis paper proposes CEC-Zero, a novel reinforcement learning (RL) framework\nenabling LLMs to self-correct through autonomous error strategy learning\nwithout external supervision. By integrating RL with LLMs' generative power,\nthe method eliminates dependency on annotated data or auxiliary models.\nExperiments reveal RL-enhanced LLMs achieve industry-viable accuracy and\nsuperior cross-domain generalization, offering a scalable solution for\nreliability optimization in Chinese NLP applications. This breakthrough\nfacilitates LLM deployment in practical Chinese text correction scenarios while\nestablishing a new paradigm for self-improving language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) demonstrate exceptional\nChinese text processing capabilities, particularly in Chinese Spelling\nCorrection (CSC). While LLMs outperform traditional BERT-based models in\naccuracy and robustness, challenges persist in reliability and generalization.\nThis paper proposes CEC-Zero, a novel reinforcement learning (RL) framework\nenabling LLMs to self-correct through autonomous error strategy learning\nwithout external supervision. By integrating RL with LLMs' generative power,\nthe method eliminates dependency on annotated data or auxiliary models.\nExperiments reveal RL-enhanced LLMs achieve industry-viable accuracy and\nsuperior cross-domain generalization, offering a scalable solution for\nreliability optimization in Chinese NLP applications. This breakthrough\nfacilitates LLM deployment in practical Chinese text correction scenarios while\nestablishing a new paradigm for self-improving language models."
                },
                "authors": [
                    {
                        "name": "Sophie Zhang"
                    },
                    {
                        "name": "Zhiming Lin"
                    }
                ],
                "author_detail": {
                    "name": "Zhiming Lin"
                },
                "author": "Zhiming Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09082v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09082v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03146v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03146v2",
                "updated": "2025-05-14T02:31:31Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    2,
                    31,
                    31,
                    2,
                    134,
                    0
                ],
                "published": "2025-03-05T03:41:57Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    3,
                    41,
                    57,
                    2,
                    64,
                    0
                ],
                "title": "PriFFT: Privacy-preserving Federated Fine-tuning of Large Language\n  Models via Hybrid Secret Sharing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PriFFT: Privacy-preserving Federated Fine-tuning of Large Language\n  Models via Hybrid Secret Sharing"
                },
                "summary": "Fine-tuning large language models (LLMs) raises privacy concerns due to the\nrisk of exposing sensitive training data. Federated learning (FL) mitigates\nthis risk by keeping training samples on local devices, while facing the\nfollowing problems in privacy-preserving federated fine-tuning. (i) Recent\nstudies show that adversaries can still infer private information in FL. (ii)\nLLM parameters are shared publicly during federated fine-tuning, while\ndevelopers are often reluctant to disclose these parameters, posing further\nsecurity challenges. (iii) Existing works focus on secure inference of LLMs but\ndo not consider privacy-preserving fine-tuning. Inspired by the above problems,\nwe propose PriFFT, a privacy-preserving federated fine-tuning mechanism, to\nprotect both the model parameters and users' privacy. Due to considerable LLM\nparameters, we present hybrid secret sharing combining arithmetic secret\nsharing (ASS) and function secret sharing (FSS) to build secure operations and\nimplement secure layers and activation for privacy-preserving fine-tuning. To\nimprove the efficiency of privacy-preserving federated fine-tuning of LLMs, we\noptimize several secure computation protocols based on FSS, including\nreciprocal calculation, tensor products, natural exponentiation, softmax,\nsigmoid, hyperbolic tangent, and dropout. The hybrid secret sharing enables\nPriFFT to apply our optimized FSS protocols while combining ASS protocols to\nsupport complex computation without extra communication. The optimized\nprotocols reduce execution time up to 62.5% and communication overhead up to\n70.7% compared to existing protocols. Besides, PriFFT reduces execution time\nand communication overhead in privacy-preserving fine-tuning up to 59.1%$ and\n77.0%$ without accuracy drop compared to the existing secret sharing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning large language models (LLMs) raises privacy concerns due to the\nrisk of exposing sensitive training data. Federated learning (FL) mitigates\nthis risk by keeping training samples on local devices, while facing the\nfollowing problems in privacy-preserving federated fine-tuning. (i) Recent\nstudies show that adversaries can still infer private information in FL. (ii)\nLLM parameters are shared publicly during federated fine-tuning, while\ndevelopers are often reluctant to disclose these parameters, posing further\nsecurity challenges. (iii) Existing works focus on secure inference of LLMs but\ndo not consider privacy-preserving fine-tuning. Inspired by the above problems,\nwe propose PriFFT, a privacy-preserving federated fine-tuning mechanism, to\nprotect both the model parameters and users' privacy. Due to considerable LLM\nparameters, we present hybrid secret sharing combining arithmetic secret\nsharing (ASS) and function secret sharing (FSS) to build secure operations and\nimplement secure layers and activation for privacy-preserving fine-tuning. To\nimprove the efficiency of privacy-preserving federated fine-tuning of LLMs, we\noptimize several secure computation protocols based on FSS, including\nreciprocal calculation, tensor products, natural exponentiation, softmax,\nsigmoid, hyperbolic tangent, and dropout. The hybrid secret sharing enables\nPriFFT to apply our optimized FSS protocols while combining ASS protocols to\nsupport complex computation without extra communication. The optimized\nprotocols reduce execution time up to 62.5% and communication overhead up to\n70.7% compared to existing protocols. Besides, PriFFT reduces execution time\nand communication overhead in privacy-preserving fine-tuning up to 59.1%$ and\n77.0%$ without accuracy drop compared to the existing secret sharing methods."
                },
                "authors": [
                    {
                        "name": "Zhichao You"
                    },
                    {
                        "name": "Xuewen Dong"
                    },
                    {
                        "name": "Ke Cheng"
                    },
                    {
                        "name": "Xutong Mu"
                    },
                    {
                        "name": "Jiaxuan Fu"
                    },
                    {
                        "name": "Shiyang Ma"
                    },
                    {
                        "name": "Qiang Qu"
                    },
                    {
                        "name": "Yulong Shen"
                    }
                ],
                "author_detail": {
                    "name": "Yulong Shen"
                },
                "author": "Yulong Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03146v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03146v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09081v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09081v1",
                "updated": "2025-05-14T02:29:46Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    2,
                    29,
                    46,
                    2,
                    134,
                    0
                ],
                "published": "2025-05-14T02:29:46Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    2,
                    29,
                    46,
                    2,
                    134,
                    0
                ],
                "title": "SALM: A Multi-Agent Framework for Language Model-Driven Social Network\n  Simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SALM: A Multi-Agent Framework for Language Model-Driven Social Network\n  Simulation"
                },
                "summary": "Contemporary approaches to agent-based modeling (ABM) of social systems have\ntraditionally emphasized rule-based behaviors, limiting their ability to\ncapture nuanced dynamics by moving beyond predefined rules and leveraging\ncontextual understanding from LMs of human social interaction. This paper\npresents SALM (Social Agent LM Framework), a novel approach for integrating\nlanguage models (LMs) into social network simulation that achieves\nunprecedented temporal stability in multi-agent scenarios. Our primary\ncontributions include: (1) a hierarchical prompting architecture enabling\nstable simulation beyond 4,000 timesteps while reducing token usage by 73%, (2)\nan attention-based memory system achieving 80% cache hit rates (95% CI [78%,\n82%]) with sub-linear memory growth of 9.5%, and (3) formal bounds on\npersonality stability. Through extensive validation against SNAP ego networks,\nwe demonstrate the first LLM-based framework capable of modeling long-term\nsocial phenomena while maintaining empirically validated behavioral fidelity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contemporary approaches to agent-based modeling (ABM) of social systems have\ntraditionally emphasized rule-based behaviors, limiting their ability to\ncapture nuanced dynamics by moving beyond predefined rules and leveraging\ncontextual understanding from LMs of human social interaction. This paper\npresents SALM (Social Agent LM Framework), a novel approach for integrating\nlanguage models (LMs) into social network simulation that achieves\nunprecedented temporal stability in multi-agent scenarios. Our primary\ncontributions include: (1) a hierarchical prompting architecture enabling\nstable simulation beyond 4,000 timesteps while reducing token usage by 73%, (2)\nan attention-based memory system achieving 80% cache hit rates (95% CI [78%,\n82%]) with sub-linear memory growth of 9.5%, and (3) formal bounds on\npersonality stability. Through extensive validation against SNAP ego networks,\nwe demonstrate the first LLM-based framework capable of modeling long-term\nsocial phenomena while maintaining empirically validated behavioral fidelity."
                },
                "authors": [
                    {
                        "name": "Gaurav Koley"
                    }
                ],
                "author_detail": {
                    "name": "Gaurav Koley"
                },
                "author": "Gaurav Koley",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09081v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09081v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09116v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09116v2",
                "updated": "2025-05-14T02:29:41Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    2,
                    29,
                    41,
                    2,
                    134,
                    0
                ],
                "published": "2024-11-14T01:29:36Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    1,
                    29,
                    36,
                    3,
                    319,
                    0
                ],
                "title": "P-MMEval: A Parallel Multilingual Multitask Benchmark for Consistent\n  Evaluation of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "P-MMEval: A Parallel Multilingual Multitask Benchmark for Consistent\n  Evaluation of LLMs"
                },
                "summary": "Recent advancements in large language models (LLMs) showcase varied\nmultilingual capabilities across tasks like translation, code generation, and\nreasoning. Previous assessments often limited their scope to fundamental\nnatural language processing (NLP) or isolated capability-specific tasks. To\nalleviate this drawback, we aim to present a comprehensive multilingual\nmultitask benchmark. First, we introduce P-MMEval, a large-scale benchmark\ncovering effective fundamental and capability-specialized datasets.\nFurthermore, P-MMEval delivers consistent language coverage across various\ndatasets and provides parallel samples. Finally, we conduct extensive\nexperiments on representative multilingual model series to compare performances\nacross models and tasks, explore the relationship between multilingual\nperformances and factors such as tasks, model sizes, languages, and prompts,\nand examine the effectiveness of knowledge transfer from English to other\nlanguages. The resulting insights are intended to offer valuable guidance for\nfuture research. The dataset is available at\nhttps://huggingface.co/datasets/Qwen/P-MMEval.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) showcase varied\nmultilingual capabilities across tasks like translation, code generation, and\nreasoning. Previous assessments often limited their scope to fundamental\nnatural language processing (NLP) or isolated capability-specific tasks. To\nalleviate this drawback, we aim to present a comprehensive multilingual\nmultitask benchmark. First, we introduce P-MMEval, a large-scale benchmark\ncovering effective fundamental and capability-specialized datasets.\nFurthermore, P-MMEval delivers consistent language coverage across various\ndatasets and provides parallel samples. Finally, we conduct extensive\nexperiments on representative multilingual model series to compare performances\nacross models and tasks, explore the relationship between multilingual\nperformances and factors such as tasks, model sizes, languages, and prompts,\nand examine the effectiveness of knowledge transfer from English to other\nlanguages. The resulting insights are intended to offer valuable guidance for\nfuture research. The dataset is available at\nhttps://huggingface.co/datasets/Qwen/P-MMEval."
                },
                "authors": [
                    {
                        "name": "Yidan Zhang"
                    },
                    {
                        "name": "Yu Wan"
                    },
                    {
                        "name": "Boyi Deng"
                    },
                    {
                        "name": "Baosong Yang"
                    },
                    {
                        "name": "Haoran Wei"
                    },
                    {
                        "name": "Fei Huang"
                    },
                    {
                        "name": "Bowen Yu"
                    },
                    {
                        "name": "Junyang Lin"
                    },
                    {
                        "name": "Fei Huang"
                    },
                    {
                        "name": "Jingren Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jingren Zhou"
                },
                "author": "Jingren Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09116v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09116v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09074v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09074v1",
                "updated": "2025-05-14T02:21:23Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    2,
                    21,
                    23,
                    2,
                    134,
                    0
                ],
                "published": "2025-05-14T02:21:23Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    2,
                    21,
                    23,
                    2,
                    134,
                    0
                ],
                "title": "Deployable and Generalizable Motion Prediction: Taxonomy, Open\n  Challenges and Future Directions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deployable and Generalizable Motion Prediction: Taxonomy, Open\n  Challenges and Future Directions"
                },
                "summary": "Motion prediction, the anticipation of future agent states or scene\nevolution, is rooted in human cognition, bridging perception and\ndecision-making. It enables intelligent systems, such as robots and\nself-driving cars, to act safely in dynamic, human-involved environments, and\ninforms broader time-series reasoning challenges. With advances in methods,\nrepresentations, and datasets, the field has seen rapid progress, reflected in\nquickly evolving benchmark results. Yet, when state-of-the-art methods are\ndeployed in the real world, they often struggle to generalize to open-world\nconditions and fall short of deployment standards. This reveals a gap between\nresearch benchmarks, which are often idealized or ill-posed, and real-world\ncomplexity.\n  To address this gap, this survey revisits the generalization and\ndeployability of motion prediction models, with an emphasis on the applications\nof robotics, autonomous driving, and human motion. We first offer a\ncomprehensive taxonomy of motion prediction methods, covering representations,\nmodeling strategies, application domains, and evaluation protocols. We then\nstudy two key challenges: (1) how to push motion prediction models to be\ndeployable to realistic deployment standards, where motion prediction does not\nact in a vacuum, but functions as one module of closed-loop autonomy stacks -\nit takes input from the localization and perception, and informs downstream\nplanning and control. 2) how to generalize motion prediction models from\nlimited seen scenarios/datasets to the open-world settings. Throughout the\npaper, we highlight critical open challenges to guide future work, aiming to\nrecalibrate the community's efforts, fostering progress that is not only\nmeasurable but also meaningful for real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Motion prediction, the anticipation of future agent states or scene\nevolution, is rooted in human cognition, bridging perception and\ndecision-making. It enables intelligent systems, such as robots and\nself-driving cars, to act safely in dynamic, human-involved environments, and\ninforms broader time-series reasoning challenges. With advances in methods,\nrepresentations, and datasets, the field has seen rapid progress, reflected in\nquickly evolving benchmark results. Yet, when state-of-the-art methods are\ndeployed in the real world, they often struggle to generalize to open-world\nconditions and fall short of deployment standards. This reveals a gap between\nresearch benchmarks, which are often idealized or ill-posed, and real-world\ncomplexity.\n  To address this gap, this survey revisits the generalization and\ndeployability of motion prediction models, with an emphasis on the applications\nof robotics, autonomous driving, and human motion. We first offer a\ncomprehensive taxonomy of motion prediction methods, covering representations,\nmodeling strategies, application domains, and evaluation protocols. We then\nstudy two key challenges: (1) how to push motion prediction models to be\ndeployable to realistic deployment standards, where motion prediction does not\nact in a vacuum, but functions as one module of closed-loop autonomy stacks -\nit takes input from the localization and perception, and informs downstream\nplanning and control. 2) how to generalize motion prediction models from\nlimited seen scenarios/datasets to the open-world settings. Throughout the\npaper, we highlight critical open challenges to guide future work, aiming to\nrecalibrate the community's efforts, fostering progress that is not only\nmeasurable but also meaningful for real-world applications."
                },
                "authors": [
                    {
                        "name": "Letian Wang"
                    },
                    {
                        "name": "Marc-Antoine Lavoie"
                    },
                    {
                        "name": "Sandro Papais"
                    },
                    {
                        "name": "Barza Nisar"
                    },
                    {
                        "name": "Yuxiao Chen"
                    },
                    {
                        "name": "Wenhao Ding"
                    },
                    {
                        "name": "Boris Ivanovic"
                    },
                    {
                        "name": "Hao Shao"
                    },
                    {
                        "name": "Abulikemu Abuduweili"
                    },
                    {
                        "name": "Evan Cook"
                    },
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Peter Karkus"
                    },
                    {
                        "name": "Jiachen Li"
                    },
                    {
                        "name": "Changliu Liu"
                    },
                    {
                        "name": "Marco Pavone"
                    },
                    {
                        "name": "Steven Waslander"
                    }
                ],
                "author_detail": {
                    "name": "Steven Waslander"
                },
                "author": "Steven Waslander",
                "arxiv_comment": "Initial draft, 162 pages, 40 figures, 13 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09074v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09074v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11197v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11197v4",
                "updated": "2025-05-14T02:12:43Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    2,
                    12,
                    43,
                    2,
                    134,
                    0
                ],
                "published": "2025-03-14T08:43:53Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    8,
                    43,
                    53,
                    4,
                    73,
                    0
                ],
                "title": "Reinforcement Learning Outperforms Supervised Fine-Tuning: A Case Study\n  on Audio Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning Outperforms Supervised Fine-Tuning: A Case Study\n  on Audio Question Answering"
                },
                "summary": "Recently, reinforcement learning (RL) has been shown to greatly enhance the\nreasoning capabilities of large language models (LLMs), and RL-based approaches\nhave been progressively applied to visual multimodal tasks. However, the audio\nmodality has largely been overlooked in these developments. Thus, we conduct a\nseries of RL explorations in audio understanding and reasoning, specifically\nfocusing on the audio question answering (AQA) task. We leverage the group\nrelative policy optimization (GRPO) algorithm to Qwen2-Audio-7B-Instruct, and\nour experiments demonstrated state-of-the-art performance on the MMAU Test-mini\nbenchmark, achieving an accuracy rate of 64.5%. The main findings in this\ntechnical report are as follows: 1) The GRPO algorithm can be effectively\napplied to large audio language models (LALMs), even when the model has only\n8.2B parameters; 2) With only 38k post-training samples, RL significantly\noutperforms supervised fine-tuning (SFT), indicating that RL-based approaches\ncan be effective without large datasets; 3) The explicit reasoning process has\nnot shown significant benefits for AQA tasks, and how to efficiently utilize\ndeep thinking remains an open question for further research; 4) LALMs still lag\nfar behind humans auditory-language reasoning, suggesting that the RL-based\napproaches warrant further exploration. Our project is available at\nhttps://github.com/xiaomi-research/r1-aqa and\nhttps://huggingface.co/mispeech/r1-aqa.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, reinforcement learning (RL) has been shown to greatly enhance the\nreasoning capabilities of large language models (LLMs), and RL-based approaches\nhave been progressively applied to visual multimodal tasks. However, the audio\nmodality has largely been overlooked in these developments. Thus, we conduct a\nseries of RL explorations in audio understanding and reasoning, specifically\nfocusing on the audio question answering (AQA) task. We leverage the group\nrelative policy optimization (GRPO) algorithm to Qwen2-Audio-7B-Instruct, and\nour experiments demonstrated state-of-the-art performance on the MMAU Test-mini\nbenchmark, achieving an accuracy rate of 64.5%. The main findings in this\ntechnical report are as follows: 1) The GRPO algorithm can be effectively\napplied to large audio language models (LALMs), even when the model has only\n8.2B parameters; 2) With only 38k post-training samples, RL significantly\noutperforms supervised fine-tuning (SFT), indicating that RL-based approaches\ncan be effective without large datasets; 3) The explicit reasoning process has\nnot shown significant benefits for AQA tasks, and how to efficiently utilize\ndeep thinking remains an open question for further research; 4) LALMs still lag\nfar behind humans auditory-language reasoning, suggesting that the RL-based\napproaches warrant further exploration. Our project is available at\nhttps://github.com/xiaomi-research/r1-aqa and\nhttps://huggingface.co/mispeech/r1-aqa."
                },
                "authors": [
                    {
                        "name": "Gang Li"
                    },
                    {
                        "name": "Jizhong Liu"
                    },
                    {
                        "name": "Heinrich Dinkel"
                    },
                    {
                        "name": "Yadong Niu"
                    },
                    {
                        "name": "Junbo Zhang"
                    },
                    {
                        "name": "Jian Luan"
                    }
                ],
                "author_detail": {
                    "name": "Jian Luan"
                },
                "author": "Jian Luan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11197v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11197v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08699v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08699v2",
                "updated": "2025-05-14T02:10:29Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    2,
                    10,
                    29,
                    2,
                    134,
                    0
                ],
                "published": "2025-05-13T15:58:57Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    15,
                    58,
                    57,
                    1,
                    133,
                    0
                ],
                "title": "Granite-speech: open-source speech-aware LLMs with strong English ASR\n  capabilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Granite-speech: open-source speech-aware LLMs with strong English ASR\n  capabilities"
                },
                "summary": "Granite-speech LLMs are compact and efficient speech language models\nspecifically designed for English ASR and automatic speech translation (AST).\nThe models were trained by modality aligning the 2B and 8B parameter variants\nof granite-3.3-instruct to speech on publicly available open-source corpora\ncontaining audio inputs and text targets consisting of either human transcripts\nfor ASR or automatically generated translations for AST. Comprehensive\nbenchmarking shows that on English ASR, which was our primary focus, they\noutperform several competitors' models that were trained on orders of magnitude\nmore proprietary data, and they keep pace on English-to-X AST for major\nEuropean languages, Japanese, and Chinese. The speech-specific components are:\na conformer acoustic encoder using block attention and self-conditioning\ntrained with connectionist temporal classification, a windowed\nquery-transformer speech modality adapter used to do temporal downsampling of\nthe acoustic embeddings and map them to the LLM text embedding space, and LoRA\nadapters to further fine-tune the text LLM. Granite-speech-3.3 operates in two\nmodes: in speech mode, it performs ASR and AST by activating the encoder,\nprojector, and LoRA adapters; in text mode, it calls the underlying\ngranite-3.3-instruct model directly (without LoRA), essentially preserving all\nthe text LLM capabilities and safety. Both models are freely available on\nHuggingFace (https://huggingface.co/ibm-granite/granite-speech-3.3-2b and\nhttps://huggingface.co/ibm-granite/granite-speech-3.3-8b) and can be used for\nboth research and commercial purposes under a permissive Apache 2.0 license.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Granite-speech LLMs are compact and efficient speech language models\nspecifically designed for English ASR and automatic speech translation (AST).\nThe models were trained by modality aligning the 2B and 8B parameter variants\nof granite-3.3-instruct to speech on publicly available open-source corpora\ncontaining audio inputs and text targets consisting of either human transcripts\nfor ASR or automatically generated translations for AST. Comprehensive\nbenchmarking shows that on English ASR, which was our primary focus, they\noutperform several competitors' models that were trained on orders of magnitude\nmore proprietary data, and they keep pace on English-to-X AST for major\nEuropean languages, Japanese, and Chinese. The speech-specific components are:\na conformer acoustic encoder using block attention and self-conditioning\ntrained with connectionist temporal classification, a windowed\nquery-transformer speech modality adapter used to do temporal downsampling of\nthe acoustic embeddings and map them to the LLM text embedding space, and LoRA\nadapters to further fine-tune the text LLM. Granite-speech-3.3 operates in two\nmodes: in speech mode, it performs ASR and AST by activating the encoder,\nprojector, and LoRA adapters; in text mode, it calls the underlying\ngranite-3.3-instruct model directly (without LoRA), essentially preserving all\nthe text LLM capabilities and safety. Both models are freely available on\nHuggingFace (https://huggingface.co/ibm-granite/granite-speech-3.3-2b and\nhttps://huggingface.co/ibm-granite/granite-speech-3.3-8b) and can be used for\nboth research and commercial purposes under a permissive Apache 2.0 license."
                },
                "authors": [
                    {
                        "name": "George Saon"
                    },
                    {
                        "name": "Avihu Dekel"
                    },
                    {
                        "name": "Alexander Brooks"
                    },
                    {
                        "name": "Tohru Nagano"
                    },
                    {
                        "name": "Abraham Daniels"
                    },
                    {
                        "name": "Aharon Satt"
                    },
                    {
                        "name": "Ashish Mittal"
                    },
                    {
                        "name": "Brian Kingsbury"
                    },
                    {
                        "name": "David Haws"
                    },
                    {
                        "name": "Edmilson Morais"
                    },
                    {
                        "name": "Gakuto Kurata"
                    },
                    {
                        "name": "Hagai Aronowitz"
                    },
                    {
                        "name": "Ibrahim Ibrahim"
                    },
                    {
                        "name": "Jeff Kuo"
                    },
                    {
                        "name": "Kate Soule"
                    },
                    {
                        "name": "Luis Lastras"
                    },
                    {
                        "name": "Masayuki Suzuki"
                    },
                    {
                        "name": "Ron Hoory"
                    },
                    {
                        "name": "Samuel Thomas"
                    },
                    {
                        "name": "Sashi Novitasari"
                    },
                    {
                        "name": "Takashi Fukuda"
                    },
                    {
                        "name": "Vishal Sunder"
                    },
                    {
                        "name": "Xiaodong Cui"
                    },
                    {
                        "name": "Zvi Kons"
                    }
                ],
                "author_detail": {
                    "name": "Zvi Kons"
                },
                "author": "Zvi Kons",
                "arxiv_comment": "7 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08699v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08699v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04717v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04717v4",
                "updated": "2025-05-14T01:48:30Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    1,
                    48,
                    30,
                    2,
                    134,
                    0
                ],
                "published": "2025-04-07T04:00:08Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    4,
                    0,
                    8,
                    0,
                    97,
                    0
                ],
                "title": "Beyond Single-Turn: A Survey on Multi-Turn Interactions with Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Single-Turn: A Survey on Multi-Turn Interactions with Large\n  Language Models"
                },
                "summary": "Recent advancements in large language models (LLMs) have revolutionized their\nability to handle single-turn tasks, yet real-world applications demand\nsophisticated multi-turn interactions. This survey provides a comprehensive\nreview of recent advancements in evaluating and enhancing multi-turn\ninteractions in LLMs. Focusing on task-specific scenarios, from instruction\nfollowing in diverse domains such as math and coding to complex conversational\nengagements in roleplay, healthcare, education, and even adversarial jailbreak\nsettings, we systematically examine the challenges of maintaining context,\ncoherence, fairness, and responsiveness over prolonged dialogues. The paper\norganizes current benchmarks and datasets into coherent categories that reflect\nthe evolving landscape of multi-turn dialogue evaluation. In addition, we\nreview a range of enhancement methodologies under multi-turn settings,\nincluding model-centric strategies (contextual learning, supervised\nfine-tuning, reinforcement learning, and new architectures), external\nintegration approaches (memory-augmented, retrieval-based methods, and\nknowledge graph), and agent-based techniques for collaborative interactions.\nFinally, we discuss open challenges and propose future directions for research\nto further advance the robustness and effectiveness of multi-turn interactions\nin LLMs. Related resources and papers are available at\nhttps://github.com/yubol-cmu/Awesome-Multi-Turn-LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have revolutionized their\nability to handle single-turn tasks, yet real-world applications demand\nsophisticated multi-turn interactions. This survey provides a comprehensive\nreview of recent advancements in evaluating and enhancing multi-turn\ninteractions in LLMs. Focusing on task-specific scenarios, from instruction\nfollowing in diverse domains such as math and coding to complex conversational\nengagements in roleplay, healthcare, education, and even adversarial jailbreak\nsettings, we systematically examine the challenges of maintaining context,\ncoherence, fairness, and responsiveness over prolonged dialogues. The paper\norganizes current benchmarks and datasets into coherent categories that reflect\nthe evolving landscape of multi-turn dialogue evaluation. In addition, we\nreview a range of enhancement methodologies under multi-turn settings,\nincluding model-centric strategies (contextual learning, supervised\nfine-tuning, reinforcement learning, and new architectures), external\nintegration approaches (memory-augmented, retrieval-based methods, and\nknowledge graph), and agent-based techniques for collaborative interactions.\nFinally, we discuss open challenges and propose future directions for research\nto further advance the robustness and effectiveness of multi-turn interactions\nin LLMs. Related resources and papers are available at\nhttps://github.com/yubol-cmu/Awesome-Multi-Turn-LLMs."
                },
                "authors": [
                    {
                        "name": "Yubo Li"
                    },
                    {
                        "name": "Xiaobin Shen"
                    },
                    {
                        "name": "Xinyu Yao"
                    },
                    {
                        "name": "Xueying Ding"
                    },
                    {
                        "name": "Yidi Miao"
                    },
                    {
                        "name": "Ramayya Krishnan"
                    },
                    {
                        "name": "Rema Padman"
                    }
                ],
                "author_detail": {
                    "name": "Rema Padman"
                },
                "author": "Rema Padman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04717v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04717v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08221v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08221v2",
                "updated": "2025-05-14T01:44:34Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    1,
                    44,
                    34,
                    2,
                    134,
                    0
                ],
                "published": "2025-05-13T04:37:56Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    4,
                    37,
                    56,
                    1,
                    133,
                    0
                ],
                "title": "Performance Analysis of Cooperative Integrated Sensing and\n  Communications for 6G Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance Analysis of Cooperative Integrated Sensing and\n  Communications for 6G Networks"
                },
                "summary": "In this work, we aim to effectively characterize the performance of\ncooperative integrated sensing and communication (ISAC) networks and to reveal\nhow performance metrics relate to network parameters. To this end, we introduce\na generalized stochastic geometry framework to model the cooperative ISAC\nnetworks, which approximates the spatial randomness of the network deployment.\nBased on this framework, we derive analytical expressions for key performance\nmetrics in both communication and sensing domains, with a particular focus on\ncommunication coverage probability and radar information rate. The analytical\nexpressions derived explicitly highlight how performance metrics depend on\nnetwork parameters, thereby offering valuable insights into the deployment and\ndesign of cooperative ISAC networks. In the end, we validate the theoretical\nperformance analysis through Monte Carlo simulation results. Our results\ndemonstrate that increasing the number of cooperative base stations (BSs)\nsignificantly improves both metrics, while increasing the BS deployment density\nhas a limited impact on communication coverage probability but substantially\nenhances the radar information rate. Additionally, increasing the number of\ntransmit antennas is effective when the total number of transmit antennas is\nrelatively small. The incremental performance gain reduces with the increase of\nthe number of transmit antennas, suggesting that indiscriminately increasing\nantennas is not an efficient strategy to improve the performance of the system\nin cooperative ISAC networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we aim to effectively characterize the performance of\ncooperative integrated sensing and communication (ISAC) networks and to reveal\nhow performance metrics relate to network parameters. To this end, we introduce\na generalized stochastic geometry framework to model the cooperative ISAC\nnetworks, which approximates the spatial randomness of the network deployment.\nBased on this framework, we derive analytical expressions for key performance\nmetrics in both communication and sensing domains, with a particular focus on\ncommunication coverage probability and radar information rate. The analytical\nexpressions derived explicitly highlight how performance metrics depend on\nnetwork parameters, thereby offering valuable insights into the deployment and\ndesign of cooperative ISAC networks. In the end, we validate the theoretical\nperformance analysis through Monte Carlo simulation results. Our results\ndemonstrate that increasing the number of cooperative base stations (BSs)\nsignificantly improves both metrics, while increasing the BS deployment density\nhas a limited impact on communication coverage probability but substantially\nenhances the radar information rate. Additionally, increasing the number of\ntransmit antennas is effective when the total number of transmit antennas is\nrelatively small. The incremental performance gain reduces with the increase of\nthe number of transmit antennas, suggesting that indiscriminately increasing\nantennas is not an efficient strategy to improve the performance of the system\nin cooperative ISAC networks."
                },
                "authors": [
                    {
                        "name": "Dongsheng Sui"
                    },
                    {
                        "name": "Cunhua Pan"
                    },
                    {
                        "name": "Hong Ren"
                    },
                    {
                        "name": "Jiahua Wan"
                    },
                    {
                        "name": "Liuchang Zhuo"
                    },
                    {
                        "name": "Jing Jin"
                    },
                    {
                        "name": "Qixing Wang"
                    },
                    {
                        "name": "Jiangzhou Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jiangzhou Wang"
                },
                "author": "Jiangzhou Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08221v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08221v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08167v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08167v2",
                "updated": "2025-05-14T01:35:33Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    1,
                    35,
                    33,
                    2,
                    134,
                    0
                ],
                "published": "2025-05-13T02:05:25Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    2,
                    5,
                    25,
                    1,
                    133,
                    0
                ],
                "title": "Fusing Bidirectional Chains of Thought and Reward Mechanisms A Method\n  for Enhancing Question-Answering Capabilities of Large Language Models for\n  Chinese Intangible Cultural Heritage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fusing Bidirectional Chains of Thought and Reward Mechanisms A Method\n  for Enhancing Question-Answering Capabilities of Large Language Models for\n  Chinese Intangible Cultural Heritage"
                },
                "summary": "The rapid development of large language models (LLMs) has provided\nsignificant support and opportunities for the advancement of domain-specific\nLLMs. However, fine-tuning these large models using Intangible Cultural\nHeritage (ICH) data inevitably faces challenges such as bias, incorrect\nknowledge inheritance, and catastrophic forgetting. To address these issues, we\npropose a novel training method that integrates a bidirectional chains of\nthought and a reward mechanism. This method is built upon ICH-Qwen, a large\nlanguage model specifically designed for the field of intangible cultural\nheritage. The proposed method enables the model to not only perform forward\nreasoning but also enhances the accuracy of the generated answers by utilizing\nreverse questioning and reverse reasoning to activate the model's latent\nknowledge. Additionally, a reward mechanism is introduced during training to\noptimize the decision-making process. This mechanism improves the quality of\nthe model's outputs through structural and content evaluations with different\nweighting schemes. We conduct comparative experiments on ICH-Qwen, with results\ndemonstrating that our method outperforms 0-shot, step-by-step reasoning,\nknowledge distillation, and question augmentation methods in terms of accuracy,\nBleu-4, and Rouge-L scores on the question-answering task. Furthermore, the\npaper highlights the effectiveness of combining the bidirectional chains of\nthought and reward mechanism through ablation experiments. In addition, a\nseries of generalizability experiments are conducted, with results showing that\nthe proposed method yields improvements on various domain-specific datasets and\nadvanced models in areas such as Finance, Wikidata, and StrategyQA. This\ndemonstrates that the method is adaptable to multiple domains and provides a\nvaluable approach for model training in future applications across diverse\nfields.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development of large language models (LLMs) has provided\nsignificant support and opportunities for the advancement of domain-specific\nLLMs. However, fine-tuning these large models using Intangible Cultural\nHeritage (ICH) data inevitably faces challenges such as bias, incorrect\nknowledge inheritance, and catastrophic forgetting. To address these issues, we\npropose a novel training method that integrates a bidirectional chains of\nthought and a reward mechanism. This method is built upon ICH-Qwen, a large\nlanguage model specifically designed for the field of intangible cultural\nheritage. The proposed method enables the model to not only perform forward\nreasoning but also enhances the accuracy of the generated answers by utilizing\nreverse questioning and reverse reasoning to activate the model's latent\nknowledge. Additionally, a reward mechanism is introduced during training to\noptimize the decision-making process. This mechanism improves the quality of\nthe model's outputs through structural and content evaluations with different\nweighting schemes. We conduct comparative experiments on ICH-Qwen, with results\ndemonstrating that our method outperforms 0-shot, step-by-step reasoning,\nknowledge distillation, and question augmentation methods in terms of accuracy,\nBleu-4, and Rouge-L scores on the question-answering task. Furthermore, the\npaper highlights the effectiveness of combining the bidirectional chains of\nthought and reward mechanism through ablation experiments. In addition, a\nseries of generalizability experiments are conducted, with results showing that\nthe proposed method yields improvements on various domain-specific datasets and\nadvanced models in areas such as Finance, Wikidata, and StrategyQA. This\ndemonstrates that the method is adaptable to multiple domains and provides a\nvaluable approach for model training in future applications across diverse\nfields."
                },
                "authors": [
                    {
                        "name": "Ruilin Liu"
                    },
                    {
                        "name": "Zhixiao Zhao"
                    },
                    {
                        "name": "Jieqiong Li"
                    },
                    {
                        "name": "Chang Liu"
                    },
                    {
                        "name": "Dongbo Wang"
                    }
                ],
                "author_detail": {
                    "name": "Dongbo Wang"
                },
                "author": "Dongbo Wang",
                "arxiv_comment": "22 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08167v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08167v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09056v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09056v1",
                "updated": "2025-05-14T01:21:46Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    1,
                    21,
                    46,
                    2,
                    134,
                    0
                ],
                "published": "2025-05-14T01:21:46Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    1,
                    21,
                    46,
                    2,
                    134,
                    0
                ],
                "title": "A Comprehensive Analysis of Large Language Model Outputs: Similarity,\n  Diversity, and Bias",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Comprehensive Analysis of Large Language Model Outputs: Similarity,\n  Diversity, and Bias"
                },
                "summary": "Large Language Models (LLMs) represent a major step toward artificial general\nintelligence, significantly advancing our ability to interact with technology.\nWhile LLMs perform well on Natural Language Processing tasks -- such as\ntranslation, generation, code writing, and summarization -- questions remain\nabout their output similarity, variability, and ethical implications. For\ninstance, how similar are texts generated by the same model? How does this\ncompare across different models? And which models best uphold ethical\nstandards? To investigate, we used 5{,}000 prompts spanning diverse tasks like\ngeneration, explanation, and rewriting. This resulted in approximately 3\nmillion texts from 12 LLMs, including proprietary and open-source systems from\nOpenAI, Google, Microsoft, Meta, and Mistral. Key findings include: (1) outputs\nfrom the same LLM are more similar to each other than to human-written texts;\n(2) models like WizardLM-2-8x22b generate highly similar outputs, while GPT-4\nproduces more varied responses; (3) LLM writing styles differ significantly,\nwith Llama 3 and Mistral showing higher similarity, and GPT-4 standing out for\ndistinctiveness; (4) differences in vocabulary and tone underscore the\nlinguistic uniqueness of LLM-generated content; (5) some LLMs demonstrate\ngreater gender balance and reduced bias. These results offer new insights into\nthe behavior and diversity of LLM outputs, helping guide future development and\nethical evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) represent a major step toward artificial general\nintelligence, significantly advancing our ability to interact with technology.\nWhile LLMs perform well on Natural Language Processing tasks -- such as\ntranslation, generation, code writing, and summarization -- questions remain\nabout their output similarity, variability, and ethical implications. For\ninstance, how similar are texts generated by the same model? How does this\ncompare across different models? And which models best uphold ethical\nstandards? To investigate, we used 5{,}000 prompts spanning diverse tasks like\ngeneration, explanation, and rewriting. This resulted in approximately 3\nmillion texts from 12 LLMs, including proprietary and open-source systems from\nOpenAI, Google, Microsoft, Meta, and Mistral. Key findings include: (1) outputs\nfrom the same LLM are more similar to each other than to human-written texts;\n(2) models like WizardLM-2-8x22b generate highly similar outputs, while GPT-4\nproduces more varied responses; (3) LLM writing styles differ significantly,\nwith Llama 3 and Mistral showing higher similarity, and GPT-4 standing out for\ndistinctiveness; (4) differences in vocabulary and tone underscore the\nlinguistic uniqueness of LLM-generated content; (5) some LLMs demonstrate\ngreater gender balance and reduced bias. These results offer new insights into\nthe behavior and diversity of LLM outputs, helping guide future development and\nethical evaluation."
                },
                "authors": [
                    {
                        "name": "Brandon Smith"
                    },
                    {
                        "name": "Mohamed Reda Bouadjenek"
                    },
                    {
                        "name": "Tahsin Alamgir Kheya"
                    },
                    {
                        "name": "Phillip Dawson"
                    },
                    {
                        "name": "Sunil Aryal"
                    }
                ],
                "author_detail": {
                    "name": "Sunil Aryal"
                },
                "author": "Sunil Aryal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09056v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09056v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.07089v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.07089v2",
                "updated": "2025-05-14T00:44:05Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    0,
                    44,
                    5,
                    2,
                    134,
                    0
                ],
                "published": "2025-05-11T18:38:00Z",
                "published_parsed": [
                    2025,
                    5,
                    11,
                    18,
                    38,
                    0,
                    6,
                    131,
                    0
                ],
                "title": "RefPentester: A Knowledge-Informed Self-Reflective Penetration Testing\n  Framework Based on Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RefPentester: A Knowledge-Informed Self-Reflective Penetration Testing\n  Framework Based on Large Language Models"
                },
                "summary": "Automated penetration testing (AutoPT) powered by large language models\n(LLMs) has gained attention for its ability to automate ethical hacking\nprocesses and identify vulnerabilities in target systems by leveraging the\nintrinsic knowledge of LLMs. However, existing LLM-based AutoPT frameworks\noften underperform compared to human experts in challenging tasks for several\nreasons: the imbalanced knowledge used in LLM training, short-sighted planning\nin the planning process, and hallucinations during command generation. In\naddition, the penetration testing (PT) process, with its trial-and-error\nnature, is limited by existing frameworks that lack mechanisms to learn from\nprevious failed operations, restricting adaptive improvement of PT strategies.\nTo address these limitations, we propose a knowledge-informed self-reflective\nPT framework powered by LLMs, called RefPentester, which is an AutoPT framework\ndesigned to assist human operators in identifying the current stage of the PT\nprocess, selecting appropriate tactic and technique for the stage, choosing\nsuggested action, providing step-by-step operational guidance, and learning\nfrom previous failed operations. We also modeled the PT process as a\nseven-state Stage Machine to integrate the proposed framework effectively. The\nevaluation shows that RefPentester can successfully reveal credentials on Hack\nThe Box's Sau machine, outperforming the baseline GPT-4o model by 16.7%. Across\nPT stages, RefPentester also demonstrates superior success rates on PT stage\ntransitions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated penetration testing (AutoPT) powered by large language models\n(LLMs) has gained attention for its ability to automate ethical hacking\nprocesses and identify vulnerabilities in target systems by leveraging the\nintrinsic knowledge of LLMs. However, existing LLM-based AutoPT frameworks\noften underperform compared to human experts in challenging tasks for several\nreasons: the imbalanced knowledge used in LLM training, short-sighted planning\nin the planning process, and hallucinations during command generation. In\naddition, the penetration testing (PT) process, with its trial-and-error\nnature, is limited by existing frameworks that lack mechanisms to learn from\nprevious failed operations, restricting adaptive improvement of PT strategies.\nTo address these limitations, we propose a knowledge-informed self-reflective\nPT framework powered by LLMs, called RefPentester, which is an AutoPT framework\ndesigned to assist human operators in identifying the current stage of the PT\nprocess, selecting appropriate tactic and technique for the stage, choosing\nsuggested action, providing step-by-step operational guidance, and learning\nfrom previous failed operations. We also modeled the PT process as a\nseven-state Stage Machine to integrate the proposed framework effectively. The\nevaluation shows that RefPentester can successfully reveal credentials on Hack\nThe Box's Sau machine, outperforming the baseline GPT-4o model by 16.7%. Across\nPT stages, RefPentester also demonstrates superior success rates on PT stage\ntransitions."
                },
                "authors": [
                    {
                        "name": "Hanzheng Dai"
                    },
                    {
                        "name": "Yuanliang Li"
                    },
                    {
                        "name": "Zhibo Zhang"
                    },
                    {
                        "name": "Jun Yan"
                    }
                ],
                "author_detail": {
                    "name": "Jun Yan"
                },
                "author": "Jun Yan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.07089v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.07089v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09039v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09039v1",
                "updated": "2025-05-14T00:39:47Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    0,
                    39,
                    47,
                    2,
                    134,
                    0
                ],
                "published": "2025-05-14T00:39:47Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    0,
                    39,
                    47,
                    2,
                    134,
                    0
                ],
                "title": "Atomic Consistency Preference Optimization for Long-Form Question\n  Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Atomic Consistency Preference Optimization for Long-Form Question\n  Answering"
                },
                "summary": "Large Language Models (LLMs) frequently produce factoid hallucinations -\nplausible yet incorrect answers. A common mitigation strategy is model\nalignment, which improves factual accuracy by training on curated factual and\nnon-factual pairs. However, this approach often relies on a stronger model\n(e.g., GPT-4) or an external knowledge base to assess factual correctness,\nwhich may not always be accessible. To address this, we propose Atomic\nConsistency Preference Optimization (ACPO), a self-supervised preference-tuning\nmethod that enhances factual accuracy without external supervision. ACPO\nleverages atomic consistency signals, i.e., the agreement of individual facts\nacross multiple stochastic responses, to identify high- and low-quality data\npairs for model alignment. By eliminating the need for costly GPT calls, ACPO\nprovides a scalable and efficient approach to improving factoid\nquestion-answering. Despite being self-supervised, empirical results\ndemonstrate that ACPO outperforms FactAlign, a strong supervised alignment\nbaseline, by 1.95 points on the LongFact and BioGen datasets, highlighting its\neffectiveness in enhancing factual reliability without relying on external\nmodels or knowledge bases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) frequently produce factoid hallucinations -\nplausible yet incorrect answers. A common mitigation strategy is model\nalignment, which improves factual accuracy by training on curated factual and\nnon-factual pairs. However, this approach often relies on a stronger model\n(e.g., GPT-4) or an external knowledge base to assess factual correctness,\nwhich may not always be accessible. To address this, we propose Atomic\nConsistency Preference Optimization (ACPO), a self-supervised preference-tuning\nmethod that enhances factual accuracy without external supervision. ACPO\nleverages atomic consistency signals, i.e., the agreement of individual facts\nacross multiple stochastic responses, to identify high- and low-quality data\npairs for model alignment. By eliminating the need for costly GPT calls, ACPO\nprovides a scalable and efficient approach to improving factoid\nquestion-answering. Despite being self-supervised, empirical results\ndemonstrate that ACPO outperforms FactAlign, a strong supervised alignment\nbaseline, by 1.95 points on the LongFact and BioGen datasets, highlighting its\neffectiveness in enhancing factual reliability without relying on external\nmodels or knowledge bases."
                },
                "authors": [
                    {
                        "name": "Jingfeng Chen"
                    },
                    {
                        "name": "Raghuveer Thirukovalluru"
                    },
                    {
                        "name": "Junlin Wang"
                    },
                    {
                        "name": "Kaiwei Luo"
                    },
                    {
                        "name": "Bhuwan Dhingra"
                    }
                ],
                "author_detail": {
                    "name": "Bhuwan Dhingra"
                },
                "author": "Bhuwan Dhingra",
                "arxiv_comment": "16 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09039v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09039v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09033v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09033v1",
                "updated": "2025-05-14T00:05:04Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    0,
                    5,
                    4,
                    2,
                    134,
                    0
                ],
                "published": "2025-05-14T00:05:04Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    0,
                    5,
                    4,
                    2,
                    134,
                    0
                ],
                "title": "Item Level Exploration Traffic Allocation in Large-scale Recommendation\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Item Level Exploration Traffic Allocation in Large-scale Recommendation\n  Systems"
                },
                "summary": "This paper contributes to addressing the item cold start problem in\nlarge-scale recommender systems, focusing on how to efficiently gain initial\nvisibility for newly ingested content. We propose an exploration system\ndesigned to efficiently allocate impressions to these fresh items. Our approach\nleverages a learned probabilistic model to predict an item's discoverability,\nwhich then informs a scalable and adaptive traffic allocation strategy. This\nsystem intelligently distributes exploration budgets, optimizing for the\nlong-term benefit of the recommendation platform. The impact is a demonstrably\nmore efficient cold-start process, leading to a significant increase in the\ndiscoverability of new content and ultimately enriching the item corpus\navailable for exploitation, as evidenced by its successful deployment in a\nlarge-scale production environment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper contributes to addressing the item cold start problem in\nlarge-scale recommender systems, focusing on how to efficiently gain initial\nvisibility for newly ingested content. We propose an exploration system\ndesigned to efficiently allocate impressions to these fresh items. Our approach\nleverages a learned probabilistic model to predict an item's discoverability,\nwhich then informs a scalable and adaptive traffic allocation strategy. This\nsystem intelligently distributes exploration budgets, optimizing for the\nlong-term benefit of the recommendation platform. The impact is a demonstrably\nmore efficient cold-start process, leading to a significant increase in the\ndiscoverability of new content and ultimately enriching the item corpus\navailable for exploitation, as evidenced by its successful deployment in a\nlarge-scale production environment."
                },
                "authors": [
                    {
                        "name": "Dong Wang"
                    },
                    {
                        "name": "Junyi Jiao"
                    },
                    {
                        "name": "Arnab Bhadury"
                    },
                    {
                        "name": "Yaping Zhang"
                    },
                    {
                        "name": "Mingyan Gao"
                    }
                ],
                "author_detail": {
                    "name": "Mingyan Gao"
                },
                "author": "Mingyan Gao",
                "arxiv_comment": "accepted by the 18th ACM Recsys Large Recsys Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09033v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09033v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09031v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09031v1",
                "updated": "2025-05-13T23:57:02Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    23,
                    57,
                    2,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T23:57:02Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    23,
                    57,
                    2,
                    1,
                    133,
                    0
                ],
                "title": "Improving the Reliability of LLMs: Combining CoT, RAG, Self-Consistency,\n  and Self-Verification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving the Reliability of LLMs: Combining CoT, RAG, Self-Consistency,\n  and Self-Verification"
                },
                "summary": "Hallucination, where large language models (LLMs) generate confident but\nincorrect or irrelevant information, remains a key limitation in their\napplication to complex, open-ended tasks. Chain-of-thought (CoT) prompting has\nemerged as a promising method for improving multistep reasoning by guiding\nmodels through intermediate steps. However, CoT alone does not fully address\nthe hallucination problem. In this work, we investigate how combining CoT with\nretrieval-augmented generation (RAG), as well as applying self-consistency and\nself-verification strategies, can reduce hallucinations and improve factual\naccuracy. By incorporating external knowledge sources during reasoning and\nenabling models to verify or revise their own outputs, we aim to generate more\naccurate and coherent responses. We present a comparative evaluation of\nbaseline LLMs against CoT, CoT+RAG, self-consistency, and self-verification\ntechniques. Our results highlight the effectiveness of each method and identify\nthe most robust approach for minimizing hallucinations while preserving fluency\nand reasoning depth.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hallucination, where large language models (LLMs) generate confident but\nincorrect or irrelevant information, remains a key limitation in their\napplication to complex, open-ended tasks. Chain-of-thought (CoT) prompting has\nemerged as a promising method for improving multistep reasoning by guiding\nmodels through intermediate steps. However, CoT alone does not fully address\nthe hallucination problem. In this work, we investigate how combining CoT with\nretrieval-augmented generation (RAG), as well as applying self-consistency and\nself-verification strategies, can reduce hallucinations and improve factual\naccuracy. By incorporating external knowledge sources during reasoning and\nenabling models to verify or revise their own outputs, we aim to generate more\naccurate and coherent responses. We present a comparative evaluation of\nbaseline LLMs against CoT, CoT+RAG, self-consistency, and self-verification\ntechniques. Our results highlight the effectiveness of each method and identify\nthe most robust approach for minimizing hallucinations while preserving fluency\nand reasoning depth."
                },
                "authors": [
                    {
                        "name": "Adarsh Kumar"
                    },
                    {
                        "name": "Hwiyoon Kim"
                    },
                    {
                        "name": "Jawahar Sai Nathani"
                    },
                    {
                        "name": "Neil Roy"
                    }
                ],
                "author_detail": {
                    "name": "Neil Roy"
                },
                "author": "Neil Roy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09031v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09031v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09027v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09027v1",
                "updated": "2025-05-13T23:47:12Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    23,
                    47,
                    12,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T23:47:12Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    23,
                    47,
                    12,
                    1,
                    133,
                    0
                ],
                "title": "Tests as Prompt: A Test-Driven-Development Benchmark for LLM Code\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tests as Prompt: A Test-Driven-Development Benchmark for LLM Code\n  Generation"
                },
                "summary": "We introduce WebApp1K, a novel benchmark for evaluating large language models\n(LLMs) in test-driven development (TDD) tasks, where test cases serve as both\nprompt and verification for code generation. Unlike traditional approaches\nrelying on natural language prompts, our benchmark emphasizes the ability of\nLLMs to interpret and implement functionality directly from test cases,\nreflecting real-world software development practices. Comprising 1000 diverse\nchallenges across 20 application domains, the benchmark evaluates LLMs on their\nability to generate compact, functional code under the constraints of context\nlength and multi-feature complexity. Our findings highlight instruction\nfollowing and in-context learning as critical capabilities for TDD success,\nsurpassing the importance of general coding proficiency or pretraining\nknowledge. Through comprehensive evaluation of 19 frontier models, we reveal\nperformance bottlenecks, such as instruction loss in long prompts, and provide\na detailed error analysis spanning multiple root causes. This work underscores\nthe practical value of TDD-specific benchmarks and lays the foundation for\nadvancing LLM capabilities in rigorous, application-driven coding scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce WebApp1K, a novel benchmark for evaluating large language models\n(LLMs) in test-driven development (TDD) tasks, where test cases serve as both\nprompt and verification for code generation. Unlike traditional approaches\nrelying on natural language prompts, our benchmark emphasizes the ability of\nLLMs to interpret and implement functionality directly from test cases,\nreflecting real-world software development practices. Comprising 1000 diverse\nchallenges across 20 application domains, the benchmark evaluates LLMs on their\nability to generate compact, functional code under the constraints of context\nlength and multi-feature complexity. Our findings highlight instruction\nfollowing and in-context learning as critical capabilities for TDD success,\nsurpassing the importance of general coding proficiency or pretraining\nknowledge. Through comprehensive evaluation of 19 frontier models, we reveal\nperformance bottlenecks, such as instruction loss in long prompts, and provide\na detailed error analysis spanning multiple root causes. This work underscores\nthe practical value of TDD-specific benchmarks and lays the foundation for\nadvancing LLM capabilities in rigorous, application-driven coding scenarios."
                },
                "authors": [
                    {
                        "name": "Yi Cui"
                    }
                ],
                "author_detail": {
                    "name": "Yi Cui"
                },
                "author": "Yi Cui",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2409.05177",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09027v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09027v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09024v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09024v1",
                "updated": "2025-05-13T23:42:36Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    23,
                    42,
                    36,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T23:42:36Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    23,
                    42,
                    36,
                    1,
                    133,
                    0
                ],
                "title": "Automated Meta Prompt Engineering for Alignment with the Theory of Mind",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Meta Prompt Engineering for Alignment with the Theory of Mind"
                },
                "summary": "We introduce a method of meta-prompting that jointly produces fluent text for\ncomplex tasks while optimizing the similarity of neural states between a\nhuman's mental expectation and a Large Language Model's (LLM) neural\nprocessing. A technique of agentic reinforcement learning is applied, in which\nan LLM as a Judge (LLMaaJ) teaches another LLM, through in-context learning,\nhow to produce content by interpreting the intended and unintended generated\ntext traits. To measure human mental beliefs around content production, users\nmodify long form AI-generated text articles before publication at the US Open\n2024 tennis Grand Slam. Now, an LLMaaJ can solve the Theory of Mind (ToM)\nalignment problem by anticipating and including human edits within the creation\nof text from an LLM. Throughout experimentation and by interpreting the results\nof a live production system, the expectations of human content reviewers had\n100% of alignment with AI 53.8% of the time with an average iteration count of\n4.38. The geometric interpretation of content traits such as factualness,\nnovelty, repetitiveness, and relevancy over a Hilbert vector space combines\nspatial volume (all trait importance) with vertices alignment (individual trait\nrelevance) enabled the LLMaaJ to optimize on Human ToM. This resulted in an\nincrease in content quality by extending the coverage of tennis action. Our\nwork that was deployed at the US Open 2024 has been used across other live\nevents within sports and entertainment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a method of meta-prompting that jointly produces fluent text for\ncomplex tasks while optimizing the similarity of neural states between a\nhuman's mental expectation and a Large Language Model's (LLM) neural\nprocessing. A technique of agentic reinforcement learning is applied, in which\nan LLM as a Judge (LLMaaJ) teaches another LLM, through in-context learning,\nhow to produce content by interpreting the intended and unintended generated\ntext traits. To measure human mental beliefs around content production, users\nmodify long form AI-generated text articles before publication at the US Open\n2024 tennis Grand Slam. Now, an LLMaaJ can solve the Theory of Mind (ToM)\nalignment problem by anticipating and including human edits within the creation\nof text from an LLM. Throughout experimentation and by interpreting the results\nof a live production system, the expectations of human content reviewers had\n100% of alignment with AI 53.8% of the time with an average iteration count of\n4.38. The geometric interpretation of content traits such as factualness,\nnovelty, repetitiveness, and relevancy over a Hilbert vector space combines\nspatial volume (all trait importance) with vertices alignment (individual trait\nrelevance) enabled the LLMaaJ to optimize on Human ToM. This resulted in an\nincrease in content quality by extending the coverage of tennis action. Our\nwork that was deployed at the US Open 2024 has been used across other live\nevents within sports and entertainment."
                },
                "authors": [
                    {
                        "name": "Aaron Baughman"
                    },
                    {
                        "name": "Rahul Agarwal"
                    },
                    {
                        "name": "Eduardo Morales"
                    },
                    {
                        "name": "Gozde Akay"
                    }
                ],
                "author_detail": {
                    "name": "Gozde Akay"
                },
                "author": "Gozde Akay",
                "arxiv_comment": "9 pages, 6 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09024v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09024v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13807v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13807v3",
                "updated": "2025-05-13T23:33:09Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    23,
                    33,
                    9,
                    1,
                    133,
                    0
                ],
                "published": "2025-04-18T17:20:27Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    17,
                    20,
                    27,
                    4,
                    108,
                    0
                ],
                "title": "DiffOG: Differentiable Policy Trajectory Optimization with\n  Generalizability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiffOG: Differentiable Policy Trajectory Optimization with\n  Generalizability"
                },
                "summary": "Imitation learning-based visuomotor policies excel at manipulation tasks but\noften produce suboptimal action trajectories compared to model-based methods.\nDirectly mapping camera data to actions via neural networks can result in jerky\nmotions and difficulties in meeting critical constraints, compromising safety\nand robustness in real-world deployment. For tasks that require high robustness\nor strict adherence to constraints, ensuring trajectory quality is crucial.\nHowever, the lack of interpretability in neural networks makes it challenging\nto generate constraint-compliant actions in a controlled manner. This paper\nintroduces differentiable policy trajectory optimization with generalizability\n(DiffOG), a learning-based trajectory optimization framework designed to\nenhance visuomotor policies. By leveraging the proposed differentiable\nformulation of trajectory optimization with transformer, DiffOG seamlessly\nintegrates policies with a generalizable optimization layer. DiffOG refines\naction trajectories to be smoother and more constraint-compliant while\nmaintaining alignment with the original demonstration distribution, thus\navoiding degradation in policy performance. We evaluated DiffOG across 11\nsimulated tasks and 2 real-world tasks. The results demonstrate that DiffOG\nsignificantly enhances the trajectory quality of visuomotor policies while\nhaving minimal impact on policy performance, outperforming trajectory\nprocessing baselines such as greedy constraint clipping and penalty-based\ntrajectory optimization. Furthermore, DiffOG achieves superior performance\ncompared to existing constrained visuomotor policy. Please visit the project\nwebsite for more details: https://zhengtongxu.github.io/diffog-website/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Imitation learning-based visuomotor policies excel at manipulation tasks but\noften produce suboptimal action trajectories compared to model-based methods.\nDirectly mapping camera data to actions via neural networks can result in jerky\nmotions and difficulties in meeting critical constraints, compromising safety\nand robustness in real-world deployment. For tasks that require high robustness\nor strict adherence to constraints, ensuring trajectory quality is crucial.\nHowever, the lack of interpretability in neural networks makes it challenging\nto generate constraint-compliant actions in a controlled manner. This paper\nintroduces differentiable policy trajectory optimization with generalizability\n(DiffOG), a learning-based trajectory optimization framework designed to\nenhance visuomotor policies. By leveraging the proposed differentiable\nformulation of trajectory optimization with transformer, DiffOG seamlessly\nintegrates policies with a generalizable optimization layer. DiffOG refines\naction trajectories to be smoother and more constraint-compliant while\nmaintaining alignment with the original demonstration distribution, thus\navoiding degradation in policy performance. We evaluated DiffOG across 11\nsimulated tasks and 2 real-world tasks. The results demonstrate that DiffOG\nsignificantly enhances the trajectory quality of visuomotor policies while\nhaving minimal impact on policy performance, outperforming trajectory\nprocessing baselines such as greedy constraint clipping and penalty-based\ntrajectory optimization. Furthermore, DiffOG achieves superior performance\ncompared to existing constrained visuomotor policy. Please visit the project\nwebsite for more details: https://zhengtongxu.github.io/diffog-website/."
                },
                "authors": [
                    {
                        "name": "Zhengtong Xu"
                    },
                    {
                        "name": "Zichen Miao"
                    },
                    {
                        "name": "Qiang Qiu"
                    },
                    {
                        "name": "Zhe Zhang"
                    },
                    {
                        "name": "Yu She"
                    }
                ],
                "author_detail": {
                    "name": "Yu She"
                },
                "author": "Yu She",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13807v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13807v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09021v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09021v1",
                "updated": "2025-05-13T23:31:32Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    23,
                    31,
                    32,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T23:31:32Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    23,
                    31,
                    32,
                    1,
                    133,
                    0
                ],
                "title": "AI-Mediated Code Comment Improvement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI-Mediated Code Comment Improvement"
                },
                "summary": "This paper describes an approach to improve code comments along different\nquality axes by rewriting those comments with customized Artificial\nIntelligence (AI)-based tools. We conduct an empirical study followed by\ngrounded theory qualitative analysis to determine the quality axes to improve.\nThen we propose a procedure using a Large Language Model (LLM) to rewrite\nexisting code comments along the quality axes. We implement our procedure using\nGPT-4o, then distil the results into a smaller model capable of being run\nin-house, so users can maintain data custody. We evaluate both our approach\nusing GPT-4o and the distilled model versions. We show in an evaluation how our\nprocedure improves code comments along the quality axes. We release all data\nand source code in an online repository for reproducibility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper describes an approach to improve code comments along different\nquality axes by rewriting those comments with customized Artificial\nIntelligence (AI)-based tools. We conduct an empirical study followed by\ngrounded theory qualitative analysis to determine the quality axes to improve.\nThen we propose a procedure using a Large Language Model (LLM) to rewrite\nexisting code comments along the quality axes. We implement our procedure using\nGPT-4o, then distil the results into a smaller model capable of being run\nin-house, so users can maintain data custody. We evaluate both our approach\nusing GPT-4o and the distilled model versions. We show in an evaluation how our\nprocedure improves code comments along the quality axes. We release all data\nand source code in an online repository for reproducibility."
                },
                "authors": [
                    {
                        "name": "Maria Dhakal"
                    },
                    {
                        "name": "Chia-Yi Su"
                    },
                    {
                        "name": "Robert Wallace"
                    },
                    {
                        "name": "Chris Fakhimi"
                    },
                    {
                        "name": "Aakash Bansal"
                    },
                    {
                        "name": "Toby Li"
                    },
                    {
                        "name": "Yu Huang"
                    },
                    {
                        "name": "Collin McMillan"
                    }
                ],
                "author_detail": {
                    "name": "Collin McMillan"
                },
                "author": "Collin McMillan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09021v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09021v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08996v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08996v1",
                "updated": "2025-05-13T22:18:51Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    22,
                    18,
                    51,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T22:18:51Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    22,
                    18,
                    51,
                    1,
                    133,
                    0
                ],
                "title": "A suite of LMs comprehend puzzle statements as well as humans",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A suite of LMs comprehend puzzle statements as well as humans"
                },
                "summary": "Recent claims suggest that large language models (LMs) underperform humans in\ncomprehending minimally complex English statements (Dentella et al., 2024).\nHere, we revisit those findings and argue that human performance was\noverestimated, while LLM abilities were underestimated. Using the same stimuli,\nwe report a preregistered study comparing human responses in two conditions:\none allowed rereading (replicating the original study), and one that restricted\nrereading (a more naturalistic comprehension test). Human accuracy dropped\nsignificantly when rereading was restricted (73%), falling below that of\nFalcon-180B-Chat (76%) and GPT-4 (81%). The newer GPT-o1 model achieves perfect\naccuracy. Results further show that both humans and models are\ndisproportionately challenged by queries involving potentially reciprocal\nactions (e.g., kissing), suggesting shared pragmatic sensitivities rather than\nmodel-specific deficits. Additional analyses using Llama-2-70B log\nprobabilities, a recoding of open-ended model responses, and grammaticality\nratings of other sentences reveal systematic underestimation of model\nperformance. We find that GPT-4o can align with either naive or expert\ngrammaticality judgments, depending on prompt framing. These findings\nunderscore the need for more careful experimental design and coding practices\nin LLM evaluation, and they challenge the assumption that current models are\ninherently weaker than humans at language comprehension.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent claims suggest that large language models (LMs) underperform humans in\ncomprehending minimally complex English statements (Dentella et al., 2024).\nHere, we revisit those findings and argue that human performance was\noverestimated, while LLM abilities were underestimated. Using the same stimuli,\nwe report a preregistered study comparing human responses in two conditions:\none allowed rereading (replicating the original study), and one that restricted\nrereading (a more naturalistic comprehension test). Human accuracy dropped\nsignificantly when rereading was restricted (73%), falling below that of\nFalcon-180B-Chat (76%) and GPT-4 (81%). The newer GPT-o1 model achieves perfect\naccuracy. Results further show that both humans and models are\ndisproportionately challenged by queries involving potentially reciprocal\nactions (e.g., kissing), suggesting shared pragmatic sensitivities rather than\nmodel-specific deficits. Additional analyses using Llama-2-70B log\nprobabilities, a recoding of open-ended model responses, and grammaticality\nratings of other sentences reveal systematic underestimation of model\nperformance. We find that GPT-4o can align with either naive or expert\ngrammaticality judgments, depending on prompt framing. These findings\nunderscore the need for more careful experimental design and coding practices\nin LLM evaluation, and they challenge the assumption that current models are\ninherently weaker than humans at language comprehension."
                },
                "authors": [
                    {
                        "name": "Adele E Goldberg"
                    },
                    {
                        "name": "Supantho Rakshit"
                    },
                    {
                        "name": "Jennifer Hu"
                    },
                    {
                        "name": "Kyle Mahowald"
                    }
                ],
                "author_detail": {
                    "name": "Kyle Mahowald"
                },
                "author": "Kyle Mahowald",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08996v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08996v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08981v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08981v1",
                "updated": "2025-05-13T21:46:56Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    21,
                    46,
                    56,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T21:46:56Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    21,
                    46,
                    56,
                    1,
                    133,
                    0
                ],
                "title": "ITERA-LLM: Boosting Sub-8-Bit Large Language Model Inference via\n  Iterative Tensor Decomposition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ITERA-LLM: Boosting Sub-8-Bit Large Language Model Inference via\n  Iterative Tensor Decomposition"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) have demonstrated\nimpressive capabilities as their scale expands to billions of parameters.\nDeploying these large-scale models on resource-constrained platforms presents\nsignificant challenges, with post-training fixed-point quantization often used\nas a model compression technique. However, quantization-only methods typically\nlead to significant accuracy degradation in LLMs when precision falls below 8\nbits. This paper addresses this challenge through a software-hardware co-design\nframework, ITERA-LLM, which integrates sub-8-bit quantization with SVD-based\niterative low-rank tensor decomposition for error compensation, leading to\nhigher compression ratios and reduced computational complexity. The proposed\napproach is complemented by a hardware-aware Design Space Exploration (DSE)\nprocess that optimizes accuracy, latency, and resource utilization, tailoring\nthe configuration to the specific requirements of the targeted LLM. Our results\nshow that ITERA-LLM achieves linear layer latency reduction of up to 41.1%,\ncompared to quantization-only baseline approach while maintaining similar model\naccuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) have demonstrated\nimpressive capabilities as their scale expands to billions of parameters.\nDeploying these large-scale models on resource-constrained platforms presents\nsignificant challenges, with post-training fixed-point quantization often used\nas a model compression technique. However, quantization-only methods typically\nlead to significant accuracy degradation in LLMs when precision falls below 8\nbits. This paper addresses this challenge through a software-hardware co-design\nframework, ITERA-LLM, which integrates sub-8-bit quantization with SVD-based\niterative low-rank tensor decomposition for error compensation, leading to\nhigher compression ratios and reduced computational complexity. The proposed\napproach is complemented by a hardware-aware Design Space Exploration (DSE)\nprocess that optimizes accuracy, latency, and resource utilization, tailoring\nthe configuration to the specific requirements of the targeted LLM. Our results\nshow that ITERA-LLM achieves linear layer latency reduction of up to 41.1%,\ncompared to quantization-only baseline approach while maintaining similar model\naccuracy."
                },
                "authors": [
                    {
                        "name": "Keran Zheng"
                    },
                    {
                        "name": "Yinting Huang"
                    },
                    {
                        "name": "Zhewen Yu"
                    },
                    {
                        "name": "Christos-Savvas Bouganis"
                    }
                ],
                "author_detail": {
                    "name": "Christos-Savvas Bouganis"
                },
                "author": "Christos-Savvas Bouganis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08981v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08981v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13223v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13223v3",
                "updated": "2025-05-13T21:39:21Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    21,
                    39,
                    21,
                    1,
                    133,
                    0
                ],
                "published": "2025-01-22T21:08:30Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    21,
                    8,
                    30,
                    2,
                    22,
                    0
                ],
                "title": "A Comprehensive Social Bias Audit of Contrastive Vision Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Comprehensive Social Bias Audit of Contrastive Vision Language Models"
                },
                "summary": "In the domain of text-to-image generative models, biases inherent in training\ndatasets often propagate into generated content, posing significant ethical\nchallenges, particularly in socially sensitive contexts. We introduce FairCoT,\na novel framework that enhances fairness in text-to-image models through\nChain-of-Thought (CoT) reasoning within multimodal generative large language\nmodels. FairCoT employs iterative CoT refinement to systematically mitigate\nbiases, and dynamically adjusts textual prompts in real time, ensuring diverse\nand equitable representation in generated images. By integrating iterative\nreasoning processes, FairCoT addresses the limitations of zero-shot CoT in\nsensitive scenarios, balancing creativity with ethical responsibility.\nExperimental evaluations across popular text-to-image systems--including DALL-E\nand various Stable Diffusion variants--demonstrate that FairCoT significantly\nenhances fairness and diversity without sacrificing image quality or semantic\nfidelity. By combining robust reasoning, lightweight deployment, and\nextensibility to multiple models, FairCoT represents a promising step toward\nmore socially responsible and transparent AI-driven content generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the domain of text-to-image generative models, biases inherent in training\ndatasets often propagate into generated content, posing significant ethical\nchallenges, particularly in socially sensitive contexts. We introduce FairCoT,\na novel framework that enhances fairness in text-to-image models through\nChain-of-Thought (CoT) reasoning within multimodal generative large language\nmodels. FairCoT employs iterative CoT refinement to systematically mitigate\nbiases, and dynamically adjusts textual prompts in real time, ensuring diverse\nand equitable representation in generated images. By integrating iterative\nreasoning processes, FairCoT addresses the limitations of zero-shot CoT in\nsensitive scenarios, balancing creativity with ethical responsibility.\nExperimental evaluations across popular text-to-image systems--including DALL-E\nand various Stable Diffusion variants--demonstrate that FairCoT significantly\nenhances fairness and diversity without sacrificing image quality or semantic\nfidelity. By combining robust reasoning, lightweight deployment, and\nextensibility to multiple models, FairCoT represents a promising step toward\nmore socially responsible and transparent AI-driven content generation."
                },
                "authors": [
                    {
                        "name": "Zahraa Al Sahili"
                    },
                    {
                        "name": "Ioannis Patras"
                    },
                    {
                        "name": "Matthew Purver"
                    }
                ],
                "author_detail": {
                    "name": "Matthew Purver"
                },
                "author": "Matthew Purver",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13223v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13223v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08971v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08971v1",
                "updated": "2025-05-13T21:27:52Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    21,
                    27,
                    52,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T21:27:52Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    21,
                    27,
                    52,
                    1,
                    133,
                    0
                ],
                "title": "Prioritizing Image-Related Tokens Enhances Vision-Language Pre-Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prioritizing Image-Related Tokens Enhances Vision-Language Pre-Training"
                },
                "summary": "In standard large vision-language models (LVLMs) pre-training, the model\ntypically maximizes the joint probability of the caption conditioned on the\nimage via next-token prediction (NTP); however, since only a small subset of\ncaption tokens directly relates to the visual content, this naive NTP\nunintentionally fits the model to noise and increases the risk of\nhallucination. We present PRIOR, a simple vision-language pre-training approach\nthat addresses this issue by prioritizing image-related tokens through\ndifferential weighting in the NTP loss, drawing from the importance sampling\nframework. PRIOR introduces a reference model-a text-only large language model\n(LLM) trained on the captions without image inputs, to weight each token based\non its probability for LVLMs training. Intuitively, tokens that are directly\nrelated to the visual inputs are harder to predict without the image and thus\nreceive lower probabilities from the text-only reference LLM. During training,\nwe implement a token-specific re-weighting term based on the importance scores\nto adjust each token's loss. We implement PRIOR in two distinct settings: LVLMs\nwith visual encoders and LVLMs without visual encoders. We observe 19% and 8%\naverage relative improvement, respectively, on several vision-language\nbenchmarks compared to NTP. In addition, PRIOR exhibits superior scaling\nproperties, as demonstrated by significantly higher scaling coefficients,\nindicating greater potential for performance gains compared to NTP given\nincreasing compute and data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In standard large vision-language models (LVLMs) pre-training, the model\ntypically maximizes the joint probability of the caption conditioned on the\nimage via next-token prediction (NTP); however, since only a small subset of\ncaption tokens directly relates to the visual content, this naive NTP\nunintentionally fits the model to noise and increases the risk of\nhallucination. We present PRIOR, a simple vision-language pre-training approach\nthat addresses this issue by prioritizing image-related tokens through\ndifferential weighting in the NTP loss, drawing from the importance sampling\nframework. PRIOR introduces a reference model-a text-only large language model\n(LLM) trained on the captions without image inputs, to weight each token based\non its probability for LVLMs training. Intuitively, tokens that are directly\nrelated to the visual inputs are harder to predict without the image and thus\nreceive lower probabilities from the text-only reference LLM. During training,\nwe implement a token-specific re-weighting term based on the importance scores\nto adjust each token's loss. We implement PRIOR in two distinct settings: LVLMs\nwith visual encoders and LVLMs without visual encoders. We observe 19% and 8%\naverage relative improvement, respectively, on several vision-language\nbenchmarks compared to NTP. In addition, PRIOR exhibits superior scaling\nproperties, as demonstrated by significantly higher scaling coefficients,\nindicating greater potential for performance gains compared to NTP given\nincreasing compute and data."
                },
                "authors": [
                    {
                        "name": "Yangyi Chen"
                    },
                    {
                        "name": "Hao Peng"
                    },
                    {
                        "name": "Tong Zhang"
                    },
                    {
                        "name": "Heng Ji"
                    }
                ],
                "author_detail": {
                    "name": "Heng Ji"
                },
                "author": "Heng Ji",
                "arxiv_comment": "The code will be available at https://github.com/Yangyi-Chen/PRIOR",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08971v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08971v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04260v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04260v2",
                "updated": "2025-05-13T21:19:59Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    21,
                    19,
                    59,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-07T09:10:51Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    9,
                    10,
                    51,
                    2,
                    127,
                    0
                ],
                "title": "Steerable Chatbots: Personalizing LLMs with Preference-Based Activation\n  Steering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Steerable Chatbots: Personalizing LLMs with Preference-Based Activation\n  Steering"
                },
                "summary": "As large language models (LLMs) improve in their capacity to serve as\npersonal AI assistants, their ability to output uniquely tailored, personalized\nresponses that align with the soft preferences of their users is essential for\nenhancing user satisfaction and retention. However, untrained lay users have\npoor prompt specification abilities and often struggle with conveying their\nlatent preferences to AI assistants. To address this, we leverage activation\nsteering to guide LLMs to align with interpretable preference dimensions during\ninference. In contrast to memory-based personalization methods that require\nlonger user history, steering is extremely lightweight and can be easily\ncontrolled by the user via an linear strength factor. We embed steering into\nthree different interactive chatbot interfaces and conduct a within-subjects\nuser study (n=14) to investigate how end users prefer to personalize their\nconversations. The results demonstrate the effectiveness of preference-based\nsteering for aligning real-world conversations with hidden user preferences,\nand highlight further insights on how diverse values around control, usability,\nand transparency lead users to prefer different interfaces.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) improve in their capacity to serve as\npersonal AI assistants, their ability to output uniquely tailored, personalized\nresponses that align with the soft preferences of their users is essential for\nenhancing user satisfaction and retention. However, untrained lay users have\npoor prompt specification abilities and often struggle with conveying their\nlatent preferences to AI assistants. To address this, we leverage activation\nsteering to guide LLMs to align with interpretable preference dimensions during\ninference. In contrast to memory-based personalization methods that require\nlonger user history, steering is extremely lightweight and can be easily\ncontrolled by the user via an linear strength factor. We embed steering into\nthree different interactive chatbot interfaces and conduct a within-subjects\nuser study (n=14) to investigate how end users prefer to personalize their\nconversations. The results demonstrate the effectiveness of preference-based\nsteering for aligning real-world conversations with hidden user preferences,\nand highlight further insights on how diverse values around control, usability,\nand transparency lead users to prefer different interfaces."
                },
                "authors": [
                    {
                        "name": "Jessica Y. Bo"
                    },
                    {
                        "name": "Tianyu Xu"
                    },
                    {
                        "name": "Ishan Chatterjee"
                    },
                    {
                        "name": "Katrina Passarella-Ward"
                    },
                    {
                        "name": "Achin Kulshrestha"
                    },
                    {
                        "name": "D Shin"
                    }
                ],
                "author_detail": {
                    "name": "D Shin"
                },
                "author": "D Shin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04260v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04260v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08933v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08933v1",
                "updated": "2025-05-13T20:01:24Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    20,
                    1,
                    24,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T20:01:24Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    20,
                    1,
                    24,
                    1,
                    133,
                    0
                ],
                "title": "Packaging HEP Heterogeneous Mini-apps for Portable Benchmarking and\n  Facility Evaluation on Modern HPCs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Packaging HEP Heterogeneous Mini-apps for Portable Benchmarking and\n  Facility Evaluation on Modern HPCs"
                },
                "summary": "High Energy Physics (HEP) experiments are making increasing use of GPUs and\nGPU dominated High Performance Computer facilities. Both the software and\nhardware of these systems are rapidly evolving, creating challenges for\nexperiments to make informed decisions as to where they wish to devote\nresources. In its first phase, the High Energy Physics Center for Computational\nExcellence (HEP-CCE) produced portable versions of a number of heterogeneous\nHEP mini-apps, such as \\ptor, FastCaloSim, Patatrack and the WireCell Toolkit,\nthat exercise a broad range of GPU characteristics, enabling cross platform and\nfacility benchmarking and evaluation. However, these mini-apps still require a\nsignificant amount of manual intervention to deploy on a new facility.\n  We present our work in developing turn-key deployments of these mini-apps,\nwhere by means of containerization and automated configuration and build\ntechniques such as Spack, we are able to quickly test new hardware, software,\nenvironments and entire facilities with minimal user intervention, and then\ntrack performance metrics over time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High Energy Physics (HEP) experiments are making increasing use of GPUs and\nGPU dominated High Performance Computer facilities. Both the software and\nhardware of these systems are rapidly evolving, creating challenges for\nexperiments to make informed decisions as to where they wish to devote\nresources. In its first phase, the High Energy Physics Center for Computational\nExcellence (HEP-CCE) produced portable versions of a number of heterogeneous\nHEP mini-apps, such as \\ptor, FastCaloSim, Patatrack and the WireCell Toolkit,\nthat exercise a broad range of GPU characteristics, enabling cross platform and\nfacility benchmarking and evaluation. However, these mini-apps still require a\nsignificant amount of manual intervention to deploy on a new facility.\n  We present our work in developing turn-key deployments of these mini-apps,\nwhere by means of containerization and automated configuration and build\ntechniques such as Spack, we are able to quickly test new hardware, software,\nenvironments and entire facilities with minimal user intervention, and then\ntrack performance metrics over time."
                },
                "authors": [
                    {
                        "name": "Mohammad Atif"
                    },
                    {
                        "name": "Pengfei Ding"
                    },
                    {
                        "name": "Ka Hei Martin Kwok"
                    },
                    {
                        "name": "Charles Leggett"
                    }
                ],
                "author_detail": {
                    "name": "Charles Leggett"
                },
                "author": "Charles Leggett",
                "arxiv_comment": "6 pages, 2 figures, to be published in CHEP 2025 Conference\n  Proceedings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08933v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08933v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-ex",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10652v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10652v2",
                "updated": "2025-05-13T19:38:19Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    19,
                    38,
                    19,
                    1,
                    133,
                    0
                ],
                "published": "2025-03-07T10:37:31Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    10,
                    37,
                    31,
                    4,
                    66,
                    0
                ],
                "title": "Simulating and Analysing Human Survey Responses with Large Language\n  Models: A Case Study in Energy Stated Preference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulating and Analysing Human Survey Responses with Large Language\n  Models: A Case Study in Energy Stated Preference"
                },
                "summary": "Survey research plays a crucial role in studies by capturing consumer\npreferences and informing policy decisions. Stated preference (SP) surveys help\nresearchers understand how individuals make trade-offs in hypothetical,\npotentially futuristic, scenarios. However, traditional methods are costly,\ntime-consuming, and affected by respondent fatigue and ethical constraints.\nLarge language models (LLMs) have shown remarkable capabilities in generating\nhuman-like responses, prompting interest in their use in survey research. This\nstudy investigates LLMs for simulating consumer choices in energy-related SP\nsurveys and explores their integration into data collection and analysis\nworkflows. Test scenarios were designed to assess the simulation performance of\nseveral LLMs (LLaMA 3.1, Mistral, GPT-3.5, DeepSeek-R1) at individual and\naggregated levels, considering prompt design, in-context learning (ICL),\nchain-of-thought (CoT) reasoning, model types, integration with traditional\nchoice models, and potential biases. While LLMs achieve accuracy above random\nguessing, performance remains insufficient for practical simulation use.\nCloud-based LLMs do not consistently outperform smaller local models.\nDeepSeek-R1 achieves the highest average accuracy (77%) and outperforms\nnon-reasoning LLMs in accuracy, factor identification, and choice distribution\nalignment. Previous SP choices are the most effective input; longer prompts\nwith more factors reduce accuracy. Mixed logit models can support LLM prompt\nrefinement. Reasoning LLMs show potential in data analysis by indicating factor\nsignificance, offering a qualitative complement to statistical models. Despite\nlimitations, pre-trained LLMs offer scalability and require minimal historical\ndata. Future work should refine prompts, further explore CoT reasoning, and\ninvestigate fine-tuning techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Survey research plays a crucial role in studies by capturing consumer\npreferences and informing policy decisions. Stated preference (SP) surveys help\nresearchers understand how individuals make trade-offs in hypothetical,\npotentially futuristic, scenarios. However, traditional methods are costly,\ntime-consuming, and affected by respondent fatigue and ethical constraints.\nLarge language models (LLMs) have shown remarkable capabilities in generating\nhuman-like responses, prompting interest in their use in survey research. This\nstudy investigates LLMs for simulating consumer choices in energy-related SP\nsurveys and explores their integration into data collection and analysis\nworkflows. Test scenarios were designed to assess the simulation performance of\nseveral LLMs (LLaMA 3.1, Mistral, GPT-3.5, DeepSeek-R1) at individual and\naggregated levels, considering prompt design, in-context learning (ICL),\nchain-of-thought (CoT) reasoning, model types, integration with traditional\nchoice models, and potential biases. While LLMs achieve accuracy above random\nguessing, performance remains insufficient for practical simulation use.\nCloud-based LLMs do not consistently outperform smaller local models.\nDeepSeek-R1 achieves the highest average accuracy (77%) and outperforms\nnon-reasoning LLMs in accuracy, factor identification, and choice distribution\nalignment. Previous SP choices are the most effective input; longer prompts\nwith more factors reduce accuracy. Mixed logit models can support LLM prompt\nrefinement. Reasoning LLMs show potential in data analysis by indicating factor\nsignificance, offering a qualitative complement to statistical models. Despite\nlimitations, pre-trained LLMs offer scalability and require minimal historical\ndata. Future work should refine prompts, further explore CoT reasoning, and\ninvestigate fine-tuning techniques."
                },
                "authors": [
                    {
                        "name": "Han Wang"
                    },
                    {
                        "name": "Jacek Pawlak"
                    },
                    {
                        "name": "Aruna Sivakumar"
                    }
                ],
                "author_detail": {
                    "name": "Aruna Sivakumar"
                },
                "author": "Aruna Sivakumar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10652v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10652v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20412v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20412v2",
                "updated": "2025-05-13T18:59:15Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    18,
                    59,
                    15,
                    1,
                    133,
                    0
                ],
                "published": "2025-04-29T04:18:51Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    4,
                    18,
                    51,
                    1,
                    119,
                    0
                ],
                "title": "CrashFixer: A crash resolution agent for the Linux kernel",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CrashFixer: A crash resolution agent for the Linux kernel"
                },
                "summary": "Code large language models (LLMs) have shown impressive capabilities on a\nmultitude of software engineering tasks. In particular, they have demonstrated\nremarkable utility in the task of code repair. However, common benchmarks used\nto evaluate the performance of code LLMs are often limited to small-scale\nsettings. In this work, we build upon kGym, which shares a benchmark for\nsystem-level Linux kernel bugs and a platform to run experiments on the Linux\nkernel.\n  This paper introduces CrashFixer, the first LLM-based software repair agent\nthat is applicable to Linux kernel bugs. Inspired by the typical workflow of a\nkernel developer, we identify the key capabilities an expert developer\nleverages to resolve a kernel crash. Using this as our guide, we revisit the\nkGym platform and identify key system improvements needed to practically run\nLLM-based agents at the scale of the Linux kernel (50K files and 20M lines of\ncode). We implement these changes by extending kGym to create an improved\nplatform - called kGymSuite, which will be open-sourced. Finally, the paper\npresents an evaluation of various repair strategies for such complex kernel\nbugs and showcases the value of explicitly generating a hypothesis before\nattempting to fix bugs in complex systems such as the Linux kernel. We also\nevaluated CrashFixer's capabilities on still open bugs, and found at least two\npatch suggestions considered plausible to resolve the reported bug.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code large language models (LLMs) have shown impressive capabilities on a\nmultitude of software engineering tasks. In particular, they have demonstrated\nremarkable utility in the task of code repair. However, common benchmarks used\nto evaluate the performance of code LLMs are often limited to small-scale\nsettings. In this work, we build upon kGym, which shares a benchmark for\nsystem-level Linux kernel bugs and a platform to run experiments on the Linux\nkernel.\n  This paper introduces CrashFixer, the first LLM-based software repair agent\nthat is applicable to Linux kernel bugs. Inspired by the typical workflow of a\nkernel developer, we identify the key capabilities an expert developer\nleverages to resolve a kernel crash. Using this as our guide, we revisit the\nkGym platform and identify key system improvements needed to practically run\nLLM-based agents at the scale of the Linux kernel (50K files and 20M lines of\ncode). We implement these changes by extending kGym to create an improved\nplatform - called kGymSuite, which will be open-sourced. Finally, the paper\npresents an evaluation of various repair strategies for such complex kernel\nbugs and showcases the value of explicitly generating a hypothesis before\nattempting to fix bugs in complex systems such as the Linux kernel. We also\nevaluated CrashFixer's capabilities on still open bugs, and found at least two\npatch suggestions considered plausible to resolve the reported bug."
                },
                "authors": [
                    {
                        "name": "Alex Mathai"
                    },
                    {
                        "name": "Chenxi Huang"
                    },
                    {
                        "name": "Suwei Ma"
                    },
                    {
                        "name": "Jihwan Kim"
                    },
                    {
                        "name": "Hailie Mitchell"
                    },
                    {
                        "name": "Aleksandr Nogikh"
                    },
                    {
                        "name": "Petros Maniatis"
                    },
                    {
                        "name": "Franjo Ivančić"
                    },
                    {
                        "name": "Junfeng Yang"
                    },
                    {
                        "name": "Baishakhi Ray"
                    }
                ],
                "author_detail": {
                    "name": "Baishakhi Ray"
                },
                "author": "Baishakhi Ray",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20412v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20412v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09650v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09650v2",
                "updated": "2025-05-13T18:54:09Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    18,
                    54,
                    9,
                    1,
                    133,
                    0
                ],
                "published": "2025-02-11T17:01:11Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    17,
                    1,
                    11,
                    1,
                    42,
                    0
                ],
                "title": "Principled Data Selection for Alignment: The Hidden Risks of Difficult\n  Examples",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Principled Data Selection for Alignment: The Hidden Risks of Difficult\n  Examples"
                },
                "summary": "The alignment of large language models (LLMs) often assumes that using more\nclean data yields better outcomes, overlooking the match between model capacity\nand example difficulty. Challenging this, we propose a new principle:\nPreference data vary in difficulty, and overly difficult examples hinder\nalignment, by exceeding the model's capacity. Through systematic\nexperimentation, we validate this principle with three key findings: (1)\npreference examples vary in difficulty, as evidenced by consistent learning\norders across alignment runs; (2) overly difficult examples significantly\ndegrade performance across four LLMs and two datasets; and (3) the capacity of\na model dictates its threshold for handling difficult examples, underscoring a\ncritical relationship between data selection and model capacity. Building on\nthis principle, we introduce Selective DPO, which filters out overly difficult\nexamples. This simple adjustment improves alignment performance by 9-16% in win\nrates on the AlpacaEval 2 benchmark compared to the DPO baseline, suppressing a\nseries of DPO variants with different algorithmic adjustments. Together, these\nresults illuminate the importance of aligning data difficulty with model\ncapacity, offering a transformative perspective for improving alignment\nstrategies in LLMs. Code is available at\nhttps://github.com/glorgao/SelectiveDPO.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The alignment of large language models (LLMs) often assumes that using more\nclean data yields better outcomes, overlooking the match between model capacity\nand example difficulty. Challenging this, we propose a new principle:\nPreference data vary in difficulty, and overly difficult examples hinder\nalignment, by exceeding the model's capacity. Through systematic\nexperimentation, we validate this principle with three key findings: (1)\npreference examples vary in difficulty, as evidenced by consistent learning\norders across alignment runs; (2) overly difficult examples significantly\ndegrade performance across four LLMs and two datasets; and (3) the capacity of\na model dictates its threshold for handling difficult examples, underscoring a\ncritical relationship between data selection and model capacity. Building on\nthis principle, we introduce Selective DPO, which filters out overly difficult\nexamples. This simple adjustment improves alignment performance by 9-16% in win\nrates on the AlpacaEval 2 benchmark compared to the DPO baseline, suppressing a\nseries of DPO variants with different algorithmic adjustments. Together, these\nresults illuminate the importance of aligning data difficulty with model\ncapacity, offering a transformative perspective for improving alignment\nstrategies in LLMs. Code is available at\nhttps://github.com/glorgao/SelectiveDPO."
                },
                "authors": [
                    {
                        "name": "Chengqian Gao"
                    },
                    {
                        "name": "Haonan Li"
                    },
                    {
                        "name": "Liu Liu"
                    },
                    {
                        "name": "Zeke Xie"
                    },
                    {
                        "name": "Peilin Zhao"
                    },
                    {
                        "name": "Zhiqiang Xu"
                    }
                ],
                "author_detail": {
                    "name": "Zhiqiang Xu"
                },
                "author": "Zhiqiang Xu",
                "arxiv_comment": "Accepted at ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09650v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09650v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08904v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08904v1",
                "updated": "2025-05-13T18:46:47Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    18,
                    46,
                    47,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T18:46:47Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    18,
                    46,
                    47,
                    1,
                    133,
                    0
                ],
                "title": "FareShare: A Tool for Labor Organizers to Estimate Lost Wages and\n  Contest Arbitrary AI and Algorithmic Deactivations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FareShare: A Tool for Labor Organizers to Estimate Lost Wages and\n  Contest Arbitrary AI and Algorithmic Deactivations"
                },
                "summary": "What happens when a rideshare driver is suddenly locked out of the platform\nconnecting them to riders, wages, and daily work? Deactivation-the abrupt\nremoval of gig workers' platform access-typically occurs through arbitrary AI\nand algorithmic decisions with little explanation or recourse. This represents\none of the most severe forms of algorithmic control and often devastates\nworkers' financial stability. Recent U.S. state policies now mandate appeals\nprocesses and recovering compensation during the period of wrongful\ndeactivation based on past earnings. Yet, labor organizers still lack effective\ntools to support these complex, error-prone workflows. We designed FareShare, a\ncomputational tool automating lost wage estimation for deactivated drivers,\nthrough a 6 month partnership with the State of Washington's largest rideshare\nlabor union. Over the following 3 months, our field deployment of FareShare\nregistered 178 account signups. We observed that the tool could reduce lost\nwage calculation time by over 95%, eliminate manual data entry errors, and\nenable legal teams to generate arbitration-ready reports more efficiently.\nBeyond these gains, the deployment also surfaced important socio-technical\nchallenges around trust, consent, and tool adoption in high-stakes labor\ncontexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What happens when a rideshare driver is suddenly locked out of the platform\nconnecting them to riders, wages, and daily work? Deactivation-the abrupt\nremoval of gig workers' platform access-typically occurs through arbitrary AI\nand algorithmic decisions with little explanation or recourse. This represents\none of the most severe forms of algorithmic control and often devastates\nworkers' financial stability. Recent U.S. state policies now mandate appeals\nprocesses and recovering compensation during the period of wrongful\ndeactivation based on past earnings. Yet, labor organizers still lack effective\ntools to support these complex, error-prone workflows. We designed FareShare, a\ncomputational tool automating lost wage estimation for deactivated drivers,\nthrough a 6 month partnership with the State of Washington's largest rideshare\nlabor union. Over the following 3 months, our field deployment of FareShare\nregistered 178 account signups. We observed that the tool could reduce lost\nwage calculation time by over 95%, eliminate manual data entry errors, and\nenable legal teams to generate arbitration-ready reports more efficiently.\nBeyond these gains, the deployment also surfaced important socio-technical\nchallenges around trust, consent, and tool adoption in high-stakes labor\ncontexts."
                },
                "authors": [
                    {
                        "name": "Varun Nagaraj Rao"
                    },
                    {
                        "name": "Samantha Dalal"
                    },
                    {
                        "name": "Andrew Schwartz"
                    },
                    {
                        "name": "Amna Liaqat"
                    },
                    {
                        "name": "Dana Calacci"
                    },
                    {
                        "name": "Andrés Monroy-Hernández"
                    }
                ],
                "author_detail": {
                    "name": "Andrés Monroy-Hernández"
                },
                "author": "Andrés Monroy-Hernández",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08904v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08904v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08903v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08903v1",
                "updated": "2025-05-13T18:45:10Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    18,
                    45,
                    10,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T18:45:10Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    18,
                    45,
                    10,
                    1,
                    133,
                    0
                ],
                "title": "Assessing and Advancing Benchmarks for Evaluating Large Language Models\n  in Software Engineering Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing and Advancing Benchmarks for Evaluating Large Language Models\n  in Software Engineering Tasks"
                },
                "summary": "Large language models (LLMs) are gaining increasing popularity in software\nengineering (SE) due to their unprecedented performance across various\napplications. These models are increasingly being utilized for a range of SE\ntasks, including requirements engineering and design, code analysis and\ngeneration, software maintenance, and quality assurance. As LLMs become more\nintegral to SE, evaluating their effectiveness is crucial for understanding\ntheir potential in this field. In recent years, substantial efforts have been\nmade to assess LLM performance in various SE tasks, resulting in the creation\nof several benchmarks tailored to this purpose. This paper offers a thorough\nreview of 191 benchmarks, addressing three main aspects: what benchmarks are\navailable, how benchmarks are constructed, and the future outlook for these\nbenchmarks. We begin by examining SE tasks such as requirements engineering and\ndesign, coding assistant, software testing, AIOPs, software maintenance, and\nquality management. We then analyze the benchmarks and their development\nprocesses, highlighting the limitations of existing benchmarks. Additionally,\nwe discuss the successes and failures of LLMs in different software tasks and\nexplore future opportunities and challenges for SE-related benchmarks. We aim\nto provide a comprehensive overview of benchmark research in SE and offer\ninsights to support the creation of more effective evaluation tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are gaining increasing popularity in software\nengineering (SE) due to their unprecedented performance across various\napplications. These models are increasingly being utilized for a range of SE\ntasks, including requirements engineering and design, code analysis and\ngeneration, software maintenance, and quality assurance. As LLMs become more\nintegral to SE, evaluating their effectiveness is crucial for understanding\ntheir potential in this field. In recent years, substantial efforts have been\nmade to assess LLM performance in various SE tasks, resulting in the creation\nof several benchmarks tailored to this purpose. This paper offers a thorough\nreview of 191 benchmarks, addressing three main aspects: what benchmarks are\navailable, how benchmarks are constructed, and the future outlook for these\nbenchmarks. We begin by examining SE tasks such as requirements engineering and\ndesign, coding assistant, software testing, AIOPs, software maintenance, and\nquality management. We then analyze the benchmarks and their development\nprocesses, highlighting the limitations of existing benchmarks. Additionally,\nwe discuss the successes and failures of LLMs in different software tasks and\nexplore future opportunities and challenges for SE-related benchmarks. We aim\nto provide a comprehensive overview of benchmark research in SE and offer\ninsights to support the creation of more effective evaluation tools."
                },
                "authors": [
                    {
                        "name": "Xing Hu"
                    },
                    {
                        "name": "Feifei Niu"
                    },
                    {
                        "name": "Junkai Chen"
                    },
                    {
                        "name": "Xin Zhou"
                    },
                    {
                        "name": "Junwei Zhang"
                    },
                    {
                        "name": "Junda He"
                    },
                    {
                        "name": "Xin Xia"
                    },
                    {
                        "name": "David Lo"
                    }
                ],
                "author_detail": {
                    "name": "David Lo"
                },
                "author": "David Lo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08903v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08903v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08902v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08902v1",
                "updated": "2025-05-13T18:44:22Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    18,
                    44,
                    22,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T18:44:22Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    18,
                    44,
                    22,
                    1,
                    133,
                    0
                ],
                "title": "Performance Gains of LLMs With Humans in a World of LLMs Versus Humans",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance Gains of LLMs With Humans in a World of LLMs Versus Humans"
                },
                "summary": "Currently, a considerable research effort is devoted to comparing LLMs to a\ngroup of human experts, where the term \"expert\" is often ill-defined or\nvariable, at best, in a state of constantly updating LLM releases. Without\nproper safeguards in place, LLMs will threaten to cause harm to the established\nstructure of safe delivery of patient care which has been carefully developed\nthroughout history to keep the safety of the patient at the forefront. A key\ndriver of LLM innovation is founded on community research efforts which, if\ncontinuing to operate under \"humans versus LLMs\" principles, will expedite this\ntrend. Therefore, research efforts moving forward must focus on effectively\ncharacterizing the safe use of LLMs in clinical settings that persist across\nthe rapid development of novel LLM models. In this communication, we\ndemonstrate that rather than comparing LLMs to humans, there is a need to\ndevelop strategies enabling efficient work of humans with LLMs in an almost\nsymbiotic manner.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Currently, a considerable research effort is devoted to comparing LLMs to a\ngroup of human experts, where the term \"expert\" is often ill-defined or\nvariable, at best, in a state of constantly updating LLM releases. Without\nproper safeguards in place, LLMs will threaten to cause harm to the established\nstructure of safe delivery of patient care which has been carefully developed\nthroughout history to keep the safety of the patient at the forefront. A key\ndriver of LLM innovation is founded on community research efforts which, if\ncontinuing to operate under \"humans versus LLMs\" principles, will expedite this\ntrend. Therefore, research efforts moving forward must focus on effectively\ncharacterizing the safe use of LLMs in clinical settings that persist across\nthe rapid development of novel LLM models. In this communication, we\ndemonstrate that rather than comparing LLMs to humans, there is a need to\ndevelop strategies enabling efficient work of humans with LLMs in an almost\nsymbiotic manner."
                },
                "authors": [
                    {
                        "name": "Lucas McCullum"
                    },
                    {
                        "name": "Pelagie Ami Agassi"
                    },
                    {
                        "name": "Leo Anthony Celi"
                    },
                    {
                        "name": "Daniel K. Ebner"
                    },
                    {
                        "name": "Chrystinne Oliveira Fernandes"
                    },
                    {
                        "name": "Rachel S. Hicklen"
                    },
                    {
                        "name": "Mkliwa Koumbia"
                    },
                    {
                        "name": "Lisa Soleymani Lehmann"
                    },
                    {
                        "name": "David Restrepo"
                    }
                ],
                "author_detail": {
                    "name": "David Restrepo"
                },
                "author": "David Restrepo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08902v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08902v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08894v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08894v1",
                "updated": "2025-05-13T18:36:18Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    18,
                    36,
                    18,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T18:36:18Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    18,
                    36,
                    18,
                    1,
                    133,
                    0
                ],
                "title": "WaLLM -- Insights from an LLM-Powered Chatbot deployment via WhatsApp",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WaLLM -- Insights from an LLM-Powered Chatbot deployment via WhatsApp"
                },
                "summary": "Recent advances in generative AI, such as ChatGPT, have transformed access to\ninformation in education, knowledge-seeking, and everyday decision-making.\nHowever, in many developing regions, access remains a challenge due to the\npersistent digital divide. To help bridge this gap, we developed WaLLM - a\ncustom AI chatbot over WhatsApp, a widely used communication platform in\ndeveloping regions. Beyond answering queries, WaLLM offers several features to\nenhance user engagement: a daily top question, suggested follow-up questions,\ntrending and recent queries, and a leaderboard-based reward system. Our service\nhas been operational for over 6 months, amassing over 14.7K queries from\napproximately 100 users. In this paper, we present WaLLM's design and a\nsystematic analysis of logs to understand user interactions. Our results show\nthat 55% of user queries seek factual information. \"Health and well-being\" was\nthe most popular topic (28%), including queries about nutrition and disease,\nsuggesting users view WaLLM as a reliable source. Two-thirds of users' activity\noccurred within 24 hours of the daily top question. Users who accessed the\n\"Leaderboard\" interacted with WaLLM 3x as those who did not. We conclude by\ndiscussing implications for culture-based customization, user interface design,\nand appropriate calibration of users' trust in AI systems for developing\nregions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in generative AI, such as ChatGPT, have transformed access to\ninformation in education, knowledge-seeking, and everyday decision-making.\nHowever, in many developing regions, access remains a challenge due to the\npersistent digital divide. To help bridge this gap, we developed WaLLM - a\ncustom AI chatbot over WhatsApp, a widely used communication platform in\ndeveloping regions. Beyond answering queries, WaLLM offers several features to\nenhance user engagement: a daily top question, suggested follow-up questions,\ntrending and recent queries, and a leaderboard-based reward system. Our service\nhas been operational for over 6 months, amassing over 14.7K queries from\napproximately 100 users. In this paper, we present WaLLM's design and a\nsystematic analysis of logs to understand user interactions. Our results show\nthat 55% of user queries seek factual information. \"Health and well-being\" was\nthe most popular topic (28%), including queries about nutrition and disease,\nsuggesting users view WaLLM as a reliable source. Two-thirds of users' activity\noccurred within 24 hours of the daily top question. Users who accessed the\n\"Leaderboard\" interacted with WaLLM 3x as those who did not. We conclude by\ndiscussing implications for culture-based customization, user interface design,\nand appropriate calibration of users' trust in AI systems for developing\nregions."
                },
                "authors": [
                    {
                        "name": "Hiba Eltigani"
                    },
                    {
                        "name": "Rukhshan Haroon"
                    },
                    {
                        "name": "Asli Kocak"
                    },
                    {
                        "name": "Abdullah Bin Faisal"
                    },
                    {
                        "name": "Noah Martin"
                    },
                    {
                        "name": "Fahad Dogar"
                    }
                ],
                "author_detail": {
                    "name": "Fahad Dogar"
                },
                "author": "Fahad Dogar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08894v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08894v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08878v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08878v1",
                "updated": "2025-05-13T18:08:12Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    18,
                    8,
                    12,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T18:08:12Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    18,
                    8,
                    12,
                    1,
                    133,
                    0
                ],
                "title": "Optimized Couplings for Watermarking Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimized Couplings for Watermarking Large Language Models"
                },
                "summary": "Large-language models (LLMs) are now able to produce text that is, in many\ncases, seemingly indistinguishable from human-generated content. This has\nfueled the development of watermarks that imprint a ``signal'' in LLM-generated\ntext with minimal perturbation of an LLM's output. This paper provides an\nanalysis of text watermarking in a one-shot setting. Through the lens of\nhypothesis testing with side information, we formulate and analyze the\nfundamental trade-off between watermark detection power and distortion in\ngenerated textual quality. We argue that a key component in watermark design is\ngenerating a coupling between the side information shared with the watermark\ndetector and a random partition of the LLM vocabulary. Our analysis identifies\nthe optimal coupling and randomization strategy under the worst-case LLM\nnext-token distribution that satisfies a min-entropy constraint. We provide a\nclosed-form expression of the resulting detection rate under the proposed\nscheme and quantify the cost in a max-min sense. Finally, we provide an array\nof numerical results, comparing the proposed scheme with the theoretical\noptimum and existing schemes, in both synthetic data and LLM watermarking. Our\ncode is available at https://github.com/Carol-Long/CC_Watermark",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-language models (LLMs) are now able to produce text that is, in many\ncases, seemingly indistinguishable from human-generated content. This has\nfueled the development of watermarks that imprint a ``signal'' in LLM-generated\ntext with minimal perturbation of an LLM's output. This paper provides an\nanalysis of text watermarking in a one-shot setting. Through the lens of\nhypothesis testing with side information, we formulate and analyze the\nfundamental trade-off between watermark detection power and distortion in\ngenerated textual quality. We argue that a key component in watermark design is\ngenerating a coupling between the side information shared with the watermark\ndetector and a random partition of the LLM vocabulary. Our analysis identifies\nthe optimal coupling and randomization strategy under the worst-case LLM\nnext-token distribution that satisfies a min-entropy constraint. We provide a\nclosed-form expression of the resulting detection rate under the proposed\nscheme and quantify the cost in a max-min sense. Finally, we provide an array\nof numerical results, comparing the proposed scheme with the theoretical\noptimum and existing schemes, in both synthetic data and LLM watermarking. Our\ncode is available at https://github.com/Carol-Long/CC_Watermark"
                },
                "authors": [
                    {
                        "name": "Dor Tsur"
                    },
                    {
                        "name": "Carol Xuan Long"
                    },
                    {
                        "name": "Claudio Mayrink Verdun"
                    },
                    {
                        "name": "Hsiang Hsu"
                    },
                    {
                        "name": "Haim Permuter"
                    },
                    {
                        "name": "Flavio P. Calmon"
                    }
                ],
                "author_detail": {
                    "name": "Flavio P. Calmon"
                },
                "author": "Flavio P. Calmon",
                "arxiv_comment": "Accepted at ISIT25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08878v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08878v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15823v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15823v4",
                "updated": "2025-05-13T18:06:09Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    18,
                    6,
                    9,
                    1,
                    133,
                    0
                ],
                "published": "2025-02-20T03:48:00Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    3,
                    48,
                    0,
                    3,
                    51,
                    0
                ],
                "title": "InductionBench: LLMs Fail in the Simplest Complexity Class",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InductionBench: LLMs Fail in the Simplest Complexity Class"
                },
                "summary": "Large language models (LLMs) have shown remarkable improvements in reasoning\nand many existing benchmarks have been addressed by models such as o1 and o3\neither fully or partially. However, a majority of these benchmarks emphasize\ndeductive reasoning, including mathematical and coding tasks in which rules\nsuch as mathematical axioms or programming syntax are clearly defined, based on\nwhich LLMs can plan and apply these rules to arrive at a solution. In contrast,\ninductive reasoning, where one infers the underlying rules from observed data,\nremains less explored. Such inductive processes lie at the heart of scientific\ndiscovery, as they enable researchers to extract general principles from\nempirical observations. To assess whether LLMs possess this capacity, we\nintroduce InductionBench, a new benchmark designed to evaluate the inductive\nreasoning ability of LLMs. Our experimental findings reveal that even the most\nadvanced models available struggle to master the simplest complexity classes\nwithin the subregular hierarchy of functions, highlighting a notable deficiency\nin current LLMs' inductive reasoning capabilities. Coda and data are available\nhttps://github.com/Wenyueh/inductive_reasoning_benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown remarkable improvements in reasoning\nand many existing benchmarks have been addressed by models such as o1 and o3\neither fully or partially. However, a majority of these benchmarks emphasize\ndeductive reasoning, including mathematical and coding tasks in which rules\nsuch as mathematical axioms or programming syntax are clearly defined, based on\nwhich LLMs can plan and apply these rules to arrive at a solution. In contrast,\ninductive reasoning, where one infers the underlying rules from observed data,\nremains less explored. Such inductive processes lie at the heart of scientific\ndiscovery, as they enable researchers to extract general principles from\nempirical observations. To assess whether LLMs possess this capacity, we\nintroduce InductionBench, a new benchmark designed to evaluate the inductive\nreasoning ability of LLMs. Our experimental findings reveal that even the most\nadvanced models available struggle to master the simplest complexity classes\nwithin the subregular hierarchy of functions, highlighting a notable deficiency\nin current LLMs' inductive reasoning capabilities. Coda and data are available\nhttps://github.com/Wenyueh/inductive_reasoning_benchmark."
                },
                "authors": [
                    {
                        "name": "Wenyue Hua"
                    },
                    {
                        "name": "Tyler Wong"
                    },
                    {
                        "name": "Sun Fei"
                    },
                    {
                        "name": "Liangming Pan"
                    },
                    {
                        "name": "Adam Jardine"
                    },
                    {
                        "name": "William Yang Wang"
                    }
                ],
                "author_detail": {
                    "name": "William Yang Wang"
                },
                "author": "William Yang Wang",
                "arxiv_comment": "25 pages, 10 figures, more details including examples and prompts are\n  added",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15823v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15823v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.FL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]