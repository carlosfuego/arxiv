[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2411.19730v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19730v1",
                "updated": "2024-11-29T14:23:25Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    14,
                    23,
                    25,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T14:23:25Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    14,
                    23,
                    25,
                    4,
                    334,
                    0
                ],
                "title": "Ten Ways in which Virtual Reality Differs from Video Streaming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ten Ways in which Virtual Reality Differs from Video Streaming"
                },
                "summary": "Virtual Reality (VR) applications have a number of unique characteristics\nthat set them apart from traditional video streaming. These characteristics\nhave major implications on the design of VR rendering, adaptation, prefetching,\ncaching, and transport mechanisms. This paper contrasts VR to video streaming,\nstored 2D video streaming in particular, and discusses how to rethink system\nand network support for VR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Virtual Reality (VR) applications have a number of unique characteristics\nthat set them apart from traditional video streaming. These characteristics\nhave major implications on the design of VR rendering, adaptation, prefetching,\ncaching, and transport mechanisms. This paper contrasts VR to video streaming,\nstored 2D video streaming in particular, and discusses how to rethink system\nand network support for VR."
                },
                "authors": [
                    {
                        "name": "Gustavo de Veciana"
                    },
                    {
                        "name": "Sonia Fahmy"
                    },
                    {
                        "name": "George Kesidis"
                    },
                    {
                        "name": "Voicu Popescu"
                    }
                ],
                "author_detail": {
                    "name": "Voicu Popescu"
                },
                "author": "Voicu Popescu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19730v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19730v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19574v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19574v1",
                "updated": "2024-11-29T09:42:38Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    9,
                    42,
                    38,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T09:42:38Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    9,
                    42,
                    38,
                    4,
                    334,
                    0
                ],
                "title": "KV Shifting Attention Enhances Language Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV Shifting Attention Enhances Language Modeling"
                },
                "summary": "The current large language models are mainly based on decode-only structure\ntransformers, which have great in-context learning (ICL) capabilities. It is\ngenerally believed that the important foundation of its ICL capability is the\ninduction heads mechanism, which requires at least two layers attention. In\norder to more efficiently implement the ability of the model's induction, we\nrevisit the induction heads mechanism and proposed a KV shifting attention. We\ntheoretically prove that the KV shifting attention reducing the model's\nrequirements for the depth and width of the induction heads mechanism. Our\nexperimental results demonstrate that KV shifting attention is beneficial to\nlearning induction heads and language modeling, which lead to better\nperformance or faster convergence from toy models to the pre-training models\nwith more than 10 B parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The current large language models are mainly based on decode-only structure\ntransformers, which have great in-context learning (ICL) capabilities. It is\ngenerally believed that the important foundation of its ICL capability is the\ninduction heads mechanism, which requires at least two layers attention. In\norder to more efficiently implement the ability of the model's induction, we\nrevisit the induction heads mechanism and proposed a KV shifting attention. We\ntheoretically prove that the KV shifting attention reducing the model's\nrequirements for the depth and width of the induction heads mechanism. Our\nexperimental results demonstrate that KV shifting attention is beneficial to\nlearning induction heads and language modeling, which lead to better\nperformance or faster convergence from toy models to the pre-training models\nwith more than 10 B parameters."
                },
                "authors": [
                    {
                        "name": "Mingyu Xu"
                    },
                    {
                        "name": "Wei Cheng"
                    },
                    {
                        "name": "Bingning Wang"
                    },
                    {
                        "name": "Weipeng Chen"
                    }
                ],
                "author_detail": {
                    "name": "Weipeng Chen"
                },
                "author": "Weipeng Chen",
                "arxiv_comment": "22 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19574v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19574v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.07533v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.07533v3",
                "updated": "2024-11-29T08:48:01Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    8,
                    48,
                    1,
                    4,
                    334,
                    0
                ],
                "published": "2024-05-13T08:03:32Z",
                "published_parsed": [
                    2024,
                    5,
                    13,
                    8,
                    3,
                    32,
                    0,
                    134,
                    0
                ],
                "title": "DID Link: Authentication in TLS with Decentralized Identifiers and\n  Verifiable Credentials",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DID Link: Authentication in TLS with Decentralized Identifiers and\n  Verifiable Credentials"
                },
                "summary": "Authentication in TLS is predominately carried out with X.509 digital\ncertificates issued by certificate authorities (CA). The centralized nature of\ncurrent public key infrastructures, however, comes along with severe risks,\nsuch as single points of failure and susceptibility to cyber-attacks,\npotentially undermining the security and trustworthiness of the entire system.\nWith Decentralized Identifiers (DID) alongside distributed ledger technology,\nit becomes technically feasible to prove ownership of a unique identifier\nwithout requiring an attestation of the proof's public key by a centralized and\ntherefore vulnerable CA. This article presents DID Link, a novel authentication\nscheme for TLS 1.3 that empowers entities to authenticate in a TLS-compliant\nway with self-issued X.509 certificates that are equipped with ledger-anchored\nDIDs instead of CA-issued identifiers. It facilitates the exchange of\ntamper-proof and 3rd-party attested claims in the form of DID-bound Verifiable\nCredentials after the TLS handshake to complete the authentication with a full\nidentification of the communication partner. A prototypical implementation\nshows comparable TLS handshake durations of DID Link if verification material\nis cached and reasonable prolongations if it is obtained from a ledger. The\nsignificant speed improvement of the resulting TLS channel over a widely used,\nDID-based alternative transport protocol on the application layer demonstrates\nthe potential of DID Link to become a viable solution for the establishment of\nsecure and trustful end-to-end communication links with decentrally managed\ndigital identities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Authentication in TLS is predominately carried out with X.509 digital\ncertificates issued by certificate authorities (CA). The centralized nature of\ncurrent public key infrastructures, however, comes along with severe risks,\nsuch as single points of failure and susceptibility to cyber-attacks,\npotentially undermining the security and trustworthiness of the entire system.\nWith Decentralized Identifiers (DID) alongside distributed ledger technology,\nit becomes technically feasible to prove ownership of a unique identifier\nwithout requiring an attestation of the proof's public key by a centralized and\ntherefore vulnerable CA. This article presents DID Link, a novel authentication\nscheme for TLS 1.3 that empowers entities to authenticate in a TLS-compliant\nway with self-issued X.509 certificates that are equipped with ledger-anchored\nDIDs instead of CA-issued identifiers. It facilitates the exchange of\ntamper-proof and 3rd-party attested claims in the form of DID-bound Verifiable\nCredentials after the TLS handshake to complete the authentication with a full\nidentification of the communication partner. A prototypical implementation\nshows comparable TLS handshake durations of DID Link if verification material\nis cached and reasonable prolongations if it is obtained from a ledger. The\nsignificant speed improvement of the resulting TLS channel over a widely used,\nDID-based alternative transport protocol on the application layer demonstrates\nthe potential of DID Link to become a viable solution for the establishment of\nsecure and trustful end-to-end communication links with decentrally managed\ndigital identities."
                },
                "authors": [
                    {
                        "name": "Sandro Rodriguez Garzon"
                    },
                    {
                        "name": "Dennis Natusch"
                    },
                    {
                        "name": "Artur Philipp"
                    },
                    {
                        "name": "Axel KÃ¼pper"
                    },
                    {
                        "name": "Hans Joachim Einsiedler"
                    },
                    {
                        "name": "Daniela Schneider"
                    }
                ],
                "author_detail": {
                    "name": "Daniela Schneider"
                },
                "author": "Daniela Schneider",
                "arxiv_comment": "Accepted by and presented at 21st Annual International Conference on\n  Privacy, Security, and Trust (PST2024). Publication by IEEE still pending",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.07533v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.07533v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18191v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18191v2",
                "updated": "2024-11-29T08:33:49Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    8,
                    33,
                    49,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-27T10:14:38Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    10,
                    14,
                    38,
                    2,
                    332,
                    0
                ],
                "title": "InputSnatch: Stealing Input in LLM Services via Timing Side-Channel\n  Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InputSnatch: Stealing Input in LLM Services via Timing Side-Channel\n  Attacks"
                },
                "summary": "Large language models (LLMs) possess extensive knowledge and\nquestion-answering capabilities, having been widely deployed in\nprivacy-sensitive domains like finance and medical consultation. During LLM\ninferences, cache-sharing methods are commonly employed to enhance efficiency\nby reusing cached states or responses for the same or similar inference\nrequests. However, we identify that these cache mechanisms pose a risk of\nprivate input leakage, as the caching can result in observable variations in\nresponse times, making them a strong candidate for a timing-based attack hint.\n  In this study, we propose a novel timing-based side-channel attack to execute\ninput theft in LLMs inference. The cache-based attack faces the challenge of\nconstructing candidate inputs in a large search space to hit and steal cached\nuser queries. To address these challenges, we propose two primary components.\nThe input constructor employs machine learning techniques and LLM-based\napproaches for vocabulary correlation learning while implementing optimized\nsearch mechanisms for generalized input construction. The time analyzer\nimplements statistical time fitting with outlier elimination to identify cache\nhit patterns, continuously providing feedback to refine the constructor's\nsearch strategy. We conduct experiments across two cache mechanisms and the\nresults demonstrate that our approach consistently attains high attack success\nrates in various applications. Our work highlights the security vulnerabilities\nassociated with performance optimizations, underscoring the necessity of\nprioritizing privacy and security alongside enhancements in LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) possess extensive knowledge and\nquestion-answering capabilities, having been widely deployed in\nprivacy-sensitive domains like finance and medical consultation. During LLM\ninferences, cache-sharing methods are commonly employed to enhance efficiency\nby reusing cached states or responses for the same or similar inference\nrequests. However, we identify that these cache mechanisms pose a risk of\nprivate input leakage, as the caching can result in observable variations in\nresponse times, making them a strong candidate for a timing-based attack hint.\n  In this study, we propose a novel timing-based side-channel attack to execute\ninput theft in LLMs inference. The cache-based attack faces the challenge of\nconstructing candidate inputs in a large search space to hit and steal cached\nuser queries. To address these challenges, we propose two primary components.\nThe input constructor employs machine learning techniques and LLM-based\napproaches for vocabulary correlation learning while implementing optimized\nsearch mechanisms for generalized input construction. The time analyzer\nimplements statistical time fitting with outlier elimination to identify cache\nhit patterns, continuously providing feedback to refine the constructor's\nsearch strategy. We conduct experiments across two cache mechanisms and the\nresults demonstrate that our approach consistently attains high attack success\nrates in various applications. Our work highlights the security vulnerabilities\nassociated with performance optimizations, underscoring the necessity of\nprioritizing privacy and security alongside enhancements in LLM inference."
                },
                "authors": [
                    {
                        "name": "Xinyao Zheng"
                    },
                    {
                        "name": "Husheng Han"
                    },
                    {
                        "name": "Shangyi Shi"
                    },
                    {
                        "name": "Qiyan Fang"
                    },
                    {
                        "name": "Zidong Du"
                    },
                    {
                        "name": "Xing Hu"
                    },
                    {
                        "name": "Qi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Qi Guo"
                },
                "author": "Qi Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18191v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18191v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19379v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19379v1",
                "updated": "2024-11-28T21:10:20Z",
                "updated_parsed": [
                    2024,
                    11,
                    28,
                    21,
                    10,
                    20,
                    3,
                    333,
                    0
                ],
                "published": "2024-11-28T21:10:20Z",
                "published_parsed": [
                    2024,
                    11,
                    28,
                    21,
                    10,
                    20,
                    3,
                    333,
                    0
                ],
                "title": "Marconi: Prefix Caching for the Era of Hybrid LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Marconi: Prefix Caching for the Era of Hybrid LLMs"
                },
                "summary": "Hybrid models that combine the language modeling capabilities of Attention\nlayers with the efficiency of Recurrent layers (e.g., State Space Models) have\ngained traction in practically supporting long contexts in Large Language Model\nserving. Yet, the unique properties of these models complicate the usage of\ncomplementary efficiency optimizations such as prefix caching that skip\nredundant computations across requests. Most notably, their use of in-place\nstate updates for recurrent layers precludes rolling back cache entries for\npartial sequence overlaps, and instead mandates only exact-match cache hits;\nthe effect is a deluge of (large) cache entries per sequence, most of which\nyield minimal reuse opportunities. We present Marconi, the first system that\nsupports efficient prefix caching with Hybrid LLMs. Key to Marconi are its\nnovel admission and eviction policies that more judiciously assess potential\ncache entries based not only on recency, but also on (1) forecasts of their\nreuse likelihood across a taxonomy of different hit scenarios, and (2) the\ncompute savings that hits deliver relative to memory footprints. Across diverse\nworkloads and Hybrid models, Marconi achieves up to 34.4$\\times$ higher token\nhit rates (71.1% or 617 ms lower TTFT) compared to state-of-the-art prefix\ncaching systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid models that combine the language modeling capabilities of Attention\nlayers with the efficiency of Recurrent layers (e.g., State Space Models) have\ngained traction in practically supporting long contexts in Large Language Model\nserving. Yet, the unique properties of these models complicate the usage of\ncomplementary efficiency optimizations such as prefix caching that skip\nredundant computations across requests. Most notably, their use of in-place\nstate updates for recurrent layers precludes rolling back cache entries for\npartial sequence overlaps, and instead mandates only exact-match cache hits;\nthe effect is a deluge of (large) cache entries per sequence, most of which\nyield minimal reuse opportunities. We present Marconi, the first system that\nsupports efficient prefix caching with Hybrid LLMs. Key to Marconi are its\nnovel admission and eviction policies that more judiciously assess potential\ncache entries based not only on recency, but also on (1) forecasts of their\nreuse likelihood across a taxonomy of different hit scenarios, and (2) the\ncompute savings that hits deliver relative to memory footprints. Across diverse\nworkloads and Hybrid models, Marconi achieves up to 34.4$\\times$ higher token\nhit rates (71.1% or 617 ms lower TTFT) compared to state-of-the-art prefix\ncaching systems."
                },
                "authors": [
                    {
                        "name": "Rui Pan"
                    },
                    {
                        "name": "Zhuang Wang"
                    },
                    {
                        "name": "Zhen Jia"
                    },
                    {
                        "name": "Can Karakus"
                    },
                    {
                        "name": "Luca Zancato"
                    },
                    {
                        "name": "Tri Dao"
                    },
                    {
                        "name": "Ravi Netravali"
                    },
                    {
                        "name": "Yida Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yida Wang"
                },
                "author": "Yida Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19379v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19379v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19248v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19248v1",
                "updated": "2024-11-28T16:35:22Z",
                "updated_parsed": [
                    2024,
                    11,
                    28,
                    16,
                    35,
                    22,
                    3,
                    333,
                    0
                ],
                "published": "2024-11-28T16:35:22Z",
                "published_parsed": [
                    2024,
                    11,
                    28,
                    16,
                    35,
                    22,
                    3,
                    333,
                    0
                ],
                "title": "Reflecting Intelligent Surfaces-Assisted Multiple-Antenna Coded Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reflecting Intelligent Surfaces-Assisted Multiple-Antenna Coded Caching"
                },
                "summary": "Reconfigurable intelligent surface (RIS) has been treated as a core technique\nin improving wireless propagation environments for the next generation wireless\ncommunication systems. This paper proposes a new coded caching problem,\nreferred to as Reconfigurable Intelligent Surface (RIS)-assisted\nmultiple-antenna coded caching, which is composed of a server with multiple\nantennas and some single-antenna cache-aided users. Different from the existing\nmulti-antenna coded caching problems, we introduce a passive RIS (with limited\nnumber of units) into the systems to further increase the multicast gain (i.e.,\ndegrees of freedom (DoF)) in the transmission, which is done by using\nRIS-assisted interference nulling. That is, by using RIS, we can `erase' any\npath between one transmission antenna and one receive antenna. We first propose\na new RIS-assisted interference nulling approach to search for the phase-shift\ncoefficients of RIS for the sake of interference nulling, which converges\nfaster than the state-of-the-art algorithm. After erasing some paths in each\ntime slot, the delivery can be divided into several non-overlapping groups\nincluding transmission antennas and users, where in each group the transmission\nantennas serve the contained users without suffering interference from the\ntransmissions by other groups. The division of groups for the sake of\nmaximizing the DoF could be formulated into a combinatorial optimization\nproblem. We propose a grouping algorithm which can find the optimal solution\nwith low complexity, and the corresponding coded caching scheme achieving this\nDoF.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reconfigurable intelligent surface (RIS) has been treated as a core technique\nin improving wireless propagation environments for the next generation wireless\ncommunication systems. This paper proposes a new coded caching problem,\nreferred to as Reconfigurable Intelligent Surface (RIS)-assisted\nmultiple-antenna coded caching, which is composed of a server with multiple\nantennas and some single-antenna cache-aided users. Different from the existing\nmulti-antenna coded caching problems, we introduce a passive RIS (with limited\nnumber of units) into the systems to further increase the multicast gain (i.e.,\ndegrees of freedom (DoF)) in the transmission, which is done by using\nRIS-assisted interference nulling. That is, by using RIS, we can `erase' any\npath between one transmission antenna and one receive antenna. We first propose\na new RIS-assisted interference nulling approach to search for the phase-shift\ncoefficients of RIS for the sake of interference nulling, which converges\nfaster than the state-of-the-art algorithm. After erasing some paths in each\ntime slot, the delivery can be divided into several non-overlapping groups\nincluding transmission antennas and users, where in each group the transmission\nantennas serve the contained users without suffering interference from the\ntransmissions by other groups. The division of groups for the sake of\nmaximizing the DoF could be formulated into a combinatorial optimization\nproblem. We propose a grouping algorithm which can find the optimal solution\nwith low complexity, and the corresponding coded caching scheme achieving this\nDoF."
                },
                "authors": [
                    {
                        "name": "Xiaofan Niu"
                    },
                    {
                        "name": "Minquan Cheng"
                    },
                    {
                        "name": "Kai Wan"
                    },
                    {
                        "name": "Robert Caiming Qiu"
                    },
                    {
                        "name": "Giuseppe Caire"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Caire"
                },
                "author": "Giuseppe Caire",
                "arxiv_comment": "The short version of this paper was presented in 2024 IEEE\n  Information Theory Workshop, Nov. 24-28, 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19248v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19248v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.12468v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.12468v2",
                "updated": "2024-11-28T14:42:54Z",
                "updated_parsed": [
                    2024,
                    11,
                    28,
                    14,
                    42,
                    54,
                    3,
                    333,
                    0
                ],
                "published": "2024-04-18T19:04:33Z",
                "published_parsed": [
                    2024,
                    4,
                    18,
                    19,
                    4,
                    33,
                    3,
                    109,
                    0
                ],
                "title": "Fresh Caching of Dynamic Contents using Restless Multi-armed Bandits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fresh Caching of Dynamic Contents using Restless Multi-armed Bandits"
                },
                "summary": "We consider a dynamic content caching problem wherein the contents get\nupdated at a central server, and local copies of a subset of contents are\ncached at a local cache associated with a Base station (BS). When a content\nrequest arrives, based on whether the content is in the local cache, the BS can\ndecide whether to fetch the content from the central server or serve the cached\nversion from the local cache. Fetching a content incurs a fixed fetching cost,\nand serving the cached version incurs an ageing cost proportional to the\nage-of-version (AoV) of the content. The BS has only partial information\nregarding AoVs of the contents. We formulate an optimal content fetching and\ncaching problem to minimize the average cost subject to cache capacity\nconstraints. The problem suffers from the curse of dimensionality and is\nprovably hard to solve. We formulate this problem as a continuous time restless\nmulti-armed bandit process (RMAB), where a single content problem of the\ncorresponding RMAB is a partially observable Markov decision process. We\nreformulate the single content problem as a semi-Markov decision process, prove\nindexability, and provide a Whittle index based solution to this problem.\nFinally, we compare the performance with recent work and show that our proposed\npolicy is optimal via simulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider a dynamic content caching problem wherein the contents get\nupdated at a central server, and local copies of a subset of contents are\ncached at a local cache associated with a Base station (BS). When a content\nrequest arrives, based on whether the content is in the local cache, the BS can\ndecide whether to fetch the content from the central server or serve the cached\nversion from the local cache. Fetching a content incurs a fixed fetching cost,\nand serving the cached version incurs an ageing cost proportional to the\nage-of-version (AoV) of the content. The BS has only partial information\nregarding AoVs of the contents. We formulate an optimal content fetching and\ncaching problem to minimize the average cost subject to cache capacity\nconstraints. The problem suffers from the curse of dimensionality and is\nprovably hard to solve. We formulate this problem as a continuous time restless\nmulti-armed bandit process (RMAB), where a single content problem of the\ncorresponding RMAB is a partially observable Markov decision process. We\nreformulate the single content problem as a semi-Markov decision process, prove\nindexability, and provide a Whittle index based solution to this problem.\nFinally, we compare the performance with recent work and show that our proposed\npolicy is optimal via simulations."
                },
                "authors": [
                    {
                        "name": "Ankita Koley"
                    },
                    {
                        "name": "Chandramani Singh"
                    }
                ],
                "author_detail": {
                    "name": "Chandramani Singh"
                },
                "author": "Chandramani Singh",
                "arxiv_comment": "14 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.12468v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.12468v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19108v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19108v1",
                "updated": "2024-11-28T12:50:05Z",
                "updated_parsed": [
                    2024,
                    11,
                    28,
                    12,
                    50,
                    5,
                    3,
                    333,
                    0
                ],
                "published": "2024-11-28T12:50:05Z",
                "published_parsed": [
                    2024,
                    11,
                    28,
                    12,
                    50,
                    5,
                    3,
                    333,
                    0
                ],
                "title": "Timestep Embedding Tells: It's Time to Cache for Video Diffusion Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Timestep Embedding Tells: It's Time to Cache for Video Diffusion Model"
                },
                "summary": "As a fundamental backbone for video generation, diffusion models are\nchallenged by low inference speed due to the sequential nature of denoising.\nPrevious methods speed up the models by caching and reusing model outputs at\nuniformly selected timesteps. However, such a strategy neglects the fact that\ndifferences among model outputs are not uniform across timesteps, which hinders\nselecting the appropriate model outputs to cache, leading to a poor balance\nbetween inference efficiency and visual quality. In this study, we introduce\nTimestep Embedding Aware Cache (TeaCache), a training-free caching approach\nthat estimates and leverages the fluctuating differences among model outputs\nacross timesteps. Rather than directly using the time-consuming model outputs,\nTeaCache focuses on model inputs, which have a strong correlation with the\nmodeloutputs while incurring negligible computational cost. TeaCache first\nmodulates the noisy inputs using the timestep embeddings to ensure their\ndifferences better approximating those of model outputs. TeaCache then\nintroduces a rescaling strategy to refine the estimated differences and\nutilizes them to indicate output caching. Experiments show that TeaCache\nachieves up to 4.41x acceleration over Open-Sora-Plan with negligible (-0.07%\nVbench score) degradation of visual quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As a fundamental backbone for video generation, diffusion models are\nchallenged by low inference speed due to the sequential nature of denoising.\nPrevious methods speed up the models by caching and reusing model outputs at\nuniformly selected timesteps. However, such a strategy neglects the fact that\ndifferences among model outputs are not uniform across timesteps, which hinders\nselecting the appropriate model outputs to cache, leading to a poor balance\nbetween inference efficiency and visual quality. In this study, we introduce\nTimestep Embedding Aware Cache (TeaCache), a training-free caching approach\nthat estimates and leverages the fluctuating differences among model outputs\nacross timesteps. Rather than directly using the time-consuming model outputs,\nTeaCache focuses on model inputs, which have a strong correlation with the\nmodeloutputs while incurring negligible computational cost. TeaCache first\nmodulates the noisy inputs using the timestep embeddings to ensure their\ndifferences better approximating those of model outputs. TeaCache then\nintroduces a rescaling strategy to refine the estimated differences and\nutilizes them to indicate output caching. Experiments show that TeaCache\nachieves up to 4.41x acceleration over Open-Sora-Plan with negligible (-0.07%\nVbench score) degradation of visual quality."
                },
                "authors": [
                    {
                        "name": "Feng Liu"
                    },
                    {
                        "name": "Shiwei Zhang"
                    },
                    {
                        "name": "Xiaofeng Wang"
                    },
                    {
                        "name": "Yujie Wei"
                    },
                    {
                        "name": "Haonan Qiu"
                    },
                    {
                        "name": "Yuzhong Zhao"
                    },
                    {
                        "name": "Yingya Zhang"
                    },
                    {
                        "name": "Qixiang Ye"
                    },
                    {
                        "name": "Fang Wan"
                    }
                ],
                "author_detail": {
                    "name": "Fang Wan"
                },
                "author": "Fang Wan",
                "arxiv_comment": "Project: https://liewfeng.github.io/TeaCache",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19108v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19108v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18077v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18077v2",
                "updated": "2024-11-28T02:01:50Z",
                "updated_parsed": [
                    2024,
                    11,
                    28,
                    2,
                    1,
                    50,
                    3,
                    333,
                    0
                ],
                "published": "2024-11-27T06:10:49Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    6,
                    10,
                    49,
                    2,
                    332,
                    0
                ],
                "title": "MiniKV: Pushing the Limits of LLM Inference via 2-Bit\n  Layer-Discriminative KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MiniKV: Pushing the Limits of LLM Inference via 2-Bit\n  Layer-Discriminative KV Cache"
                },
                "summary": "How to efficiently serve LLMs in practice has become exceptionally\nchallenging due to their prohibitive memory and computation requirements. In\nthis study, we investigate optimizing the KV cache, whose memory footprint\nposes a critical bottleneck in LLM inference, especially when dealing with long\ncontext tasks. To tackle the challenge, we introduce MiniKV, a KV cache\noptimization method that simultaneously preserves long context task accuracy\nwhile significantly reducing KV cache size via a novel 2-bit\nlayer-discriminative KV cache. More importantly, we develop specialized CUDA\nkernels to make MiniKV compatible with FlashAttention. Experiments on a wide\nrange of long context tasks show that MiniKV effectively achieves 86% KV cache\ncompression ratio while recovering over 98.5% of accuracy, outperforming\nstate-of-the-art methods while achieving excellent measured system performance\nimprovements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How to efficiently serve LLMs in practice has become exceptionally\nchallenging due to their prohibitive memory and computation requirements. In\nthis study, we investigate optimizing the KV cache, whose memory footprint\nposes a critical bottleneck in LLM inference, especially when dealing with long\ncontext tasks. To tackle the challenge, we introduce MiniKV, a KV cache\noptimization method that simultaneously preserves long context task accuracy\nwhile significantly reducing KV cache size via a novel 2-bit\nlayer-discriminative KV cache. More importantly, we develop specialized CUDA\nkernels to make MiniKV compatible with FlashAttention. Experiments on a wide\nrange of long context tasks show that MiniKV effectively achieves 86% KV cache\ncompression ratio while recovering over 98.5% of accuracy, outperforming\nstate-of-the-art methods while achieving excellent measured system performance\nimprovements."
                },
                "authors": [
                    {
                        "name": "Akshat Sharma"
                    },
                    {
                        "name": "Hangliang Ding"
                    },
                    {
                        "name": "Jianping Li"
                    },
                    {
                        "name": "Neel Dani"
                    },
                    {
                        "name": "Minjia Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Minjia Zhang"
                },
                "author": "Minjia Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18077v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18077v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18566v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18566v1",
                "updated": "2024-11-27T18:09:29Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    18,
                    9,
                    29,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-27T18:09:29Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    18,
                    9,
                    29,
                    2,
                    332,
                    0
                ],
                "title": "OASIS-UROS: Open Acquisition System for IEPE Sensors -- Upgraded,\n  Refined, and Overhauled Software",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OASIS-UROS: Open Acquisition System for IEPE Sensors -- Upgraded,\n  Refined, and Overhauled Software"
                },
                "summary": "OASIS-UROS continues the previously published Open Acquisition System for\nIEPE Sensors (OASIS). While still building on the ESP32 microcontroller, this\nversion improves the overall performance by switching to an SD card caching\nsystem and upgrading the analog-digital converter to an AD7606C-18, which has a\nhigher resolution, provides eight channels, oversampling, and\nsoftware-adjustable voltage ranges. Also improved is the IEPE front-end and\npower supply, as well as the firmware of the acquisition system, which can now\nachieve a sample rate of up to 36 kHz while sampling all eight channels. This\npaper documents the hardware and software of OASIS-UROS and provides all\nmaterials required to reproduce the open acquisition system. Lastly, the system\nwas validated against commercial hardware and software in an experimental modal\nanalysis context. This showed that the system performs close to the commercial\none in some aspects with respect to the utilized test case. While OASIS-UROS\ncannot match the full performance of the commercial system, the developed\nsystem can be a viable alternative for students, people in academia, or smaller\ncompanies that have a constrained budget or require complete insight as well as\nadaptability of the hardware and software.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OASIS-UROS continues the previously published Open Acquisition System for\nIEPE Sensors (OASIS). While still building on the ESP32 microcontroller, this\nversion improves the overall performance by switching to an SD card caching\nsystem and upgrading the analog-digital converter to an AD7606C-18, which has a\nhigher resolution, provides eight channels, oversampling, and\nsoftware-adjustable voltage ranges. Also improved is the IEPE front-end and\npower supply, as well as the firmware of the acquisition system, which can now\nachieve a sample rate of up to 36 kHz while sampling all eight channels. This\npaper documents the hardware and software of OASIS-UROS and provides all\nmaterials required to reproduce the open acquisition system. Lastly, the system\nwas validated against commercial hardware and software in an experimental modal\nanalysis context. This showed that the system performs close to the commercial\none in some aspects with respect to the utilized test case. While OASIS-UROS\ncannot match the full performance of the commercial system, the developed\nsystem can be a viable alternative for students, people in academia, or smaller\ncompanies that have a constrained budget or require complete insight as well as\nadaptability of the hardware and software."
                },
                "authors": [
                    {
                        "name": "Oliver Maximilian Zobel"
                    },
                    {
                        "name": "Johannes Maierhofer"
                    },
                    {
                        "name": "Andreas KÃ¶stler"
                    },
                    {
                        "name": "Daniel J. Rixen"
                    }
                ],
                "author_detail": {
                    "name": "Daniel J. Rixen"
                },
                "author": "Daniel J. Rixen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18566v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18566v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.08895v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.08895v4",
                "updated": "2024-11-27T18:05:57Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    18,
                    5,
                    57,
                    2,
                    332,
                    0
                ],
                "published": "2024-01-17T00:36:58Z",
                "published_parsed": [
                    2024,
                    1,
                    17,
                    0,
                    36,
                    58,
                    2,
                    17,
                    0
                ],
                "title": "cedar: Optimized and Unified Machine Learning Input Data Pipelines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "cedar: Optimized and Unified Machine Learning Input Data Pipelines"
                },
                "summary": "The input data pipeline is an essential component of each machine learning\n(ML) training job. It is responsible for reading massive amounts of training\ndata, processing batches of samples using complex transformations, and loading\nthem onto training nodes at low latency and high throughput. Performant input\ndata systems are becoming increasingly critical, driven by skyrocketing data\nvolumes and training throughput demands. Unfortunately, current input data\nsystems cannot fully leverage key performance optimizations, resulting in\nhugely inefficient infrastructures that require significant resources - or\nworse - underutilize expensive accelerators.\n  To address these demands, we present cedar, an optimized and unified\nprogramming framework for ML input data pipelines. cedar allows users to define\ninput data pipelines using composable operators that support arbitrary ML\nframeworks and libraries. cedar introduces an extensible optimizer that\nsystematically applies a complex combination of optimizations (e.g.,\noffloading, caching, prefetching, fusion, and reordering). It orchestrates\nprocessing across a customizable set of local and distributed compute resources\nin order to improve processing performance and efficiency, all without user\ninput. Across eight pipelines, cedar improves performance by up to 1.87x to\n10.65x compared to state-of-the-art input data systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The input data pipeline is an essential component of each machine learning\n(ML) training job. It is responsible for reading massive amounts of training\ndata, processing batches of samples using complex transformations, and loading\nthem onto training nodes at low latency and high throughput. Performant input\ndata systems are becoming increasingly critical, driven by skyrocketing data\nvolumes and training throughput demands. Unfortunately, current input data\nsystems cannot fully leverage key performance optimizations, resulting in\nhugely inefficient infrastructures that require significant resources - or\nworse - underutilize expensive accelerators.\n  To address these demands, we present cedar, an optimized and unified\nprogramming framework for ML input data pipelines. cedar allows users to define\ninput data pipelines using composable operators that support arbitrary ML\nframeworks and libraries. cedar introduces an extensible optimizer that\nsystematically applies a complex combination of optimizations (e.g.,\noffloading, caching, prefetching, fusion, and reordering). It orchestrates\nprocessing across a customizable set of local and distributed compute resources\nin order to improve processing performance and efficiency, all without user\ninput. Across eight pipelines, cedar improves performance by up to 1.87x to\n10.65x compared to state-of-the-art input data systems."
                },
                "authors": [
                    {
                        "name": "Mark Zhao"
                    },
                    {
                        "name": "Emanuel Adamiak"
                    },
                    {
                        "name": "Christos Kozyrakis"
                    }
                ],
                "author_detail": {
                    "name": "Christos Kozyrakis"
                },
                "author": "Christos Kozyrakis",
                "arxiv_comment": "Published in PVLDB Volume 18, Issue 2",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.08895v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.08895v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18424v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18424v1",
                "updated": "2024-11-27T15:07:28Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    15,
                    7,
                    28,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-27T15:07:28Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    15,
                    7,
                    28,
                    2,
                    332,
                    0
                ],
                "title": "FastSwitch: Optimizing Context Switching Efficiency in Fairness-aware\n  Large Language Model Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FastSwitch: Optimizing Context Switching Efficiency in Fairness-aware\n  Large Language Model Serving"
                },
                "summary": "Serving numerous users and requests concurrently requires good fairness in\nLarge Language Models (LLMs) serving system. This ensures that, at the same\ncost, the system can meet the Service Level Objectives (SLOs) of more users ,\nsuch as time to first token (TTFT) and time between tokens (TBT), rather than\nallowing a few users to experience performance far exceeding the SLOs. To\nachieve better fairness, the preemption-based scheduling policy dynamically\nadjusts the priority of each request to maintain balance during runtime.\nHowever, existing systems tend to overly prioritize throughput, overlooking the\noverhead caused by preemption-induced context switching, which is crucial for\nmaintaining fairness through priority adjustments. In this work, we identify\nthree main challenges that result in this overhead. 1) Inadequate I/O\nutilization. 2) GPU idleness. 3) Unnecessary I/O transmission during multi-turn\nconversations. Our key insight is that the block-based KV cache memory policy\nin existing systems, while achieving near-zero memory waste, leads to\ndiscontinuity and insufficient granularity in the KV cache memory. To respond,\nwe introduce FastSwitch, a fairness-aware serving system that not only aligns\nwith existing KV cache memory allocation policy but also mitigates context\nswitching overhead. Our evaluation shows that FastSwitch outperforms the\nstate-of-the-art LLM serving system vLLM with speedups of 1.4-11.2x across\ndifferent tail TTFT and TBT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving numerous users and requests concurrently requires good fairness in\nLarge Language Models (LLMs) serving system. This ensures that, at the same\ncost, the system can meet the Service Level Objectives (SLOs) of more users ,\nsuch as time to first token (TTFT) and time between tokens (TBT), rather than\nallowing a few users to experience performance far exceeding the SLOs. To\nachieve better fairness, the preemption-based scheduling policy dynamically\nadjusts the priority of each request to maintain balance during runtime.\nHowever, existing systems tend to overly prioritize throughput, overlooking the\noverhead caused by preemption-induced context switching, which is crucial for\nmaintaining fairness through priority adjustments. In this work, we identify\nthree main challenges that result in this overhead. 1) Inadequate I/O\nutilization. 2) GPU idleness. 3) Unnecessary I/O transmission during multi-turn\nconversations. Our key insight is that the block-based KV cache memory policy\nin existing systems, while achieving near-zero memory waste, leads to\ndiscontinuity and insufficient granularity in the KV cache memory. To respond,\nwe introduce FastSwitch, a fairness-aware serving system that not only aligns\nwith existing KV cache memory allocation policy but also mitigates context\nswitching overhead. Our evaluation shows that FastSwitch outperforms the\nstate-of-the-art LLM serving system vLLM with speedups of 1.4-11.2x across\ndifferent tail TTFT and TBT."
                },
                "authors": [
                    {
                        "name": "Ao Shen"
                    },
                    {
                        "name": "Zhiyao Li"
                    },
                    {
                        "name": "Mingyu Gao"
                    }
                ],
                "author_detail": {
                    "name": "Mingyu Gao"
                },
                "author": "Mingyu Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18424v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18424v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17616v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17616v2",
                "updated": "2024-11-27T14:43:46Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    14,
                    43,
                    46,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-26T17:28:10Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    17,
                    28,
                    10,
                    1,
                    331,
                    0
                ],
                "title": "Accelerating Vision Diffusion Transformers with Skip Branches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Vision Diffusion Transformers with Skip Branches"
                },
                "summary": "Diffusion Transformers (DiT), an emerging image and video generation model\narchitecture, has demonstrated great potential because of its high generation\nquality and scalability properties. Despite the impressive performance, its\npractical deployment is constrained by computational complexity and redundancy\nin the sequential denoising process. While feature caching across timesteps has\nproven effective in accelerating diffusion models, its application to DiT is\nlimited by fundamental architectural differences from U-Net-based approaches.\nThrough empirical analysis of DiT feature dynamics, we identify that\nsignificant feature variation between DiT blocks presents a key challenge for\nfeature reusability. To address this, we convert standard DiT into Skip-DiT\nwith skip branches to enhance feature smoothness. Further, we introduce\nSkip-Cache which utilizes the skip branches to cache DiT features across\ntimesteps at the inference time. We validated effectiveness of our proposal on\ndifferent DiT backbones for video and image generation, showcasing skip\nbranches to help preserve generation quality and achieve higher speedup.\nExperimental results indicate that Skip-DiT achieves a 1.5x speedup almost for\nfree and a 2.2x speedup with only a minor reduction in quantitative metrics.\nCode is available at https://github.com/OpenSparseLLMs/Skip-DiT.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiT), an emerging image and video generation model\narchitecture, has demonstrated great potential because of its high generation\nquality and scalability properties. Despite the impressive performance, its\npractical deployment is constrained by computational complexity and redundancy\nin the sequential denoising process. While feature caching across timesteps has\nproven effective in accelerating diffusion models, its application to DiT is\nlimited by fundamental architectural differences from U-Net-based approaches.\nThrough empirical analysis of DiT feature dynamics, we identify that\nsignificant feature variation between DiT blocks presents a key challenge for\nfeature reusability. To address this, we convert standard DiT into Skip-DiT\nwith skip branches to enhance feature smoothness. Further, we introduce\nSkip-Cache which utilizes the skip branches to cache DiT features across\ntimesteps at the inference time. We validated effectiveness of our proposal on\ndifferent DiT backbones for video and image generation, showcasing skip\nbranches to help preserve generation quality and achieve higher speedup.\nExperimental results indicate that Skip-DiT achieves a 1.5x speedup almost for\nfree and a 2.2x speedup with only a minor reduction in quantitative metrics.\nCode is available at https://github.com/OpenSparseLLMs/Skip-DiT.git."
                },
                "authors": [
                    {
                        "name": "Guanjie Chen"
                    },
                    {
                        "name": "Xinyu Zhao"
                    },
                    {
                        "name": "Yucheng Zhou"
                    },
                    {
                        "name": "Tianlong Chen"
                    },
                    {
                        "name": "Yu Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Yu Cheng"
                },
                "author": "Yu Cheng",
                "arxiv_comment": "17 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17616v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17616v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17459v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17459v2",
                "updated": "2024-11-27T08:21:47Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    8,
                    21,
                    47,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-26T14:23:53Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    14,
                    23,
                    53,
                    1,
                    331,
                    0
                ],
                "title": "WF-VAE: Enhancing Video VAE by Wavelet-Driven Energy Flow for Latent\n  Video Diffusion Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WF-VAE: Enhancing Video VAE by Wavelet-Driven Energy Flow for Latent\n  Video Diffusion Model"
                },
                "summary": "Video Variational Autoencoder (VAE) encodes videos into a low-dimensional\nlatent space, becoming a key component of most Latent Video Diffusion Models\n(LVDMs) to reduce model training costs. However, as the resolution and duration\nof generated videos increase, the encoding cost of Video VAEs becomes a\nlimiting bottleneck in training LVDMs. Moreover, the block-wise inference\nmethod adopted by most LVDMs can lead to discontinuities of latent space when\nprocessing long-duration videos. The key to addressing the computational\nbottleneck lies in decomposing videos into distinct components and efficiently\nencoding the critical information. Wavelet transform can decompose videos into\nmultiple frequency-domain components and improve the efficiency significantly,\nwe thus propose Wavelet Flow VAE (WF-VAE), an autoencoder that leverages\nmulti-level wavelet transform to facilitate low-frequency energy flow into\nlatent representation. Furthermore, we introduce a method called Causal Cache,\nwhich maintains the integrity of latent space during block-wise inference.\nCompared to state-of-the-art video VAEs, WF-VAE demonstrates superior\nperformance in both PSNR and LPIPS metrics, achieving 2x higher throughput and\n4x lower memory consumption while maintaining competitive reconstruction\nquality. Our code and models are available at\nhttps://github.com/PKU-YuanGroup/WF-VAE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Variational Autoencoder (VAE) encodes videos into a low-dimensional\nlatent space, becoming a key component of most Latent Video Diffusion Models\n(LVDMs) to reduce model training costs. However, as the resolution and duration\nof generated videos increase, the encoding cost of Video VAEs becomes a\nlimiting bottleneck in training LVDMs. Moreover, the block-wise inference\nmethod adopted by most LVDMs can lead to discontinuities of latent space when\nprocessing long-duration videos. The key to addressing the computational\nbottleneck lies in decomposing videos into distinct components and efficiently\nencoding the critical information. Wavelet transform can decompose videos into\nmultiple frequency-domain components and improve the efficiency significantly,\nwe thus propose Wavelet Flow VAE (WF-VAE), an autoencoder that leverages\nmulti-level wavelet transform to facilitate low-frequency energy flow into\nlatent representation. Furthermore, we introduce a method called Causal Cache,\nwhich maintains the integrity of latent space during block-wise inference.\nCompared to state-of-the-art video VAEs, WF-VAE demonstrates superior\nperformance in both PSNR and LPIPS metrics, achieving 2x higher throughput and\n4x lower memory consumption while maintaining competitive reconstruction\nquality. Our code and models are available at\nhttps://github.com/PKU-YuanGroup/WF-VAE."
                },
                "authors": [
                    {
                        "name": "Zongjian Li"
                    },
                    {
                        "name": "Bin Lin"
                    },
                    {
                        "name": "Yang Ye"
                    },
                    {
                        "name": "Liuhan Chen"
                    },
                    {
                        "name": "Xinhua Cheng"
                    },
                    {
                        "name": "Shenghai Yuan"
                    },
                    {
                        "name": "Li Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Li Yuan"
                },
                "author": "Li Yuan",
                "arxiv_comment": "8 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17459v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17459v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15785v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15785v2",
                "updated": "2024-11-27T03:07:20Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    3,
                    7,
                    20,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-24T11:30:00Z",
                "published_parsed": [
                    2024,
                    11,
                    24,
                    11,
                    30,
                    0,
                    6,
                    329,
                    0
                ],
                "title": "A Method for Building Large Language Models with Predefined KV Cache\n  Capacity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Method for Building Large Language Models with Predefined KV Cache\n  Capacity"
                },
                "summary": "This paper introduces a novel approach, the Bounded-Cache Transformer (BCT),\nfor building large language models with a predefined Key-Value (KV) cache\ncapacity. The BCT addresses the excessive memory consumption issue in\ntraditional KV caches by implementing a bounded-length KV cache, which is\nparticularly suitable for the attention layers in Transformer decode-only\narchitectures. By dynamically updating the key-value vector sequences, the BCT\nachieves efficient inference within limited cache capacity, significantly\nreducing memory usage while maintaining model performance and system\nthroughput. Experimental results demonstrate that the BCT significantly reduces\nmemory usage while maintaining the model's inference quality, offering a new\nsolution for efficient inference in large language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a novel approach, the Bounded-Cache Transformer (BCT),\nfor building large language models with a predefined Key-Value (KV) cache\ncapacity. The BCT addresses the excessive memory consumption issue in\ntraditional KV caches by implementing a bounded-length KV cache, which is\nparticularly suitable for the attention layers in Transformer decode-only\narchitectures. By dynamically updating the key-value vector sequences, the BCT\nachieves efficient inference within limited cache capacity, significantly\nreducing memory usage while maintaining model performance and system\nthroughput. Experimental results demonstrate that the BCT significantly reduces\nmemory usage while maintaining the model's inference quality, offering a new\nsolution for efficient inference in large language models."
                },
                "authors": [
                    {
                        "name": "Zhonghua Yi"
                    },
                    {
                        "name": "Ge Niu"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Wei Tang"
                    },
                    {
                        "name": "Liqiu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Liqiu Zhang"
                },
                "author": "Liqiu Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15785v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15785v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17685v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17685v1",
                "updated": "2024-11-26T18:52:06Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    18,
                    52,
                    6,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T18:52:06Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    18,
                    52,
                    6,
                    1,
                    331,
                    0
                ],
                "title": "Attamba: Attending To Multi-Token States",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attamba: Attending To Multi-Token States"
                },
                "summary": "When predicting the next token in a sequence, vanilla transformers compute\nattention over all previous tokens, resulting in quadratic scaling of compute\nwith sequence length. State-space models compress the entire sequence of tokens\ninto a fixed-dimensional representation to improve efficiency, while other\narchitectures achieve sub-quadratic complexity via low-rank projections or\nsparse attention patterns over the sequence. In this paper, we introduce\nAttamba, a novel architecture that uses state-space models to compress chunks\nof tokens and applies attention on these compressed key-value representations.\nWe find that replacing key and value projections in a transformer with SSMs can\nimprove model quality and enable flexible token chunking, resulting in 24%\nimproved perplexity with transformer of similar KV-Cache and attention\nfootprint, and ~4 times smaller KV-Cache and Attention FLOPs for 5% perplexity\ntrade-off. Attamba can perform attention on chunked-sequences of variable\nlength, enabling a smooth transition between quadratic and linear scaling,\noffering adaptable efficiency gains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When predicting the next token in a sequence, vanilla transformers compute\nattention over all previous tokens, resulting in quadratic scaling of compute\nwith sequence length. State-space models compress the entire sequence of tokens\ninto a fixed-dimensional representation to improve efficiency, while other\narchitectures achieve sub-quadratic complexity via low-rank projections or\nsparse attention patterns over the sequence. In this paper, we introduce\nAttamba, a novel architecture that uses state-space models to compress chunks\nof tokens and applies attention on these compressed key-value representations.\nWe find that replacing key and value projections in a transformer with SSMs can\nimprove model quality and enable flexible token chunking, resulting in 24%\nimproved perplexity with transformer of similar KV-Cache and attention\nfootprint, and ~4 times smaller KV-Cache and Attention FLOPs for 5% perplexity\ntrade-off. Attamba can perform attention on chunked-sequences of variable\nlength, enabling a smooth transition between quadratic and linear scaling,\noffering adaptable efficiency gains."
                },
                "authors": [
                    {
                        "name": "Yash Akhauri"
                    },
                    {
                        "name": "Safeen Huda"
                    },
                    {
                        "name": "Mohamed S. Abdelfattah"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed S. Abdelfattah"
                },
                "author": "Mohamed S. Abdelfattah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17685v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17685v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17800v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17800v1",
                "updated": "2024-11-26T18:42:42Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    18,
                    42,
                    42,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T18:42:42Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    18,
                    42,
                    42,
                    1,
                    331,
                    0
                ],
                "title": "STAR: Synthesis of Tailored Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "STAR: Synthesis of Tailored Architectures"
                },
                "summary": "Iterative improvement of model architectures is fundamental to deep learning:\nTransformers first enabled scaling, and recent advances in model hybridization\nhave pushed the quality-efficiency frontier. However, optimizing architectures\nremains challenging and expensive. Current automated or manual approaches fall\nshort, largely due to limited progress in the design of search spaces and due\nto the simplicity of resulting patterns and heuristics. In this work, we\npropose a new approach for the synthesis of tailored architectures (STAR). Our\napproach combines a novel search space based on the theory of linear\ninput-varying systems, supporting a hierarchical numerical encoding into\narchitecture genomes. STAR genomes are automatically refined and recombined\nwith gradient-free, evolutionary algorithms to optimize for multiple model\nquality and efficiency metrics. Using STAR, we optimize large populations of\nnew architectures, leveraging diverse computational units and interconnection\npatterns, improving over highly-optimized Transformers and striped hybrid\nmodels on the frontier of quality, parameter size, and inference cache for\nautoregressive language modeling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Iterative improvement of model architectures is fundamental to deep learning:\nTransformers first enabled scaling, and recent advances in model hybridization\nhave pushed the quality-efficiency frontier. However, optimizing architectures\nremains challenging and expensive. Current automated or manual approaches fall\nshort, largely due to limited progress in the design of search spaces and due\nto the simplicity of resulting patterns and heuristics. In this work, we\npropose a new approach for the synthesis of tailored architectures (STAR). Our\napproach combines a novel search space based on the theory of linear\ninput-varying systems, supporting a hierarchical numerical encoding into\narchitecture genomes. STAR genomes are automatically refined and recombined\nwith gradient-free, evolutionary algorithms to optimize for multiple model\nquality and efficiency metrics. Using STAR, we optimize large populations of\nnew architectures, leveraging diverse computational units and interconnection\npatterns, improving over highly-optimized Transformers and striped hybrid\nmodels on the frontier of quality, parameter size, and inference cache for\nautoregressive language modeling."
                },
                "authors": [
                    {
                        "name": "Armin W. Thomas"
                    },
                    {
                        "name": "Rom Parnichkun"
                    },
                    {
                        "name": "Alexander Amini"
                    },
                    {
                        "name": "Stefano Massaroli"
                    },
                    {
                        "name": "Michael Poli"
                    }
                ],
                "author_detail": {
                    "name": "Michael Poli"
                },
                "author": "Michael Poli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17800v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17800v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.15651v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.15651v3",
                "updated": "2024-11-26T17:28:06Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    17,
                    28,
                    6,
                    1,
                    331,
                    0
                ],
                "published": "2024-03-22T23:47:19Z",
                "published_parsed": [
                    2024,
                    3,
                    22,
                    23,
                    47,
                    19,
                    4,
                    82,
                    0
                ],
                "title": "GaNI: Global and Near Field Illumination Aware Neural Inverse Rendering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GaNI: Global and Near Field Illumination Aware Neural Inverse Rendering"
                },
                "summary": "In this paper, we present GaNI, a Global and Near-field Illumination-aware\nneural inverse rendering technique that can reconstruct geometry, albedo, and\nroughness parameters from images of a scene captured with co-located light and\ncamera. Existing inverse rendering techniques with co-located light-camera\nfocus on single objects only, without modeling global illumination and\nnear-field lighting more prominent in scenes with multiple objects. We\nintroduce a system that solves this problem in two stages; we first reconstruct\nthe geometry powered by neural volumetric rendering NeuS, followed by inverse\nneural radiosity that uses the previously predicted geometry to estimate albedo\nand roughness. However, such a naive combination fails and we propose multiple\ntechnical contributions that enable this two-stage approach. We observe that\nNeuS fails to handle near-field illumination and strong specular reflections\nfrom the flashlight in a scene. We propose to implicitly model the effects of\nnear-field illumination and introduce a surface angle loss function to handle\nspecular reflections. Similarly, we observe that invNeRad assumes constant\nillumination throughout the capture and cannot handle moving flashlights during\ncapture. We propose a light position-aware radiance cache network and\nadditional smoothness priors on roughness to reconstruct reflectance.\nExperimental evaluation on synthetic and real data shows that our method\noutperforms the existing co-located light-camera-based inverse rendering\ntechniques. Our approach produces significantly better reflectance and slightly\nbetter geometry than capture strategies that do not require a dark room.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present GaNI, a Global and Near-field Illumination-aware\nneural inverse rendering technique that can reconstruct geometry, albedo, and\nroughness parameters from images of a scene captured with co-located light and\ncamera. Existing inverse rendering techniques with co-located light-camera\nfocus on single objects only, without modeling global illumination and\nnear-field lighting more prominent in scenes with multiple objects. We\nintroduce a system that solves this problem in two stages; we first reconstruct\nthe geometry powered by neural volumetric rendering NeuS, followed by inverse\nneural radiosity that uses the previously predicted geometry to estimate albedo\nand roughness. However, such a naive combination fails and we propose multiple\ntechnical contributions that enable this two-stage approach. We observe that\nNeuS fails to handle near-field illumination and strong specular reflections\nfrom the flashlight in a scene. We propose to implicitly model the effects of\nnear-field illumination and introduce a surface angle loss function to handle\nspecular reflections. Similarly, we observe that invNeRad assumes constant\nillumination throughout the capture and cannot handle moving flashlights during\ncapture. We propose a light position-aware radiance cache network and\nadditional smoothness priors on roughness to reconstruct reflectance.\nExperimental evaluation on synthetic and real data shows that our method\noutperforms the existing co-located light-camera-based inverse rendering\ntechniques. Our approach produces significantly better reflectance and slightly\nbetter geometry than capture strategies that do not require a dark room."
                },
                "authors": [
                    {
                        "name": "Jiaye Wu"
                    },
                    {
                        "name": "Saeed Hadadan"
                    },
                    {
                        "name": "Geng Lin"
                    },
                    {
                        "name": "Matthias Zwicker"
                    },
                    {
                        "name": "David Jacobs"
                    },
                    {
                        "name": "Roni Sengupta"
                    }
                ],
                "author_detail": {
                    "name": "Roni Sengupta"
                },
                "author": "Roni Sengupta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.15651v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.15651v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17559v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17559v1",
                "updated": "2024-11-26T16:21:10Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    16,
                    21,
                    10,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T16:21:10Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    16,
                    21,
                    10,
                    1,
                    331,
                    0
                ],
                "title": "Degrees of Freedom of Cache-Aided Interference Channels Assisted by\n  Active Intelligent Reflecting Surfaces",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Degrees of Freedom of Cache-Aided Interference Channels Assisted by\n  Active Intelligent Reflecting Surfaces"
                },
                "summary": "This paper studies cache-aided wireless networks in the presence of active\nintelligent reflecting surfaces (IRS) from an information-theoretic\nperspective. Specifically, we explore interference management in a cache-aided\nwireless network assisted by an active IRS, to enhance the achievable degrees\nof freedom (DoF). To this end, we jointly design the content placement,\ndelivery phase, and phase shifts of the IRS and propose a one-shot achievable\nscheme. Our scheme exploits transmitters' cooperation, cache contents (as side\ninformation), interference alignment, and IRS capabilities, adapting to the\nnetwork's parameters. We derive the achievable one-shot sum-DoF for different\nsizes of cache memories, network configurations, and numbers of IRS elements.\nOur results highlight the potential of deploying an IRS in cache-aided wireless\ncommunication systems, underscoring the enhancement of achievable DoF for\nvarious parameter regimes, particularly when the sizes of the caches\n(especially at the transmitters) are inadequate. Notably, we show that access\nto an IRS with a sufficient number of elements enables the achievement of the\nmaximum possible DoF for various parameter regimes of interest.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper studies cache-aided wireless networks in the presence of active\nintelligent reflecting surfaces (IRS) from an information-theoretic\nperspective. Specifically, we explore interference management in a cache-aided\nwireless network assisted by an active IRS, to enhance the achievable degrees\nof freedom (DoF). To this end, we jointly design the content placement,\ndelivery phase, and phase shifts of the IRS and propose a one-shot achievable\nscheme. Our scheme exploits transmitters' cooperation, cache contents (as side\ninformation), interference alignment, and IRS capabilities, adapting to the\nnetwork's parameters. We derive the achievable one-shot sum-DoF for different\nsizes of cache memories, network configurations, and numbers of IRS elements.\nOur results highlight the potential of deploying an IRS in cache-aided wireless\ncommunication systems, underscoring the enhancement of achievable DoF for\nvarious parameter regimes, particularly when the sizes of the caches\n(especially at the transmitters) are inadequate. Notably, we show that access\nto an IRS with a sufficient number of elements enables the achievement of the\nmaximum possible DoF for various parameter regimes of interest."
                },
                "authors": [
                    {
                        "name": "Abolfazl Changizi"
                    },
                    {
                        "name": "Ali H. Abdollahi Bafghi"
                    },
                    {
                        "name": "Masoumeh Nasiri-Kenari"
                    }
                ],
                "author_detail": {
                    "name": "Masoumeh Nasiri-Kenari"
                },
                "author": "Masoumeh Nasiri-Kenari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17559v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17559v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17786v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17786v1",
                "updated": "2024-11-26T15:03:14Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    15,
                    3,
                    14,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T15:03:14Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    15,
                    3,
                    14,
                    1,
                    331,
                    0
                ],
                "title": "DreamCache: Finetuning-Free Lightweight Personalized Image Generation\n  via Feature Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DreamCache: Finetuning-Free Lightweight Personalized Image Generation\n  via Feature Caching"
                },
                "summary": "Personalized image generation requires text-to-image generative models that\ncapture the core features of a reference subject to allow for controlled\ngeneration across different contexts. Existing methods face challenges due to\ncomplex training requirements, high inference costs, limited flexibility, or a\ncombination of these issues. In this paper, we introduce DreamCache, a scalable\napproach for efficient and high-quality personalized image generation. By\ncaching a small number of reference image features from a subset of layers and\na single timestep of the pretrained diffusion denoiser, DreamCache enables\ndynamic modulation of the generated image features through lightweight, trained\nconditioning adapters. DreamCache achieves state-of-the-art image and text\nalignment, utilizing an order of magnitude fewer extra parameters, and is both\nmore computationally effective and versatile than existing models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalized image generation requires text-to-image generative models that\ncapture the core features of a reference subject to allow for controlled\ngeneration across different contexts. Existing methods face challenges due to\ncomplex training requirements, high inference costs, limited flexibility, or a\ncombination of these issues. In this paper, we introduce DreamCache, a scalable\napproach for efficient and high-quality personalized image generation. By\ncaching a small number of reference image features from a subset of layers and\na single timestep of the pretrained diffusion denoiser, DreamCache enables\ndynamic modulation of the generated image features through lightweight, trained\nconditioning adapters. DreamCache achieves state-of-the-art image and text\nalignment, utilizing an order of magnitude fewer extra parameters, and is both\nmore computationally effective and versatile than existing models."
                },
                "authors": [
                    {
                        "name": "Emanuele Aiello"
                    },
                    {
                        "name": "Umberto Michieli"
                    },
                    {
                        "name": "Diego Valsesia"
                    },
                    {
                        "name": "Mete Ozay"
                    },
                    {
                        "name": "Enrico Magli"
                    }
                ],
                "author_detail": {
                    "name": "Enrico Magli"
                },
                "author": "Enrico Magli",
                "arxiv_comment": "16 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17786v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17786v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17116v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17116v1",
                "updated": "2024-11-26T05:10:04Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    5,
                    10,
                    4,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T05:10:04Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    5,
                    10,
                    4,
                    1,
                    331,
                    0
                ],
                "title": "Star Attention: Efficient LLM Inference over Long Sequences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Star Attention: Efficient LLM Inference over Long Sequences"
                },
                "summary": "Inference with Transformer-based Large Language Models (LLMs) on long\nsequences is both costly and slow due to the quadratic complexity of the\nself-attention mechanism. We introduce Star Attention, a two-phase block-sparse\napproximation that improves computational efficiency by sharding attention\nacross multiple hosts while minimizing communication overhead. In the first\nphase, the context is processed using blockwise-local attention across hosts,\nin parallel. In the second phase, query and response tokens attend to all prior\ncached tokens through sequence-global attention. Star Attention integrates\nseamlessly with most Transformer-based LLMs trained with global attention,\nreducing memory requirements and inference time by up to 11x while preserving\n95-100% of accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference with Transformer-based Large Language Models (LLMs) on long\nsequences is both costly and slow due to the quadratic complexity of the\nself-attention mechanism. We introduce Star Attention, a two-phase block-sparse\napproximation that improves computational efficiency by sharding attention\nacross multiple hosts while minimizing communication overhead. In the first\nphase, the context is processed using blockwise-local attention across hosts,\nin parallel. In the second phase, query and response tokens attend to all prior\ncached tokens through sequence-global attention. Star Attention integrates\nseamlessly with most Transformer-based LLMs trained with global attention,\nreducing memory requirements and inference time by up to 11x while preserving\n95-100% of accuracy."
                },
                "authors": [
                    {
                        "name": "Shantanu Acharya"
                    },
                    {
                        "name": "Fei Jia"
                    },
                    {
                        "name": "Boris Ginsburg"
                    }
                ],
                "author_detail": {
                    "name": "Boris Ginsburg"
                },
                "author": "Boris Ginsburg",
                "arxiv_comment": "Code: https://github.com/NVIDIA/Star-Attention",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17116v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17116v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17089v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17089v1",
                "updated": "2024-11-26T04:03:14Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    4,
                    3,
                    14,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T04:03:14Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    4,
                    3,
                    14,
                    1,
                    331,
                    0
                ],
                "title": "Efficient LLM Inference with I/O-Aware Partial KV Cache Recomputation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient LLM Inference with I/O-Aware Partial KV Cache Recomputation"
                },
                "summary": "Inference for Large Language Models (LLMs) is computationally demanding. To\nreduce the cost of auto-regressive decoding, Key-Value (KV) caching is used to\nstore intermediate activations, enabling GPUs to perform only the incremental\ncomputation required for each new token. This approach significantly lowers the\ncomputational overhead for token generation. However, the memory required for\nKV caching grows rapidly, often exceeding the capacity of GPU memory. A\ncost-effective alternative is to offload KV cache to CPU memory, which\nalleviates GPU memory pressure but shifts the bottleneck to the limited\nbandwidth of the PCIe connection between the CPU and GPU. Existing methods\nattempt to address these issues by overlapping GPU computation with I/O or\nemploying CPU-GPU heterogeneous execution, but they are hindered by excessive\ndata movement and dependence on CPU capabilities. In this paper, we introduce\nan efficient CPU-GPU I/O-aware LLM inference method that avoids transferring\nthe entire KV cache from CPU to GPU by recomputing partial KV cache from\nactivations while concurrently transferring the remaining KV cache via PCIe\nbus. This approach overlaps GPU recomputation with data transfer to minimize\nidle GPU time and maximize inference performance. Our method is fully automated\nby integrating a profiler module that utilizes input characteristics and system\nhardware information, a scheduler module to optimize the distribution of\ncomputation and communication workloads, and a runtime module to efficiently\nexecute the derived execution plan. Experimental results show that our method\nachieves up to 35.8% lower latency and 46.2% higher throughput during decoding\ncompared to state-of-the-art approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference for Large Language Models (LLMs) is computationally demanding. To\nreduce the cost of auto-regressive decoding, Key-Value (KV) caching is used to\nstore intermediate activations, enabling GPUs to perform only the incremental\ncomputation required for each new token. This approach significantly lowers the\ncomputational overhead for token generation. However, the memory required for\nKV caching grows rapidly, often exceeding the capacity of GPU memory. A\ncost-effective alternative is to offload KV cache to CPU memory, which\nalleviates GPU memory pressure but shifts the bottleneck to the limited\nbandwidth of the PCIe connection between the CPU and GPU. Existing methods\nattempt to address these issues by overlapping GPU computation with I/O or\nemploying CPU-GPU heterogeneous execution, but they are hindered by excessive\ndata movement and dependence on CPU capabilities. In this paper, we introduce\nan efficient CPU-GPU I/O-aware LLM inference method that avoids transferring\nthe entire KV cache from CPU to GPU by recomputing partial KV cache from\nactivations while concurrently transferring the remaining KV cache via PCIe\nbus. This approach overlaps GPU recomputation with data transfer to minimize\nidle GPU time and maximize inference performance. Our method is fully automated\nby integrating a profiler module that utilizes input characteristics and system\nhardware information, a scheduler module to optimize the distribution of\ncomputation and communication workloads, and a runtime module to efficiently\nexecute the derived execution plan. Experimental results show that our method\nachieves up to 35.8% lower latency and 46.2% higher throughput during decoding\ncompared to state-of-the-art approaches."
                },
                "authors": [
                    {
                        "name": "Chaoyi Jiang"
                    },
                    {
                        "name": "Lei Gao"
                    },
                    {
                        "name": "Hossein Entezari Zarch"
                    },
                    {
                        "name": "Murali Annavaram"
                    }
                ],
                "author_detail": {
                    "name": "Murali Annavaram"
                },
                "author": "Murali Annavaram",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17089v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17089v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16375v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16375v1",
                "updated": "2024-11-25T13:33:41Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    13,
                    33,
                    41,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-25T13:33:41Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    13,
                    33,
                    41,
                    0,
                    330,
                    0
                ],
                "title": "Ca2-VDM: Efficient Autoregressive Video Diffusion Model with Causal\n  Generation and Cache Sharing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ca2-VDM: Efficient Autoregressive Video Diffusion Model with Causal\n  Generation and Cache Sharing"
                },
                "summary": "With the advance of diffusion models, today's video generation has achieved\nimpressive quality. To extend the generation length and facilitate real-world\napplications, a majority of video diffusion models (VDMs) generate videos in an\nautoregressive manner, i.e., generating subsequent clips conditioned on the\nlast frame(s) of the previous clip. However, existing autoregressive VDMs are\nhighly inefficient and redundant: The model must re-compute all the conditional\nframes that are overlapped between adjacent clips. This issue is exacerbated\nwhen the conditional frames are extended autoregressively to provide the model\nwith long-term context. In such cases, the computational demands increase\nsignificantly (i.e., with a quadratic complexity w.r.t. the autoregression\nstep). In this paper, we propose Ca2-VDM, an efficient autoregressive VDM with\nCausal generation and Cache sharing. For causal generation, it introduces\nunidirectional feature computation, which ensures that the cache of conditional\nframes can be precomputed in previous autoregression steps and reused in every\nsubsequent step, eliminating redundant computations. For cache sharing, it\nshares the cache across all denoising steps to avoid the huge cache storage\ncost. Extensive experiments demonstrated that our Ca2-VDM achieves\nstate-of-the-art quantitative and qualitative video generation results and\nsignificantly improves the generation speed. Code is available at\nhttps://github.com/Dawn-LX/CausalCache-VDM",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the advance of diffusion models, today's video generation has achieved\nimpressive quality. To extend the generation length and facilitate real-world\napplications, a majority of video diffusion models (VDMs) generate videos in an\nautoregressive manner, i.e., generating subsequent clips conditioned on the\nlast frame(s) of the previous clip. However, existing autoregressive VDMs are\nhighly inefficient and redundant: The model must re-compute all the conditional\nframes that are overlapped between adjacent clips. This issue is exacerbated\nwhen the conditional frames are extended autoregressively to provide the model\nwith long-term context. In such cases, the computational demands increase\nsignificantly (i.e., with a quadratic complexity w.r.t. the autoregression\nstep). In this paper, we propose Ca2-VDM, an efficient autoregressive VDM with\nCausal generation and Cache sharing. For causal generation, it introduces\nunidirectional feature computation, which ensures that the cache of conditional\nframes can be precomputed in previous autoregression steps and reused in every\nsubsequent step, eliminating redundant computations. For cache sharing, it\nshares the cache across all denoising steps to avoid the huge cache storage\ncost. Extensive experiments demonstrated that our Ca2-VDM achieves\nstate-of-the-art quantitative and qualitative video generation results and\nsignificantly improves the generation speed. Code is available at\nhttps://github.com/Dawn-LX/CausalCache-VDM"
                },
                "authors": [
                    {
                        "name": "Kaifeng Gao"
                    },
                    {
                        "name": "Jiaxin Shi"
                    },
                    {
                        "name": "Hanwang Zhang"
                    },
                    {
                        "name": "Chunping Wang"
                    },
                    {
                        "name": "Jun Xiao"
                    },
                    {
                        "name": "Long Chen"
                    }
                ],
                "author_detail": {
                    "name": "Long Chen"
                },
                "author": "Long Chen",
                "arxiv_comment": "Technical Report. Code is available at\n  https://github.com/Dawn-LX/CausalCache-VDM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16375v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16375v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19315v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19315v2",
                "updated": "2024-11-25T12:14:33Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    12,
                    14,
                    33,
                    0,
                    330,
                    0
                ],
                "published": "2024-09-28T11:00:11Z",
                "published_parsed": [
                    2024,
                    9,
                    28,
                    11,
                    0,
                    11,
                    5,
                    272,
                    0
                ],
                "title": "Analog In-Memory Computing Attention Mechanism for Fast and\n  Energy-Efficient Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analog In-Memory Computing Attention Mechanism for Fast and\n  Energy-Efficient Large Language Models"
                },
                "summary": "Transformer networks, driven by self-attention, are central to Large Language\nModels. In generative Transformers, self-attention uses cache memory to store\ntoken projections, avoiding recomputation at each time step. However,\nGPU-stored projections must be loaded into SRAM for each new generation step,\ncausing latency and energy bottlenecks.\n  We present a custom self-attention in-memory computing architecture based on\nemerging charge-based memories called gain cells, which can be efficiently\nwritten to store new tokens during sequence generation and enable parallel\nanalog dot-product computation required for self-attention. However, the analog\ngain cell circuits introduce non-idealities and constraints preventing the\ndirect mapping of pre-trained models. To circumvent this problem, we design an\ninitialization algorithm achieving text processing performance comparable to\nGPT-2 without training from scratch. Our architecture respectively reduces\nattention latency and energy consumption by up to two and five orders of\nmagnitude compared to GPUs, marking a significant step toward ultra-fast,\nlow-power generative Transformers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer networks, driven by self-attention, are central to Large Language\nModels. In generative Transformers, self-attention uses cache memory to store\ntoken projections, avoiding recomputation at each time step. However,\nGPU-stored projections must be loaded into SRAM for each new generation step,\ncausing latency and energy bottlenecks.\n  We present a custom self-attention in-memory computing architecture based on\nemerging charge-based memories called gain cells, which can be efficiently\nwritten to store new tokens during sequence generation and enable parallel\nanalog dot-product computation required for self-attention. However, the analog\ngain cell circuits introduce non-idealities and constraints preventing the\ndirect mapping of pre-trained models. To circumvent this problem, we design an\ninitialization algorithm achieving text processing performance comparable to\nGPT-2 without training from scratch. Our architecture respectively reduces\nattention latency and energy consumption by up to two and five orders of\nmagnitude compared to GPUs, marking a significant step toward ultra-fast,\nlow-power generative Transformers."
                },
                "authors": [
                    {
                        "name": "Nathan Leroux"
                    },
                    {
                        "name": "Paul-Philipp Manea"
                    },
                    {
                        "name": "Chirag Sudarshan"
                    },
                    {
                        "name": "Jan Finkbeiner"
                    },
                    {
                        "name": "Sebastian Siegel"
                    },
                    {
                        "name": "John Paul Strachan"
                    },
                    {
                        "name": "Emre Neftci"
                    }
                ],
                "author_detail": {
                    "name": "Emre Neftci"
                },
                "author": "Emre Neftci",
                "arxiv_comment": "25 pages, 6 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19315v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19315v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11469v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11469v2",
                "updated": "2024-11-24T21:57:29Z",
                "updated_parsed": [
                    2024,
                    11,
                    24,
                    21,
                    57,
                    29,
                    6,
                    329,
                    0
                ],
                "published": "2024-11-18T11:12:57Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    11,
                    12,
                    57,
                    0,
                    323,
                    0
                ],
                "title": "Deegen: A JIT-Capable VM Generator for Dynamic Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deegen: A JIT-Capable VM Generator for Dynamic Languages"
                },
                "summary": "Building a high-performance JIT-capable VM for a dynamic language has\ntraditionally required a tremendous amount of time, money, and expertise. We\npresent Deegen, a meta-compiler that allows users to generate a\nhigh-performance JIT-capable VM for their own language at an engineering cost\nsimilar to writing a simple interpreter. Deegen takes in the execution\nsemantics of the bytecodes implemented as C++ functions, and automatically\ngenerates a two-tier VM execution engine with a state-of-the-art interpreter, a\nstate-of-the-art baseline JIT, and the tier-switching logic that connects them\ninto a self-adaptive system.\n  We are the first to demonstrate the automatic generation of a JIT compiler,\nand the automatic generation of an interpreter that outperforms the state of\nthe art. Our performance comes from a long list of optimizations supported by\nDeegen, including bytecode specialization and quickening, register pinning, tag\nregister optimization, call inline caching, generic inline caching, JIT\npolymorphic IC, JIT IC inline slab, type-check removal and strength reduction,\ntype-based slow-path extraction and outlining, JIT hot-cold code splitting, and\nJIT OSR-entry. These optimizations are either employed automatically, or guided\nby the language implementer through intuitive APIs. As a result, the\ndisassembly of the Deegen-generated interpreter, baseline JIT, and the\ngenerated JIT code rivals the assembly code hand-written by experts in\nstate-of-the-art VMs.\n  We implement LuaJIT Remake (LJR), a standard-compliant Lua 5.1 VM, using\nDeegen. Across 44 benchmarks, LJR's interpreter is on average 179% faster than\nthe official PUC Lua interpreter, and 31% faster than LuaJIT's interpreter.\nLJR's baseline JIT has negligible startup delay, and its execution performance\nis on average 360% faster than PUC Lua and only 33% slower (but faster on 13/44\nbenchmarks) than LuaJIT's optimizing JIT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building a high-performance JIT-capable VM for a dynamic language has\ntraditionally required a tremendous amount of time, money, and expertise. We\npresent Deegen, a meta-compiler that allows users to generate a\nhigh-performance JIT-capable VM for their own language at an engineering cost\nsimilar to writing a simple interpreter. Deegen takes in the execution\nsemantics of the bytecodes implemented as C++ functions, and automatically\ngenerates a two-tier VM execution engine with a state-of-the-art interpreter, a\nstate-of-the-art baseline JIT, and the tier-switching logic that connects them\ninto a self-adaptive system.\n  We are the first to demonstrate the automatic generation of a JIT compiler,\nand the automatic generation of an interpreter that outperforms the state of\nthe art. Our performance comes from a long list of optimizations supported by\nDeegen, including bytecode specialization and quickening, register pinning, tag\nregister optimization, call inline caching, generic inline caching, JIT\npolymorphic IC, JIT IC inline slab, type-check removal and strength reduction,\ntype-based slow-path extraction and outlining, JIT hot-cold code splitting, and\nJIT OSR-entry. These optimizations are either employed automatically, or guided\nby the language implementer through intuitive APIs. As a result, the\ndisassembly of the Deegen-generated interpreter, baseline JIT, and the\ngenerated JIT code rivals the assembly code hand-written by experts in\nstate-of-the-art VMs.\n  We implement LuaJIT Remake (LJR), a standard-compliant Lua 5.1 VM, using\nDeegen. Across 44 benchmarks, LJR's interpreter is on average 179% faster than\nthe official PUC Lua interpreter, and 31% faster than LuaJIT's interpreter.\nLJR's baseline JIT has negligible startup delay, and its execution performance\nis on average 360% faster than PUC Lua and only 33% slower (but faster on 13/44\nbenchmarks) than LuaJIT's optimizing JIT."
                },
                "authors": [
                    {
                        "name": "Haoran Xu"
                    },
                    {
                        "name": "Fredrik Kjolstad"
                    }
                ],
                "author_detail": {
                    "name": "Fredrik Kjolstad"
                },
                "author": "Fredrik Kjolstad",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11469v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11469v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17741v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17741v1",
                "updated": "2024-11-24T16:20:57Z",
                "updated_parsed": [
                    2024,
                    11,
                    24,
                    16,
                    20,
                    57,
                    6,
                    329,
                    0
                ],
                "published": "2024-11-24T16:20:57Z",
                "published_parsed": [
                    2024,
                    11,
                    24,
                    16,
                    20,
                    57,
                    6,
                    329,
                    0
                ],
                "title": "Chameleon: Adaptive Caching and Scheduling for Many-Adapter LLM\n  Inference Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chameleon: Adaptive Caching and Scheduling for Many-Adapter LLM\n  Inference Environments"
                },
                "summary": "The widespread adoption of LLMs has driven an exponential rise in their\ndeployment, imposing substantial demands on inference clusters. These clusters\nmust handle numerous concurrent queries for different LLM downstream tasks. To\nhandle multi-task settings with vast LLM parameter counts, methods like\nLow-Rank Adaptation (LoRA) enable task-specific fine-tuning while sharing most\nof the base LLM model across tasks. Hence, they allow concurrent task serving\nwith minimal memory requirements. However, existing LLM serving systems face\ninefficiencies: they overlook workload heterogeneity, impose high link\nbandwidth from frequent adapter loading, and suffer from head-of-line blocking\nin their schedulers. To address these challenges, we present Chameleon, a novel\nLLM serving system optimized for many adapter environments, that relies on two\ncore ideas: adapter caching and adapter-aware scheduling. First, Chameleon\ncaches popular adapters in GPU memory, minimizing the adapter loading times.\nImportantly, it uses the otherwise idle GPU memory, avoiding extra memory\ncosts. Second, Chameleon uses a non-preemptive multi-queue scheduling to\nefficiently account for workload heterogeneity. In this way, Chameleon\nsimultaneously prevents head of line blocking and starvation. We implement\nChameleon on top of a state-of-the-art LLM serving platform and evaluate it\nwith real-world production traces and open-source LLMs. Under high loads,\nChameleon reduces P99 and P50 TTFT latency by 80.7% and 48.1%, respectively,\nwhile improving throughput by 1.5x compared to state-of-the-art baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The widespread adoption of LLMs has driven an exponential rise in their\ndeployment, imposing substantial demands on inference clusters. These clusters\nmust handle numerous concurrent queries for different LLM downstream tasks. To\nhandle multi-task settings with vast LLM parameter counts, methods like\nLow-Rank Adaptation (LoRA) enable task-specific fine-tuning while sharing most\nof the base LLM model across tasks. Hence, they allow concurrent task serving\nwith minimal memory requirements. However, existing LLM serving systems face\ninefficiencies: they overlook workload heterogeneity, impose high link\nbandwidth from frequent adapter loading, and suffer from head-of-line blocking\nin their schedulers. To address these challenges, we present Chameleon, a novel\nLLM serving system optimized for many adapter environments, that relies on two\ncore ideas: adapter caching and adapter-aware scheduling. First, Chameleon\ncaches popular adapters in GPU memory, minimizing the adapter loading times.\nImportantly, it uses the otherwise idle GPU memory, avoiding extra memory\ncosts. Second, Chameleon uses a non-preemptive multi-queue scheduling to\nefficiently account for workload heterogeneity. In this way, Chameleon\nsimultaneously prevents head of line blocking and starvation. We implement\nChameleon on top of a state-of-the-art LLM serving platform and evaluate it\nwith real-world production traces and open-source LLMs. Under high loads,\nChameleon reduces P99 and P50 TTFT latency by 80.7% and 48.1%, respectively,\nwhile improving throughput by 1.5x compared to state-of-the-art baselines."
                },
                "authors": [
                    {
                        "name": "Nikoleta Iliakopoulou"
                    },
                    {
                        "name": "Jovan Stojkovic"
                    },
                    {
                        "name": "Chloe Alverti"
                    },
                    {
                        "name": "Tianyin Xu"
                    },
                    {
                        "name": "Hubertus Franke"
                    },
                    {
                        "name": "Josep Torrellas"
                    }
                ],
                "author_detail": {
                    "name": "Josep Torrellas"
                },
                "author": "Josep Torrellas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17741v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17741v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.0; D.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15735v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15735v1",
                "updated": "2024-11-24T06:43:38Z",
                "updated_parsed": [
                    2024,
                    11,
                    24,
                    6,
                    43,
                    38,
                    6,
                    329,
                    0
                ],
                "published": "2024-11-24T06:43:38Z",
                "published_parsed": [
                    2024,
                    11,
                    24,
                    6,
                    43,
                    38,
                    6,
                    329,
                    0
                ],
                "title": "Test-time Alignment-Enhanced Adapter for Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time Alignment-Enhanced Adapter for Vision-Language Models"
                },
                "summary": "Test-time adaptation with pre-trained vision-language models (VLMs) has\nattracted increasing attention for tackling the issue of distribution shift\nduring the test phase. While prior methods have shown effectiveness in\naddressing distribution shift by adjusting classification logits, they are not\noptimal due to keeping text features unchanged. To address this issue, we\nintroduce a new approach called Test-time Alignment-Enhanced Adapter (TAEA),\nwhich trains an adapter with test samples to adjust text features during the\ntest phase. We can enhance the text-to-image alignment prediction by utilizing\nan adapter to adapt text features. Furthermore, we also propose to adopt the\nnegative cache from TDA as enhancement module, which further improves the\nperformance of TAEA. Our approach outperforms the state-of-the-art TTA method\nof pre-trained VLMs by an average of 0.75% on the out-of-distribution benchmark\nand 2.5% on the cross-domain benchmark, with an acceptable training time. Code\nwill be available at https://github.com/BaoshunWq/clip-TAEA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time adaptation with pre-trained vision-language models (VLMs) has\nattracted increasing attention for tackling the issue of distribution shift\nduring the test phase. While prior methods have shown effectiveness in\naddressing distribution shift by adjusting classification logits, they are not\noptimal due to keeping text features unchanged. To address this issue, we\nintroduce a new approach called Test-time Alignment-Enhanced Adapter (TAEA),\nwhich trains an adapter with test samples to adjust text features during the\ntest phase. We can enhance the text-to-image alignment prediction by utilizing\nan adapter to adapt text features. Furthermore, we also propose to adopt the\nnegative cache from TDA as enhancement module, which further improves the\nperformance of TAEA. Our approach outperforms the state-of-the-art TTA method\nof pre-trained VLMs by an average of 0.75% on the out-of-distribution benchmark\nand 2.5% on the cross-domain benchmark, with an acceptable training time. Code\nwill be available at https://github.com/BaoshunWq/clip-TAEA."
                },
                "authors": [
                    {
                        "name": "Baoshun Tong"
                    },
                    {
                        "name": "Kaiyu Song"
                    },
                    {
                        "name": "Hanjiang Lai"
                    }
                ],
                "author_detail": {
                    "name": "Hanjiang Lai"
                },
                "author": "Hanjiang Lai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15735v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15735v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09688v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09688v2",
                "updated": "2024-11-23T22:11:42Z",
                "updated_parsed": [
                    2024,
                    11,
                    23,
                    22,
                    11,
                    42,
                    5,
                    328,
                    0
                ],
                "published": "2024-11-14T18:54:19Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    18,
                    54,
                    19,
                    3,
                    319,
                    0
                ],
                "title": "Squeezed Attention: Accelerating Long Context Length LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Squeezed Attention: Accelerating Long Context Length LLM Inference"
                },
                "summary": "Emerging Large Language Model (LLM) applications require long input prompts\nto perform complex downstream tasks like document analysis and code generation.\nFor these long context length applications, the length of the input prompt\nposes a significant challenge in terms of inference efficiency since the\ninference costs increase linearly with sequence length. However, for many of\nthese applications, much of the context in the prompt is fixed across different\nuser inputs, thereby providing the opportunity to perform offline optimizations\nto process user inputs quickly, as they are received. In this work, we propose\nSqueezed Attention as a mechanism to accelerate LLM applications where a large\nportion of the input prompt is fixed. We first leverage K-means clustering\noffline to group the keys for the fixed context based on semantic similarity\nand represent each cluster with a single centroid value. During inference, we\ncompare query tokens from the user input with the centroids to predict which of\nthe keys from the fixed context are semantically relevant and need to be loaded\nduring inference. We then compute exact attention using only these important\nkeys from the fixed context, thereby reducing bandwidth and computational\ncosts. We also extend our method to use a hierarchical centroid lookup to\nidentify important keys, which can reduce the complexity of attention from\nlinear to logarithmic with respect to the context length. We implement\noptimized Triton kernels for centroid comparison and sparse FlashAttention with\nimportant keys, achieving more than 4x speedups during both the prefill and\ngeneration phases for long-context inference. Furthermore, we have extensively\nevaluated our method on various long-context benchmarks including LongBench,\nwhere it achieves a 3x reduction in KV cache budget without accuracy loss and\nup to an 8x reduction with <0.5 point accuracy gap for various models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emerging Large Language Model (LLM) applications require long input prompts\nto perform complex downstream tasks like document analysis and code generation.\nFor these long context length applications, the length of the input prompt\nposes a significant challenge in terms of inference efficiency since the\ninference costs increase linearly with sequence length. However, for many of\nthese applications, much of the context in the prompt is fixed across different\nuser inputs, thereby providing the opportunity to perform offline optimizations\nto process user inputs quickly, as they are received. In this work, we propose\nSqueezed Attention as a mechanism to accelerate LLM applications where a large\nportion of the input prompt is fixed. We first leverage K-means clustering\noffline to group the keys for the fixed context based on semantic similarity\nand represent each cluster with a single centroid value. During inference, we\ncompare query tokens from the user input with the centroids to predict which of\nthe keys from the fixed context are semantically relevant and need to be loaded\nduring inference. We then compute exact attention using only these important\nkeys from the fixed context, thereby reducing bandwidth and computational\ncosts. We also extend our method to use a hierarchical centroid lookup to\nidentify important keys, which can reduce the complexity of attention from\nlinear to logarithmic with respect to the context length. We implement\noptimized Triton kernels for centroid comparison and sparse FlashAttention with\nimportant keys, achieving more than 4x speedups during both the prefill and\ngeneration phases for long-context inference. Furthermore, we have extensively\nevaluated our method on various long-context benchmarks including LongBench,\nwhere it achieves a 3x reduction in KV cache budget without accuracy loss and\nup to an 8x reduction with <0.5 point accuracy gap for various models."
                },
                "authors": [
                    {
                        "name": "Coleman Hooper"
                    },
                    {
                        "name": "Sehoon Kim"
                    },
                    {
                        "name": "Hiva Mohammadzadeh"
                    },
                    {
                        "name": "Monishwaran Maheswaran"
                    },
                    {
                        "name": "June Paik"
                    },
                    {
                        "name": "Michael W. Mahoney"
                    },
                    {
                        "name": "Kurt Keutzer"
                    },
                    {
                        "name": "Amir Gholami"
                    }
                ],
                "author_detail": {
                    "name": "Amir Gholami"
                },
                "author": "Amir Gholami",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09688v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09688v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.05396v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.05396v3",
                "updated": "2024-11-23T10:42:11Z",
                "updated_parsed": [
                    2024,
                    11,
                    23,
                    10,
                    42,
                    11,
                    5,
                    328,
                    0
                ],
                "published": "2024-02-08T04:16:35Z",
                "published_parsed": [
                    2024,
                    2,
                    8,
                    4,
                    16,
                    35,
                    3,
                    39,
                    0
                ],
                "title": "TASER: Temporal Adaptive Sampling for Fast and Accurate Dynamic Graph\n  Representation Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TASER: Temporal Adaptive Sampling for Fast and Accurate Dynamic Graph\n  Representation Learning"
                },
                "summary": "Recently, Temporal Graph Neural Networks (TGNNs) have demonstrated\nstate-of-the-art performance in various high-impact applications, including\nfraud detection and content recommendation. Despite the success of TGNNs, they\nare prone to the prevalent noise found in real-world dynamic graphs like\ntime-deprecated links and skewed interaction distribution. The noise causes two\ncritical issues that significantly compromise the accuracy of TGNNs: (1) models\nare supervised by inferior interactions, and (2) noisy input induces high\nvariance in the aggregated messages. However, current TGNN denoising techniques\ndo not consider the diverse and dynamic noise pattern of each node. In\naddition, they also suffer from the excessive mini-batch generation overheads\ncaused by traversing more neighbors. We believe the remedy for fast and\naccurate TGNNs lies in temporal adaptive sampling. In this work, we propose\nTASER, the first adaptive sampling method for TGNNs optimized for accuracy,\nefficiency, and scalability. TASER adapts its mini-batch selection based on\ntraining dynamics and temporal neighbor selection based on the contextual,\nstructural, and temporal properties of past interactions. To alleviate the\nbottleneck in mini-batch generation, TASER implements a pure GPU-based temporal\nneighbor finder and a dedicated GPU feature cache. We evaluate the performance\nof TASER using two state-of-the-art backbone TGNNs. On five popular datasets,\nTASER outperforms the corresponding baselines by an average of 2.3% in Mean\nReciprocal Rank (MRR) while achieving an average of 5.1x speedup in training\ntime.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, Temporal Graph Neural Networks (TGNNs) have demonstrated\nstate-of-the-art performance in various high-impact applications, including\nfraud detection and content recommendation. Despite the success of TGNNs, they\nare prone to the prevalent noise found in real-world dynamic graphs like\ntime-deprecated links and skewed interaction distribution. The noise causes two\ncritical issues that significantly compromise the accuracy of TGNNs: (1) models\nare supervised by inferior interactions, and (2) noisy input induces high\nvariance in the aggregated messages. However, current TGNN denoising techniques\ndo not consider the diverse and dynamic noise pattern of each node. In\naddition, they also suffer from the excessive mini-batch generation overheads\ncaused by traversing more neighbors. We believe the remedy for fast and\naccurate TGNNs lies in temporal adaptive sampling. In this work, we propose\nTASER, the first adaptive sampling method for TGNNs optimized for accuracy,\nefficiency, and scalability. TASER adapts its mini-batch selection based on\ntraining dynamics and temporal neighbor selection based on the contextual,\nstructural, and temporal properties of past interactions. To alleviate the\nbottleneck in mini-batch generation, TASER implements a pure GPU-based temporal\nneighbor finder and a dedicated GPU feature cache. We evaluate the performance\nof TASER using two state-of-the-art backbone TGNNs. On five popular datasets,\nTASER outperforms the corresponding baselines by an average of 2.3% in Mean\nReciprocal Rank (MRR) while achieving an average of 5.1x speedup in training\ntime."
                },
                "authors": [
                    {
                        "name": "Gangda Deng"
                    },
                    {
                        "name": "Hongkuan Zhou"
                    },
                    {
                        "name": "Hanqing Zeng"
                    },
                    {
                        "name": "Yinglong Xia"
                    },
                    {
                        "name": "Christopher Leung"
                    },
                    {
                        "name": "Jianbo Li"
                    },
                    {
                        "name": "Rajgopal Kannan"
                    },
                    {
                        "name": "Viktor Prasanna"
                    }
                ],
                "author_detail": {
                    "name": "Viktor Prasanna"
                },
                "author": "Viktor Prasanna",
                "arxiv_comment": "IPDPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.05396v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.05396v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.02109v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.02109v2",
                "updated": "2024-11-23T01:44:00Z",
                "updated_parsed": [
                    2024,
                    11,
                    23,
                    1,
                    44,
                    0,
                    5,
                    328,
                    0
                ],
                "published": "2024-07-02T09:51:56Z",
                "published_parsed": [
                    2024,
                    7,
                    2,
                    9,
                    51,
                    56,
                    1,
                    184,
                    0
                ],
                "title": "HRSAM: Efficient Interactive Segmentation in High-Resolution Images",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HRSAM: Efficient Interactive Segmentation in High-Resolution Images"
                },
                "summary": "The Segment Anything Model (SAM) has advanced interactive segmentation but is\nlimited by the high computational cost on high-resolution images. This requires\ndownsampling to meet GPU constraints, sacrificing the fine-grained details\nneeded for high-precision interactive segmentation. To address SAM's\nlimitations, we focus on visual length extrapolation and propose a lightweight\nmodel named HRSAM. The extrapolation enables HRSAM trained on low resolutions\nto generalize to high resolutions. We begin by finding the link between the\nextrapolation and attention scores, which leads us to base HRSAM on Swin\nattention. We then introduce the Flexible Local Attention (FLA) framework,\nusing CUDA-optimized Efficient Memory Attention to accelerate HRSAM. Within\nFLA, we implement Flash Swin attention, achieving over a 35% speedup compared\nto traditional Swin attention, and propose a KV-only padding mechanism to\nenhance extrapolation. We also develop the Cycle-scan module that uses State\nSpace models to efficiently expand HRSAM's receptive field. We further develop\nthe HRSAM++ within FLA by adding an anchor map, providing multi-scale data\naugmentation for the extrapolation and a larger receptive field at slight\ncomputational cost. Experiments show that, under standard training, HRSAMs\nsurpass the previous SOTA with only 38% of the latency. With SAM-distillation,\nthe extrapolation enables HRSAMs to outperform the teacher model at lower\nlatency. Further finetuning achieves performance significantly exceeding the\nprevious SOTA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Segment Anything Model (SAM) has advanced interactive segmentation but is\nlimited by the high computational cost on high-resolution images. This requires\ndownsampling to meet GPU constraints, sacrificing the fine-grained details\nneeded for high-precision interactive segmentation. To address SAM's\nlimitations, we focus on visual length extrapolation and propose a lightweight\nmodel named HRSAM. The extrapolation enables HRSAM trained on low resolutions\nto generalize to high resolutions. We begin by finding the link between the\nextrapolation and attention scores, which leads us to base HRSAM on Swin\nattention. We then introduce the Flexible Local Attention (FLA) framework,\nusing CUDA-optimized Efficient Memory Attention to accelerate HRSAM. Within\nFLA, we implement Flash Swin attention, achieving over a 35% speedup compared\nto traditional Swin attention, and propose a KV-only padding mechanism to\nenhance extrapolation. We also develop the Cycle-scan module that uses State\nSpace models to efficiently expand HRSAM's receptive field. We further develop\nthe HRSAM++ within FLA by adding an anchor map, providing multi-scale data\naugmentation for the extrapolation and a larger receptive field at slight\ncomputational cost. Experiments show that, under standard training, HRSAMs\nsurpass the previous SOTA with only 38% of the latency. With SAM-distillation,\nthe extrapolation enables HRSAMs to outperform the teacher model at lower\nlatency. Further finetuning achieves performance significantly exceeding the\nprevious SOTA."
                },
                "authors": [
                    {
                        "name": "You Huang"
                    },
                    {
                        "name": "Wenbin Lai"
                    },
                    {
                        "name": "Jiayi Ji"
                    },
                    {
                        "name": "Liujuan Cao"
                    },
                    {
                        "name": "Shengchuan Zhang"
                    },
                    {
                        "name": "Rongrong Ji"
                    }
                ],
                "author_detail": {
                    "name": "Rongrong Ji"
                },
                "author": "Rongrong Ji",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.02109v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.02109v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15322v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15322v1",
                "updated": "2024-11-22T19:30:40Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    19,
                    30,
                    40,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-22T19:30:40Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    19,
                    30,
                    40,
                    4,
                    327,
                    0
                ],
                "title": "Deep Learning-Based Automatic Delineation of Liver Domes in kV Triggered\n  Images for Online Breath-hold Reproducibility Verification of Liver\n  Stereotactic Body Radiation Therapy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Learning-Based Automatic Delineation of Liver Domes in kV Triggered\n  Images for Online Breath-hold Reproducibility Verification of Liver\n  Stereotactic Body Radiation Therapy"
                },
                "summary": "Stereotactic Body Radiation Therapy (SBRT) can be a precise, minimally\ninvasive treatment method for liver cancer and liver metastases. However, the\neffectiveness of SBRT relies on the accurate delivery of the dose to the tumor\nwhile sparing healthy tissue. Challenges persist in ensuring breath-hold\nreproducibility, with current methods often requiring manual verification of\nliver dome positions from kV-triggered images. To address this, we propose a\nproof-of-principle study of a deep learning-based pipeline to automatically\ndelineate the liver dome from kV-planar images. From 24 patients who received\nSBRT for liver cancer or metastasis inside liver, 711 KV-triggered images\nacquired for online breath-hold verification were included in the current\nstudy. We developed a pipeline comprising a trained U-Net for automatic liver\ndome region segmentation from the triggered images followed by extraction of\nthe liver dome via thresholding, edge detection, and morphological operations.\nThe performance and generalizability of the pipeline was evaluated using 2-fold\ncross validation. The training of the U-Net model for liver region segmentation\ntook under 30 minutes and the automatic delineation of a liver dome for any\ntriggered image took less than one second. The RMSE and rate of detection for\nFold1 with 366 images was (6.4 +/- 1.6) mm and 91.7%, respectively. For Fold2\nwith 345 images, the RMSE and rate of detection was (7.7 +/- 2.3) mm and 76.3%\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stereotactic Body Radiation Therapy (SBRT) can be a precise, minimally\ninvasive treatment method for liver cancer and liver metastases. However, the\neffectiveness of SBRT relies on the accurate delivery of the dose to the tumor\nwhile sparing healthy tissue. Challenges persist in ensuring breath-hold\nreproducibility, with current methods often requiring manual verification of\nliver dome positions from kV-triggered images. To address this, we propose a\nproof-of-principle study of a deep learning-based pipeline to automatically\ndelineate the liver dome from kV-planar images. From 24 patients who received\nSBRT for liver cancer or metastasis inside liver, 711 KV-triggered images\nacquired for online breath-hold verification were included in the current\nstudy. We developed a pipeline comprising a trained U-Net for automatic liver\ndome region segmentation from the triggered images followed by extraction of\nthe liver dome via thresholding, edge detection, and morphological operations.\nThe performance and generalizability of the pipeline was evaluated using 2-fold\ncross validation. The training of the U-Net model for liver region segmentation\ntook under 30 minutes and the automatic delineation of a liver dome for any\ntriggered image took less than one second. The RMSE and rate of detection for\nFold1 with 366 images was (6.4 +/- 1.6) mm and 91.7%, respectively. For Fold2\nwith 345 images, the RMSE and rate of detection was (7.7 +/- 2.3) mm and 76.3%\nrespectively."
                },
                "authors": [
                    {
                        "name": "Sugandima Weragoda"
                    },
                    {
                        "name": "Ping Xia"
                    },
                    {
                        "name": "Kevin Stephans"
                    },
                    {
                        "name": "Neil Woody"
                    },
                    {
                        "name": "Michael Martens"
                    },
                    {
                        "name": "Robert Brown"
                    },
                    {
                        "name": "Bingqi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Bingqi Guo"
                },
                "author": "Bingqi Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15322v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15322v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.med-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15102v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15102v1",
                "updated": "2024-11-22T18:06:14Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    18,
                    6,
                    14,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-22T18:06:14Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    18,
                    6,
                    14,
                    4,
                    327,
                    0
                ],
                "title": "AttriBoT: A Bag of Tricks for Efficiently Approximating Leave-One-Out\n  Context Attribution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AttriBoT: A Bag of Tricks for Efficiently Approximating Leave-One-Out\n  Context Attribution"
                },
                "summary": "The influence of contextual input on the behavior of large language models\n(LLMs) has prompted the development of context attribution methods that aim to\nquantify each context span's effect on an LLM's generations. The leave-one-out\n(LOO) error, which measures the change in the likelihood of the LLM's response\nwhen a given span of the context is removed, provides a principled way to\nperform context attribution, but can be prohibitively expensive to compute for\nlarge models. In this work, we introduce AttriBoT, a series of novel techniques\nfor efficiently computing an approximation of the LOO error for context\nattribution. Specifically, AttriBoT uses cached activations to avoid redundant\noperations, performs hierarchical attribution to reduce computation, and\nemulates the behavior of large target models with smaller proxy models. Taken\ntogether, AttriBoT can provide a >300x speedup while remaining more faithful to\na target model's LOO error than prior context attribution methods. This stark\nincrease in performance makes computing context attributions for a given\nresponse 30x faster than generating the response itself, empowering real-world\napplications that require computing attributions at scale. We release a\nuser-friendly and efficient implementation of AttriBoT to enable efficient LLM\ninterpretability as well as encourage future development of efficient context\nattribution methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The influence of contextual input on the behavior of large language models\n(LLMs) has prompted the development of context attribution methods that aim to\nquantify each context span's effect on an LLM's generations. The leave-one-out\n(LOO) error, which measures the change in the likelihood of the LLM's response\nwhen a given span of the context is removed, provides a principled way to\nperform context attribution, but can be prohibitively expensive to compute for\nlarge models. In this work, we introduce AttriBoT, a series of novel techniques\nfor efficiently computing an approximation of the LOO error for context\nattribution. Specifically, AttriBoT uses cached activations to avoid redundant\noperations, performs hierarchical attribution to reduce computation, and\nemulates the behavior of large target models with smaller proxy models. Taken\ntogether, AttriBoT can provide a >300x speedup while remaining more faithful to\na target model's LOO error than prior context attribution methods. This stark\nincrease in performance makes computing context attributions for a given\nresponse 30x faster than generating the response itself, empowering real-world\napplications that require computing attributions at scale. We release a\nuser-friendly and efficient implementation of AttriBoT to enable efficient LLM\ninterpretability as well as encourage future development of efficient context\nattribution methods."
                },
                "authors": [
                    {
                        "name": "Fengyuan Liu"
                    },
                    {
                        "name": "Nikhil Kandpal"
                    },
                    {
                        "name": "Colin Raffel"
                    }
                ],
                "author_detail": {
                    "name": "Colin Raffel"
                },
                "author": "Colin Raffel",
                "arxiv_comment": "29 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15102v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15102v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15024v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15024v1",
                "updated": "2024-11-22T15:55:19Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    15,
                    55,
                    19,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-22T15:55:19Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    15,
                    55,
                    19,
                    4,
                    327,
                    0
                ],
                "title": "DyCoke: Dynamic Compression of Tokens for Fast Video Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DyCoke: Dynamic Compression of Tokens for Fast Video Large Language\n  Models"
                },
                "summary": "Video large language models (VLLMs) have significantly advanced recently in\nprocessing complex video content, yet their inference efficiency remains\nconstrained because of the high computational cost stemming from the thousands\nof visual tokens generated from the video inputs. We empirically observe that,\nunlike single image inputs, VLLMs typically attend visual tokens from different\nframes at different decoding iterations, making a one-shot pruning strategy\nprone to removing important tokens by mistake. Motivated by this, we present\nDyCoke, a training-free token compression method to optimize token\nrepresentation and accelerate VLLMs. DyCoke incorporates a plug-and-play\ntemporal compression module to minimize temporal redundancy by merging\nredundant tokens across frames, and applies dynamic KV cache reduction to prune\nspatially redundant tokens selectively. It ensures high-quality inference by\ndynamically retaining the critical tokens at each decoding step. Extensive\nexperimental results demonstrate that DyCoke can outperform the prior SoTA\ncounterparts, achieving 1.5X inference speedup, 1.4X memory reduction against\nthe baseline VLLM, while still improving the performance, with no training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video large language models (VLLMs) have significantly advanced recently in\nprocessing complex video content, yet their inference efficiency remains\nconstrained because of the high computational cost stemming from the thousands\nof visual tokens generated from the video inputs. We empirically observe that,\nunlike single image inputs, VLLMs typically attend visual tokens from different\nframes at different decoding iterations, making a one-shot pruning strategy\nprone to removing important tokens by mistake. Motivated by this, we present\nDyCoke, a training-free token compression method to optimize token\nrepresentation and accelerate VLLMs. DyCoke incorporates a plug-and-play\ntemporal compression module to minimize temporal redundancy by merging\nredundant tokens across frames, and applies dynamic KV cache reduction to prune\nspatially redundant tokens selectively. It ensures high-quality inference by\ndynamically retaining the critical tokens at each decoding step. Extensive\nexperimental results demonstrate that DyCoke can outperform the prior SoTA\ncounterparts, achieving 1.5X inference speedup, 1.4X memory reduction against\nthe baseline VLLM, while still improving the performance, with no training."
                },
                "authors": [
                    {
                        "name": "Keda Tao"
                    },
                    {
                        "name": "Can Qin"
                    },
                    {
                        "name": "Haoxuan You"
                    },
                    {
                        "name": "Yang Sui"
                    },
                    {
                        "name": "Huan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Huan Wang"
                },
                "author": "Huan Wang",
                "arxiv_comment": "12 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15024v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15024v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.04032v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.04032v5",
                "updated": "2024-11-21T05:55:43Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    5,
                    55,
                    43,
                    3,
                    326,
                    0
                ],
                "published": "2024-02-06T14:26:22Z",
                "published_parsed": [
                    2024,
                    2,
                    6,
                    14,
                    26,
                    22,
                    1,
                    37,
                    0
                ],
                "title": "ProactivePIM: Accelerating Weight-Sharing Embedding Layer with PIM for\n  Scalable Recommendation System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProactivePIM: Accelerating Weight-Sharing Embedding Layer with PIM for\n  Scalable Recommendation System"
                },
                "summary": "The model size growth of personalized recommendation systems poses new\nchallenges for inference. Weight-sharing algorithms have been proposed for size\nreduction, but they increase memory access. Recent advancements in\nprocessing-in-memory (PIM) enhanced the model throughput by exploiting memory\nparallelism, but such algorithms introduce massive CPU-PIM communication into\nprior PIM systems. We propose ProactivePIM, a PIM system for weight-sharing\nrecommendation system acceleration. ProactivePIM integrates a cache within the\nPIM with a prefetching scheme to leverage a unique locality of the algorithm\nand eliminate communication overhead through a subtable mapping strategy.\nProactivePIM achieves a 4.8x speedup compared to prior works.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The model size growth of personalized recommendation systems poses new\nchallenges for inference. Weight-sharing algorithms have been proposed for size\nreduction, but they increase memory access. Recent advancements in\nprocessing-in-memory (PIM) enhanced the model throughput by exploiting memory\nparallelism, but such algorithms introduce massive CPU-PIM communication into\nprior PIM systems. We propose ProactivePIM, a PIM system for weight-sharing\nrecommendation system acceleration. ProactivePIM integrates a cache within the\nPIM with a prefetching scheme to leverage a unique locality of the algorithm\nand eliminate communication overhead through a subtable mapping strategy.\nProactivePIM achieves a 4.8x speedup compared to prior works."
                },
                "authors": [
                    {
                        "name": "Youngsuk Kim"
                    },
                    {
                        "name": "Junghwan Lim"
                    },
                    {
                        "name": "Hyuk-Jae Lee"
                    },
                    {
                        "name": "Chae Eun Rhee"
                    }
                ],
                "author_detail": {
                    "name": "Chae Eun Rhee"
                },
                "author": "Chae Eun Rhee",
                "arxiv_comment": "8 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.04032v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.04032v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13854v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13854v1",
                "updated": "2024-11-21T05:26:57Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    5,
                    26,
                    57,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T05:26:57Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    5,
                    26,
                    57,
                    3,
                    326,
                    0
                ],
                "title": "Static Reuse Profile Estimation for Array Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Static Reuse Profile Estimation for Array Applications"
                },
                "summary": "Reuse distance analysis is a widely recognized method for application\ncharacterization that illustrates cache locality. Although there are various\ntechniques to calculate the reuse profile from dynamic memory traces, it is\nboth time and space-consuming due to the requirement to collect dynamic memory\ntraces at runtime. In contrast, static analysis reuse profile estimation is a\npromisingly faster approach since it is calculated at compile time without\nrunning the program or collecting memory traces. This work presents a static\nanalysis technique to estimate the reuse profile of loop-based programs. For an\ninput program, we generate a basic block-level control flow graph and the\nexecution count by analyzing the LLVM IR of the program. We present the memory\naccesses of the application kernel in a compact bracketed format and use a\nrecursive algorithm to predict the reuse distance histogram. We deploy a\nseparate predictor that unrolls the loop(s) for smaller bounds and generates a\ntemporary reuse distance profile for those small cases. Using these smaller\nprofiles, the reuse profile is extrapolated for the actual loop bound(s). We\nuse this reuse profile to predict the cache hit rate. Results show that our\nmodel can predict cache hit rates with an average accuracy of 95% relative to\nthe dynamic reuse profile methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reuse distance analysis is a widely recognized method for application\ncharacterization that illustrates cache locality. Although there are various\ntechniques to calculate the reuse profile from dynamic memory traces, it is\nboth time and space-consuming due to the requirement to collect dynamic memory\ntraces at runtime. In contrast, static analysis reuse profile estimation is a\npromisingly faster approach since it is calculated at compile time without\nrunning the program or collecting memory traces. This work presents a static\nanalysis technique to estimate the reuse profile of loop-based programs. For an\ninput program, we generate a basic block-level control flow graph and the\nexecution count by analyzing the LLVM IR of the program. We present the memory\naccesses of the application kernel in a compact bracketed format and use a\nrecursive algorithm to predict the reuse distance histogram. We deploy a\nseparate predictor that unrolls the loop(s) for smaller bounds and generates a\ntemporary reuse distance profile for those small cases. Using these smaller\nprofiles, the reuse profile is extrapolated for the actual loop bound(s). We\nuse this reuse profile to predict the cache hit rate. Results show that our\nmodel can predict cache hit rates with an average accuracy of 95% relative to\nthe dynamic reuse profile methods."
                },
                "authors": [
                    {
                        "name": "Abdur Razzak"
                    },
                    {
                        "name": "Atanu Barai"
                    },
                    {
                        "name": "Nandakishore Santhi"
                    },
                    {
                        "name": "Abdel-Hameed A. Badawy"
                    }
                ],
                "author_detail": {
                    "name": "Abdel-Hameed A. Badawy"
                },
                "author": "Abdel-Hameed A. Badawy",
                "arxiv_comment": "Accepted in The International Symposium on Memory Systems (MEMSYS\n  24), September 30 to October 03, 2024, Washington, DC, USA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13854v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13854v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2306.02243v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2306.02243v3",
                "updated": "2024-11-21T04:12:53Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    4,
                    12,
                    53,
                    3,
                    326,
                    0
                ],
                "published": "2023-06-04T03:06:37Z",
                "published_parsed": [
                    2023,
                    6,
                    4,
                    3,
                    6,
                    37,
                    6,
                    155,
                    0
                ],
                "title": "Retrieval-Enhanced Visual Prompt Learning for Few-shot Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Enhanced Visual Prompt Learning for Few-shot Classification"
                },
                "summary": "The Contrastive Language-Image Pretraining (CLIP) model has been widely used\nin various downstream vision tasks. The few-shot learning paradigm has been\nwidely adopted to augment its capacity for these tasks. However, current\nparadigms may struggle with fine-grained classification, such as satellite\nimage recognition, due to widening domain gaps. To address this limitation, we\npropose retrieval-enhanced visual prompt learning (RePrompt), which introduces\nretrieval mechanisms to cache and reuse the knowledge of downstream tasks.\nRePrompt constructs a retrieval database from either training examples or\nexternal data if available, and uses a retrieval mechanism to enhance multiple\nstages of a simple prompt learning baseline, thus narrowing the domain gap.\nDuring inference, our enhanced model can reference similar samples brought by\nretrieval to make more accurate predictions. A detailed analysis reveals that\nretrieval helps to improve the distribution of late features, thus, improving\ngeneralization for downstream tasks. Reprompt attains state-of-the-art\nperformance on a wide range of vision datasets, including 11 image datasets, 3\nvideo datasets, 1 multi-view dataset, and 4 domain generalization benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Contrastive Language-Image Pretraining (CLIP) model has been widely used\nin various downstream vision tasks. The few-shot learning paradigm has been\nwidely adopted to augment its capacity for these tasks. However, current\nparadigms may struggle with fine-grained classification, such as satellite\nimage recognition, due to widening domain gaps. To address this limitation, we\npropose retrieval-enhanced visual prompt learning (RePrompt), which introduces\nretrieval mechanisms to cache and reuse the knowledge of downstream tasks.\nRePrompt constructs a retrieval database from either training examples or\nexternal data if available, and uses a retrieval mechanism to enhance multiple\nstages of a simple prompt learning baseline, thus narrowing the domain gap.\nDuring inference, our enhanced model can reference similar samples brought by\nretrieval to make more accurate predictions. A detailed analysis reveals that\nretrieval helps to improve the distribution of late features, thus, improving\ngeneralization for downstream tasks. Reprompt attains state-of-the-art\nperformance on a wide range of vision datasets, including 11 image datasets, 3\nvideo datasets, 1 multi-view dataset, and 4 domain generalization benchmarks."
                },
                "authors": [
                    {
                        "name": "Jintao Rong"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Linlin Ou"
                    },
                    {
                        "name": "Tianxiao Chen"
                    },
                    {
                        "name": "Xinyi Yu"
                    },
                    {
                        "name": "Yifan Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yifan Liu"
                },
                "author": "Yifan Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2306.02243v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2306.02243v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13820v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13820v1",
                "updated": "2024-11-21T03:52:41Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    3,
                    52,
                    41,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T03:52:41Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    3,
                    52,
                    41,
                    3,
                    326,
                    0
                ],
                "title": "InstCache: A Predictive Cache for LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InstCache: A Predictive Cache for LLM Serving"
                },
                "summary": "Large language models are revolutionizing every aspect of human life.\nHowever, the unprecedented power comes at the cost of significant computing\nintensity, suggesting long latency and large energy footprint. Key-Value Cache\nand Semantic Cache have been proposed as a solution to the above problem, but\nboth suffer from limited scalability due to significant memory cost for each\ntoken or instruction embeddings. Motivated by the observations that most\ninstructions are short, repetitive and predictable by LLMs, we propose to\npredict user-instructions by an instruction-aligned LLM and store them in a\npredictive cache, so-called InstCache. We introduce an instruction\npre-population algorithm based on the negative log likelihood of instructions,\ndetermining the cache size with regard to the hit rate. The proposed InstCache\nis efficiently implemented as a hash table with minimal lookup latency for\ndeployment. Experimental results show that InstCache can achieve up to 51.34%\nhit rate on LMSys dataset, which corresponds to a 2x speedup, at a memory cost\nof only 4.5GB.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models are revolutionizing every aspect of human life.\nHowever, the unprecedented power comes at the cost of significant computing\nintensity, suggesting long latency and large energy footprint. Key-Value Cache\nand Semantic Cache have been proposed as a solution to the above problem, but\nboth suffer from limited scalability due to significant memory cost for each\ntoken or instruction embeddings. Motivated by the observations that most\ninstructions are short, repetitive and predictable by LLMs, we propose to\npredict user-instructions by an instruction-aligned LLM and store them in a\npredictive cache, so-called InstCache. We introduce an instruction\npre-population algorithm based on the negative log likelihood of instructions,\ndetermining the cache size with regard to the hit rate. The proposed InstCache\nis efficiently implemented as a hash table with minimal lookup latency for\ndeployment. Experimental results show that InstCache can achieve up to 51.34%\nhit rate on LMSys dataset, which corresponds to a 2x speedup, at a memory cost\nof only 4.5GB."
                },
                "authors": [
                    {
                        "name": "Longwei Zou"
                    },
                    {
                        "name": "Tingfeng Liu"
                    },
                    {
                        "name": "Kai Chen"
                    },
                    {
                        "name": "Jiangang Kong"
                    },
                    {
                        "name": "Yangdong Deng"
                    }
                ],
                "author_detail": {
                    "name": "Yangdong Deng"
                },
                "author": "Yangdong Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13820v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13820v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22649v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22649v2",
                "updated": "2024-11-21T03:34:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    3,
                    34,
                    44,
                    3,
                    326,
                    0
                ],
                "published": "2024-10-30T02:36:55Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    2,
                    36,
                    55,
                    2,
                    304,
                    0
                ],
                "title": "WaveRoRA: Wavelet Rotary Route Attention for Multivariate Time Series\n  Forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WaveRoRA: Wavelet Rotary Route Attention for Multivariate Time Series\n  Forecasting"
                },
                "summary": "In recent years, Transformer-based models (Transformers) have achieved\nsignificant success in multivariate time series forecasting (MTSF). However,\nprevious works focus on extracting features either from the time domain or the\nfrequency domain, which inadequately captures the trends and periodic\ncharacteristics. To address this issue, we propose a wavelet learning framework\nto model complex temporal dependencies of the time series data. The wavelet\ndomain integrates both time and frequency information, allowing for the\nanalysis of local characteristics of signals at different scales. Additionally,\nthe Softmax self-attention mechanism used by Transformers has quadratic\ncomplexity, which leads to excessive computational costs when capturing\nlong-term dependencies. Therefore, we propose a novel attention mechanism:\nRotary Route Attention (RoRA). Unlike Softmax attention, RoRA utilizes rotary\nposition embeddings to inject relative positional information to sequence\ntokens and introduces a small number of routing tokens $r$ to aggregate\ninformation from the $KV$ matrices and redistribute it to the $Q$ matrix,\noffering linear complexity. We further propose WaveRoRA, which leverages RoRA\nto capture inter-series dependencies in the wavelet domain. We conduct\nextensive experiments on eight real-world datasets. The results indicate that\nWaveRoRA outperforms existing state-of-the-art models while maintaining lower\ncomputational costs. Our code is available at\nhttps://github.com/Leopold2333/WaveRoRA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, Transformer-based models (Transformers) have achieved\nsignificant success in multivariate time series forecasting (MTSF). However,\nprevious works focus on extracting features either from the time domain or the\nfrequency domain, which inadequately captures the trends and periodic\ncharacteristics. To address this issue, we propose a wavelet learning framework\nto model complex temporal dependencies of the time series data. The wavelet\ndomain integrates both time and frequency information, allowing for the\nanalysis of local characteristics of signals at different scales. Additionally,\nthe Softmax self-attention mechanism used by Transformers has quadratic\ncomplexity, which leads to excessive computational costs when capturing\nlong-term dependencies. Therefore, we propose a novel attention mechanism:\nRotary Route Attention (RoRA). Unlike Softmax attention, RoRA utilizes rotary\nposition embeddings to inject relative positional information to sequence\ntokens and introduces a small number of routing tokens $r$ to aggregate\ninformation from the $KV$ matrices and redistribute it to the $Q$ matrix,\noffering linear complexity. We further propose WaveRoRA, which leverages RoRA\nto capture inter-series dependencies in the wavelet domain. We conduct\nextensive experiments on eight real-world datasets. The results indicate that\nWaveRoRA outperforms existing state-of-the-art models while maintaining lower\ncomputational costs. Our code is available at\nhttps://github.com/Leopold2333/WaveRoRA."
                },
                "authors": [
                    {
                        "name": "Aobo Liang"
                    },
                    {
                        "name": "Yan Sun"
                    },
                    {
                        "name": "Nadra Guizani"
                    }
                ],
                "author_detail": {
                    "name": "Nadra Guizani"
                },
                "author": "Nadra Guizani",
                "arxiv_comment": "Model architecture changed",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22649v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22649v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13786v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13786v1",
                "updated": "2024-11-21T02:15:52Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    2,
                    15,
                    52,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T02:15:52Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    2,
                    15,
                    52,
                    3,
                    326,
                    0
                ],
                "title": "Adaptable Embeddings Network (AEN)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptable Embeddings Network (AEN)"
                },
                "summary": "Modern day Language Models see extensive use in text classification, yet this\ncomes at significant computational cost. Compute-effective classification\nmodels are needed for low-resource environments, most notably on edge devices.\nWe introduce Adaptable Embeddings Networks (AEN), a novel dual-encoder\narchitecture using Kernel Density Estimation (KDE). This architecture allows\nfor runtime adaptation of classification criteria without retraining and is\nnon-autoregressive. Through thorough synthetic data experimentation, we\ndemonstrate our model outputs comparable and in certain cases superior results\nto that of autoregressive models an order of magnitude larger than AEN's size.\nThe architecture's ability to preprocess and cache condition embeddings makes\nit ideal for edge computing applications and real-time monitoring systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern day Language Models see extensive use in text classification, yet this\ncomes at significant computational cost. Compute-effective classification\nmodels are needed for low-resource environments, most notably on edge devices.\nWe introduce Adaptable Embeddings Networks (AEN), a novel dual-encoder\narchitecture using Kernel Density Estimation (KDE). This architecture allows\nfor runtime adaptation of classification criteria without retraining and is\nnon-autoregressive. Through thorough synthetic data experimentation, we\ndemonstrate our model outputs comparable and in certain cases superior results\nto that of autoregressive models an order of magnitude larger than AEN's size.\nThe architecture's ability to preprocess and cache condition embeddings makes\nit ideal for edge computing applications and real-time monitoring systems."
                },
                "authors": [
                    {
                        "name": "Stan Loosmore"
                    },
                    {
                        "name": "Alexander Titus"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Titus"
                },
                "author": "Alexander Titus",
                "arxiv_comment": "20 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13786v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13786v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13676v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13676v1",
                "updated": "2024-11-20T19:51:25Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    19,
                    51,
                    25,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T19:51:25Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    19,
                    51,
                    25,
                    2,
                    325,
                    0
                ],
                "title": "Hymba: A Hybrid-head Architecture for Small Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hymba: A Hybrid-head Architecture for Small Language Models"
                },
                "summary": "We propose Hymba, a family of small language models featuring a hybrid-head\nparallel architecture that integrates transformer attention mechanisms with\nstate space models (SSMs) for enhanced efficiency. Attention heads provide\nhigh-resolution recall, while SSM heads enable efficient context summarization.\nAdditionally, we introduce learnable meta tokens that are prepended to prompts,\nstoring critical information and alleviating the \"forced-to-attend\" burden\nassociated with attention mechanisms. This model is further optimized by\nincorporating cross-layer key-value (KV) sharing and partial sliding window\nattention, resulting in a compact cache size. During development, we conducted\na controlled study comparing various architectures under identical settings and\nobserved significant advantages of our proposed architecture. Notably, Hymba\nachieves state-of-the-art results for small LMs: Our Hymba-1.5B-Base model\nsurpasses all sub-2B public models in performance and even outperforms\nLlama-3.2-3B with 1.32% higher average accuracy, an 11.67x cache size\nreduction, and 3.49x throughput.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose Hymba, a family of small language models featuring a hybrid-head\nparallel architecture that integrates transformer attention mechanisms with\nstate space models (SSMs) for enhanced efficiency. Attention heads provide\nhigh-resolution recall, while SSM heads enable efficient context summarization.\nAdditionally, we introduce learnable meta tokens that are prepended to prompts,\nstoring critical information and alleviating the \"forced-to-attend\" burden\nassociated with attention mechanisms. This model is further optimized by\nincorporating cross-layer key-value (KV) sharing and partial sliding window\nattention, resulting in a compact cache size. During development, we conducted\na controlled study comparing various architectures under identical settings and\nobserved significant advantages of our proposed architecture. Notably, Hymba\nachieves state-of-the-art results for small LMs: Our Hymba-1.5B-Base model\nsurpasses all sub-2B public models in performance and even outperforms\nLlama-3.2-3B with 1.32% higher average accuracy, an 11.67x cache size\nreduction, and 3.49x throughput."
                },
                "authors": [
                    {
                        "name": "Xin Dong"
                    },
                    {
                        "name": "Yonggan Fu"
                    },
                    {
                        "name": "Shizhe Diao"
                    },
                    {
                        "name": "Wonmin Byeon"
                    },
                    {
                        "name": "Zijia Chen"
                    },
                    {
                        "name": "Ameya Sunil Mahabaleshwarkar"
                    },
                    {
                        "name": "Shih-Yang Liu"
                    },
                    {
                        "name": "Matthijs Van Keirsbilck"
                    },
                    {
                        "name": "Min-Hung Chen"
                    },
                    {
                        "name": "Yoshi Suhara"
                    },
                    {
                        "name": "Yingyan Lin"
                    },
                    {
                        "name": "Jan Kautz"
                    },
                    {
                        "name": "Pavlo Molchanov"
                    }
                ],
                "author_detail": {
                    "name": "Pavlo Molchanov"
                },
                "author": "Pavlo Molchanov",
                "arxiv_comment": "20 pages, models are available on huggingface",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13676v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13676v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17720v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17720v1",
                "updated": "2024-11-20T19:44:26Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    19,
                    44,
                    26,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T19:44:26Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    19,
                    44,
                    26,
                    2,
                    325,
                    0
                ],
                "title": "MAS-Attention: Memory-Aware Stream Processing for Attention Acceleration\n  on Resource-Constrained Edge Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MAS-Attention: Memory-Aware Stream Processing for Attention Acceleration\n  on Resource-Constrained Edge Devices"
                },
                "summary": "The advent of foundation models have revolutionized various fields, enabling\nunprecedented task accuracy and flexibility in computational linguistics,\ncomputer vision and other domains. Attention mechanism has become an essential\ncomponent of foundation models, due to their superb capability of capturing\ncorrelations in a sequence. However, attention results in quadratic complexity\nin memory and compute as the context length grows. Although many fusion-based\nexact attention acceleration algorithms have been developed for\ndatacenter-grade GPUs and accelerators leveraging multi-core parallelism and\ndata locality, yet it remains a significant challenge to accelerate attention\non resource-constrained edge neural accelerators with limited compute units and\nstringent on-chip caches. In this paper, we propose a scheme for exact\nattention inference acceleration on memory-constrained edge accelerators, by\nparallelizing the utilization of heterogeneous compute units, i.e., vector\nprocessing units and matrix processing units. Our method involves scheduling\nworkloads onto these different compute units in a multi-tiered tiling scheme to\nprocess tiled vector workloads and matrix workloads in attention as two\nstreams, respecting the workload dependencies. We search for tiling factors to\nmaximize the parallelization of both compute units while considering I/O\noverhead, and propose a proactive cache overwrite strategy to avoid undesirable\ncache spills in reality. Extensive results based on open-sourced simulation\nframeworks show up to 2.75x speedup and 54% reduction in energy consumption as\ncompared to the state-of-the-art attention fusion method (FLAT) in the edge\ncomputing scenario. Further experiments on a real-world edge neural processing\nunit demonstrate speedup of up to 1.76x for attention as compared to FLAT,\nwithout affecting model output accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advent of foundation models have revolutionized various fields, enabling\nunprecedented task accuracy and flexibility in computational linguistics,\ncomputer vision and other domains. Attention mechanism has become an essential\ncomponent of foundation models, due to their superb capability of capturing\ncorrelations in a sequence. However, attention results in quadratic complexity\nin memory and compute as the context length grows. Although many fusion-based\nexact attention acceleration algorithms have been developed for\ndatacenter-grade GPUs and accelerators leveraging multi-core parallelism and\ndata locality, yet it remains a significant challenge to accelerate attention\non resource-constrained edge neural accelerators with limited compute units and\nstringent on-chip caches. In this paper, we propose a scheme for exact\nattention inference acceleration on memory-constrained edge accelerators, by\nparallelizing the utilization of heterogeneous compute units, i.e., vector\nprocessing units and matrix processing units. Our method involves scheduling\nworkloads onto these different compute units in a multi-tiered tiling scheme to\nprocess tiled vector workloads and matrix workloads in attention as two\nstreams, respecting the workload dependencies. We search for tiling factors to\nmaximize the parallelization of both compute units while considering I/O\noverhead, and propose a proactive cache overwrite strategy to avoid undesirable\ncache spills in reality. Extensive results based on open-sourced simulation\nframeworks show up to 2.75x speedup and 54% reduction in energy consumption as\ncompared to the state-of-the-art attention fusion method (FLAT) in the edge\ncomputing scenario. Further experiments on a real-world edge neural processing\nunit demonstrate speedup of up to 1.76x for attention as compared to FLAT,\nwithout affecting model output accuracy."
                },
                "authors": [
                    {
                        "name": "Mohammadali Shakerdargah"
                    },
                    {
                        "name": "Shan Lu"
                    },
                    {
                        "name": "Chao Gao"
                    },
                    {
                        "name": "Di Niu"
                    }
                ],
                "author_detail": {
                    "name": "Di Niu"
                },
                "author": "Di Niu",
                "arxiv_comment": "10 pages, 6 figures, under review for MLSys 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17720v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17720v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.1.4; I.2.7; I.5.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13532v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13532v1",
                "updated": "2024-11-20T18:31:39Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    18,
                    31,
                    39,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T18:31:39Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    18,
                    31,
                    39,
                    2,
                    325,
                    0
                ],
                "title": "A Distributed-memory Tridiagonal Solver Based on a Specialised Data\n  Structure Optimised for CPU and GPU Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Distributed-memory Tridiagonal Solver Based on a Specialised Data\n  Structure Optimised for CPU and GPU Architectures"
                },
                "summary": "Various numerical methods used for solving partial differential equations\n(PDE) result in tridiagonal systems. Solving tridiagonal systems on\ndistributed-memory environments is not straightforward, and often requires\nsignificant amount of communication. In this article, we present a novel\ndistributed-memory tridiagonal solver algorithm, DistD2-TDS, based on a\nspecialised data structure. DistD2-TDS algorithm takes advantage of the\ndiagonal dominance in tridiagonal systems to reduce the communications in\ndistributed-memory environments. The underlying data structure plays a crucial\nrole for the performance of the algorithm. First, the data structure improves\ndata localities and makes it possible to minimise data movements via cache\nblocking and kernel fusion strategies. Second, data continuity enables a\ncontiguous data access pattern and results in efficient utilisation of the\navailable memory bandwidth. Finally, the data layout supports vectorisation on\nCPUs and thread level parallelisation on GPUs for improved performance. In\norder to demonstrate the robustness of the algorithm, we implemented and\nbenchmarked the algorithm on CPUs and GPUs. We investigated the single rank\nperformance and compared against existing algorithms. Furthermore, we analysed\nthe strong scaling of the implementation up to 384 NVIDIA H100 GPUs and up to\n8192 AMD EPYC 7742 CPUs. Finally, we demonstrated a practical use case of the\nalgorithm by using compact finite difference schemes to solve a 3D non-linear\nPDE. The results demonstrate that DistD2 algorithm can sustain around 66% of\nthe theoretical peak bandwidth at scale on CPU and GPU based supercomputers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Various numerical methods used for solving partial differential equations\n(PDE) result in tridiagonal systems. Solving tridiagonal systems on\ndistributed-memory environments is not straightforward, and often requires\nsignificant amount of communication. In this article, we present a novel\ndistributed-memory tridiagonal solver algorithm, DistD2-TDS, based on a\nspecialised data structure. DistD2-TDS algorithm takes advantage of the\ndiagonal dominance in tridiagonal systems to reduce the communications in\ndistributed-memory environments. The underlying data structure plays a crucial\nrole for the performance of the algorithm. First, the data structure improves\ndata localities and makes it possible to minimise data movements via cache\nblocking and kernel fusion strategies. Second, data continuity enables a\ncontiguous data access pattern and results in efficient utilisation of the\navailable memory bandwidth. Finally, the data layout supports vectorisation on\nCPUs and thread level parallelisation on GPUs for improved performance. In\norder to demonstrate the robustness of the algorithm, we implemented and\nbenchmarked the algorithm on CPUs and GPUs. We investigated the single rank\nperformance and compared against existing algorithms. Furthermore, we analysed\nthe strong scaling of the implementation up to 384 NVIDIA H100 GPUs and up to\n8192 AMD EPYC 7742 CPUs. Finally, we demonstrated a practical use case of the\nalgorithm by using compact finite difference schemes to solve a 3D non-linear\nPDE. The results demonstrate that DistD2 algorithm can sustain around 66% of\nthe theoretical peak bandwidth at scale on CPU and GPU based supercomputers."
                },
                "authors": [
                    {
                        "name": "Semih Akkurt"
                    },
                    {
                        "name": "SÃ©bastien Lemaire"
                    },
                    {
                        "name": "Paul Bartholomew"
                    },
                    {
                        "name": "Sylvain Laizet"
                    }
                ],
                "author_detail": {
                    "name": "Sylvain Laizet"
                },
                "author": "Sylvain Laizet",
                "arxiv_comment": "42 pages, 13 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13532v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13532v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13373v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13373v1",
                "updated": "2024-11-20T14:52:36Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    14,
                    52,
                    36,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T14:52:36Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    14,
                    52,
                    36,
                    2,
                    325,
                    0
                ],
                "title": "Upgrade of the Diagnostic Neutral Beam Injector for the RFX-mod2\n  experiment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Upgrade of the Diagnostic Neutral Beam Injector for the RFX-mod2\n  experiment"
                },
                "summary": "Diagnostic Neutral Beam Injectors (DNBI), through the combined use of Charge\nExchange Recombination Spectroscopy (CHERS) and Motional Stark effect\ndiagnostics (MSE), are a well-known tool to access important information about\nmagnetically confined plasmas, such as radial profiles of ion temperature, ion\nflow, impurity content and intensity and direction of the magnetic field. For\nthis purpose, a DNBI was installed and operated in the RFX-mod experiment,\nwhich was designed to confine plasma mainly through the Reversed Field Pinch\nconfiguration. The DNBI, designed and built by the Budker Institute of Plasma\nPhysics, was based on a source of positive hydrogen ions, accelerated to 50 keV\nand for an equivalent neutral beam current of about 5 A at the source. The beam\ncould be modulated and the maximum overall duration was 50 ms. With the upgrade\nof RFX-mod to the present RFX-mod2 machine, the DNBI is being renovated to\nsolve several plant faults and improve the overall reliability of the system.\nThe 50 kV power supply is being improved, as well as the power supplies in the\nhigh voltage deck and its insulation transformer. The control system,\noriginally based on CAMAC technology, was redesigned to be fully replaced. This\ncontribution reviews the technical criticalities emerged in the DNBI check-up\nand the new solutions adopted to make the DNBI operative and more reliable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diagnostic Neutral Beam Injectors (DNBI), through the combined use of Charge\nExchange Recombination Spectroscopy (CHERS) and Motional Stark effect\ndiagnostics (MSE), are a well-known tool to access important information about\nmagnetically confined plasmas, such as radial profiles of ion temperature, ion\nflow, impurity content and intensity and direction of the magnetic field. For\nthis purpose, a DNBI was installed and operated in the RFX-mod experiment,\nwhich was designed to confine plasma mainly through the Reversed Field Pinch\nconfiguration. The DNBI, designed and built by the Budker Institute of Plasma\nPhysics, was based on a source of positive hydrogen ions, accelerated to 50 keV\nand for an equivalent neutral beam current of about 5 A at the source. The beam\ncould be modulated and the maximum overall duration was 50 ms. With the upgrade\nof RFX-mod to the present RFX-mod2 machine, the DNBI is being renovated to\nsolve several plant faults and improve the overall reliability of the system.\nThe 50 kV power supply is being improved, as well as the power supplies in the\nhigh voltage deck and its insulation transformer. The control system,\noriginally based on CAMAC technology, was redesigned to be fully replaced. This\ncontribution reviews the technical criticalities emerged in the DNBI check-up\nand the new solutions adopted to make the DNBI operative and more reliable."
                },
                "authors": [
                    {
                        "name": "Marco Barbisan"
                    },
                    {
                        "name": "Marco Boldrin"
                    },
                    {
                        "name": "Luca Cinnirella"
                    },
                    {
                        "name": "Bruno Laterza"
                    },
                    {
                        "name": "Alberto Maistrello"
                    },
                    {
                        "name": "Lionello Marrelli"
                    },
                    {
                        "name": "Federico Molon"
                    },
                    {
                        "name": "Simone Peruzzo"
                    },
                    {
                        "name": "Cesare Taliercio"
                    },
                    {
                        "name": "Marco Valisa"
                    },
                    {
                        "name": "Enrico Zampiva"
                    }
                ],
                "author_detail": {
                    "name": "Enrico Zampiva"
                },
                "author": "Enrico Zampiva",
                "arxiv_comment": "6 pages (excl. highlights), 8 figures. Contribution to the 33rd\n  Symposium on Fusion Technology (SOFT), 22-27 September 2024. This is a\n  preprint for the \"Fusion Engineering and Design\" journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13373v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13373v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.18003v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.18003v4",
                "updated": "2024-11-20T02:04:10Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    2,
                    4,
                    10,
                    2,
                    325,
                    0
                ],
                "published": "2024-07-25T12:56:22Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    12,
                    56,
                    22,
                    3,
                    207,
                    0
                ],
                "title": "Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache\n  Consumption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache\n  Consumption"
                },
                "summary": "Large Language Models (LLMs), epitomized by ChatGPT's release in late 2022,\nhave revolutionized various industries with their advanced language\ncomprehension. However, their efficiency is challenged by the Transformer\narchitecture's struggle with handling long texts. KV Cache has emerged as a\npivotal solution to this issue, converting the time complexity of token\ngeneration from quadratic to linear, albeit with increased GPU memory overhead\nproportional to conversation length. With the development of the LLM community\nand academia, various KV Cache compression methods have been proposed. In this\nreview, we dissect the various properties of KV Cache and elaborate on various\nmethods currently used to optimize the KV Cache space usage of LLMs. These\nmethods span the pre-training phase, deployment phase, and inference phase, and\nwe summarize the commonalities and differences among these methods.\nAdditionally, we list some metrics for evaluating the long-text capabilities of\nlarge language models, from both efficiency and capability perspectives. Our\nreview thus sheds light on the evolving landscape of LLM optimization, offering\ninsights into future advancements in this dynamic field. Links to the papers\nmentioned in this review can be found in our Github Repo\nhttps://github.com/zcli-charlie/Awesome-KV-Cache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), epitomized by ChatGPT's release in late 2022,\nhave revolutionized various industries with their advanced language\ncomprehension. However, their efficiency is challenged by the Transformer\narchitecture's struggle with handling long texts. KV Cache has emerged as a\npivotal solution to this issue, converting the time complexity of token\ngeneration from quadratic to linear, albeit with increased GPU memory overhead\nproportional to conversation length. With the development of the LLM community\nand academia, various KV Cache compression methods have been proposed. In this\nreview, we dissect the various properties of KV Cache and elaborate on various\nmethods currently used to optimize the KV Cache space usage of LLMs. These\nmethods span the pre-training phase, deployment phase, and inference phase, and\nwe summarize the commonalities and differences among these methods.\nAdditionally, we list some metrics for evaluating the long-text capabilities of\nlarge language models, from both efficiency and capability perspectives. Our\nreview thus sheds light on the evolving landscape of LLM optimization, offering\ninsights into future advancements in this dynamic field. Links to the papers\nmentioned in this review can be found in our Github Repo\nhttps://github.com/zcli-charlie/Awesome-KV-Cache."
                },
                "authors": [
                    {
                        "name": "Luohe Shi"
                    },
                    {
                        "name": "Hongyi Zhang"
                    },
                    {
                        "name": "Yao Yao"
                    },
                    {
                        "name": "Zuchao Li"
                    },
                    {
                        "name": "Hai Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Hai Zhao"
                },
                "author": "Hai Zhao",
                "arxiv_comment": "Published on the First Conference on Language Modeling (COLM 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.18003v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.18003v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17918v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17918v3",
                "updated": "2024-11-19T18:24:03Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    18,
                    24,
                    3,
                    1,
                    324,
                    0
                ],
                "published": "2024-06-25T20:00:32Z",
                "published_parsed": [
                    2024,
                    6,
                    25,
                    20,
                    0,
                    32,
                    1,
                    177,
                    0
                ],
                "title": "GraphSnapShot: Graph Machine Learning Acceleration with Fast Storage and\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GraphSnapShot: Graph Machine Learning Acceleration with Fast Storage and\n  Retrieval"
                },
                "summary": "In our recent research, we have developed a framework called GraphSnapShot,\nwhich has been proven an useful tool for graph learning acceleration.\nGraphSnapShot is a framework for fast cache, storage, retrieval and computation\nfor graph learning. It can quickly store and update the local topology of graph\nstructure and allows us to track patterns in the structure of graph networks,\njust like take snapshots of the graphs. In experiments, GraphSnapShot shows\nefficiency, it can achieve up to 30% training acceleration and 73% memory\nreduction for lossless graph ML training compared to current baselines such as\ndgl.This technique is particular useful for large dynamic graph learning tasks\nsuch as social media analysis and recommendation systems to process complex\nrelationships between entities.\n  The code for GraphSnapShot is publicly available at\nhttps://github.com/NoakLiu/GraphSnapShot.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In our recent research, we have developed a framework called GraphSnapShot,\nwhich has been proven an useful tool for graph learning acceleration.\nGraphSnapShot is a framework for fast cache, storage, retrieval and computation\nfor graph learning. It can quickly store and update the local topology of graph\nstructure and allows us to track patterns in the structure of graph networks,\njust like take snapshots of the graphs. In experiments, GraphSnapShot shows\nefficiency, it can achieve up to 30% training acceleration and 73% memory\nreduction for lossless graph ML training compared to current baselines such as\ndgl.This technique is particular useful for large dynamic graph learning tasks\nsuch as social media analysis and recommendation systems to process complex\nrelationships between entities.\n  The code for GraphSnapShot is publicly available at\nhttps://github.com/NoakLiu/GraphSnapShot."
                },
                "authors": [
                    {
                        "name": "Dong Liu"
                    },
                    {
                        "name": "Roger Waleffe"
                    },
                    {
                        "name": "Meng Jiang"
                    },
                    {
                        "name": "Shivaram Venkataraman"
                    }
                ],
                "author_detail": {
                    "name": "Shivaram Venkataraman"
                },
                "author": "Shivaram Venkataraman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17918v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17918v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12430v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12430v1",
                "updated": "2024-11-19T11:40:56Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    11,
                    40,
                    56,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-19T11:40:56Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    11,
                    40,
                    56,
                    1,
                    324,
                    0
                ],
                "title": "An Eulerian approach to regularized JKO scheme with low-rank tensor\n  decompositions for Bayesian inversion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Eulerian approach to regularized JKO scheme with low-rank tensor\n  decompositions for Bayesian inversion"
                },
                "summary": "The possibility of using the Eulerian discretization for the problem of\nmodelling high-dimensional distributions and sampling, is studied. The problem\nis posed as a minimization problem over the space of probability measures with\nrespect to the Wasserstein distance and solved with entropy-regularized JKO\nscheme. Each proximal step can be formulated as a fixed-point equation and\nsolved with accelerated methods, such as Anderson's. The usage of low-rank\nTensor Train format allows to overcome the \\emph{curse of dimensionality}, i.e.\nthe exponential growth of degrees of freedom with dimension, inherent to\nEulerian approaches. The resulting method requires only pointwise computations\nof the unnormalized posterior and is, in particular, gradient-free. Fixed\nEulerian grid allows to employ a caching strategy, significally reducing the\nexpensive evaluations of the posterior. When the Eulerian model of the target\ndistribution is fitted, the passage back to the Lagrangian perspective can also\nbe made, allowing to approximately sample from it. We test our method both for\nsynthetic target distributions and particular Bayesian inverse problems and\nreport comparable or better performance than the baseline Metropolis-Hastings\nMCMC with same amount of resources. Finally, the fitted model can be modified\nto facilitate the solution of certain associated problems, which we demonstrate\nby fitting an importance distribution for a particular quantity of interest.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The possibility of using the Eulerian discretization for the problem of\nmodelling high-dimensional distributions and sampling, is studied. The problem\nis posed as a minimization problem over the space of probability measures with\nrespect to the Wasserstein distance and solved with entropy-regularized JKO\nscheme. Each proximal step can be formulated as a fixed-point equation and\nsolved with accelerated methods, such as Anderson's. The usage of low-rank\nTensor Train format allows to overcome the \\emph{curse of dimensionality}, i.e.\nthe exponential growth of degrees of freedom with dimension, inherent to\nEulerian approaches. The resulting method requires only pointwise computations\nof the unnormalized posterior and is, in particular, gradient-free. Fixed\nEulerian grid allows to employ a caching strategy, significally reducing the\nexpensive evaluations of the posterior. When the Eulerian model of the target\ndistribution is fitted, the passage back to the Lagrangian perspective can also\nbe made, allowing to approximately sample from it. We test our method both for\nsynthetic target distributions and particular Bayesian inverse problems and\nreport comparable or better performance than the baseline Metropolis-Hastings\nMCMC with same amount of resources. Finally, the fitted model can be modified\nto facilitate the solution of certain associated problems, which we demonstrate\nby fitting an importance distribution for a particular quantity of interest."
                },
                "authors": [
                    {
                        "name": "Vitalii Aksenov"
                    },
                    {
                        "name": "Martin Eigel"
                    }
                ],
                "author_detail": {
                    "name": "Martin Eigel"
                },
                "author": "Martin Eigel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12430v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12430v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.NA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "46E27, 49Q22, 62F15, 68W25",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12161v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12161v1",
                "updated": "2024-11-19T01:55:26Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    1,
                    55,
                    26,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-19T01:55:26Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    1,
                    55,
                    26,
                    1,
                    324,
                    0
                ],
                "title": "Adaptive Cache Management for Complex Storage Systems Using\n  CNN-LSTM-Based Spatiotemporal Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Cache Management for Complex Storage Systems Using\n  CNN-LSTM-Based Spatiotemporal Prediction"
                },
                "summary": "This paper proposes an intelligent cache management strategy based on\nCNN-LSTM to improve the performance and cache hit rate of storage systems.\nThrough comparative experiments with traditional algorithms (such as LRU and\nLFU) and other deep learning models (such as RNN, GRU-RNN and LSTM), the\nresults show that the CNN-LSTM model has significant advantages in cache demand\nprediction. The MSE and MAE values of this model are significantly reduced,\nproving its effectiveness under complex data access patterns. This study not\nonly verifies the potential of deep learning technology in storage system\noptimization, but also provides direction and reference for further optimizing\nand improving cache management strategies. This intelligent cache management\nstrategy performs well in complex storage environments. By combining the\nspatial feature extraction capabilities of convolutional neural networks and\nthe time series modeling capabilities of long short-term memory networks, the\nCNN-LSTM model can more accurately predict cache needs, thereby Dynamically\noptimize cache allocation to improve system response speed and resource\nutilization. This research provides theoretical support and practical reference\nfor cache optimization under large-scale data access modes, and is of great\nsignificance to improving the performance of future storage systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes an intelligent cache management strategy based on\nCNN-LSTM to improve the performance and cache hit rate of storage systems.\nThrough comparative experiments with traditional algorithms (such as LRU and\nLFU) and other deep learning models (such as RNN, GRU-RNN and LSTM), the\nresults show that the CNN-LSTM model has significant advantages in cache demand\nprediction. The MSE and MAE values of this model are significantly reduced,\nproving its effectiveness under complex data access patterns. This study not\nonly verifies the potential of deep learning technology in storage system\noptimization, but also provides direction and reference for further optimizing\nand improving cache management strategies. This intelligent cache management\nstrategy performs well in complex storage environments. By combining the\nspatial feature extraction capabilities of convolutional neural networks and\nthe time series modeling capabilities of long short-term memory networks, the\nCNN-LSTM model can more accurately predict cache needs, thereby Dynamically\noptimize cache allocation to improve system response speed and resource\nutilization. This research provides theoretical support and practical reference\nfor cache optimization under large-scale data access modes, and is of great\nsignificance to improving the performance of future storage systems."
                },
                "authors": [
                    {
                        "name": "Xiaoye Wang"
                    },
                    {
                        "name": "Xuan Li"
                    },
                    {
                        "name": "Linji Wang"
                    },
                    {
                        "name": "Tingyi Ruan"
                    },
                    {
                        "name": "Pochun Li"
                    }
                ],
                "author_detail": {
                    "name": "Pochun Li"
                },
                "author": "Pochun Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12161v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12161v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11843v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11843v1",
                "updated": "2024-11-18T18:59:15Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    18,
                    59,
                    15,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T18:59:15Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    18,
                    59,
                    15,
                    0,
                    323,
                    0
                ],
                "title": "Bi-Mamba: Towards Accurate 1-Bit State Space Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bi-Mamba: Towards Accurate 1-Bit State Space Models"
                },
                "summary": "The typical selective state-space model (SSM) of Mamba addresses several\nlimitations of Transformers, such as quadratic computational complexity with\nsequence length and significant inference-time memory requirements due to the\nkey-value cache. However, the growing size of Mamba models continues to pose\ntraining and deployment challenges and raises environmental concerns due to\nconsiderable energy consumption. In this work, we introduce Bi-Mamba, a\nscalable and powerful 1-bit Mamba architecture designed for more efficient\nlarge language models with multiple sizes across 780M, 1.3B, and 2.7B. Bi-Mamba\nmodels are trained from scratch on data volume as regular LLM pertaining using\nan autoregressive distillation loss. Extensive experimental results on language\nmodeling demonstrate that Bi-Mamba achieves performance comparable to its\nfull-precision counterparts (e.g., FP16 or BF16) and much better accuracy than\npost-training-binarization (PTB) Mamba baselines, while significantly reducing\nmemory footprint and energy consumption compared to the original Mamba model.\nOur study pioneers a new linear computational complexity LLM framework under\nlow-bit representation and facilitates the future design of specialized\nhardware tailored for efficient 1-bit Mamba-based LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The typical selective state-space model (SSM) of Mamba addresses several\nlimitations of Transformers, such as quadratic computational complexity with\nsequence length and significant inference-time memory requirements due to the\nkey-value cache. However, the growing size of Mamba models continues to pose\ntraining and deployment challenges and raises environmental concerns due to\nconsiderable energy consumption. In this work, we introduce Bi-Mamba, a\nscalable and powerful 1-bit Mamba architecture designed for more efficient\nlarge language models with multiple sizes across 780M, 1.3B, and 2.7B. Bi-Mamba\nmodels are trained from scratch on data volume as regular LLM pertaining using\nan autoregressive distillation loss. Extensive experimental results on language\nmodeling demonstrate that Bi-Mamba achieves performance comparable to its\nfull-precision counterparts (e.g., FP16 or BF16) and much better accuracy than\npost-training-binarization (PTB) Mamba baselines, while significantly reducing\nmemory footprint and energy consumption compared to the original Mamba model.\nOur study pioneers a new linear computational complexity LLM framework under\nlow-bit representation and facilitates the future design of specialized\nhardware tailored for efficient 1-bit Mamba-based LLMs."
                },
                "authors": [
                    {
                        "name": "Shengkun Tang"
                    },
                    {
                        "name": "Liqun Ma"
                    },
                    {
                        "name": "Haonan Li"
                    },
                    {
                        "name": "Mingjie Sun"
                    },
                    {
                        "name": "Zhiqiang Shen"
                    }
                ],
                "author_detail": {
                    "name": "Zhiqiang Shen"
                },
                "author": "Zhiqiang Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11843v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11843v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11739v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11739v1",
                "updated": "2024-11-18T17:08:35Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    17,
                    8,
                    35,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T17:08:35Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    17,
                    8,
                    35,
                    0,
                    323,
                    0
                ],
                "title": "QARM: Quantitative Alignment Multi-Modal Recommendation at Kuaishou",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QARM: Quantitative Alignment Multi-Modal Recommendation at Kuaishou"
                },
                "summary": "In recent years, with the significant evolution of multi-modal large models,\nmany recommender researchers realized the potential of multi-modal information\nfor user interest modeling. In industry, a wide-used modeling architecture is a\ncascading paradigm: (1) first pre-training a multi-modal model to provide\nomnipotent representations for downstream services; (2) The downstream\nrecommendation model takes the multi-modal representation as additional input\nto fit real user-item behaviours. Although such paradigm achieves remarkable\nimprovements, however, there still exist two problems that limit model\nperformance: (1) Representation Unmatching: The pre-trained multi-modal model\nis always supervised by the classic NLP/CV tasks, while the recommendation\nmodels are supervised by real user-item interaction. As a result, the two\nfundamentally different tasks' goals were relatively separate, and there was a\nlack of consistent objective on their representations; (2) Representation\nUnlearning: The generated multi-modal representations are always stored in\ncache store and serve as extra fixed input of recommendation model, thus could\nnot be updated by recommendation model gradient, further unfriendly for\ndownstream training. Inspired by the two difficulties challenges in downstream\ntasks usage, we introduce a quantitative multi-modal framework to customize the\nspecialized and trainable multi-modal information for different downstream\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, with the significant evolution of multi-modal large models,\nmany recommender researchers realized the potential of multi-modal information\nfor user interest modeling. In industry, a wide-used modeling architecture is a\ncascading paradigm: (1) first pre-training a multi-modal model to provide\nomnipotent representations for downstream services; (2) The downstream\nrecommendation model takes the multi-modal representation as additional input\nto fit real user-item behaviours. Although such paradigm achieves remarkable\nimprovements, however, there still exist two problems that limit model\nperformance: (1) Representation Unmatching: The pre-trained multi-modal model\nis always supervised by the classic NLP/CV tasks, while the recommendation\nmodels are supervised by real user-item interaction. As a result, the two\nfundamentally different tasks' goals were relatively separate, and there was a\nlack of consistent objective on their representations; (2) Representation\nUnlearning: The generated multi-modal representations are always stored in\ncache store and serve as extra fixed input of recommendation model, thus could\nnot be updated by recommendation model gradient, further unfriendly for\ndownstream training. Inspired by the two difficulties challenges in downstream\ntasks usage, we introduce a quantitative multi-modal framework to customize the\nspecialized and trainable multi-modal information for different downstream\nmodels."
                },
                "authors": [
                    {
                        "name": "Xinchen Luo"
                    },
                    {
                        "name": "Jiangxia Cao"
                    },
                    {
                        "name": "Tianyu Sun"
                    },
                    {
                        "name": "Jinkai Yu"
                    },
                    {
                        "name": "Rui Huang"
                    },
                    {
                        "name": "Wei Yuan"
                    },
                    {
                        "name": "Hezheng Lin"
                    },
                    {
                        "name": "Yichen Zheng"
                    },
                    {
                        "name": "Shiyao Wang"
                    },
                    {
                        "name": "Qigen Hu"
                    },
                    {
                        "name": "Changqing Qiu"
                    },
                    {
                        "name": "Jiaqi Zhang"
                    },
                    {
                        "name": "Xu Zhang"
                    },
                    {
                        "name": "Zhiheng Yan"
                    },
                    {
                        "name": "Jingming Zhang"
                    },
                    {
                        "name": "Simin Zhang"
                    },
                    {
                        "name": "Mingxing Wen"
                    },
                    {
                        "name": "Zhaojie Liu"
                    },
                    {
                        "name": "Kun Gai"
                    },
                    {
                        "name": "Guorui Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Guorui Zhou"
                },
                "author": "Guorui Zhou",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11739v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11739v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "N/A",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11300v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11300v1",
                "updated": "2024-11-18T05:50:58Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    5,
                    50,
                    58,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T05:50:58Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    5,
                    50,
                    58,
                    0,
                    323,
                    0
                ],
                "title": "Accelerating spherical K-means clustering for large-scale sparse\n  document data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating spherical K-means clustering for large-scale sparse\n  document data"
                },
                "summary": "This paper presents an accelerated spherical K-means clustering algorithm for\nlarge-scale and high-dimensional sparse document data sets. We design an\nalgorithm working in an architecture-friendly manner (AFM), which is a\nprocedure of suppressing performance-degradation factors such as the numbers of\ninstructions, branch mispredictions, and cache misses in CPUs of a modern\ncomputer system. For the AFM operation, we leverage unique universal\ncharacteristics (UCs) of a data-object and a cluster's mean set, which are\nskewed distributions on data relationships such as Zipf's law and a\nfeature-value concentration phenomenon. The UCs indicate that the most part of\nthe number of multiplications for similarity calculations is executed regarding\nterms with high document frequencies (df) and the most part of a similarity\nbetween an object- and a mean-feature vector is obtained by the multiplications\nregarding a few high mean-feature values. Our proposed algorithm applies an\ninverted-index data structure to a mean set, extracts the specific region with\nhigh-df terms and high mean-feature values in the mean-inverted index by newly\nintroduced two structural parameters, and exploits the index divided into three\nparts for efficient pruning. The algorithm determines the two structural\nparameters by minimizing the approximate number of multiplications related to\nthat of instructions, reduces the branch mispredictions by sharing the index\nstructure including the two parameters with all the objects, and suppressing\nthe cache misses by keeping in the caches the frequently used data in the\nforegoing specific region, resulting in working in the AFM. We experimentally\ndemonstrate that our algorithm efficiently achieves superior speed performance\nin large-scale documents compared with algorithms using the state-of-the-art\ntechniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents an accelerated spherical K-means clustering algorithm for\nlarge-scale and high-dimensional sparse document data sets. We design an\nalgorithm working in an architecture-friendly manner (AFM), which is a\nprocedure of suppressing performance-degradation factors such as the numbers of\ninstructions, branch mispredictions, and cache misses in CPUs of a modern\ncomputer system. For the AFM operation, we leverage unique universal\ncharacteristics (UCs) of a data-object and a cluster's mean set, which are\nskewed distributions on data relationships such as Zipf's law and a\nfeature-value concentration phenomenon. The UCs indicate that the most part of\nthe number of multiplications for similarity calculations is executed regarding\nterms with high document frequencies (df) and the most part of a similarity\nbetween an object- and a mean-feature vector is obtained by the multiplications\nregarding a few high mean-feature values. Our proposed algorithm applies an\ninverted-index data structure to a mean set, extracts the specific region with\nhigh-df terms and high mean-feature values in the mean-inverted index by newly\nintroduced two structural parameters, and exploits the index divided into three\nparts for efficient pruning. The algorithm determines the two structural\nparameters by minimizing the approximate number of multiplications related to\nthat of instructions, reduces the branch mispredictions by sharing the index\nstructure including the two parameters with all the objects, and suppressing\nthe cache misses by keeping in the caches the frequently used data in the\nforegoing specific region, resulting in working in the AFM. We experimentally\ndemonstrate that our algorithm efficiently achieves superior speed performance\nin large-scale documents compared with algorithms using the state-of-the-art\ntechniques."
                },
                "authors": [
                    {
                        "name": "Kazuo Aoyama"
                    },
                    {
                        "name": "Kazumi Saito"
                    }
                ],
                "author_detail": {
                    "name": "Kazumi Saito"
                },
                "author": "Kazumi Saito",
                "arxiv_comment": "28 pages, 23 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11300v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11300v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13588v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13588v1",
                "updated": "2024-11-18T02:49:23Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    2,
                    49,
                    23,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T02:49:23Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    2,
                    49,
                    23,
                    0,
                    323,
                    0
                ],
                "title": "Unveiling Redundancy in Diffusion Transformers (DiTs): A Systematic\n  Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unveiling Redundancy in Diffusion Transformers (DiTs): A Systematic\n  Study"
                },
                "summary": "The increased model capacity of Diffusion Transformers (DiTs) and the demand\nfor generating higher resolutions of images and videos have led to a\nsignificant rise in inference latency, impacting real-time performance\nadversely. While prior research has highlighted the presence of high similarity\nin activation values between adjacent diffusion steps (referred to as\nredundancy) and proposed various caching mechanisms to mitigate computational\noverhead, the exploration of redundancy in existing literature remains limited,\nwith findings often not generalizable across different DiT models. This study\naims to address this gap by conducting a comprehensive investigation into\nredundancy across a broad spectrum of mainstream DiT models. Our experimental\nanalysis reveals substantial variations in the distribution of redundancy\nacross diffusion steps among different DiT models. Interestingly, within a\nsingle model, the redundancy distribution remains stable regardless of\nvariations in input prompts, step counts, or scheduling strategies. Given the\nlack of a consistent pattern across diverse models, caching strategies designed\nfor a specific group of models may not easily transfer to others. To overcome\nthis challenge, we introduce a tool for analyzing the redundancy of individual\nmodels, enabling subsequent research to develop tailored caching strategies for\nspecific model architectures. The project is publicly available at\nhttps://github.com/xdit-project/DiTCacheAnalysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increased model capacity of Diffusion Transformers (DiTs) and the demand\nfor generating higher resolutions of images and videos have led to a\nsignificant rise in inference latency, impacting real-time performance\nadversely. While prior research has highlighted the presence of high similarity\nin activation values between adjacent diffusion steps (referred to as\nredundancy) and proposed various caching mechanisms to mitigate computational\noverhead, the exploration of redundancy in existing literature remains limited,\nwith findings often not generalizable across different DiT models. This study\naims to address this gap by conducting a comprehensive investigation into\nredundancy across a broad spectrum of mainstream DiT models. Our experimental\nanalysis reveals substantial variations in the distribution of redundancy\nacross diffusion steps among different DiT models. Interestingly, within a\nsingle model, the redundancy distribution remains stable regardless of\nvariations in input prompts, step counts, or scheduling strategies. Given the\nlack of a consistent pattern across diverse models, caching strategies designed\nfor a specific group of models may not easily transfer to others. To overcome\nthis challenge, we introduce a tool for analyzing the redundancy of individual\nmodels, enabling subsequent research to develop tailored caching strategies for\nspecific model architectures. The project is publicly available at\nhttps://github.com/xdit-project/DiTCacheAnalysis."
                },
                "authors": [
                    {
                        "name": "Xibo Sun"
                    },
                    {
                        "name": "Jiarui Fang"
                    },
                    {
                        "name": "Aoyu Li"
                    },
                    {
                        "name": "Jinzhe Pan"
                    }
                ],
                "author_detail": {
                    "name": "Jinzhe Pan"
                },
                "author": "Jinzhe Pan",
                "arxiv_comment": "9 pages including reference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13588v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13588v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06392v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06392v2",
                "updated": "2024-11-18T02:10:28Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    2,
                    10,
                    28,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-10T08:31:18Z",
                "published_parsed": [
                    2024,
                    11,
                    10,
                    8,
                    31,
                    18,
                    6,
                    315,
                    0
                ],
                "title": "LSMGraph: A High-Performance Dynamic Graph Storage System with\n  Multi-Level CSR",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LSMGraph: A High-Performance Dynamic Graph Storage System with\n  Multi-Level CSR"
                },
                "summary": "The growing volume of graph data may exhaust the main memory. It is crucial\nto design a disk-based graph storage system to ingest updates and analyze\ngraphs efficiently. However, existing dynamic graph storage systems suffer from\nread or write amplification and face the challenge of optimizing both read and\nwrite performance simultaneously. To address this challenge, we propose\nLSMGraph, a novel dynamic graph storage system that combines the write-friendly\nLSM-tree and the read-friendly CSR. It leverages the multi-level structure of\nLSM-trees to optimize write performance while utilizing the compact CSR\nstructures embedded in the LSM-trees to boost read performance. LSMGraph uses a\nnew memory structure, MemGraph, to efficiently cache graph updates and uses a\nmulti-level index to speed up reads within the multi-level structure.\nFurthermore, LSMGraph incorporates a vertex-grained version control mechanism\nto mitigate the impact of LSM-tree compaction on read performance and ensure\nthe correctness of concurrent read and write operations. Our evaluation shows\nthat LSMGraph significantly outperforms state-of-the-art (graph) storage\nsystems on both graph update and graph analytical workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing volume of graph data may exhaust the main memory. It is crucial\nto design a disk-based graph storage system to ingest updates and analyze\ngraphs efficiently. However, existing dynamic graph storage systems suffer from\nread or write amplification and face the challenge of optimizing both read and\nwrite performance simultaneously. To address this challenge, we propose\nLSMGraph, a novel dynamic graph storage system that combines the write-friendly\nLSM-tree and the read-friendly CSR. It leverages the multi-level structure of\nLSM-trees to optimize write performance while utilizing the compact CSR\nstructures embedded in the LSM-trees to boost read performance. LSMGraph uses a\nnew memory structure, MemGraph, to efficiently cache graph updates and uses a\nmulti-level index to speed up reads within the multi-level structure.\nFurthermore, LSMGraph incorporates a vertex-grained version control mechanism\nto mitigate the impact of LSM-tree compaction on read performance and ensure\nthe correctness of concurrent read and write operations. Our evaluation shows\nthat LSMGraph significantly outperforms state-of-the-art (graph) storage\nsystems on both graph update and graph analytical workloads."
                },
                "authors": [
                    {
                        "name": "Song Yu"
                    },
                    {
                        "name": "Shufeng Gong"
                    },
                    {
                        "name": "Qian Tao"
                    },
                    {
                        "name": "Sijie Shen"
                    },
                    {
                        "name": "Yanfeng Zhang"
                    },
                    {
                        "name": "Wenyuan Yu"
                    },
                    {
                        "name": "Pengxi Liu"
                    },
                    {
                        "name": "Zhixin Zhang"
                    },
                    {
                        "name": "Hongfu Li"
                    },
                    {
                        "name": "Xiaojian Luo"
                    },
                    {
                        "name": "Ge Yu"
                    },
                    {
                        "name": "Jingren Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jingren Zhou"
                },
                "author": "Jingren Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06392v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06392v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11091v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11091v1",
                "updated": "2024-11-17T14:47:15Z",
                "updated_parsed": [
                    2024,
                    11,
                    17,
                    14,
                    47,
                    15,
                    6,
                    322,
                    0
                ],
                "published": "2024-11-17T14:47:15Z",
                "published_parsed": [
                    2024,
                    11,
                    17,
                    14,
                    47,
                    15,
                    6,
                    322,
                    0
                ],
                "title": "KV-Tandem -- a Modular Approach to Building High-Speed LSM Storage\n  Engines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV-Tandem -- a Modular Approach to Building High-Speed LSM Storage\n  Engines"
                },
                "summary": "We present~\\emph{KV-Tandem}, a modular architecture for building LSM-based\nstorage engines on top of simple, non-ordered persistent key-value stores\n(KVSs). KV-Tandem enables advanced functionalities such as range queries and\nsnapshot reads, while maintaining the native KVS performance for random reads\nand writes. Its modular design offers better performance trade-offs compared to\nprevious KV-separation solutions, which struggle to decompose the monolithic\nLSM structure. Central to KV-Tandem is~\\emph{LSM bypass} -- a novel algorithm\nthat offers a fast path to basic operations while ensuring the correctness of\nadvanced APIs.\n  We implement KV-Tandem in \\emph{XDP-Rocks}, a RocksDB-compatible storage\nengine that leverages the XDP KVS and incorporates practical design\noptimizations for real-world deployment. Through extensive microbenchmark and\nsystem-level comparisons, we demonstrate that XDP-Rocks achieves 3x to 4x\nperformance improvements over RocksDB across various workloads. XDP-Rocks is\nalready deployed in production, delivering significant operator cost savings\nconsistent with these performance gains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present~\\emph{KV-Tandem}, a modular architecture for building LSM-based\nstorage engines on top of simple, non-ordered persistent key-value stores\n(KVSs). KV-Tandem enables advanced functionalities such as range queries and\nsnapshot reads, while maintaining the native KVS performance for random reads\nand writes. Its modular design offers better performance trade-offs compared to\nprevious KV-separation solutions, which struggle to decompose the monolithic\nLSM structure. Central to KV-Tandem is~\\emph{LSM bypass} -- a novel algorithm\nthat offers a fast path to basic operations while ensuring the correctness of\nadvanced APIs.\n  We implement KV-Tandem in \\emph{XDP-Rocks}, a RocksDB-compatible storage\nengine that leverages the XDP KVS and incorporates practical design\noptimizations for real-world deployment. Through extensive microbenchmark and\nsystem-level comparisons, we demonstrate that XDP-Rocks achieves 3x to 4x\nperformance improvements over RocksDB across various workloads. XDP-Rocks is\nalready deployed in production, delivering significant operator cost savings\nconsistent with these performance gains."
                },
                "authors": [
                    {
                        "name": "Edward Bortnikov"
                    },
                    {
                        "name": "Michael Azran"
                    },
                    {
                        "name": "Asa Bornstein"
                    },
                    {
                        "name": "Shmuel Dashevsky"
                    },
                    {
                        "name": "Dennis Huang"
                    },
                    {
                        "name": "Omer Kepten"
                    },
                    {
                        "name": "Michael Pan"
                    },
                    {
                        "name": "Gali Sheffi"
                    },
                    {
                        "name": "Moshe Twitto"
                    },
                    {
                        "name": "Tamar Weiss Orzech"
                    },
                    {
                        "name": "Idit Keidar"
                    },
                    {
                        "name": "Guy Gueta"
                    },
                    {
                        "name": "Roey Maor"
                    },
                    {
                        "name": "Niv Dayan"
                    }
                ],
                "author_detail": {
                    "name": "Niv Dayan"
                },
                "author": "Niv Dayan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11091v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11091v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07635v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07635v3",
                "updated": "2024-11-17T12:56:16Z",
                "updated_parsed": [
                    2024,
                    11,
                    17,
                    12,
                    56,
                    16,
                    6,
                    322,
                    0
                ],
                "published": "2024-11-12T08:30:59Z",
                "published_parsed": [
                    2024,
                    11,
                    12,
                    8,
                    30,
                    59,
                    1,
                    317,
                    0
                ],
                "title": "Breaking the Low-Rank Dilemma of Linear Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breaking the Low-Rank Dilemma of Linear Attention"
                },
                "summary": "The Softmax attention mechanism in Transformer models is notoriously\ncomputationally expensive, particularly due to its quadratic complexity, posing\nsignificant challenges in vision applications. In contrast, linear attention\nprovides a far more efficient solution by reducing the complexity to linear\nlevels. However, compared to Softmax attention, linear attention often\nexperiences significant performance degradation. Our experiments indicate that\nthis performance drop is due to the low-rank nature of linear attention's\nfeature map, which hinders its ability to adequately model complex spatial\ninformation. In this paper, to break the low-rank dilemma of linear attention,\nwe conduct rank analysis from two perspectives: the KV buffer and the output\nfeatures. Consequently, we introduce Rank-Augmented Linear Attention (RALA),\nwhich rivals the performance of Softmax attention while maintaining linear\ncomplexity and high efficiency. Based on RALA, we construct the Rank-Augmented\nVision Linear Transformer (RAVLT). Extensive experiments demonstrate that RAVLT\nachieves excellent performance across various vision tasks. Specifically,\nwithout using any additional labels, data, or supervision during training,\nRAVLT achieves an 84.4% Top-1 accuracy on ImageNet-1k with only 26M parameters\nand 4.6G FLOPs. This result significantly surpasses previous linear attention\nmechanisms, fully illustrating the potential of RALA. Code will be available at\nhttps://github.com/qhfan/RALA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Softmax attention mechanism in Transformer models is notoriously\ncomputationally expensive, particularly due to its quadratic complexity, posing\nsignificant challenges in vision applications. In contrast, linear attention\nprovides a far more efficient solution by reducing the complexity to linear\nlevels. However, compared to Softmax attention, linear attention often\nexperiences significant performance degradation. Our experiments indicate that\nthis performance drop is due to the low-rank nature of linear attention's\nfeature map, which hinders its ability to adequately model complex spatial\ninformation. In this paper, to break the low-rank dilemma of linear attention,\nwe conduct rank analysis from two perspectives: the KV buffer and the output\nfeatures. Consequently, we introduce Rank-Augmented Linear Attention (RALA),\nwhich rivals the performance of Softmax attention while maintaining linear\ncomplexity and high efficiency. Based on RALA, we construct the Rank-Augmented\nVision Linear Transformer (RAVLT). Extensive experiments demonstrate that RAVLT\nachieves excellent performance across various vision tasks. Specifically,\nwithout using any additional labels, data, or supervision during training,\nRAVLT achieves an 84.4% Top-1 accuracy on ImageNet-1k with only 26M parameters\nand 4.6G FLOPs. This result significantly surpasses previous linear attention\nmechanisms, fully illustrating the potential of RALA. Code will be available at\nhttps://github.com/qhfan/RALA."
                },
                "authors": [
                    {
                        "name": "Qihang Fan"
                    },
                    {
                        "name": "Huaibo Huang"
                    },
                    {
                        "name": "Ran He"
                    }
                ],
                "author_detail": {
                    "name": "Ran He"
                },
                "author": "Ran He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07635v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07635v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10883v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10883v1",
                "updated": "2024-11-16T20:40:08Z",
                "updated_parsed": [
                    2024,
                    11,
                    16,
                    20,
                    40,
                    8,
                    5,
                    321,
                    0
                ],
                "published": "2024-11-16T20:40:08Z",
                "published_parsed": [
                    2024,
                    11,
                    16,
                    20,
                    40,
                    8,
                    5,
                    321,
                    0
                ],
                "title": "I Know What You Sync: Covert and Side Channel Attacks on File Systems\n  via syncfs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "I Know What You Sync: Covert and Side Channel Attacks on File Systems\n  via syncfs"
                },
                "summary": "Operating Systems enforce logical isolation using abstractions such as\nprocesses, containers, and isolation technologies to protect a system from\nmalicious or buggy code. In this paper, we show new types of side channels\nthrough the file system that break this logical isolation. The file system\nplays a critical role in the operating system, managing all I/O activities\nbetween the application layer and the physical storage device. We observe that\nthe file system implementation is shared, leading to timing leakage when using\ncommon I/O system calls. Specifically, we found that modern operating systems\ntake advantage of any flush operation (which saves cached blocks in memory to\nthe SSD or disk) to flush all of the I/O buffers, even those used by other\nisolation domains. Thus, by measuring the delay of syncfs, the attacker can\ninfer the I/O behavior of victim programs. We then demonstrate a syncfs covert\nchannel attack on multiple file systems, including both Linux native file\nsystems and the Windows file system, achieving a maximum bandwidth of 5 Kbps\nwith an error rate of 0.15% on Linux and 7.6 Kbps with an error rate of 1.9% on\nWindows. In addition, we construct three side-channel attacks targeting both\nLinux and Android devices. On Linux devices, we implement a website\nfingerprinting attack and a video fingerprinting attack by tracking the write\npatterns of temporary buffering files. On Android devices, we design an\napplication fingerprinting attack that leaks application write patterns during\nboot-up. The attacks achieve over 90% F1 score, precision, and recall. Finally,\nwe demonstrate that these attacks can be exploited across containers\nimplementing a container detection technique and a cross-container covert\nchannel attack.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Operating Systems enforce logical isolation using abstractions such as\nprocesses, containers, and isolation technologies to protect a system from\nmalicious or buggy code. In this paper, we show new types of side channels\nthrough the file system that break this logical isolation. The file system\nplays a critical role in the operating system, managing all I/O activities\nbetween the application layer and the physical storage device. We observe that\nthe file system implementation is shared, leading to timing leakage when using\ncommon I/O system calls. Specifically, we found that modern operating systems\ntake advantage of any flush operation (which saves cached blocks in memory to\nthe SSD or disk) to flush all of the I/O buffers, even those used by other\nisolation domains. Thus, by measuring the delay of syncfs, the attacker can\ninfer the I/O behavior of victim programs. We then demonstrate a syncfs covert\nchannel attack on multiple file systems, including both Linux native file\nsystems and the Windows file system, achieving a maximum bandwidth of 5 Kbps\nwith an error rate of 0.15% on Linux and 7.6 Kbps with an error rate of 1.9% on\nWindows. In addition, we construct three side-channel attacks targeting both\nLinux and Android devices. On Linux devices, we implement a website\nfingerprinting attack and a video fingerprinting attack by tracking the write\npatterns of temporary buffering files. On Android devices, we design an\napplication fingerprinting attack that leaks application write patterns during\nboot-up. The attacks achieve over 90% F1 score, precision, and recall. Finally,\nwe demonstrate that these attacks can be exploited across containers\nimplementing a container detection technique and a cross-container covert\nchannel attack."
                },
                "authors": [
                    {
                        "name": "Cheng Gu"
                    },
                    {
                        "name": "Yicheng Zhang"
                    },
                    {
                        "name": "Nael Abu-Ghazaleh"
                    }
                ],
                "author_detail": {
                    "name": "Nael Abu-Ghazaleh"
                },
                "author": "Nael Abu-Ghazaleh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10883v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10883v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.13112v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.13112v3",
                "updated": "2024-11-16T20:39:46Z",
                "updated_parsed": [
                    2024,
                    11,
                    16,
                    20,
                    39,
                    46,
                    5,
                    321,
                    0
                ],
                "published": "2024-03-19T19:27:23Z",
                "published_parsed": [
                    2024,
                    3,
                    19,
                    19,
                    27,
                    23,
                    1,
                    79,
                    0
                ],
                "title": "Efficient Encoder-Decoder Transformer Decoding for Decomposable Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Encoder-Decoder Transformer Decoding for Decomposable Tasks"
                },
                "summary": "Transformer-based NLP models are powerful but have high computational costs\nthat limit deployment. Finetuned encoder-decoder models are popular in\nspecialized domains and can outperform larger more generalized decoder-only\nmodels, such as GPT-4. We introduce a new configuration for encoder-decoder\nmodels that improves efficiency on structured output and decomposable tasks\nwhere multiple outputs are required for a single shared input. Our method,\nprompt-in-decoder (PiD), encodes the input once and decodes the output in\nparallel, boosting both training and inference efficiency by avoiding duplicate\ninput encoding and increasing the operational intensity (ratio of numbers of\narithmetic operation to memory access) of decoding process by sharing the input\nkey-value cache. We achieve computation reduction that roughly scales with the\nnumber of subtasks, gaining up to 4.6x speed-up over state-of-the-art models\nfor dialogue state tracking, summarization, and question-answering tasks, with\ncomparable or better performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based NLP models are powerful but have high computational costs\nthat limit deployment. Finetuned encoder-decoder models are popular in\nspecialized domains and can outperform larger more generalized decoder-only\nmodels, such as GPT-4. We introduce a new configuration for encoder-decoder\nmodels that improves efficiency on structured output and decomposable tasks\nwhere multiple outputs are required for a single shared input. Our method,\nprompt-in-decoder (PiD), encodes the input once and decodes the output in\nparallel, boosting both training and inference efficiency by avoiding duplicate\ninput encoding and increasing the operational intensity (ratio of numbers of\narithmetic operation to memory access) of decoding process by sharing the input\nkey-value cache. We achieve computation reduction that roughly scales with the\nnumber of subtasks, gaining up to 4.6x speed-up over state-of-the-art models\nfor dialogue state tracking, summarization, and question-answering tasks, with\ncomparable or better performance."
                },
                "authors": [
                    {
                        "name": "Bo-Ru Lu"
                    },
                    {
                        "name": "Nikita Haduong"
                    },
                    {
                        "name": "Chien-Yu Lin"
                    },
                    {
                        "name": "Hao Cheng"
                    },
                    {
                        "name": "Noah A. Smith"
                    },
                    {
                        "name": "Mari Ostendorf"
                    }
                ],
                "author_detail": {
                    "name": "Mari Ostendorf"
                },
                "author": "Mari Ostendorf",
                "arxiv_comment": "18 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.13112v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.13112v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10803v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10803v1",
                "updated": "2024-11-16T13:45:33Z",
                "updated_parsed": [
                    2024,
                    11,
                    16,
                    13,
                    45,
                    33,
                    5,
                    321,
                    0
                ],
                "published": "2024-11-16T13:45:33Z",
                "published_parsed": [
                    2024,
                    11,
                    16,
                    13,
                    45,
                    33,
                    5,
                    321,
                    0
                ],
                "title": "Multi-Stage Vision Token Dropping: Towards Efficient Multimodal Large\n  Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Stage Vision Token Dropping: Towards Efficient Multimodal Large\n  Language Model"
                },
                "summary": "The vision tokens in multimodal large language models usually exhibit\nsignificant spatial and temporal redundancy and take up most of the input\ntokens, which harms their inference efficiency. To solve this problem, some\nrecent works were introduced to drop the unimportant tokens during inference\nwhere the importance of each token is decided only by the information in either\nthe vision encoding stage or the prefilling stage. In this paper, we propose\nMulti-stage Token Dropping (MustDrop) to measure the importance of each token\nfrom the whole lifecycle, including the vision encoding stage, prefilling\nstage, and decoding stage. Concretely, in the visual encoding stage, MustDrop\nmerges spatially adjacent tokens with high similarity, and establishes a key\ntoken set to retain the most vision-critical tokens, preventing them from being\ndiscarded in later stages. In the prefilling stage, MustDrop further compresses\nvision tokens by the guidance of text semantics, with a dual-attention\nfiltering strategy. In the decoding stage, an output-aware cache policy is\nproposed to further reduce the size of the KV cache. By leveraging tailored\nstrategies in the multi-stage process, MustDrop can more precisely recognize\nthe important and redundant tokens, thus achieving an optimal balance between\nperformance and efficiency. For instance, MustDrop reduces about 88.5\\% FLOPs\non LLaVA with a compression ratio of 92.2\\% while maintaining comparable\naccuracy. Our codes are available at\n\\url{https://github.com/liuting20/MustDrop}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The vision tokens in multimodal large language models usually exhibit\nsignificant spatial and temporal redundancy and take up most of the input\ntokens, which harms their inference efficiency. To solve this problem, some\nrecent works were introduced to drop the unimportant tokens during inference\nwhere the importance of each token is decided only by the information in either\nthe vision encoding stage or the prefilling stage. In this paper, we propose\nMulti-stage Token Dropping (MustDrop) to measure the importance of each token\nfrom the whole lifecycle, including the vision encoding stage, prefilling\nstage, and decoding stage. Concretely, in the visual encoding stage, MustDrop\nmerges spatially adjacent tokens with high similarity, and establishes a key\ntoken set to retain the most vision-critical tokens, preventing them from being\ndiscarded in later stages. In the prefilling stage, MustDrop further compresses\nvision tokens by the guidance of text semantics, with a dual-attention\nfiltering strategy. In the decoding stage, an output-aware cache policy is\nproposed to further reduce the size of the KV cache. By leveraging tailored\nstrategies in the multi-stage process, MustDrop can more precisely recognize\nthe important and redundant tokens, thus achieving an optimal balance between\nperformance and efficiency. For instance, MustDrop reduces about 88.5\\% FLOPs\non LLaVA with a compression ratio of 92.2\\% while maintaining comparable\naccuracy. Our codes are available at\n\\url{https://github.com/liuting20/MustDrop}."
                },
                "authors": [
                    {
                        "name": "Ting Liu"
                    },
                    {
                        "name": "Liangtao Shi"
                    },
                    {
                        "name": "Richang Hong"
                    },
                    {
                        "name": "Yue Hu"
                    },
                    {
                        "name": "Quanjun Yin"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "8 pages, 4figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10803v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10803v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.01733v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.01733v2",
                "updated": "2024-11-16T07:43:28Z",
                "updated_parsed": [
                    2024,
                    11,
                    16,
                    7,
                    43,
                    28,
                    5,
                    321,
                    0
                ],
                "published": "2024-06-03T18:49:57Z",
                "published_parsed": [
                    2024,
                    6,
                    3,
                    18,
                    49,
                    57,
                    0,
                    155,
                    0
                ],
                "title": "Learning-to-Cache: Accelerating Diffusion Transformer via Layer Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning-to-Cache: Accelerating Diffusion Transformer via Layer Caching"
                },
                "summary": "Diffusion Transformers have recently demonstrated unprecedented generative\ncapabilities for various tasks. The encouraging results, however, come with the\ncost of slow inference, since each denoising step requires inference on a\ntransformer model with a large scale of parameters. In this study, we make an\ninteresting and somehow surprising observation: the computation of a large\nproportion of layers in the diffusion transformer, through introducing a\ncaching mechanism, can be readily removed even without updating the model\nparameters. In the case of U-ViT-H/2, for example, we may remove up to 93.68%\nof the computation in the cache steps (46.84% for all steps), with less than\n0.01 drop in FID. To achieve this, we introduce a novel scheme, named\nLearning-to-Cache (L2C), that learns to conduct caching in a dynamic manner for\ndiffusion transformers. Specifically, by leveraging the identical structure of\nlayers in transformers and the sequential nature of diffusion, we explore\nredundant computations between timesteps by treating each layer as the\nfundamental unit for caching. To address the challenge of the exponential\nsearch space in deep models for identifying layers to cache and remove, we\npropose a novel differentiable optimization objective. An input-invariant yet\ntimestep-variant router is then optimized, which can finally produce a static\ncomputation graph. Experimental results show that L2C largely outperforms\nsamplers such as DDIM and DPM-Solver, alongside prior cache-based methods at\nthe same inference speed. Code is available at\nhttps://github.com/horseee/learning-to-cache",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers have recently demonstrated unprecedented generative\ncapabilities for various tasks. The encouraging results, however, come with the\ncost of slow inference, since each denoising step requires inference on a\ntransformer model with a large scale of parameters. In this study, we make an\ninteresting and somehow surprising observation: the computation of a large\nproportion of layers in the diffusion transformer, through introducing a\ncaching mechanism, can be readily removed even without updating the model\nparameters. In the case of U-ViT-H/2, for example, we may remove up to 93.68%\nof the computation in the cache steps (46.84% for all steps), with less than\n0.01 drop in FID. To achieve this, we introduce a novel scheme, named\nLearning-to-Cache (L2C), that learns to conduct caching in a dynamic manner for\ndiffusion transformers. Specifically, by leveraging the identical structure of\nlayers in transformers and the sequential nature of diffusion, we explore\nredundant computations between timesteps by treating each layer as the\nfundamental unit for caching. To address the challenge of the exponential\nsearch space in deep models for identifying layers to cache and remove, we\npropose a novel differentiable optimization objective. An input-invariant yet\ntimestep-variant router is then optimized, which can finally produce a static\ncomputation graph. Experimental results show that L2C largely outperforms\nsamplers such as DDIM and DPM-Solver, alongside prior cache-based methods at\nthe same inference speed. Code is available at\nhttps://github.com/horseee/learning-to-cache"
                },
                "authors": [
                    {
                        "name": "Xinyin Ma"
                    },
                    {
                        "name": "Gongfan Fang"
                    },
                    {
                        "name": "Michael Bi Mi"
                    },
                    {
                        "name": "Xinchao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xinchao Wang"
                },
                "author": "Xinchao Wang",
                "arxiv_comment": "Accepted at NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.01733v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.01733v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10659v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10659v1",
                "updated": "2024-11-16T01:39:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    16,
                    1,
                    39,
                    44,
                    5,
                    321,
                    0
                ],
                "published": "2024-11-16T01:39:44Z",
                "published_parsed": [
                    2024,
                    11,
                    16,
                    1,
                    39,
                    44,
                    5,
                    321,
                    0
                ],
                "title": "Spineless Traversal for Layout Invalidation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spineless Traversal for Layout Invalidation"
                },
                "summary": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty; only the dirty\nelements need be processed to draw the next frame, dramatically reducing\nlatency. However, the standard incremental layout algorithm must search the\npage for dirty elements, accessing a number of auxiliary elements in the\nprocess. These auxiliary elements add cache misses and stalled cycles, and are\nresponsible for a sizable fraction of all layout latency. We introduce a new,\nfaster incremental layout algorithm called Spineless Traversal. Spineless\nTraversal uses a more computationally demanding priority queue algorithm to\navoid the need to access auxiliary nodes and thus reduces cache traffic and\nstalls. This leads to dramatic speedups on the most latency-critical\ninteractions such as hovering, typing, or animations. Moreover, thanks to\nnumerous low-level optimizations, we are able to make Spineless Traversal\ncompetitive across the whole spectrum of incremental layout workloads. As a\nresult, across 2216 benchmarks, Spineless Traversal is faster on 78.2% of the\nbenchmark, with a mean speedup of 3.23x concentrated in the most\nlatency-critical interactions such as hovering, typing, and animations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty; only the dirty\nelements need be processed to draw the next frame, dramatically reducing\nlatency. However, the standard incremental layout algorithm must search the\npage for dirty elements, accessing a number of auxiliary elements in the\nprocess. These auxiliary elements add cache misses and stalled cycles, and are\nresponsible for a sizable fraction of all layout latency. We introduce a new,\nfaster incremental layout algorithm called Spineless Traversal. Spineless\nTraversal uses a more computationally demanding priority queue algorithm to\navoid the need to access auxiliary nodes and thus reduces cache traffic and\nstalls. This leads to dramatic speedups on the most latency-critical\ninteractions such as hovering, typing, or animations. Moreover, thanks to\nnumerous low-level optimizations, we are able to make Spineless Traversal\ncompetitive across the whole spectrum of incremental layout workloads. As a\nresult, across 2216 benchmarks, Spineless Traversal is faster on 78.2% of the\nbenchmark, with a mean speedup of 3.23x concentrated in the most\nlatency-critical interactions such as hovering, typing, and animations."
                },
                "authors": [
                    {
                        "name": "Marisa Kirisame"
                    },
                    {
                        "name": "Tiezhi Wang"
                    },
                    {
                        "name": "Pavel Panchekha"
                    }
                ],
                "author_detail": {
                    "name": "Pavel Panchekha"
                },
                "author": "Pavel Panchekha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10659v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10659v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.16219v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.16219v4",
                "updated": "2024-11-15T22:37:48Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    22,
                    37,
                    48,
                    4,
                    320,
                    0
                ],
                "published": "2024-04-24T21:35:12Z",
                "published_parsed": [
                    2024,
                    4,
                    24,
                    21,
                    35,
                    12,
                    2,
                    115,
                    0
                ],
                "title": "Can Increasing the Hit Ratio Hurt Cache Throughput? (Long Version)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Increasing the Hit Ratio Hurt Cache Throughput? (Long Version)"
                },
                "summary": "Software caches are an intrinsic component of almost every computer system.\nConsequently, caching algorithms, particularly eviction policies, are the topic\nof many papers. Almost all these prior papers evaluate the caching algorithm\nbased on its hit ratio, namely the fraction of requests that are found in the\ncache, as opposed to disk. The hit ratio is viewed as a proxy for traditional\nperformance metrics like system throughput or response time. Intuitively it\nmakes sense that higher hit ratio should lead to higher throughput (and lower\nresponse time), since more requests are found in the cache (low access time) as\nopposed to the disk (high access time).\n  This paper challenges this intuition. We show that increasing the hit ratio\ncan actually hurt the throughput (and response time) for many caching\nalgorithms. Our investigation follows a three-pronged approach involving (i)\nqueueing modeling and analysis, (ii) implementation and measurement, and (iii)\nsimulation to validate the accuracy of the queueing model. We also show that\nthe phenomenon of throughput decreasing at higher hit ratios is likely to be\nmore pronounced in future systems, where the trend is towards faster disks and\nhigher numbers of cores per CPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Software caches are an intrinsic component of almost every computer system.\nConsequently, caching algorithms, particularly eviction policies, are the topic\nof many papers. Almost all these prior papers evaluate the caching algorithm\nbased on its hit ratio, namely the fraction of requests that are found in the\ncache, as opposed to disk. The hit ratio is viewed as a proxy for traditional\nperformance metrics like system throughput or response time. Intuitively it\nmakes sense that higher hit ratio should lead to higher throughput (and lower\nresponse time), since more requests are found in the cache (low access time) as\nopposed to the disk (high access time).\n  This paper challenges this intuition. We show that increasing the hit ratio\ncan actually hurt the throughput (and response time) for many caching\nalgorithms. Our investigation follows a three-pronged approach involving (i)\nqueueing modeling and analysis, (ii) implementation and measurement, and (iii)\nsimulation to validate the accuracy of the queueing model. We also show that\nthe phenomenon of throughput decreasing at higher hit ratios is likely to be\nmore pronounced in future systems, where the trend is towards faster disks and\nhigher numbers of cores per CPU."
                },
                "authors": [
                    {
                        "name": "Ziyue Qiu"
                    },
                    {
                        "name": "Juncheng Yang"
                    },
                    {
                        "name": "Mor Harchol-Balter"
                    }
                ],
                "author_detail": {
                    "name": "Mor Harchol-Balter"
                },
                "author": "Mor Harchol-Balter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.16219v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.16219v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.13853v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.13853v2",
                "updated": "2024-11-15T22:30:38Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    22,
                    30,
                    38,
                    4,
                    320,
                    0
                ],
                "published": "2024-07-18T18:47:52Z",
                "published_parsed": [
                    2024,
                    7,
                    18,
                    18,
                    47,
                    52,
                    3,
                    200,
                    0
                ],
                "title": "Forecasting GPU Performance for Deep Learning Training and Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Forecasting GPU Performance for Deep Learning Training and Inference"
                },
                "summary": "Deep learning kernels exhibit predictable memory accesses and compute\npatterns, making GPUs' parallel architecture well-suited for their execution.\nSoftware and runtime systems for GPUs are optimized to better utilize the\nstream multiprocessors, on-chip cache, and off-chip high-bandwidth memory. As\ndeep learning models and GPUs evolve, access to newer GPUs is often limited,\nraising questions about the performance of new model architectures on existing\nGPUs, existing models on new GPUs, and new model architectures on new GPUs. To\naddress these questions, we introduce NeuSight, a framework to predict the\nperformance of various deep learning models, for both training and inference,\non unseen GPUs without requiring actual execution. The framework leverages both\nGPU hardware behavior and software library optimizations to estimate end-to-end\nperformance. Previous work uses regression models that capture linear trends or\nmultilayer perceptrons to predict the overall latency of deep learning kernels\non GPUs. These approaches suffer from higher error percentages when forecasting\nperformance on unseen models and new GPUs. Instead, NeuSight decomposes the\nprediction problem into smaller problems, bounding the prediction through\nfundamental performance laws. NeuSight decomposes a single deep learning kernel\nprediction into smaller working sets called tiles, which are executed\nindependently on the GPU. Tile-granularity predictions are determined using a\nmachine learning approach and aggregated to estimate end-to-end latency.\nNeuSight outperforms prior work across various deep learning workloads and the\nlatest GPUs. It reduces the percentage error from 198% and 19.7% to 3.8% in\npredicting the latency of GPT3 model for training and inference on H100,\ncompared to state-of-the-art prior works, where both GPT3 and H100 were not\nused to train the framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning kernels exhibit predictable memory accesses and compute\npatterns, making GPUs' parallel architecture well-suited for their execution.\nSoftware and runtime systems for GPUs are optimized to better utilize the\nstream multiprocessors, on-chip cache, and off-chip high-bandwidth memory. As\ndeep learning models and GPUs evolve, access to newer GPUs is often limited,\nraising questions about the performance of new model architectures on existing\nGPUs, existing models on new GPUs, and new model architectures on new GPUs. To\naddress these questions, we introduce NeuSight, a framework to predict the\nperformance of various deep learning models, for both training and inference,\non unseen GPUs without requiring actual execution. The framework leverages both\nGPU hardware behavior and software library optimizations to estimate end-to-end\nperformance. Previous work uses regression models that capture linear trends or\nmultilayer perceptrons to predict the overall latency of deep learning kernels\non GPUs. These approaches suffer from higher error percentages when forecasting\nperformance on unseen models and new GPUs. Instead, NeuSight decomposes the\nprediction problem into smaller problems, bounding the prediction through\nfundamental performance laws. NeuSight decomposes a single deep learning kernel\nprediction into smaller working sets called tiles, which are executed\nindependently on the GPU. Tile-granularity predictions are determined using a\nmachine learning approach and aggregated to estimate end-to-end latency.\nNeuSight outperforms prior work across various deep learning workloads and the\nlatest GPUs. It reduces the percentage error from 198% and 19.7% to 3.8% in\npredicting the latency of GPT3 model for training and inference on H100,\ncompared to state-of-the-art prior works, where both GPT3 and H100 were not\nused to train the framework."
                },
                "authors": [
                    {
                        "name": "Seonho Lee"
                    },
                    {
                        "name": "Amar Phanishayee"
                    },
                    {
                        "name": "Divya Mahajan"
                    }
                ],
                "author_detail": {
                    "name": "Divya Mahajan"
                },
                "author": "Divya Mahajan",
                "arxiv_comment": "Accepted at the 30th ACM International Conference on Architectural\n  Support for Programming Languages and Operating Systems (ASPLOS), 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.13853v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.13853v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10510v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10510v1",
                "updated": "2024-11-15T16:24:02Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    16,
                    24,
                    2,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-15T16:24:02Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    16,
                    24,
                    2,
                    4,
                    320,
                    0
                ],
                "title": "SmoothCache: A Universal Inference Acceleration Technique for Diffusion\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SmoothCache: A Universal Inference Acceleration Technique for Diffusion\n  Transformers"
                },
                "summary": "Diffusion Transformers (DiT) have emerged as powerful generative models for\nvarious tasks, including image, video, and speech synthesis. However, their\ninference process remains computationally expensive due to the repeated\nevaluation of resource-intensive attention and feed-forward modules. To address\nthis, we introduce SmoothCache, a model-agnostic inference acceleration\ntechnique for DiT architectures. SmoothCache leverages the observed high\nsimilarity between layer outputs across adjacent diffusion timesteps. By\nanalyzing layer-wise representation errors from a small calibration set,\nSmoothCache adaptively caches and reuses key features during inference. Our\nexperiments demonstrate that SmoothCache achieves 8% to 71% speed up while\nmaintaining or even improving generation quality across diverse modalities. We\nshowcase its effectiveness on DiT-XL for image generation, Open-Sora for\ntext-to-video, and Stable Audio Open for text-to-audio, highlighting its\npotential to enable real-time applications and broaden the accessibility of\npowerful DiT models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiT) have emerged as powerful generative models for\nvarious tasks, including image, video, and speech synthesis. However, their\ninference process remains computationally expensive due to the repeated\nevaluation of resource-intensive attention and feed-forward modules. To address\nthis, we introduce SmoothCache, a model-agnostic inference acceleration\ntechnique for DiT architectures. SmoothCache leverages the observed high\nsimilarity between layer outputs across adjacent diffusion timesteps. By\nanalyzing layer-wise representation errors from a small calibration set,\nSmoothCache adaptively caches and reuses key features during inference. Our\nexperiments demonstrate that SmoothCache achieves 8% to 71% speed up while\nmaintaining or even improving generation quality across diverse modalities. We\nshowcase its effectiveness on DiT-XL for image generation, Open-Sora for\ntext-to-video, and Stable Audio Open for text-to-audio, highlighting its\npotential to enable real-time applications and broaden the accessibility of\npowerful DiT models."
                },
                "authors": [
                    {
                        "name": "Joseph Liu"
                    },
                    {
                        "name": "Joshua Geddes"
                    },
                    {
                        "name": "Ziyu Guo"
                    },
                    {
                        "name": "Haomiao Jiang"
                    },
                    {
                        "name": "Mahesh Kumar Nandwana"
                    }
                ],
                "author_detail": {
                    "name": "Mahesh Kumar Nandwana"
                },
                "author": "Mahesh Kumar Nandwana",
                "arxiv_comment": "Code can be found at https://github.com/Roblox/SmoothCache",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10510v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10510v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03174v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03174v2",
                "updated": "2024-11-15T07:25:54Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    7,
                    25,
                    54,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-05T15:22:11Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    15,
                    22,
                    11,
                    1,
                    310,
                    0
                ],
                "title": "ZipCache: A DRAM/SSD Cache with Built-in Transparent Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZipCache: A DRAM/SSD Cache with Built-in Transparent Compression"
                },
                "summary": "As a core component in modern data centers, key-value cache provides\nhigh-throughput and low-latency services for high-speed data processing. The\neffectiveness of a key-value cache relies on its ability of accommodating the\nneeded data. However, expanding the cache capacity is often more difficult than\ncommonly expected because of many practical constraints, such as server costs,\ncooling issues, rack space, and even human resource expenses. A potential\nsolution is compression, which virtually extends the cache capacity by\ncondensing data in cache. In practice, this seemingly simple idea has not\ngained much traction in key-value cache system design, due to several critical\nissues: the compression-unfriendly index structure, severe read/write\namplification, wasteful decompression operations, and heavy computing cost.\nThis paper presents a hybrid DRAM-SSD cache design to realize a systematic\nintegration of data compression in key-value cache. By treating compression as\nan essential component, we have redesigned the indexing structure, data\nmanagement, and leveraged the emerging computational SSD hardware for\ncollaborative optimizations. We have developed a prototype, called ZipCache.\nOur experimental results show that ZipCache can achieve up to 72.4% higher\nthroughput and 42.4% lower latency, while reducing the write amplification by\nup to 26.2 times.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As a core component in modern data centers, key-value cache provides\nhigh-throughput and low-latency services for high-speed data processing. The\neffectiveness of a key-value cache relies on its ability of accommodating the\nneeded data. However, expanding the cache capacity is often more difficult than\ncommonly expected because of many practical constraints, such as server costs,\ncooling issues, rack space, and even human resource expenses. A potential\nsolution is compression, which virtually extends the cache capacity by\ncondensing data in cache. In practice, this seemingly simple idea has not\ngained much traction in key-value cache system design, due to several critical\nissues: the compression-unfriendly index structure, severe read/write\namplification, wasteful decompression operations, and heavy computing cost.\nThis paper presents a hybrid DRAM-SSD cache design to realize a systematic\nintegration of data compression in key-value cache. By treating compression as\nan essential component, we have redesigned the indexing structure, data\nmanagement, and leveraged the emerging computational SSD hardware for\ncollaborative optimizations. We have developed a prototype, called ZipCache.\nOur experimental results show that ZipCache can achieve up to 72.4% higher\nthroughput and 42.4% lower latency, while reducing the write amplification by\nup to 26.2 times."
                },
                "authors": [
                    {
                        "name": "Rui Xie"
                    },
                    {
                        "name": "Linsen Ma"
                    },
                    {
                        "name": "Alex Zhong"
                    },
                    {
                        "name": "Feng Chen"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "author": "Tong Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03174v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03174v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09859v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09859v1",
                "updated": "2024-11-15T00:37:31Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    0,
                    37,
                    31,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-15T00:37:31Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    0,
                    37,
                    31,
                    4,
                    320,
                    0
                ],
                "title": "Skew-Symmetric Matrix Decompositions on Shared-Memory Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Skew-Symmetric Matrix Decompositions on Shared-Memory Architectures"
                },
                "summary": "The factorization of skew-symmetric matrices is a critically understudied\narea of dense linear algebra (DLA), particularly in comparison to that of\nsymmetric matrices. While some algorithms can be adapted from the symmetric\ncase, the cost of algorithms can be reduced by exploiting skew-symmetry. A\nmotivating example is the factorization $X=LTL^T$ of a skew-symmetric matrix\n$X$, which is used in practical applications as a means of determining the\ndeterminant of $X$ as the square of the (cheaply-computed) Pfaffian of the\nskew-symmetric tridiagonal matrix $T$, for example in fields such as quantum\nelectronic structure and machine learning. Such applications also often require\npivoting in order to improve numerical stability. In this work we explore a\ncombination of known literature algorithms and new algorithms recently derived\nusing formal methods. High-performance parallel CPU implementations are\ncreated, leveraging the concept of fusion at multiple levels in order to reduce\nmemory traffic overhead, as well as the BLIS framework which provides\nhigh-performance GEMM kernels, hierarchical parallelism, and cache blocking. We\nfind that operation fusion and improved use of available bandwidth via\nparallelization of bandwidth-bound (level-2 BLAS) operations are essential for\nobtaining high performance, while a concise C++ implementation provides a clear\nand close connection to the formal derivation process without sacrificing\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The factorization of skew-symmetric matrices is a critically understudied\narea of dense linear algebra (DLA), particularly in comparison to that of\nsymmetric matrices. While some algorithms can be adapted from the symmetric\ncase, the cost of algorithms can be reduced by exploiting skew-symmetry. A\nmotivating example is the factorization $X=LTL^T$ of a skew-symmetric matrix\n$X$, which is used in practical applications as a means of determining the\ndeterminant of $X$ as the square of the (cheaply-computed) Pfaffian of the\nskew-symmetric tridiagonal matrix $T$, for example in fields such as quantum\nelectronic structure and machine learning. Such applications also often require\npivoting in order to improve numerical stability. In this work we explore a\ncombination of known literature algorithms and new algorithms recently derived\nusing formal methods. High-performance parallel CPU implementations are\ncreated, leveraging the concept of fusion at multiple levels in order to reduce\nmemory traffic overhead, as well as the BLIS framework which provides\nhigh-performance GEMM kernels, hierarchical parallelism, and cache blocking. We\nfind that operation fusion and improved use of available bandwidth via\nparallelization of bandwidth-bound (level-2 BLAS) operations are essential for\nobtaining high performance, while a concise C++ implementation provides a clear\nand close connection to the formal derivation process without sacrificing\nperformance."
                },
                "authors": [
                    {
                        "name": "Ishna Satyarth"
                    },
                    {
                        "name": "Chao Yin"
                    },
                    {
                        "name": "RuQing G. Xu"
                    },
                    {
                        "name": "Devin A. Matthews"
                    }
                ],
                "author_detail": {
                    "name": "Devin A. Matthews"
                },
                "author": "Devin A. Matthews",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09859v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09859v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09812v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09812v1",
                "updated": "2024-11-14T21:01:29Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    21,
                    1,
                    29,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T21:01:29Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    21,
                    1,
                    29,
                    3,
                    319,
                    0
                ],
                "title": "Edge Caching Optimization with PPO and Transfer Learning for Dynamic\n  Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edge Caching Optimization with PPO and Transfer Learning for Dynamic\n  Environments"
                },
                "summary": "This paper addresses the challenge of edge caching in dynamic environments,\nwhere rising traffic loads strain backhaul links and core networks. We propose\na Proximal Policy Optimization (PPO)-based caching strategy that fully\nincorporates key file attributes such as size, lifetime, importance, and\npopularity, while also considering random file request arrivals, reflecting\nmore realistic edge caching scenarios. In dynamic environments, changes such as\nshifts in content popularity and variations in request rates frequently occur,\nmaking previously learned policies less effective as they were optimized for\nearlier conditions. Without adaptation, caching efficiency and response times\ncan degrade. While learning a new policy from scratch in a new environment is\nan option, it is highly inefficient and computationally expensive. Thus,\nadapting an existing policy to these changes is critical. To address this, we\ndevelop a mechanism that detects changes in content popularity and request\nrates, ensuring timely adjustments to the caching strategy. We also propose a\ntransfer learning-based PPO algorithm that accelerates convergence in new\nenvironments by leveraging prior knowledge. Simulation results demonstrate the\nsignificant effectiveness of our approach, outperforming a recent Deep\nReinforcement Learning (DRL)-based method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper addresses the challenge of edge caching in dynamic environments,\nwhere rising traffic loads strain backhaul links and core networks. We propose\na Proximal Policy Optimization (PPO)-based caching strategy that fully\nincorporates key file attributes such as size, lifetime, importance, and\npopularity, while also considering random file request arrivals, reflecting\nmore realistic edge caching scenarios. In dynamic environments, changes such as\nshifts in content popularity and variations in request rates frequently occur,\nmaking previously learned policies less effective as they were optimized for\nearlier conditions. Without adaptation, caching efficiency and response times\ncan degrade. While learning a new policy from scratch in a new environment is\nan option, it is highly inefficient and computationally expensive. Thus,\nadapting an existing policy to these changes is critical. To address this, we\ndevelop a mechanism that detects changes in content popularity and request\nrates, ensuring timely adjustments to the caching strategy. We also propose a\ntransfer learning-based PPO algorithm that accelerates convergence in new\nenvironments by leveraging prior knowledge. Simulation results demonstrate the\nsignificant effectiveness of our approach, outperforming a recent Deep\nReinforcement Learning (DRL)-based method."
                },
                "authors": [
                    {
                        "name": "Farnaz Niknia"
                    },
                    {
                        "name": "Ping Wang"
                    }
                ],
                "author_detail": {
                    "name": "Ping Wang"
                },
                "author": "Ping Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09812v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09812v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17897v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17897v2",
                "updated": "2024-11-14T17:46:04Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    17,
                    46,
                    4,
                    3,
                    319,
                    0
                ],
                "published": "2024-10-23T14:15:07Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    14,
                    15,
                    7,
                    2,
                    297,
                    0
                ],
                "title": "Value Residual Learning For Alleviating Attention Concentration In\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Value Residual Learning For Alleviating Attention Concentration In\n  Transformers"
                },
                "summary": "Transformers can capture long-range dependencies using self-attention,\nallowing tokens to attend to all others directly. However, stacking multiple\nattention layers leads to attention concentration. One natural way to address\nthis issue is to use cross-layer attention, allowing information from earlier\nlayers to be directly accessible to later layers. However, this approach is\ncomputationally expensive. To address this problem, we propose Transformer with\nresidual value (ResFormer) which approximates cross-layer attention through\nadding a residual connection from the values of the the first layer to all\nsubsequent layers. Based on this method, one variant is the Transformer with\nsingle layer value (SVFormer), where all layers share the same value embedding\nfrom first layer, reducing the $KV$ cache by nearly 50\\%. Comprehensive\nempirical evidence demonstrates that ResFormer mitigates attention\nconcentration problem in deeper layers and enhances representation across most\nlayers, outperforming the vanilla Transformer, DenseFormer, and NeuTRENO in\ntraining error as well as downstream tasks. Further visualization results\nsuggest that Resformer alleviates attention sinks through avoiding value-state\ndrains. SVFormer trains significantly faster than the vanilla Transformer and\nperforms better than other methods like GQA and CLA, with performance\ninfluenced by sequence length and cumulative learning rate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers can capture long-range dependencies using self-attention,\nallowing tokens to attend to all others directly. However, stacking multiple\nattention layers leads to attention concentration. One natural way to address\nthis issue is to use cross-layer attention, allowing information from earlier\nlayers to be directly accessible to later layers. However, this approach is\ncomputationally expensive. To address this problem, we propose Transformer with\nresidual value (ResFormer) which approximates cross-layer attention through\nadding a residual connection from the values of the the first layer to all\nsubsequent layers. Based on this method, one variant is the Transformer with\nsingle layer value (SVFormer), where all layers share the same value embedding\nfrom first layer, reducing the $KV$ cache by nearly 50\\%. Comprehensive\nempirical evidence demonstrates that ResFormer mitigates attention\nconcentration problem in deeper layers and enhances representation across most\nlayers, outperforming the vanilla Transformer, DenseFormer, and NeuTRENO in\ntraining error as well as downstream tasks. Further visualization results\nsuggest that Resformer alleviates attention sinks through avoiding value-state\ndrains. SVFormer trains significantly faster than the vanilla Transformer and\nperforms better than other methods like GQA and CLA, with performance\ninfluenced by sequence length and cumulative learning rate."
                },
                "authors": [
                    {
                        "name": "Zhanchao Zhou"
                    },
                    {
                        "name": "Tianyi Wu"
                    },
                    {
                        "name": "Zhiyun Jiang"
                    },
                    {
                        "name": "Zhenzhong Lan"
                    }
                ],
                "author_detail": {
                    "name": "Zhenzhong Lan"
                },
                "author": "Zhenzhong Lan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17897v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17897v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09546v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09546v1",
                "updated": "2024-11-14T16:01:05Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    16,
                    1,
                    5,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T16:01:05Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    16,
                    1,
                    5,
                    3,
                    319,
                    0
                ],
                "title": "Architectural Exploration of Application-Specific Resonant SRAM\n  Compute-in-Memory (rCiM)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Architectural Exploration of Application-Specific Resonant SRAM\n  Compute-in-Memory (rCiM)"
                },
                "summary": "While general-purpose computing follows Von Neumann's architecture, the data\nmovement between memory and processor elements dictates the processor's\nperformance. The evolving compute-in-memory (CiM) paradigm tackles this issue\nby facilitating simultaneous processing and storage within static random-access\nmemory (SRAM) elements. Numerous design decisions taken at different levels of\nhierarchy affect the figure of merits (FoMs) of SRAM, such as power,\nperformance, area, and yield. The absence of a rapid assessment mechanism for\nthe impact of changes at different hierarchy levels on global FoMs poses a\nchallenge to accurately evaluating innovative SRAM designs. This paper presents\nan automation tool designed to optimize the energy and latency of SRAM designs\nincorporating diverse implementation strategies for executing logic operations\nwithin the SRAM. The tool structure allows easy comparison across different\narray topologies and various design strategies to result in energy-efficient\nimplementations. Our study involves a comprehensive comparison of over 6900+\ndistinct design implementation strategies for EPFL combinational benchmark\ncircuits on the energy-recycling resonant compute-in-memory (rCiM) architecture\ndesigned using TSMC 28 nm technology. When provided with a combinational\ncircuit, the tool aims to generate an energy-efficient implementation strategy\ntailored to the specified input memory and latency constraints. The tool\nreduces 80.9% of energy consumption on average across all benchmarks while\nusing the six-topology implementation compared to baseline implementation of\nsingle-macro topology by considering the parallel processing capability of rCiM\ncache size ranging from 4KB to 192KB.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While general-purpose computing follows Von Neumann's architecture, the data\nmovement between memory and processor elements dictates the processor's\nperformance. The evolving compute-in-memory (CiM) paradigm tackles this issue\nby facilitating simultaneous processing and storage within static random-access\nmemory (SRAM) elements. Numerous design decisions taken at different levels of\nhierarchy affect the figure of merits (FoMs) of SRAM, such as power,\nperformance, area, and yield. The absence of a rapid assessment mechanism for\nthe impact of changes at different hierarchy levels on global FoMs poses a\nchallenge to accurately evaluating innovative SRAM designs. This paper presents\nan automation tool designed to optimize the energy and latency of SRAM designs\nincorporating diverse implementation strategies for executing logic operations\nwithin the SRAM. The tool structure allows easy comparison across different\narray topologies and various design strategies to result in energy-efficient\nimplementations. Our study involves a comprehensive comparison of over 6900+\ndistinct design implementation strategies for EPFL combinational benchmark\ncircuits on the energy-recycling resonant compute-in-memory (rCiM) architecture\ndesigned using TSMC 28 nm technology. When provided with a combinational\ncircuit, the tool aims to generate an energy-efficient implementation strategy\ntailored to the specified input memory and latency constraints. The tool\nreduces 80.9% of energy consumption on average across all benchmarks while\nusing the six-topology implementation compared to baseline implementation of\nsingle-macro topology by considering the parallel processing capability of rCiM\ncache size ranging from 4KB to 192KB."
                },
                "authors": [
                    {
                        "name": "Dhandeep Challagundla"
                    },
                    {
                        "name": "Ignatius Bezzam"
                    },
                    {
                        "name": "Riadul Islam"
                    }
                ],
                "author_detail": {
                    "name": "Riadul Islam"
                },
                "author": "Riadul Islam",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09546v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09546v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09473v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09473v1",
                "updated": "2024-11-14T14:28:31Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    14,
                    28,
                    31,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T14:28:31Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    14,
                    28,
                    31,
                    3,
                    319,
                    0
                ],
                "title": "Enhancing Scalability and Performance in Influence Maximization with\n  Optimized Parallel Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Scalability and Performance in Influence Maximization with\n  Optimized Parallel Processing"
                },
                "summary": "Influence Maximization (IM) is vital in viral marketing and biological\nnetwork analysis for identifying key influencers. Given its NP-hard nature,\napproximate solutions are employed. This paper addresses scalability challenges\nin scale-out shared memory system by focusing on the state-of-the-art Influence\nMaximization via Martingales (IMM) benchmark. To enhance the work efficiency of\nthe current IMM implementation, we propose EFFICIENTIMM with key strategies,\nincluding new parallelization scheme, NUMA-aware memory usage, dynamic load\nbalancing and fine-grained adaptive data structures. Benchmarking on a 128-core\nCPU system with 8 NUMA nodes, EFFICIENTIMM demonstrated significant performance\nimprovements, achieving an average 5.9x speedup over Ripples across 8 diverse\nSNAP datasets, when compared to the best execution times of the original\nRipples framework. Additionally, on the Youtube graph, EFFICIENTIMM\ndemonstrates a better memory access pattern with 357.4x reduction in L1+L2\ncache misses as compared to Ripples.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Influence Maximization (IM) is vital in viral marketing and biological\nnetwork analysis for identifying key influencers. Given its NP-hard nature,\napproximate solutions are employed. This paper addresses scalability challenges\nin scale-out shared memory system by focusing on the state-of-the-art Influence\nMaximization via Martingales (IMM) benchmark. To enhance the work efficiency of\nthe current IMM implementation, we propose EFFICIENTIMM with key strategies,\nincluding new parallelization scheme, NUMA-aware memory usage, dynamic load\nbalancing and fine-grained adaptive data structures. Benchmarking on a 128-core\nCPU system with 8 NUMA nodes, EFFICIENTIMM demonstrated significant performance\nimprovements, achieving an average 5.9x speedup over Ripples across 8 diverse\nSNAP datasets, when compared to the best execution times of the original\nRipples framework. Additionally, on the Youtube graph, EFFICIENTIMM\ndemonstrates a better memory access pattern with 357.4x reduction in L1+L2\ncache misses as compared to Ripples."
                },
                "authors": [
                    {
                        "name": "Hanjiang Wu"
                    },
                    {
                        "name": "Huan Xu"
                    },
                    {
                        "name": "Joongun Park"
                    },
                    {
                        "name": "Jesmin Jahan Tithi"
                    },
                    {
                        "name": "Fabio Checconi"
                    },
                    {
                        "name": "Jordi Wolfson-Pou"
                    },
                    {
                        "name": "Fabrizio Petrini"
                    },
                    {
                        "name": "Tushar Krishna"
                    }
                ],
                "author_detail": {
                    "name": "Tushar Krishna"
                },
                "author": "Tushar Krishna",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09473v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09473v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09425v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09425v1",
                "updated": "2024-11-14T13:22:41Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    13,
                    22,
                    41,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T13:22:41Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    13,
                    22,
                    41,
                    3,
                    319,
                    0
                ],
                "title": "MARM: Unlocking the Future of Recommendation Systems through Memory\n  Augmentation and Scalable Complexity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MARM: Unlocking the Future of Recommendation Systems through Memory\n  Augmentation and Scalable Complexity"
                },
                "summary": "Scaling-law has guided the language model designing for past years, however,\nit is worth noting that the scaling laws of NLP cannot be directly applied to\nRecSys due to the following reasons: (1) The amount of training samples and\nmodel parameters is typically not the bottleneck for the model. Our\nrecommendation system can generate over 50 billion user samples daily, and such\na massive amount of training data can easily allow our model parameters to\nexceed 200 billion, surpassing many LLMs (about 100B). (2) To ensure the\nstability and robustness of the recommendation system, it is essential to\ncontrol computational complexity FLOPs carefully. Considering the above\ndifferences with LLM, we can draw a conclusion that: for a RecSys model,\ncompared to model parameters, the computational complexity FLOPs is a more\nexpensive factor that requires careful control. In this paper, we propose our\nmilestone work, MARM (Memory Augmented Recommendation Model), which explores a\nnew cache scaling-laws successfully.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling-law has guided the language model designing for past years, however,\nit is worth noting that the scaling laws of NLP cannot be directly applied to\nRecSys due to the following reasons: (1) The amount of training samples and\nmodel parameters is typically not the bottleneck for the model. Our\nrecommendation system can generate over 50 billion user samples daily, and such\na massive amount of training data can easily allow our model parameters to\nexceed 200 billion, surpassing many LLMs (about 100B). (2) To ensure the\nstability and robustness of the recommendation system, it is essential to\ncontrol computational complexity FLOPs carefully. Considering the above\ndifferences with LLM, we can draw a conclusion that: for a RecSys model,\ncompared to model parameters, the computational complexity FLOPs is a more\nexpensive factor that requires careful control. In this paper, we propose our\nmilestone work, MARM (Memory Augmented Recommendation Model), which explores a\nnew cache scaling-laws successfully."
                },
                "authors": [
                    {
                        "name": "Xiao Lv"
                    },
                    {
                        "name": "Jiangxia Cao"
                    },
                    {
                        "name": "Shijie Guan"
                    },
                    {
                        "name": "Xiaoyou Zhou"
                    },
                    {
                        "name": "Zhiguang Qi"
                    },
                    {
                        "name": "Yaqiang Zang"
                    },
                    {
                        "name": "Ming Li"
                    },
                    {
                        "name": "Ben Wang"
                    },
                    {
                        "name": "Kun Gai"
                    },
                    {
                        "name": "Guorui Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Guorui Zhou"
                },
                "author": "Guorui Zhou",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09425v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09425v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "N/A",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09317v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09317v1",
                "updated": "2024-11-14T09:50:41Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    9,
                    50,
                    41,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T09:50:41Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    9,
                    50,
                    41,
                    3,
                    319,
                    0
                ],
                "title": "Pie: Pooling CPU Memory for LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pie: Pooling CPU Memory for LLM Inference"
                },
                "summary": "The rapid growth of LLMs has revolutionized natural language processing and\nAI analysis, but their increasing size and memory demands present significant\nchallenges. A common solution is to spill over to CPU memory; however,\ntraditional GPU-CPU memory swapping often results in higher latency and lower\nthroughput.\n  This paper introduces Pie, an LLM inference framework that addresses these\nchallenges with performance-transparent swapping and adaptive expansion. By\nleveraging predictable memory access patterns and the high bandwidth of modern\nhardware like the NVIDIA GH200 Grace Hopper Superchip, Pie enables concurrent\ndata swapping without affecting foreground computation, expanding effective\nmemory without added latency. Adaptive expansion dynamically adjusts CPU memory\nallocation based on real-time information, optimizing memory usage and\nperformance under varying conditions.\n  Pie maintains low computation latency, high throughput, and high elasticity.\nOur experimental evaluation demonstrates that Pie achieves optimal swapping\npolicy during cache warmup and effectively balances increased memory capacity\nwith negligible impact on computation. With its extended capacity, Pie\noutperforms vLLM by up to 1.9X in throughput and 2X in latency. Additionally,\nPie can reduce GPU memory usage by up to 1.67X while maintaining the same\nperformance. Compared to FlexGen, an offline profiling-based swapping solution,\nPie achieves magnitudes lower latency and 9.4X higher throughput.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid growth of LLMs has revolutionized natural language processing and\nAI analysis, but their increasing size and memory demands present significant\nchallenges. A common solution is to spill over to CPU memory; however,\ntraditional GPU-CPU memory swapping often results in higher latency and lower\nthroughput.\n  This paper introduces Pie, an LLM inference framework that addresses these\nchallenges with performance-transparent swapping and adaptive expansion. By\nleveraging predictable memory access patterns and the high bandwidth of modern\nhardware like the NVIDIA GH200 Grace Hopper Superchip, Pie enables concurrent\ndata swapping without affecting foreground computation, expanding effective\nmemory without added latency. Adaptive expansion dynamically adjusts CPU memory\nallocation based on real-time information, optimizing memory usage and\nperformance under varying conditions.\n  Pie maintains low computation latency, high throughput, and high elasticity.\nOur experimental evaluation demonstrates that Pie achieves optimal swapping\npolicy during cache warmup and effectively balances increased memory capacity\nwith negligible impact on computation. With its extended capacity, Pie\noutperforms vLLM by up to 1.9X in throughput and 2X in latency. Additionally,\nPie can reduce GPU memory usage by up to 1.67X while maintaining the same\nperformance. Compared to FlexGen, an offline profiling-based swapping solution,\nPie achieves magnitudes lower latency and 9.4X higher throughput."
                },
                "authors": [
                    {
                        "name": "Yi Xu"
                    },
                    {
                        "name": "Ziming Mao"
                    },
                    {
                        "name": "Xiangxi Mo"
                    },
                    {
                        "name": "Shu Liu"
                    },
                    {
                        "name": "Ion Stoica"
                    }
                ],
                "author_detail": {
                    "name": "Ion Stoica"
                },
                "author": "Ion Stoica",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09317v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09317v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09275v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09275v1",
                "updated": "2024-11-14T08:25:31Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    8,
                    25,
                    31,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T08:25:31Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    8,
                    25,
                    31,
                    3,
                    319,
                    0
                ],
                "title": "Pkd-tree: Parallel $k$d-tree with Batch Updates",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pkd-tree: Parallel $k$d-tree with Batch Updates"
                },
                "summary": "The $k$d-tree is one of the most widely used data structures to manage\nmulti-dimensional data. Due to the ever-growing data volume, it is imperative\nto consider parallelism in $k$d-trees. However, we observed challenges in\nexisting parallel kd-tree implementations, for both constructions and updates.\n  The goal of this paper is to develop efficient in-memory $k$d-trees by\nsupporting high parallelism and cache-efficiency. We propose the Pkd-tree\n(Parallel $k$d-tree), a parallel $k$d-tree that is efficient both in theory and\nin practice. The Pkd-tree supports parallel tree construction, batch update\n(insertion and deletion), and various queries including k-nearest neighbor\nsearch, range query, and range count. We proved that our algorithms have strong\ntheoretical bounds in work (sequential time complexity), span (parallelism),\nand cache complexity. Our key techniques include 1) an efficient construction\nalgorithm that optimizes work, span, and cache complexity simultaneously, and\n2) reconstruction-based update algorithms that guarantee the tree to be\nweight-balanced. With the new algorithmic insights and careful engineering\neffort, we achieved a highly optimized implementation of the Pkd-tree.\n  We tested Pkd-tree with various synthetic and real-world datasets, including\nboth uniform and highly skewed data. We compare the Pkd-tree with\nstate-of-the-art parallel $k$d-tree implementations. In all tests, with better\nor competitive query performance, Pkd-tree is much faster in construction and\nupdates consistently than all baselines. We released our code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The $k$d-tree is one of the most widely used data structures to manage\nmulti-dimensional data. Due to the ever-growing data volume, it is imperative\nto consider parallelism in $k$d-trees. However, we observed challenges in\nexisting parallel kd-tree implementations, for both constructions and updates.\n  The goal of this paper is to develop efficient in-memory $k$d-trees by\nsupporting high parallelism and cache-efficiency. We propose the Pkd-tree\n(Parallel $k$d-tree), a parallel $k$d-tree that is efficient both in theory and\nin practice. The Pkd-tree supports parallel tree construction, batch update\n(insertion and deletion), and various queries including k-nearest neighbor\nsearch, range query, and range count. We proved that our algorithms have strong\ntheoretical bounds in work (sequential time complexity), span (parallelism),\nand cache complexity. Our key techniques include 1) an efficient construction\nalgorithm that optimizes work, span, and cache complexity simultaneously, and\n2) reconstruction-based update algorithms that guarantee the tree to be\nweight-balanced. With the new algorithmic insights and careful engineering\neffort, we achieved a highly optimized implementation of the Pkd-tree.\n  We tested Pkd-tree with various synthetic and real-world datasets, including\nboth uniform and highly skewed data. We compare the Pkd-tree with\nstate-of-the-art parallel $k$d-tree implementations. In all tests, with better\nor competitive query performance, Pkd-tree is much faster in construction and\nupdates consistently than all baselines. We released our code."
                },
                "authors": [
                    {
                        "name": "Ziyang Men"
                    },
                    {
                        "name": "Zheqi Shen"
                    },
                    {
                        "name": "Yan Gu"
                    },
                    {
                        "name": "Yihan Sun"
                    }
                ],
                "author_detail": {
                    "name": "Yihan Sun"
                },
                "author": "Yihan Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09275v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09275v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19258v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19258v3",
                "updated": "2024-11-14T01:56:11Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    1,
                    56,
                    11,
                    3,
                    319,
                    0
                ],
                "published": "2024-10-25T02:22:00Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    2,
                    22,
                    0,
                    4,
                    299,
                    0
                ],
                "title": "Not All Heads Matter: A Head-Level KV Cache Compression Method with\n  Integrated Retrieval and Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not All Heads Matter: A Head-Level KV Cache Compression Method with\n  Integrated Retrieval and Reasoning"
                },
                "summary": "Key-Value (KV) caching is a common technique to enhance the computational\nefficiency of Large Language Models (LLMs), but its memory overhead grows\nrapidly with input length. Prior work has shown that not all tokens are equally\nimportant for text generation, proposing layer-level KV cache compression to\nselectively retain key information. Recognizing the distinct roles of attention\nheads in generation, we propose HeadKV, a head-level KV cache compression\nmethod, and HeadKV-R2, which leverages a novel contextual reasoning ability\nestimation for compression. Our approach operates at the level of individual\nheads, estimating their importance for contextual QA tasks that require both\nretrieval and reasoning capabilities. Extensive experiments across diverse\nbenchmarks (LongBench, LooGLE), model architectures (e.g., Llama-3-8B-Instruct,\nMistral-7B-Instruct), and long-context abilities tests demonstrate that our\nhead-level KV cache compression significantly outperforms strong baselines,\nparticularly in low-resource settings (KV size = 64 & 128). Notably, our method\nretains just 1.5% of the KV cache while achieving 97% of the performance of the\nfull KV cache on the contextual question answering benchmark.Codes are\navailable at https://github.com/FYYFU/HeadKV",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-Value (KV) caching is a common technique to enhance the computational\nefficiency of Large Language Models (LLMs), but its memory overhead grows\nrapidly with input length. Prior work has shown that not all tokens are equally\nimportant for text generation, proposing layer-level KV cache compression to\nselectively retain key information. Recognizing the distinct roles of attention\nheads in generation, we propose HeadKV, a head-level KV cache compression\nmethod, and HeadKV-R2, which leverages a novel contextual reasoning ability\nestimation for compression. Our approach operates at the level of individual\nheads, estimating their importance for contextual QA tasks that require both\nretrieval and reasoning capabilities. Extensive experiments across diverse\nbenchmarks (LongBench, LooGLE), model architectures (e.g., Llama-3-8B-Instruct,\nMistral-7B-Instruct), and long-context abilities tests demonstrate that our\nhead-level KV cache compression significantly outperforms strong baselines,\nparticularly in low-resource settings (KV size = 64 & 128). Notably, our method\nretains just 1.5% of the KV cache while achieving 97% of the performance of the\nfull KV cache on the contextual question answering benchmark.Codes are\navailable at https://github.com/FYYFU/HeadKV"
                },
                "authors": [
                    {
                        "name": "Yu Fu"
                    },
                    {
                        "name": "Zefan Cai"
                    },
                    {
                        "name": "Abedelkadir Asi"
                    },
                    {
                        "name": "Wayne Xiong"
                    },
                    {
                        "name": "Yue Dong"
                    },
                    {
                        "name": "Wen Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Wen Xiao"
                },
                "author": "Wen Xiao",
                "arxiv_comment": "18pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19258v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19258v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.08672v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.08672v1",
                "updated": "2024-11-13T15:07:15Z",
                "updated_parsed": [
                    2024,
                    11,
                    13,
                    15,
                    7,
                    15,
                    2,
                    318,
                    0
                ],
                "published": "2024-11-13T15:07:15Z",
                "published_parsed": [
                    2024,
                    11,
                    13,
                    15,
                    7,
                    15,
                    2,
                    318,
                    0
                ],
                "title": "Joint Model Caching and Resource Allocation in Generative AI-Enabled\n  Wireless Edge Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Joint Model Caching and Resource Allocation in Generative AI-Enabled\n  Wireless Edge Networks"
                },
                "summary": "With the rapid advancement of artificial intelligence (AI), generative AI\n(GenAI) has emerged as a transformative tool, enabling customized and\npersonalized AI-generated content (AIGC) services. However, GenAI models with\nbillions of parameters require substantial memory capacity and computational\npower for deployment and execution, presenting significant challenges to\nresource-limited edge networks. In this paper, we address the joint model\ncaching and resource allocation problem in GenAI-enabled wireless edge\nnetworks. Our objective is to balance the trade-off between delivering\nhigh-quality AIGC and minimizing the delay in AIGC service provisioning. To\ntackle this problem, we employ a deep deterministic policy gradient\n(DDPG)-based reinforcement learning approach, capable of efficiently\ndetermining optimal model caching and resource allocation decisions for AIGC\nservices in response to user mobility and time-varying channel conditions.\nNumerical results demonstrate that DDPG achieves a higher model hit ratio and\nprovides superior-quality, lower-latency AIGC services compared to other\nbenchmark solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid advancement of artificial intelligence (AI), generative AI\n(GenAI) has emerged as a transformative tool, enabling customized and\npersonalized AI-generated content (AIGC) services. However, GenAI models with\nbillions of parameters require substantial memory capacity and computational\npower for deployment and execution, presenting significant challenges to\nresource-limited edge networks. In this paper, we address the joint model\ncaching and resource allocation problem in GenAI-enabled wireless edge\nnetworks. Our objective is to balance the trade-off between delivering\nhigh-quality AIGC and minimizing the delay in AIGC service provisioning. To\ntackle this problem, we employ a deep deterministic policy gradient\n(DDPG)-based reinforcement learning approach, capable of efficiently\ndetermining optimal model caching and resource allocation decisions for AIGC\nservices in response to user mobility and time-varying channel conditions.\nNumerical results demonstrate that DDPG achieves a higher model hit ratio and\nprovides superior-quality, lower-latency AIGC services compared to other\nbenchmark solutions."
                },
                "authors": [
                    {
                        "name": "Zhang Liu"
                    },
                    {
                        "name": "Hongyang Du"
                    },
                    {
                        "name": "Lianfen Huang"
                    },
                    {
                        "name": "Zhibin Gao"
                    },
                    {
                        "name": "Dusit Niyato"
                    }
                ],
                "author_detail": {
                    "name": "Dusit Niyato"
                },
                "author": "Dusit Niyato",
                "arxiv_comment": "conference paper with 6 pages and 5 figures. arXiv admin note: text\n  overlap with arXiv:2411.01458",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.08672v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.08672v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.08312v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.08312v1",
                "updated": "2024-11-13T03:28:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    13,
                    3,
                    28,
                    44,
                    2,
                    318,
                    0
                ],
                "published": "2024-11-13T03:28:44Z",
                "published_parsed": [
                    2024,
                    11,
                    13,
                    3,
                    28,
                    44,
                    2,
                    318,
                    0
                ],
                "title": "A Novel Extensible Simulation Framework for CXL-Enabled Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Novel Extensible Simulation Framework for CXL-Enabled Systems"
                },
                "summary": "Compute Express Link (CXL) serves as a rising industry standard, delivering\nhigh-speed cache-coherent links to a variety of devices, including host CPUs,\ncomputational accelerators, and memory devices. It is designed to promote\nsystem scalability, enable peer-to-peer exchanges, and accelerate data\ntransmissions. To achieve these objectives, the most recent CXL protocol has\nbrought forth several innovative features, such as port-focused routing,\ndevice-handled coherence, and PCIe 6.0 compatibility. However, due to the\nlimited availability of hardware prototypes and simulators compatible with CXL,\nearlier CXL research has largely depended on emulating CXL devices using remote\nNUMA nodes. Unfortunately, these NUMA-based emulators have difficulties in\naccurately representing the new features due to fundamental differences in\nhardware and protocols. Moreover, the absence of support for non-tree topology\nand PCIe links makes it complex to merely adapt existing simulators for CXL\nsimulation. To overcome these problems, we introduce ESF, a simulation\nframework specifically designed for CXL systems. ESF has been developed to\naccurately reflect the unique features of the latest CXL protocol from the\nground up. It uses a specialized interconnect layer to facilitate connections\nwithin a wide range of system topologies and also includes key components to\ncarry out specific functions required by these features. By utilizing ESF, we\nthoroughly investigate various aspects of CXL systems, including system\ntopology, device-handled coherence, and the effects of PCIe characteristics,\nleading to important findings that can guide the creation of high-performance\nCXL systems. The ESF source codes are fully open-source and can be accessed at\nhttps://anonymous.4open.science/r/ESF-1CE3.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compute Express Link (CXL) serves as a rising industry standard, delivering\nhigh-speed cache-coherent links to a variety of devices, including host CPUs,\ncomputational accelerators, and memory devices. It is designed to promote\nsystem scalability, enable peer-to-peer exchanges, and accelerate data\ntransmissions. To achieve these objectives, the most recent CXL protocol has\nbrought forth several innovative features, such as port-focused routing,\ndevice-handled coherence, and PCIe 6.0 compatibility. However, due to the\nlimited availability of hardware prototypes and simulators compatible with CXL,\nearlier CXL research has largely depended on emulating CXL devices using remote\nNUMA nodes. Unfortunately, these NUMA-based emulators have difficulties in\naccurately representing the new features due to fundamental differences in\nhardware and protocols. Moreover, the absence of support for non-tree topology\nand PCIe links makes it complex to merely adapt existing simulators for CXL\nsimulation. To overcome these problems, we introduce ESF, a simulation\nframework specifically designed for CXL systems. ESF has been developed to\naccurately reflect the unique features of the latest CXL protocol from the\nground up. It uses a specialized interconnect layer to facilitate connections\nwithin a wide range of system topologies and also includes key components to\ncarry out specific functions required by these features. By utilizing ESF, we\nthoroughly investigate various aspects of CXL systems, including system\ntopology, device-handled coherence, and the effects of PCIe characteristics,\nleading to important findings that can guide the creation of high-performance\nCXL systems. The ESF source codes are fully open-source and can be accessed at\nhttps://anonymous.4open.science/r/ESF-1CE3."
                },
                "authors": [
                    {
                        "name": "Yuda An"
                    },
                    {
                        "name": "Shushu Yi"
                    },
                    {
                        "name": "Bo Mao"
                    },
                    {
                        "name": "Qiao Li"
                    },
                    {
                        "name": "Mingzhe Zhang"
                    },
                    {
                        "name": "Ke Zhou"
                    },
                    {
                        "name": "Nong Xiao"
                    },
                    {
                        "name": "Guangyu Sun"
                    },
                    {
                        "name": "Xiaolin Wang"
                    },
                    {
                        "name": "Yingwei Luo"
                    },
                    {
                        "name": "Jie Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Zhang"
                },
                "arxiv_affiliation": "Peking University",
                "author": "Jie Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.08312v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.08312v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.08203v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.08203v1",
                "updated": "2024-11-12T21:50:03Z",
                "updated_parsed": [
                    2024,
                    11,
                    12,
                    21,
                    50,
                    3,
                    1,
                    317,
                    0
                ],
                "published": "2024-11-12T21:50:03Z",
                "published_parsed": [
                    2024,
                    11,
                    12,
                    21,
                    50,
                    3,
                    1,
                    317,
                    0
                ],
                "title": "FaaS and Furious: abstractions and differential caching for efficient\n  data pre-processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FaaS and Furious: abstractions and differential caching for efficient\n  data pre-processing"
                },
                "summary": "Data pre-processing pipelines are the bread and butter of any successful AI\nproject. We introduce a novel programming model for pipelines in a data\nlakehouse, allowing users to interact declaratively with assets in object\nstorage. Motivated by real-world industry usage patterns, we exploit these new\nabstractions with a columnar and differential cache to maximize iteration speed\nfor data scientists, who spent most of their time in pre-processing - adding or\nremoving features, restricting or relaxing time windows, wrangling current or\nolder datasets. We show how the new cache works transparently across\nprogramming languages, schemas and time windows, and provide preliminary\nevidence on its efficiency on standard data workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data pre-processing pipelines are the bread and butter of any successful AI\nproject. We introduce a novel programming model for pipelines in a data\nlakehouse, allowing users to interact declaratively with assets in object\nstorage. Motivated by real-world industry usage patterns, we exploit these new\nabstractions with a columnar and differential cache to maximize iteration speed\nfor data scientists, who spent most of their time in pre-processing - adding or\nremoving features, restricting or relaxing time windows, wrangling current or\nolder datasets. We show how the new cache works transparently across\nprogramming languages, schemas and time windows, and provide preliminary\nevidence on its efficiency on standard data workloads."
                },
                "authors": [
                    {
                        "name": "Jacopo Tagliabue"
                    },
                    {
                        "name": "Ryan Curtin"
                    },
                    {
                        "name": "Ciro Greco"
                    }
                ],
                "author_detail": {
                    "name": "Ciro Greco"
                },
                "author": "Ciro Greco",
                "arxiv_comment": "Pre-print of the paper accepted at DEMAI@IEEE Big Data 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.08203v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.08203v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.06219v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.06219v3",
                "updated": "2024-11-12T08:18:45Z",
                "updated_parsed": [
                    2024,
                    11,
                    12,
                    8,
                    18,
                    45,
                    1,
                    317,
                    0
                ],
                "published": "2024-05-10T03:06:24Z",
                "published_parsed": [
                    2024,
                    5,
                    10,
                    3,
                    6,
                    24,
                    4,
                    131,
                    0
                ],
                "title": "SKVQ: Sliding-window Key and Value Cache Quantization for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SKVQ: Sliding-window Key and Value Cache Quantization for Large Language\n  Models"
                },
                "summary": "Large language models (LLMs) can now handle longer sequences of tokens,\nenabling complex tasks like book understanding and generating lengthy novels.\nHowever, the key-value (KV) cache required for LLMs consumes substantial memory\nas context length increasing, becoming the bottleneck for deployment. In this\npaper, we present a strategy called SKVQ, which stands for sliding-window KV\ncache quantization, to address the issue of extremely low bitwidth KV cache\nquantization. To achieve this, SKVQ rearranges the channels of the KV cache in\norder to improve the similarity of channels in quantization groups, and applies\nclipped dynamic quantization at the group level. Additionally, SKVQ ensures\nthat the most recent window tokens in the KV cache are preserved with high\nprecision. This helps maintain the accuracy of a small but important portion of\nthe KV cache.SKVQ achieves high compression ratios while maintaining accuracy.\nOur evaluation on LLMs demonstrates that SKVQ surpasses previous quantization\napproaches, allowing for quantization of the KV cache to 2-bit keys and 1.5-bit\nvalues with minimal loss of accuracy. With SKVQ, it is possible to process\ncontext lengths of up to 1M on an 80GB memory GPU for a 7b model and up to 7\ntimes faster decoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) can now handle longer sequences of tokens,\nenabling complex tasks like book understanding and generating lengthy novels.\nHowever, the key-value (KV) cache required for LLMs consumes substantial memory\nas context length increasing, becoming the bottleneck for deployment. In this\npaper, we present a strategy called SKVQ, which stands for sliding-window KV\ncache quantization, to address the issue of extremely low bitwidth KV cache\nquantization. To achieve this, SKVQ rearranges the channels of the KV cache in\norder to improve the similarity of channels in quantization groups, and applies\nclipped dynamic quantization at the group level. Additionally, SKVQ ensures\nthat the most recent window tokens in the KV cache are preserved with high\nprecision. This helps maintain the accuracy of a small but important portion of\nthe KV cache.SKVQ achieves high compression ratios while maintaining accuracy.\nOur evaluation on LLMs demonstrates that SKVQ surpasses previous quantization\napproaches, allowing for quantization of the KV cache to 2-bit keys and 1.5-bit\nvalues with minimal loss of accuracy. With SKVQ, it is possible to process\ncontext lengths of up to 1M on an 80GB memory GPU for a 7b model and up to 7\ntimes faster decoding."
                },
                "authors": [
                    {
                        "name": "Haojie Duanmu"
                    },
                    {
                        "name": "Zhihang Yuan"
                    },
                    {
                        "name": "Xiuhong Li"
                    },
                    {
                        "name": "Jiangfei Duan"
                    },
                    {
                        "name": "Xingcheng Zhang"
                    },
                    {
                        "name": "Dahua Lin"
                    }
                ],
                "author_detail": {
                    "name": "Dahua Lin"
                },
                "author": "Dahua Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.06219v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.06219v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07627v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07627v1",
                "updated": "2024-11-12T08:17:15Z",
                "updated_parsed": [
                    2024,
                    11,
                    12,
                    8,
                    17,
                    15,
                    1,
                    317,
                    0
                ],
                "published": "2024-11-12T08:17:15Z",
                "published_parsed": [
                    2024,
                    11,
                    12,
                    8,
                    17,
                    15,
                    1,
                    317,
                    0
                ],
                "title": "Leveraging Previous Steps: A Training-free Fast Solver for Flow\n  Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Previous Steps: A Training-free Fast Solver for Flow\n  Diffusion"
                },
                "summary": "Flow diffusion models (FDMs) have recently shown potential in generation\ntasks due to the high generation quality. However, the current ordinary\ndifferential equation (ODE) solver for FDMs, e.g., the Euler solver, still\nsuffers from slow generation since ODE solvers need many number function\nevaluations (NFE) to keep high-quality generation. In this paper, we propose a\nnovel training-free flow-solver to reduce NFE while maintaining high-quality\ngeneration. The key insight for the flow-solver is to leverage the previous\nsteps to reduce the NFE, where a cache is created to reuse these results from\nthe previous steps. Specifically, the Taylor expansion is first used to\napproximate the ODE. To calculate the high-order derivatives of Taylor\nexpansion, the flow-solver proposes to use the previous steps and a polynomial\ninterpolation to approximate it, where the number of orders we could\napproximate equals the number of previous steps we cached. We also prove that\nthe flow-solver has a more minor approximation error and faster generation\nspeed. Experimental results on the CIFAR-10, CelebA-HQ, LSUN-Bedroom,\nLSUN-Church, ImageNet, and real text-to-image generation prove the efficiency\nof the flow-solver. Specifically, the flow-solver improves the FID-30K from\n13.79 to 6.75, from 46.64 to 19.49 with $\\text{NFE}=10$ on CIFAR-10 and\nLSUN-Church, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flow diffusion models (FDMs) have recently shown potential in generation\ntasks due to the high generation quality. However, the current ordinary\ndifferential equation (ODE) solver for FDMs, e.g., the Euler solver, still\nsuffers from slow generation since ODE solvers need many number function\nevaluations (NFE) to keep high-quality generation. In this paper, we propose a\nnovel training-free flow-solver to reduce NFE while maintaining high-quality\ngeneration. The key insight for the flow-solver is to leverage the previous\nsteps to reduce the NFE, where a cache is created to reuse these results from\nthe previous steps. Specifically, the Taylor expansion is first used to\napproximate the ODE. To calculate the high-order derivatives of Taylor\nexpansion, the flow-solver proposes to use the previous steps and a polynomial\ninterpolation to approximate it, where the number of orders we could\napproximate equals the number of previous steps we cached. We also prove that\nthe flow-solver has a more minor approximation error and faster generation\nspeed. Experimental results on the CIFAR-10, CelebA-HQ, LSUN-Bedroom,\nLSUN-Church, ImageNet, and real text-to-image generation prove the efficiency\nof the flow-solver. Specifically, the flow-solver improves the FID-30K from\n13.79 to 6.75, from 46.64 to 19.49 with $\\text{NFE}=10$ on CIFAR-10 and\nLSUN-Church, respectively."
                },
                "authors": [
                    {
                        "name": "Kaiyu Song"
                    },
                    {
                        "name": "Hanjiang Lai"
                    }
                ],
                "author_detail": {
                    "name": "Hanjiang Lai"
                },
                "author": "Hanjiang Lai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07627v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07627v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06681v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06681v1",
                "updated": "2024-11-11T02:48:00Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    2,
                    48,
                    0,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T02:48:00Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    2,
                    48,
                    0,
                    0,
                    316,
                    0
                ],
                "title": "WDMoE: Wireless Distributed Mixture of Experts for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WDMoE: Wireless Distributed Mixture of Experts for Large Language Models"
                },
                "summary": "Large Language Models (LLMs) have achieved significant success in various\nnatural language processing tasks, but the role of wireless networks in\nsupporting LLMs has not been thoroughly explored. In this paper, we propose a\nwireless distributed Mixture of Experts (WDMoE) architecture to enable\ncollaborative deployment of LLMs across edge servers at the base station (BS)\nand mobile devices in wireless networks. Specifically, we decompose the MoE\nlayer in LLMs by placing the gating network and the preceding neural network\nlayer at BS, while distributing the expert networks among the devices. This\ndeployment leverages the parallel inference capabilities of expert networks on\nmobile devices, effectively utilizing the limited computing and caching\nresources of these devices. Accordingly, we develop a performance metric for\nWDMoE-based LLMs, which accounts for both model capability and latency. To\nminimize the latency while maintaining accuracy, we jointly optimize expert\nselection and bandwidth allocation based on the performance metric. Moreover,\nwe build a hardware testbed using NVIDIA Jetson kits to validate the\neffectiveness of WDMoE. Both theoretical simulations and practical hardware\nexperiments demonstrate that the proposed method can significantly reduce the\nlatency without compromising LLM performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved significant success in various\nnatural language processing tasks, but the role of wireless networks in\nsupporting LLMs has not been thoroughly explored. In this paper, we propose a\nwireless distributed Mixture of Experts (WDMoE) architecture to enable\ncollaborative deployment of LLMs across edge servers at the base station (BS)\nand mobile devices in wireless networks. Specifically, we decompose the MoE\nlayer in LLMs by placing the gating network and the preceding neural network\nlayer at BS, while distributing the expert networks among the devices. This\ndeployment leverages the parallel inference capabilities of expert networks on\nmobile devices, effectively utilizing the limited computing and caching\nresources of these devices. Accordingly, we develop a performance metric for\nWDMoE-based LLMs, which accounts for both model capability and latency. To\nminimize the latency while maintaining accuracy, we jointly optimize expert\nselection and bandwidth allocation based on the performance metric. Moreover,\nwe build a hardware testbed using NVIDIA Jetson kits to validate the\neffectiveness of WDMoE. Both theoretical simulations and practical hardware\nexperiments demonstrate that the proposed method can significantly reduce the\nlatency without compromising LLM performance."
                },
                "authors": [
                    {
                        "name": "Nan Xue"
                    },
                    {
                        "name": "Yaping Sun"
                    },
                    {
                        "name": "Zhiyong Chen"
                    },
                    {
                        "name": "Meixia Tao"
                    },
                    {
                        "name": "Xiaodong Xu"
                    },
                    {
                        "name": "Liang Qian"
                    },
                    {
                        "name": "Shuguang Cui"
                    },
                    {
                        "name": "Wenjun Zhang"
                    },
                    {
                        "name": "Ping Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ping Zhang"
                },
                "author": "Ping Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06681v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06681v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06680v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06680v1",
                "updated": "2024-11-11T02:47:05Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    2,
                    47,
                    5,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T02:47:05Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    2,
                    47,
                    5,
                    0,
                    316,
                    0
                ],
                "title": "Anchor Attention, Small Cache: Code Generation with Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Anchor Attention, Small Cache: Code Generation with Large Language\n  Models"
                },
                "summary": "The development of large language models (LLMs) has revolutionized automated\ncode generation. However, their high demand of computation resources has\nhindered a broader deployment and raised environmental concerns. A common\nstrategy for diminishing computational demands is to cache Key-Value (KV)\nstates from the attention mechanism which is adopted predominately by\nmainstream LLMs. It can mitigate the need of repeated attention computations,\nbut brings significant memory overhead. Current practices in NLP often use\nsparse attention which may, unfortunately, lead to substantial inaccuracies, or\nhallucinations, in code generation tasks. In this paper, we analyze the\nattention weights distribution within code generation models via an empirical\nstudy, uncovering a sparsity pattern, i.e., the aggregation of information at\nspecific anchor points. Based on this observation, we propose a novel approach,\nAnchorCoder, which features token-wise anchor attention designed to extract and\ncompress the contextual information, and layer-wise anchor attention enabling\ncross-layer communication to mitigate the issue of excessive superposition\ncaused by the compression. The extensive experiments across multiple benchmark\ndatasets confirm the effectiveness of AnchorCoder, which can consistently\nachieve a significant (at least 70%) reduction in KV cache requirements, while\npreserving the majority of model's performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The development of large language models (LLMs) has revolutionized automated\ncode generation. However, their high demand of computation resources has\nhindered a broader deployment and raised environmental concerns. A common\nstrategy for diminishing computational demands is to cache Key-Value (KV)\nstates from the attention mechanism which is adopted predominately by\nmainstream LLMs. It can mitigate the need of repeated attention computations,\nbut brings significant memory overhead. Current practices in NLP often use\nsparse attention which may, unfortunately, lead to substantial inaccuracies, or\nhallucinations, in code generation tasks. In this paper, we analyze the\nattention weights distribution within code generation models via an empirical\nstudy, uncovering a sparsity pattern, i.e., the aggregation of information at\nspecific anchor points. Based on this observation, we propose a novel approach,\nAnchorCoder, which features token-wise anchor attention designed to extract and\ncompress the contextual information, and layer-wise anchor attention enabling\ncross-layer communication to mitigate the issue of excessive superposition\ncaused by the compression. The extensive experiments across multiple benchmark\ndatasets confirm the effectiveness of AnchorCoder, which can consistently\nachieve a significant (at least 70%) reduction in KV cache requirements, while\npreserving the majority of model's performance."
                },
                "authors": [
                    {
                        "name": "Xiangyu Zhang"
                    },
                    {
                        "name": "Yu Zhou"
                    },
                    {
                        "name": "Guang Yang"
                    },
                    {
                        "name": "Harald C. Gall"
                    },
                    {
                        "name": "Taolue Chen"
                    }
                ],
                "author_detail": {
                    "name": "Taolue Chen"
                },
                "author": "Taolue Chen",
                "arxiv_comment": "14 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06680v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06680v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68N19",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06659v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06659v1",
                "updated": "2024-11-11T01:53:14Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    1,
                    53,
                    14,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T01:53:14Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    1,
                    53,
                    14,
                    0,
                    316,
                    0
                ],
                "title": "An Efficient Memory Module for Graph Few-Shot Class-Incremental Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Efficient Memory Module for Graph Few-Shot Class-Incremental Learning"
                },
                "summary": "Incremental graph learning has gained significant attention for its ability\nto address the catastrophic forgetting problem in graph representation\nlearning. However, traditional methods often rely on a large number of labels\nfor node classification, which is impractical in real-world applications. This\nmakes few-shot incremental learning on graphs a pressing need. Current methods\ntypically require extensive training samples from meta-learning to build memory\nand perform intensive fine-tuning of GNN parameters, leading to high memory\nconsumption and potential loss of previously learned knowledge. To tackle these\nchallenges, we introduce Mecoin, an efficient method for building and\nmaintaining memory. Mecoin employs Structured Memory Units to cache prototypes\nof learned categories, as well as Memory Construction Modules to update these\nprototypes for new categories through interactions between the nodes and the\ncached prototypes. Additionally, we have designed a Memory Representation\nAdaptation Module to store probabilities associated with each class prototype,\nreducing the need for parameter fine-tuning and lowering the forgetting rate.\nWhen a sample matches its corresponding class prototype, the relevant\nprobabilities are retrieved from the MRaM. Knowledge is then distilled back\ninto the GNN through a Graph Knowledge Distillation Module, preserving the\nmodel's memory. We analyze the effectiveness of Mecoin in terms of\ngeneralization error and explore the impact of different distillation\nstrategies on model performance through experiments and VC-dimension analysis.\nCompared to other related works, Mecoin shows superior performance in accuracy\nand forgetting rate. Our code is publicly available on the\nhttps://github.com/Arvin0313/Mecoin-GFSCIL.git .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Incremental graph learning has gained significant attention for its ability\nto address the catastrophic forgetting problem in graph representation\nlearning. However, traditional methods often rely on a large number of labels\nfor node classification, which is impractical in real-world applications. This\nmakes few-shot incremental learning on graphs a pressing need. Current methods\ntypically require extensive training samples from meta-learning to build memory\nand perform intensive fine-tuning of GNN parameters, leading to high memory\nconsumption and potential loss of previously learned knowledge. To tackle these\nchallenges, we introduce Mecoin, an efficient method for building and\nmaintaining memory. Mecoin employs Structured Memory Units to cache prototypes\nof learned categories, as well as Memory Construction Modules to update these\nprototypes for new categories through interactions between the nodes and the\ncached prototypes. Additionally, we have designed a Memory Representation\nAdaptation Module to store probabilities associated with each class prototype,\nreducing the need for parameter fine-tuning and lowering the forgetting rate.\nWhen a sample matches its corresponding class prototype, the relevant\nprobabilities are retrieved from the MRaM. Knowledge is then distilled back\ninto the GNN through a Graph Knowledge Distillation Module, preserving the\nmodel's memory. We analyze the effectiveness of Mecoin in terms of\ngeneralization error and explore the impact of different distillation\nstrategies on model performance through experiments and VC-dimension analysis.\nCompared to other related works, Mecoin shows superior performance in accuracy\nand forgetting rate. Our code is publicly available on the\nhttps://github.com/Arvin0313/Mecoin-GFSCIL.git ."
                },
                "authors": [
                    {
                        "name": "Dong Li"
                    },
                    {
                        "name": "Aijia Zhang"
                    },
                    {
                        "name": "Junqi Gao"
                    },
                    {
                        "name": "Biqing Qi"
                    }
                ],
                "author_detail": {
                    "name": "Biqing Qi"
                },
                "author": "Biqing Qi",
                "arxiv_comment": "16 pages, 6 figures, 38th Conference on Neural Information Processing\n  Systems, 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06659v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06659v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01783v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01783v2",
                "updated": "2024-11-10T23:04:12Z",
                "updated_parsed": [
                    2024,
                    11,
                    10,
                    23,
                    4,
                    12,
                    6,
                    315,
                    0
                ],
                "published": "2024-11-04T04:15:36Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    4,
                    15,
                    36,
                    0,
                    309,
                    0
                ],
                "title": "Context Parallelism for Scalable Million-Token Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context Parallelism for Scalable Million-Token Inference"
                },
                "summary": "We present context parallelism for long-context large language model\ninference, which achieves near-linear scaling for long-context prefill latency\nwith up to 128 H100 GPUs across 16 nodes. Particularly, our method achieves 1M\ncontext prefill with Llama3 405B model in 77s (93% parallelization efficiency,\n63% FLOPS utilization) and 128K context prefill in 3.8s. We develop two\nlossless exact ring attention variants: pass-KV and pass-Q to cover a wide\nrange of use cases with the state-of-the-art performance: full prefill,\npersistent KV prefill and decode. Benchmarks on H100 GPU hosts inter-connected\nwith RDMA and TCP both show similar scalability for long-context prefill,\ndemonstrating that our method scales well using common commercial data center\nwith medium-to-low inter-host bandwidth.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present context parallelism for long-context large language model\ninference, which achieves near-linear scaling for long-context prefill latency\nwith up to 128 H100 GPUs across 16 nodes. Particularly, our method achieves 1M\ncontext prefill with Llama3 405B model in 77s (93% parallelization efficiency,\n63% FLOPS utilization) and 128K context prefill in 3.8s. We develop two\nlossless exact ring attention variants: pass-KV and pass-Q to cover a wide\nrange of use cases with the state-of-the-art performance: full prefill,\npersistent KV prefill and decode. Benchmarks on H100 GPU hosts inter-connected\nwith RDMA and TCP both show similar scalability for long-context prefill,\ndemonstrating that our method scales well using common commercial data center\nwith medium-to-low inter-host bandwidth."
                },
                "authors": [
                    {
                        "name": "Amy Yang"
                    },
                    {
                        "name": "Jingyi Yang"
                    },
                    {
                        "name": "Aya Ibrahim"
                    },
                    {
                        "name": "Xinfeng Xie"
                    },
                    {
                        "name": "Bangsheng Tang"
                    },
                    {
                        "name": "Grigory Sizov"
                    },
                    {
                        "name": "Jeremy Reizenstein"
                    },
                    {
                        "name": "Jongsoo Park"
                    },
                    {
                        "name": "Jianyu Huang"
                    }
                ],
                "author_detail": {
                    "name": "Jianyu Huang"
                },
                "author": "Jianyu Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01783v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01783v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.14396v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.14396v4",
                "updated": "2024-11-10T15:58:07Z",
                "updated_parsed": [
                    2024,
                    11,
                    10,
                    15,
                    58,
                    7,
                    6,
                    315,
                    0
                ],
                "published": "2023-12-22T02:52:59Z",
                "published_parsed": [
                    2023,
                    12,
                    22,
                    2,
                    52,
                    59,
                    4,
                    356,
                    0
                ],
                "title": "GastCoCo: Graph Storage and Coroutine-Based Prefetch Co-Design for\n  Dynamic Graph Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GastCoCo: Graph Storage and Coroutine-Based Prefetch Co-Design for\n  Dynamic Graph Processing"
                },
                "summary": "An efficient data structure is fundamental to meeting the growing demands in\ndynamic graph processing. However, the dual requirements for graph computation\nefficiency (with contiguous structures) and graph update efficiency (with\nlinked list-like structures) present a conflict in the design principles of\ngraph structures. After experimental studies of existing state-of-the-art\ndynamic graph structures, we observe that the overhead of cache misses accounts\nfor a major portion of the graph computation time. This paper presents\nGastCoCo, a system with graph storage and coroutine-based prefetch co-design.\nBy employing software prefetching via stackless coroutines and introducing a\nprefetch-friendly data structure CBList, GastCoCo significantly alleviates the\nperformance degradation caused by cache misses. Our results show that GastCoCo\noutperforms state-of-the-art graph storage systems by 1.3x - 180x in graph\nupdates and 1.4x - 41.1x in graph computation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An efficient data structure is fundamental to meeting the growing demands in\ndynamic graph processing. However, the dual requirements for graph computation\nefficiency (with contiguous structures) and graph update efficiency (with\nlinked list-like structures) present a conflict in the design principles of\ngraph structures. After experimental studies of existing state-of-the-art\ndynamic graph structures, we observe that the overhead of cache misses accounts\nfor a major portion of the graph computation time. This paper presents\nGastCoCo, a system with graph storage and coroutine-based prefetch co-design.\nBy employing software prefetching via stackless coroutines and introducing a\nprefetch-friendly data structure CBList, GastCoCo significantly alleviates the\nperformance degradation caused by cache misses. Our results show that GastCoCo\noutperforms state-of-the-art graph storage systems by 1.3x - 180x in graph\nupdates and 1.4x - 41.1x in graph computation."
                },
                "authors": [
                    {
                        "name": "Hongfu Li"
                    }
                ],
                "author_detail": {
                    "name": "Hongfu Li"
                },
                "author": "Hongfu Li",
                "arxiv_comment": "This paper was accepted by VLDB2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.14396v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.14396v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.04873v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.04873v2",
                "updated": "2024-11-10T10:08:37Z",
                "updated_parsed": [
                    2024,
                    11,
                    10,
                    10,
                    8,
                    37,
                    6,
                    315,
                    0
                ],
                "published": "2024-06-07T12:12:25Z",
                "published_parsed": [
                    2024,
                    6,
                    7,
                    12,
                    12,
                    25,
                    4,
                    159,
                    0
                ],
                "title": "Ada-VE: Training-Free Consistent Video Editing Using Adaptive Motion\n  Prior",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ada-VE: Training-Free Consistent Video Editing Using Adaptive Motion\n  Prior"
                },
                "summary": "Video-to-video synthesis poses significant challenges in maintaining\ncharacter consistency, smooth temporal transitions, and preserving visual\nquality during fast motion. While recent fully cross-frame self-attention\nmechanisms have improved character consistency across multiple frames, they\ncome with high computational costs and often include redundant operations,\nespecially for videos with higher frame rates. To address these inefficiencies,\nwe propose an adaptive motion-guided cross-frame attention mechanism that\nselectively reduces redundant computations. This enables a greater number of\ncross-frame attentions over more frames within the same computational budget,\nthereby enhancing both video quality and temporal coherence. Our method\nleverages optical flow to focus on moving regions while sparsely attending to\nstationary areas, allowing for the joint editing of more frames without\nincreasing computational demands. Traditional frame interpolation techniques\nstruggle with motion blur and flickering in intermediate frames, which\ncompromises visual fidelity. To mitigate this, we introduce KV-caching for\njointly edited frames, reusing keys and values across intermediate frames to\npreserve visual quality and maintain temporal consistency throughout the video.\nWith our adaptive cross-frame self-attention approach, we achieve a threefold\nincrease in the number of keyframes processed compared to existing methods, all\nwithin the same computational budget as fully cross-frame attention baselines.\nThis results in significant improvements in prediction accuracy and temporal\nconsistency, outperforming state-of-the-art approaches. Code will be made\npublicly available at https://github.com/tanvir-utexas/AdaVE/tree/main",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video-to-video synthesis poses significant challenges in maintaining\ncharacter consistency, smooth temporal transitions, and preserving visual\nquality during fast motion. While recent fully cross-frame self-attention\nmechanisms have improved character consistency across multiple frames, they\ncome with high computational costs and often include redundant operations,\nespecially for videos with higher frame rates. To address these inefficiencies,\nwe propose an adaptive motion-guided cross-frame attention mechanism that\nselectively reduces redundant computations. This enables a greater number of\ncross-frame attentions over more frames within the same computational budget,\nthereby enhancing both video quality and temporal coherence. Our method\nleverages optical flow to focus on moving regions while sparsely attending to\nstationary areas, allowing for the joint editing of more frames without\nincreasing computational demands. Traditional frame interpolation techniques\nstruggle with motion blur and flickering in intermediate frames, which\ncompromises visual fidelity. To mitigate this, we introduce KV-caching for\njointly edited frames, reusing keys and values across intermediate frames to\npreserve visual quality and maintain temporal consistency throughout the video.\nWith our adaptive cross-frame self-attention approach, we achieve a threefold\nincrease in the number of keyframes processed compared to existing methods, all\nwithin the same computational budget as fully cross-frame attention baselines.\nThis results in significant improvements in prediction accuracy and temporal\nconsistency, outperforming state-of-the-art approaches. Code will be made\npublicly available at https://github.com/tanvir-utexas/AdaVE/tree/main"
                },
                "authors": [
                    {
                        "name": "Tanvir Mahmud"
                    },
                    {
                        "name": "Mustafa Munir"
                    },
                    {
                        "name": "Radu Marculescu"
                    },
                    {
                        "name": "Diana Marculescu"
                    }
                ],
                "author_detail": {
                    "name": "Diana Marculescu"
                },
                "author": "Diana Marculescu",
                "arxiv_comment": "Accepted in WACV 2025. Project page:\n  https://tanvir-utexas.github.io/AdaVE_Demo/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.04873v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.04873v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06364v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06364v1",
                "updated": "2024-11-10T05:12:51Z",
                "updated_parsed": [
                    2024,
                    11,
                    10,
                    5,
                    12,
                    51,
                    6,
                    315,
                    0
                ],
                "published": "2024-11-10T05:12:51Z",
                "published_parsed": [
                    2024,
                    11,
                    10,
                    5,
                    12,
                    51,
                    6,
                    315,
                    0
                ],
                "title": "EcoServe: Maximizing Multi-Resource Utilization with SLO Guarantees in\n  LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EcoServe: Maximizing Multi-Resource Utilization with SLO Guarantees in\n  LLM Serving"
                },
                "summary": "As Large Language Models (LLMs) continue to grow, reducing costs and\nalleviating GPU demands has become increasingly critical. However, existing\nschedulers primarily target either GPU compute or Key-Value Cache (KVC)\nutilization, failing to fully optimize both GPU compute and KVC usage during\neach iteration or guarantee timely KVC allocations when needed. To address\nthese challenges, we conducted a trace-based experimental analysis and made\ninsightful observations, leading to the design of a system called EcoServe.\nEcoServe maximizes multi-resource utilization while ensuring service-level\nobjective (SLO) guarantees in LLM serving. To enable adding prompts to a batch\nto maximize GPU utilization in each iteration, EcoServe maintains separate\nwaiting queues for prompt processing tasks (PTs) and generation tasks (GTs). It\nbatches GTs with the same predicted response lengths (RL) to save scheduling\ntime and allocates KVC space for the predicted RL to avoid KVC allocation\nfailures. It further has a novel KVC pipelining method, allowing sharing\nallocated but unused KVC space to enhance KVC utilization. In addition, it\nprioritizes queued requests that occupy more KVC to release KVC earlier and\nsatisfy request service-level-objective (SLO). Experimental results demonstrate\nthat EcoServe increases throughput by up to 4$\\times$ with the same level of\nlatency, generates up to 91\\% lower job completion time and up to 91\\% higher\nSLO satisfaction ratio compared to vLLM. It also reduces the number of GPUs\nused in DistServe by up to 78\\% while maintaining the same level of goodput.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) continue to grow, reducing costs and\nalleviating GPU demands has become increasingly critical. However, existing\nschedulers primarily target either GPU compute or Key-Value Cache (KVC)\nutilization, failing to fully optimize both GPU compute and KVC usage during\neach iteration or guarantee timely KVC allocations when needed. To address\nthese challenges, we conducted a trace-based experimental analysis and made\ninsightful observations, leading to the design of a system called EcoServe.\nEcoServe maximizes multi-resource utilization while ensuring service-level\nobjective (SLO) guarantees in LLM serving. To enable adding prompts to a batch\nto maximize GPU utilization in each iteration, EcoServe maintains separate\nwaiting queues for prompt processing tasks (PTs) and generation tasks (GTs). It\nbatches GTs with the same predicted response lengths (RL) to save scheduling\ntime and allocates KVC space for the predicted RL to avoid KVC allocation\nfailures. It further has a novel KVC pipelining method, allowing sharing\nallocated but unused KVC space to enhance KVC utilization. In addition, it\nprioritizes queued requests that occupy more KVC to release KVC earlier and\nsatisfy request service-level-objective (SLO). Experimental results demonstrate\nthat EcoServe increases throughput by up to 4$\\times$ with the same level of\nlatency, generates up to 91\\% lower job completion time and up to 91\\% higher\nSLO satisfaction ratio compared to vLLM. It also reduces the number of GPUs\nused in DistServe by up to 78\\% while maintaining the same level of goodput."
                },
                "authors": [
                    {
                        "name": "Haiying Shen"
                    },
                    {
                        "name": "Tanmoy Sen"
                    }
                ],
                "author_detail": {
                    "name": "Tanmoy Sen"
                },
                "author": "Tanmoy Sen",
                "arxiv_comment": "14 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06364v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06364v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05646v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05646v2",
                "updated": "2024-11-08T16:29:33Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    16,
                    29,
                    33,
                    4,
                    313,
                    0
                ],
                "published": "2024-08-10T22:47:12Z",
                "published_parsed": [
                    2024,
                    8,
                    10,
                    22,
                    47,
                    12,
                    5,
                    223,
                    0
                ],
                "title": "Eigen Attention: Attention in Low-Rank Space for KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Eigen Attention: Attention in Low-Rank Space for KV Cache Compression"
                },
                "summary": "Large language models (LLMs) represent a groundbreaking advancement in the\ndomain of natural language processing due to their impressive reasoning\nabilities. Recently, there has been considerable interest in increasing the\ncontext lengths for these models to enhance their applicability to complex\ntasks. However, at long context lengths and large batch sizes, the key-value\n(KV) cache, which stores the attention keys and values, emerges as the new\nbottleneck in memory usage during inference. To address this, we propose Eigen\nAttention, which performs the attention operation in a low-rank space, thereby\nreducing the KV cache memory overhead. Our proposed approach is orthogonal to\nexisting KV cache compression techniques and can be used synergistically with\nthem. Through extensive experiments over OPT, MPT, and Llama model families, we\ndemonstrate that Eigen Attention results in up to 40% reduction in KV cache\nsizes and up to 60% reduction in attention operation latency with minimal drop\nin performance. Code is available at\nhttps://github.com/UtkarshSaxena1/EigenAttn.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) represent a groundbreaking advancement in the\ndomain of natural language processing due to their impressive reasoning\nabilities. Recently, there has been considerable interest in increasing the\ncontext lengths for these models to enhance their applicability to complex\ntasks. However, at long context lengths and large batch sizes, the key-value\n(KV) cache, which stores the attention keys and values, emerges as the new\nbottleneck in memory usage during inference. To address this, we propose Eigen\nAttention, which performs the attention operation in a low-rank space, thereby\nreducing the KV cache memory overhead. Our proposed approach is orthogonal to\nexisting KV cache compression techniques and can be used synergistically with\nthem. Through extensive experiments over OPT, MPT, and Llama model families, we\ndemonstrate that Eigen Attention results in up to 40% reduction in KV cache\nsizes and up to 60% reduction in attention operation latency with minimal drop\nin performance. Code is available at\nhttps://github.com/UtkarshSaxena1/EigenAttn."
                },
                "authors": [
                    {
                        "name": "Utkarsh Saxena"
                    },
                    {
                        "name": "Gobinda Saha"
                    },
                    {
                        "name": "Sakshi Choudhary"
                    },
                    {
                        "name": "Kaushik Roy"
                    }
                ],
                "author_detail": {
                    "name": "Kaushik Roy"
                },
                "author": "Kaushik Roy",
                "arxiv_comment": "12 page, 6 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05646v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05646v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05555v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05555v1",
                "updated": "2024-11-08T13:24:01Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    13,
                    24,
                    1,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-08T13:24:01Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    13,
                    24,
                    1,
                    4,
                    313,
                    0
                ],
                "title": "AcceLLM: Accelerating LLM Inference using Redundancy for Load Balancing\n  and Data Locality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AcceLLM: Accelerating LLM Inference using Redundancy for Load Balancing\n  and Data Locality"
                },
                "summary": "Large Language Model (LLM) inference on large-scale systems is expected to\ndominate future cloud infrastructures. Efficient LLM inference in cloud\nenvironments with numerous AI accelerators is challenging, necessitating\nextensive optimizations for optimal performance. Current systems batch prefill\nand decoding to boost throughput but encounter latency issues, while others\ndisaggregate these phases, leading to resource underutilization. We propose\nAcceLLM, a novel method addressing latency and load balancing, inspired by the\ncache data management. It strategically utilizes redundant data to enhance\ninference via load balancing and optimal hardware use. Simulated evaluations on\nNvidia H100 GPU and Huawei Ascend 910B2 show AcceLLM surpasses state-of-the-art\nsystems up to 30% in latency and efficiency, handling diverse workloads\neffectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) inference on large-scale systems is expected to\ndominate future cloud infrastructures. Efficient LLM inference in cloud\nenvironments with numerous AI accelerators is challenging, necessitating\nextensive optimizations for optimal performance. Current systems batch prefill\nand decoding to boost throughput but encounter latency issues, while others\ndisaggregate these phases, leading to resource underutilization. We propose\nAcceLLM, a novel method addressing latency and load balancing, inspired by the\ncache data management. It strategically utilizes redundant data to enhance\ninference via load balancing and optimal hardware use. Simulated evaluations on\nNvidia H100 GPU and Huawei Ascend 910B2 show AcceLLM surpasses state-of-the-art\nsystems up to 30% in latency and efficiency, handling diverse workloads\neffectively."
                },
                "authors": [
                    {
                        "name": "Ilias Bournias"
                    },
                    {
                        "name": "Lukas Cavigelli"
                    },
                    {
                        "name": "Georgios Zacharopoulos"
                    }
                ],
                "author_detail": {
                    "name": "Georgios Zacharopoulos"
                },
                "author": "Georgios Zacharopoulos",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05555v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05555v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05276v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05276v1",
                "updated": "2024-11-08T02:21:19Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    2,
                    21,
                    19,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-08T02:21:19Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    2,
                    21,
                    19,
                    4,
                    313,
                    0
                ],
                "title": "GPT Semantic Cache: Reducing LLM Costs and Latency via Semantic\n  Embedding Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPT Semantic Cache: Reducing LLM Costs and Latency via Semantic\n  Embedding Caching"
                },
                "summary": "Large Language Models (LLMs), such as GPT (Radford et al., 2019), have\nsignificantly advanced artificial intelligence by enabling sophisticated\nnatural language understanding and generation. However, the high computational\nand financial costs associated with frequent API calls to these models present\na substantial bottleneck, especially for applications like customer service\nchatbots that handle repetitive queries. In this paper, we introduce GPT\nSemantic Cache, a method that leverages semantic caching of query embeddings in\nin-memory storage (Redis). By storing embeddings of user queries, our approach\nefficiently identifies semantically similar questions, allowing for the\nretrieval of pre-generated responses without redundant API calls to the LLM.\nThis technique reduces operational costs and improves response times, enhancing\nthe efficiency of LLM-powered applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), such as GPT (Radford et al., 2019), have\nsignificantly advanced artificial intelligence by enabling sophisticated\nnatural language understanding and generation. However, the high computational\nand financial costs associated with frequent API calls to these models present\na substantial bottleneck, especially for applications like customer service\nchatbots that handle repetitive queries. In this paper, we introduce GPT\nSemantic Cache, a method that leverages semantic caching of query embeddings in\nin-memory storage (Redis). By storing embeddings of user queries, our approach\nefficiently identifies semantically similar questions, allowing for the\nretrieval of pre-generated responses without redundant API calls to the LLM.\nThis technique reduces operational costs and improves response times, enhancing\nthe efficiency of LLM-powered applications."
                },
                "authors": [
                    {
                        "name": "Sajal Regmi"
                    },
                    {
                        "name": "Chetan Phakami Pun"
                    }
                ],
                "author_detail": {
                    "name": "Chetan Phakami Pun"
                },
                "author": "Chetan Phakami Pun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05276v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05276v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.02542v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.02542v2",
                "updated": "2024-11-07T18:58:50Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    18,
                    58,
                    50,
                    3,
                    312,
                    0
                ],
                "published": "2024-06-04T17:58:03Z",
                "published_parsed": [
                    2024,
                    6,
                    4,
                    17,
                    58,
                    3,
                    1,
                    156,
                    0
                ],
                "title": "Loki: Low-rank Keys for Efficient Sparse Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Loki: Low-rank Keys for Efficient Sparse Attention"
                },
                "summary": "Inference on large language models (LLMs) can be expensive in terms of the\ncompute and memory costs involved, especially when long sequence lengths are\nused. In particular, the self-attention mechanism used in LLM inference\ncontributes significantly to these costs, which has sparked an interest in\napproximating the self-attention computation to reduce such costs. In this\nwork, we propose to approximate self-attention by focusing on the\ndimensionality of key vectors computed in the attention block. Our analysis\nreveals that key vectors lie in a significantly lower-dimensional space,\nconsistently across several datasets and models. Exploiting this observation,\nwe propose Loki, a novel sparse attention method that ranks and selects tokens\nin the KV-cache based on attention scores computed in low-dimensional space.\nOur evaluations show that Loki is able to speed up the attention computation\ndue to reduced data movement (load/store) and compute costs while maintaining\nthe efficacy of the models better than other popular approximation methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference on large language models (LLMs) can be expensive in terms of the\ncompute and memory costs involved, especially when long sequence lengths are\nused. In particular, the self-attention mechanism used in LLM inference\ncontributes significantly to these costs, which has sparked an interest in\napproximating the self-attention computation to reduce such costs. In this\nwork, we propose to approximate self-attention by focusing on the\ndimensionality of key vectors computed in the attention block. Our analysis\nreveals that key vectors lie in a significantly lower-dimensional space,\nconsistently across several datasets and models. Exploiting this observation,\nwe propose Loki, a novel sparse attention method that ranks and selects tokens\nin the KV-cache based on attention scores computed in low-dimensional space.\nOur evaluations show that Loki is able to speed up the attention computation\ndue to reduced data movement (load/store) and compute costs while maintaining\nthe efficacy of the models better than other popular approximation methods."
                },
                "authors": [
                    {
                        "name": "Prajwal Singhania"
                    },
                    {
                        "name": "Siddharth Singh"
                    },
                    {
                        "name": "Shwai He"
                    },
                    {
                        "name": "Soheil Feizi"
                    },
                    {
                        "name": "Abhinav Bhatele"
                    }
                ],
                "author_detail": {
                    "name": "Abhinav Bhatele"
                },
                "author": "Abhinav Bhatele",
                "arxiv_comment": "Proceedings of the Thirty-Eighth Annual Conference on Neural\n  Information Processing Systems (Main Conference Track)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.02542v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.02542v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04965v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04965v1",
                "updated": "2024-11-07T18:41:50Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    18,
                    41,
                    50,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T18:41:50Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    18,
                    41,
                    50,
                    3,
                    312,
                    0
                ],
                "title": "BitNet a4.8: 4-bit Activations for 1-bit LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BitNet a4.8: 4-bit Activations for 1-bit LLMs"
                },
                "summary": "Recent research on the 1-bit Large Language Models (LLMs), such as BitNet\nb1.58, presents a promising direction for reducing the inference cost of LLMs\nwhile maintaining their performance. In this work, we introduce BitNet a4.8,\nenabling 4-bit activations for 1-bit LLMs. BitNet a4.8 employs a hybrid\nquantization and sparsification strategy to mitigate the quantization errors\nintroduced by the outlier channels. Specifically, we utilize 4-bit activations\nfor inputs to the attention and feed-forward network layers, while sparsifying\nintermediate states followed with 8-bit quantization. Extensive experiments\ndemonstrate that BitNet a4.8 achieves performance comparable to BitNet b1.58\nwith equivalent training costs, while being faster in inference with enabling\n4-bit (INT4/FP4) kernels. Additionally, BitNet a4.8 activates only 55% of\nparameters and supports 3-bit KV cache, further enhancing the efficiency of\nlarge-scale LLM deployment and inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent research on the 1-bit Large Language Models (LLMs), such as BitNet\nb1.58, presents a promising direction for reducing the inference cost of LLMs\nwhile maintaining their performance. In this work, we introduce BitNet a4.8,\nenabling 4-bit activations for 1-bit LLMs. BitNet a4.8 employs a hybrid\nquantization and sparsification strategy to mitigate the quantization errors\nintroduced by the outlier channels. Specifically, we utilize 4-bit activations\nfor inputs to the attention and feed-forward network layers, while sparsifying\nintermediate states followed with 8-bit quantization. Extensive experiments\ndemonstrate that BitNet a4.8 achieves performance comparable to BitNet b1.58\nwith equivalent training costs, while being faster in inference with enabling\n4-bit (INT4/FP4) kernels. Additionally, BitNet a4.8 activates only 55% of\nparameters and supports 3-bit KV cache, further enhancing the efficiency of\nlarge-scale LLM deployment and inference."
                },
                "authors": [
                    {
                        "name": "Hongyu Wang"
                    },
                    {
                        "name": "Shuming Ma"
                    },
                    {
                        "name": "Furu Wei"
                    }
                ],
                "author_detail": {
                    "name": "Furu Wei"
                },
                "author": "Furu Wei",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04965v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04965v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02397v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02397v2",
                "updated": "2024-11-07T17:06:32Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    17,
                    6,
                    32,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-04T18:59:44Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    18,
                    59,
                    44,
                    0,
                    309,
                    0
                ],
                "title": "Adaptive Caching for Faster Video Generation with Diffusion Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Caching for Faster Video Generation with Diffusion Transformers"
                },
                "summary": "Generating temporally-consistent high-fidelity videos can be computationally\nexpensive, especially over longer temporal spans. More-recent Diffusion\nTransformers (DiTs) -- despite making significant headway in this context --\nhave only heightened such challenges as they rely on larger models and heavier\nattention mechanisms, resulting in slower inference speeds. In this paper, we\nintroduce a training-free method to accelerate video DiTs, termed Adaptive\nCaching (AdaCache), which is motivated by the fact that \"not all videos are\ncreated equal\": meaning, some videos require fewer denoising steps to attain a\nreasonable quality than others. Building on this, we not only cache\ncomputations through the diffusion process, but also devise a caching schedule\ntailored to each video generation, maximizing the quality-latency trade-off. We\nfurther introduce a Motion Regularization (MoReg) scheme to utilize video\ninformation within AdaCache, essentially controlling the compute allocation\nbased on motion content. Altogether, our plug-and-play contributions grant\nsignificant inference speedups (e.g. up to 4.7x on Open-Sora 720p - 2s video\ngeneration) without sacrificing the generation quality, across multiple video\nDiT baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating temporally-consistent high-fidelity videos can be computationally\nexpensive, especially over longer temporal spans. More-recent Diffusion\nTransformers (DiTs) -- despite making significant headway in this context --\nhave only heightened such challenges as they rely on larger models and heavier\nattention mechanisms, resulting in slower inference speeds. In this paper, we\nintroduce a training-free method to accelerate video DiTs, termed Adaptive\nCaching (AdaCache), which is motivated by the fact that \"not all videos are\ncreated equal\": meaning, some videos require fewer denoising steps to attain a\nreasonable quality than others. Building on this, we not only cache\ncomputations through the diffusion process, but also devise a caching schedule\ntailored to each video generation, maximizing the quality-latency trade-off. We\nfurther introduce a Motion Regularization (MoReg) scheme to utilize video\ninformation within AdaCache, essentially controlling the compute allocation\nbased on motion content. Altogether, our plug-and-play contributions grant\nsignificant inference speedups (e.g. up to 4.7x on Open-Sora 720p - 2s video\ngeneration) without sacrificing the generation quality, across multiple video\nDiT baselines."
                },
                "authors": [
                    {
                        "name": "Kumara Kahatapitiya"
                    },
                    {
                        "name": "Haozhe Liu"
                    },
                    {
                        "name": "Sen He"
                    },
                    {
                        "name": "Ding Liu"
                    },
                    {
                        "name": "Menglin Jia"
                    },
                    {
                        "name": "Chenyang Zhang"
                    },
                    {
                        "name": "Michael S. Ryoo"
                    },
                    {
                        "name": "Tian Xie"
                    }
                ],
                "author_detail": {
                    "name": "Tian Xie"
                },
                "author": "Tian Xie",
                "arxiv_comment": "Project-page is available at https://adacache-dit.github.io",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02397v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02397v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04762v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04762v1",
                "updated": "2024-11-07T14:59:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    14,
                    59,
                    44,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T14:59:44Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    14,
                    59,
                    44,
                    3,
                    312,
                    0
                ],
                "title": "JC5A: Service Delay Minimization for Aerial MEC-assisted Industrial\n  Cyber-Physical Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JC5A: Service Delay Minimization for Aerial MEC-assisted Industrial\n  Cyber-Physical Systems"
                },
                "summary": "In the era of the sixth generation (6G) and industrial Internet of Things\n(IIoT), an industrial cyber-physical system (ICPS) drives the proliferation of\nsensor devices and computing-intensive tasks. To address the limited resources\nof IIoT sensor devices, unmanned aerial vehicle (UAV)-assisted mobile edge\ncomputing (MEC) has emerged as a promising solution, providing flexible and\ncost-effective services in close proximity of IIoT sensor devices (ISDs).\nHowever, leveraging aerial MEC to meet the delay-sensitive and\ncomputation-intensive requirements of the ISDs could face several challenges,\nincluding the limited communication, computation and caching (3C) resources,\nstringent offloading requirements for 3C services, and constrained on-board\nenergy of UAVs. To address these issues, we first present a collaborative\naerial MEC-assisted ICPS architecture by incorporating the computing\ncapabilities of the macro base station (MBS) and UAVs. We then formulate a\nservice delay minimization optimization problem (SDMOP). Since the SDMOP is\nproved to be an NP-hard problem, we propose a joint computation offloading,\ncaching, communication resource allocation, computation resource allocation,\nand UAV trajectory control approach (JC5A). Specifically, JC5A consists of a\nblock successive upper bound minimization method of multipliers (BSUMM) for\ncomputation offloading and service caching, a convex optimization-based method\nfor communication and computation resource allocation, and a successive convex\napproximation (SCA)-based method for UAV trajectory control. Moreover, we\ntheoretically prove the convergence and polynomial complexity of JC5A.\nSimulation results demonstrate that the proposed approach can achieve superior\nsystem performance compared to the benchmark approaches and algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the era of the sixth generation (6G) and industrial Internet of Things\n(IIoT), an industrial cyber-physical system (ICPS) drives the proliferation of\nsensor devices and computing-intensive tasks. To address the limited resources\nof IIoT sensor devices, unmanned aerial vehicle (UAV)-assisted mobile edge\ncomputing (MEC) has emerged as a promising solution, providing flexible and\ncost-effective services in close proximity of IIoT sensor devices (ISDs).\nHowever, leveraging aerial MEC to meet the delay-sensitive and\ncomputation-intensive requirements of the ISDs could face several challenges,\nincluding the limited communication, computation and caching (3C) resources,\nstringent offloading requirements for 3C services, and constrained on-board\nenergy of UAVs. To address these issues, we first present a collaborative\naerial MEC-assisted ICPS architecture by incorporating the computing\ncapabilities of the macro base station (MBS) and UAVs. We then formulate a\nservice delay minimization optimization problem (SDMOP). Since the SDMOP is\nproved to be an NP-hard problem, we propose a joint computation offloading,\ncaching, communication resource allocation, computation resource allocation,\nand UAV trajectory control approach (JC5A). Specifically, JC5A consists of a\nblock successive upper bound minimization method of multipliers (BSUMM) for\ncomputation offloading and service caching, a convex optimization-based method\nfor communication and computation resource allocation, and a successive convex\napproximation (SCA)-based method for UAV trajectory control. Moreover, we\ntheoretically prove the convergence and polynomial complexity of JC5A.\nSimulation results demonstrate that the proposed approach can achieve superior\nsystem performance compared to the benchmark approaches and algorithms."
                },
                "authors": [
                    {
                        "name": "Geng Sun"
                    },
                    {
                        "name": "Jiaxu Wu"
                    },
                    {
                        "name": "Long He"
                    },
                    {
                        "name": "Jiacheng Wang"
                    },
                    {
                        "name": "Dusit Niyato"
                    },
                    {
                        "name": "Abbas Jamalipour"
                    },
                    {
                        "name": "Shiwen Mao"
                    }
                ],
                "author_detail": {
                    "name": "Shiwen Mao"
                },
                "author": "Shiwen Mao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04762v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04762v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.16591v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.16591v2",
                "updated": "2024-11-07T09:33:40Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    9,
                    33,
                    40,
                    3,
                    312,
                    0
                ],
                "published": "2024-05-26T14:50:40Z",
                "published_parsed": [
                    2024,
                    5,
                    26,
                    14,
                    50,
                    40,
                    6,
                    147,
                    0
                ],
                "title": "CapS-Adapter: Caption-based MultiModal Adapter in Zero-Shot\n  Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CapS-Adapter: Caption-based MultiModal Adapter in Zero-Shot\n  Classification"
                },
                "summary": "Recent advances in vision-language foundational models, such as CLIP, have\ndemonstrated significant strides in zero-shot classification. However, the\nextensive parameterization of models like CLIP necessitates a\nresource-intensive fine-tuning process. In response, TIP-Adapter and SuS-X have\nintroduced training-free methods aimed at bolstering the efficacy of downstream\ntasks. While these approaches incorporate support sets to maintain data\ndistribution consistency between knowledge cache and test sets, they often fall\nshort in terms of generalization on the test set, particularly when faced with\ntest data exhibiting substantial distributional variations. In this work, we\npresent CapS-Adapter, an innovative method that employs a caption-based support\nset, effectively harnessing both image and caption features to exceed existing\nstate-of-the-art techniques in training-free scenarios. CapS-Adapter adeptly\nconstructs support sets that closely mirror target distributions, utilizing\ninstance-level distribution features extracted from multimodal large models. By\nleveraging CLIP's single and cross-modal strengths, CapS-Adapter enhances\npredictive accuracy through the use of multimodal support sets. Our method\nachieves outstanding zero-shot classification results across 19 benchmark\ndatasets, improving accuracy by 2.19\\% over the previous leading method. Our\ncontributions are substantiated through extensive validation on multiple\nbenchmark datasets, demonstrating superior performance and robust\ngeneralization capabilities. Our code is made publicly available at\nhttps://github.com/WLuLi/CapS-Adapter.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in vision-language foundational models, such as CLIP, have\ndemonstrated significant strides in zero-shot classification. However, the\nextensive parameterization of models like CLIP necessitates a\nresource-intensive fine-tuning process. In response, TIP-Adapter and SuS-X have\nintroduced training-free methods aimed at bolstering the efficacy of downstream\ntasks. While these approaches incorporate support sets to maintain data\ndistribution consistency between knowledge cache and test sets, they often fall\nshort in terms of generalization on the test set, particularly when faced with\ntest data exhibiting substantial distributional variations. In this work, we\npresent CapS-Adapter, an innovative method that employs a caption-based support\nset, effectively harnessing both image and caption features to exceed existing\nstate-of-the-art techniques in training-free scenarios. CapS-Adapter adeptly\nconstructs support sets that closely mirror target distributions, utilizing\ninstance-level distribution features extracted from multimodal large models. By\nleveraging CLIP's single and cross-modal strengths, CapS-Adapter enhances\npredictive accuracy through the use of multimodal support sets. Our method\nachieves outstanding zero-shot classification results across 19 benchmark\ndatasets, improving accuracy by 2.19\\% over the previous leading method. Our\ncontributions are substantiated through extensive validation on multiple\nbenchmark datasets, demonstrating superior performance and robust\ngeneralization capabilities. Our code is made publicly available at\nhttps://github.com/WLuLi/CapS-Adapter."
                },
                "authors": [
                    {
                        "name": "Qijie Wang"
                    },
                    {
                        "name": "Guandu Liu"
                    },
                    {
                        "name": "Bin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Bin Wang"
                },
                "author": "Bin Wang",
                "arxiv_doi": "10.1145/3664647.3681566",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3664647.3681566",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.16591v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.16591v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "ACM Multimedia 2024 Poster",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01288v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01288v2",
                "updated": "2024-11-07T06:40:40Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    6,
                    40,
                    40,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-02T15:45:54Z",
                "published_parsed": [
                    2024,
                    11,
                    2,
                    15,
                    45,
                    54,
                    5,
                    307,
                    0
                ],
                "title": "HEXA-MoE: Efficient and Heterogeneous-aware MoE Acceleration with ZERO\n  Computation Redundancy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HEXA-MoE: Efficient and Heterogeneous-aware MoE Acceleration with ZERO\n  Computation Redundancy"
                },
                "summary": "Mixture-of-Experts (MoE) has emerged as a practical approach to scale up\nparameters for the Transformer model to achieve better generalization while\nmaintaining a sub-linear increase in computation overhead. Current MoE models\nare mainly built with expert parallelism on distributed devices. However, it\nusually depends on homogeneous devices to deploy and suffers from heavy\ncommunication overhead and computation redundancy. In this paper, we explore\ndeveloping a \\texttt{H}eterogeneous-aware \\texttt{EX}pert \\texttt{A}llocation\nframework, \\textbf{\\texttt{HEXA-MoE}}, with significantly enhanced computing\nefficiency. It contains two components: ($1$) \\textit{Expert-Specific\nOperators}. We replace the typical general matrix multiplication or grouped\nmatrix multiplication interfaces with our operators, which allows the computing\nto be performed in an in-place manner with \\textbf{ZERO} redundancy. ($2$)\n\\textit{Adaptive Data- and Model-Centric Configurations} for different workload\nscales. Specifically, we introduce a pipeline-shared cache on each device to\ntackle the heavy memory consumption in the existing data-centric MoE library.\nComprehensive experiments on the Swin-MoE benchmark consistently reveal the\neffectiveness of our \\texttt{HEXA-MoE} framework, i.e., reducing $10\\%\\sim48\\%$\nmemory consumption and achieving $0.5\\sim4.3\\times$ speed up compared to\ncurrent state-of-the-art MoE libraries. Furthermore, we examine our\n\\texttt{HEXA-MoE} with heterogeneous devices for both data- and model-centric\nsettings. Promising results show that employing optimal parallel configuration\nwith \\texttt{HEXA-MoE} on heterogeneous devices can substantially minimize\noverall latency. Codes are available at https://github.com/UNITES-Lab/HEXA-MoE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) has emerged as a practical approach to scale up\nparameters for the Transformer model to achieve better generalization while\nmaintaining a sub-linear increase in computation overhead. Current MoE models\nare mainly built with expert parallelism on distributed devices. However, it\nusually depends on homogeneous devices to deploy and suffers from heavy\ncommunication overhead and computation redundancy. In this paper, we explore\ndeveloping a \\texttt{H}eterogeneous-aware \\texttt{EX}pert \\texttt{A}llocation\nframework, \\textbf{\\texttt{HEXA-MoE}}, with significantly enhanced computing\nefficiency. It contains two components: ($1$) \\textit{Expert-Specific\nOperators}. We replace the typical general matrix multiplication or grouped\nmatrix multiplication interfaces with our operators, which allows the computing\nto be performed in an in-place manner with \\textbf{ZERO} redundancy. ($2$)\n\\textit{Adaptive Data- and Model-Centric Configurations} for different workload\nscales. Specifically, we introduce a pipeline-shared cache on each device to\ntackle the heavy memory consumption in the existing data-centric MoE library.\nComprehensive experiments on the Swin-MoE benchmark consistently reveal the\neffectiveness of our \\texttt{HEXA-MoE} framework, i.e., reducing $10\\%\\sim48\\%$\nmemory consumption and achieving $0.5\\sim4.3\\times$ speed up compared to\ncurrent state-of-the-art MoE libraries. Furthermore, we examine our\n\\texttt{HEXA-MoE} with heterogeneous devices for both data- and model-centric\nsettings. Promising results show that employing optimal parallel configuration\nwith \\texttt{HEXA-MoE} on heterogeneous devices can substantially minimize\noverall latency. Codes are available at https://github.com/UNITES-Lab/HEXA-MoE."
                },
                "authors": [
                    {
                        "name": "Shuqing Luo"
                    },
                    {
                        "name": "Jie Peng"
                    },
                    {
                        "name": "Pingzhi Li"
                    },
                    {
                        "name": "Tianlong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tianlong Chen"
                },
                "author": "Tianlong Chen",
                "arxiv_comment": "16 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01288v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01288v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02265v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02265v3",
                "updated": "2024-11-06T09:15:27Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    9,
                    15,
                    27,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-04T16:56:26Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    16,
                    56,
                    26,
                    0,
                    309,
                    0
                ],
                "title": "Hunyuan-Large: An Open-Source MoE Model with 52 Billion Activated\n  Parameters by Tencent",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hunyuan-Large: An Open-Source MoE Model with 52 Billion Activated\n  Parameters by Tencent"
                },
                "summary": "In this paper, we introduce Hunyuan-Large, which is currently the largest\nopen-source Transformer-based mixture of experts model, with a total of 389\nbillion parameters and 52 billion activation parameters, capable of handling up\nto 256K tokens. We conduct a thorough evaluation of Hunyuan-Large's superior\nperformance across various benchmarks including language understanding and\ngeneration, logical reasoning, mathematical problem-solving, coding,\nlong-context, and aggregated tasks, where it outperforms LLama3.1-70B and\nexhibits comparable performance when compared to the significantly larger\nLLama3.1-405B model. Key practice of Hunyuan-Large include large-scale\nsynthetic data that is orders larger than in previous literature, a mixed\nexpert routing strategy, a key-value cache compression technique, and an\nexpert-specific learning rate strategy. Additionally, we also investigate the\nscaling laws and learning rate schedule of mixture of experts models, providing\nvaluable insights and guidances for future model development and optimization.\nThe code and checkpoints of Hunyuan-Large are released to facilitate future\ninnovations and applications.\n  Codes: https://github.com/Tencent/Hunyuan-Large\n  Models: https://huggingface.co/tencent/Tencent-Hunyuan-Large",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce Hunyuan-Large, which is currently the largest\nopen-source Transformer-based mixture of experts model, with a total of 389\nbillion parameters and 52 billion activation parameters, capable of handling up\nto 256K tokens. We conduct a thorough evaluation of Hunyuan-Large's superior\nperformance across various benchmarks including language understanding and\ngeneration, logical reasoning, mathematical problem-solving, coding,\nlong-context, and aggregated tasks, where it outperforms LLama3.1-70B and\nexhibits comparable performance when compared to the significantly larger\nLLama3.1-405B model. Key practice of Hunyuan-Large include large-scale\nsynthetic data that is orders larger than in previous literature, a mixed\nexpert routing strategy, a key-value cache compression technique, and an\nexpert-specific learning rate strategy. Additionally, we also investigate the\nscaling laws and learning rate schedule of mixture of experts models, providing\nvaluable insights and guidances for future model development and optimization.\nThe code and checkpoints of Hunyuan-Large are released to facilitate future\ninnovations and applications.\n  Codes: https://github.com/Tencent/Hunyuan-Large\n  Models: https://huggingface.co/tencent/Tencent-Hunyuan-Large"
                },
                "authors": [
                    {
                        "name": "Xingwu Sun"
                    },
                    {
                        "name": "Yanfeng Chen"
                    },
                    {
                        "name": "Yiqing Huang"
                    },
                    {
                        "name": "Ruobing Xie"
                    },
                    {
                        "name": "Jiaqi Zhu"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Shuaipeng Li"
                    },
                    {
                        "name": "Zhen Yang"
                    },
                    {
                        "name": "Jonny Han"
                    },
                    {
                        "name": "Xiaobo Shu"
                    },
                    {
                        "name": "Jiahao Bu"
                    },
                    {
                        "name": "Zhongzhi Chen"
                    },
                    {
                        "name": "Xuemeng Huang"
                    },
                    {
                        "name": "Fengzong Lian"
                    },
                    {
                        "name": "Saiyong Yang"
                    },
                    {
                        "name": "Jianfeng Yan"
                    },
                    {
                        "name": "Yuyuan Zeng"
                    },
                    {
                        "name": "Xiaoqin Ren"
                    },
                    {
                        "name": "Chao Yu"
                    },
                    {
                        "name": "Lulu Wu"
                    },
                    {
                        "name": "Yue Mao"
                    },
                    {
                        "name": "Jun Xia"
                    },
                    {
                        "name": "Tao Yang"
                    },
                    {
                        "name": "Suncong Zheng"
                    },
                    {
                        "name": "Kan Wu"
                    },
                    {
                        "name": "Dian Jiao"
                    },
                    {
                        "name": "Jinbao Xue"
                    },
                    {
                        "name": "Xipeng Zhang"
                    },
                    {
                        "name": "Decheng Wu"
                    },
                    {
                        "name": "Kai Liu"
                    },
                    {
                        "name": "Dengpeng Wu"
                    },
                    {
                        "name": "Guanghui Xu"
                    },
                    {
                        "name": "Shaohua Chen"
                    },
                    {
                        "name": "Shuang Chen"
                    },
                    {
                        "name": "Xiao Feng"
                    },
                    {
                        "name": "Yigeng Hong"
                    },
                    {
                        "name": "Junqiang Zheng"
                    },
                    {
                        "name": "Chengcheng Xu"
                    },
                    {
                        "name": "Zongwei Li"
                    },
                    {
                        "name": "Xiong Kuang"
                    },
                    {
                        "name": "Jianglu Hu"
                    },
                    {
                        "name": "Yiqi Chen"
                    },
                    {
                        "name": "Yuchi Deng"
                    },
                    {
                        "name": "Guiyang Li"
                    },
                    {
                        "name": "Ao Liu"
                    },
                    {
                        "name": "Chenchen Zhang"
                    },
                    {
                        "name": "Shihui Hu"
                    },
                    {
                        "name": "Zilong Zhao"
                    },
                    {
                        "name": "Zifan Wu"
                    },
                    {
                        "name": "Yao Ding"
                    },
                    {
                        "name": "Weichao Wang"
                    },
                    {
                        "name": "Han Liu"
                    },
                    {
                        "name": "Roberts Wang"
                    },
                    {
                        "name": "Hao Fei"
                    },
                    {
                        "name": "Peijie Yu"
                    },
                    {
                        "name": "Ze Zhao"
                    },
                    {
                        "name": "Xun Cao"
                    },
                    {
                        "name": "Hai Wang"
                    },
                    {
                        "name": "Fusheng Xiang"
                    },
                    {
                        "name": "Mengyuan Huang"
                    },
                    {
                        "name": "Zhiyuan Xiong"
                    },
                    {
                        "name": "Bin Hu"
                    },
                    {
                        "name": "Xuebin Hou"
                    },
                    {
                        "name": "Lei Jiang"
                    },
                    {
                        "name": "Jianqiang Ma"
                    },
                    {
                        "name": "Jiajia Wu"
                    },
                    {
                        "name": "Yaping Deng"
                    },
                    {
                        "name": "Yi Shen"
                    },
                    {
                        "name": "Qian Wang"
                    },
                    {
                        "name": "Weijie Liu"
                    },
                    {
                        "name": "Jie Liu"
                    },
                    {
                        "name": "Meng Chen"
                    },
                    {
                        "name": "Liang Dong"
                    },
                    {
                        "name": "Weiwen Jia"
                    },
                    {
                        "name": "Hu Chen"
                    },
                    {
                        "name": "Feifei Liu"
                    },
                    {
                        "name": "Rui Yuan"
                    },
                    {
                        "name": "Huilin Xu"
                    },
                    {
                        "name": "Zhenxiang Yan"
                    },
                    {
                        "name": "Tengfei Cao"
                    },
                    {
                        "name": "Zhichao Hu"
                    },
                    {
                        "name": "Xinhua Feng"
                    },
                    {
                        "name": "Dong Du"
                    },
                    {
                        "name": "Tinghao Yu"
                    },
                    {
                        "name": "Yangyu Tao"
                    },
                    {
                        "name": "Feng Zhang"
                    },
                    {
                        "name": "Jianchen Zhu"
                    },
                    {
                        "name": "Chengzhong Xu"
                    },
                    {
                        "name": "Xirui Li"
                    },
                    {
                        "name": "Chong Zha"
                    },
                    {
                        "name": "Wen Ouyang"
                    },
                    {
                        "name": "Yinben Xia"
                    },
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Zekun He"
                    },
                    {
                        "name": "Rongpeng Chen"
                    },
                    {
                        "name": "Jiawei Song"
                    },
                    {
                        "name": "Ruibin Chen"
                    },
                    {
                        "name": "Fan Jiang"
                    },
                    {
                        "name": "Chongqing Zhao"
                    },
                    {
                        "name": "Bo Wang"
                    },
                    {
                        "name": "Hao Gong"
                    },
                    {
                        "name": "Rong Gan"
                    },
                    {
                        "name": "Winston Hu"
                    },
                    {
                        "name": "Zhanhui Kang"
                    },
                    {
                        "name": "Yong Yang"
                    },
                    {
                        "name": "Yuhong Liu"
                    },
                    {
                        "name": "Di Wang"
                    },
                    {
                        "name": "Jie Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Jiang"
                },
                "author": "Jie Jiang",
                "arxiv_comment": "17 pages, 4 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02265v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02265v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03731v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03731v1",
                "updated": "2024-11-06T07:53:04Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    7,
                    53,
                    4,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-06T07:53:04Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    7,
                    53,
                    4,
                    2,
                    311,
                    0
                ],
                "title": "Reducing Hyperparameter Tuning Costs in ML, Vision and Language Model\n  Training Pipelines via Memoization-Awareness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reducing Hyperparameter Tuning Costs in ML, Vision and Language Model\n  Training Pipelines via Memoization-Awareness"
                },
                "summary": "The training or fine-tuning of machine learning, vision, and language models\nis often implemented as a pipeline: a sequence of stages encompassing data\npreparation, model training and evaluation. In this paper, we exploit pipeline\nstructures to reduce the cost of hyperparameter tuning for model\ntraining/fine-tuning, which is particularly valuable for language models given\ntheir high costs in GPU-days. We propose a \"memoization-aware\" Bayesian\nOptimization (BO) algorithm, EEIPU, that works in tandem with a pipeline\ncaching system, allowing it to evaluate significantly more hyperparameter\ncandidates per GPU-day than other tuning algorithms. The result is\nbetter-quality hyperparameters in the same amount of search time, or\nequivalently, reduced search time to reach the same hyperparameter quality. In\nour benchmarks on machine learning (model ensembles), vision (convolutional\narchitecture) and language (T5 architecture) pipelines, we compare EEIPU\nagainst recent BO algorithms: EEIPU produces an average of $103\\%$ more\nhyperparameter candidates (within the same budget), and increases the\nvalidation metric by an average of $108\\%$ more than other algorithms (where\nthe increase is measured starting from the end of warm-up iterations).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The training or fine-tuning of machine learning, vision, and language models\nis often implemented as a pipeline: a sequence of stages encompassing data\npreparation, model training and evaluation. In this paper, we exploit pipeline\nstructures to reduce the cost of hyperparameter tuning for model\ntraining/fine-tuning, which is particularly valuable for language models given\ntheir high costs in GPU-days. We propose a \"memoization-aware\" Bayesian\nOptimization (BO) algorithm, EEIPU, that works in tandem with a pipeline\ncaching system, allowing it to evaluate significantly more hyperparameter\ncandidates per GPU-day than other tuning algorithms. The result is\nbetter-quality hyperparameters in the same amount of search time, or\nequivalently, reduced search time to reach the same hyperparameter quality. In\nour benchmarks on machine learning (model ensembles), vision (convolutional\narchitecture) and language (T5 architecture) pipelines, we compare EEIPU\nagainst recent BO algorithms: EEIPU produces an average of $103\\%$ more\nhyperparameter candidates (within the same budget), and increases the\nvalidation metric by an average of $108\\%$ more than other algorithms (where\nthe increase is measured starting from the end of warm-up iterations)."
                },
                "authors": [
                    {
                        "name": "Abdelmajid Essofi"
                    },
                    {
                        "name": "Ridwan Salahuddeen"
                    },
                    {
                        "name": "Munachiso Nwadike"
                    },
                    {
                        "name": "Elnura Zhalieva"
                    },
                    {
                        "name": "Kun Zhang"
                    },
                    {
                        "name": "Eric Xing"
                    },
                    {
                        "name": "Willie Neiswanger"
                    },
                    {
                        "name": "Qirong Ho"
                    }
                ],
                "author_detail": {
                    "name": "Qirong Ho"
                },
                "author": "Qirong Ho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03731v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03731v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20002v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20002v2",
                "updated": "2024-11-06T07:12:55Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    7,
                    12,
                    55,
                    2,
                    311,
                    0
                ],
                "published": "2024-09-30T06:55:00Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    6,
                    55,
                    0,
                    0,
                    274,
                    0
                ],
                "title": "The Early Bird Catches the Leak: Unveiling Timing Side Channels in LLM\n  Serving Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Early Bird Catches the Leak: Unveiling Timing Side Channels in LLM\n  Serving Systems"
                },
                "summary": "The wide deployment of Large Language Models (LLMs) has given rise to strong\ndemands for optimizing their inference performance. Today's techniques serving\nthis purpose primarily focus on reducing latency and improving throughput\nthrough algorithmic and hardware enhancements, while largely overlooking their\nprivacy side effects, particularly in a multi-user environment. In our\nresearch, for the first time, we discovered a set of new timing side channels\nin LLM systems, arising from shared caches and GPU memory allocations, which\ncan be exploited to infer both confidential system prompts and those issued by\nother users. These vulnerabilities echo security challenges observed in\ntraditional computing systems, highlighting an urgent need to address potential\ninformation leakage in LLM serving infrastructures. In this paper, we report\nnovel attack strategies designed to exploit such timing side channels inherent\nin LLM deployments, specifically targeting the Key-Value (KV) cache and\nsemantic cache widely used to enhance LLM inference performance. Our approach\nleverages timing measurements and classification models to detect cache hits,\nallowing an adversary to infer private prompts with high accuracy. We also\npropose a token-by-token search algorithm to efficiently recover shared prompt\nprefixes in the caches, showing the feasibility of stealing system prompts and\nthose produced by peer users. Our experimental studies on black-box testing of\npopular online LLM services demonstrate that such privacy risks are completely\nrealistic, with significant consequences. Our findings underscore the need for\nrobust mitigation to protect LLM systems against such emerging threats.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The wide deployment of Large Language Models (LLMs) has given rise to strong\ndemands for optimizing their inference performance. Today's techniques serving\nthis purpose primarily focus on reducing latency and improving throughput\nthrough algorithmic and hardware enhancements, while largely overlooking their\nprivacy side effects, particularly in a multi-user environment. In our\nresearch, for the first time, we discovered a set of new timing side channels\nin LLM systems, arising from shared caches and GPU memory allocations, which\ncan be exploited to infer both confidential system prompts and those issued by\nother users. These vulnerabilities echo security challenges observed in\ntraditional computing systems, highlighting an urgent need to address potential\ninformation leakage in LLM serving infrastructures. In this paper, we report\nnovel attack strategies designed to exploit such timing side channels inherent\nin LLM deployments, specifically targeting the Key-Value (KV) cache and\nsemantic cache widely used to enhance LLM inference performance. Our approach\nleverages timing measurements and classification models to detect cache hits,\nallowing an adversary to infer private prompts with high accuracy. We also\npropose a token-by-token search algorithm to efficiently recover shared prompt\nprefixes in the caches, showing the feasibility of stealing system prompts and\nthose produced by peer users. Our experimental studies on black-box testing of\npopular online LLM services demonstrate that such privacy risks are completely\nrealistic, with significant consequences. Our findings underscore the need for\nrobust mitigation to protect LLM systems against such emerging threats."
                },
                "authors": [
                    {
                        "name": "Linke Song"
                    },
                    {
                        "name": "Zixuan Pang"
                    },
                    {
                        "name": "Wenhao Wang"
                    },
                    {
                        "name": "Zihao Wang"
                    },
                    {
                        "name": "XiaoFeng Wang"
                    },
                    {
                        "name": "Hongbo Chen"
                    },
                    {
                        "name": "Wei Song"
                    },
                    {
                        "name": "Yier Jin"
                    },
                    {
                        "name": "Dan Meng"
                    },
                    {
                        "name": "Rui Hou"
                    }
                ],
                "author_detail": {
                    "name": "Rui Hou"
                },
                "author": "Rui Hou",
                "arxiv_comment": "This work was submitted for review on Sept. 5, 2024, and the initial\n  version was uploaded to Arxiv on Sept. 30, 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20002v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20002v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01433v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01433v2",
                "updated": "2024-11-06T01:49:45Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    1,
                    49,
                    45,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-03T04:25:46Z",
                "published_parsed": [
                    2024,
                    11,
                    3,
                    4,
                    25,
                    46,
                    6,
                    308,
                    0
                ],
                "title": "HOBBIT: A Mixed Precision Expert Offloading System for Fast MoE\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HOBBIT: A Mixed Precision Expert Offloading System for Fast MoE\n  Inference"
                },
                "summary": "The Mixture-of-Experts (MoE) architecture has demonstrated significant\nadvantages in the era of Large Language Models (LLMs), offering enhanced\ncapabilities with reduced inference costs. However, deploying MoE-based LLMs on\nmemoryconstrained edge devices remains challenging due to their substantial\nmemory requirements. While existing expertoffloading methods alleviate the\nmemory requirements, they often incur significant expert-loading costs or\ncompromise model accuracy. We present HOBBIT, a mixed precision expert\noffloading system to enable flexible and efficient MoE inference. Our key\ninsight is that dynamically replacing less critical cache-miss experts with low\nprecision versions can substantially reduce expert-loading latency while\npreserving model accuracy. HOBBIT introduces three innovative techniques that\nmap the natural hierarchy of MoE computation: (1) a token-level dynamic expert\nloading mechanism, (2) a layer-level adaptive expert prefetching technique, and\n(3) a sequence-level multidimensional expert caching policy. These innovations\nfully leverage the benefits of mixedprecision expert inference. By implementing\nHOBBIT on top of the renowned LLM inference framework Llama.cpp, we evaluate\nits performance across different edge devices with representative MoE models.\nThe results demonstrate that HOBBIT achieves up to a 9.93x speedup in decoding\ncompared to state-of-the-art MoE offloading systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Mixture-of-Experts (MoE) architecture has demonstrated significant\nadvantages in the era of Large Language Models (LLMs), offering enhanced\ncapabilities with reduced inference costs. However, deploying MoE-based LLMs on\nmemoryconstrained edge devices remains challenging due to their substantial\nmemory requirements. While existing expertoffloading methods alleviate the\nmemory requirements, they often incur significant expert-loading costs or\ncompromise model accuracy. We present HOBBIT, a mixed precision expert\noffloading system to enable flexible and efficient MoE inference. Our key\ninsight is that dynamically replacing less critical cache-miss experts with low\nprecision versions can substantially reduce expert-loading latency while\npreserving model accuracy. HOBBIT introduces three innovative techniques that\nmap the natural hierarchy of MoE computation: (1) a token-level dynamic expert\nloading mechanism, (2) a layer-level adaptive expert prefetching technique, and\n(3) a sequence-level multidimensional expert caching policy. These innovations\nfully leverage the benefits of mixedprecision expert inference. By implementing\nHOBBIT on top of the renowned LLM inference framework Llama.cpp, we evaluate\nits performance across different edge devices with representative MoE models.\nThe results demonstrate that HOBBIT achieves up to a 9.93x speedup in decoding\ncompared to state-of-the-art MoE offloading systems."
                },
                "authors": [
                    {
                        "name": "Peng Tang"
                    },
                    {
                        "name": "Jiacheng Liu"
                    },
                    {
                        "name": "Xiaofeng Hou"
                    },
                    {
                        "name": "Yifei Pu"
                    },
                    {
                        "name": "Jing Wang"
                    },
                    {
                        "name": "Pheng-Ann Heng"
                    },
                    {
                        "name": "Chao Li"
                    },
                    {
                        "name": "Minyi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Minyi Guo"
                },
                "author": "Minyi Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01433v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01433v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2308.05591v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2308.05591v3",
                "updated": "2024-11-05T08:34:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    8,
                    34,
                    44,
                    1,
                    310,
                    0
                ],
                "published": "2023-08-10T13:57:37Z",
                "published_parsed": [
                    2023,
                    8,
                    10,
                    13,
                    57,
                    37,
                    3,
                    222,
                    0
                ],
                "title": "Wireless Edge Content Broadcast via Integrated Terrestrial and\n  Non-terrestrial Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wireless Edge Content Broadcast via Integrated Terrestrial and\n  Non-terrestrial Networks"
                },
                "summary": "Non-terrestrial networks (NTN) have emerged as a transformative solution to\nbridge the digital divide and deliver essential services to remote and\nunderserved areas. In this context, low Earth orbit (LEO) satellite\nconstellations offer remarkable potential for efficient cache content broadcast\nin remote regions, thereby extending the reach of digital services. In this\npaper, we introduce a novel approach to optimize wireless edge content\nplacement using NTN. Despite wide coverage, the varying NTN transmission\ncapabilities must be carefully aligned with each content placement to maximize\nbroadcast efficiency. In this paper, we introduce a novel approach to optimize\nwireless edge content placement using NTN, positioning NTN as a complement to\nTN for achieving optimal content broadcasting. Specifically, we dynamically\nselect content for placement via NTN links. This selection is based on\npopularity and suitability for delivery through NTN, while considering the\norbital motion of LEO satellites. Our system-level case studies, based on a\npractical LEO constellation, demonstrate the significant improvement in\nplacement speed compared to existing methods, which neglect network mobility.\nWe also demonstrate that NTN links significantly outperform standalone wireless\nTN solutions, particularly in the early stages of content delivery. This\nadvantage is amplified when there is a higher correlation of content popularity\nacross geographical regions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Non-terrestrial networks (NTN) have emerged as a transformative solution to\nbridge the digital divide and deliver essential services to remote and\nunderserved areas. In this context, low Earth orbit (LEO) satellite\nconstellations offer remarkable potential for efficient cache content broadcast\nin remote regions, thereby extending the reach of digital services. In this\npaper, we introduce a novel approach to optimize wireless edge content\nplacement using NTN. Despite wide coverage, the varying NTN transmission\ncapabilities must be carefully aligned with each content placement to maximize\nbroadcast efficiency. In this paper, we introduce a novel approach to optimize\nwireless edge content placement using NTN, positioning NTN as a complement to\nTN for achieving optimal content broadcasting. Specifically, we dynamically\nselect content for placement via NTN links. This selection is based on\npopularity and suitability for delivery through NTN, while considering the\norbital motion of LEO satellites. Our system-level case studies, based on a\npractical LEO constellation, demonstrate the significant improvement in\nplacement speed compared to existing methods, which neglect network mobility.\nWe also demonstrate that NTN links significantly outperform standalone wireless\nTN solutions, particularly in the early stages of content delivery. This\nadvantage is amplified when there is a higher correlation of content popularity\nacross geographical regions."
                },
                "authors": [
                    {
                        "name": "Feng Wang"
                    },
                    {
                        "name": "Giovanni Geraci"
                    },
                    {
                        "name": "Lingxiang Li"
                    },
                    {
                        "name": "Peng Wang"
                    },
                    {
                        "name": "Tony Q. S. Quek"
                    }
                ],
                "author_detail": {
                    "name": "Tony Q. S. Quek"
                },
                "author": "Tony Q. S. Quek",
                "arxiv_comment": "This work is expanded on our paper presented at IEEE Globecom 2023:\n  F. Wang, G. Geraci and T. Q. S. Quek, \"Optimizing Cache Content Placement in\n  Integrated Terrestrial and Non-terrestrial Networks,\" GLOBECOM 2023 - 2023\n  IEEE Global Communications Conference, Kuala Lumpur, Malaysia, 2023, pp.\n  6609-6614",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2308.05591v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2308.05591v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02886v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02886v1",
                "updated": "2024-11-05T07:56:24Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    7,
                    56,
                    24,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-05T07:56:24Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    7,
                    56,
                    24,
                    1,
                    310,
                    0
                ],
                "title": "TokenSelect: Efficient Long-Context Inference and Length Extrapolation\n  for LLMs via Dynamic Token-Level KV Cache Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TokenSelect: Efficient Long-Context Inference and Length Extrapolation\n  for LLMs via Dynamic Token-Level KV Cache Selection"
                },
                "summary": "With the development of large language models (LLMs), the ability to handle\nlonger contexts has become a key capability for Web applications such as\ncross-document understanding and LLM-powered search systems. However, this\nprogress faces two major challenges: performance degradation due to sequence\nlengths out-of-distribution, and excessively long inference times caused by the\nquadratic computational complexity of attention. These issues hinder the\napplication of LLMs in long-context scenarios. In this paper, we propose\nDynamic Token-Level KV Cache Selection (TokenSelect), a model-agnostic,\ntraining-free method for efficient and accurate long-context inference.\nTokenSelect builds upon the observation of non-contiguous attention sparsity,\nusing Query-Key dot products to measure per-head KV Cache criticality at\ntoken-level. By per-head soft voting mechanism, TokenSelect selectively\ninvolves a small number of critical KV cache tokens in the attention\ncalculation without sacrificing accuracy. To further accelerate TokenSelect, we\ndesigned the Selection Cache based on observations of consecutive Query\nsimilarity and implemented efficient dot product kernel, significantly reducing\nthe overhead of token selection. A comprehensive evaluation of TokenSelect\ndemonstrates up to 23.84x speedup in attention computation and up to 2.28x\nacceleration in end-to-end latency, while providing superior performance\ncompared to state-of-the-art long-context inference methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the development of large language models (LLMs), the ability to handle\nlonger contexts has become a key capability for Web applications such as\ncross-document understanding and LLM-powered search systems. However, this\nprogress faces two major challenges: performance degradation due to sequence\nlengths out-of-distribution, and excessively long inference times caused by the\nquadratic computational complexity of attention. These issues hinder the\napplication of LLMs in long-context scenarios. In this paper, we propose\nDynamic Token-Level KV Cache Selection (TokenSelect), a model-agnostic,\ntraining-free method for efficient and accurate long-context inference.\nTokenSelect builds upon the observation of non-contiguous attention sparsity,\nusing Query-Key dot products to measure per-head KV Cache criticality at\ntoken-level. By per-head soft voting mechanism, TokenSelect selectively\ninvolves a small number of critical KV cache tokens in the attention\ncalculation without sacrificing accuracy. To further accelerate TokenSelect, we\ndesigned the Selection Cache based on observations of consecutive Query\nsimilarity and implemented efficient dot product kernel, significantly reducing\nthe overhead of token selection. A comprehensive evaluation of TokenSelect\ndemonstrates up to 23.84x speedup in attention computation and up to 2.28x\nacceleration in end-to-end latency, while providing superior performance\ncompared to state-of-the-art long-context inference methods."
                },
                "authors": [
                    {
                        "name": "Wei Wu"
                    },
                    {
                        "name": "Zhuoshi Pan"
                    },
                    {
                        "name": "Chao Wang"
                    },
                    {
                        "name": "Liyi Chen"
                    },
                    {
                        "name": "Yunchu Bai"
                    },
                    {
                        "name": "Kun Fu"
                    },
                    {
                        "name": "Zheng Wang"
                    },
                    {
                        "name": "Hui Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Hui Xiong"
                },
                "author": "Hui Xiong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02886v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02886v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02820v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02820v1",
                "updated": "2024-11-05T05:41:41Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    5,
                    41,
                    41,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-05T05:41:41Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    5,
                    41,
                    41,
                    1,
                    310,
                    0
                ],
                "title": "DroidSpeak: Enhancing Cross-LLM Communication",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DroidSpeak: Enhancing Cross-LLM Communication"
                },
                "summary": "In multi-agent systems utilizing Large Language Models (LLMs), communication\nbetween agents traditionally relies on natural language. This communication\noften includes the full context of the query so far, which can introduce\nsignificant prefill-phase latency, especially with long contexts.\n  We introduce DroidSpeak, a novel framework to target this cross-LLM\ncommunication by leveraging the reuse of intermediate data, such as input\nembeddings (E-cache) and key-value caches (KV-cache). We efficiently bypass the\nneed to reprocess entire contexts for fine-tuned versions of the same\nfoundational model. This approach allows faster context integration while\nmaintaining the quality of task performance. Experimental evaluations\ndemonstrate DroidSpeak's ability to significantly accelerate inter-agent\ncommunication, achieving up to a 2.78x speedup in prefill latency with\nnegligible loss in accuracy. Our findings underscore the potential to create\nmore efficient and scalable multi-agent systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In multi-agent systems utilizing Large Language Models (LLMs), communication\nbetween agents traditionally relies on natural language. This communication\noften includes the full context of the query so far, which can introduce\nsignificant prefill-phase latency, especially with long contexts.\n  We introduce DroidSpeak, a novel framework to target this cross-LLM\ncommunication by leveraging the reuse of intermediate data, such as input\nembeddings (E-cache) and key-value caches (KV-cache). We efficiently bypass the\nneed to reprocess entire contexts for fine-tuned versions of the same\nfoundational model. This approach allows faster context integration while\nmaintaining the quality of task performance. Experimental evaluations\ndemonstrate DroidSpeak's ability to significantly accelerate inter-agent\ncommunication, achieving up to a 2.78x speedup in prefill latency with\nnegligible loss in accuracy. Our findings underscore the potential to create\nmore efficient and scalable multi-agent systems."
                },
                "authors": [
                    {
                        "name": "Yuhan Liu"
                    },
                    {
                        "name": "Esha Choukse"
                    },
                    {
                        "name": "Shan Lu"
                    },
                    {
                        "name": "Junchen Jiang"
                    },
                    {
                        "name": "Madan Musuvathi"
                    }
                ],
                "author_detail": {
                    "name": "Madan Musuvathi"
                },
                "author": "Madan Musuvathi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02820v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02820v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2411.19951v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19951v1",
                "updated": "2024-11-29T18:59:54Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    18,
                    59,
                    54,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T18:59:54Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    18,
                    59,
                    54,
                    4,
                    334,
                    0
                ],
                "title": "T2Vid: Translating Long Text into Multi-Image is the Catalyst for\n  Video-LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "T2Vid: Translating Long Text into Multi-Image is the Catalyst for\n  Video-LLMs"
                },
                "summary": "The success of Multimodal Large Language Models (MLLMs) in the image domain\nhas garnered wide attention from the research community. Drawing on previous\nsuccessful experiences, researchers have recently explored extending the\nsuccess to the video understanding realms. Apart from training from scratch, an\nefficient way is to utilize the pre-trained image-LLMs, leading to two\nmainstream approaches, i.e. zero-shot inference and further fine-tuning with\nvideo data. In this work, our study of these approaches harvests an effective\ndata augmentation method. We first make a deeper inspection of the zero-shot\ninference way and identify two limitations, i.e. limited generalization and\nlack of temporal understanding capabilities. Thus, we further investigate the\nfine-tuning approach and find a low learning efficiency when simply using all\nthe video data samples, which can be attributed to a lack of instruction\ndiversity. Aiming at this issue, we develop a method called T2Vid to synthesize\nvideo-like samples to enrich the instruction diversity in the training corpus.\nIntegrating these data enables a simple and efficient training scheme, which\nachieves performance comparable to or even superior to using full video\ndatasets by training with just 15% the sample size. Meanwhile, we find that the\nproposed scheme can boost the performance of long video understanding without\ntraining with long video samples. We hope our study will spark more thinking\nabout using MLLMs for video understanding and curation of high-quality data.\nThe code is released at https://github.com/xjtupanda/T2Vid.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The success of Multimodal Large Language Models (MLLMs) in the image domain\nhas garnered wide attention from the research community. Drawing on previous\nsuccessful experiences, researchers have recently explored extending the\nsuccess to the video understanding realms. Apart from training from scratch, an\nefficient way is to utilize the pre-trained image-LLMs, leading to two\nmainstream approaches, i.e. zero-shot inference and further fine-tuning with\nvideo data. In this work, our study of these approaches harvests an effective\ndata augmentation method. We first make a deeper inspection of the zero-shot\ninference way and identify two limitations, i.e. limited generalization and\nlack of temporal understanding capabilities. Thus, we further investigate the\nfine-tuning approach and find a low learning efficiency when simply using all\nthe video data samples, which can be attributed to a lack of instruction\ndiversity. Aiming at this issue, we develop a method called T2Vid to synthesize\nvideo-like samples to enrich the instruction diversity in the training corpus.\nIntegrating these data enables a simple and efficient training scheme, which\nachieves performance comparable to or even superior to using full video\ndatasets by training with just 15% the sample size. Meanwhile, we find that the\nproposed scheme can boost the performance of long video understanding without\ntraining with long video samples. We hope our study will spark more thinking\nabout using MLLMs for video understanding and curation of high-quality data.\nThe code is released at https://github.com/xjtupanda/T2Vid."
                },
                "authors": [
                    {
                        "name": "Shukang Yin"
                    },
                    {
                        "name": "Chaoyou Fu"
                    },
                    {
                        "name": "Sirui Zhao"
                    },
                    {
                        "name": "Yunhang Shen"
                    },
                    {
                        "name": "Chunjiang Ge"
                    },
                    {
                        "name": "Yan Yang"
                    },
                    {
                        "name": "Zuwei Long"
                    },
                    {
                        "name": "Yuhan Dai"
                    },
                    {
                        "name": "Tong Xu"
                    },
                    {
                        "name": "Xing Sun"
                    },
                    {
                        "name": "Ran He"
                    },
                    {
                        "name": "Caifeng Shan"
                    },
                    {
                        "name": "Enhong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Enhong Chen"
                },
                "author": "Enhong Chen",
                "arxiv_comment": "13 pages, 9 figures, 5 tables. Project page:\n  https://github.com/xjtupanda/T2Vid",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19951v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19951v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19943v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19943v1",
                "updated": "2024-11-29T18:58:22Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    18,
                    58,
                    22,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T18:58:22Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    18,
                    58,
                    22,
                    4,
                    334,
                    0
                ],
                "title": "Critical Tokens Matter: Token-Level Contrastive Estimation Enhence LLM's\n  Reasoning Capability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Critical Tokens Matter: Token-Level Contrastive Estimation Enhence LLM's\n  Reasoning Capability"
                },
                "summary": "Large Language Models (LLMs) have exhibited remarkable performance on\nreasoning tasks. They utilize autoregressive token generation to construct\nreasoning trajectories, enabling the development of a coherent chain of\nthought. In this work, we explore the impact of individual tokens on the final\noutcomes of reasoning tasks. We identify the existence of ``critical tokens''\nthat lead to incorrect reasoning trajectories in LLMs. Specifically, we find\nthat LLMs tend to produce positive outcomes when forced to decode other tokens\ninstead of critical tokens. Motivated by this observation, we propose a novel\napproach - cDPO - designed to automatically recognize and conduct token-level\nrewards for the critical tokens during the alignment process. Specifically, we\ndevelop a contrastive estimation approach to automatically identify critical\ntokens. It is achieved by comparing the generation likelihood of positive and\nnegative models. To achieve this, we separately fine-tune the positive and\nnegative models on various reasoning trajectories, consequently, they are\ncapable of identifying identify critical tokens within incorrect trajectories\nthat contribute to erroneous outcomes. Moreover, to further align the model\nwith the critical token information during the alignment process, we extend the\nconventional DPO algorithms to token-level DPO and utilize the differential\nlikelihood from the aforementioned positive and negative model as important\nweight for token-level DPO learning.Experimental results on GSM8K and MATH500\nbenchmarks with two-widely used models Llama-3 (8B and 70B) and deepseek-math\n(7B) demonstrate the effectiveness of the propsoed approach cDPO.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have exhibited remarkable performance on\nreasoning tasks. They utilize autoregressive token generation to construct\nreasoning trajectories, enabling the development of a coherent chain of\nthought. In this work, we explore the impact of individual tokens on the final\noutcomes of reasoning tasks. We identify the existence of ``critical tokens''\nthat lead to incorrect reasoning trajectories in LLMs. Specifically, we find\nthat LLMs tend to produce positive outcomes when forced to decode other tokens\ninstead of critical tokens. Motivated by this observation, we propose a novel\napproach - cDPO - designed to automatically recognize and conduct token-level\nrewards for the critical tokens during the alignment process. Specifically, we\ndevelop a contrastive estimation approach to automatically identify critical\ntokens. It is achieved by comparing the generation likelihood of positive and\nnegative models. To achieve this, we separately fine-tune the positive and\nnegative models on various reasoning trajectories, consequently, they are\ncapable of identifying identify critical tokens within incorrect trajectories\nthat contribute to erroneous outcomes. Moreover, to further align the model\nwith the critical token information during the alignment process, we extend the\nconventional DPO algorithms to token-level DPO and utilize the differential\nlikelihood from the aforementioned positive and negative model as important\nweight for token-level DPO learning.Experimental results on GSM8K and MATH500\nbenchmarks with two-widely used models Llama-3 (8B and 70B) and deepseek-math\n(7B) demonstrate the effectiveness of the propsoed approach cDPO."
                },
                "authors": [
                    {
                        "name": "Zicheng Lin"
                    },
                    {
                        "name": "Tian Liang"
                    },
                    {
                        "name": "Jiahao Xu"
                    },
                    {
                        "name": "Xing Wang"
                    },
                    {
                        "name": "Ruilin Luo"
                    },
                    {
                        "name": "Chufan Shi"
                    },
                    {
                        "name": "Siheng Li"
                    },
                    {
                        "name": "Yujiu Yang"
                    },
                    {
                        "name": "Zhaopeng Tu"
                    }
                ],
                "author_detail": {
                    "name": "Zhaopeng Tu"
                },
                "author": "Zhaopeng Tu",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19943v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19943v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19933v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19933v1",
                "updated": "2024-11-29T18:49:55Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    18,
                    49,
                    55,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T18:49:55Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    18,
                    49,
                    55,
                    4,
                    334,
                    0
                ],
                "title": "Transfer Learning for High-dimensional Quantile Regression with\n  Distribution Shift",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transfer Learning for High-dimensional Quantile Regression with\n  Distribution Shift"
                },
                "summary": "Information from related source studies can often enhance the findings of a\ntarget study. However, the distribution shift between target and source studies\ncan severely impact the efficiency of knowledge transfer. In the\nhigh-dimensional regression setting, existing transfer approaches mainly focus\non the parameter shift. In this paper, we focus on the high-dimensional\nquantile regression with knowledge transfer under three types of distribution\nshift: parameter shift, covariate shift, and residual shift. We propose a novel\ntransferable set and a new transfer framework to address the above three\ndiscrepancies. Non-asymptotic estimation error bounds and source detection\nconsistency are established to validate the availability and superiority of our\nmethod in the presence of distribution shift. Additionally, an orthogonal\ndebiased approach is proposed for statistical inference with knowledge\ntransfer, leading to sharper asymptotic results. Extensive simulation results\nas well as real data applications further demonstrate the effectiveness of our\nproposed procedure.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Information from related source studies can often enhance the findings of a\ntarget study. However, the distribution shift between target and source studies\ncan severely impact the efficiency of knowledge transfer. In the\nhigh-dimensional regression setting, existing transfer approaches mainly focus\non the parameter shift. In this paper, we focus on the high-dimensional\nquantile regression with knowledge transfer under three types of distribution\nshift: parameter shift, covariate shift, and residual shift. We propose a novel\ntransferable set and a new transfer framework to address the above three\ndiscrepancies. Non-asymptotic estimation error bounds and source detection\nconsistency are established to validate the availability and superiority of our\nmethod in the presence of distribution shift. Additionally, an orthogonal\ndebiased approach is proposed for statistical inference with knowledge\ntransfer, leading to sharper asymptotic results. Extensive simulation results\nas well as real data applications further demonstrate the effectiveness of our\nproposed procedure."
                },
                "authors": [
                    {
                        "name": "Ruiqi Bai"
                    },
                    {
                        "name": "Yijiao Zhang"
                    },
                    {
                        "name": "Hanbo Yang"
                    },
                    {
                        "name": "Zhongyi Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Zhongyi Zhu"
                },
                "author": "Zhongyi Zhu",
                "arxiv_comment": "53 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19933v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19933v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19921v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19921v1",
                "updated": "2024-11-29T18:36:15Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    18,
                    36,
                    15,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T18:36:15Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    18,
                    36,
                    15,
                    4,
                    334,
                    0
                ],
                "title": "SIMS: Simulating Human-Scene Interactions with Real World Script\n  Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SIMS: Simulating Human-Scene Interactions with Real World Script\n  Planning"
                },
                "summary": "Simulating long-term human-scene interaction is a challenging yet fascinating\ntask. Previous works have not effectively addressed the generation of long-term\nhuman scene interactions with detailed narratives for physics-based animation.\nThis paper introduces a novel framework for the planning and controlling of\nlong-horizon physical plausible human-scene interaction. On the one hand, films\nand shows with stylish human locomotions or interactions with scenes are\nabundantly available on the internet, providing a rich source of data for\nscript planning. On the other hand, Large Language Models (LLMs) can understand\nand generate logical storylines.\n  This motivates us to marry the two by using an LLM-based pipeline to extract\nscripts from videos, and then employ LLMs to imitate and create new scripts,\ncapturing complex, time-series human behaviors and interactions with\nenvironments. By leveraging this, we utilize a dual-aware policy that achieves\nboth language comprehension and scene understanding to guide character motions\nwithin contextual and spatial constraints. To facilitate training and\nevaluation, we contribute a comprehensive planning dataset containing diverse\nmotion sequences extracted from real-world videos and expand them with large\nlanguage models. We also collect and re-annotate motion clips from existing\nkinematic datasets to enable our policy learn diverse skills. Extensive\nexperiments demonstrate the effectiveness of our framework in versatile task\nexecution and its generalization ability to various scenarios, showing\nremarkably enhanced performance compared with existing methods. Our code and\ndata will be publicly available soon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulating long-term human-scene interaction is a challenging yet fascinating\ntask. Previous works have not effectively addressed the generation of long-term\nhuman scene interactions with detailed narratives for physics-based animation.\nThis paper introduces a novel framework for the planning and controlling of\nlong-horizon physical plausible human-scene interaction. On the one hand, films\nand shows with stylish human locomotions or interactions with scenes are\nabundantly available on the internet, providing a rich source of data for\nscript planning. On the other hand, Large Language Models (LLMs) can understand\nand generate logical storylines.\n  This motivates us to marry the two by using an LLM-based pipeline to extract\nscripts from videos, and then employ LLMs to imitate and create new scripts,\ncapturing complex, time-series human behaviors and interactions with\nenvironments. By leveraging this, we utilize a dual-aware policy that achieves\nboth language comprehension and scene understanding to guide character motions\nwithin contextual and spatial constraints. To facilitate training and\nevaluation, we contribute a comprehensive planning dataset containing diverse\nmotion sequences extracted from real-world videos and expand them with large\nlanguage models. We also collect and re-annotate motion clips from existing\nkinematic datasets to enable our policy learn diverse skills. Extensive\nexperiments demonstrate the effectiveness of our framework in versatile task\nexecution and its generalization ability to various scenarios, showing\nremarkably enhanced performance compared with existing methods. Our code and\ndata will be publicly available soon."
                },
                "authors": [
                    {
                        "name": "Wenjia Wang"
                    },
                    {
                        "name": "Liang Pan"
                    },
                    {
                        "name": "Zhiyang Dou"
                    },
                    {
                        "name": "Zhouyingcheng Liao"
                    },
                    {
                        "name": "Yuke Lou"
                    },
                    {
                        "name": "Lei Yang"
                    },
                    {
                        "name": "Jingbo Wang"
                    },
                    {
                        "name": "Taku Komura"
                    }
                ],
                "author_detail": {
                    "name": "Taku Komura"
                },
                "author": "Taku Komura",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19921v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19921v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17660v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17660v2",
                "updated": "2024-11-29T18:31:24Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    18,
                    31,
                    24,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-26T18:25:51Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    18,
                    25,
                    51,
                    1,
                    331,
                    0
                ],
                "title": "DROID-Splat: Combining end-to-end SLAM with 3D Gaussian Splatting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DROID-Splat: Combining end-to-end SLAM with 3D Gaussian Splatting"
                },
                "summary": "Recent progress in scene synthesis makes standalone SLAM systems purely based\non optimizing hyperprimitives with a Rendering objective possible. However, the\ntracking performance still lacks behind traditional and end-to-end SLAM\nsystems. An optimal trade-off between robustness, speed and accuracy has not\nyet been reached, especially for monocular video. In this paper, we introduce a\nSLAM system based on an end-to-end Tracker and extend it with a Renderer based\non recent 3D Gaussian Splatting techniques. Our framework \\textbf{DroidSplat}\nachieves both SotA tracking and rendering results on common SLAM benchmarks. We\nimplemented multiple building blocks of modern SLAM systems to run in parallel,\nallowing for fast inference on common consumer GPU's. Recent progress in\nmonocular depth prediction and camera calibration allows our system to achieve\nstrong results even on in-the-wild data without known camera intrinsics. Code\nwill be available at \\url{https://github.com/ChenHoy/DROID-Splat}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent progress in scene synthesis makes standalone SLAM systems purely based\non optimizing hyperprimitives with a Rendering objective possible. However, the\ntracking performance still lacks behind traditional and end-to-end SLAM\nsystems. An optimal trade-off between robustness, speed and accuracy has not\nyet been reached, especially for monocular video. In this paper, we introduce a\nSLAM system based on an end-to-end Tracker and extend it with a Renderer based\non recent 3D Gaussian Splatting techniques. Our framework \\textbf{DroidSplat}\nachieves both SotA tracking and rendering results on common SLAM benchmarks. We\nimplemented multiple building blocks of modern SLAM systems to run in parallel,\nallowing for fast inference on common consumer GPU's. Recent progress in\nmonocular depth prediction and camera calibration allows our system to achieve\nstrong results even on in-the-wild data without known camera intrinsics. Code\nwill be available at \\url{https://github.com/ChenHoy/DROID-Splat}."
                },
                "authors": [
                    {
                        "name": "Christian Homeyer"
                    },
                    {
                        "name": "Leon Begiristain"
                    },
                    {
                        "name": "Christoph SchnÃ¶rr"
                    }
                ],
                "author_detail": {
                    "name": "Christoph SchnÃ¶rr"
                },
                "author": "Christoph SchnÃ¶rr",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17660v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17660v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19908v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19908v1",
                "updated": "2024-11-29T18:12:50Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    18,
                    12,
                    50,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T18:12:50Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    18,
                    12,
                    50,
                    4,
                    334,
                    0
                ],
                "title": "Another look at inference after prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Another look at inference after prediction"
                },
                "summary": "Prediction-based (PB) inference is increasingly used in applications where\nthe outcome of interest is difficult to obtain, but its predictors are readily\navailable. Unlike traditional inference, PB inference performs statistical\ninference using a partially observed outcome and a set of covariates by\nleveraging a prediction of the outcome generated from a machine learning (ML)\nmodel. Motwani and Witten (2023) recently revisited two innovative PB inference\napproaches for ordinary least squares. They found that the method proposed by\nWang et al. (2020) yields a consistent estimator for the association of\ninterest when the ML model perfectly captures the underlying regression\nfunction. Conversely, the prediction-powered inference (PPI) method proposed by\nAngelopoulos et al. (2023) yields valid inference regardless of the model's\naccuracy. In this paper, we study the statistical efficiency of the PPI\nestimator. Our analysis reveals that a more efficient estimator, proposed 25\nyears ago by Chen and Chen (2000), can be obtained by simply adding a weight to\nthe PPI estimator. We also contextualize PB inference with methods from the\neconomics and statistics literature dating back to the 1960s. Our extensive\ntheoretical and numerical analyses indicate that the Chen and Chen (CC)\nestimator offers a balance between robustness to ML model specification and\nstatistical efficiency, making it the preferred choice for use in practice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prediction-based (PB) inference is increasingly used in applications where\nthe outcome of interest is difficult to obtain, but its predictors are readily\navailable. Unlike traditional inference, PB inference performs statistical\ninference using a partially observed outcome and a set of covariates by\nleveraging a prediction of the outcome generated from a machine learning (ML)\nmodel. Motwani and Witten (2023) recently revisited two innovative PB inference\napproaches for ordinary least squares. They found that the method proposed by\nWang et al. (2020) yields a consistent estimator for the association of\ninterest when the ML model perfectly captures the underlying regression\nfunction. Conversely, the prediction-powered inference (PPI) method proposed by\nAngelopoulos et al. (2023) yields valid inference regardless of the model's\naccuracy. In this paper, we study the statistical efficiency of the PPI\nestimator. Our analysis reveals that a more efficient estimator, proposed 25\nyears ago by Chen and Chen (2000), can be obtained by simply adding a weight to\nthe PPI estimator. We also contextualize PB inference with methods from the\neconomics and statistics literature dating back to the 1960s. Our extensive\ntheoretical and numerical analyses indicate that the Chen and Chen (CC)\nestimator offers a balance between robustness to ML model specification and\nstatistical efficiency, making it the preferred choice for use in practice."
                },
                "authors": [
                    {
                        "name": "Jessica Gronsbell"
                    },
                    {
                        "name": "Jianhui Gao"
                    },
                    {
                        "name": "Yaqi Shi"
                    },
                    {
                        "name": "Zachary R. McCaw"
                    },
                    {
                        "name": "David Cheng"
                    }
                ],
                "author_detail": {
                    "name": "David Cheng"
                },
                "author": "David Cheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19908v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19908v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19906v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19906v1",
                "updated": "2024-11-29T18:11:39Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    18,
                    11,
                    39,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T18:11:39Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    18,
                    11,
                    39,
                    4,
                    334,
                    0
                ],
                "title": "Classical and Quantum Algorithms for the Deterministic L-system\n  Inductive Inference Problem",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Classical and Quantum Algorithms for the Deterministic L-system\n  Inductive Inference Problem"
                },
                "summary": "L-systems can be made to model and create simulations of many biological\nprocesses, such as plant development. Finding an L-system for a given process\nis typically solved by hand, by experts, in a hugely time-consuming process. It\nwould be significant if this could be done automatically from data, such as\nfrom sequences of images. In this paper, we are interested in inferring a\nparticular type of L-system, deterministic context-free L-system (D0L-system)\nfrom a sequence of strings. We introduce the characteristic graph of a sequence\nof strings, which we then utilize to translate our problem (inferring\nD0L-system) in polynomial time into the maximum independent set problem (MIS)\nand the SAT problem. After that, we offer a classical exact algorithm and an\napproximate quantum algorithm for the problem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "L-systems can be made to model and create simulations of many biological\nprocesses, such as plant development. Finding an L-system for a given process\nis typically solved by hand, by experts, in a hugely time-consuming process. It\nwould be significant if this could be done automatically from data, such as\nfrom sequences of images. In this paper, we are interested in inferring a\nparticular type of L-system, deterministic context-free L-system (D0L-system)\nfrom a sequence of strings. We introduce the characteristic graph of a sequence\nof strings, which we then utilize to translate our problem (inferring\nD0L-system) in polynomial time into the maximum independent set problem (MIS)\nand the SAT problem. After that, we offer a classical exact algorithm and an\napproximate quantum algorithm for the problem."
                },
                "authors": [
                    {
                        "name": "Ali Lotfi"
                    },
                    {
                        "name": "Ian McQuillan"
                    },
                    {
                        "name": "Steven Rayan"
                    }
                ],
                "author_detail": {
                    "name": "Steven Rayan"
                },
                "author": "Steven Rayan",
                "arxiv_comment": "16 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19906v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19906v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.FL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19886v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19886v1",
                "updated": "2024-11-29T17:52:39Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    17,
                    52,
                    39,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T17:52:39Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    17,
                    52,
                    39,
                    4,
                    334,
                    0
                ],
                "title": "PDDLFuse: A Tool for Generating Diverse Planning Domains",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PDDLFuse: A Tool for Generating Diverse Planning Domains"
                },
                "summary": "Various real-world challenges require planning algorithms that can adapt to a\nbroad range of domains. Traditionally, the creation of planning domains has\nrelied heavily on human implementation, which limits the scale and diversity of\navailable domains. While recent advancements have leveraged generative AI\ntechnologies such as large language models (LLMs) for domain creation, these\nefforts have predominantly focused on translating existing domains from natural\nlanguage descriptions rather than generating novel ones. In contrast, the\nconcept of domain randomization, which has been highly effective in\nreinforcement learning, enhances performance and generalizability by training\non a diverse array of randomized new domains. Inspired by this success, our\ntool, PDDLFuse, aims to bridge this gap in Planning Domain Definition Language\n(PDDL). PDDLFuse is designed to generate new, diverse planning domains that can\nbe used to validate new planners or test foundational planning models. We have\ndeveloped methods to adjust the domain generators parameters to modulate the\ndifficulty of the domains it generates. This adaptability is crucial as\nexisting domain-independent planners often struggle with more complex problems.\nInitial tests indicate that PDDLFuse efficiently creates intricate and varied\ndomains, representing a significant advancement over traditional domain\ngeneration methods and making a contribution towards planning research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Various real-world challenges require planning algorithms that can adapt to a\nbroad range of domains. Traditionally, the creation of planning domains has\nrelied heavily on human implementation, which limits the scale and diversity of\navailable domains. While recent advancements have leveraged generative AI\ntechnologies such as large language models (LLMs) for domain creation, these\nefforts have predominantly focused on translating existing domains from natural\nlanguage descriptions rather than generating novel ones. In contrast, the\nconcept of domain randomization, which has been highly effective in\nreinforcement learning, enhances performance and generalizability by training\non a diverse array of randomized new domains. Inspired by this success, our\ntool, PDDLFuse, aims to bridge this gap in Planning Domain Definition Language\n(PDDL). PDDLFuse is designed to generate new, diverse planning domains that can\nbe used to validate new planners or test foundational planning models. We have\ndeveloped methods to adjust the domain generators parameters to modulate the\ndifficulty of the domains it generates. This adaptability is crucial as\nexisting domain-independent planners often struggle with more complex problems.\nInitial tests indicate that PDDLFuse efficiently creates intricate and varied\ndomains, representing a significant advancement over traditional domain\ngeneration methods and making a contribution towards planning research."
                },
                "authors": [
                    {
                        "name": "Vedant Khandelwal"
                    },
                    {
                        "name": "Amit Sheth"
                    },
                    {
                        "name": "Forest Agostinelli"
                    }
                ],
                "author_detail": {
                    "name": "Forest Agostinelli"
                },
                "author": "Forest Agostinelli",
                "arxiv_comment": "218 Tables, 3 Figures, 4 Algorithms",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19886v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19886v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19876v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19876v1",
                "updated": "2024-11-29T17:38:56Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    17,
                    38,
                    56,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T17:38:56Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    17,
                    38,
                    56,
                    4,
                    334,
                    0
                ],
                "title": "LUMIA: Linear probing for Unimodal and MultiModal Membership Inference\n  A!acks leveraging internal LLM states",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LUMIA: Linear probing for Unimodal and MultiModal Membership Inference\n  A!acks leveraging internal LLM states"
                },
                "summary": "Large Language Models (LLMs) are increasingly used in a variety of\napplications, but concerns around membership inference have grown in parallel.\nPrevious efforts focus on black-to-grey-box models, thus neglecting the\npotential benefit from internal LLM information. To address this, we propose\nthe use of Linear Probes (LPs) as a method to detect Membership Inference\nAttacks (MIAs) by examining internal activations of LLMs. Our approach, dubbed\nLUMIA, applies LPs layer-by-layer to get fine-grained data on the model inner\nworkings. We test this method across several model architectures, sizes and\ndatasets, including unimodal and multimodal tasks. In unimodal MIA, LUMIA\nachieves an average gain of 15.71 % in Area Under the Curve (AUC) over previous\ntechniques. Remarkably, LUMIA reaches AUC>60% in 65.33% of cases -- an\nincrement of 46.80% against the state of the art. Furthermore, our approach\nreveals key insights, such as the model layers where MIAs are most detectable.\nIn multimodal models, LPs indicate that visual inputs can significantly\ncontribute to detect MIAs -- AUC>60% is reached in 85.90% of experiments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly used in a variety of\napplications, but concerns around membership inference have grown in parallel.\nPrevious efforts focus on black-to-grey-box models, thus neglecting the\npotential benefit from internal LLM information. To address this, we propose\nthe use of Linear Probes (LPs) as a method to detect Membership Inference\nAttacks (MIAs) by examining internal activations of LLMs. Our approach, dubbed\nLUMIA, applies LPs layer-by-layer to get fine-grained data on the model inner\nworkings. We test this method across several model architectures, sizes and\ndatasets, including unimodal and multimodal tasks. In unimodal MIA, LUMIA\nachieves an average gain of 15.71 % in Area Under the Curve (AUC) over previous\ntechniques. Remarkably, LUMIA reaches AUC>60% in 65.33% of cases -- an\nincrement of 46.80% against the state of the art. Furthermore, our approach\nreveals key insights, such as the model layers where MIAs are most detectable.\nIn multimodal models, LPs indicate that visual inputs can significantly\ncontribute to detect MIAs -- AUC>60% is reached in 85.90% of experiments."
                },
                "authors": [
                    {
                        "name": "Luis Ibanez-Lissen"
                    },
                    {
                        "name": "Lorena Gonzalez-Manzano"
                    },
                    {
                        "name": "Jose Maria de Fuentes"
                    },
                    {
                        "name": "Nicolas Anciaux"
                    },
                    {
                        "name": "Joaquin Garcia-Alfaro"
                    }
                ],
                "author_detail": {
                    "name": "Joaquin Garcia-Alfaro"
                },
                "author": "Joaquin Garcia-Alfaro",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19876v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19876v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19869v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19869v1",
                "updated": "2024-11-29T17:31:42Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    17,
                    31,
                    42,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T17:31:42Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    17,
                    31,
                    42,
                    4,
                    334,
                    0
                ],
                "title": "AIDetx: a compression-based method for identification of\n  machine-learning generated text",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AIDetx: a compression-based method for identification of\n  machine-learning generated text"
                },
                "summary": "This paper introduces AIDetx, a novel method for detecting machine-generated\ntext using data compression techniques. Traditional approaches, such as deep\nlearning classifiers, often suffer from high computational costs and limited\ninterpretability. To address these limitations, we propose a compression-based\nclassification framework that leverages finite-context models (FCMs). AIDetx\nconstructs distinct compression models for human-written and AI-generated text,\nclassifying new inputs based on which model achieves a higher compression\nratio. We evaluated AIDetx on two benchmark datasets, achieving F1 scores\nexceeding 97% and 99%, respectively, highlighting its high accuracy. Compared\nto current methods, such as large language models (LLMs), AIDetx offers a more\ninterpretable and computationally efficient solution, significantly reducing\nboth training time and hardware requirements (e.g., no GPUs needed). The full\nimplementation is publicly available at https://github.com/AIDetx/AIDetx.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces AIDetx, a novel method for detecting machine-generated\ntext using data compression techniques. Traditional approaches, such as deep\nlearning classifiers, often suffer from high computational costs and limited\ninterpretability. To address these limitations, we propose a compression-based\nclassification framework that leverages finite-context models (FCMs). AIDetx\nconstructs distinct compression models for human-written and AI-generated text,\nclassifying new inputs based on which model achieves a higher compression\nratio. We evaluated AIDetx on two benchmark datasets, achieving F1 scores\nexceeding 97% and 99%, respectively, highlighting its high accuracy. Compared\nto current methods, such as large language models (LLMs), AIDetx offers a more\ninterpretable and computationally efficient solution, significantly reducing\nboth training time and hardware requirements (e.g., no GPUs needed). The full\nimplementation is publicly available at https://github.com/AIDetx/AIDetx."
                },
                "authors": [
                    {
                        "name": "Leonardo Almeida"
                    },
                    {
                        "name": "Pedro Rodrigues"
                    },
                    {
                        "name": "Diogo MagalhÃ£es"
                    },
                    {
                        "name": "Armando J. Pinho"
                    },
                    {
                        "name": "Diogo Pratas"
                    }
                ],
                "author_detail": {
                    "name": "Diogo Pratas"
                },
                "author": "Diogo Pratas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19869v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19869v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19865v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19865v1",
                "updated": "2024-11-29T17:27:05Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    17,
                    27,
                    5,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T17:27:05Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    17,
                    27,
                    5,
                    4,
                    334,
                    0
                ],
                "title": "Reverse Thinking Makes LLMs Stronger Reasoners",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reverse Thinking Makes LLMs Stronger Reasoners"
                },
                "summary": "Reverse thinking plays a crucial role in human reasoning. Humans can reason\nnot only from a problem to a solution but also in reverse, i.e., start from the\nsolution and reason towards the problem. This often enhances overall reasoning\nperformance as it enables consistency checks between their forward and backward\nthinking. To enable Large Language Models (LLMs) to perform reverse thinking,\nwe introduce Reverse-Enhanced Thinking (RevThink), a framework composed of data\naugmentation and learning objectives. In RevThink, we augment the dataset by\ncollecting structured forward-backward reasoning from a teacher model,\nconsisting of: (1) the original question, (2) forward reasoning, (3) backward\nquestion, and (4) backward reasoning. We then employ three objectives to train\na smaller student model in a multi-task learning fashion: (a) generate forward\nreasoning from a question, (b) generate a backward question from a question,\nand (c) generate backward reasoning from the backward question. Experiments\nacross 12 datasets covering commonsense, math, and logical reasoning show an\naverage 13.53% improvement over the student model's zero-shot performance and a\n6.84% improvement over the strongest knowledge distillation baselines.\nMoreover, our method demonstrates sample efficiency -- using only 10% of the\ncorrect forward reasoning from the training data, it outperforms a standard\nfine-tuning method trained on 10x more forward reasoning. RevThink also\nexhibits strong generalization to out-of-distribution held-out datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reverse thinking plays a crucial role in human reasoning. Humans can reason\nnot only from a problem to a solution but also in reverse, i.e., start from the\nsolution and reason towards the problem. This often enhances overall reasoning\nperformance as it enables consistency checks between their forward and backward\nthinking. To enable Large Language Models (LLMs) to perform reverse thinking,\nwe introduce Reverse-Enhanced Thinking (RevThink), a framework composed of data\naugmentation and learning objectives. In RevThink, we augment the dataset by\ncollecting structured forward-backward reasoning from a teacher model,\nconsisting of: (1) the original question, (2) forward reasoning, (3) backward\nquestion, and (4) backward reasoning. We then employ three objectives to train\na smaller student model in a multi-task learning fashion: (a) generate forward\nreasoning from a question, (b) generate a backward question from a question,\nand (c) generate backward reasoning from the backward question. Experiments\nacross 12 datasets covering commonsense, math, and logical reasoning show an\naverage 13.53% improvement over the student model's zero-shot performance and a\n6.84% improvement over the strongest knowledge distillation baselines.\nMoreover, our method demonstrates sample efficiency -- using only 10% of the\ncorrect forward reasoning from the training data, it outperforms a standard\nfine-tuning method trained on 10x more forward reasoning. RevThink also\nexhibits strong generalization to out-of-distribution held-out datasets."
                },
                "authors": [
                    {
                        "name": "Justin Chih-Yao Chen"
                    },
                    {
                        "name": "Zifeng Wang"
                    },
                    {
                        "name": "Hamid Palangi"
                    },
                    {
                        "name": "Rujun Han"
                    },
                    {
                        "name": "Sayna Ebrahimi"
                    },
                    {
                        "name": "Long Le"
                    },
                    {
                        "name": "Vincent Perot"
                    },
                    {
                        "name": "Swaroop Mishra"
                    },
                    {
                        "name": "Mohit Bansal"
                    },
                    {
                        "name": "Chen-Yu Lee"
                    },
                    {
                        "name": "Tomas Pfister"
                    }
                ],
                "author_detail": {
                    "name": "Tomas Pfister"
                },
                "author": "Tomas Pfister",
                "arxiv_comment": "20 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19865v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19865v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19862v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19862v1",
                "updated": "2024-11-29T17:25:00Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    17,
                    25,
                    0,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T17:25:00Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    17,
                    25,
                    0,
                    4,
                    334,
                    0
                ],
                "title": "Cross-Domain Recommendation Meets Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-Domain Recommendation Meets Large Language Models"
                },
                "summary": "Cross-domain recommendation (CDR) has emerged as a promising solution to the\ncold-start problem, faced by single-domain recommender systems. However,\nexisting CDR models rely on complex neural architectures, large datasets, and\nsignificant computational resources, making them less effective in data-scarce\nscenarios or when simplicity is crucial. In this work, we leverage the\nreasoning capabilities of large language models (LLMs) and explore their\nperformance in the CDR domain across multiple domain pairs. We introduce two\nnovel prompt designs tailored for CDR and demonstrate that LLMs, when prompted\neffectively, outperform state-of-the-art CDR baselines across various metrics\nand domain combinations in the rating prediction and ranking tasks. This work\nbridges the gap between LLMs and recommendation systems, showcasing their\npotential as effective cross-domain recommenders.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-domain recommendation (CDR) has emerged as a promising solution to the\ncold-start problem, faced by single-domain recommender systems. However,\nexisting CDR models rely on complex neural architectures, large datasets, and\nsignificant computational resources, making them less effective in data-scarce\nscenarios or when simplicity is crucial. In this work, we leverage the\nreasoning capabilities of large language models (LLMs) and explore their\nperformance in the CDR domain across multiple domain pairs. We introduce two\nnovel prompt designs tailored for CDR and demonstrate that LLMs, when prompted\neffectively, outperform state-of-the-art CDR baselines across various metrics\nand domain combinations in the rating prediction and ranking tasks. This work\nbridges the gap between LLMs and recommendation systems, showcasing their\npotential as effective cross-domain recommenders."
                },
                "authors": [
                    {
                        "name": "Ajay Krishna Vajjala"
                    },
                    {
                        "name": "Dipak Meher"
                    },
                    {
                        "name": "Ziwei Zhu"
                    },
                    {
                        "name": "David S. Rosenblum"
                    }
                ],
                "author_detail": {
                    "name": "David S. Rosenblum"
                },
                "author": "David S. Rosenblum",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19862v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19862v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15623v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15623v2",
                "updated": "2024-11-29T17:18:49Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    17,
                    18,
                    49,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-23T18:27:35Z",
                "published_parsed": [
                    2024,
                    11,
                    23,
                    18,
                    27,
                    35,
                    5,
                    328,
                    0
                ],
                "title": "Multi-label Sequential Sentence Classification via Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-label Sequential Sentence Classification via Large Language Model"
                },
                "summary": "Sequential sentence classification (SSC) in scientific publications is\ncrucial for supporting downstream tasks such as fine-grained information\nretrieval and extractive summarization. However, current SSC methods are\nconstrained by model size, sequence length, and single-label setting. To\naddress these limitations, this paper proposes LLM-SSC, a large language model\n(LLM)-based framework for both single- and multi-label SSC tasks. Unlike\nprevious approaches that employ small- or medium-sized language models, the\nproposed framework utilizes LLMs to generate SSC labels through designed\nprompts, which enhance task understanding by incorporating demonstrations and a\nquery to describe the prediction target. We also present a multi-label\ncontrastive learning loss with auto-weighting scheme, enabling the multi-label\nclassification task. To support our multi-label SSC analysis, we introduce and\nrelease a new dataset, biorc800, which mainly contains unstructured abstracts\nin the biomedical domain with manual annotations. Experiments demonstrate\nLLM-SSC's strong performance in SSC under both in-context learning and\ntask-specific tuning settings. We release biorc800 and our code at:\nhttps://github.com/ScienceNLP-Lab/LLM-SSC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequential sentence classification (SSC) in scientific publications is\ncrucial for supporting downstream tasks such as fine-grained information\nretrieval and extractive summarization. However, current SSC methods are\nconstrained by model size, sequence length, and single-label setting. To\naddress these limitations, this paper proposes LLM-SSC, a large language model\n(LLM)-based framework for both single- and multi-label SSC tasks. Unlike\nprevious approaches that employ small- or medium-sized language models, the\nproposed framework utilizes LLMs to generate SSC labels through designed\nprompts, which enhance task understanding by incorporating demonstrations and a\nquery to describe the prediction target. We also present a multi-label\ncontrastive learning loss with auto-weighting scheme, enabling the multi-label\nclassification task. To support our multi-label SSC analysis, we introduce and\nrelease a new dataset, biorc800, which mainly contains unstructured abstracts\nin the biomedical domain with manual annotations. Experiments demonstrate\nLLM-SSC's strong performance in SSC under both in-context learning and\ntask-specific tuning settings. We release biorc800 and our code at:\nhttps://github.com/ScienceNLP-Lab/LLM-SSC."
                },
                "authors": [
                    {
                        "name": "Mengfei Lan"
                    },
                    {
                        "name": "Lecheng Zheng"
                    },
                    {
                        "name": "Shufan Ming"
                    },
                    {
                        "name": "Halil Kilicoglu"
                    }
                ],
                "author_detail": {
                    "name": "Halil Kilicoglu"
                },
                "author": "Halil Kilicoglu",
                "arxiv_comment": "Accepted by EMNLP 2024 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15623v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15623v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2205.15935v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2205.15935v4",
                "updated": "2024-11-29T17:12:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    17,
                    12,
                    44,
                    4,
                    334,
                    0
                ],
                "published": "2022-05-31T16:27:57Z",
                "published_parsed": [
                    2022,
                    5,
                    31,
                    16,
                    27,
                    57,
                    1,
                    151,
                    0
                ],
                "title": "Bias-inducing geometries: an exactly solvable data model with fairness\n  implications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bias-inducing geometries: an exactly solvable data model with fairness\n  implications"
                },
                "summary": "Machine learning (ML) may be oblivious to human bias but it is not immune to\nits perpetuation. Marginalisation and iniquitous group representation are often\ntraceable in the very data used for training, and may be reflected or even\nenhanced by the learning models. In the present work, we aim at clarifying the\nrole played by data geometry in the emergence of ML bias. We introduce an\nexactly solvable high-dimensional model of data imbalance, where parametric\ncontrol over the many bias-inducing factors allows for an extensive exploration\nof the bias inheritance mechanism. Through the tools of statistical physics, we\nanalytically characterise the typical properties of learning models trained in\nthis synthetic framework and obtain exact predictions for the observables that\nare commonly employed for fairness assessment. Despite the simplicity of the\ndata model, we retrace and unpack typical unfairness behaviour observed on\nreal-world datasets. We also obtain a detailed analytical characterisation of a\nclass of bias mitigation strategies. We first consider a basic loss-reweighing\nscheme, which allows for an implicit minimisation of different unfairness\nmetrics, and quantify the incompatibilities between some existing fairness\ncriteria. Then, we consider a novel mitigation strategy based on a matched\ninference approach, consisting in the introduction of coupled learning models.\nOur theoretical analysis of this approach shows that the coupled strategy can\nstrike superior fairness-accuracy trade-offs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning (ML) may be oblivious to human bias but it is not immune to\nits perpetuation. Marginalisation and iniquitous group representation are often\ntraceable in the very data used for training, and may be reflected or even\nenhanced by the learning models. In the present work, we aim at clarifying the\nrole played by data geometry in the emergence of ML bias. We introduce an\nexactly solvable high-dimensional model of data imbalance, where parametric\ncontrol over the many bias-inducing factors allows for an extensive exploration\nof the bias inheritance mechanism. Through the tools of statistical physics, we\nanalytically characterise the typical properties of learning models trained in\nthis synthetic framework and obtain exact predictions for the observables that\nare commonly employed for fairness assessment. Despite the simplicity of the\ndata model, we retrace and unpack typical unfairness behaviour observed on\nreal-world datasets. We also obtain a detailed analytical characterisation of a\nclass of bias mitigation strategies. We first consider a basic loss-reweighing\nscheme, which allows for an implicit minimisation of different unfairness\nmetrics, and quantify the incompatibilities between some existing fairness\ncriteria. Then, we consider a novel mitigation strategy based on a matched\ninference approach, consisting in the introduction of coupled learning models.\nOur theoretical analysis of this approach shows that the coupled strategy can\nstrike superior fairness-accuracy trade-offs."
                },
                "authors": [
                    {
                        "name": "Stefano Sarao Mannelli"
                    },
                    {
                        "name": "Federica Gerace"
                    },
                    {
                        "name": "Negar Rostamzadeh"
                    },
                    {
                        "name": "Luca Saglietti"
                    }
                ],
                "author_detail": {
                    "name": "Luca Saglietti"
                },
                "author": "Luca Saglietti",
                "arxiv_comment": "10 pages + appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2205.15935v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2205.15935v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.dis-nn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18868v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18868v2",
                "updated": "2024-11-29T17:02:31Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    17,
                    2,
                    31,
                    4,
                    334,
                    0
                ],
                "published": "2024-10-24T15:53:21Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    15,
                    53,
                    21,
                    3,
                    298,
                    0
                ],
                "title": "A Riemannian Framework for Learning Reduced-order Lagrangian Dynamics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Riemannian Framework for Learning Reduced-order Lagrangian Dynamics"
                },
                "summary": "By incorporating physical consistency as inductive bias, deep neural networks\ndisplay increased generalization capabilities and data efficiency in learning\nnonlinear dynamic models. However, the complexity of these models generally\nincreases with the system dimensionality, requiring larger datasets, more\ncomplex deep networks, and significant computational effort. We propose a novel\ngeometric network architecture to learn physically-consistent reduced-order\ndynamic parameters that accurately describe the original high-dimensional\nsystem behavior. This is achieved by building on recent advances in model-order\nreduction and by adopting a Riemannian perspective to jointly learn a\nnon-linear structure-preserving latent space and the associated low-dimensional\ndynamics. Our approach enables accurate long-term predictions of the\nhigh-dimensional dynamics of rigid and deformable systems with increased data\nefficiency by inferring interpretable and physically plausible reduced\nLagrangian models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "By incorporating physical consistency as inductive bias, deep neural networks\ndisplay increased generalization capabilities and data efficiency in learning\nnonlinear dynamic models. However, the complexity of these models generally\nincreases with the system dimensionality, requiring larger datasets, more\ncomplex deep networks, and significant computational effort. We propose a novel\ngeometric network architecture to learn physically-consistent reduced-order\ndynamic parameters that accurately describe the original high-dimensional\nsystem behavior. This is achieved by building on recent advances in model-order\nreduction and by adopting a Riemannian perspective to jointly learn a\nnon-linear structure-preserving latent space and the associated low-dimensional\ndynamics. Our approach enables accurate long-term predictions of the\nhigh-dimensional dynamics of rigid and deformable systems with increased data\nefficiency by inferring interpretable and physically plausible reduced\nLagrangian models."
                },
                "authors": [
                    {
                        "name": "Katharina Friedl"
                    },
                    {
                        "name": "NoÃ©mie Jaquier"
                    },
                    {
                        "name": "Jens Lundell"
                    },
                    {
                        "name": "Tamim Asfour"
                    },
                    {
                        "name": "Danica Kragic"
                    }
                ],
                "author_detail": {
                    "name": "Danica Kragic"
                },
                "author": "Danica Kragic",
                "arxiv_comment": "29 pages, 16 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18868v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18868v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19832v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19832v1",
                "updated": "2024-11-29T16:44:02Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    16,
                    44,
                    2,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T16:44:02Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    16,
                    44,
                    2,
                    4,
                    334,
                    0
                ],
                "title": "Sensitive Content Classification in Social Media: A Holistic Resource\n  and Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sensitive Content Classification in Social Media: A Holistic Resource\n  and Evaluation"
                },
                "summary": "The detection of sensitive content in large datasets is crucial for ensuring\nthat shared and analysed data is free from harmful material. However, current\nmoderation tools, such as external APIs, suffer from limitations in\ncustomisation, accuracy across diverse sensitive categories, and privacy\nconcerns. Additionally, existing datasets and open-source models focus\npredominantly on toxic language, leaving gaps in detecting other sensitive\ncategories such as substance abuse or self-harm. In this paper, we put forward\na unified dataset tailored for social media content moderation across six\nsensitive categories: conflictual language, profanity, sexually explicit\nmaterial, drug-related content, self-harm, and spam. By collecting and\nannotating data with consistent retrieval strategies and guidelines, we address\nthe shortcomings of previous focalised research. Our analysis demonstrates that\nfine-tuning large language models (LLMs) on this novel dataset yields\nsignificant improvements in detection performance compared to open\noff-the-shelf models such as LLaMA, and even proprietary OpenAI models, which\nunderperform by 10-15% overall. This limitation is even more pronounced on\npopular moderation APIs, which cannot be easily tailored to specific sensitive\ncontent categories, among others.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The detection of sensitive content in large datasets is crucial for ensuring\nthat shared and analysed data is free from harmful material. However, current\nmoderation tools, such as external APIs, suffer from limitations in\ncustomisation, accuracy across diverse sensitive categories, and privacy\nconcerns. Additionally, existing datasets and open-source models focus\npredominantly on toxic language, leaving gaps in detecting other sensitive\ncategories such as substance abuse or self-harm. In this paper, we put forward\na unified dataset tailored for social media content moderation across six\nsensitive categories: conflictual language, profanity, sexually explicit\nmaterial, drug-related content, self-harm, and spam. By collecting and\nannotating data with consistent retrieval strategies and guidelines, we address\nthe shortcomings of previous focalised research. Our analysis demonstrates that\nfine-tuning large language models (LLMs) on this novel dataset yields\nsignificant improvements in detection performance compared to open\noff-the-shelf models such as LLaMA, and even proprietary OpenAI models, which\nunderperform by 10-15% overall. This limitation is even more pronounced on\npopular moderation APIs, which cannot be easily tailored to specific sensitive\ncontent categories, among others."
                },
                "authors": [
                    {
                        "name": "Dimosthenis Antypas"
                    },
                    {
                        "name": "Indira Sen"
                    },
                    {
                        "name": "Carla Perez-Almendros"
                    },
                    {
                        "name": "Jose Camacho-Collados"
                    },
                    {
                        "name": "Francesco Barbieri"
                    }
                ],
                "author_detail": {
                    "name": "Francesco Barbieri"
                },
                "author": "Francesco Barbieri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19832v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19832v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19828v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19828v1",
                "updated": "2024-11-29T16:41:30Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    16,
                    41,
                    30,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T16:41:30Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    16,
                    41,
                    30,
                    4,
                    334,
                    0
                ],
                "title": "Classical transport in a maximally chaotic chain",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Classical transport in a maximally chaotic chain"
                },
                "summary": "A model for a lattice of coupled cat maps has been recently introduced. This\nnew and specific choice of the coupling makes the description especially easy\nand nontrivial quantities as Lyapunov exponents determined exactly. We studied\nthe ergodic property of the dynamics along such a chain for a local\nperturbation. While the perturbation spreads across a front growing\nballistically, the position and momentum profiles show large fluctuations due\nto chaos leading to diffusive transport in the phase space. It provides an\nexample where the diffusion can be directly inferred from the microscopic\nchaos.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A model for a lattice of coupled cat maps has been recently introduced. This\nnew and specific choice of the coupling makes the description especially easy\nand nontrivial quantities as Lyapunov exponents determined exactly. We studied\nthe ergodic property of the dynamics along such a chain for a local\nperturbation. While the perturbation spreads across a front growing\nballistically, the position and momentum profiles show large fluctuations due\nto chaos leading to diffusive transport in the phase space. It provides an\nexample where the diffusion can be directly inferred from the microscopic\nchaos."
                },
                "authors": [
                    {
                        "name": "William Alderson"
                    },
                    {
                        "name": "RÃ©my Dubertrand"
                    },
                    {
                        "name": "Akira Shudo"
                    }
                ],
                "author_detail": {
                    "name": "Akira Shudo"
                },
                "author": "Akira Shudo",
                "arxiv_comment": "28 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19828v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19828v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "nlin.CD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "nlin.CD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.stat-mech",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.MP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19824v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19824v1",
                "updated": "2024-11-29T16:34:46Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    16,
                    34,
                    46,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T16:34:46Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    16,
                    34,
                    46,
                    4,
                    334,
                    0
                ],
                "title": "SAT-HMR: Real-Time Multi-Person 3D Mesh Estimation via Scale-Adaptive\n  Tokens",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SAT-HMR: Real-Time Multi-Person 3D Mesh Estimation via Scale-Adaptive\n  Tokens"
                },
                "summary": "We propose a one-stage framework for real-time multi-person 3D human mesh\nestimation from a single RGB image. While current one-stage methods, which\nfollow a DETR-style pipeline, achieve state-of-the-art (SOTA) performance with\nhigh-resolution inputs, we observe that this particularly benefits the\nestimation of individuals in smaller scales of the image (e.g., those far from\nthe camera), but at the cost of significantly increased computation overhead.\nTo address this, we introduce scale-adaptive tokens that are dynamically\nadjusted based on the relative scale of each individual in the image within the\nDETR framework. Specifically, individuals in smaller scales are processed at\nhigher resolutions, larger ones at lower resolutions, and background regions\nare further distilled. These scale-adaptive tokens more efficiently encode the\nimage features, facilitating subsequent decoding to regress the human mesh,\nwhile allowing the model to allocate computational resources more effectively\nand focus on more challenging cases. Experiments show that our method preserves\nthe accuracy benefits of high-resolution processing while substantially\nreducing computational cost, achieving real-time inference with performance\ncomparable to SOTA methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a one-stage framework for real-time multi-person 3D human mesh\nestimation from a single RGB image. While current one-stage methods, which\nfollow a DETR-style pipeline, achieve state-of-the-art (SOTA) performance with\nhigh-resolution inputs, we observe that this particularly benefits the\nestimation of individuals in smaller scales of the image (e.g., those far from\nthe camera), but at the cost of significantly increased computation overhead.\nTo address this, we introduce scale-adaptive tokens that are dynamically\nadjusted based on the relative scale of each individual in the image within the\nDETR framework. Specifically, individuals in smaller scales are processed at\nhigher resolutions, larger ones at lower resolutions, and background regions\nare further distilled. These scale-adaptive tokens more efficiently encode the\nimage features, facilitating subsequent decoding to regress the human mesh,\nwhile allowing the model to allocate computational resources more effectively\nand focus on more challenging cases. Experiments show that our method preserves\nthe accuracy benefits of high-resolution processing while substantially\nreducing computational cost, achieving real-time inference with performance\ncomparable to SOTA methods."
                },
                "authors": [
                    {
                        "name": "Chi Su"
                    },
                    {
                        "name": "Xiaoxuan Ma"
                    },
                    {
                        "name": "Jiajun Su"
                    },
                    {
                        "name": "Yizhou Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yizhou Wang"
                },
                "author": "Yizhou Wang",
                "arxiv_comment": "16 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19824v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19824v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19819v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19819v1",
                "updated": "2024-11-29T16:27:55Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    16,
                    27,
                    55,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T16:27:55Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    16,
                    27,
                    55,
                    4,
                    334,
                    0
                ],
                "title": "GradAlign for Training-free Model Performance Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GradAlign for Training-free Model Performance Inference"
                },
                "summary": "Architecture plays an important role in deciding the performance of deep\nneural networks. However, the search for the optimal architecture is often\nhindered by the vast search space, making it a time-intensive process.\nRecently, a novel approach known as training-free neural architecture search\n(NAS) has emerged, aiming to discover the ideal architecture without\nnecessitating extensive training. Training-free NAS leverages various\nindicators for architecture selection, including metrics such as the count of\nlinear regions, the density of per-sample losses, and the stability of the\nfinite-width Neural Tangent Kernel (NTK) matrix. Despite the competitive\nempirical performance of current training-free NAS techniques, they suffer from\ncertain limitations, including inconsistent performance and a lack of deep\nunderstanding. In this paper, we introduce GradAlign, a simple yet effective\nmethod designed for inferring model performance without the need for training.\nAt its core, GradAlign quantifies the extent of conflicts within per-sample\ngradients during initialization, as substantial conflicts hinder model\nconvergence and ultimately result in worse performance. We evaluate GradAlign\nagainst established training-free NAS methods using standard NAS benchmarks,\nshowing a better overall performance. Moreover, we show that the widely adopted\nmetric of linear region count may not suffice as a dependable criterion for\nselecting network architectures during at initialization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Architecture plays an important role in deciding the performance of deep\nneural networks. However, the search for the optimal architecture is often\nhindered by the vast search space, making it a time-intensive process.\nRecently, a novel approach known as training-free neural architecture search\n(NAS) has emerged, aiming to discover the ideal architecture without\nnecessitating extensive training. Training-free NAS leverages various\nindicators for architecture selection, including metrics such as the count of\nlinear regions, the density of per-sample losses, and the stability of the\nfinite-width Neural Tangent Kernel (NTK) matrix. Despite the competitive\nempirical performance of current training-free NAS techniques, they suffer from\ncertain limitations, including inconsistent performance and a lack of deep\nunderstanding. In this paper, we introduce GradAlign, a simple yet effective\nmethod designed for inferring model performance without the need for training.\nAt its core, GradAlign quantifies the extent of conflicts within per-sample\ngradients during initialization, as substantial conflicts hinder model\nconvergence and ultimately result in worse performance. We evaluate GradAlign\nagainst established training-free NAS methods using standard NAS benchmarks,\nshowing a better overall performance. Moreover, we show that the widely adopted\nmetric of linear region count may not suffice as a dependable criterion for\nselecting network architectures during at initialization."
                },
                "authors": [
                    {
                        "name": "Yuxuan Li"
                    },
                    {
                        "name": "Yunhui Guo"
                    }
                ],
                "author_detail": {
                    "name": "Yunhui Guo"
                },
                "author": "Yunhui Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19819v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19819v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.18653v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.18653v2",
                "updated": "2024-11-29T16:19:01Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    16,
                    19,
                    1,
                    4,
                    334,
                    0
                ],
                "published": "2024-05-28T23:32:46Z",
                "published_parsed": [
                    2024,
                    5,
                    28,
                    23,
                    32,
                    46,
                    1,
                    149,
                    0
                ],
                "title": "Recent Advances of Foundation Language Models-based Continual Learning:\n  A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent Advances of Foundation Language Models-based Continual Learning:\n  A Survey"
                },
                "summary": "Recently, foundation language models (LMs) have marked significant\nachievements in the domains of natural language processing (NLP) and computer\nvision (CV). Unlike traditional neural network models, foundation LMs obtain a\ngreat ability for transfer learning by acquiring rich commonsense knowledge\nthrough pre-training on extensive unsupervised datasets with a vast number of\nparameters. However, they still can not emulate human-like continuous learning\ndue to catastrophic forgetting. Consequently, various continual learning\n(CL)-based methodologies have been developed to refine LMs, enabling them to\nadapt to new tasks without forgetting previous knowledge. However, a systematic\ntaxonomy of existing approaches and a comparison of their performance are still\nlacking, which is the gap that our survey aims to fill. We delve into a\ncomprehensive review, summarization, and classification of the existing\nliterature on CL-based approaches applied to foundation language models, such\nas pre-trained language models (PLMs), large language models (LLMs) and\nvision-language models (VLMs). We divide these studies into offline CL and\nonline CL, which consist of traditional methods, parameter-efficient-based\nmethods, instruction tuning-based methods and continual pre-training methods.\nOffline CL encompasses domain-incremental learning, task-incremental learning,\nand class-incremental learning, while online CL is subdivided into hard task\nboundary and blurry task boundary settings. Additionally, we outline the\ntypical datasets and metrics employed in CL research and provide a detailed\nanalysis of the challenges and future work for LMs-based continual learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, foundation language models (LMs) have marked significant\nachievements in the domains of natural language processing (NLP) and computer\nvision (CV). Unlike traditional neural network models, foundation LMs obtain a\ngreat ability for transfer learning by acquiring rich commonsense knowledge\nthrough pre-training on extensive unsupervised datasets with a vast number of\nparameters. However, they still can not emulate human-like continuous learning\ndue to catastrophic forgetting. Consequently, various continual learning\n(CL)-based methodologies have been developed to refine LMs, enabling them to\nadapt to new tasks without forgetting previous knowledge. However, a systematic\ntaxonomy of existing approaches and a comparison of their performance are still\nlacking, which is the gap that our survey aims to fill. We delve into a\ncomprehensive review, summarization, and classification of the existing\nliterature on CL-based approaches applied to foundation language models, such\nas pre-trained language models (PLMs), large language models (LLMs) and\nvision-language models (VLMs). We divide these studies into offline CL and\nonline CL, which consist of traditional methods, parameter-efficient-based\nmethods, instruction tuning-based methods and continual pre-training methods.\nOffline CL encompasses domain-incremental learning, task-incremental learning,\nand class-incremental learning, while online CL is subdivided into hard task\nboundary and blurry task boundary settings. Additionally, we outline the\ntypical datasets and metrics employed in CL research and provide a detailed\nanalysis of the challenges and future work for LMs-based continual learning."
                },
                "authors": [
                    {
                        "name": "Yutao Yang"
                    },
                    {
                        "name": "Jie Zhou"
                    },
                    {
                        "name": "Xuanwen Ding"
                    },
                    {
                        "name": "Tianyu Huai"
                    },
                    {
                        "name": "Shunyu Liu"
                    },
                    {
                        "name": "Qin Chen"
                    },
                    {
                        "name": "Yuan Xie"
                    },
                    {
                        "name": "Liang He"
                    }
                ],
                "author_detail": {
                    "name": "Liang He"
                },
                "author": "Liang He",
                "arxiv_comment": "Accepted by ACM Computing Survey",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.18653v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.18653v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08130v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08130v2",
                "updated": "2024-11-29T16:18:29Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    16,
                    18,
                    29,
                    4,
                    334,
                    0
                ],
                "published": "2024-10-10T17:14:36Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    17,
                    14,
                    36,
                    3,
                    284,
                    0
                ],
                "title": "Think Beyond Size: Adaptive Prompting for More Effective Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Think Beyond Size: Adaptive Prompting for More Effective Reasoning"
                },
                "summary": "Pretrained large language models (LLMs) are increasingly utilized across a\nwide range of natural language processing (NLP) tasks due to their impressive\ncapabilities as few-shot learners. Recent techniques, such as chain-of-thought\n(CoT) prompting, have significantly advanced multi-step reasoning by\nintroducing step-by-step decomposition, achieving state-of-the-art results on\ncomplex reasoning benchmarks. However, these approaches often rely on static\nprompting templates that do not adapt to task complexity or errors during the\nreasoning process. In this work, we introduce Adaptive Prompting, a dynamic and\niterative framework designed to enhance reasoning by incorporating real-time\nadjustments to prompt structures and validation mechanisms.Experimental results\ndemonstrate that Adaptive Prompting significantly improves performance on\ndiverse reasoning benchmarks, including arithmetic reasoning (GSM8K,\nMultiArith), logical reasoning and commonsense tasks, achieving substantial\naccuracy gains compared to static prompting baselines. By integrating guided\nprompts, intermediate validation, and self-corrective steps, our approach\nenables smaller models to achieve competitive performance with larger\ncounterparts, such as GPT-4, while maintaining computational efficiency. The\nframework achieves this without requiring fine-tuning or task-specific training\ndata, highlighting the untapped potential of iterative reasoning methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pretrained large language models (LLMs) are increasingly utilized across a\nwide range of natural language processing (NLP) tasks due to their impressive\ncapabilities as few-shot learners. Recent techniques, such as chain-of-thought\n(CoT) prompting, have significantly advanced multi-step reasoning by\nintroducing step-by-step decomposition, achieving state-of-the-art results on\ncomplex reasoning benchmarks. However, these approaches often rely on static\nprompting templates that do not adapt to task complexity or errors during the\nreasoning process. In this work, we introduce Adaptive Prompting, a dynamic and\niterative framework designed to enhance reasoning by incorporating real-time\nadjustments to prompt structures and validation mechanisms.Experimental results\ndemonstrate that Adaptive Prompting significantly improves performance on\ndiverse reasoning benchmarks, including arithmetic reasoning (GSM8K,\nMultiArith), logical reasoning and commonsense tasks, achieving substantial\naccuracy gains compared to static prompting baselines. By integrating guided\nprompts, intermediate validation, and self-corrective steps, our approach\nenables smaller models to achieve competitive performance with larger\ncounterparts, such as GPT-4, while maintaining computational efficiency. The\nframework achieves this without requiring fine-tuning or task-specific training\ndata, highlighting the untapped potential of iterative reasoning methods."
                },
                "authors": [
                    {
                        "name": "Kamesh R"
                    }
                ],
                "author_detail": {
                    "name": "Kamesh R"
                },
                "author": "Kamesh R",
                "arxiv_comment": "Submitted to ICLR 2025. This is a preprint version. Future revisions\n  will include additional evaluations and refinements",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08130v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08130v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19804v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19804v1",
                "updated": "2024-11-29T16:09:43Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    16,
                    9,
                    43,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T16:09:43Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    16,
                    9,
                    43,
                    4,
                    334,
                    0
                ],
                "title": "Advanced System Integration: Analyzing OpenAPI Chunking for\n  Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advanced System Integration: Analyzing OpenAPI Chunking for\n  Retrieval-Augmented Generation"
                },
                "summary": "Integrating multiple (sub-)systems is essential to create advanced\nInformation Systems (ISs). Difficulties mainly arise when integrating dynamic\nenvironments across the IS lifecycle. A traditional approach is a registry that\nprovides the API documentation of the systems' endpoints. Large Language Models\n(LLMs) have shown to be capable of automatically creating system integrations\n(e.g., as service composition) based on this documentation but require concise\ninput due to input token limitations, especially regarding comprehensive API\ndescriptions. Currently, it is unknown how best to preprocess these API\ndescriptions. Within this work, we (i) analyze the usage of Retrieval Augmented\nGeneration (RAG) for endpoint discovery and the chunking, i.e., preprocessing,\nof OpenAPIs to reduce the input token length while preserving the most relevant\ninformation. To further reduce the input token length for the composition\nprompt and improve endpoint retrieval, we propose (ii) a Discovery Agent that\nonly receives a summary of the most relevant endpoints and retrieves details on\ndemand. We evaluate RAG for endpoint discovery using the RestBench benchmark,\nfirst, for the different chunking possibilities and parameters measuring the\nendpoint retrieval recall, precision, and F1 score. Then, we assess the\nDiscovery Agent using the same test set. With our prototype, we demonstrate how\nto successfully employ RAG for endpoint discovery to reduce the token count.\nWhile revealing high values for recall, precision, and F1, further research is\nnecessary to retrieve all requisite endpoints. Our experiments show that for\npreprocessing, LLM-based and format-specific approaches outperform na\\\"ive\nchunking methods. Relying on an agent further enhances these results as the\nagent splits the tasks into multiple fine granular subtasks, improving the\noverall RAG performance in the token count, precision, and F1 score.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating multiple (sub-)systems is essential to create advanced\nInformation Systems (ISs). Difficulties mainly arise when integrating dynamic\nenvironments across the IS lifecycle. A traditional approach is a registry that\nprovides the API documentation of the systems' endpoints. Large Language Models\n(LLMs) have shown to be capable of automatically creating system integrations\n(e.g., as service composition) based on this documentation but require concise\ninput due to input token limitations, especially regarding comprehensive API\ndescriptions. Currently, it is unknown how best to preprocess these API\ndescriptions. Within this work, we (i) analyze the usage of Retrieval Augmented\nGeneration (RAG) for endpoint discovery and the chunking, i.e., preprocessing,\nof OpenAPIs to reduce the input token length while preserving the most relevant\ninformation. To further reduce the input token length for the composition\nprompt and improve endpoint retrieval, we propose (ii) a Discovery Agent that\nonly receives a summary of the most relevant endpoints and retrieves details on\ndemand. We evaluate RAG for endpoint discovery using the RestBench benchmark,\nfirst, for the different chunking possibilities and parameters measuring the\nendpoint retrieval recall, precision, and F1 score. Then, we assess the\nDiscovery Agent using the same test set. With our prototype, we demonstrate how\nto successfully employ RAG for endpoint discovery to reduce the token count.\nWhile revealing high values for recall, precision, and F1, further research is\nnecessary to retrieve all requisite endpoints. Our experiments show that for\npreprocessing, LLM-based and format-specific approaches outperform na\\\"ive\nchunking methods. Relying on an agent further enhances these results as the\nagent splits the tasks into multiple fine granular subtasks, improving the\noverall RAG performance in the token count, precision, and F1 score."
                },
                "authors": [
                    {
                        "name": "Robin D. Pesl"
                    },
                    {
                        "name": "Jerin G. Mathew"
                    },
                    {
                        "name": "Massimo Mecella"
                    },
                    {
                        "name": "Marco Aiello"
                    }
                ],
                "author_detail": {
                    "name": "Marco Aiello"
                },
                "author": "Marco Aiello",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19804v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19804v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.02791v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.02791v3",
                "updated": "2024-11-29T16:03:59Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    16,
                    3,
                    59,
                    4,
                    334,
                    0
                ],
                "published": "2024-05-05T02:11:57Z",
                "published_parsed": [
                    2024,
                    5,
                    5,
                    2,
                    11,
                    57,
                    6,
                    126,
                    0
                ],
                "title": "Efficient Text-driven Motion Generation via Latent Consistency Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Text-driven Motion Generation via Latent Consistency Training"
                },
                "summary": "Text-driven human motion generation based on diffusion strategies establishes\na reliable foundation for multimodal applications in human-computer\ninteractions. However, existing advances face significant efficiency challenges\ndue to the substantial computational overhead of iteratively solving for\nnonlinear reverse diffusion trajectories during the inference phase. To this\nend, we propose the motion latent consistency training framework (MLCT), which\nprecomputes reverse diffusion trajectories from raw data in the training phase\nand enables few-step or single-step inference via self-consistency constraints\nin the inference phase. Specifically, a motion autoencoder with quantization\nconstraints is first proposed for constructing concise and bounded solution\ndistributions for motion diffusion processes. Subsequently, a classifier-free\nguidance format is constructed via an additional unconditional loss function to\naccomplish the precomputation of conditional diffusion trajectories in the\ntraining phase. Finally, a clustering guidance module based on the\nK-nearest-neighbor algorithm is developed for the chain-conduction optimization\nmechanism of self-consistency constraints, which provides additional references\nof solution distributions at a small query cost. By combining these\nenhancements, we achieve stable and consistency training in non-pixel modality\nand latent representation spaces. Benchmark experiments demonstrate that our\nmethod significantly outperforms traditional consistency distillation methods\nwith reduced training cost and enhances the consistency model to perform\ncomparably to state-of-the-art models with lower inference costs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-driven human motion generation based on diffusion strategies establishes\na reliable foundation for multimodal applications in human-computer\ninteractions. However, existing advances face significant efficiency challenges\ndue to the substantial computational overhead of iteratively solving for\nnonlinear reverse diffusion trajectories during the inference phase. To this\nend, we propose the motion latent consistency training framework (MLCT), which\nprecomputes reverse diffusion trajectories from raw data in the training phase\nand enables few-step or single-step inference via self-consistency constraints\nin the inference phase. Specifically, a motion autoencoder with quantization\nconstraints is first proposed for constructing concise and bounded solution\ndistributions for motion diffusion processes. Subsequently, a classifier-free\nguidance format is constructed via an additional unconditional loss function to\naccomplish the precomputation of conditional diffusion trajectories in the\ntraining phase. Finally, a clustering guidance module based on the\nK-nearest-neighbor algorithm is developed for the chain-conduction optimization\nmechanism of self-consistency constraints, which provides additional references\nof solution distributions at a small query cost. By combining these\nenhancements, we achieve stable and consistency training in non-pixel modality\nand latent representation spaces. Benchmark experiments demonstrate that our\nmethod significantly outperforms traditional consistency distillation methods\nwith reduced training cost and enhances the consistency model to perform\ncomparably to state-of-the-art models with lower inference costs."
                },
                "authors": [
                    {
                        "name": "Mengxian Hu"
                    },
                    {
                        "name": "Minghao Zhu"
                    },
                    {
                        "name": "Xun Zhou"
                    },
                    {
                        "name": "Qingqing Yan"
                    },
                    {
                        "name": "Shu Li"
                    },
                    {
                        "name": "Chengju Liu"
                    },
                    {
                        "name": "Qijun Chen"
                    }
                ],
                "author_detail": {
                    "name": "Qijun Chen"
                },
                "author": "Qijun Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.02791v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.02791v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19799v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19799v1",
                "updated": "2024-11-29T16:03:14Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    16,
                    3,
                    14,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T16:03:14Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    16,
                    3,
                    14,
                    4,
                    334,
                    0
                ],
                "title": "INCLUDE: Evaluating Multilingual Language Understanding with Regional\n  Knowledge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "INCLUDE: Evaluating Multilingual Language Understanding with Regional\n  Knowledge"
                },
                "summary": "The performance differential of large language models (LLM) between languages\nhinders their effective deployment in many regions, inhibiting the potential\neconomic and societal value of generative AI tools in many communities.\nHowever, the development of functional LLMs in many languages (\\ie,\nmultilingual LLMs) is bottlenecked by the lack of high-quality evaluation\nresources in languages other than English. Moreover, current practices in\nmultilingual benchmark construction often translate English resources, ignoring\nthe regional and cultural knowledge of the environments in which multilingual\nsystems would be used. In this work, we construct an evaluation suite of\n197,243 QA pairs from local exam sources to measure the capabilities of\nmultilingual LLMs in a variety of regional contexts. Our novel resource,\nINCLUDE, is a comprehensive knowledge- and reasoning-centric benchmark across\n44 written languages that evaluates multilingual LLMs for performance in the\nactual language environments where they would be deployed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The performance differential of large language models (LLM) between languages\nhinders their effective deployment in many regions, inhibiting the potential\neconomic and societal value of generative AI tools in many communities.\nHowever, the development of functional LLMs in many languages (\\ie,\nmultilingual LLMs) is bottlenecked by the lack of high-quality evaluation\nresources in languages other than English. Moreover, current practices in\nmultilingual benchmark construction often translate English resources, ignoring\nthe regional and cultural knowledge of the environments in which multilingual\nsystems would be used. In this work, we construct an evaluation suite of\n197,243 QA pairs from local exam sources to measure the capabilities of\nmultilingual LLMs in a variety of regional contexts. Our novel resource,\nINCLUDE, is a comprehensive knowledge- and reasoning-centric benchmark across\n44 written languages that evaluates multilingual LLMs for performance in the\nactual language environments where they would be deployed."
                },
                "authors": [
                    {
                        "name": "Angelika Romanou"
                    },
                    {
                        "name": "Negar Foroutan"
                    },
                    {
                        "name": "Anna Sotnikova"
                    },
                    {
                        "name": "Zeming Chen"
                    },
                    {
                        "name": "Sree Harsha Nelaturu"
                    },
                    {
                        "name": "Shivalika Singh"
                    },
                    {
                        "name": "Rishabh Maheshwary"
                    },
                    {
                        "name": "Micol Altomare"
                    },
                    {
                        "name": "Mohamed A. Haggag"
                    },
                    {
                        "name": "Snegha A"
                    },
                    {
                        "name": "Alfonso Amayuelas"
                    },
                    {
                        "name": "Azril Hafizi Amirudin"
                    },
                    {
                        "name": "Viraat Aryabumi"
                    },
                    {
                        "name": "Danylo Boiko"
                    },
                    {
                        "name": "Michael Chang"
                    },
                    {
                        "name": "Jenny Chim"
                    },
                    {
                        "name": "Gal Cohen"
                    },
                    {
                        "name": "Aditya Kumar Dalmia"
                    },
                    {
                        "name": "Abraham Diress"
                    },
                    {
                        "name": "Sharad Duwal"
                    },
                    {
                        "name": "Daniil Dzenhaliou"
                    },
                    {
                        "name": "Daniel Fernando Erazo Florez"
                    },
                    {
                        "name": "Fabian Farestam"
                    },
                    {
                        "name": "Joseph Marvin Imperial"
                    },
                    {
                        "name": "Shayekh Bin Islam"
                    },
                    {
                        "name": "Perttu Isotalo"
                    },
                    {
                        "name": "Maral Jabbarishiviari"
                    },
                    {
                        "name": "BÃ¶rje F. Karlsson"
                    },
                    {
                        "name": "Eldar Khalilov"
                    },
                    {
                        "name": "Christopher Klamm"
                    },
                    {
                        "name": "Fajri Koto"
                    },
                    {
                        "name": "Dominik KrzemiÅski"
                    },
                    {
                        "name": "Gabriel Adriano de Melo"
                    },
                    {
                        "name": "Syrielle Montariol"
                    },
                    {
                        "name": "Yiyang Nan"
                    },
                    {
                        "name": "Joel Niklaus"
                    },
                    {
                        "name": "Jekaterina Novikova"
                    },
                    {
                        "name": "Johan Samir Obando Ceron"
                    },
                    {
                        "name": "Debjit Paul"
                    },
                    {
                        "name": "Esther Ploeger"
                    },
                    {
                        "name": "Jebish Purbey"
                    },
                    {
                        "name": "Swati Rajwal"
                    },
                    {
                        "name": "Selvan Sunitha Ravi"
                    },
                    {
                        "name": "Sara Rydell"
                    },
                    {
                        "name": "Roshan Santhosh"
                    },
                    {
                        "name": "Drishti Sharma"
                    },
                    {
                        "name": "Marjana Prifti Skenduli"
                    },
                    {
                        "name": "Arshia Soltani Moakhar"
                    },
                    {
                        "name": "Bardia Soltani Moakhar"
                    },
                    {
                        "name": "Ran Tamir"
                    },
                    {
                        "name": "Ayush Kumar Tarun"
                    },
                    {
                        "name": "Azmine Toushik Wasi"
                    },
                    {
                        "name": "Thenuka Ovin Weerasinghe"
                    },
                    {
                        "name": "Serhan Yilmaz"
                    },
                    {
                        "name": "Mike Zhang"
                    },
                    {
                        "name": "Imanol Schlag"
                    },
                    {
                        "name": "Marzieh Fadaee"
                    },
                    {
                        "name": "Sara Hooker"
                    },
                    {
                        "name": "Antoine Bosselut"
                    }
                ],
                "author_detail": {
                    "name": "Antoine Bosselut"
                },
                "author": "Antoine Bosselut",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19799v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19799v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.13842v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.13842v2",
                "updated": "2024-11-29T16:02:21Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    16,
                    2,
                    21,
                    4,
                    334,
                    0
                ],
                "published": "2023-12-21T13:40:31Z",
                "published_parsed": [
                    2023,
                    12,
                    21,
                    13,
                    40,
                    31,
                    3,
                    355,
                    0
                ],
                "title": "Statistical learning theory and Occam's razor: The core argument",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Statistical learning theory and Occam's razor: The core argument"
                },
                "summary": "Statistical learning theory is often associated with the principle of Occam's\nrazor, which recommends a simplicity preference in inductive inference. This\npaper distills the core argument for simplicity obtainable from statistical\nlearning theory, built on the theory's central learning guarantee for the\nmethod of empirical risk minimization. This core \"means-ends\" argument is that\na simpler hypothesis class or inductive model is better because it has better\nlearning guarantees; however, these guarantees are model-relative and so the\ntheoretical push towards simplicity is checked by our prior knowledge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Statistical learning theory is often associated with the principle of Occam's\nrazor, which recommends a simplicity preference in inductive inference. This\npaper distills the core argument for simplicity obtainable from statistical\nlearning theory, built on the theory's central learning guarantee for the\nmethod of empirical risk minimization. This core \"means-ends\" argument is that\na simpler hypothesis class or inductive model is better because it has better\nlearning guarantees; however, these guarantees are model-relative and so the\ntheoretical push towards simplicity is checked by our prior knowledge."
                },
                "authors": [
                    {
                        "name": "Tom F. Sterkenburg"
                    }
                ],
                "author_detail": {
                    "name": "Tom F. Sterkenburg"
                },
                "author": "Tom F. Sterkenburg",
                "arxiv_doi": "10.1007/s11023-024-09703-y",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/s11023-024-09703-y",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2312.13842v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.13842v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19789v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19789v1",
                "updated": "2024-11-29T15:51:55Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    15,
                    51,
                    55,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T15:51:55Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    15,
                    51,
                    55,
                    4,
                    334,
                    0
                ],
                "title": "Adjusting auxiliary variables under approximate neighborhood\n  interference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adjusting auxiliary variables under approximate neighborhood\n  interference"
                },
                "summary": "Randomized experiments are the gold standard for causal inference. However,\ntraditional assumptions, such as the Stable Unit Treatment Value Assumption\n(SUTVA), often fail in real-world settings where interference between units is\npresent. Network interference, in particular, has garnered significant\nattention. Structural models, like the linear-in-means model, are commonly used\nto describe interference; but they rely on the correct specification of the\nmodel, which can be restrictive. Recent advancements in the literature, such as\nthe Approximate Neighborhood Interference (ANI) framework, offer more flexible\napproaches by assuming negligible interference from distant units. In this\npaper, we introduce a general framework for regression adjustment for the\nnetwork experiments under the ANI assumption. This framework expands\ntraditional regression adjustment by accounting for imbalances in network-based\ncovariates, ensuring precision improvement, and providing shorter confidence\nintervals. We establish the validity of our approach using a design-based\ninference framework, which relies solely on randomization of treatment\nassignments for inference without requiring correctly specified outcome models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Randomized experiments are the gold standard for causal inference. However,\ntraditional assumptions, such as the Stable Unit Treatment Value Assumption\n(SUTVA), often fail in real-world settings where interference between units is\npresent. Network interference, in particular, has garnered significant\nattention. Structural models, like the linear-in-means model, are commonly used\nto describe interference; but they rely on the correct specification of the\nmodel, which can be restrictive. Recent advancements in the literature, such as\nthe Approximate Neighborhood Interference (ANI) framework, offer more flexible\napproaches by assuming negligible interference from distant units. In this\npaper, we introduce a general framework for regression adjustment for the\nnetwork experiments under the ANI assumption. This framework expands\ntraditional regression adjustment by accounting for imbalances in network-based\ncovariates, ensuring precision improvement, and providing shorter confidence\nintervals. We establish the validity of our approach using a design-based\ninference framework, which relies solely on randomization of treatment\nassignments for inference without requiring correctly specified outcome models."
                },
                "authors": [
                    {
                        "name": "Xin Lu"
                    },
                    {
                        "name": "Yuhao Wang"
                    },
                    {
                        "name": "Zhiheng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Zhiheng Zhang"
                },
                "author": "Zhiheng Zhang",
                "arxiv_comment": "46 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19789v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19789v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2306.13549v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2306.13549v4",
                "updated": "2024-11-29T15:51:23Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    15,
                    51,
                    23,
                    4,
                    334,
                    0
                ],
                "published": "2023-06-23T15:21:52Z",
                "published_parsed": [
                    2023,
                    6,
                    23,
                    15,
                    21,
                    52,
                    4,
                    174,
                    0
                ],
                "title": "A Survey on Multimodal Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Multimodal Large Language Models"
                },
                "summary": "Recently, Multimodal Large Language Model (MLLM) represented by GPT-4V has\nbeen a new rising research hotspot, which uses powerful Large Language Models\n(LLMs) as a brain to perform multimodal tasks. The surprising emergent\ncapabilities of MLLM, such as writing stories based on images and OCR-free math\nreasoning, are rare in traditional multimodal methods, suggesting a potential\npath to artificial general intelligence. To this end, both academia and\nindustry have endeavored to develop MLLMs that can compete with or even better\nthan GPT-4V, pushing the limit of research at a surprising speed. In this\npaper, we aim to trace and summarize the recent progress of MLLMs. First of\nall, we present the basic formulation of MLLM and delineate its related\nconcepts, including architecture, training strategy and data, as well as\nevaluation. Then, we introduce research topics about how MLLMs can be extended\nto support more granularity, modalities, languages, and scenarios. We continue\nwith multimodal hallucination and extended techniques, including Multimodal ICL\n(M-ICL), Multimodal CoT (M-CoT), and LLM-Aided Visual Reasoning (LAVR). To\nconclude the paper, we discuss existing challenges and point out promising\nresearch directions. In light of the fact that the era of MLLM has only just\nbegun, we will keep updating this survey and hope it can inspire more research.\nAn associated GitHub link collecting the latest papers is available at\nhttps://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, Multimodal Large Language Model (MLLM) represented by GPT-4V has\nbeen a new rising research hotspot, which uses powerful Large Language Models\n(LLMs) as a brain to perform multimodal tasks. The surprising emergent\ncapabilities of MLLM, such as writing stories based on images and OCR-free math\nreasoning, are rare in traditional multimodal methods, suggesting a potential\npath to artificial general intelligence. To this end, both academia and\nindustry have endeavored to develop MLLMs that can compete with or even better\nthan GPT-4V, pushing the limit of research at a surprising speed. In this\npaper, we aim to trace and summarize the recent progress of MLLMs. First of\nall, we present the basic formulation of MLLM and delineate its related\nconcepts, including architecture, training strategy and data, as well as\nevaluation. Then, we introduce research topics about how MLLMs can be extended\nto support more granularity, modalities, languages, and scenarios. We continue\nwith multimodal hallucination and extended techniques, including Multimodal ICL\n(M-ICL), Multimodal CoT (M-CoT), and LLM-Aided Visual Reasoning (LAVR). To\nconclude the paper, we discuss existing challenges and point out promising\nresearch directions. In light of the fact that the era of MLLM has only just\nbegun, we will keep updating this survey and hope it can inspire more research.\nAn associated GitHub link collecting the latest papers is available at\nhttps://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models."
                },
                "authors": [
                    {
                        "name": "Shukang Yin"
                    },
                    {
                        "name": "Chaoyou Fu"
                    },
                    {
                        "name": "Sirui Zhao"
                    },
                    {
                        "name": "Ke Li"
                    },
                    {
                        "name": "Xing Sun"
                    },
                    {
                        "name": "Tong Xu"
                    },
                    {
                        "name": "Enhong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Enhong Chen"
                },
                "author": "Enhong Chen",
                "arxiv_doi": "10.1093/nsr/nwae403",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1093/nsr/nwae403",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2306.13549v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2306.13549v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted for publication in National Science Review. Project\n  page:https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.02837v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.02837v2",
                "updated": "2024-11-29T15:49:20Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    15,
                    49,
                    20,
                    4,
                    334,
                    0
                ],
                "published": "2024-04-03T16:16:31Z",
                "published_parsed": [
                    2024,
                    4,
                    3,
                    16,
                    16,
                    31,
                    2,
                    94,
                    0
                ],
                "title": "Cherry on Top: Parameter Heterogeneity and Quantization in Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cherry on Top: Parameter Heterogeneity and Quantization in Large\n  Language Models"
                },
                "summary": "This paper reveals the phenomenon of parameter heterogeneity in large\nlanguage models (LLMs). We find that a small subset of \"cherry\" parameters\nexhibit a disproportionately large influence on model performance, while the\nvast majority of parameters have minimal impact. This heterogeneity is found to\nbe prevalent across different model families, scales, and types. Motivated by\nthis observation, we propose CherryQ, a novel quantization method that unifies\nthe optimization of mixed-precision parameters. CherryQ identifies and\npreserves the critical cherry parameters in high precision while aggressively\nquantizing the remaining parameters to low precision. Extensive experiments\ndemonstrate the effectiveness of CherryQ. CherryQ outperforms existing\nquantization approaches in terms of perplexity and downstream task performance.\nNotably, our 3-bit quantized Vicuna-1.5 exhibits competitive performance\ncompared to their 16-bit counterparts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper reveals the phenomenon of parameter heterogeneity in large\nlanguage models (LLMs). We find that a small subset of \"cherry\" parameters\nexhibit a disproportionately large influence on model performance, while the\nvast majority of parameters have minimal impact. This heterogeneity is found to\nbe prevalent across different model families, scales, and types. Motivated by\nthis observation, we propose CherryQ, a novel quantization method that unifies\nthe optimization of mixed-precision parameters. CherryQ identifies and\npreserves the critical cherry parameters in high precision while aggressively\nquantizing the remaining parameters to low precision. Extensive experiments\ndemonstrate the effectiveness of CherryQ. CherryQ outperforms existing\nquantization approaches in terms of perplexity and downstream task performance.\nNotably, our 3-bit quantized Vicuna-1.5 exhibits competitive performance\ncompared to their 16-bit counterparts."
                },
                "authors": [
                    {
                        "name": "Wanyun Cui"
                    },
                    {
                        "name": "Qianle Wang"
                    }
                ],
                "author_detail": {
                    "name": "Qianle Wang"
                },
                "author": "Qianle Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.02837v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.02837v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19774v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19774v1",
                "updated": "2024-11-29T15:20:29Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    15,
                    20,
                    29,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T15:20:29Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    15,
                    20,
                    29,
                    4,
                    334,
                    0
                ],
                "title": "PerLA: Perceptive 3D Language Assistant",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PerLA: Perceptive 3D Language Assistant"
                },
                "summary": "Enabling Large Language Models (LLMs) to understand the 3D physical world is\nan emerging yet challenging research direction. Current strategies for\nprocessing point clouds typically downsample the scene or divide it into\nsmaller parts for separate analysis. However, both approaches risk losing key\nlocal details or global contextual information. In this paper, we introduce\nPerLA, a 3D language assistant designed to be more perceptive to both details\nand context, making visual representations more informative for the LLM. PerLA\ncaptures high-resolution (local) details in parallel from different point cloud\nareas and integrates them with (global) context obtained from a\nlower-resolution whole point cloud. We present a novel algorithm that preserves\npoint cloud locality through the Hilbert curve and effectively aggregates\nlocal-to-global information via cross-attention and a graph neural network.\nLastly, we introduce a novel loss for local representation consensus to promote\ntraining stability. PerLA outperforms state-of-the-art 3D language assistants,\nwith gains of up to +1.34 CiDEr on ScanQA for question answering, and +4.22 on\nScanRefer and +3.88 on Nr3D for dense\ncaptioning.\\url{https://gfmei.github.io/PerLA/}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling Large Language Models (LLMs) to understand the 3D physical world is\nan emerging yet challenging research direction. Current strategies for\nprocessing point clouds typically downsample the scene or divide it into\nsmaller parts for separate analysis. However, both approaches risk losing key\nlocal details or global contextual information. In this paper, we introduce\nPerLA, a 3D language assistant designed to be more perceptive to both details\nand context, making visual representations more informative for the LLM. PerLA\ncaptures high-resolution (local) details in parallel from different point cloud\nareas and integrates them with (global) context obtained from a\nlower-resolution whole point cloud. We present a novel algorithm that preserves\npoint cloud locality through the Hilbert curve and effectively aggregates\nlocal-to-global information via cross-attention and a graph neural network.\nLastly, we introduce a novel loss for local representation consensus to promote\ntraining stability. PerLA outperforms state-of-the-art 3D language assistants,\nwith gains of up to +1.34 CiDEr on ScanQA for question answering, and +4.22 on\nScanRefer and +3.88 on Nr3D for dense\ncaptioning.\\url{https://gfmei.github.io/PerLA/}"
                },
                "authors": [
                    {
                        "name": "Guofeng Mei"
                    },
                    {
                        "name": "Wei Lin"
                    },
                    {
                        "name": "Luigi Riz"
                    },
                    {
                        "name": "Yujiao Wu"
                    },
                    {
                        "name": "Fabio Poiesi"
                    },
                    {
                        "name": "Yiming Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yiming Wang"
                },
                "author": "Yiming Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19774v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19774v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19772v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19772v1",
                "updated": "2024-11-29T15:18:06Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    15,
                    18,
                    6,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T15:18:06Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    15,
                    18,
                    6,
                    4,
                    334,
                    0
                ],
                "title": "LongVALE: Vision-Audio-Language-Event Benchmark Towards Time-Aware\n  Omni-Modal Perception of Long Videos",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LongVALE: Vision-Audio-Language-Event Benchmark Towards Time-Aware\n  Omni-Modal Perception of Long Videos"
                },
                "summary": "Despite impressive advancements in video understanding, most efforts remain\nlimited to coarse-grained or visual-only video tasks. However, real-world\nvideos encompass omni-modal information (vision, audio, and speech) with a\nseries of events forming a cohesive storyline. The lack of multi-modal video\ndata with fine-grained event annotations and the high cost of manual labeling\nare major obstacles to comprehensive omni-modality video perception. To address\nthis gap, we propose an automatic pipeline consisting of high-quality\nmulti-modal video filtering, semantically coherent omni-modal event boundary\ndetection, and cross-modal correlation-aware event captioning. In this way, we\npresent LongVALE, the first-ever Vision-Audio-Language Event understanding\nbenchmark comprising 105K omni-modal events with precise temporal boundaries\nand detailed relation-aware captions within 8.4K high-quality long videos.\nFurther, we build a baseline that leverages LongVALE to enable video large\nlanguage models (LLMs) for omni-modality fine-grained temporal video\nunderstanding for the first time. Extensive experiments demonstrate the\neffectiveness and great potential of LongVALE in advancing comprehensive\nmulti-modal video understanding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite impressive advancements in video understanding, most efforts remain\nlimited to coarse-grained or visual-only video tasks. However, real-world\nvideos encompass omni-modal information (vision, audio, and speech) with a\nseries of events forming a cohesive storyline. The lack of multi-modal video\ndata with fine-grained event annotations and the high cost of manual labeling\nare major obstacles to comprehensive omni-modality video perception. To address\nthis gap, we propose an automatic pipeline consisting of high-quality\nmulti-modal video filtering, semantically coherent omni-modal event boundary\ndetection, and cross-modal correlation-aware event captioning. In this way, we\npresent LongVALE, the first-ever Vision-Audio-Language Event understanding\nbenchmark comprising 105K omni-modal events with precise temporal boundaries\nand detailed relation-aware captions within 8.4K high-quality long videos.\nFurther, we build a baseline that leverages LongVALE to enable video large\nlanguage models (LLMs) for omni-modality fine-grained temporal video\nunderstanding for the first time. Extensive experiments demonstrate the\neffectiveness and great potential of LongVALE in advancing comprehensive\nmulti-modal video understanding."
                },
                "authors": [
                    {
                        "name": "Tiantian Geng"
                    },
                    {
                        "name": "Jinrui Zhang"
                    },
                    {
                        "name": "Qingni Wang"
                    },
                    {
                        "name": "Teng Wang"
                    },
                    {
                        "name": "Jinming Duan"
                    },
                    {
                        "name": "Feng Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Feng Zheng"
                },
                "author": "Feng Zheng",
                "arxiv_comment": "18 pages, 15 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19772v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19772v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19757v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19757v1",
                "updated": "2024-11-29T15:01:25Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    15,
                    1,
                    25,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T15:01:25Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    15,
                    1,
                    25,
                    4,
                    334,
                    0
                ],
                "title": "Dual Risk Minimization: Towards Next-Level Robustness in Fine-tuning\n  Zero-Shot Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dual Risk Minimization: Towards Next-Level Robustness in Fine-tuning\n  Zero-Shot Models"
                },
                "summary": "Fine-tuning foundation models often compromises their robustness to\ndistribution shifts. To remedy this, most robust fine-tuning methods aim to\npreserve the pre-trained features. However, not all pre-trained features are\nrobust and those methods are largely indifferent to which ones to preserve. We\npropose dual risk minimization (DRM), which combines empirical risk\nminimization with worst-case risk minimization, to better preserve the core\nfeatures of downstream tasks. In particular, we utilize core-feature\ndescriptions generated by LLMs to induce core-based zero-shot predictions which\nthen serve as proxies to estimate the worst-case risk. DRM balances two crucial\naspects of model robustness: expected performance and worst-case performance,\nestablishing a new state of the art on various real-world benchmarks. DRM\nsignificantly improves the out-of-distribution performance of CLIP ViT-L/14@336\non ImageNet (75.9 to 77.1), WILDS-iWildCam (47.1 to 51.8), and WILDS-FMoW (50.7\nto 53.1); opening up new avenues for robust fine-tuning. Our code is available\nat https://github.com/vaynexie/DRM .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning foundation models often compromises their robustness to\ndistribution shifts. To remedy this, most robust fine-tuning methods aim to\npreserve the pre-trained features. However, not all pre-trained features are\nrobust and those methods are largely indifferent to which ones to preserve. We\npropose dual risk minimization (DRM), which combines empirical risk\nminimization with worst-case risk minimization, to better preserve the core\nfeatures of downstream tasks. In particular, we utilize core-feature\ndescriptions generated by LLMs to induce core-based zero-shot predictions which\nthen serve as proxies to estimate the worst-case risk. DRM balances two crucial\naspects of model robustness: expected performance and worst-case performance,\nestablishing a new state of the art on various real-world benchmarks. DRM\nsignificantly improves the out-of-distribution performance of CLIP ViT-L/14@336\non ImageNet (75.9 to 77.1), WILDS-iWildCam (47.1 to 51.8), and WILDS-FMoW (50.7\nto 53.1); opening up new avenues for robust fine-tuning. Our code is available\nat https://github.com/vaynexie/DRM ."
                },
                "authors": [
                    {
                        "name": "Kaican Li"
                    },
                    {
                        "name": "Weiyan Xie"
                    },
                    {
                        "name": "Yongxiang Huang"
                    },
                    {
                        "name": "Didan Deng"
                    },
                    {
                        "name": "Lanqing Hong"
                    },
                    {
                        "name": "Zhenguo Li"
                    },
                    {
                        "name": "Ricardo Silva"
                    },
                    {
                        "name": "Nevin L. Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Nevin L. Zhang"
                },
                "author": "Nevin L. Zhang",
                "arxiv_comment": "NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19757v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19757v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.07808v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.07808v3",
                "updated": "2024-11-29T14:50:25Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    14,
                    50,
                    25,
                    4,
                    334,
                    0
                ],
                "published": "2024-02-12T17:13:02Z",
                "published_parsed": [
                    2024,
                    2,
                    12,
                    17,
                    13,
                    2,
                    0,
                    43,
                    0
                ],
                "title": "Sourcerer: Sample-based Maximum Entropy Source Distribution Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sourcerer: Sample-based Maximum Entropy Source Distribution Estimation"
                },
                "summary": "Scientific modeling applications often require estimating a distribution of\nparameters consistent with a dataset of observations - an inference task also\nknown as source distribution estimation. This problem can be ill-posed,\nhowever, since many different source distributions might produce the same\ndistribution of data-consistent simulations. To make a principled choice among\nmany equally valid sources, we propose an approach which targets the maximum\nentropy distribution, i.e., prioritizes retaining as much uncertainty as\npossible. Our method is purely sample-based - leveraging the Sliced-Wasserstein\ndistance to measure the discrepancy between the dataset and simulations - and\nthus suitable for simulators with intractable likelihoods. We benchmark our\nmethod on several tasks, and show that it can recover source distributions with\nsubstantially higher entropy than recent source estimation methods, without\nsacrificing the fidelity of the simulations. Finally, to demonstrate the\nutility of our approach, we infer source distributions for parameters of the\nHodgkin-Huxley model from experimental datasets with thousands of single-neuron\nmeasurements. In summary, we propose a principled method for inferring source\ndistributions of scientific simulator parameters while retaining as much\nuncertainty as possible.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scientific modeling applications often require estimating a distribution of\nparameters consistent with a dataset of observations - an inference task also\nknown as source distribution estimation. This problem can be ill-posed,\nhowever, since many different source distributions might produce the same\ndistribution of data-consistent simulations. To make a principled choice among\nmany equally valid sources, we propose an approach which targets the maximum\nentropy distribution, i.e., prioritizes retaining as much uncertainty as\npossible. Our method is purely sample-based - leveraging the Sliced-Wasserstein\ndistance to measure the discrepancy between the dataset and simulations - and\nthus suitable for simulators with intractable likelihoods. We benchmark our\nmethod on several tasks, and show that it can recover source distributions with\nsubstantially higher entropy than recent source estimation methods, without\nsacrificing the fidelity of the simulations. Finally, to demonstrate the\nutility of our approach, we infer source distributions for parameters of the\nHodgkin-Huxley model from experimental datasets with thousands of single-neuron\nmeasurements. In summary, we propose a principled method for inferring source\ndistributions of scientific simulator parameters while retaining as much\nuncertainty as possible."
                },
                "authors": [
                    {
                        "name": "Julius Vetter"
                    },
                    {
                        "name": "Guy Moss"
                    },
                    {
                        "name": "Cornelius SchrÃ¶der"
                    },
                    {
                        "name": "Richard Gao"
                    },
                    {
                        "name": "Jakob H. Macke"
                    }
                ],
                "author_detail": {
                    "name": "Jakob H. Macke"
                },
                "author": "Jakob H. Macke",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.07808v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.07808v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.08349v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.08349v3",
                "updated": "2024-11-29T14:44:27Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    14,
                    44,
                    27,
                    4,
                    334,
                    0
                ],
                "published": "2024-02-13T10:28:57Z",
                "published_parsed": [
                    2024,
                    2,
                    13,
                    10,
                    28,
                    57,
                    1,
                    44,
                    0
                ],
                "title": "Evaluating the Data Model Robustness of Text-to-SQL Systems Based on\n  Real User Queries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating the Data Model Robustness of Text-to-SQL Systems Based on\n  Real User Queries"
                },
                "summary": "Text-to-SQL systems (also known as NL-to-SQL systems) have become an\nincreasingly popular solution for bridging the gap between user capabilities\nand SQL-based data access. These systems translate user requests in natural\nlanguage to valid SQL statements for a specific database. Recent Text-to-SQL\nsystems have benefited from the rapid improvement of transformer-based language\nmodels. However, while Text-to-SQL systems that incorporate such models\ncontinuously reach new high scores on -- often synthetic -- benchmark datasets,\na systematic exploration of their robustness towards different data models in a\nreal-world, realistic scenario is notably missing. This paper provides the\nfirst in-depth evaluation of the data model robustness of Text-to-SQL systems\nin practice based on a multi-year international project focused on Text-to-SQL\ninterfaces. Our evaluation is based on a real-world deployment of FootballDB, a\nsystem that was deployed over a 9 month period in the context of the FIFA World\nCup 2022, during which about 6K natural language questions were asked and\nexecuted. All of our data is based on real user questions that were asked live\nto the system. We manually labeled and translated a subset of these questions\nfor three different data models. For each data model, we explore the\nperformance of representative Text-to-SQL systems and language models. We\nfurther quantify the impact of training data size, pre-, and post-processing\nsteps as well as language model inference time. Our comprehensive evaluation\nsheds light on the design choices of real-world Text-to-SQL systems and their\nimpact on moving from research prototypes to real deployments. Last, we provide\na new benchmark dataset to the community, which is the first to enable the\nevaluation of different data models for the same dataset and is substantially\nmore challenging than most previous datasets in terms of query complexity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-SQL systems (also known as NL-to-SQL systems) have become an\nincreasingly popular solution for bridging the gap between user capabilities\nand SQL-based data access. These systems translate user requests in natural\nlanguage to valid SQL statements for a specific database. Recent Text-to-SQL\nsystems have benefited from the rapid improvement of transformer-based language\nmodels. However, while Text-to-SQL systems that incorporate such models\ncontinuously reach new high scores on -- often synthetic -- benchmark datasets,\na systematic exploration of their robustness towards different data models in a\nreal-world, realistic scenario is notably missing. This paper provides the\nfirst in-depth evaluation of the data model robustness of Text-to-SQL systems\nin practice based on a multi-year international project focused on Text-to-SQL\ninterfaces. Our evaluation is based on a real-world deployment of FootballDB, a\nsystem that was deployed over a 9 month period in the context of the FIFA World\nCup 2022, during which about 6K natural language questions were asked and\nexecuted. All of our data is based on real user questions that were asked live\nto the system. We manually labeled and translated a subset of these questions\nfor three different data models. For each data model, we explore the\nperformance of representative Text-to-SQL systems and language models. We\nfurther quantify the impact of training data size, pre-, and post-processing\nsteps as well as language model inference time. Our comprehensive evaluation\nsheds light on the design choices of real-world Text-to-SQL systems and their\nimpact on moving from research prototypes to real deployments. Last, we provide\na new benchmark dataset to the community, which is the first to enable the\nevaluation of different data models for the same dataset and is substantially\nmore challenging than most previous datasets in terms of query complexity."
                },
                "authors": [
                    {
                        "name": "Jonathan FÃ¼rst"
                    },
                    {
                        "name": "Catherine Kosten"
                    },
                    {
                        "name": "Farhad Nooralahzadeh"
                    },
                    {
                        "name": "Yi Zhang"
                    },
                    {
                        "name": "Kurt Stockinger"
                    }
                ],
                "author_detail": {
                    "name": "Kurt Stockinger"
                },
                "author": "Kurt Stockinger",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.08349v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.08349v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17593v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17593v2",
                "updated": "2024-11-29T14:41:48Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    14,
                    41,
                    48,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-26T17:01:27Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    17,
                    1,
                    27,
                    1,
                    331,
                    0
                ],
                "title": "What Differentiates Educational Literature? A Multimodal Fusion Approach\n  of Transformers and Computational Linguistics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What Differentiates Educational Literature? A Multimodal Fusion Approach\n  of Transformers and Computational Linguistics"
                },
                "summary": "The integration of new literature into the English curriculum remains a\nchallenge since educators often lack scalable tools to rapidly evaluate\nreadability and adapt texts for diverse classroom needs. This study proposes to\naddress this gap through a multimodal approach that combines transformer-based\ntext classification with linguistic feature analysis to align texts with UK Key\nStages. Eight state-of-the-art Transformers were fine-tuned on segmented text\ndata, with BERT achieving the highest unimodal F1 score of 0.75. In parallel,\n500 deep neural network topologies were searched for the classification of\nlinguistic characteristics, achieving an F1 score of 0.392. The fusion of these\nmodalities shows a significant improvement, with every multimodal approach\noutperforming all unimodal models. In particular, the ELECTRA Transformer fused\nwith the neural network achieved an F1 score of 0.996. Unimodal and multimodal\napproaches are shown to have statistically significant differences in all\nvalidation metrics (accuracy, precision, recall, F1 score) except for inference\ntime. The proposed approach is finally encapsulated in a stakeholder-facing web\napplication, providing non-technical stakeholder access to real-time insights\non text complexity, reading difficulty, curriculum alignment, and\nrecommendations for learning age range. The application empowers data-driven\ndecision making and reduces manual workload by integrating AI-based\nrecommendations into lesson planning for English literature.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of new literature into the English curriculum remains a\nchallenge since educators often lack scalable tools to rapidly evaluate\nreadability and adapt texts for diverse classroom needs. This study proposes to\naddress this gap through a multimodal approach that combines transformer-based\ntext classification with linguistic feature analysis to align texts with UK Key\nStages. Eight state-of-the-art Transformers were fine-tuned on segmented text\ndata, with BERT achieving the highest unimodal F1 score of 0.75. In parallel,\n500 deep neural network topologies were searched for the classification of\nlinguistic characteristics, achieving an F1 score of 0.392. The fusion of these\nmodalities shows a significant improvement, with every multimodal approach\noutperforming all unimodal models. In particular, the ELECTRA Transformer fused\nwith the neural network achieved an F1 score of 0.996. Unimodal and multimodal\napproaches are shown to have statistically significant differences in all\nvalidation metrics (accuracy, precision, recall, F1 score) except for inference\ntime. The proposed approach is finally encapsulated in a stakeholder-facing web\napplication, providing non-technical stakeholder access to real-time insights\non text complexity, reading difficulty, curriculum alignment, and\nrecommendations for learning age range. The application empowers data-driven\ndecision making and reduces manual workload by integrating AI-based\nrecommendations into lesson planning for English literature."
                },
                "authors": [
                    {
                        "name": "Jordan J. Bird"
                    }
                ],
                "author_detail": {
                    "name": "Jordan J. Bird"
                },
                "author": "Jordan J. Bird",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17593v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17593v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.00754v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.00754v2",
                "updated": "2024-11-29T14:27:46Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    14,
                    27,
                    46,
                    4,
                    334,
                    0
                ],
                "published": "2024-05-01T07:24:30Z",
                "published_parsed": [
                    2024,
                    5,
                    1,
                    7,
                    24,
                    30,
                    2,
                    122,
                    0
                ],
                "title": "CLIPArTT: Adaptation of CLIP to New Domains at Test Time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CLIPArTT: Adaptation of CLIP to New Domains at Test Time"
                },
                "summary": "Pre-trained vision-language models (VLMs), exemplified by CLIP, demonstrate\nremarkable adaptability across zero-shot classification tasks without\nadditional training. However, their performance diminishes in the presence of\ndomain shifts. In this study, we introduce CLIP Adaptation duRing Test-Time\n(CLIPArTT), a fully test-time adaptation (TTA) approach for CLIP, which\ninvolves automatic text prompts construction during inference for their use as\ntext supervision. Our method employs a unique, minimally invasive text prompt\ntuning process, wherein multiple predicted classes are aggregated into a single\nnew text prompt, used as \\emph{pseudo label} to re-classify inputs in a\ntransductive manner. Additionally, we pioneer the standardization of TTA\nbenchmarks (e.g., TENT) in the realm of VLMs. Our findings demonstrate that,\nwithout requiring additional transformations nor new trainable modules,\nCLIPArTT enhances performance dynamically across non-corrupted datasets such as\nCIFAR-100, corrupted datasets like CIFAR-100-C and ImageNet-C, alongside\nsynthetic datasets such as VisDA-C. This research underscores the potential for\nimproving VLMs' adaptability through novel test-time strategies, offering\ninsights for robust performance across varied datasets and environments. The\ncode can be found at: https://github.com/dosowiechi/CLIPArTT.git",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pre-trained vision-language models (VLMs), exemplified by CLIP, demonstrate\nremarkable adaptability across zero-shot classification tasks without\nadditional training. However, their performance diminishes in the presence of\ndomain shifts. In this study, we introduce CLIP Adaptation duRing Test-Time\n(CLIPArTT), a fully test-time adaptation (TTA) approach for CLIP, which\ninvolves automatic text prompts construction during inference for their use as\ntext supervision. Our method employs a unique, minimally invasive text prompt\ntuning process, wherein multiple predicted classes are aggregated into a single\nnew text prompt, used as \\emph{pseudo label} to re-classify inputs in a\ntransductive manner. Additionally, we pioneer the standardization of TTA\nbenchmarks (e.g., TENT) in the realm of VLMs. Our findings demonstrate that,\nwithout requiring additional transformations nor new trainable modules,\nCLIPArTT enhances performance dynamically across non-corrupted datasets such as\nCIFAR-100, corrupted datasets like CIFAR-100-C and ImageNet-C, alongside\nsynthetic datasets such as VisDA-C. This research underscores the potential for\nimproving VLMs' adaptability through novel test-time strategies, offering\ninsights for robust performance across varied datasets and environments. The\ncode can be found at: https://github.com/dosowiechi/CLIPArTT.git"
                },
                "authors": [
                    {
                        "name": "Gustavo Adolfo Vargas Hakim"
                    },
                    {
                        "name": "David Osowiechi"
                    },
                    {
                        "name": "Mehrdad Noori"
                    },
                    {
                        "name": "Milad Cheraghalikhani"
                    },
                    {
                        "name": "Ali Bahri"
                    },
                    {
                        "name": "Moslem Yazdanpanah"
                    },
                    {
                        "name": "Ismail Ben Ayed"
                    },
                    {
                        "name": "Christian Desrosiers"
                    }
                ],
                "author_detail": {
                    "name": "Christian Desrosiers"
                },
                "author": "Christian Desrosiers",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.00754v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.00754v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19722v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19722v1",
                "updated": "2024-11-29T14:14:59Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    14,
                    14,
                    59,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T14:14:59Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    14,
                    14,
                    59,
                    4,
                    334,
                    0
                ],
                "title": "JetFormer: An Autoregressive Generative Model of Raw Images and Text",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JetFormer: An Autoregressive Generative Model of Raw Images and Text"
                },
                "summary": "Removing modeling constraints and unifying architectures across domains has\nbeen a key driver of the recent progress in training large multimodal models.\nHowever, most of these models still rely on many separately trained components\nsuch as modality-specific encoders and decoders. In this work, we further\nstreamline joint generative modeling of images and text. We propose an\nautoregressive decoder-only transformer - JetFormer - which is trained to\ndirectly maximize the likelihood of raw data, without relying on any separately\npretrained components, and can understand and generate both text and images.\nSpecifically, we leverage a normalizing flow model to obtain a soft-token image\nrepresentation that is jointly trained with an autoregressive multimodal\ntransformer. The normalizing flow model serves as both an image encoder for\nperception tasks and an image decoder for image generation tasks during\ninference. JetFormer achieves text-to-image generation quality competitive with\nrecent VQ-VAE- and VAE-based baselines. These baselines rely on pretrained\nimage autoencoders, which are trained with a complex mixture of losses,\nincluding perceptual ones. At the same time, JetFormer demonstrates robust\nimage understanding capabilities. To the best of our knowledge, JetFormer is\nthe first model that is capable of generating high-fidelity images and\nproducing strong log-likelihood bounds.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Removing modeling constraints and unifying architectures across domains has\nbeen a key driver of the recent progress in training large multimodal models.\nHowever, most of these models still rely on many separately trained components\nsuch as modality-specific encoders and decoders. In this work, we further\nstreamline joint generative modeling of images and text. We propose an\nautoregressive decoder-only transformer - JetFormer - which is trained to\ndirectly maximize the likelihood of raw data, without relying on any separately\npretrained components, and can understand and generate both text and images.\nSpecifically, we leverage a normalizing flow model to obtain a soft-token image\nrepresentation that is jointly trained with an autoregressive multimodal\ntransformer. The normalizing flow model serves as both an image encoder for\nperception tasks and an image decoder for image generation tasks during\ninference. JetFormer achieves text-to-image generation quality competitive with\nrecent VQ-VAE- and VAE-based baselines. These baselines rely on pretrained\nimage autoencoders, which are trained with a complex mixture of losses,\nincluding perceptual ones. At the same time, JetFormer demonstrates robust\nimage understanding capabilities. To the best of our knowledge, JetFormer is\nthe first model that is capable of generating high-fidelity images and\nproducing strong log-likelihood bounds."
                },
                "authors": [
                    {
                        "name": "Michael Tschannen"
                    },
                    {
                        "name": "AndrÃ© Susano Pinto"
                    },
                    {
                        "name": "Alexander Kolesnikov"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Kolesnikov"
                },
                "author": "Alexander Kolesnikov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19722v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19722v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19710v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19710v1",
                "updated": "2024-11-29T13:57:07Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    13,
                    57,
                    7,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T13:57:07Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    13,
                    57,
                    7,
                    4,
                    334,
                    0
                ],
                "title": "Know Your RAG: Dataset Taxonomy and Generation Strategies for Evaluating\n  RAG Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Know Your RAG: Dataset Taxonomy and Generation Strategies for Evaluating\n  RAG Systems"
                },
                "summary": "Retrieval Augmented Generation (RAG) systems are a widespread application of\nLarge Language Models (LLMs) in the industry. While many tools exist empowering\ndevelopers to build their own systems, measuring their performance locally,\nwith datasets reflective of the system's use cases, is a technological\nchallenge. Solutions to this problem range from non-specific and cheap (most\npublic datasets) to specific and costly (generating data from local documents).\nIn this paper, we show that using public question and answer (Q&A) datasets to\nassess retrieval performance can lead to non-optimal systems design, and that\ncommon tools for RAG dataset generation can lead to unbalanced data. We propose\nsolutions to these issues based on the characterization of RAG datasets through\nlabels and through label-targeted data generation. Finally, we show that\nfine-tuned small LLMs can efficiently generate Q&A datasets. We believe that\nthese observations are invaluable to the know-your-data step of RAG systems\ndevelopment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval Augmented Generation (RAG) systems are a widespread application of\nLarge Language Models (LLMs) in the industry. While many tools exist empowering\ndevelopers to build their own systems, measuring their performance locally,\nwith datasets reflective of the system's use cases, is a technological\nchallenge. Solutions to this problem range from non-specific and cheap (most\npublic datasets) to specific and costly (generating data from local documents).\nIn this paper, we show that using public question and answer (Q&A) datasets to\nassess retrieval performance can lead to non-optimal systems design, and that\ncommon tools for RAG dataset generation can lead to unbalanced data. We propose\nsolutions to these issues based on the characterization of RAG datasets through\nlabels and through label-targeted data generation. Finally, we show that\nfine-tuned small LLMs can efficiently generate Q&A datasets. We believe that\nthese observations are invaluable to the know-your-data step of RAG systems\ndevelopment."
                },
                "authors": [
                    {
                        "name": "Rafael Teixeira de Lima"
                    },
                    {
                        "name": "Shubham Gupta"
                    },
                    {
                        "name": "Cesar Berrospi"
                    },
                    {
                        "name": "Lokesh Mishra"
                    },
                    {
                        "name": "Michele Dolfi"
                    },
                    {
                        "name": "Peter Staar"
                    },
                    {
                        "name": "Panagiotis Vagenas"
                    }
                ],
                "author_detail": {
                    "name": "Panagiotis Vagenas"
                },
                "arxiv_affiliation": "IBM Research Zurich",
                "author": "Panagiotis Vagenas",
                "arxiv_comment": "to be published in the 31st International Conference on Computational\n  Linguistics (COLING 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19710v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19710v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19689v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19689v1",
                "updated": "2024-11-29T13:24:10Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    13,
                    24,
                    10,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T13:24:10Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    13,
                    24,
                    10,
                    4,
                    334,
                    0
                ],
                "title": "MIMDE: Exploring the Use of Synthetic vs Human Data for Evaluating\n  Multi-Insight Multi-Document Extraction Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MIMDE: Exploring the Use of Synthetic vs Human Data for Evaluating\n  Multi-Insight Multi-Document Extraction Tasks"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable capabilities in\ntext analysis tasks, yet their evaluation on complex, real-world applications\nremains challenging. We define a set of tasks, Multi-Insight Multi-Document\nExtraction (MIMDE) tasks, which involves extracting an optimal set of insights\nfrom a document corpus and mapping these insights back to their source\ndocuments. This task is fundamental to many practical applications, from\nanalyzing survey responses to processing medical records, where identifying and\ntracing key insights across documents is crucial. We develop an evaluation\nframework for MIMDE and introduce a novel set of complementary human and\nsynthetic datasets to examine the potential of synthetic data for LLM\nevaluation. After establishing optimal metrics for comparing extracted\ninsights, we benchmark 20 state-of-the-art LLMs on both datasets. Our analysis\nreveals a strong correlation (0.71) between the ability of LLMs to extracts\ninsights on our two datasets but synthetic data fails to capture the complexity\nof document-level analysis. These findings offer crucial guidance for the use\nof synthetic data in evaluating text analysis systems, highlighting both its\npotential and limitations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable capabilities in\ntext analysis tasks, yet their evaluation on complex, real-world applications\nremains challenging. We define a set of tasks, Multi-Insight Multi-Document\nExtraction (MIMDE) tasks, which involves extracting an optimal set of insights\nfrom a document corpus and mapping these insights back to their source\ndocuments. This task is fundamental to many practical applications, from\nanalyzing survey responses to processing medical records, where identifying and\ntracing key insights across documents is crucial. We develop an evaluation\nframework for MIMDE and introduce a novel set of complementary human and\nsynthetic datasets to examine the potential of synthetic data for LLM\nevaluation. After establishing optimal metrics for comparing extracted\ninsights, we benchmark 20 state-of-the-art LLMs on both datasets. Our analysis\nreveals a strong correlation (0.71) between the ability of LLMs to extracts\ninsights on our two datasets but synthetic data fails to capture the complexity\nof document-level analysis. These findings offer crucial guidance for the use\nof synthetic data in evaluating text analysis systems, highlighting both its\npotential and limitations."
                },
                "authors": [
                    {
                        "name": "John Francis"
                    },
                    {
                        "name": "Saba Esnaashari"
                    },
                    {
                        "name": "Anton Poletaev"
                    },
                    {
                        "name": "Sukankana Chakraborty"
                    },
                    {
                        "name": "Youmna Hashem"
                    },
                    {
                        "name": "Jonathan Bright"
                    }
                ],
                "author_detail": {
                    "name": "Jonathan Bright"
                },
                "author": "Jonathan Bright",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19689v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19689v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19688v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19688v1",
                "updated": "2024-11-29T13:22:52Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    13,
                    22,
                    52,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T13:22:52Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    13,
                    22,
                    52,
                    4,
                    334,
                    0
                ],
                "title": "SURE-VQA: Systematic Understanding of Robustness Evaluation in Medical\n  VQA Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SURE-VQA: Systematic Understanding of Robustness Evaluation in Medical\n  VQA Tasks"
                },
                "summary": "Vision-Language Models (VLMs) have great potential in medical tasks, like\nVisual Question Answering (VQA), where they could act as interactive assistants\nfor both patients and clinicians. Yet their robustness to distribution shifts\non unseen data remains a critical concern for safe deployment. Evaluating such\nrobustness requires a controlled experimental setup that allows for systematic\ninsights into the model's behavior. However, we demonstrate that current setups\nfail to offer sufficiently thorough evaluations, limiting their ability to\naccurately assess model robustness. To address this gap, our work introduces a\nnovel framework, called SURE-VQA, centered around three key requirements to\novercome the current pitfalls and systematically analyze the robustness of\nVLMs: 1) Since robustness on synthetic shifts does not necessarily translate to\nreal-world shifts, robustness should be measured on real-world shifts that are\ninherent to the VQA data; 2) Traditional token-matching metrics often fail to\ncapture underlying semantics, necessitating the use of large language models\n(LLMs) for more accurate semantic evaluation; 3) Model performance often lacks\ninterpretability due to missing sanity baselines, thus meaningful baselines\nshould be reported that allow assessing the multimodal impact on the VLM. To\ndemonstrate the relevance of this framework, we conduct a study on the\nrobustness of various fine-tuning methods across three medical datasets with\nfour different types of distribution shifts. Our study reveals several\nimportant findings: 1) Sanity baselines that do not utilize image data can\nperform surprisingly well; 2) We confirm LoRA as the best-performing PEFT\nmethod; 3) No PEFT method consistently outperforms others in terms of\nrobustness to shifts. Code is provided at https://github.com/IML-DKFZ/sure-vqa.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language Models (VLMs) have great potential in medical tasks, like\nVisual Question Answering (VQA), where they could act as interactive assistants\nfor both patients and clinicians. Yet their robustness to distribution shifts\non unseen data remains a critical concern for safe deployment. Evaluating such\nrobustness requires a controlled experimental setup that allows for systematic\ninsights into the model's behavior. However, we demonstrate that current setups\nfail to offer sufficiently thorough evaluations, limiting their ability to\naccurately assess model robustness. To address this gap, our work introduces a\nnovel framework, called SURE-VQA, centered around three key requirements to\novercome the current pitfalls and systematically analyze the robustness of\nVLMs: 1) Since robustness on synthetic shifts does not necessarily translate to\nreal-world shifts, robustness should be measured on real-world shifts that are\ninherent to the VQA data; 2) Traditional token-matching metrics often fail to\ncapture underlying semantics, necessitating the use of large language models\n(LLMs) for more accurate semantic evaluation; 3) Model performance often lacks\ninterpretability due to missing sanity baselines, thus meaningful baselines\nshould be reported that allow assessing the multimodal impact on the VLM. To\ndemonstrate the relevance of this framework, we conduct a study on the\nrobustness of various fine-tuning methods across three medical datasets with\nfour different types of distribution shifts. Our study reveals several\nimportant findings: 1) Sanity baselines that do not utilize image data can\nperform surprisingly well; 2) We confirm LoRA as the best-performing PEFT\nmethod; 3) No PEFT method consistently outperforms others in terms of\nrobustness to shifts. Code is provided at https://github.com/IML-DKFZ/sure-vqa."
                },
                "authors": [
                    {
                        "name": "Kim-Celine Kahl"
                    },
                    {
                        "name": "Selen Erkan"
                    },
                    {
                        "name": "Jeremias Traub"
                    },
                    {
                        "name": "Carsten T. LÃ¼th"
                    },
                    {
                        "name": "Klaus Maier-Hein"
                    },
                    {
                        "name": "Lena Maier-Hein"
                    },
                    {
                        "name": "Paul F. Jaeger"
                    }
                ],
                "author_detail": {
                    "name": "Paul F. Jaeger"
                },
                "author": "Paul F. Jaeger",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19688v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19688v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19678v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19678v1",
                "updated": "2024-11-29T13:12:11Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    13,
                    12,
                    11,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T13:12:11Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    13,
                    12,
                    11,
                    4,
                    334,
                    0
                ],
                "title": "Privacy-Preserving Orthogonal Aggregation for Guaranteeing Gender\n  Fairness in Federated Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Privacy-Preserving Orthogonal Aggregation for Guaranteeing Gender\n  Fairness in Federated Recommendation"
                },
                "summary": "Under stringent privacy constraints, whether federated recommendation systems\ncan achieve group fairness remains an inadequately explored question. Taking\ngender fairness as a representative issue, we identify three phenomena in\nfederated recommendation systems: performance difference, data imbalance, and\npreference disparity. We discover that the state-of-the-art methods only focus\non the first phenomenon. Consequently, their imposition of inappropriate\nfairness constraints detrimentally affects the model training. Moreover, due to\ninsufficient sensitive attribute protection of existing works, we can infer the\ngender of all users with 99.90% accuracy even with the addition of maximal\nnoise. In this work, we propose Privacy-Preserving Orthogonal Aggregation\n(PPOA), which employs the secure aggregation scheme and quantization technique,\nto prevent the suppression of minority groups by the majority and preserve the\ndistinct preferences for better group fairness. PPOA can assist different\ngroups in obtaining their respective model aggregation results through a\ndesigned orthogonal mapping while keeping their attributes private.\nExperimental results on three real-world datasets demonstrate that PPOA\nenhances recommendation effectiveness for both females and males by up to 8.25%\nand 6.36%, respectively, with a maximum overall improvement of 7.30%, and\nachieves optimal fairness in most cases. Extensive ablation experiments and\nvisualizations indicate that PPOA successfully maintains preferences for\ndifferent gender groups.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Under stringent privacy constraints, whether federated recommendation systems\ncan achieve group fairness remains an inadequately explored question. Taking\ngender fairness as a representative issue, we identify three phenomena in\nfederated recommendation systems: performance difference, data imbalance, and\npreference disparity. We discover that the state-of-the-art methods only focus\non the first phenomenon. Consequently, their imposition of inappropriate\nfairness constraints detrimentally affects the model training. Moreover, due to\ninsufficient sensitive attribute protection of existing works, we can infer the\ngender of all users with 99.90% accuracy even with the addition of maximal\nnoise. In this work, we propose Privacy-Preserving Orthogonal Aggregation\n(PPOA), which employs the secure aggregation scheme and quantization technique,\nto prevent the suppression of minority groups by the majority and preserve the\ndistinct preferences for better group fairness. PPOA can assist different\ngroups in obtaining their respective model aggregation results through a\ndesigned orthogonal mapping while keeping their attributes private.\nExperimental results on three real-world datasets demonstrate that PPOA\nenhances recommendation effectiveness for both females and males by up to 8.25%\nand 6.36%, respectively, with a maximum overall improvement of 7.30%, and\nachieves optimal fairness in most cases. Extensive ablation experiments and\nvisualizations indicate that PPOA successfully maintains preferences for\ndifferent gender groups."
                },
                "authors": [
                    {
                        "name": "Siqing Zhang"
                    },
                    {
                        "name": "Yuchen Ding"
                    },
                    {
                        "name": "Wei Tang"
                    },
                    {
                        "name": "Wei Sun"
                    },
                    {
                        "name": "Yong Liao"
                    },
                    {
                        "name": "Peng Yuan Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Peng Yuan Zhou"
                },
                "author": "Peng Yuan Zhou",
                "arxiv_comment": "accepted by WSDM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19678v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19678v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19668v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19668v1",
                "updated": "2024-11-29T12:48:49Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    12,
                    48,
                    49,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T12:48:49Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    12,
                    48,
                    49,
                    4,
                    334,
                    0
                ],
                "title": "ChineseWebText 2.0: Large-Scale High-quality Chinese Web Text with\n  Multi-dimensional and fine-grained information",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChineseWebText 2.0: Large-Scale High-quality Chinese Web Text with\n  Multi-dimensional and fine-grained information"
                },
                "summary": "During the development of large language models (LLMs), pre-training data\nplay a critical role in shaping LLMs' capabilities. In recent years several\nlarge-scale and high-quality pre-training datasets have been released to\naccelerate the research of LLMs, including ChineseWebText1.0, C4, Pile,\nWanJuan, MAPCC and others. However, as LLMs continue to evolve, focus has\nincreasingly shifted to domain-specific capabilities and safety concerns,\nmaking those previous coarse-grained texts insufficient for meeting training\nrequirements. Furthermore, fine-grained information, such as quality, domain\nand toxicity, is becoming increasingly important in building powerful and\nreliable LLMs for various scenarios. To address these challenges, in this paper\nwe propose a new tool-chain called MDFG-tool for constructing large-scale and\nhigh-quality Chinese datasets with multi-dimensional and fine-grained\ninformation. First, we employ manually crafted rules to discard explicit noisy\ntexts from raw contents. Second, the quality evaluation model, domain\nclassifier, and toxicity evaluation model are well-designed to assess the\nremaining cleaned data respectively. Finally, we integrate these three types of\nfine-grained information for each text. With this approach, we release the\nlargest, high-quality and fine-grained Chinese text ChineseWebText2.0, which\nconsists of 3.8TB and each text is associated with a quality score, domain\nlabels, a toxicity label and a toxicity score, facilitating the LLM researchers\nto select data based on various types of fine-grained information. The data,\ncodes and the tool-chain are available on this website\nhttps://github.com/CASIA-LM/ChineseWebText-2.0",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "During the development of large language models (LLMs), pre-training data\nplay a critical role in shaping LLMs' capabilities. In recent years several\nlarge-scale and high-quality pre-training datasets have been released to\naccelerate the research of LLMs, including ChineseWebText1.0, C4, Pile,\nWanJuan, MAPCC and others. However, as LLMs continue to evolve, focus has\nincreasingly shifted to domain-specific capabilities and safety concerns,\nmaking those previous coarse-grained texts insufficient for meeting training\nrequirements. Furthermore, fine-grained information, such as quality, domain\nand toxicity, is becoming increasingly important in building powerful and\nreliable LLMs for various scenarios. To address these challenges, in this paper\nwe propose a new tool-chain called MDFG-tool for constructing large-scale and\nhigh-quality Chinese datasets with multi-dimensional and fine-grained\ninformation. First, we employ manually crafted rules to discard explicit noisy\ntexts from raw contents. Second, the quality evaluation model, domain\nclassifier, and toxicity evaluation model are well-designed to assess the\nremaining cleaned data respectively. Finally, we integrate these three types of\nfine-grained information for each text. With this approach, we release the\nlargest, high-quality and fine-grained Chinese text ChineseWebText2.0, which\nconsists of 3.8TB and each text is associated with a quality score, domain\nlabels, a toxicity label and a toxicity score, facilitating the LLM researchers\nto select data based on various types of fine-grained information. The data,\ncodes and the tool-chain are available on this website\nhttps://github.com/CASIA-LM/ChineseWebText-2.0"
                },
                "authors": [
                    {
                        "name": "Wanyue Zhang"
                    },
                    {
                        "name": "Ziyong Li"
                    },
                    {
                        "name": "Wen Yang"
                    },
                    {
                        "name": "Chunlin Leng"
                    },
                    {
                        "name": "Yinan Bai"
                    },
                    {
                        "name": "Qianlong Du"
                    },
                    {
                        "name": "Chengqing Zong"
                    },
                    {
                        "name": "Jiajun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jiajun Zhang"
                },
                "author": "Jiajun Zhang",
                "arxiv_comment": "ChineseWebTex2.0 dataset is available at\n  https://github.com/CASIA-LM/ChineseWebText-2.0",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19668v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19668v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.03157v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.03157v2",
                "updated": "2024-11-29T12:33:45Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    12,
                    33,
                    45,
                    4,
                    334,
                    0
                ],
                "published": "2023-11-06T14:52:30Z",
                "published_parsed": [
                    2023,
                    11,
                    6,
                    14,
                    52,
                    30,
                    0,
                    310,
                    0
                ],
                "title": "GPTuner: A Manual-Reading Database Tuning System via GPT-Guided Bayesian\n  Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPTuner: A Manual-Reading Database Tuning System via GPT-Guided Bayesian\n  Optimization"
                },
                "summary": "Modern database management systems (DBMS) expose hundreds of configurable\nknobs to control system behaviours. Determining the appropriate values for\nthese knobs to improve DBMS performance is a long-standing problem in the\ndatabase community. As there is an increasing number of knobs to tune and each\nknob could be in continuous or categorical values, manual tuning becomes\nimpractical. Recently, automatic tuning systems using machine learning methods\nhave shown great potentials. However, existing approaches still incur\nsignificant tuning costs or only yield sub-optimal performance. This is because\nthey either ignore the extensive domain knowledge available (e.g., DBMS manuals\nand forum discussions) and only rely on the runtime feedback of benchmark\nevaluations to guide the optimization, or they utilize the domain knowledge in\na limited way. Hence, we propose GPTuner, a manual-reading database tuning\nsystem. Firstly, we develop a Large Language Model (LLM)-based pipeline to\ncollect and refine heterogeneous knowledge, and propose a prompt ensemble\nalgorithm to unify a structured view of the refined knowledge. Secondly, using\nthe structured knowledge, we (1) design a workload-aware and training-free knob\nselection strategy, (2) develop a search space optimization technique\nconsidering the value range of each knob, and (3) propose a Coarse-to-Fine\nBayesian Optimization Framework to explore the optimized space. Finally, we\nevaluate GPTuner under different benchmarks (TPC-C and TPC-H), metrics\n(throughput and latency) as well as DBMS (PostgreSQL and MySQL). Compared to\nthe state-of-the-art approaches, GPTuner identifies better configurations in\n16x less time on average. Moreover, GPTuner achieves up to 30% performance\nimprovement (higher throughput or lower latency) over the best-performing\nalternative.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern database management systems (DBMS) expose hundreds of configurable\nknobs to control system behaviours. Determining the appropriate values for\nthese knobs to improve DBMS performance is a long-standing problem in the\ndatabase community. As there is an increasing number of knobs to tune and each\nknob could be in continuous or categorical values, manual tuning becomes\nimpractical. Recently, automatic tuning systems using machine learning methods\nhave shown great potentials. However, existing approaches still incur\nsignificant tuning costs or only yield sub-optimal performance. This is because\nthey either ignore the extensive domain knowledge available (e.g., DBMS manuals\nand forum discussions) and only rely on the runtime feedback of benchmark\nevaluations to guide the optimization, or they utilize the domain knowledge in\na limited way. Hence, we propose GPTuner, a manual-reading database tuning\nsystem. Firstly, we develop a Large Language Model (LLM)-based pipeline to\ncollect and refine heterogeneous knowledge, and propose a prompt ensemble\nalgorithm to unify a structured view of the refined knowledge. Secondly, using\nthe structured knowledge, we (1) design a workload-aware and training-free knob\nselection strategy, (2) develop a search space optimization technique\nconsidering the value range of each knob, and (3) propose a Coarse-to-Fine\nBayesian Optimization Framework to explore the optimized space. Finally, we\nevaluate GPTuner under different benchmarks (TPC-C and TPC-H), metrics\n(throughput and latency) as well as DBMS (PostgreSQL and MySQL). Compared to\nthe state-of-the-art approaches, GPTuner identifies better configurations in\n16x less time on average. Moreover, GPTuner achieves up to 30% performance\nimprovement (higher throughput or lower latency) over the best-performing\nalternative."
                },
                "authors": [
                    {
                        "name": "Jiale Lao"
                    },
                    {
                        "name": "Yibo Wang"
                    },
                    {
                        "name": "Yufei Li"
                    },
                    {
                        "name": "Jianping Wang"
                    },
                    {
                        "name": "Yunjia Zhang"
                    },
                    {
                        "name": "Zhiyuan Cheng"
                    },
                    {
                        "name": "Wanghu Chen"
                    },
                    {
                        "name": "Mingjie Tang"
                    },
                    {
                        "name": "Jianguo Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jianguo Wang"
                },
                "author": "Jianguo Wang",
                "arxiv_doi": "10.14778/3659437.3659449",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.14778/3659437.3659449",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2311.03157v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.03157v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by VLDB2024",
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19655v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19655v1",
                "updated": "2024-11-29T12:21:15Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    12,
                    21,
                    15,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T12:21:15Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    12,
                    21,
                    15,
                    4,
                    334,
                    0
                ],
                "title": "Truth or Mirage? Towards End-to-End Factuality Evaluation with LLM-OASIS",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Truth or Mirage? Towards End-to-End Factuality Evaluation with LLM-OASIS"
                },
                "summary": "After the introduction of Large Language Models (LLMs), there have been\nsubstantial improvements in the performance of Natural Language Generation\n(NLG) tasks, including Text Summarization and Machine Translation. However,\nLLMs still produce outputs containing hallucinations, that is, content not\ngrounded in factual information. Therefore, developing methods to assess the\nfactuality of LLMs has become urgent.\n  Indeed, resources for factuality evaluation have recently emerged. Although\nchallenging, these resources face one or more of the following limitations: (i)\nthey are tailored to a specific task or domain; (ii) they are limited in size,\nthereby preventing the training of new factuality evaluators; (iii) they are\ndesigned for simpler verification tasks, such as claim verification.\n  To address these issues, we introduce LLM-Oasis, to the best of our knowledge\nthe largest resource for training end-to-end factuality evaluators. LLM-Oasis\nis constructed by extracting claims from Wikipedia, falsifying a subset of\nthese claims, and generating pairs of factual and unfactual texts. We then rely\non human annotators to both validate the quality of our dataset and to create a\ngold standard test set for benchmarking factuality evaluation systems.\n  Our experiments demonstrate that LLM-Oasis presents a significant challenge\nfor state-of-the-art LLMs, with GPT-4o achieving up to 60% accuracy in our\nproposed end-to-end factuality evaluation task, highlighting its potential to\ndrive future research in the field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "After the introduction of Large Language Models (LLMs), there have been\nsubstantial improvements in the performance of Natural Language Generation\n(NLG) tasks, including Text Summarization and Machine Translation. However,\nLLMs still produce outputs containing hallucinations, that is, content not\ngrounded in factual information. Therefore, developing methods to assess the\nfactuality of LLMs has become urgent.\n  Indeed, resources for factuality evaluation have recently emerged. Although\nchallenging, these resources face one or more of the following limitations: (i)\nthey are tailored to a specific task or domain; (ii) they are limited in size,\nthereby preventing the training of new factuality evaluators; (iii) they are\ndesigned for simpler verification tasks, such as claim verification.\n  To address these issues, we introduce LLM-Oasis, to the best of our knowledge\nthe largest resource for training end-to-end factuality evaluators. LLM-Oasis\nis constructed by extracting claims from Wikipedia, falsifying a subset of\nthese claims, and generating pairs of factual and unfactual texts. We then rely\non human annotators to both validate the quality of our dataset and to create a\ngold standard test set for benchmarking factuality evaluation systems.\n  Our experiments demonstrate that LLM-Oasis presents a significant challenge\nfor state-of-the-art LLMs, with GPT-4o achieving up to 60% accuracy in our\nproposed end-to-end factuality evaluation task, highlighting its potential to\ndrive future research in the field."
                },
                "authors": [
                    {
                        "name": "Alessandro ScirÃ¨"
                    },
                    {
                        "name": "Andrei Stefan Bejgu"
                    },
                    {
                        "name": "Simone Tedeschi"
                    },
                    {
                        "name": "Karim Ghonim"
                    },
                    {
                        "name": "Federico Martelli"
                    },
                    {
                        "name": "Roberto Navigli"
                    }
                ],
                "author_detail": {
                    "name": "Roberto Navigli"
                },
                "author": "Roberto Navigli",
                "arxiv_comment": "15 pages. To be submitted to CL journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19655v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19655v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19648v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19648v1",
                "updated": "2024-11-29T12:02:28Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    12,
                    2,
                    28,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T12:02:28Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    12,
                    2,
                    28,
                    4,
                    334,
                    0
                ],
                "title": "Enhancing Security in Third-Party Library Reuse -- Comprehensive\n  Detection of 1-day Vulnerability through Code Patch Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Security in Third-Party Library Reuse -- Comprehensive\n  Detection of 1-day Vulnerability through Code Patch Analysis"
                },
                "summary": "Nowadays, software development progresses rapidly to incorporate new\nfeatures. To facilitate such growth and provide convenience for developers when\ncreating and updating software, reusing open-source software (i.e., thirdparty\nlibrary reuses) has become one of the most effective and efficient methods.\nUnfortunately, the practice of reusing third-party libraries (TPLs) can also\nintroduce vulnerabilities (known as 1-day vulnerabilities) because of the low\nmaintenance of TPLs, resulting in many vulnerable versions remaining in use. If\nthe software incorporating these TPLs fails to detect the introduced\nvulnerabilities and leads to delayed updates, it will exacerbate the security\nrisks. However, the complicated code dependencies and flexibility of TPL reuses\nmake the detection of 1-day vulnerability a challenging task. To support\ndevelopers in securely reusing TPLs during software development, we design and\nimplement VULTURE, an effective and efficient detection tool, aiming at\nidentifying 1-day vulnerabilities that arise from the reuse of vulnerable TPLs.\nIt first executes a database creation method, TPLFILTER, which leverages the\nLarge Language Model (LLM) to automatically build a unique database for the\ntargeted platform. Instead of relying on code-level similarity comparison,\nVULTURE employs hashing-based comparison to explore the dependencies among the\ncollected TPLs and identify the similarities between the TPLs and the target\nprojects. Recognizing that developers have the flexibility to reuse TPLs\nexactly or in a custom manner, VULTURE separately conducts version-based\ncomparison and chunk-based analysis to capture fine-grained semantic features\nat the function levels. We applied VULTURE to 10 real-world projects to assess\nits effectiveness and efficiency in detecting 1-day vulnerabilities. VULTURE\nsuccessfully identified 175 vulnerabilities from 178 reused TPLs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nowadays, software development progresses rapidly to incorporate new\nfeatures. To facilitate such growth and provide convenience for developers when\ncreating and updating software, reusing open-source software (i.e., thirdparty\nlibrary reuses) has become one of the most effective and efficient methods.\nUnfortunately, the practice of reusing third-party libraries (TPLs) can also\nintroduce vulnerabilities (known as 1-day vulnerabilities) because of the low\nmaintenance of TPLs, resulting in many vulnerable versions remaining in use. If\nthe software incorporating these TPLs fails to detect the introduced\nvulnerabilities and leads to delayed updates, it will exacerbate the security\nrisks. However, the complicated code dependencies and flexibility of TPL reuses\nmake the detection of 1-day vulnerability a challenging task. To support\ndevelopers in securely reusing TPLs during software development, we design and\nimplement VULTURE, an effective and efficient detection tool, aiming at\nidentifying 1-day vulnerabilities that arise from the reuse of vulnerable TPLs.\nIt first executes a database creation method, TPLFILTER, which leverages the\nLarge Language Model (LLM) to automatically build a unique database for the\ntargeted platform. Instead of relying on code-level similarity comparison,\nVULTURE employs hashing-based comparison to explore the dependencies among the\ncollected TPLs and identify the similarities between the TPLs and the target\nprojects. Recognizing that developers have the flexibility to reuse TPLs\nexactly or in a custom manner, VULTURE separately conducts version-based\ncomparison and chunk-based analysis to capture fine-grained semantic features\nat the function levels. We applied VULTURE to 10 real-world projects to assess\nits effectiveness and efficiency in detecting 1-day vulnerabilities. VULTURE\nsuccessfully identified 175 vulnerabilities from 178 reused TPLs."
                },
                "authors": [
                    {
                        "name": "Shangzhi Xu"
                    },
                    {
                        "name": "Jialiang Dong"
                    },
                    {
                        "name": "Weiting Cai"
                    },
                    {
                        "name": "Juanru Li"
                    },
                    {
                        "name": "Arash Shaghaghi"
                    },
                    {
                        "name": "Nan Sun"
                    },
                    {
                        "name": "Siqi Ma"
                    }
                ],
                "author_detail": {
                    "name": "Siqi Ma"
                },
                "author": "Siqi Ma",
                "arxiv_comment": "17 pages, NDSS 25'",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19648v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19648v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.16314v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.16314v2",
                "updated": "2024-11-29T11:52:31Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    11,
                    52,
                    31,
                    4,
                    334,
                    0
                ],
                "published": "2024-10-09T10:09:37Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    10,
                    9,
                    37,
                    2,
                    283,
                    0
                ],
                "title": "Steering Large Language Models using Conceptors: Improving\n  Addition-Based Activation Engineering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Steering Large Language Models using Conceptors: Improving\n  Addition-Based Activation Engineering"
                },
                "summary": "Large language models have transformed AI, yet reliably controlling their\noutputs remains a challenge. This paper explores activation engineering, where\noutputs of pre-trained LLMs are controlled by manipulating their activations at\ninference time. Unlike traditional methods using a single steering vector, we\nintroduce conceptors - mathematical constructs that represent sets of\nactivation vectors as ellipsoidal regions. Conceptors act as soft projection\nmatrices and offer more precise control over complex activation patterns. Our\nexperiments demonstrate that conceptors outperform traditional methods across\nmultiple steering tasks. We further use Boolean operations on conceptors for\ncombined steering goals that empirically outperform additively combining\nsteering vectors on a set of tasks. These results highlight conceptors as a\npromising tool for more effective steering of LLMs. Our code is available on\ngithub.com/jorispos/conceptorsteering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have transformed AI, yet reliably controlling their\noutputs remains a challenge. This paper explores activation engineering, where\noutputs of pre-trained LLMs are controlled by manipulating their activations at\ninference time. Unlike traditional methods using a single steering vector, we\nintroduce conceptors - mathematical constructs that represent sets of\nactivation vectors as ellipsoidal regions. Conceptors act as soft projection\nmatrices and offer more precise control over complex activation patterns. Our\nexperiments demonstrate that conceptors outperform traditional methods across\nmultiple steering tasks. We further use Boolean operations on conceptors for\ncombined steering goals that empirically outperform additively combining\nsteering vectors on a set of tasks. These results highlight conceptors as a\npromising tool for more effective steering of LLMs. Our code is available on\ngithub.com/jorispos/conceptorsteering."
                },
                "authors": [
                    {
                        "name": "Joris Postmus"
                    },
                    {
                        "name": "Steven Abreu"
                    }
                ],
                "author_detail": {
                    "name": "Steven Abreu"
                },
                "author": "Steven Abreu",
                "arxiv_comment": "Presented at the MINT workshop at NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.16314v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.16314v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2303.05263v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2303.05263v3",
                "updated": "2024-11-29T11:49:04Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    11,
                    49,
                    4,
                    4,
                    334,
                    0
                ],
                "published": "2023-03-09T13:58:35Z",
                "published_parsed": [
                    2023,
                    3,
                    9,
                    13,
                    58,
                    35,
                    3,
                    68,
                    0
                ],
                "title": "Fast post-process Bayesian inference with Variational Sparse Bayesian\n  Quadrature",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast post-process Bayesian inference with Variational Sparse Bayesian\n  Quadrature"
                },
                "summary": "In applied Bayesian inference scenarios, users may have access to a large\nnumber of pre-existing model evaluations, for example from maximum-a-posteriori\n(MAP) optimization runs. However, traditional approximate inference techniques\nmake little to no use of this available information. We propose the framework\nof post-process Bayesian inference as a means to obtain a quick posterior\napproximation from existing target density evaluations, with no further model\ncalls. Within this framework, we introduce Variational Sparse Bayesian\nQuadrature (VSBQ), a method for post-process approximate inference for models\nwith black-box and potentially noisy likelihoods. VSBQ reuses existing target\ndensity evaluations to build a sparse Gaussian process (GP) surrogate model of\nthe log posterior density function. Subsequently, we leverage sparse-GP\nBayesian quadrature combined with variational inference to achieve fast\napproximate posterior inference over the surrogate. We validate our method on\nchallenging synthetic scenarios and real-world applications from computational\nneuroscience. The experiments show that VSBQ builds high-quality posterior\napproximations by post-processing existing optimization traces, with no further\nmodel evaluations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In applied Bayesian inference scenarios, users may have access to a large\nnumber of pre-existing model evaluations, for example from maximum-a-posteriori\n(MAP) optimization runs. However, traditional approximate inference techniques\nmake little to no use of this available information. We propose the framework\nof post-process Bayesian inference as a means to obtain a quick posterior\napproximation from existing target density evaluations, with no further model\ncalls. Within this framework, we introduce Variational Sparse Bayesian\nQuadrature (VSBQ), a method for post-process approximate inference for models\nwith black-box and potentially noisy likelihoods. VSBQ reuses existing target\ndensity evaluations to build a sparse Gaussian process (GP) surrogate model of\nthe log posterior density function. Subsequently, we leverage sparse-GP\nBayesian quadrature combined with variational inference to achieve fast\napproximate posterior inference over the surrogate. We validate our method on\nchallenging synthetic scenarios and real-world applications from computational\nneuroscience. The experiments show that VSBQ builds high-quality posterior\napproximations by post-processing existing optimization traces, with no further\nmodel evaluations."
                },
                "authors": [
                    {
                        "name": "Chengkun Li"
                    },
                    {
                        "name": "GrÃ©goire ClartÃ©"
                    },
                    {
                        "name": "Martin JÃ¸rgensen"
                    },
                    {
                        "name": "Luigi Acerbi"
                    }
                ],
                "author_detail": {
                    "name": "Luigi Acerbi"
                },
                "author": "Luigi Acerbi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2303.05263v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2303.05263v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.11295v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.11295v6",
                "updated": "2024-11-29T11:47:55Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    11,
                    47,
                    55,
                    4,
                    334,
                    0
                ],
                "published": "2024-02-17T14:26:57Z",
                "published_parsed": [
                    2024,
                    2,
                    17,
                    14,
                    26,
                    57,
                    5,
                    48,
                    0
                ],
                "title": "OneBit: Towards Extremely Low-bit Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OneBit: Towards Extremely Low-bit Large Language Models"
                },
                "summary": "Model quantification uses low bit-width values to represent the weight\nmatrices of existing models to be quantized, which is a promising approach to\nreduce both storage and computational overheads of deploying highly anticipated\nLLMs. However, current quantization methods suffer severe performance\ndegradation when the bit-width is extremely reduced, and thus focus on\nutilizing 4-bit or 8-bit values to quantize models. This paper boldly quantizes\nthe weight matrices of LLMs to 1-bit, paving the way for the extremely low\nbit-width deployment of LLMs. For this target, we introduce a 1-bit model\ncompressing framework named OneBit, including a novel 1-bit parameter\nrepresentation method to better quantize LLMs as well as an effective parameter\ninitialization method based on matrix decomposition to improve the convergence\nspeed of the quantization framework. Sufficient experimental results indicate\nthat OneBit achieves good performance (at least 81% of the non-quantized\nperformance on LLaMA models) with robust training processes when only using\n1-bit weight matrices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model quantification uses low bit-width values to represent the weight\nmatrices of existing models to be quantized, which is a promising approach to\nreduce both storage and computational overheads of deploying highly anticipated\nLLMs. However, current quantization methods suffer severe performance\ndegradation when the bit-width is extremely reduced, and thus focus on\nutilizing 4-bit or 8-bit values to quantize models. This paper boldly quantizes\nthe weight matrices of LLMs to 1-bit, paving the way for the extremely low\nbit-width deployment of LLMs. For this target, we introduce a 1-bit model\ncompressing framework named OneBit, including a novel 1-bit parameter\nrepresentation method to better quantize LLMs as well as an effective parameter\ninitialization method based on matrix decomposition to improve the convergence\nspeed of the quantization framework. Sufficient experimental results indicate\nthat OneBit achieves good performance (at least 81% of the non-quantized\nperformance on LLaMA models) with robust training processes when only using\n1-bit weight matrices."
                },
                "authors": [
                    {
                        "name": "Yuzhuang Xu"
                    },
                    {
                        "name": "Xu Han"
                    },
                    {
                        "name": "Zonghan Yang"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Qingfu Zhu"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Weidong Liu"
                    },
                    {
                        "name": "Wanxiang Che"
                    }
                ],
                "author_detail": {
                    "name": "Wanxiang Che"
                },
                "author": "Wanxiang Che",
                "arxiv_comment": "Accepted by NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.11295v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.11295v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19639v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19639v1",
                "updated": "2024-11-29T11:45:21Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    11,
                    45,
                    21,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T11:45:21Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    11,
                    45,
                    21,
                    4,
                    334,
                    0
                ],
                "title": "RMIO: A Model-Based MARL Framework for Scenarios with Observation Loss\n  in Some Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RMIO: A Model-Based MARL Framework for Scenarios with Observation Loss\n  in Some Agents"
                },
                "summary": "In recent years, model-based reinforcement learning (MBRL) has emerged as a\nsolution to address sample complexity in multi-agent reinforcement learning\n(MARL) by modeling agent-environment dynamics to improve sample efficiency.\nHowever, most MBRL methods assume complete and continuous observations from\neach agent during the inference stage, which can be overly idealistic in\npractical applications. A novel model-based MARL approach called RMIO is\nintroduced to address this limitation, specifically designed for scenarios\nwhere observation is lost in some agent. RMIO leverages the world model to\nreconstruct missing observations, and further reduces reconstruction errors\nthrough inter-agent information integration to ensure stable multi-agent\ndecision-making. Secondly, unlike CTCE methods such as MAMBA, RMIO adopts the\nCTDE paradigm in standard environment, and enabling limited communication only\nwhen agents lack observation data, thereby reducing reliance on communication.\nAdditionally, RMIO improves asymptotic performance through strategies such as\nreward smoothing, a dual-layer experience replay buffer, and an RNN-augmented\npolicy model, surpassing previous work. Our experiments conducted in both the\nSMAC and MaMuJoCo environments demonstrate that RMIO outperforms current\nstate-of-the-art approaches in terms of asymptotic convergence performance and\npolicy robustness, both in standard mission settings and in scenarios involving\nobservation loss.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, model-based reinforcement learning (MBRL) has emerged as a\nsolution to address sample complexity in multi-agent reinforcement learning\n(MARL) by modeling agent-environment dynamics to improve sample efficiency.\nHowever, most MBRL methods assume complete and continuous observations from\neach agent during the inference stage, which can be overly idealistic in\npractical applications. A novel model-based MARL approach called RMIO is\nintroduced to address this limitation, specifically designed for scenarios\nwhere observation is lost in some agent. RMIO leverages the world model to\nreconstruct missing observations, and further reduces reconstruction errors\nthrough inter-agent information integration to ensure stable multi-agent\ndecision-making. Secondly, unlike CTCE methods such as MAMBA, RMIO adopts the\nCTDE paradigm in standard environment, and enabling limited communication only\nwhen agents lack observation data, thereby reducing reliance on communication.\nAdditionally, RMIO improves asymptotic performance through strategies such as\nreward smoothing, a dual-layer experience replay buffer, and an RNN-augmented\npolicy model, surpassing previous work. Our experiments conducted in both the\nSMAC and MaMuJoCo environments demonstrate that RMIO outperforms current\nstate-of-the-art approaches in terms of asymptotic convergence performance and\npolicy robustness, both in standard mission settings and in scenarios involving\nobservation loss."
                },
                "authors": [
                    {
                        "name": "Shi Zifeng"
                    },
                    {
                        "name": "Liu Meiqin"
                    },
                    {
                        "name": "Zhang Senlin"
                    },
                    {
                        "name": "Zheng Ronghao"
                    },
                    {
                        "name": "Dong Shanling"
                    }
                ],
                "author_detail": {
                    "name": "Dong Shanling"
                },
                "author": "Dong Shanling",
                "arxiv_comment": "17 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19639v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19639v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19638v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19638v1",
                "updated": "2024-11-29T11:42:58Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    11,
                    42,
                    58,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T11:42:58Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    11,
                    42,
                    58,
                    4,
                    334,
                    0
                ],
                "title": "LLM Teacher-Student Framework for Text Classification With No Manually\n  Annotated Data: A Case Study in IPTC News Topic Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Teacher-Student Framework for Text Classification With No Manually\n  Annotated Data: A Case Study in IPTC News Topic Classification"
                },
                "summary": "With the ever-increasing number of news stories available online, classifying\nthem by topic, regardless of the language they are written in, has become\ncrucial for enhancing readers' access to relevant content. To address this\nchallenge, we propose a teacher-student framework based on large language\nmodels (LLMs) for developing multilingual news classification models of\nreasonable size with no need for manual data annotation. The framework employs\na Generative Pretrained Transformer (GPT) model as the teacher model to develop\nan IPTC Media Topic training dataset through automatic annotation of news\narticles in Slovenian, Croatian, Greek, and Catalan. The teacher model exhibits\na high zero-shot performance on all four languages. Its agreement with human\nannotators is comparable to that between the human annotators themselves. To\nmitigate the computational limitations associated with the requirement of\nprocessing millions of texts daily, smaller BERT-like student models are\nfine-tuned on the GPT-annotated dataset. These student models achieve high\nperformance comparable to the teacher model. Furthermore, we explore the impact\nof the training data size on the performance of the student models and\ninvestigate their monolingual, multilingual and zero-shot cross-lingual\ncapabilities. The findings indicate that student models can achieve high\nperformance with a relatively small number of training instances, and\ndemonstrate strong zero-shot cross-lingual abilities. Finally, we publish the\nbest-performing news topic classifier, enabling multilingual classification\nwith the top-level categories of the IPTC Media Topic schema.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the ever-increasing number of news stories available online, classifying\nthem by topic, regardless of the language they are written in, has become\ncrucial for enhancing readers' access to relevant content. To address this\nchallenge, we propose a teacher-student framework based on large language\nmodels (LLMs) for developing multilingual news classification models of\nreasonable size with no need for manual data annotation. The framework employs\na Generative Pretrained Transformer (GPT) model as the teacher model to develop\nan IPTC Media Topic training dataset through automatic annotation of news\narticles in Slovenian, Croatian, Greek, and Catalan. The teacher model exhibits\na high zero-shot performance on all four languages. Its agreement with human\nannotators is comparable to that between the human annotators themselves. To\nmitigate the computational limitations associated with the requirement of\nprocessing millions of texts daily, smaller BERT-like student models are\nfine-tuned on the GPT-annotated dataset. These student models achieve high\nperformance comparable to the teacher model. Furthermore, we explore the impact\nof the training data size on the performance of the student models and\ninvestigate their monolingual, multilingual and zero-shot cross-lingual\ncapabilities. The findings indicate that student models can achieve high\nperformance with a relatively small number of training instances, and\ndemonstrate strong zero-shot cross-lingual abilities. Finally, we publish the\nbest-performing news topic classifier, enabling multilingual classification\nwith the top-level categories of the IPTC Media Topic schema."
                },
                "authors": [
                    {
                        "name": "Taja Kuzman"
                    },
                    {
                        "name": "Nikola LjubeÅ¡iÄ"
                    }
                ],
                "author_detail": {
                    "name": "Nikola LjubeÅ¡iÄ"
                },
                "author": "Nikola LjubeÅ¡iÄ",
                "arxiv_comment": "This work has been submitted to the IEEE for possible publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19638v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19638v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19635v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19635v1",
                "updated": "2024-11-29T11:37:12Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    11,
                    37,
                    12,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T11:37:12Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    11,
                    37,
                    12,
                    4,
                    334,
                    0
                ],
                "title": "Build An Influential Bot In Social Media Simulations With Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Build An Influential Bot In Social Media Simulations With Large Language\n  Models"
                },
                "summary": "Understanding the dynamics of public opinion evolution on online social\nplatforms is critical for analyzing influence mechanisms. Traditional\napproaches to influencer analysis are typically divided into qualitative\nassessments of personal attributes and quantitative evaluations of influence\npower. In this study, we introduce a novel simulated environment that combines\nAgent-Based Modeling (ABM) with Large Language Models (LLMs), enabling agents\nto generate posts, form opinions, and update follower networks. This simulation\nallows for more detailed observations of how opinion leaders emerge.\nAdditionally, we present an innovative application of Reinforcement Learning\n(RL) to replicate the process of opinion leader formation. Our findings reveal\nthat limiting the action space and incorporating self-observation are key\nfactors for achieving stable opinion leader generation. The learning curves\ndemonstrate the model's capacity to identify optimal strategies and adapt to\ncomplex, unpredictable dynamics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the dynamics of public opinion evolution on online social\nplatforms is critical for analyzing influence mechanisms. Traditional\napproaches to influencer analysis are typically divided into qualitative\nassessments of personal attributes and quantitative evaluations of influence\npower. In this study, we introduce a novel simulated environment that combines\nAgent-Based Modeling (ABM) with Large Language Models (LLMs), enabling agents\nto generate posts, form opinions, and update follower networks. This simulation\nallows for more detailed observations of how opinion leaders emerge.\nAdditionally, we present an innovative application of Reinforcement Learning\n(RL) to replicate the process of opinion leader formation. Our findings reveal\nthat limiting the action space and incorporating self-observation are key\nfactors for achieving stable opinion leader generation. The learning curves\ndemonstrate the model's capacity to identify optimal strategies and adapt to\ncomplex, unpredictable dynamics."
                },
                "authors": [
                    {
                        "name": "Bailu Jin"
                    },
                    {
                        "name": "Weisi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Weisi Guo"
                },
                "author": "Weisi Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19635v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19635v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14569v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14569v2",
                "updated": "2024-11-29T11:30:49Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    11,
                    30,
                    49,
                    4,
                    334,
                    0
                ],
                "published": "2024-10-18T16:16:34Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    16,
                    16,
                    34,
                    4,
                    292,
                    0
                ],
                "title": "When LLMs Go Online: The Emerging Threat of Web-Enabled LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When LLMs Go Online: The Emerging Threat of Web-Enabled LLMs"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) have established them as\nagentic systems capable of planning and interacting with various tools. These\nLLM agents are often paired with web-based tools, enabling access to diverse\nsources and real-time information. Although these advancements offer\nsignificant benefits across various applications, they also increase the risk\nof malicious use, particularly in cyberattacks involving personal information.\nIn this work, we investigate the risks associated with misuse of LLM agents in\ncyberattacks involving personal data. Specifically, we aim to understand: 1)\nhow potent LLM agents can be when directed to conduct cyberattacks, 2) how\ncyberattacks are enhanced by web-based tools, and 3) how affordable and easy it\nbecomes to launch cyberattacks using LLM agents. We examine three attack\nscenarios: the collection of Personally Identifiable Information (PII), the\ngeneration of impersonation posts, and the creation of spear-phishing emails.\nOur experiments reveal the effectiveness of LLM agents in these attacks: LLM\nagents achieved a precision of up to 95.9% in collecting PII, up to 93.9% of\nimpersonation posts created by LLM agents were evaluated as authentic, and the\nclick rate for links in spear phishing emails created by LLM agents reached up\nto 46.67%. Additionally, our findings underscore the limitations of existing\nsafeguards in contemporary commercial LLMs, emphasizing the urgent need for\nmore robust security measures to prevent the misuse of LLM agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) have established them as\nagentic systems capable of planning and interacting with various tools. These\nLLM agents are often paired with web-based tools, enabling access to diverse\nsources and real-time information. Although these advancements offer\nsignificant benefits across various applications, they also increase the risk\nof malicious use, particularly in cyberattacks involving personal information.\nIn this work, we investigate the risks associated with misuse of LLM agents in\ncyberattacks involving personal data. Specifically, we aim to understand: 1)\nhow potent LLM agents can be when directed to conduct cyberattacks, 2) how\ncyberattacks are enhanced by web-based tools, and 3) how affordable and easy it\nbecomes to launch cyberattacks using LLM agents. We examine three attack\nscenarios: the collection of Personally Identifiable Information (PII), the\ngeneration of impersonation posts, and the creation of spear-phishing emails.\nOur experiments reveal the effectiveness of LLM agents in these attacks: LLM\nagents achieved a precision of up to 95.9% in collecting PII, up to 93.9% of\nimpersonation posts created by LLM agents were evaluated as authentic, and the\nclick rate for links in spear phishing emails created by LLM agents reached up\nto 46.67%. Additionally, our findings underscore the limitations of existing\nsafeguards in contemporary commercial LLMs, emphasizing the urgent need for\nmore robust security measures to prevent the misuse of LLM agents."
                },
                "authors": [
                    {
                        "name": "Hanna Kim"
                    },
                    {
                        "name": "Minkyoo Song"
                    },
                    {
                        "name": "Seung Ho Na"
                    },
                    {
                        "name": "Seungwon Shin"
                    },
                    {
                        "name": "Kimin Lee"
                    }
                ],
                "author_detail": {
                    "name": "Kimin Lee"
                },
                "author": "Kimin Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14569v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14569v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19628v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19628v1",
                "updated": "2024-11-29T11:24:23Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    11,
                    24,
                    23,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T11:24:23Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    11,
                    24,
                    23,
                    4,
                    334,
                    0
                ],
                "title": "Accelerating Multimodal Large Language Models via Dynamic Visual-Token\n  Exit and the Empirical Findings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Multimodal Large Language Models via Dynamic Visual-Token\n  Exit and the Empirical Findings"
                },
                "summary": "The excessive use of visual tokens in existing Multimoal Large Language\nModels (MLLMs) often exhibits obvious redundancy and brings in prohibitively\nexpensive computation. To gain insights into this problem, we first conduct\nextensive empirical studies on the attention behaviors of MLLMs, and summarize\nthree main inference stages in MLLMs: (i) Early fusion between tokens is first\naccomplished quickly. (ii) Intra-modality modeling then comes to play. (iii)\nMultimodal reasoning} resumes and lasts until the end of inference. In\nparticular, we reveal that visual tokens will stop contributing to reasoning\nwhen the text tokens receive enough image information, yielding obvious visual\nredundancy. Based on these generalized observations, we propose a simple yet\neffective method to improve the efficiency of MLLMs, termed dynamic\nvisual-token exit (DyVTE). DyVTE uses lightweight hyper-networks to perceive\nthe text token status and decide the removal of all visual tokens after a\ncertain layer, thereby addressing the observed visual redundancy. To validate\nVTE, we apply it to a set of MLLMs, including LLaVA, VILA, Eagle and InternVL,\nand conduct extensive experiments on a bunch of benchmarks. The experiment\nresults not only show the effectiveness of our VTE in improving MLLMs'\nefficiency, but also yield the general modeling patterns of MLLMs, well\nfacilitating the in-depth understanding of MLLMs. Our code is anonymously\nreleased at https://github.com/DoubtedSteam/DyVTE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The excessive use of visual tokens in existing Multimoal Large Language\nModels (MLLMs) often exhibits obvious redundancy and brings in prohibitively\nexpensive computation. To gain insights into this problem, we first conduct\nextensive empirical studies on the attention behaviors of MLLMs, and summarize\nthree main inference stages in MLLMs: (i) Early fusion between tokens is first\naccomplished quickly. (ii) Intra-modality modeling then comes to play. (iii)\nMultimodal reasoning} resumes and lasts until the end of inference. In\nparticular, we reveal that visual tokens will stop contributing to reasoning\nwhen the text tokens receive enough image information, yielding obvious visual\nredundancy. Based on these generalized observations, we propose a simple yet\neffective method to improve the efficiency of MLLMs, termed dynamic\nvisual-token exit (DyVTE). DyVTE uses lightweight hyper-networks to perceive\nthe text token status and decide the removal of all visual tokens after a\ncertain layer, thereby addressing the observed visual redundancy. To validate\nVTE, we apply it to a set of MLLMs, including LLaVA, VILA, Eagle and InternVL,\nand conduct extensive experiments on a bunch of benchmarks. The experiment\nresults not only show the effectiveness of our VTE in improving MLLMs'\nefficiency, but also yield the general modeling patterns of MLLMs, well\nfacilitating the in-depth understanding of MLLMs. Our code is anonymously\nreleased at https://github.com/DoubtedSteam/DyVTE."
                },
                "authors": [
                    {
                        "name": "Qiong Wu"
                    },
                    {
                        "name": "Wenhao Lin"
                    },
                    {
                        "name": "Weihao Ye"
                    },
                    {
                        "name": "Yiyi Zhou"
                    },
                    {
                        "name": "Xiaoshuai Sun"
                    },
                    {
                        "name": "Rongrong Ji"
                    }
                ],
                "author_detail": {
                    "name": "Rongrong Ji"
                },
                "author": "Rongrong Ji",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19628v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19628v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19626v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19626v1",
                "updated": "2024-11-29T11:23:15Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    11,
                    23,
                    15,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T11:23:15Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    11,
                    23,
                    15,
                    4,
                    334,
                    0
                ],
                "title": "GREAT: Geometry-Intention Collaborative Inference for Open-Vocabulary 3D\n  Object Affordance Grounding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GREAT: Geometry-Intention Collaborative Inference for Open-Vocabulary 3D\n  Object Affordance Grounding"
                },
                "summary": "Open-Vocabulary 3D object affordance grounding aims to anticipate ``action\npossibilities'' regions on 3D objects with arbitrary instructions, which is\ncrucial for robots to generically perceive real scenarios and respond to\noperational changes. Existing methods focus on combining images or languages\nthat depict interactions with 3D geometries to introduce external interaction\npriors. However, they are still vulnerable to a limited semantic space by\nfailing to leverage implied invariant geometries and potential interaction\nintentions. Normally, humans address complex tasks through multi-step reasoning\nand respond to diverse situations by leveraging associative and analogical\nthinking. In light of this, we propose GREAT (GeometRy-intEntion collAboraTive\ninference) for Open-Vocabulary 3D Object Affordance Grounding, a novel\nframework that mines the object invariant geometry attributes and performs\nanalogically reason in potential interaction scenarios to form affordance\nknowledge, fully combining the knowledge with both geometries and visual\ncontents to ground 3D object affordance. Besides, we introduce the Point Image\nAffordance Dataset v2 (PIADv2), the largest 3D object affordance dataset at\npresent to support the task. Extensive experiments demonstrate the\neffectiveness and superiority of GREAT. Code and dataset are available at\nproject.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open-Vocabulary 3D object affordance grounding aims to anticipate ``action\npossibilities'' regions on 3D objects with arbitrary instructions, which is\ncrucial for robots to generically perceive real scenarios and respond to\noperational changes. Existing methods focus on combining images or languages\nthat depict interactions with 3D geometries to introduce external interaction\npriors. However, they are still vulnerable to a limited semantic space by\nfailing to leverage implied invariant geometries and potential interaction\nintentions. Normally, humans address complex tasks through multi-step reasoning\nand respond to diverse situations by leveraging associative and analogical\nthinking. In light of this, we propose GREAT (GeometRy-intEntion collAboraTive\ninference) for Open-Vocabulary 3D Object Affordance Grounding, a novel\nframework that mines the object invariant geometry attributes and performs\nanalogically reason in potential interaction scenarios to form affordance\nknowledge, fully combining the knowledge with both geometries and visual\ncontents to ground 3D object affordance. Besides, we introduce the Point Image\nAffordance Dataset v2 (PIADv2), the largest 3D object affordance dataset at\npresent to support the task. Extensive experiments demonstrate the\neffectiveness and superiority of GREAT. Code and dataset are available at\nproject."
                },
                "authors": [
                    {
                        "name": "Yawen Shao"
                    },
                    {
                        "name": "Wei Zhai"
                    },
                    {
                        "name": "Yuhang Yang"
                    },
                    {
                        "name": "Hongchen Luo"
                    },
                    {
                        "name": "Yang Cao"
                    },
                    {
                        "name": "Zheng-Jun Zha"
                    }
                ],
                "author_detail": {
                    "name": "Zheng-Jun Zha"
                },
                "author": "Zheng-Jun Zha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19626v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19626v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.13299v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.13299v2",
                "updated": "2024-11-29T11:21:29Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    11,
                    21,
                    29,
                    4,
                    334,
                    0
                ],
                "published": "2024-10-17T07:55:47Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    7,
                    55,
                    47,
                    3,
                    291,
                    0
                ],
                "title": "LLM-Rank: A Graph Theoretical Approach to Pruning Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Rank: A Graph Theoretical Approach to Pruning Large Language Models"
                },
                "summary": "The evolving capabilities of large language models are accompanied by growing\nsizes and deployment costs, necessitating effective inference optimisation\ntechniques. We propose a novel pruning method utilising centrality measures\nfrom graph theory, reducing both the computational requirements and the memory\nfootprint of these models. Specifically, we devise a method for creating a\nweighted directed acyclical graph representation of multilayer perceptrons to\nwhich we apply a modified version of the weighted PageRank centrality measure\nto compute node importance scores. In combination with uniform pruning this\nleads to structured sparsity. We call this pruning method MLPRank. Furthermore\nwe introduce an extension to decoder-only transformer models and call it\nLLMRank. For both variants we demonstrate a strong performance. With MLPRank on\naverage leading to 6.09 % higher accuracy retention than three popular\nbaselines and 13.42 % with LLMRank compared to two popular baselines. Code is\navailable at https://github.com/amazon-science/llm-rank-pruning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The evolving capabilities of large language models are accompanied by growing\nsizes and deployment costs, necessitating effective inference optimisation\ntechniques. We propose a novel pruning method utilising centrality measures\nfrom graph theory, reducing both the computational requirements and the memory\nfootprint of these models. Specifically, we devise a method for creating a\nweighted directed acyclical graph representation of multilayer perceptrons to\nwhich we apply a modified version of the weighted PageRank centrality measure\nto compute node importance scores. In combination with uniform pruning this\nleads to structured sparsity. We call this pruning method MLPRank. Furthermore\nwe introduce an extension to decoder-only transformer models and call it\nLLMRank. For both variants we demonstrate a strong performance. With MLPRank on\naverage leading to 6.09 % higher accuracy retention than three popular\nbaselines and 13.42 % with LLMRank compared to two popular baselines. Code is\navailable at https://github.com/amazon-science/llm-rank-pruning."
                },
                "authors": [
                    {
                        "name": "David Hoffmann"
                    },
                    {
                        "name": "Kailash Budhathoki"
                    },
                    {
                        "name": "Matthaeus Kleindessner"
                    }
                ],
                "author_detail": {
                    "name": "Matthaeus Kleindessner"
                },
                "author": "Matthaeus Kleindessner",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.13299v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.13299v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19617v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19617v1",
                "updated": "2024-11-29T11:10:29Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    11,
                    10,
                    29,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T11:10:29Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    11,
                    10,
                    29,
                    4,
                    334,
                    0
                ],
                "title": "Materials Learning Algorithms (MALA): Scalable Machine Learning for\n  Electronic Structure Calculations in Large-Scale Atomistic Simulations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Materials Learning Algorithms (MALA): Scalable Machine Learning for\n  Electronic Structure Calculations in Large-Scale Atomistic Simulations"
                },
                "summary": "We present the Materials Learning Algorithms (MALA) package, a scalable\nmachine learning framework designed to accelerate density functional theory\n(DFT) calculations suitable for large-scale atomistic simulations. Using local\ndescriptors of the atomic environment, MALA models efficiently predict key\nelectronic observables, including local density of states, electronic density,\ndensity of states, and total energy. The package integrates data sampling,\nmodel training and scalable inference into a unified library, while ensuring\ncompatibility with standard DFT and molecular dynamics codes. We demonstrate\nMALA's capabilities with examples including boron clusters, aluminum across its\nsolid-liquid phase boundary, and predicting the electronic structure of a\nstacking fault in a large beryllium slab. Scaling analyses reveal MALA's\ncomputational efficiency and identify bottlenecks for future optimization. With\nits ability to model electronic structures at scales far beyond standard DFT,\nMALA is well suited for modeling complex material systems, making it a\nversatile tool for advanced materials research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present the Materials Learning Algorithms (MALA) package, a scalable\nmachine learning framework designed to accelerate density functional theory\n(DFT) calculations suitable for large-scale atomistic simulations. Using local\ndescriptors of the atomic environment, MALA models efficiently predict key\nelectronic observables, including local density of states, electronic density,\ndensity of states, and total energy. The package integrates data sampling,\nmodel training and scalable inference into a unified library, while ensuring\ncompatibility with standard DFT and molecular dynamics codes. We demonstrate\nMALA's capabilities with examples including boron clusters, aluminum across its\nsolid-liquid phase boundary, and predicting the electronic structure of a\nstacking fault in a large beryllium slab. Scaling analyses reveal MALA's\ncomputational efficiency and identify bottlenecks for future optimization. With\nits ability to model electronic structures at scales far beyond standard DFT,\nMALA is well suited for modeling complex material systems, making it a\nversatile tool for advanced materials research."
                },
                "authors": [
                    {
                        "name": "Attila Cangi"
                    },
                    {
                        "name": "Lenz Fiedler"
                    },
                    {
                        "name": "Bartosz Brzoza"
                    },
                    {
                        "name": "Karan Shah"
                    },
                    {
                        "name": "Timothy J. Callow"
                    },
                    {
                        "name": "Daniel Kotik"
                    },
                    {
                        "name": "Steve Schmerler"
                    },
                    {
                        "name": "Matthew C. Barry"
                    },
                    {
                        "name": "James M. Goff"
                    },
                    {
                        "name": "Andrew Rohskopf"
                    },
                    {
                        "name": "Dayton J. Vogel"
                    },
                    {
                        "name": "Normand Modine"
                    },
                    {
                        "name": "Aidan P. Thompson"
                    },
                    {
                        "name": "Sivasankaran Rajamanickam"
                    }
                ],
                "author_detail": {
                    "name": "Sivasankaran Rajamanickam"
                },
                "author": "Sivasankaran Rajamanickam",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19617v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19617v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.12025v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.12025v3",
                "updated": "2024-11-29T10:46:22Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    10,
                    46,
                    22,
                    4,
                    334,
                    0
                ],
                "published": "2024-02-19T10:34:13Z",
                "published_parsed": [
                    2024,
                    2,
                    19,
                    10,
                    34,
                    13,
                    0,
                    50,
                    0
                ],
                "title": "Speech Translation with Speech Foundation Models and Large Language\n  Models: What is There and What is Missing?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speech Translation with Speech Foundation Models and Large Language\n  Models: What is There and What is Missing?"
                },
                "summary": "The field of natural language processing (NLP) has recently witnessed a\ntransformative shift with the emergence of foundation models, particularly\nLarge Language Models (LLMs) that have revolutionized text-based NLP. This\nparadigm has extended to other modalities, including speech, where researchers\nare actively exploring the combination of Speech Foundation Models (SFMs) and\nLLMs into single, unified models capable of addressing multimodal tasks. Among\nsuch tasks, this paper focuses on speech-to-text translation (ST). By examining\nthe published papers on the topic, we propose a unified view of the\narchitectural solutions and training strategies presented so far, highlighting\nsimilarities and differences among them. Based on this examination, we not only\norganize the lessons learned but also show how diverse settings and evaluation\napproaches hinder the identification of the best-performing solution for each\narchitectural building block and training choice. Lastly, we outline\nrecommendations for future works on the topic aimed at better understanding the\nstrengths and weaknesses of the SFM+LLM solutions for ST.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The field of natural language processing (NLP) has recently witnessed a\ntransformative shift with the emergence of foundation models, particularly\nLarge Language Models (LLMs) that have revolutionized text-based NLP. This\nparadigm has extended to other modalities, including speech, where researchers\nare actively exploring the combination of Speech Foundation Models (SFMs) and\nLLMs into single, unified models capable of addressing multimodal tasks. Among\nsuch tasks, this paper focuses on speech-to-text translation (ST). By examining\nthe published papers on the topic, we propose a unified view of the\narchitectural solutions and training strategies presented so far, highlighting\nsimilarities and differences among them. Based on this examination, we not only\norganize the lessons learned but also show how diverse settings and evaluation\napproaches hinder the identification of the best-performing solution for each\narchitectural building block and training choice. Lastly, we outline\nrecommendations for future works on the topic aimed at better understanding the\nstrengths and weaknesses of the SFM+LLM solutions for ST."
                },
                "authors": [
                    {
                        "name": "Marco Gaido"
                    },
                    {
                        "name": "Sara Papi"
                    },
                    {
                        "name": "Matteo Negri"
                    },
                    {
                        "name": "Luisa Bentivogli"
                    }
                ],
                "author_detail": {
                    "name": "Luisa Bentivogli"
                },
                "author": "Luisa Bentivogli",
                "arxiv_comment": "Outstanding paper at the ACL 2024 main conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.12025v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.12025v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.08367v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.08367v2",
                "updated": "2024-11-29T10:39:26Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    10,
                    39,
                    26,
                    4,
                    334,
                    0
                ],
                "published": "2023-10-12T14:38:25Z",
                "published_parsed": [
                    2023,
                    10,
                    12,
                    14,
                    38,
                    25,
                    3,
                    285,
                    0
                ],
                "title": "Towards Evaluating Generalist Agents: An Automated Benchmark in Open\n  World",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Evaluating Generalist Agents: An Automated Benchmark in Open\n  World"
                },
                "summary": "Evaluating generalist agents presents significant challenges due to their\nwide-ranging abilities and the limitations of current benchmarks in assessing\ntrue generalization. We introduce the Minecraft Universe (MCU), a fully\nautomated benchmarking framework set within the open-world game Minecraft. MCU\ndynamically generates and evaluates a broad spectrum of tasks, offering three\ncore components: 1) a task generation mechanism that provides high degrees of\nfreedom and variability, 2) an ever-expanding set of over 3K composable atomic\ntasks, and 3) a general evaluation framework that supports open-ended task\nassessment. By integrating large language models (LLMs), MCU dynamically\ncreates diverse environments for each evaluation, fostering agent\ngeneralization. The framework uses a vision-language model (VLM) to\nautomatically generate evaluation criteria, achieving over 90% agreement with\nhuman ratings across multi-dimensional assessments, which demonstrates that MCU\nis a scalable and explainable solution for evaluating generalist agents.\nAdditionally, we show that while state-of-the-art foundational models perform\nwell on specific tasks, they often struggle with increased task diversity and\ndifficulty.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating generalist agents presents significant challenges due to their\nwide-ranging abilities and the limitations of current benchmarks in assessing\ntrue generalization. We introduce the Minecraft Universe (MCU), a fully\nautomated benchmarking framework set within the open-world game Minecraft. MCU\ndynamically generates and evaluates a broad spectrum of tasks, offering three\ncore components: 1) a task generation mechanism that provides high degrees of\nfreedom and variability, 2) an ever-expanding set of over 3K composable atomic\ntasks, and 3) a general evaluation framework that supports open-ended task\nassessment. By integrating large language models (LLMs), MCU dynamically\ncreates diverse environments for each evaluation, fostering agent\ngeneralization. The framework uses a vision-language model (VLM) to\nautomatically generate evaluation criteria, achieving over 90% agreement with\nhuman ratings across multi-dimensional assessments, which demonstrates that MCU\nis a scalable and explainable solution for evaluating generalist agents.\nAdditionally, we show that while state-of-the-art foundational models perform\nwell on specific tasks, they often struggle with increased task diversity and\ndifficulty."
                },
                "authors": [
                    {
                        "name": "Xinyue Zheng"
                    },
                    {
                        "name": "Haowei Lin"
                    },
                    {
                        "name": "Kaichen He"
                    },
                    {
                        "name": "Zihao Wang"
                    },
                    {
                        "name": "Zilong Zheng"
                    },
                    {
                        "name": "Yitao Liang"
                    }
                ],
                "author_detail": {
                    "name": "Yitao Liang"
                },
                "author": "Yitao Liang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.08367v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.08367v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06622v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06622v2",
                "updated": "2024-11-29T10:30:11Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    10,
                    30,
                    11,
                    4,
                    334,
                    0
                ],
                "published": "2024-09-10T16:22:18Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    16,
                    22,
                    18,
                    1,
                    254,
                    0
                ],
                "title": "Exploring Italian sentence embeddings properties through multi-tasking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Italian sentence embeddings properties through multi-tasking"
                },
                "summary": "We investigate to what degree existing LLMs encode abstract linguistic\ninformation in Italian in a multi-task setting. We exploit curated synthetic\ndata on a large scale -- several Blackbird Language Matrices (BLMs) problems in\nItalian -- and use them to study how sentence representations built using\npre-trained language models encode specific syntactic and semantic information.\nWe use a two-level architecture to model separately a compression of the\nsentence embeddings into a representation that contains relevant information\nfor a task, and a BLM task. We then investigate whether we can obtain\ncompressed sentence representations that encode syntactic and semantic\ninformation relevant to several BLM tasks. While we expected that the sentence\nstructure -- in terms of sequence of phrases/chunks -- and chunk properties\ncould be shared across tasks, performance and error analysis show that the\nclues for the different tasks are encoded in different manners in the sentence\nembeddings, suggesting that abstract linguistic notions such as constituents or\nthematic roles does not seem to be present in the pretrained sentence\nembeddings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate to what degree existing LLMs encode abstract linguistic\ninformation in Italian in a multi-task setting. We exploit curated synthetic\ndata on a large scale -- several Blackbird Language Matrices (BLMs) problems in\nItalian -- and use them to study how sentence representations built using\npre-trained language models encode specific syntactic and semantic information.\nWe use a two-level architecture to model separately a compression of the\nsentence embeddings into a representation that contains relevant information\nfor a task, and a BLM task. We then investigate whether we can obtain\ncompressed sentence representations that encode syntactic and semantic\ninformation relevant to several BLM tasks. While we expected that the sentence\nstructure -- in terms of sequence of phrases/chunks -- and chunk properties\ncould be shared across tasks, performance and error analysis show that the\nclues for the different tasks are encoded in different manners in the sentence\nembeddings, suggesting that abstract linguistic notions such as constituents or\nthematic roles does not seem to be present in the pretrained sentence\nembeddings."
                },
                "authors": [
                    {
                        "name": "Vivi Nastase"
                    },
                    {
                        "name": "Giuseppe Samo"
                    },
                    {
                        "name": "Chunyang Jiang"
                    },
                    {
                        "name": "Paola Merlo"
                    }
                ],
                "author_detail": {
                    "name": "Paola Merlo"
                },
                "author": "Paola Merlo",
                "arxiv_comment": "11 pages, 6 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06622v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06622v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.02085v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.02085v4",
                "updated": "2024-11-29T10:10:43Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    10,
                    10,
                    43,
                    4,
                    334,
                    0
                ],
                "published": "2024-08-04T16:50:07Z",
                "published_parsed": [
                    2024,
                    8,
                    4,
                    16,
                    50,
                    7,
                    6,
                    217,
                    0
                ],
                "title": "Unleashing the Power of Data Tsunami: A Comprehensive Survey on Data\n  Assessment and Selection for Instruction Tuning of Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unleashing the Power of Data Tsunami: A Comprehensive Survey on Data\n  Assessment and Selection for Instruction Tuning of Language Models"
                },
                "summary": "Instruction tuning plays a critical role in aligning large language models\n(LLMs) with human preference. Despite the vast amount of open instruction\ndatasets, naively training a LLM on all existing instructions may not be\noptimal and practical. To pinpoint the most beneficial datapoints, data\nassessment and selection methods have been proposed in the fields of natural\nlanguage processing (NLP) and deep learning. However, under the context of\ninstruction tuning, there still exists a gap in knowledge on what kind of data\nevaluation metrics can be employed and how they can be integrated into the\nselection mechanism. To bridge this gap, we present a comprehensive review on\nexisting literature of data assessment and selection especially for instruction\ntuning of LLMs. We systematically categorize all applicable methods into\nquality-based, diversity-based, and importance-based ones where a unified,\nfine-grained taxonomy is structured. For each category, representative methods\nare elaborated to describe the landscape of relevant research. In addition,\ncomparison between the latest methods is conducted on their officially reported\nresults to provide in-depth discussions on their limitations. Finally, we\nsummarize the open challenges and propose the promosing avenues for future\nstudies. All related contents are available at\nhttps://github.com/yuleiqin/fantastic-data-engineering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction tuning plays a critical role in aligning large language models\n(LLMs) with human preference. Despite the vast amount of open instruction\ndatasets, naively training a LLM on all existing instructions may not be\noptimal and practical. To pinpoint the most beneficial datapoints, data\nassessment and selection methods have been proposed in the fields of natural\nlanguage processing (NLP) and deep learning. However, under the context of\ninstruction tuning, there still exists a gap in knowledge on what kind of data\nevaluation metrics can be employed and how they can be integrated into the\nselection mechanism. To bridge this gap, we present a comprehensive review on\nexisting literature of data assessment and selection especially for instruction\ntuning of LLMs. We systematically categorize all applicable methods into\nquality-based, diversity-based, and importance-based ones where a unified,\nfine-grained taxonomy is structured. For each category, representative methods\nare elaborated to describe the landscape of relevant research. In addition,\ncomparison between the latest methods is conducted on their officially reported\nresults to provide in-depth discussions on their limitations. Finally, we\nsummarize the open challenges and propose the promosing avenues for future\nstudies. All related contents are available at\nhttps://github.com/yuleiqin/fantastic-data-engineering."
                },
                "authors": [
                    {
                        "name": "Yulei Qin"
                    },
                    {
                        "name": "Yuncheng Yang"
                    },
                    {
                        "name": "Pengcheng Guo"
                    },
                    {
                        "name": "Gang Li"
                    },
                    {
                        "name": "Hang Shao"
                    },
                    {
                        "name": "Yuchen Shi"
                    },
                    {
                        "name": "Zihan Xu"
                    },
                    {
                        "name": "Yun Gu"
                    },
                    {
                        "name": "Ke Li"
                    },
                    {
                        "name": "Xing Sun"
                    }
                ],
                "author_detail": {
                    "name": "Xing Sun"
                },
                "author": "Xing Sun",
                "arxiv_comment": "review, survey, 37 pages, 5 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.02085v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.02085v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19589v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19589v1",
                "updated": "2024-11-29T10:10:16Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    10,
                    10,
                    16,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T10:10:16Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    10,
                    10,
                    16,
                    4,
                    334,
                    0
                ],
                "title": "Can Large Language Models Reason about the Region Connection Calculus?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Large Language Models Reason about the Region Connection Calculus?"
                },
                "summary": "Qualitative Spatial Reasoning is a well explored area of Knowledge\nRepresentation and Reasoning and has multiple applications ranging from\nGeographical Information Systems to Robotics and Computer Vision. Recently,\nmany claims have been made for the reasoning capabilities of Large Language\nModels (LLMs). Here, we investigate the extent to which a set of representative\nLLMs can perform classical qualitative spatial reasoning tasks on the\nmereotopological Region Connection Calculus, RCC-8. We conduct three pairs of\nexperiments (reconstruction of composition tables, alignment to human\ncomposition preferences, conceptual neighbourhood reconstruction) using\nstate-of-the-art LLMs; in each pair one experiment uses eponymous relations and\none, anonymous relations (to test the extent to which the LLM relies on\nknowledge about the relation names obtained during training). All instances are\nrepeated 30 times to measure the stochasticity of the LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Qualitative Spatial Reasoning is a well explored area of Knowledge\nRepresentation and Reasoning and has multiple applications ranging from\nGeographical Information Systems to Robotics and Computer Vision. Recently,\nmany claims have been made for the reasoning capabilities of Large Language\nModels (LLMs). Here, we investigate the extent to which a set of representative\nLLMs can perform classical qualitative spatial reasoning tasks on the\nmereotopological Region Connection Calculus, RCC-8. We conduct three pairs of\nexperiments (reconstruction of composition tables, alignment to human\ncomposition preferences, conceptual neighbourhood reconstruction) using\nstate-of-the-art LLMs; in each pair one experiment uses eponymous relations and\none, anonymous relations (to test the extent to which the LLM relies on\nknowledge about the relation names obtained during training). All instances are\nrepeated 30 times to measure the stochasticity of the LLMs."
                },
                "authors": [
                    {
                        "name": "Anthony G Cohn"
                    },
                    {
                        "name": "Robert E Blackwell"
                    }
                ],
                "author_detail": {
                    "name": "Robert E Blackwell"
                },
                "author": "Robert E Blackwell",
                "arxiv_comment": "13 pages. arXiv admin note: text overlap with arXiv:2309.15577",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19589v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19589v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19581v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19581v1",
                "updated": "2024-11-29T09:54:08Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    9,
                    54,
                    8,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T09:54:08Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    9,
                    54,
                    8,
                    4,
                    334,
                    0
                ],
                "title": "In-Context Learning with Noisy Labels",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-Context Learning with Noisy Labels"
                },
                "summary": "In-context learning refers to the emerging ability of large language models\n(LLMs) to perform a target task without additional training, utilizing\ndemonstrations of the task. Recent studies aim to enhance in-context learning\nperformance by selecting more useful demonstrations. However, they overlook the\npresence of inevitable noisy labels in task demonstrations that arise during\nthe labeling process in the real-world. In this paper, we propose a new task,\nin-context learning with noisy labels, which aims to solve real-world problems\nfor in-context learning where labels in task demonstrations would be corrupted.\nMoreover, we propose a new method and baseline methods for the new task,\ninspired by studies in learning with noisy labels. Through experiments, we\ndemonstrate that our proposed method can serve as a safeguard against\nperformance degradation in in-context learning caused by noisy labels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context learning refers to the emerging ability of large language models\n(LLMs) to perform a target task without additional training, utilizing\ndemonstrations of the task. Recent studies aim to enhance in-context learning\nperformance by selecting more useful demonstrations. However, they overlook the\npresence of inevitable noisy labels in task demonstrations that arise during\nthe labeling process in the real-world. In this paper, we propose a new task,\nin-context learning with noisy labels, which aims to solve real-world problems\nfor in-context learning where labels in task demonstrations would be corrupted.\nMoreover, we propose a new method and baseline methods for the new task,\ninspired by studies in learning with noisy labels. Through experiments, we\ndemonstrate that our proposed method can serve as a safeguard against\nperformance degradation in in-context learning caused by noisy labels."
                },
                "authors": [
                    {
                        "name": "Junyong Kang"
                    },
                    {
                        "name": "Donghyun Son"
                    },
                    {
                        "name": "Hwanjun Song"
                    },
                    {
                        "name": "Buru Chang"
                    }
                ],
                "author_detail": {
                    "name": "Buru Chang"
                },
                "author": "Buru Chang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19581v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19581v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11401v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11401v3",
                "updated": "2024-11-29T09:48:33Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    9,
                    48,
                    33,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-18T09:24:01Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    9,
                    24,
                    1,
                    0,
                    323,
                    0
                ],
                "title": "Deep Learning-based Code Reviews: A Paradigm Shift or a Double-Edged\n  Sword?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Learning-based Code Reviews: A Paradigm Shift or a Double-Edged\n  Sword?"
                },
                "summary": "Several techniques have been proposed to automate code review. Early support\nconsisted in recommending the most suited reviewer for a given change or in\nprioritizing the review tasks. With the advent of deep learning in software\nengineering, the level of automation has been pushed to new heights, with\napproaches able to provide feedback on source code in natural language as a\nhuman reviewer would do. Also, recent work documented open source projects\nadopting Large Language Models (LLMs) as co-reviewers. Although the research in\nthis field is very active, little is known about the actual impact of including\nautomatically generated code reviews in the code review process. While there\nare many aspects worth investigating, in this work we focus on three of them:\n(i) review quality, i.e., the reviewer's ability to identify issues in the\ncode; (ii) review cost, i.e., the time spent reviewing the code; and (iii)\nreviewer's confidence, i.e., how confident is the reviewer about the provided\nfeedback. We run a controlled experiment with 29 experts who reviewed different\nprograms with/without the support of an automatically generated code review.\nDuring the experiment we monitored the reviewers' activities, for over 50 hours\nof recorded code reviews. We show that reviewers consider valid most of the\nissues automatically identified by the LLM and that the availability of an\nautomated review as a starting point strongly influences their behavior:\nReviewers tend to focus on the code locations indicated by the LLM rather than\nsearching for additional issues in other parts of the code. The reviewers who\nstarted from an automated review identified a higher number of low-severity\nissues while, however, not identifying more high-severity issues as compared to\na completely manual process. Finally, the automated support did not result in\nsaved time and did not increase the reviewers' confidence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Several techniques have been proposed to automate code review. Early support\nconsisted in recommending the most suited reviewer for a given change or in\nprioritizing the review tasks. With the advent of deep learning in software\nengineering, the level of automation has been pushed to new heights, with\napproaches able to provide feedback on source code in natural language as a\nhuman reviewer would do. Also, recent work documented open source projects\nadopting Large Language Models (LLMs) as co-reviewers. Although the research in\nthis field is very active, little is known about the actual impact of including\nautomatically generated code reviews in the code review process. While there\nare many aspects worth investigating, in this work we focus on three of them:\n(i) review quality, i.e., the reviewer's ability to identify issues in the\ncode; (ii) review cost, i.e., the time spent reviewing the code; and (iii)\nreviewer's confidence, i.e., how confident is the reviewer about the provided\nfeedback. We run a controlled experiment with 29 experts who reviewed different\nprograms with/without the support of an automatically generated code review.\nDuring the experiment we monitored the reviewers' activities, for over 50 hours\nof recorded code reviews. We show that reviewers consider valid most of the\nissues automatically identified by the LLM and that the availability of an\nautomated review as a starting point strongly influences their behavior:\nReviewers tend to focus on the code locations indicated by the LLM rather than\nsearching for additional issues in other parts of the code. The reviewers who\nstarted from an automated review identified a higher number of low-severity\nissues while, however, not identifying more high-severity issues as compared to\na completely manual process. Finally, the automated support did not result in\nsaved time and did not increase the reviewers' confidence."
                },
                "authors": [
                    {
                        "name": "Rosalia Tufano"
                    },
                    {
                        "name": "Alberto Martin-Lopez"
                    },
                    {
                        "name": "Ahmad Tayeb"
                    },
                    {
                        "name": "Ozren DabiÄ"
                    },
                    {
                        "name": "Sonia Haiduc"
                    },
                    {
                        "name": "Gabriele Bavota"
                    }
                ],
                "author_detail": {
                    "name": "Gabriele Bavota"
                },
                "author": "Gabriele Bavota",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11401v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11401v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19576v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19576v1",
                "updated": "2024-11-29T09:47:32Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    9,
                    47,
                    32,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T09:47:32Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    9,
                    47,
                    32,
                    4,
                    334,
                    0
                ],
                "title": "A Review of LLM-based Explanations in Recommender Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Review of LLM-based Explanations in Recommender Systems"
                },
                "summary": "The rise of Large Language Models (LLMs), such as LLaMA and ChatGPT, has\nopened new opportunities for enhancing recommender systems through improved\nexplainability. This paper provides a systematic literature review focused on\nleveraging LLMs to generate explanations for recommendations -- a critical\naspect for fostering transparency and user trust. We conducted a comprehensive\nsearch within the ACM Guide to Computing Literature, covering publications from\nthe launch of ChatGPT (November 2022) to the present (November 2024). Our\nsearch yielded 232 articles, but after applying inclusion criteria, only six\nwere identified as directly addressing the use of LLMs in explaining\nrecommendations. This scarcity highlights that, despite the rise of LLMs, their\napplication in explainable recommender systems is still in an early stage. We\nanalyze these select studies to understand current methodologies, identify\nchallenges, and suggest directions for future research. Our findings underscore\nthe potential of LLMs improving explanations of recommender systems and\nencourage the development of more transparent and user-centric recommendation\nexplanation solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of Large Language Models (LLMs), such as LLaMA and ChatGPT, has\nopened new opportunities for enhancing recommender systems through improved\nexplainability. This paper provides a systematic literature review focused on\nleveraging LLMs to generate explanations for recommendations -- a critical\naspect for fostering transparency and user trust. We conducted a comprehensive\nsearch within the ACM Guide to Computing Literature, covering publications from\nthe launch of ChatGPT (November 2022) to the present (November 2024). Our\nsearch yielded 232 articles, but after applying inclusion criteria, only six\nwere identified as directly addressing the use of LLMs in explaining\nrecommendations. This scarcity highlights that, despite the rise of LLMs, their\napplication in explainable recommender systems is still in an early stage. We\nanalyze these select studies to understand current methodologies, identify\nchallenges, and suggest directions for future research. Our findings underscore\nthe potential of LLMs improving explanations of recommender systems and\nencourage the development of more transparent and user-centric recommendation\nexplanation solutions."
                },
                "authors": [
                    {
                        "name": "Alan Said"
                    }
                ],
                "author_detail": {
                    "name": "Alan Said"
                },
                "author": "Alan Said",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19576v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19576v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17515v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17515v3",
                "updated": "2024-11-29T09:44:13Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    9,
                    44,
                    13,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-26T15:26:06Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    15,
                    26,
                    6,
                    1,
                    331,
                    0
                ],
                "title": "SuperMat: Physically Consistent PBR Material Estimation at Interactive\n  Rates",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SuperMat: Physically Consistent PBR Material Estimation at Interactive\n  Rates"
                },
                "summary": "Decomposing physically-based materials from images into their constituent\nproperties remains challenging, particularly when maintaining both\ncomputational efficiency and physical consistency. While recent diffusion-based\napproaches have shown promise, they face substantial computational overhead due\nto multiple denoising steps and separate models for different material\nproperties. We present SuperMat, a single-step framework that achieves\nhigh-quality material decomposition with one-step inference. This enables\nend-to-end training with perceptual and re-render losses while decomposing\nalbedo, metallic, and roughness maps at millisecond-scale speeds. We further\nextend our framework to 3D objects through a UV refinement network, enabling\nconsistent material estimation across viewpoints while maintaining efficiency.\nExperiments demonstrate that SuperMat achieves state-of-the-art PBR material\ndecomposition quality while reducing inference time from seconds to\nmilliseconds per image, and completes PBR material estimation for 3D objects in\napproximately 3 seconds. The project page is at\nhttps://hyj542682306.github.io/SuperMat/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decomposing physically-based materials from images into their constituent\nproperties remains challenging, particularly when maintaining both\ncomputational efficiency and physical consistency. While recent diffusion-based\napproaches have shown promise, they face substantial computational overhead due\nto multiple denoising steps and separate models for different material\nproperties. We present SuperMat, a single-step framework that achieves\nhigh-quality material decomposition with one-step inference. This enables\nend-to-end training with perceptual and re-render losses while decomposing\nalbedo, metallic, and roughness maps at millisecond-scale speeds. We further\nextend our framework to 3D objects through a UV refinement network, enabling\nconsistent material estimation across viewpoints while maintaining efficiency.\nExperiments demonstrate that SuperMat achieves state-of-the-art PBR material\ndecomposition quality while reducing inference time from seconds to\nmilliseconds per image, and completes PBR material estimation for 3D objects in\napproximately 3 seconds. The project page is at\nhttps://hyj542682306.github.io/SuperMat/."
                },
                "authors": [
                    {
                        "name": "Yijia Hong"
                    },
                    {
                        "name": "Yuan-Chen Guo"
                    },
                    {
                        "name": "Ran Yi"
                    },
                    {
                        "name": "Yulong Chen"
                    },
                    {
                        "name": "Yan-Pei Cao"
                    },
                    {
                        "name": "Lizhuang Ma"
                    }
                ],
                "author_detail": {
                    "name": "Lizhuang Ma"
                },
                "author": "Lizhuang Ma",
                "arxiv_comment": "https://hyj542682306.github.io/SuperMat/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17515v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17515v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19572v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19572v1",
                "updated": "2024-11-29T09:36:10Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    9,
                    36,
                    10,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T09:36:10Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    9,
                    36,
                    10,
                    4,
                    334,
                    0
                ],
                "title": "Canonical correlation analysis of stochastic trends via functional\n  approximation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Canonical correlation analysis of stochastic trends via functional\n  approximation"
                },
                "summary": "This paper proposes a novel canonical correlation analysis for semiparametric\ninference in $I(1)/I(0)$ systems via functional approximation. The approach can\nbe applied coherently to panels of $p$ variables with a generic number $s$ of\nstochastic trends, as well as to subsets or aggregations of variables. This\nstudy discusses inferential tools on $s$ and on the loading matrix $\\psi$ of\nthe stochastic trends (and on their duals $r$ and $\\beta$, the cointegration\nrank and the cointegrating matrix): asymptotically pivotal test sequences and\nconsistent estimators of $s$ and $r$, $T$-consistent, mixed Gaussian and\nefficient estimators of $\\psi$ and $\\beta$, Wald tests thereof, and\nmisspecification tests for checking model assumptions. Monte Carlo simulations\nshow that these tools have reliable performance uniformly in $s$ for small,\nmedium and large-dimensional systems, with $p$ ranging from 10 to 300. An\nempirical analysis of 20 exchange rates illustrates the methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes a novel canonical correlation analysis for semiparametric\ninference in $I(1)/I(0)$ systems via functional approximation. The approach can\nbe applied coherently to panels of $p$ variables with a generic number $s$ of\nstochastic trends, as well as to subsets or aggregations of variables. This\nstudy discusses inferential tools on $s$ and on the loading matrix $\\psi$ of\nthe stochastic trends (and on their duals $r$ and $\\beta$, the cointegration\nrank and the cointegrating matrix): asymptotically pivotal test sequences and\nconsistent estimators of $s$ and $r$, $T$-consistent, mixed Gaussian and\nefficient estimators of $\\psi$ and $\\beta$, Wald tests thereof, and\nmisspecification tests for checking model assumptions. Monte Carlo simulations\nshow that these tools have reliable performance uniformly in $s$ for small,\nmedium and large-dimensional systems, with $p$ ranging from 10 to 300. An\nempirical analysis of 20 exchange rates illustrates the methods."
                },
                "authors": [
                    {
                        "name": "Massimo Franchi"
                    },
                    {
                        "name": "Iliyan Georgiev"
                    },
                    {
                        "name": "Paolo Paruolo"
                    }
                ],
                "author_detail": {
                    "name": "Paolo Paruolo"
                },
                "author": "Paolo Paruolo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19572v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19572v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19563v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19563v1",
                "updated": "2024-11-29T09:18:32Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    9,
                    18,
                    32,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T09:18:32Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    9,
                    18,
                    32,
                    4,
                    334,
                    0
                ],
                "title": "Ensemble Watermarks for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ensemble Watermarks for Large Language Models"
                },
                "summary": "The rapid advancement of large language models (LLMs) has made it\nincreasingly difficult to distinguish between text written by humans and\nmachines. While watermarks already exist for LLMs, they often lack flexibility,\nand struggle with attacks such as paraphrasing. To address these issues, we\npropose a multi-feature method for generating watermarks that combines multiple\ndistinct watermark features into an ensemble watermark. Concretely, we combine\nacrostica and sensorimotor norms with the established red-green watermark to\nachieve a 98% detection rate. After a paraphrasing attack the performance\nremains high with 95% detection rate. The red-green feature alone as baseline\nachieves a detection rate of 49%. The evaluation of all feature combinations\nreveals that the ensemble of all three consistently has the highest detection\nrate across several LLMs and watermark strength settings. Due to the\nflexibility of combining features in the ensemble, various requirements and\ntrade-offs can be addressed. Additionally, for all ensemble configurations the\nsame detection function can be used without adaptations. This method is\nparticularly of interest to facilitate accountability and prevent societal\nharm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of large language models (LLMs) has made it\nincreasingly difficult to distinguish between text written by humans and\nmachines. While watermarks already exist for LLMs, they often lack flexibility,\nand struggle with attacks such as paraphrasing. To address these issues, we\npropose a multi-feature method for generating watermarks that combines multiple\ndistinct watermark features into an ensemble watermark. Concretely, we combine\nacrostica and sensorimotor norms with the established red-green watermark to\nachieve a 98% detection rate. After a paraphrasing attack the performance\nremains high with 95% detection rate. The red-green feature alone as baseline\nachieves a detection rate of 49%. The evaluation of all feature combinations\nreveals that the ensemble of all three consistently has the highest detection\nrate across several LLMs and watermark strength settings. Due to the\nflexibility of combining features in the ensemble, various requirements and\ntrade-offs can be addressed. Additionally, for all ensemble configurations the\nsame detection function can be used without adaptations. This method is\nparticularly of interest to facilitate accountability and prevent societal\nharm."
                },
                "authors": [
                    {
                        "name": "Georg Niess"
                    },
                    {
                        "name": "Roman Kern"
                    }
                ],
                "author_detail": {
                    "name": "Roman Kern"
                },
                "author": "Roman Kern",
                "arxiv_comment": "9 pages in the main body. Code is available at\n  http://github.com/CommodoreEU/master-generation. arXiv admin note:\n  substantial text overlap with arXiv:2405.08400",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19563v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19563v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06752v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06752v2",
                "updated": "2024-11-29T09:17:29Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    9,
                    17,
                    29,
                    4,
                    334,
                    0
                ],
                "published": "2024-08-13T09:19:21Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    9,
                    19,
                    21,
                    1,
                    226,
                    0
                ],
                "title": "Evaluating Research Quality with Large Language Models: An Analysis of\n  ChatGPT's Effectiveness with Different Settings and Inputs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Research Quality with Large Language Models: An Analysis of\n  ChatGPT's Effectiveness with Different Settings and Inputs"
                },
                "summary": "Evaluating the quality of academic journal articles is a time consuming but\ncritical task for national research evaluation exercises, appointments and\npromotion. It is therefore important to investigate whether Large Language\nModels (LLMs) can play a role in this process. This article assesses which\nChatGPT inputs (full text without tables, figures and references; title and\nabstract; title only) produce better quality score estimates, and the extent to\nwhich scores are affected by ChatGPT models and system prompts. The results\nshow that the optimal input is the article title and abstract, with average\nChatGPT scores based on these (30 iterations on a dataset of 51 papers)\ncorrelating at 0.67 with human scores, the highest ever reported. ChatGPT 4o is\nslightly better than 3.5-turbo (0.66), and 4o-mini (0.66). The results suggest\nthat article full texts might confuse LLM research quality evaluations, even\nthough complex system instructions for the task are more effective than simple\nones. Thus, whilst abstracts contain insufficient information for a thorough\nassessment of rigour, they may contain strong pointers about originality and\nsignificance. Finally, linear regression can be used to convert the model\nscores into the human scale scores, which is 31% more accurate than guessing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating the quality of academic journal articles is a time consuming but\ncritical task for national research evaluation exercises, appointments and\npromotion. It is therefore important to investigate whether Large Language\nModels (LLMs) can play a role in this process. This article assesses which\nChatGPT inputs (full text without tables, figures and references; title and\nabstract; title only) produce better quality score estimates, and the extent to\nwhich scores are affected by ChatGPT models and system prompts. The results\nshow that the optimal input is the article title and abstract, with average\nChatGPT scores based on these (30 iterations on a dataset of 51 papers)\ncorrelating at 0.67 with human scores, the highest ever reported. ChatGPT 4o is\nslightly better than 3.5-turbo (0.66), and 4o-mini (0.66). The results suggest\nthat article full texts might confuse LLM research quality evaluations, even\nthough complex system instructions for the task are more effective than simple\nones. Thus, whilst abstracts contain insufficient information for a thorough\nassessment of rigour, they may contain strong pointers about originality and\nsignificance. Finally, linear regression can be used to convert the model\nscores into the human scale scores, which is 31% more accurate than guessing."
                },
                "authors": [
                    {
                        "name": "Mike Thelwall"
                    }
                ],
                "author_detail": {
                    "name": "Mike Thelwall"
                },
                "author": "Mike Thelwall",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06752v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06752v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19557v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19557v1",
                "updated": "2024-11-29T09:10:30Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    9,
                    10,
                    30,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T09:10:30Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    9,
                    10,
                    30,
                    4,
                    334,
                    0
                ],
                "title": "Initialization using Update Approximation is a Silver Bullet for\n  Extremely Efficient Low-Rank Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Initialization using Update Approximation is a Silver Bullet for\n  Extremely Efficient Low-Rank Fine-Tuning"
                },
                "summary": "Low-rank adapters have become a standard approach for efficiently fine-tuning\nlarge language models (LLMs), but they often fall short of achieving the\nperformance of full fine-tuning. We propose a method, LoRA Silver Bullet or\nLoRA-SB, that approximates full fine-tuning within low-rank subspaces using a\ncarefully designed initialization strategy. We theoretically demonstrate that\nthe architecture of LoRA-XS, which inserts a trainable (r x r) matrix between B\nand A while keeping other matrices fixed, provides the precise conditions\nneeded for this approximation. We leverage its constrained update space to\nachieve optimal scaling for high-rank gradient updates while removing the need\nfor hyperparameter tuning. We prove that our initialization offers an optimal\nlow-rank approximation of the initial gradient and preserves update directions\nthroughout training. Extensive experiments across mathematical reasoning,\ncommonsense reasoning, and language understanding tasks demonstrate that our\napproach exceeds the performance of standard LoRA while using 27-90x fewer\nparameters, and comprehensively outperforms LoRA-XS. Our findings establish\nthat it is possible to simulate full fine-tuning in low-rank subspaces, and\nachieve significant efficiency gains without sacrificing performance. Our code\nis publicly available at https://github.com/RaghavSinghal10/lora-sb.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-rank adapters have become a standard approach for efficiently fine-tuning\nlarge language models (LLMs), but they often fall short of achieving the\nperformance of full fine-tuning. We propose a method, LoRA Silver Bullet or\nLoRA-SB, that approximates full fine-tuning within low-rank subspaces using a\ncarefully designed initialization strategy. We theoretically demonstrate that\nthe architecture of LoRA-XS, which inserts a trainable (r x r) matrix between B\nand A while keeping other matrices fixed, provides the precise conditions\nneeded for this approximation. We leverage its constrained update space to\nachieve optimal scaling for high-rank gradient updates while removing the need\nfor hyperparameter tuning. We prove that our initialization offers an optimal\nlow-rank approximation of the initial gradient and preserves update directions\nthroughout training. Extensive experiments across mathematical reasoning,\ncommonsense reasoning, and language understanding tasks demonstrate that our\napproach exceeds the performance of standard LoRA while using 27-90x fewer\nparameters, and comprehensively outperforms LoRA-XS. Our findings establish\nthat it is possible to simulate full fine-tuning in low-rank subspaces, and\nachieve significant efficiency gains without sacrificing performance. Our code\nis publicly available at https://github.com/RaghavSinghal10/lora-sb."
                },
                "authors": [
                    {
                        "name": "Kaustubh Ponkshe"
                    },
                    {
                        "name": "Raghav Singhal"
                    },
                    {
                        "name": "Eduard Gorbunov"
                    },
                    {
                        "name": "Alexey Tumanov"
                    },
                    {
                        "name": "Samuel Horvath"
                    },
                    {
                        "name": "Praneeth Vepakomma"
                    }
                ],
                "author_detail": {
                    "name": "Praneeth Vepakomma"
                },
                "author": "Praneeth Vepakomma",
                "arxiv_comment": "Kaustubh Ponkshe and Raghav Singhal contributed equally to this work",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19557v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19557v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19554v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19554v1",
                "updated": "2024-11-29T09:07:21Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    9,
                    7,
                    21,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T09:07:21Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    9,
                    7,
                    21,
                    4,
                    334,
                    0
                ],
                "title": "Unimib Assistant: designing a student-friendly RAG-based chatbot for all\n  their needs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unimib Assistant: designing a student-friendly RAG-based chatbot for all\n  their needs"
                },
                "summary": "Natural language processing skills of Large Language Models (LLMs) are\nunprecedented, having wide diffusion and application in different tasks. This\npilot study focuses on specializing ChatGPT behavior through a\nRetrieval-Augmented Generation (RAG) system using the OpenAI custom GPTs\nfeature. The purpose of our chatbot, called Unimib Assistant, is to provide\ninformation and solutions to the specific needs of University of Milano-Bicocca\n(Unimib) students through a question-answering approach. We provided the system\nwith a prompt highlighting its specific purpose and behavior, as well as\nuniversity-related documents and links obtained from an initial need-finding\nphase, interviewing six students. After a preliminary customization phase, a\nqualitative usability test was conducted with six other students to identify\nthe strengths and weaknesses of the chatbot, with the goal of improving it in a\nsubsequent redesign phase. While the chatbot was appreciated for its\nuser-friendly experience, perceived general reliability, well-structured\nresponses, and conversational tone, several significant technical and\nfunctional limitations emerged. In particular, the satisfaction and overall\nexperience of the users was impaired by the system's inability to always\nprovide fully accurate information. Moreover, it would often neglect to report\nrelevant information even if present in the materials uploaded and prompt\ngiven. Furthermore, it sometimes generated unclickable links, undermining its\ntrustworthiness, since providing the source of information was an important\naspect for our users. Further in-depth studies and feedback from other users as\nwell as implementation iterations are planned to refine our Unimib Assistant.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Natural language processing skills of Large Language Models (LLMs) are\nunprecedented, having wide diffusion and application in different tasks. This\npilot study focuses on specializing ChatGPT behavior through a\nRetrieval-Augmented Generation (RAG) system using the OpenAI custom GPTs\nfeature. The purpose of our chatbot, called Unimib Assistant, is to provide\ninformation and solutions to the specific needs of University of Milano-Bicocca\n(Unimib) students through a question-answering approach. We provided the system\nwith a prompt highlighting its specific purpose and behavior, as well as\nuniversity-related documents and links obtained from an initial need-finding\nphase, interviewing six students. After a preliminary customization phase, a\nqualitative usability test was conducted with six other students to identify\nthe strengths and weaknesses of the chatbot, with the goal of improving it in a\nsubsequent redesign phase. While the chatbot was appreciated for its\nuser-friendly experience, perceived general reliability, well-structured\nresponses, and conversational tone, several significant technical and\nfunctional limitations emerged. In particular, the satisfaction and overall\nexperience of the users was impaired by the system's inability to always\nprovide fully accurate information. Moreover, it would often neglect to report\nrelevant information even if present in the materials uploaded and prompt\ngiven. Furthermore, it sometimes generated unclickable links, undermining its\ntrustworthiness, since providing the source of information was an important\naspect for our users. Further in-depth studies and feedback from other users as\nwell as implementation iterations are planned to refine our Unimib Assistant."
                },
                "authors": [
                    {
                        "name": "Chiara Antico"
                    },
                    {
                        "name": "Stefano Giordano"
                    },
                    {
                        "name": "Cansu Koyuturk"
                    },
                    {
                        "name": "Dimitri Ognibene"
                    }
                ],
                "author_detail": {
                    "name": "Dimitri Ognibene"
                },
                "author": "Dimitri Ognibene",
                "arxiv_comment": "Accepted for Italian Workshop on Artificial Intelligence for Human\n  Machine Interaction (AIxHMI 2024), November 26, 2024, Bolzano, Italy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19554v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19554v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16205v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16205v3",
                "updated": "2024-11-29T08:48:17Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    8,
                    48,
                    17,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-25T09:05:36Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    9,
                    5,
                    36,
                    0,
                    330,
                    0
                ],
                "title": "MH-MoE: Multi-Head Mixture-of-Experts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MH-MoE: Multi-Head Mixture-of-Experts"
                },
                "summary": "Multi-Head Mixture-of-Experts (MH-MoE) demonstrates superior performance by\nusing the multi-head mechanism to collectively attend to information from\nvarious representation spaces within different experts. In this paper, we\npresent a novel implementation of MH-MoE that maintains both FLOPs and\nparameter parity with sparse Mixture of Experts models. Experimental results on\nlanguage models show that the new implementation yields quality improvements\nover both vanilla MoE and fine-grained MoE models. Additionally, our\nexperiments demonstrate that MH-MoE is compatible with 1-bit Large Language\nModels (LLMs) such as BitNet.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Head Mixture-of-Experts (MH-MoE) demonstrates superior performance by\nusing the multi-head mechanism to collectively attend to information from\nvarious representation spaces within different experts. In this paper, we\npresent a novel implementation of MH-MoE that maintains both FLOPs and\nparameter parity with sparse Mixture of Experts models. Experimental results on\nlanguage models show that the new implementation yields quality improvements\nover both vanilla MoE and fine-grained MoE models. Additionally, our\nexperiments demonstrate that MH-MoE is compatible with 1-bit Large Language\nModels (LLMs) such as BitNet."
                },
                "authors": [
                    {
                        "name": "Shaohan Huang"
                    },
                    {
                        "name": "Xun Wu"
                    },
                    {
                        "name": "Shuming Ma"
                    },
                    {
                        "name": "Furu Wei"
                    }
                ],
                "author_detail": {
                    "name": "Furu Wei"
                },
                "author": "Furu Wei",
                "arxiv_comment": "7 pages, 0 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16205v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16205v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19547v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19547v1",
                "updated": "2024-11-29T08:47:04Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    8,
                    47,
                    4,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T08:47:04Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    8,
                    47,
                    4,
                    4,
                    334,
                    0
                ],
                "title": "Training Agents with Weakly Supervised Feedback from Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training Agents with Weakly Supervised Feedback from Large Language\n  Models"
                },
                "summary": "Large Language Models (LLMs) offer a promising basis for creating agents that\ncan tackle complex tasks through iterative environmental interaction. Existing\nmethods either require these agents to mimic expert-provided trajectories or\nrely on definitive environmental feedback for reinforcement learning which\nlimits their application to specific scenarios like gaming or code generation.\nThis paper introduces a novel training method for LLM-based agents using weakly\nsupervised signals from a critic LLM, bypassing the need for expert\ntrajectories or definitive feedback. Our agents are trained in iterative\nmanner, where they initially generate trajectories through environmental\ninteraction. Subsequently, a critic LLM selects a subset of good trajectories,\nwhich are then used to update the agents, enabling them to generate improved\ntrajectories in the next iteration. Extensive tests on the API-bank dataset\nshow consistent improvement in our agents' capabilities and comparable\nperformance to GPT-4, despite using open-source models with much fewer\nparameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) offer a promising basis for creating agents that\ncan tackle complex tasks through iterative environmental interaction. Existing\nmethods either require these agents to mimic expert-provided trajectories or\nrely on definitive environmental feedback for reinforcement learning which\nlimits their application to specific scenarios like gaming or code generation.\nThis paper introduces a novel training method for LLM-based agents using weakly\nsupervised signals from a critic LLM, bypassing the need for expert\ntrajectories or definitive feedback. Our agents are trained in iterative\nmanner, where they initially generate trajectories through environmental\ninteraction. Subsequently, a critic LLM selects a subset of good trajectories,\nwhich are then used to update the agents, enabling them to generate improved\ntrajectories in the next iteration. Extensive tests on the API-bank dataset\nshow consistent improvement in our agents' capabilities and comparable\nperformance to GPT-4, despite using open-source models with much fewer\nparameters."
                },
                "authors": [
                    {
                        "name": "Dihong Gong"
                    },
                    {
                        "name": "Pu Lu"
                    },
                    {
                        "name": "Zelong Wang"
                    },
                    {
                        "name": "Meng Zhou"
                    },
                    {
                        "name": "Xiuqiang He"
                    }
                ],
                "author_detail": {
                    "name": "Xiuqiang He"
                },
                "author": "Xiuqiang He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19547v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19547v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19542v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19542v1",
                "updated": "2024-11-29T08:39:24Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    8,
                    39,
                    24,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T08:39:24Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    8,
                    39,
                    24,
                    4,
                    334,
                    0
                ],
                "title": "A dynamic parallel method for performance optimization on hybrid CPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A dynamic parallel method for performance optimization on hybrid CPUs"
                },
                "summary": "The AIPC concept is gaining popularity, and more and more hybrid CPUs will be\nrunning AI models on client devices. However, the current AI inference\nframework overlooks the imbalanced hardware capability of hybrid CPUs, leading\nto low inference performance. To address this issue, we have introduced a\ndynamic parallel method for hybrid CPUs, which significantly increases LLM\ninference performance by balancing the workload for each core of a hybrid CPU\nbefore the parallel work starts. This method has enabled Neural Speed to\nachieve more than 90% (on average) of memory bandwidth on two hybrid Intel\nCPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The AIPC concept is gaining popularity, and more and more hybrid CPUs will be\nrunning AI models on client devices. However, the current AI inference\nframework overlooks the imbalanced hardware capability of hybrid CPUs, leading\nto low inference performance. To address this issue, we have introduced a\ndynamic parallel method for hybrid CPUs, which significantly increases LLM\ninference performance by balancing the workload for each core of a hybrid CPU\nbefore the parallel work starts. This method has enabled Neural Speed to\nachieve more than 90% (on average) of memory bandwidth on two hybrid Intel\nCPUs."
                },
                "authors": [
                    {
                        "name": "Luo Yu"
                    },
                    {
                        "name": "Liu Yucheng"
                    },
                    {
                        "name": "Shen Haihao"
                    }
                ],
                "author_detail": {
                    "name": "Shen Haihao"
                },
                "author": "Shen Haihao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19542v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19542v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19539v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19539v1",
                "updated": "2024-11-29T08:34:07Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    8,
                    34,
                    7,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T08:34:07Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    8,
                    34,
                    7,
                    4,
                    334,
                    0
                ],
                "title": "Knowledge Management for Automobile Failure Analysis Using Graph RAG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Management for Automobile Failure Analysis Using Graph RAG"
                },
                "summary": "This paper presents a knowledge management system for automobile failure\nanalysis using retrieval-augmented generation (RAG) with large language models\n(LLMs) and knowledge graphs (KGs). In the automotive industry, there is a\ngrowing demand for knowledge transfer of failure analysis from experienced\nengineers to young engineers. However, failure events are phenomena that occur\nin a chain reaction, making them difficult for beginners to analyze them. While\nknowledge graphs, which can describe semantic relationships and structure\ninformation is effective in representing failure events, due to their\ncapability of representing the relationships between components, there is much\ninformation in KGs, so it is challenging for young engineers to extract and\nunderstand sub-graphs from the KG. On the other hand, there is increasing\ninterest in the use of Graph RAG, a type of RAG that combines LLMs and KGs for\nknowledge management. However, when using the current Graph RAG framework with\nan existing knowledge graph for automobile failures, several issues arise\nbecause it is difficult to generate executable queries for a knowledge graph\ndatabase which is not constructed by LLMs. To address this, we focused on\noptimizing the Graph RAG pipeline for existing knowledge graphs. Using an\noriginal Q&A dataset, the ROUGE F1 score of the sentences generated by the\nproposed method showed an average improvement of 157.6% compared to the current\nmethod. This highlights the effectiveness of the proposed method for automobile\nfailure analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a knowledge management system for automobile failure\nanalysis using retrieval-augmented generation (RAG) with large language models\n(LLMs) and knowledge graphs (KGs). In the automotive industry, there is a\ngrowing demand for knowledge transfer of failure analysis from experienced\nengineers to young engineers. However, failure events are phenomena that occur\nin a chain reaction, making them difficult for beginners to analyze them. While\nknowledge graphs, which can describe semantic relationships and structure\ninformation is effective in representing failure events, due to their\ncapability of representing the relationships between components, there is much\ninformation in KGs, so it is challenging for young engineers to extract and\nunderstand sub-graphs from the KG. On the other hand, there is increasing\ninterest in the use of Graph RAG, a type of RAG that combines LLMs and KGs for\nknowledge management. However, when using the current Graph RAG framework with\nan existing knowledge graph for automobile failures, several issues arise\nbecause it is difficult to generate executable queries for a knowledge graph\ndatabase which is not constructed by LLMs. To address this, we focused on\noptimizing the Graph RAG pipeline for existing knowledge graphs. Using an\noriginal Q&A dataset, the ROUGE F1 score of the sentences generated by the\nproposed method showed an average improvement of 157.6% compared to the current\nmethod. This highlights the effectiveness of the proposed method for automobile\nfailure analysis."
                },
                "authors": [
                    {
                        "name": "Yuta Ojima"
                    },
                    {
                        "name": "Hiroki Sakaji"
                    },
                    {
                        "name": "Tadashi Nakamura"
                    },
                    {
                        "name": "Hiroaki Sakata"
                    },
                    {
                        "name": "Kazuya Seki"
                    },
                    {
                        "name": "Yuu Teshigawara"
                    },
                    {
                        "name": "Masami Yamashita"
                    },
                    {
                        "name": "Kazuhiro Aoyama"
                    }
                ],
                "author_detail": {
                    "name": "Kazuhiro Aoyama"
                },
                "author": "Kazuhiro Aoyama",
                "arxiv_comment": "7 pages, 6 figures, to be published in 2024 IEEE International\n  Conference on Bid Data (BigData)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19539v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19539v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17651v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17651v4",
                "updated": "2024-11-29T08:33:53Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    8,
                    33,
                    53,
                    4,
                    334,
                    0
                ],
                "published": "2024-06-25T15:43:20Z",
                "published_parsed": [
                    2024,
                    6,
                    25,
                    15,
                    43,
                    20,
                    1,
                    177,
                    0
                ],
                "title": "Software Model Evolution with Large Language Models: Experiments on\n  Simulated, Public, and Industrial Datasets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Software Model Evolution with Large Language Models: Experiments on\n  Simulated, Public, and Industrial Datasets"
                },
                "summary": "Modeling structure and behavior of software systems plays a crucial role in\nthe industrial practice of software engineering. As with other software\nengineering artifacts, software models are subject to evolution. Supporting\nmodelers in evolving software models with recommendations for model completions\nis still an open problem, though. In this paper, we explore the potential of\nlarge language models for this task. In particular, we propose an approach,\nRAMC, leveraging large language models, model histories, and\nretrieval-augmented generation for model completion. Through experiments on\nthree datasets, including an industrial application, one public open-source\ncommunity dataset, and one controlled collection of simulated model\nrepositories, we evaluate the potential of large language models for model\ncompletion with RAMC. We found that large language models are indeed a\npromising technology for supporting software model evolution (62.30%\nsemantically correct completions on real-world industrial data and up to 86.19%\ntype-correct completions). The general inference capabilities of large language\nmodels are particularly useful when dealing with concepts for which there are\nfew, noisy, or no examples at all.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling structure and behavior of software systems plays a crucial role in\nthe industrial practice of software engineering. As with other software\nengineering artifacts, software models are subject to evolution. Supporting\nmodelers in evolving software models with recommendations for model completions\nis still an open problem, though. In this paper, we explore the potential of\nlarge language models for this task. In particular, we propose an approach,\nRAMC, leveraging large language models, model histories, and\nretrieval-augmented generation for model completion. Through experiments on\nthree datasets, including an industrial application, one public open-source\ncommunity dataset, and one controlled collection of simulated model\nrepositories, we evaluate the potential of large language models for model\ncompletion with RAMC. We found that large language models are indeed a\npromising technology for supporting software model evolution (62.30%\nsemantically correct completions on real-world industrial data and up to 86.19%\ntype-correct completions). The general inference capabilities of large language\nmodels are particularly useful when dealing with concepts for which there are\nfew, noisy, or no examples at all."
                },
                "authors": [
                    {
                        "name": "Christof Tinnes"
                    },
                    {
                        "name": "Alisa Welter"
                    },
                    {
                        "name": "Sven Apel"
                    }
                ],
                "author_detail": {
                    "name": "Sven Apel"
                },
                "author": "Sven Apel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17651v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17651v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "94-04",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18191v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18191v2",
                "updated": "2024-11-29T08:33:49Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    8,
                    33,
                    49,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-27T10:14:38Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    10,
                    14,
                    38,
                    2,
                    332,
                    0
                ],
                "title": "InputSnatch: Stealing Input in LLM Services via Timing Side-Channel\n  Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InputSnatch: Stealing Input in LLM Services via Timing Side-Channel\n  Attacks"
                },
                "summary": "Large language models (LLMs) possess extensive knowledge and\nquestion-answering capabilities, having been widely deployed in\nprivacy-sensitive domains like finance and medical consultation. During LLM\ninferences, cache-sharing methods are commonly employed to enhance efficiency\nby reusing cached states or responses for the same or similar inference\nrequests. However, we identify that these cache mechanisms pose a risk of\nprivate input leakage, as the caching can result in observable variations in\nresponse times, making them a strong candidate for a timing-based attack hint.\n  In this study, we propose a novel timing-based side-channel attack to execute\ninput theft in LLMs inference. The cache-based attack faces the challenge of\nconstructing candidate inputs in a large search space to hit and steal cached\nuser queries. To address these challenges, we propose two primary components.\nThe input constructor employs machine learning techniques and LLM-based\napproaches for vocabulary correlation learning while implementing optimized\nsearch mechanisms for generalized input construction. The time analyzer\nimplements statistical time fitting with outlier elimination to identify cache\nhit patterns, continuously providing feedback to refine the constructor's\nsearch strategy. We conduct experiments across two cache mechanisms and the\nresults demonstrate that our approach consistently attains high attack success\nrates in various applications. Our work highlights the security vulnerabilities\nassociated with performance optimizations, underscoring the necessity of\nprioritizing privacy and security alongside enhancements in LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) possess extensive knowledge and\nquestion-answering capabilities, having been widely deployed in\nprivacy-sensitive domains like finance and medical consultation. During LLM\ninferences, cache-sharing methods are commonly employed to enhance efficiency\nby reusing cached states or responses for the same or similar inference\nrequests. However, we identify that these cache mechanisms pose a risk of\nprivate input leakage, as the caching can result in observable variations in\nresponse times, making them a strong candidate for a timing-based attack hint.\n  In this study, we propose a novel timing-based side-channel attack to execute\ninput theft in LLMs inference. The cache-based attack faces the challenge of\nconstructing candidate inputs in a large search space to hit and steal cached\nuser queries. To address these challenges, we propose two primary components.\nThe input constructor employs machine learning techniques and LLM-based\napproaches for vocabulary correlation learning while implementing optimized\nsearch mechanisms for generalized input construction. The time analyzer\nimplements statistical time fitting with outlier elimination to identify cache\nhit patterns, continuously providing feedback to refine the constructor's\nsearch strategy. We conduct experiments across two cache mechanisms and the\nresults demonstrate that our approach consistently attains high attack success\nrates in various applications. Our work highlights the security vulnerabilities\nassociated with performance optimizations, underscoring the necessity of\nprioritizing privacy and security alongside enhancements in LLM inference."
                },
                "authors": [
                    {
                        "name": "Xinyao Zheng"
                    },
                    {
                        "name": "Husheng Han"
                    },
                    {
                        "name": "Shangyi Shi"
                    },
                    {
                        "name": "Qiyan Fang"
                    },
                    {
                        "name": "Zidong Du"
                    },
                    {
                        "name": "Xing Hu"
                    },
                    {
                        "name": "Qi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Qi Guo"
                },
                "author": "Qi Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18191v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18191v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10666v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10666v2",
                "updated": "2024-11-29T08:16:29Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    8,
                    16,
                    29,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-16T02:02:49Z",
                "published_parsed": [
                    2024,
                    11,
                    16,
                    2,
                    2,
                    49,
                    5,
                    321,
                    0
                ],
                "title": "SAM Decoding: Speculative Decoding via Suffix Automaton",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SAM Decoding: Speculative Decoding via Suffix Automaton"
                },
                "summary": "Large Language Models (LLMs) have revolutionized natural language processing\nby unifying tasks into text generation, yet their large parameter sizes and\nautoregressive nature limit inference speed. SAM-Decoding addresses this by\nintroducing a novel retrieval-based speculative decoding method that uses a\nsuffix automaton for efficient and accurate draft generation. Unlike n-gram\nmatching used by the existing method, SAM-Decoding finds the longest suffix\nmatch in generating text and text corpuss, achieving an average time complexity\nof $O(1)$ per generation step. SAM-Decoding constructs static and dynamic\nsuffix automatons for the text corpus and input prompts, respectively, enabling\nfast and precise draft generation. Meanwhile, it is designed as an approach\nthat can be combined with existing methods, allowing SAM-Decoding to adaptively\nselect a draft generation strategy based on the matching length, thus\nincreasing the inference speed of the LLM. When combined with Token Recycling,\nevaluations show SAM-Decoding outperforms existing model-free methods,\nachieving a speedup of $2.27\\times$ over autoregressive decoding on Spec-Bench.\nWhen combined with EAGLE2, it reaches a speedup of $2.49\\times$, surpassing all\ncurrent approaches. Our code is available at\nhttps://github.com/hyx1999/SAM-Decoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized natural language processing\nby unifying tasks into text generation, yet their large parameter sizes and\nautoregressive nature limit inference speed. SAM-Decoding addresses this by\nintroducing a novel retrieval-based speculative decoding method that uses a\nsuffix automaton for efficient and accurate draft generation. Unlike n-gram\nmatching used by the existing method, SAM-Decoding finds the longest suffix\nmatch in generating text and text corpuss, achieving an average time complexity\nof $O(1)$ per generation step. SAM-Decoding constructs static and dynamic\nsuffix automatons for the text corpus and input prompts, respectively, enabling\nfast and precise draft generation. Meanwhile, it is designed as an approach\nthat can be combined with existing methods, allowing SAM-Decoding to adaptively\nselect a draft generation strategy based on the matching length, thus\nincreasing the inference speed of the LLM. When combined with Token Recycling,\nevaluations show SAM-Decoding outperforms existing model-free methods,\nachieving a speedup of $2.27\\times$ over autoregressive decoding on Spec-Bench.\nWhen combined with EAGLE2, it reaches a speedup of $2.49\\times$, surpassing all\ncurrent approaches. Our code is available at\nhttps://github.com/hyx1999/SAM-Decoding."
                },
                "authors": [
                    {
                        "name": "Yuxuan Hu"
                    },
                    {
                        "name": "Ke Wang"
                    },
                    {
                        "name": "Xiaokang Zhang"
                    },
                    {
                        "name": "Fanjin Zhang"
                    },
                    {
                        "name": "Cuiping Li"
                    },
                    {
                        "name": "Hong Chen"
                    },
                    {
                        "name": "Jing Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jing Zhang"
                },
                "author": "Jing Zhang",
                "arxiv_comment": "17 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10666v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10666v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19528v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19528v1",
                "updated": "2024-11-29T07:57:32Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    7,
                    57,
                    32,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T07:57:32Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    7,
                    57,
                    32,
                    4,
                    334,
                    0
                ],
                "title": "RAGDiffusion: Faithful Cloth Generation via External Knowledge\n  Assimilation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAGDiffusion: Faithful Cloth Generation via External Knowledge\n  Assimilation"
                },
                "summary": "Standard clothing asset generation involves creating forward-facing flat-lay\ngarment images displayed on a clear background by extracting clothing\ninformation from diverse real-world contexts, which presents significant\nchallenges due to highly standardized sampling distributions and precise\nstructural requirements in the generated images. Existing models have limited\nspatial perception and often exhibit structural hallucinations in this\nhigh-specification generative task. To address this issue, we propose a novel\nRetrieval-Augmented Generation (RAG) framework, termed RAGDiffusion, to enhance\nstructure determinacy and mitigate hallucinations by assimilating external\nknowledge from LLM and databases. RAGDiffusion consists of two core processes:\n(1) Retrieval-based structure aggregation, which employs contrastive learning\nand a Structure Locally Linear Embedding (SLLE) to derive global structure and\nspatial landmarks, providing both soft and hard guidance to counteract\nstructural ambiguities; and (2) Omni-level faithful garment generation, which\nintroduces a three-level alignment that ensures fidelity in structural,\npattern, and decoding components within the diffusing. Extensive experiments on\nchallenging real-world datasets demonstrate that RAGDiffusion synthesizes\nstructurally and detail-faithful clothing assets with significant performance\nimprovements, representing a pioneering effort in high-specification faithful\ngeneration with RAG to confront intrinsic hallucinations and enhance fidelity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Standard clothing asset generation involves creating forward-facing flat-lay\ngarment images displayed on a clear background by extracting clothing\ninformation from diverse real-world contexts, which presents significant\nchallenges due to highly standardized sampling distributions and precise\nstructural requirements in the generated images. Existing models have limited\nspatial perception and often exhibit structural hallucinations in this\nhigh-specification generative task. To address this issue, we propose a novel\nRetrieval-Augmented Generation (RAG) framework, termed RAGDiffusion, to enhance\nstructure determinacy and mitigate hallucinations by assimilating external\nknowledge from LLM and databases. RAGDiffusion consists of two core processes:\n(1) Retrieval-based structure aggregation, which employs contrastive learning\nand a Structure Locally Linear Embedding (SLLE) to derive global structure and\nspatial landmarks, providing both soft and hard guidance to counteract\nstructural ambiguities; and (2) Omni-level faithful garment generation, which\nintroduces a three-level alignment that ensures fidelity in structural,\npattern, and decoding components within the diffusing. Extensive experiments on\nchallenging real-world datasets demonstrate that RAGDiffusion synthesizes\nstructurally and detail-faithful clothing assets with significant performance\nimprovements, representing a pioneering effort in high-specification faithful\ngeneration with RAG to confront intrinsic hallucinations and enhance fidelity."
                },
                "authors": [
                    {
                        "name": "Xianfeng Tan"
                    },
                    {
                        "name": "Yuhan Li"
                    },
                    {
                        "name": "Wenxiang Shang"
                    },
                    {
                        "name": "Yubo Wu"
                    },
                    {
                        "name": "Jian Wang"
                    },
                    {
                        "name": "Xuanhong Chen"
                    },
                    {
                        "name": "Yi Zhang"
                    },
                    {
                        "name": "Ran Lin"
                    },
                    {
                        "name": "Bingbing Ni"
                    }
                ],
                "author_detail": {
                    "name": "Bingbing Ni"
                },
                "author": "Bingbing Ni",
                "arxiv_comment": "Project website: https://colorful-liyu.github.io/RAGDiffusion-page/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19528v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19528v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2212.08701v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2212.08701v3",
                "updated": "2024-11-29T07:21:45Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    7,
                    21,
                    45,
                    4,
                    334,
                    0
                ],
                "published": "2022-12-16T20:02:03Z",
                "published_parsed": [
                    2022,
                    12,
                    16,
                    20,
                    2,
                    3,
                    4,
                    350,
                    0
                ],
                "title": "An Upper Bound for the Distribution Overlap Index and Its Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Upper Bound for the Distribution Overlap Index and Its Applications"
                },
                "summary": "This paper proposes an easy-to-compute upper bound for the overlap index\nbetween two probability distributions without requiring any knowledge of the\ndistribution models. The computation of our bound is time-efficient and\nmemory-efficient and only requires finite samples. The proposed bound shows its\nvalue in one-class classification and domain shift analysis. Specifically, in\none-class classification, we build a novel one-class classifier by converting\nthe bound into a confidence score function. Unlike most one-class classifiers,\nthe training process is not needed for our classifier. Additionally, the\nexperimental results show that our classifier can be accurate with only a small\nnumber of in-class samples and outperform many state-of-the-art methods on\nvarious datasets in different one-class classification scenarios. In domain\nshift analysis, we propose a theorem based on our bound. The theorem is useful\nin detecting the existence of domain shift and inferring data information. The\ndetection and inference processes are both computation-efficient and\nmemory-efficient. Our work shows significant promise toward broadening the\napplications of overlap-based metrics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes an easy-to-compute upper bound for the overlap index\nbetween two probability distributions without requiring any knowledge of the\ndistribution models. The computation of our bound is time-efficient and\nmemory-efficient and only requires finite samples. The proposed bound shows its\nvalue in one-class classification and domain shift analysis. Specifically, in\none-class classification, we build a novel one-class classifier by converting\nthe bound into a confidence score function. Unlike most one-class classifiers,\nthe training process is not needed for our classifier. Additionally, the\nexperimental results show that our classifier can be accurate with only a small\nnumber of in-class samples and outperform many state-of-the-art methods on\nvarious datasets in different one-class classification scenarios. In domain\nshift analysis, we propose a theorem based on our bound. The theorem is useful\nin detecting the existence of domain shift and inferring data information. The\ndetection and inference processes are both computation-efficient and\nmemory-efficient. Our work shows significant promise toward broadening the\napplications of overlap-based metrics."
                },
                "authors": [
                    {
                        "name": "Hao Fu"
                    },
                    {
                        "name": "Prashanth Krishnamurthy"
                    },
                    {
                        "name": "Siddharth Garg"
                    },
                    {
                        "name": "Farshad Khorrami"
                    }
                ],
                "author_detail": {
                    "name": "Farshad Khorrami"
                },
                "author": "Farshad Khorrami",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2212.08701v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2212.08701v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19515v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19515v1",
                "updated": "2024-11-29T07:18:50Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    7,
                    18,
                    50,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T07:18:50Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    7,
                    18,
                    50,
                    4,
                    334,
                    0
                ],
                "title": "Leveraging Large Language Models for Institutional Portfolio Management:\n  Persona-Based Ensembles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Large Language Models for Institutional Portfolio Management:\n  Persona-Based Ensembles"
                },
                "summary": "Large language models (LLMs) have demonstrated promising performance in\nvarious financial applications, though their potential in complex investment\nstrategies remains underexplored. To address this gap, we investigate how LLMs\ncan predict price movements in stock and bond portfolios using economic\nindicators, enabling portfolio adjustments akin to those employed by\ninstitutional investors. Additionally, we explore the impact of incorporating\ndifferent personas within LLMs, using an ensemble approach to leverage their\ndiverse predictions. Our findings show that LLM-based strategies, especially\nwhen combined with the mode ensemble, outperform the buy-and-hold strategy in\nterms of Sharpe ratio during periods of rising consumer price index (CPI).\nHowever, traditional strategies are more effective during declining CPI trends\nor sharp market downturns. These results suggest that while LLMs can enhance\nportfolio management, they may require complementary strategies to optimize\nperformance across varying market conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated promising performance in\nvarious financial applications, though their potential in complex investment\nstrategies remains underexplored. To address this gap, we investigate how LLMs\ncan predict price movements in stock and bond portfolios using economic\nindicators, enabling portfolio adjustments akin to those employed by\ninstitutional investors. Additionally, we explore the impact of incorporating\ndifferent personas within LLMs, using an ensemble approach to leverage their\ndiverse predictions. Our findings show that LLM-based strategies, especially\nwhen combined with the mode ensemble, outperform the buy-and-hold strategy in\nterms of Sharpe ratio during periods of rising consumer price index (CPI).\nHowever, traditional strategies are more effective during declining CPI trends\nor sharp market downturns. These results suggest that while LLMs can enhance\nportfolio management, they may require complementary strategies to optimize\nperformance across varying market conditions."
                },
                "authors": [
                    {
                        "name": "Yoshia Abe"
                    },
                    {
                        "name": "Shuhei Matsuo"
                    },
                    {
                        "name": "Ryoma Kondo"
                    },
                    {
                        "name": "Ryohei Hisano"
                    }
                ],
                "author_detail": {
                    "name": "Ryohei Hisano"
                },
                "author": "Ryohei Hisano",
                "arxiv_comment": "10 pages, 5 figures, submitted to The IEEE International Workshop on\n  Large Language Models for Finance 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19515v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19515v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19509v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19509v1",
                "updated": "2024-11-29T07:01:31Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    7,
                    1,
                    31,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T07:01:31Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    7,
                    1,
                    31,
                    4,
                    334,
                    0
                ],
                "title": "Ditto: Motion-Space Diffusion for Controllable Realtime Talking Head\n  Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ditto: Motion-Space Diffusion for Controllable Realtime Talking Head\n  Synthesis"
                },
                "summary": "Recent advances in diffusion models have revolutionized audio-driven talking\nhead synthesis. Beyond precise lip synchronization, diffusion-based methods\nexcel in generating subtle expressions and natural head movements that are\nwell-aligned with the audio signal. However, these methods are confronted by\nslow inference speed, insufficient fine-grained control over facial motions,\nand occasional visual artifacts largely due to an implicit latent space derived\nfrom Variational Auto-Encoders (VAE), which prevent their adoption in realtime\ninteraction applications. To address these issues, we introduce Ditto, a\ndiffusion-based framework that enables controllable realtime talking head\nsynthesis. Our key innovation lies in bridging motion generation and\nphotorealistic neural rendering through an explicit identity-agnostic motion\nspace, replacing conventional VAE representations. This design substantially\nreduces the complexity of diffusion learning while enabling precise control\nover the synthesized talking heads. We further propose an inference strategy\nthat jointly optimizes three key components: audio feature extraction, motion\ngeneration, and video synthesis. This optimization enables streaming\nprocessing, realtime inference, and low first-frame delay, which are the\nfunctionalities crucial for interactive applications such as AI assistants.\nExtensive experimental results demonstrate that Ditto generates compelling\ntalking head videos and substantially outperforms existing methods in both\nmotion control and realtime performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in diffusion models have revolutionized audio-driven talking\nhead synthesis. Beyond precise lip synchronization, diffusion-based methods\nexcel in generating subtle expressions and natural head movements that are\nwell-aligned with the audio signal. However, these methods are confronted by\nslow inference speed, insufficient fine-grained control over facial motions,\nand occasional visual artifacts largely due to an implicit latent space derived\nfrom Variational Auto-Encoders (VAE), which prevent their adoption in realtime\ninteraction applications. To address these issues, we introduce Ditto, a\ndiffusion-based framework that enables controllable realtime talking head\nsynthesis. Our key innovation lies in bridging motion generation and\nphotorealistic neural rendering through an explicit identity-agnostic motion\nspace, replacing conventional VAE representations. This design substantially\nreduces the complexity of diffusion learning while enabling precise control\nover the synthesized talking heads. We further propose an inference strategy\nthat jointly optimizes three key components: audio feature extraction, motion\ngeneration, and video synthesis. This optimization enables streaming\nprocessing, realtime inference, and low first-frame delay, which are the\nfunctionalities crucial for interactive applications such as AI assistants.\nExtensive experimental results demonstrate that Ditto generates compelling\ntalking head videos and substantially outperforms existing methods in both\nmotion control and realtime performance."
                },
                "authors": [
                    {
                        "name": "Tianqi Li"
                    },
                    {
                        "name": "Ruobing Zheng"
                    },
                    {
                        "name": "Minghui Yang"
                    },
                    {
                        "name": "Jingdong Chen"
                    },
                    {
                        "name": "Ming Yang"
                    }
                ],
                "author_detail": {
                    "name": "Ming Yang"
                },
                "author": "Ming Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19509v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19509v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19508v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19508v1",
                "updated": "2024-11-29T07:00:47Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    7,
                    0,
                    47,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T07:00:47Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    7,
                    0,
                    47,
                    4,
                    334,
                    0
                ],
                "title": "On the Adversarial Robustness of Instruction-Tuned Large Language Models\n  for Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Adversarial Robustness of Instruction-Tuned Large Language Models\n  for Code"
                },
                "summary": "The advent of instruction-tuned Large Language Models designed for coding\ntasks (Code LLMs) has transformed software engineering practices. However,\ntheir robustness against various input challenges remains a critical concern.\nThis study introduces DegradePrompter, a novel method designed to\nsystematically evaluate the robustness of instruction-tuned Code LLMs. We\nassess the impact of diverse input challenges on the functionality and\ncorrectness of generated code using rigorous metrics and established\nbenchmarks. Our comprehensive evaluation includes five state-of-the-art\nopen-source models and three production-grade closed-source models, revealing\nvarying degrees of robustness. Open-source models demonstrate an increased\nsusceptibility to input perturbations, resulting in declines in functional\ncorrectness ranging from 12% to 34%. In contrast, commercial models demonstrate\nrelatively greater resilience, with performance degradation ranging from 3% to\n24%. To enhance the robustness of the models against these vulnerabilities, we\ninvestigate a straightforward yet effective mitigation strategy. Our findings\nhighlight the need for robust defense mechanisms and comprehensive evaluations\nduring both the development and deployment phases to ensure the resilience and\nreliability of automated code generation systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advent of instruction-tuned Large Language Models designed for coding\ntasks (Code LLMs) has transformed software engineering practices. However,\ntheir robustness against various input challenges remains a critical concern.\nThis study introduces DegradePrompter, a novel method designed to\nsystematically evaluate the robustness of instruction-tuned Code LLMs. We\nassess the impact of diverse input challenges on the functionality and\ncorrectness of generated code using rigorous metrics and established\nbenchmarks. Our comprehensive evaluation includes five state-of-the-art\nopen-source models and three production-grade closed-source models, revealing\nvarying degrees of robustness. Open-source models demonstrate an increased\nsusceptibility to input perturbations, resulting in declines in functional\ncorrectness ranging from 12% to 34%. In contrast, commercial models demonstrate\nrelatively greater resilience, with performance degradation ranging from 3% to\n24%. To enhance the robustness of the models against these vulnerabilities, we\ninvestigate a straightforward yet effective mitigation strategy. Our findings\nhighlight the need for robust defense mechanisms and comprehensive evaluations\nduring both the development and deployment phases to ensure the resilience and\nreliability of automated code generation systems."
                },
                "authors": [
                    {
                        "name": "Md Imran Hossen"
                    },
                    {
                        "name": "Xiali Hei"
                    }
                ],
                "author_detail": {
                    "name": "Xiali Hei"
                },
                "author": "Xiali Hei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19508v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19508v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19504v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19504v1",
                "updated": "2024-11-29T06:48:13Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    6,
                    48,
                    13,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T06:48:13Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    6,
                    48,
                    13,
                    4,
                    334,
                    0
                ],
                "title": "TQA-Bench: Evaluating LLMs for Multi-Table Question Answering with\n  Scalable Context and Symbolic Extension",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TQA-Bench: Evaluating LLMs for Multi-Table Question Answering with\n  Scalable Context and Symbolic Extension"
                },
                "summary": "The advent of large language models (LLMs) has unlocked great opportunities\nin complex data management tasks, particularly in question answering (QA) over\ncomplicated multi-table relational data. Despite significant progress,\nsystematically evaluating LLMs on multi-table QA remains a critical challenge\ndue to the inherent complexity of analyzing heterogeneous table structures and\npotential large scale of serialized relational data. Existing benchmarks\nprimarily focus on single-table QA, failing to capture the intricacies of\nreasoning across multiple relational tables, as required in real-world domains\nsuch as finance, healthcare, and e-commerce. To address this gap, we present\nTQA-Bench, a new multi-table QA benchmark designed to evaluate the capabilities\nof LLMs in tackling complex QA tasks over relational data. Our benchmark\nincorporates diverse relational database instances sourced from real-world\npublic datasets and introduces a flexible sampling mechanism to create tasks\nwith varying multi-table context lengths, ranging from 8K to 64K tokens. To\nensure robustness and reliability, we integrate symbolic extensions into the\nevaluation framework, enabling the assessment of LLM reasoning capabilities\nbeyond simple data retrieval or probabilistic pattern matching. We\nsystematically evaluate a range of LLMs, both open-source and closed-source,\nspanning model scales from 7 billion to 70 billion parameters. Our extensive\nexperiments reveal critical insights into the performance of LLMs in\nmulti-table QA, highlighting both challenges and opportunities for advancing\ntheir application in complex, data-driven environments. Our benchmark\nimplementation and results are available at\nhttps://github.com/Relaxed-System-Lab/TQA-Bench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advent of large language models (LLMs) has unlocked great opportunities\nin complex data management tasks, particularly in question answering (QA) over\ncomplicated multi-table relational data. Despite significant progress,\nsystematically evaluating LLMs on multi-table QA remains a critical challenge\ndue to the inherent complexity of analyzing heterogeneous table structures and\npotential large scale of serialized relational data. Existing benchmarks\nprimarily focus on single-table QA, failing to capture the intricacies of\nreasoning across multiple relational tables, as required in real-world domains\nsuch as finance, healthcare, and e-commerce. To address this gap, we present\nTQA-Bench, a new multi-table QA benchmark designed to evaluate the capabilities\nof LLMs in tackling complex QA tasks over relational data. Our benchmark\nincorporates diverse relational database instances sourced from real-world\npublic datasets and introduces a flexible sampling mechanism to create tasks\nwith varying multi-table context lengths, ranging from 8K to 64K tokens. To\nensure robustness and reliability, we integrate symbolic extensions into the\nevaluation framework, enabling the assessment of LLM reasoning capabilities\nbeyond simple data retrieval or probabilistic pattern matching. We\nsystematically evaluate a range of LLMs, both open-source and closed-source,\nspanning model scales from 7 billion to 70 billion parameters. Our extensive\nexperiments reveal critical insights into the performance of LLMs in\nmulti-table QA, highlighting both challenges and opportunities for advancing\ntheir application in complex, data-driven environments. Our benchmark\nimplementation and results are available at\nhttps://github.com/Relaxed-System-Lab/TQA-Bench."
                },
                "authors": [
                    {
                        "name": "Zipeng Qiu"
                    },
                    {
                        "name": "You Peng"
                    },
                    {
                        "name": "Guangxin He"
                    },
                    {
                        "name": "Binhang Yuan"
                    },
                    {
                        "name": "Chen Wang"
                    }
                ],
                "author_detail": {
                    "name": "Chen Wang"
                },
                "author": "Chen Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19504v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19504v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19500v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19500v1",
                "updated": "2024-11-29T06:37:13Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    6,
                    37,
                    13,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T06:37:13Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    6,
                    37,
                    13,
                    4,
                    334,
                    0
                ],
                "title": "COLD: Causal reasOning in cLosed Daily activities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "COLD: Causal reasOning in cLosed Daily activities"
                },
                "summary": "Large Language Models (LLMs) have shown state-of-the-art performance in a\nvariety of tasks, including arithmetic and reasoning; however, to gauge the\nintellectual capabilities of LLMs, causal reasoning has become a reliable proxy\nfor validating a general understanding of the mechanics and intricacies of the\nworld similar to humans. Previous works in natural language processing (NLP)\nhave either focused on open-ended causal reasoning via causal commonsense\nreasoning (CCR) or framed a symbolic representation-based question answering\nfor theoretically backed-up analysis via a causal inference engine. The former\nadds an advantage of real-world grounding but lacks theoretically backed-up\nanalysis/validation, whereas the latter is far from real-world grounding. In\nthis work, we bridge this gap by proposing the COLD (Causal reasOning in cLosed\nDaily activities) framework, which is built upon human understanding of daily\nreal-world activities to reason about the causal nature of events. We show that\nthe proposed framework facilitates the creation of enormous causal queries (~ 9\nmillion) and comes close to the mini-turing test, simulating causal reasoning\nto evaluate the understanding of a daily real-world task. We evaluate multiple\nLLMs on the created causal queries and find that causal reasoning is\nchallenging even for activities trivial to humans. We further explore (the\ncausal reasoning abilities of LLMs) using the backdoor criterion to determine\nthe causal strength between events.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown state-of-the-art performance in a\nvariety of tasks, including arithmetic and reasoning; however, to gauge the\nintellectual capabilities of LLMs, causal reasoning has become a reliable proxy\nfor validating a general understanding of the mechanics and intricacies of the\nworld similar to humans. Previous works in natural language processing (NLP)\nhave either focused on open-ended causal reasoning via causal commonsense\nreasoning (CCR) or framed a symbolic representation-based question answering\nfor theoretically backed-up analysis via a causal inference engine. The former\nadds an advantage of real-world grounding but lacks theoretically backed-up\nanalysis/validation, whereas the latter is far from real-world grounding. In\nthis work, we bridge this gap by proposing the COLD (Causal reasOning in cLosed\nDaily activities) framework, which is built upon human understanding of daily\nreal-world activities to reason about the causal nature of events. We show that\nthe proposed framework facilitates the creation of enormous causal queries (~ 9\nmillion) and comes close to the mini-turing test, simulating causal reasoning\nto evaluate the understanding of a daily real-world task. We evaluate multiple\nLLMs on the created causal queries and find that causal reasoning is\nchallenging even for activities trivial to humans. We further explore (the\ncausal reasoning abilities of LLMs) using the backdoor criterion to determine\nthe causal strength between events."
                },
                "authors": [
                    {
                        "name": "Abhinav Joshi"
                    },
                    {
                        "name": "Areeb Ahmad"
                    },
                    {
                        "name": "Ashutosh Modi"
                    }
                ],
                "author_detail": {
                    "name": "Ashutosh Modi"
                },
                "author": "Ashutosh Modi",
                "arxiv_comment": "Paper accepted at NeurIPS 2024; Total 37 Pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19500v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19500v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19498v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19498v1",
                "updated": "2024-11-29T06:33:31Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    6,
                    33,
                    31,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T06:33:31Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    6,
                    33,
                    31,
                    4,
                    334,
                    0
                ],
                "title": "Protecting Multiple Types of Privacy Simultaneously in EEG-based\n  Brain-Computer Interfaces",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Protecting Multiple Types of Privacy Simultaneously in EEG-based\n  Brain-Computer Interfaces"
                },
                "summary": "A brain-computer interface (BCI) enables direct communication between the\nbrain and an external device. Electroencephalogram (EEG) is the preferred input\nsignal in non-invasive BCIs, due to its convenience and low cost. EEG-based\nBCIs have been successfully used in many applications, such as neurological\nrehabilitation, text input, games, and so on. However, EEG signals inherently\ncarry rich personal information, necessitating privacy protection. This paper\ndemonstrates that multiple types of private information (user identity, gender,\nand BCI-experience) can be easily inferred from EEG data, imposing a serious\nprivacy threat to BCIs. To address this issue, we design perturbations to\nconvert the original EEG data into privacy-protected EEG data, which conceal\nthe private information while maintaining the primary BCI task performance.\nExperimental results demonstrated that the privacy-protected EEG data can\nsignificantly reduce the classification accuracy of user identity, gender and\nBCI-experience, but almost do not affect at all the classification accuracy of\nthe primary BCI task, enabling user privacy protection in EEG-based BCIs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A brain-computer interface (BCI) enables direct communication between the\nbrain and an external device. Electroencephalogram (EEG) is the preferred input\nsignal in non-invasive BCIs, due to its convenience and low cost. EEG-based\nBCIs have been successfully used in many applications, such as neurological\nrehabilitation, text input, games, and so on. However, EEG signals inherently\ncarry rich personal information, necessitating privacy protection. This paper\ndemonstrates that multiple types of private information (user identity, gender,\nand BCI-experience) can be easily inferred from EEG data, imposing a serious\nprivacy threat to BCIs. To address this issue, we design perturbations to\nconvert the original EEG data into privacy-protected EEG data, which conceal\nthe private information while maintaining the primary BCI task performance.\nExperimental results demonstrated that the privacy-protected EEG data can\nsignificantly reduce the classification accuracy of user identity, gender and\nBCI-experience, but almost do not affect at all the classification accuracy of\nthe primary BCI task, enabling user privacy protection in EEG-based BCIs."
                },
                "authors": [
                    {
                        "name": "Lubin Meng"
                    },
                    {
                        "name": "Xue Jiang"
                    },
                    {
                        "name": "Tianwang Jia"
                    },
                    {
                        "name": "Dongrui Wu"
                    }
                ],
                "author_detail": {
                    "name": "Dongrui Wu"
                },
                "author": "Dongrui Wu",
                "arxiv_journal_ref": "IEEE Int'l Conf. on Systems, Man and Cybernetics, Sarawak,\n  Malaysia, October 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19498v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19498v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21670v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21670v5",
                "updated": "2024-11-29T06:16:25Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    6,
                    16,
                    25,
                    4,
                    334,
                    0
                ],
                "published": "2024-07-31T15:13:39Z",
                "published_parsed": [
                    2024,
                    7,
                    31,
                    15,
                    13,
                    39,
                    2,
                    213,
                    0
                ],
                "title": "Dynamic Universal Approximation Theory: Foundations for Parallelism in\n  Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Universal Approximation Theory: Foundations for Parallelism in\n  Neural Networks"
                },
                "summary": "Neural networks are increasingly evolving towards training large models with\nbig data, a method that has demonstrated superior performance across many\ntasks. However, this approach introduces an urgent problem: current deep\nlearning models are predominantly serial, meaning that as the number of network\nlayers increases, so do the training and inference times. This is unacceptable\nif deep learning is to continue advancing. Therefore, this paper proposes a\ndeep learning parallelization strategy based on the Universal Approximation\nTheorem (UAT). From this foundation, we designed a parallel network called\nPara-Former to test our theory. Unlike traditional serial models, the inference\ntime of Para-Former does not increase with the number of layers, significantly\naccelerating the inference speed of multi-layer networks. Experimental results\nvalidate the effectiveness of this network.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural networks are increasingly evolving towards training large models with\nbig data, a method that has demonstrated superior performance across many\ntasks. However, this approach introduces an urgent problem: current deep\nlearning models are predominantly serial, meaning that as the number of network\nlayers increases, so do the training and inference times. This is unacceptable\nif deep learning is to continue advancing. Therefore, this paper proposes a\ndeep learning parallelization strategy based on the Universal Approximation\nTheorem (UAT). From this foundation, we designed a parallel network called\nPara-Former to test our theory. Unlike traditional serial models, the inference\ntime of Para-Former does not increase with the number of layers, significantly\naccelerating the inference speed of multi-layer networks. Experimental results\nvalidate the effectiveness of this network."
                },
                "authors": [
                    {
                        "name": "Wei Wang"
                    },
                    {
                        "name": "Qing Li"
                    }
                ],
                "author_detail": {
                    "name": "Qing Li"
                },
                "author": "Qing Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21670v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21670v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19488v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19488v1",
                "updated": "2024-11-29T06:06:35Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    6,
                    6,
                    35,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T06:06:35Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    6,
                    6,
                    35,
                    4,
                    334,
                    0
                ],
                "title": "Interleaved-Modal Chain-of-Thought",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interleaved-Modal Chain-of-Thought"
                },
                "summary": "Chain-of-Thought (CoT) prompting elicits large language models (LLMs) to\nproduce a series of intermediate reasoning steps before arriving at the final\nanswer. However, when transitioning to vision-language models (VLMs), their\ntext-only rationales struggle to express the fine-grained associations with the\noriginal image. In this paper, we propose an image-incorporated multimodal\nChain-of-Thought, named \\textbf{Interleaved-modal Chain-of-Thought (ICoT)},\nwhich generates sequential reasoning steps consisting of paired visual and\ntextual rationales to infer the final answer. Intuitively, the novel ICoT\nrequires VLMs to enable the generation of fine-grained interleaved-modal\ncontent, which is hard for current VLMs to fulfill. Considering that the\nrequired visual information is usually part of the input image, we propose\n\\textbf{Attention-driven Selection (ADS)} to realize ICoT over existing VLMs.\nADS intelligently inserts regions of the input image to generate the\ninterleaved-modal reasoning steps with ignorable additional latency. ADS relies\nsolely on the attention map of VLMs without the need for parameterization, and\ntherefore it is a plug-and-play strategy that can be generalized to a spectrum\nof VLMs. We apply ADS to realize ICoT on two popular VLMs of different\narchitectures. Extensive evaluations of three benchmarks have shown that ICoT\nprompting achieves substantial performance (up to 14\\%) and interpretability\nimprovements compared to existing multimodal CoT prompting methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-Thought (CoT) prompting elicits large language models (LLMs) to\nproduce a series of intermediate reasoning steps before arriving at the final\nanswer. However, when transitioning to vision-language models (VLMs), their\ntext-only rationales struggle to express the fine-grained associations with the\noriginal image. In this paper, we propose an image-incorporated multimodal\nChain-of-Thought, named \\textbf{Interleaved-modal Chain-of-Thought (ICoT)},\nwhich generates sequential reasoning steps consisting of paired visual and\ntextual rationales to infer the final answer. Intuitively, the novel ICoT\nrequires VLMs to enable the generation of fine-grained interleaved-modal\ncontent, which is hard for current VLMs to fulfill. Considering that the\nrequired visual information is usually part of the input image, we propose\n\\textbf{Attention-driven Selection (ADS)} to realize ICoT over existing VLMs.\nADS intelligently inserts regions of the input image to generate the\ninterleaved-modal reasoning steps with ignorable additional latency. ADS relies\nsolely on the attention map of VLMs without the need for parameterization, and\ntherefore it is a plug-and-play strategy that can be generalized to a spectrum\nof VLMs. We apply ADS to realize ICoT on two popular VLMs of different\narchitectures. Extensive evaluations of three benchmarks have shown that ICoT\nprompting achieves substantial performance (up to 14\\%) and interpretability\nimprovements compared to existing multimodal CoT prompting methods."
                },
                "authors": [
                    {
                        "name": "Jun Gao"
                    },
                    {
                        "name": "Yongqi Li"
                    },
                    {
                        "name": "Ziqiang Cao"
                    },
                    {
                        "name": "Wenjie Li"
                    }
                ],
                "author_detail": {
                    "name": "Wenjie Li"
                },
                "author": "Wenjie Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19488v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19488v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19487v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19487v1",
                "updated": "2024-11-29T06:04:08Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    6,
                    4,
                    8,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T06:04:08Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    6,
                    4,
                    8,
                    4,
                    334,
                    0
                ],
                "title": "HE2C: A Holistic Approach for Allocating Latency-Sensitive AI Tasks\n  across Edge-Cloud",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HE2C: A Holistic Approach for Allocating Latency-Sensitive AI Tasks\n  across Edge-Cloud"
                },
                "summary": "The high computational, memory, and energy demands of Deep Learning (DL)\napplications often exceed the capabilities of battery-powered edge devices,\ncreating difficulties in meeting task deadlines and accuracy requirements.\nUnlike previous solutions that optimize a single metric (e.g., accuracy or\nenergy efficiency), HE2C framework is designed to holistically address the\nlatency, memory, accuracy, throughput, and energy demands of DL applications\nacross edge-cloud continuum, thereby, delivering a more comprehensive and\neffective user experience. HE2C comprises three key modules: (a) a\n\"feasibility-check module that evaluates the likelihood of meeting deadlines\nacross both edge and cloud resources; (b) a \"resource allocation strategy\" that\nmaximizes energy efficiency without sacrificing the inference accuracy; and (c)\na \"rescue module\" that enhances throughput by leveraging approximate computing\nto trade accuracy for latency when necessary. Our primary objective is to\nmaximize system prolong battery lifespan, throughput, and accuracy while\nadhering to strict latency constraints. Experimental evaluations in the context\nof wearable technologies for blind and visually impaired users demonstrate that\nHE2C significantly improves task throughput via completing a larger number of\ntasks within their specified deadlines, while preserving edge device battery\nand maintaining prediction accuracy with minimal latency impact. These results\nunderscore HE2C's potential as a robust solution for resource management in\nlatency-sensitive, energy-constrained edge-to-cloud environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The high computational, memory, and energy demands of Deep Learning (DL)\napplications often exceed the capabilities of battery-powered edge devices,\ncreating difficulties in meeting task deadlines and accuracy requirements.\nUnlike previous solutions that optimize a single metric (e.g., accuracy or\nenergy efficiency), HE2C framework is designed to holistically address the\nlatency, memory, accuracy, throughput, and energy demands of DL applications\nacross edge-cloud continuum, thereby, delivering a more comprehensive and\neffective user experience. HE2C comprises three key modules: (a) a\n\"feasibility-check module that evaluates the likelihood of meeting deadlines\nacross both edge and cloud resources; (b) a \"resource allocation strategy\" that\nmaximizes energy efficiency without sacrificing the inference accuracy; and (c)\na \"rescue module\" that enhances throughput by leveraging approximate computing\nto trade accuracy for latency when necessary. Our primary objective is to\nmaximize system prolong battery lifespan, throughput, and accuracy while\nadhering to strict latency constraints. Experimental evaluations in the context\nof wearable technologies for blind and visually impaired users demonstrate that\nHE2C significantly improves task throughput via completing a larger number of\ntasks within their specified deadlines, while preserving edge device battery\nand maintaining prediction accuracy with minimal latency impact. These results\nunderscore HE2C's potential as a robust solution for resource management in\nlatency-sensitive, energy-constrained edge-to-cloud environments."
                },
                "authors": [
                    {
                        "name": "Minseo Kim"
                    },
                    {
                        "name": "Wei Shu"
                    },
                    {
                        "name": "Mohsen Amini Salehi"
                    }
                ],
                "author_detail": {
                    "name": "Mohsen Amini Salehi"
                },
                "author": "Mohsen Amini Salehi",
                "arxiv_comment": "Accepted in Utility Cloud Computing (UCC '24) Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19487v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19487v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19485v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19485v1",
                "updated": "2024-11-29T05:54:41Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    5,
                    54,
                    41,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T05:54:41Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    5,
                    54,
                    41,
                    4,
                    334,
                    0
                ],
                "title": "Action Engine: An LLM-based Framework for Automatic FaaS Workflow\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Action Engine: An LLM-based Framework for Automatic FaaS Workflow\n  Generation"
                },
                "summary": "Function as a Service (FaaS) is poised to become the foundation of the next\ngeneration of cloud systems due to its inherent advantages in scalability,\ncost-efficiency, and ease of use. However, challenges such as the need for\nspecialized knowledge and difficulties in building function workflows persist\nfor cloud-native application developers. To overcome these challenges and\nmitigate the burden of developing FaaS-based applications, in this paper, we\npropose a mechanism called Action Engine, that makes use of Tool-Augmented\nLarge Language Models (LLMs) at its kernel to interpret human language queries\nand automates FaaS workflow generation, thereby, reducing the need for\nspecialized expertise and manual design. Action Engine includes modules to\nidentify relevant functions from the FaaS repository and seamlessly manage the\ndata dependency between them, ensuring that the developer's query is processed\nand resolved. Beyond that, Action Engine can execute the generated workflow by\nfeeding the user-provided parameters. Our evaluations show that Action Engine\ncan generate workflows with up to 20\\% higher correctness without developer\ninvolvement. We notice that Action Engine can unlock FaaS workflow generation\nfor non-cloud-savvy developers and expedite the development cycles of\ncloud-native applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Function as a Service (FaaS) is poised to become the foundation of the next\ngeneration of cloud systems due to its inherent advantages in scalability,\ncost-efficiency, and ease of use. However, challenges such as the need for\nspecialized knowledge and difficulties in building function workflows persist\nfor cloud-native application developers. To overcome these challenges and\nmitigate the burden of developing FaaS-based applications, in this paper, we\npropose a mechanism called Action Engine, that makes use of Tool-Augmented\nLarge Language Models (LLMs) at its kernel to interpret human language queries\nand automates FaaS workflow generation, thereby, reducing the need for\nspecialized expertise and manual design. Action Engine includes modules to\nidentify relevant functions from the FaaS repository and seamlessly manage the\ndata dependency between them, ensuring that the developer's query is processed\nand resolved. Beyond that, Action Engine can execute the generated workflow by\nfeeding the user-provided parameters. Our evaluations show that Action Engine\ncan generate workflows with up to 20\\% higher correctness without developer\ninvolvement. We notice that Action Engine can unlock FaaS workflow generation\nfor non-cloud-savvy developers and expedite the development cycles of\ncloud-native applications."
                },
                "authors": [
                    {
                        "name": "Akiharu Esashi"
                    },
                    {
                        "name": "Pawissanutt Lertpongrujikorn"
                    },
                    {
                        "name": "Mohsen Amini Salehi"
                    }
                ],
                "author_detail": {
                    "name": "Mohsen Amini Salehi"
                },
                "author": "Mohsen Amini Salehi",
                "arxiv_comment": "Accepted at Utility Cloud Computing (UCC '24) conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19485v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19485v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.00958v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.00958v4",
                "updated": "2024-11-29T05:50:09Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    5,
                    50,
                    9,
                    4,
                    334,
                    0
                ],
                "published": "2024-07-01T04:29:35Z",
                "published_parsed": [
                    2024,
                    7,
                    1,
                    4,
                    29,
                    35,
                    0,
                    183,
                    0
                ],
                "title": "Dynamic Universal Approximation Theory: The Basic Theory for\n  Transformer-based Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Universal Approximation Theory: The Basic Theory for\n  Transformer-based Large Language Models"
                },
                "summary": "Language models have emerged as a critical area of focus in artificial\nintelligence, particularly with the introduction of groundbreaking innovations\nlike ChatGPT. Large-scale Transformer networks have quickly become the leading\napproach for advancing natural language processing algorithms. Built on the\nTransformer architecture, these models enable interactions that closely mimic\nhuman communication and, equipped with extensive knowledge, can even assist in\nguiding human tasks. Despite their impressive capabilities and growing\ncomplexity, a key question remains-the theoretical foundations of large\nlanguage models (LLMs). What makes Transformer so effective for powering\nintelligent language applications, such as translation and coding? What\nunderlies LLMs' ability for In-Context Learning (ICL)? How does the LoRA scheme\nenhance the fine-tuning of LLMs? And what supports the practicality of pruning\nLLMs? To address these critical questions and explore the technological\nstrategies within LLMs, we leverage the Universal Approximation Theory (UAT) to\noffer a theoretical backdrop, shedding light on the mechanisms that underpin\nthese advancements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language models have emerged as a critical area of focus in artificial\nintelligence, particularly with the introduction of groundbreaking innovations\nlike ChatGPT. Large-scale Transformer networks have quickly become the leading\napproach for advancing natural language processing algorithms. Built on the\nTransformer architecture, these models enable interactions that closely mimic\nhuman communication and, equipped with extensive knowledge, can even assist in\nguiding human tasks. Despite their impressive capabilities and growing\ncomplexity, a key question remains-the theoretical foundations of large\nlanguage models (LLMs). What makes Transformer so effective for powering\nintelligent language applications, such as translation and coding? What\nunderlies LLMs' ability for In-Context Learning (ICL)? How does the LoRA scheme\nenhance the fine-tuning of LLMs? And what supports the practicality of pruning\nLLMs? To address these critical questions and explore the technological\nstrategies within LLMs, we leverage the Universal Approximation Theory (UAT) to\noffer a theoretical backdrop, shedding light on the mechanisms that underpin\nthese advancements."
                },
                "authors": [
                    {
                        "name": "Wei Wang"
                    },
                    {
                        "name": "Qing Li"
                    }
                ],
                "author_detail": {
                    "name": "Qing Li"
                },
                "author": "Qing Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.00958v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.00958v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19478v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19478v1",
                "updated": "2024-11-29T05:31:04Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    5,
                    31,
                    4,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T05:31:04Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    5,
                    31,
                    4,
                    4,
                    334,
                    0
                ],
                "title": "Zero-Indexing Internet Search Augmented Generation for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-Indexing Internet Search Augmented Generation for Large Language\n  Models"
                },
                "summary": "Retrieval augmented generation has emerged as an effective method to enhance\nlarge language model performance. This approach typically relies on an internal\nretrieval module that uses various indexing mechanisms to manage a static\npre-processed corpus. However, such a paradigm often falls short when it is\nnecessary to integrate the most up-to-date information that has not been\nupdated into the corpus during generative inference time. In this paper, we\nexplore an alternative approach that leverages standard search engine APIs to\ndynamically integrate the latest online information (without maintaining any\nindex for any fixed corpus), thereby improving the quality of generated\ncontent. We design a collaborative LLM-based paradigm, where we include: (i) a\nparser-LLM that determines if the Internet augmented generation is demanded and\nextracts the search keywords if so with a single inference; (ii) a mixed\nranking strategy that re-ranks the retrieved HTML files to eliminate bias\nintroduced from the search engine API; and (iii) an extractor-LLM that can\naccurately and efficiently extract relevant information from the fresh content\nin each HTML file. We conduct extensive empirical studies to evaluate the\nperformance of this Internet search augmented generation paradigm. The\nexperimental results demonstrate that our method generates content with\nsignificantly improved quality. Our system has been successfully deployed in a\nproduction environment to serve 01.AI's generative inference requests.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval augmented generation has emerged as an effective method to enhance\nlarge language model performance. This approach typically relies on an internal\nretrieval module that uses various indexing mechanisms to manage a static\npre-processed corpus. However, such a paradigm often falls short when it is\nnecessary to integrate the most up-to-date information that has not been\nupdated into the corpus during generative inference time. In this paper, we\nexplore an alternative approach that leverages standard search engine APIs to\ndynamically integrate the latest online information (without maintaining any\nindex for any fixed corpus), thereby improving the quality of generated\ncontent. We design a collaborative LLM-based paradigm, where we include: (i) a\nparser-LLM that determines if the Internet augmented generation is demanded and\nextracts the search keywords if so with a single inference; (ii) a mixed\nranking strategy that re-ranks the retrieved HTML files to eliminate bias\nintroduced from the search engine API; and (iii) an extractor-LLM that can\naccurately and efficiently extract relevant information from the fresh content\nin each HTML file. We conduct extensive empirical studies to evaluate the\nperformance of this Internet search augmented generation paradigm. The\nexperimental results demonstrate that our method generates content with\nsignificantly improved quality. Our system has been successfully deployed in a\nproduction environment to serve 01.AI's generative inference requests."
                },
                "authors": [
                    {
                        "name": "Guangxin He"
                    },
                    {
                        "name": "Zonghong Dai"
                    },
                    {
                        "name": "Jiangcheng Zhu"
                    },
                    {
                        "name": "Binqiang Zhao"
                    },
                    {
                        "name": "Chenyue Li"
                    },
                    {
                        "name": "You Peng"
                    },
                    {
                        "name": "Chen Wang"
                    },
                    {
                        "name": "Binhang Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Binhang Yuan"
                },
                "author": "Binhang Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19478v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19478v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19477v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19477v1",
                "updated": "2024-11-29T05:29:47Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    5,
                    29,
                    47,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T05:29:47Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    5,
                    29,
                    47,
                    4,
                    334,
                    0
                ],
                "title": "A Simple and Provable Scaling Law for the Test-Time Compute of Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Simple and Provable Scaling Law for the Test-Time Compute of Large\n  Language Models"
                },
                "summary": "We propose a general two-stage algorithm that enjoys a provable scaling law\nfor the test-time compute of large language models (LLMs). Given an input\nproblem, the proposed algorithm first generates $N$ candidate solutions, and\nthen chooses the best one via a multiple-round knockout tournament where each\npair of candidates are compared for $K$ times and only the winners move on to\nthe next round. In a minimalistic implementation, both stages can be executed\nwith a black-box LLM alone and nothing else (e.g., no external verifier or\nreward model), and a total of $N \\times (K + 1)$ highly parallelizable LLM\ncalls are needed for solving an input problem. Assuming that a generated\ncandidate solution is correct with probability $p_{\\text{gen}} > 0$ and a\ncomparison between a pair of correct and incorrect solutions identifies the\nright winner with probability $p_{\\text{comp}} > 0.5$ (i.e., better than a\nrandom guess), we prove theoretically that the failure probability of the\nproposed algorithm decays to zero exponentially with respect to $N$ and $K$:\n$$\\mathbb{P}(\\text{final output is incorrect}) \\le (1 - p_{\\text{gen}})^N +\n\\lceil \\log_2 N \\rceil e^{-2 K (p_{\\text{comp}} - 0.5)^2}.$$ Our empirical\nresults with the challenging MMLU-Pro benchmark validate the technical\nassumptions, as well as the efficacy of the proposed algorithm and the gains\nfrom scaling up its test-time compute.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a general two-stage algorithm that enjoys a provable scaling law\nfor the test-time compute of large language models (LLMs). Given an input\nproblem, the proposed algorithm first generates $N$ candidate solutions, and\nthen chooses the best one via a multiple-round knockout tournament where each\npair of candidates are compared for $K$ times and only the winners move on to\nthe next round. In a minimalistic implementation, both stages can be executed\nwith a black-box LLM alone and nothing else (e.g., no external verifier or\nreward model), and a total of $N \\times (K + 1)$ highly parallelizable LLM\ncalls are needed for solving an input problem. Assuming that a generated\ncandidate solution is correct with probability $p_{\\text{gen}} > 0$ and a\ncomparison between a pair of correct and incorrect solutions identifies the\nright winner with probability $p_{\\text{comp}} > 0.5$ (i.e., better than a\nrandom guess), we prove theoretically that the failure probability of the\nproposed algorithm decays to zero exponentially with respect to $N$ and $K$:\n$$\\mathbb{P}(\\text{final output is incorrect}) \\le (1 - p_{\\text{gen}})^N +\n\\lceil \\log_2 N \\rceil e^{-2 K (p_{\\text{comp}} - 0.5)^2}.$$ Our empirical\nresults with the challenging MMLU-Pro benchmark validate the technical\nassumptions, as well as the efficacy of the proposed algorithm and the gains\nfrom scaling up its test-time compute."
                },
                "authors": [
                    {
                        "name": "Yanxi Chen"
                    },
                    {
                        "name": "Xuchen Pan"
                    },
                    {
                        "name": "Yaliang Li"
                    },
                    {
                        "name": "Bolin Ding"
                    },
                    {
                        "name": "Jingren Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jingren Zhou"
                },
                "author": "Jingren Zhou",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19477v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19477v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.00627v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.00627v3",
                "updated": "2024-11-29T05:05:13Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    5,
                    5,
                    13,
                    4,
                    334,
                    0
                ],
                "published": "2024-06-02T06:09:56Z",
                "published_parsed": [
                    2024,
                    6,
                    2,
                    6,
                    9,
                    56,
                    6,
                    154,
                    0
                ],
                "title": "Prompt Framework for Role-playing: Generation and Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt Framework for Role-playing: Generation and Evaluation"
                },
                "summary": "Large language models (LLMs) exhibit impressive proficiency in natural\nlanguage generation, understanding user instructions, and emulating human-like\nlanguage use, which has led to significant interest in their application to\nrole-playing scenarios. However, the manual collection of role-specific script\ndata and the evaluation of model performance are resource-intensive processes.\nThis project introduces a prompt-based framework designed to leverage GPT's\ncapabilities for the generation of role-playing dialogue datasets and the\nevaluation of role-playing performance. To validate the effectiveness of the\nGPT-based generation and evaluation, we further incorporate the recall-oriented\nRouge-L metric, providing an additional quantitative measure of performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) exhibit impressive proficiency in natural\nlanguage generation, understanding user instructions, and emulating human-like\nlanguage use, which has led to significant interest in their application to\nrole-playing scenarios. However, the manual collection of role-specific script\ndata and the evaluation of model performance are resource-intensive processes.\nThis project introduces a prompt-based framework designed to leverage GPT's\ncapabilities for the generation of role-playing dialogue datasets and the\nevaluation of role-playing performance. To validate the effectiveness of the\nGPT-based generation and evaluation, we further incorporate the recall-oriented\nRouge-L metric, providing an additional quantitative measure of performance."
                },
                "authors": [
                    {
                        "name": "Xun Liu"
                    },
                    {
                        "name": "Zhengwei Ni"
                    }
                ],
                "author_detail": {
                    "name": "Zhengwei Ni"
                },
                "author": "Zhengwei Ni",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.00627v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.00627v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.06350v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.06350v2",
                "updated": "2024-11-29T04:50:35Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    4,
                    50,
                    35,
                    4,
                    334,
                    0
                ],
                "published": "2024-03-11T00:46:56Z",
                "published_parsed": [
                    2024,
                    3,
                    11,
                    0,
                    46,
                    56,
                    0,
                    71,
                    0
                ],
                "title": "IndicLLMSuite: A Blueprint for Creating Pre-training and Fine-Tuning\n  Datasets for Indian Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IndicLLMSuite: A Blueprint for Creating Pre-training and Fine-Tuning\n  Datasets for Indian Languages"
                },
                "summary": "Despite the considerable advancements in English LLMs, the progress in\nbuilding comparable models for other languages has been hindered due to the\nscarcity of tailored resources. Our work aims to bridge this divide by\nintroducing an expansive suite of resources specifically designed for the\ndevelopment of Indic LLMs, covering 22 languages, containing a total of 251B\ntokens and 74.8M instruction-response pairs. Recognizing the importance of both\ndata quality and quantity, our approach combines highly curated manually\nverified data, unverified yet valuable data, and synthetic data. We build a\nclean, open-source pipeline for curating pre-training data from diverse\nsources, including websites, PDFs, and videos, incorporating best practices for\ncrawling, cleaning, flagging, and deduplication. For instruction-fine tuning,\nwe amalgamate existing Indic datasets, translate/transliterate English datasets\ninto Indian languages, and utilize LLaMa2 and Mixtral models to create\nconversations grounded in articles from Indian Wikipedia and Wikihow.\nAdditionally, we address toxicity alignment by generating toxic prompts for\nmultiple scenarios and then generate non-toxic responses by feeding these toxic\nprompts to an aligned LLaMa2 model. We hope that the datasets, tools, and\nresources released as a part of this work will not only propel the research and\ndevelopment of Indic LLMs but also establish an open-source blueprint for\nextending such efforts to other languages. The data and other artifacts created\nas part of this work are released with permissive licenses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the considerable advancements in English LLMs, the progress in\nbuilding comparable models for other languages has been hindered due to the\nscarcity of tailored resources. Our work aims to bridge this divide by\nintroducing an expansive suite of resources specifically designed for the\ndevelopment of Indic LLMs, covering 22 languages, containing a total of 251B\ntokens and 74.8M instruction-response pairs. Recognizing the importance of both\ndata quality and quantity, our approach combines highly curated manually\nverified data, unverified yet valuable data, and synthetic data. We build a\nclean, open-source pipeline for curating pre-training data from diverse\nsources, including websites, PDFs, and videos, incorporating best practices for\ncrawling, cleaning, flagging, and deduplication. For instruction-fine tuning,\nwe amalgamate existing Indic datasets, translate/transliterate English datasets\ninto Indian languages, and utilize LLaMa2 and Mixtral models to create\nconversations grounded in articles from Indian Wikipedia and Wikihow.\nAdditionally, we address toxicity alignment by generating toxic prompts for\nmultiple scenarios and then generate non-toxic responses by feeding these toxic\nprompts to an aligned LLaMa2 model. We hope that the datasets, tools, and\nresources released as a part of this work will not only propel the research and\ndevelopment of Indic LLMs but also establish an open-source blueprint for\nextending such efforts to other languages. The data and other artifacts created\nas part of this work are released with permissive licenses."
                },
                "authors": [
                    {
                        "name": "Mohammed Safi Ur Rahman Khan"
                    },
                    {
                        "name": "Priyam Mehta"
                    },
                    {
                        "name": "Ananth Sankar"
                    },
                    {
                        "name": "Umashankar Kumaravelan"
                    },
                    {
                        "name": "Sumanth Doddapaneni"
                    },
                    {
                        "name": "Suriyaprasaad B"
                    },
                    {
                        "name": "Varun Balan G"
                    },
                    {
                        "name": "Sparsh Jain"
                    },
                    {
                        "name": "Anoop Kunchukuttan"
                    },
                    {
                        "name": "Pratyush Kumar"
                    },
                    {
                        "name": "Raj Dabre"
                    },
                    {
                        "name": "Mitesh M. Khapra"
                    }
                ],
                "author_detail": {
                    "name": "Mitesh M. Khapra"
                },
                "author": "Mitesh M. Khapra",
                "arxiv_doi": "10.18653/v1/2024.acl-long.843",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.18653/v1/2024.acl-long.843",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2403.06350v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.06350v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "ACL-2024 Outstanding Paper",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.04845v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.04845v4",
                "updated": "2024-11-29T04:39:57Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    4,
                    39,
                    57,
                    4,
                    334,
                    0
                ],
                "published": "2024-05-08T06:46:14Z",
                "published_parsed": [
                    2024,
                    5,
                    8,
                    6,
                    46,
                    14,
                    2,
                    129,
                    0
                ],
                "title": "Weighted Particle-Based Optimization for Efficient Generalized Posterior\n  Calibration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Weighted Particle-Based Optimization for Efficient Generalized Posterior\n  Calibration"
                },
                "summary": "In the realm of statistical learning, the increasing volume of accessible\ndata and increasing model complexity necessitate robust methodologies. This\npaper explores two branches of robust Bayesian methods in response to this\ntrend. The first is generalized Bayesian inference, which introduces a learning\nrate parameter to enhance robustness against model misspecifications. The\nsecond is Gibbs posterior inference, which formulates inferential problems\nusing generic loss functions rather than probabilistic models. In such\napproaches, it is necessary to calibrate the spread of the posterior\ndistribution by selecting a learning rate parameter. The study aims to enhance\nthe generalized posterior calibration (GPC) algorithm proposed by [1]. Their\nalgorithm chooses the learning rate to achieve the nominal frequentist coverage\nprobability, but it is computationally intensive because it requires repeated\nposterior simulations for bootstrap samples. We propose a more efficient\nversion of the GPC inspired by sequential Monte Carlo (SMC) samplers. A target\ndistribution with a different learning rate is evaluated without posterior\nsimulation as in the reweighting step in SMC sampling. Thus, the proposed\nalgorithm can reach the desirable value within a few iterations. This\nimprovement substantially reduces the computational cost of the GPC. Its\nefficacy is demonstrated through synthetic and real data applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the realm of statistical learning, the increasing volume of accessible\ndata and increasing model complexity necessitate robust methodologies. This\npaper explores two branches of robust Bayesian methods in response to this\ntrend. The first is generalized Bayesian inference, which introduces a learning\nrate parameter to enhance robustness against model misspecifications. The\nsecond is Gibbs posterior inference, which formulates inferential problems\nusing generic loss functions rather than probabilistic models. In such\napproaches, it is necessary to calibrate the spread of the posterior\ndistribution by selecting a learning rate parameter. The study aims to enhance\nthe generalized posterior calibration (GPC) algorithm proposed by [1]. Their\nalgorithm chooses the learning rate to achieve the nominal frequentist coverage\nprobability, but it is computationally intensive because it requires repeated\nposterior simulations for bootstrap samples. We propose a more efficient\nversion of the GPC inspired by sequential Monte Carlo (SMC) samplers. A target\ndistribution with a different learning rate is evaluated without posterior\nsimulation as in the reweighting step in SMC sampling. Thus, the proposed\nalgorithm can reach the desirable value within a few iterations. This\nimprovement substantially reduces the computational cost of the GPC. Its\nefficacy is demonstrated through synthetic and real data applications."
                },
                "authors": [
                    {
                        "name": "Masahiro Tanaka"
                    }
                ],
                "author_detail": {
                    "name": "Masahiro Tanaka"
                },
                "author": "Masahiro Tanaka",
                "arxiv_doi": "10.1109/ICoDSA62899.2024.10651910",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/ICoDSA62899.2024.10651910",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.04845v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.04845v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Proceedings of the 7th International Conference on Data Science\n  and Its Applications 2024 (ICoDSA), pp. 515-521",
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19468v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19468v1",
                "updated": "2024-11-29T04:38:12Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    4,
                    38,
                    12,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T04:38:12Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    4,
                    38,
                    12,
                    4,
                    334,
                    0
                ],
                "title": "Random Feature Models with Learnable Activation Functions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Random Feature Models with Learnable Activation Functions"
                },
                "summary": "Current random feature models typically rely on fixed activation functions,\nlimiting their ability to capture diverse patterns in data. To address this, we\nintroduce the Random Feature model with Learnable Activation Functions (RFLAF),\na novel model that significantly enhances the expressivity and interpretability\nof traditional random feature (RF) models. We begin by studying the RF model\nwith a single radial basis function, where we discover a new kernel and provide\nthe first theoretical analysis on it. By integrating the basis functions with\nlearnable weights, we show that RFLAF can represent a broad class of random\nfeature models whose activation functions belong in $C_c(\\mathbb{R})$.\nTheoretically, we prove that the model requires only about twice the parameter\nnumber compared to a traditional RF model to achieve the significant leap in\nexpressivity. Experimentally, RFLAF demonstrates two key advantages: (1) it\nperforms better across various tasks compared to traditional RF model with the\nsame number of parameters, and (2) the optimized weights offer\ninterpretability, as the learned activation function can be directly inferred\nfrom these weights. Our model paves the way for developing more expressive and\ninterpretable frameworks within random feature models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current random feature models typically rely on fixed activation functions,\nlimiting their ability to capture diverse patterns in data. To address this, we\nintroduce the Random Feature model with Learnable Activation Functions (RFLAF),\na novel model that significantly enhances the expressivity and interpretability\nof traditional random feature (RF) models. We begin by studying the RF model\nwith a single radial basis function, where we discover a new kernel and provide\nthe first theoretical analysis on it. By integrating the basis functions with\nlearnable weights, we show that RFLAF can represent a broad class of random\nfeature models whose activation functions belong in $C_c(\\mathbb{R})$.\nTheoretically, we prove that the model requires only about twice the parameter\nnumber compared to a traditional RF model to achieve the significant leap in\nexpressivity. Experimentally, RFLAF demonstrates two key advantages: (1) it\nperforms better across various tasks compared to traditional RF model with the\nsame number of parameters, and (2) the optimized weights offer\ninterpretability, as the learned activation function can be directly inferred\nfrom these weights. Our model paves the way for developing more expressive and\ninterpretable frameworks within random feature models."
                },
                "authors": [
                    {
                        "name": "Zailin Ma"
                    },
                    {
                        "name": "Jiansheng Yang"
                    },
                    {
                        "name": "Yaodong Yang"
                    }
                ],
                "author_detail": {
                    "name": "Yaodong Yang"
                },
                "author": "Yaodong Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19468v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19468v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19463v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19463v1",
                "updated": "2024-11-29T04:25:31Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    4,
                    25,
                    31,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T04:25:31Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    4,
                    25,
                    31,
                    4,
                    334,
                    0
                ],
                "title": "Towards Understanding Retrieval Accuracy and Prompt Quality in RAG\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Understanding Retrieval Accuracy and Prompt Quality in RAG\n  Systems"
                },
                "summary": "Retrieval-Augmented Generation (RAG) is a pivotal technique for enhancing the\ncapability of large language models (LLMs) and has demonstrated promising\nefficacy across a diverse spectrum of tasks. While LLM-driven RAG systems show\nsuperior performance, they face unique challenges in stability and reliability.\nTheir complexity hinders developers' efforts to design, maintain, and optimize\neffective RAG systems. Therefore, it is crucial to understand how RAG's\nperformance is impacted by its design. In this work, we conduct an early\nexploratory study toward a better understanding of the mechanism of RAG\nsystems, covering three code datasets, three QA datasets, and two LLMs. We\nfocus on four design factors: retrieval document type, retrieval recall,\ndocument selection, and prompt techniques. Our study uncovers how each factor\nimpacts system correctness and confidence, providing valuable insights for\ndeveloping an accurate and reliable RAG system. Based on these findings, we\npresent nine actionable guidelines for detecting defects and optimizing the\nperformance of RAG systems. We hope our early exploration can inspire further\nadvancements in engineering, improving and maintaining LLM-driven intelligent\nsoftware systems for greater efficiency and reliability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) is a pivotal technique for enhancing the\ncapability of large language models (LLMs) and has demonstrated promising\nefficacy across a diverse spectrum of tasks. While LLM-driven RAG systems show\nsuperior performance, they face unique challenges in stability and reliability.\nTheir complexity hinders developers' efforts to design, maintain, and optimize\neffective RAG systems. Therefore, it is crucial to understand how RAG's\nperformance is impacted by its design. In this work, we conduct an early\nexploratory study toward a better understanding of the mechanism of RAG\nsystems, covering three code datasets, three QA datasets, and two LLMs. We\nfocus on four design factors: retrieval document type, retrieval recall,\ndocument selection, and prompt techniques. Our study uncovers how each factor\nimpacts system correctness and confidence, providing valuable insights for\ndeveloping an accurate and reliable RAG system. Based on these findings, we\npresent nine actionable guidelines for detecting defects and optimizing the\nperformance of RAG systems. We hope our early exploration can inspire further\nadvancements in engineering, improving and maintaining LLM-driven intelligent\nsoftware systems for greater efficiency and reliability."
                },
                "authors": [
                    {
                        "name": "Shengming Zhao"
                    },
                    {
                        "name": "Yuheng Huang"
                    },
                    {
                        "name": "Jiayang Song"
                    },
                    {
                        "name": "Zhijie Wang"
                    },
                    {
                        "name": "Chengcheng Wan"
                    },
                    {
                        "name": "Lei Ma"
                    }
                ],
                "author_detail": {
                    "name": "Lei Ma"
                },
                "author": "Lei Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19463v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19463v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19461v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19461v1",
                "updated": "2024-11-29T04:14:17Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    4,
                    14,
                    17,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T04:14:17Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    4,
                    14,
                    17,
                    4,
                    334,
                    0
                ],
                "title": "Robust Bayesian Scene Reconstruction by Leveraging Retrieval-Augmented\n  Priors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust Bayesian Scene Reconstruction by Leveraging Retrieval-Augmented\n  Priors"
                },
                "summary": "Constructing 3D representations of object geometry is critical for many\ndownstream manipulation tasks. These representations must be built from\npotentially noisy partial observations. In this work we focus on the problem of\nreconstructing a multi-object scene from a single RGBD image. Current deep\nlearning approaches to this problem can be brittle to noisy real world\nobservations and out-of-distribution objects. Other approaches that do not rely\non training data cannot accurately infer the backside of objects. We propose\nBRRP, a reconstruction method that can leverage preexisting mesh datasets to\nbuild an informative prior during robust probabilistic reconstruction. In order\nto make our method more efficient, we introduce the concept of\nretrieval-augmented prior, where we retrieve relevant components of our prior\ndistribution during inference. Our method produces a distribution over object\nshape that can be used for reconstruction or measuring uncertainty. We evaluate\nour method in both procedurally generated scenes and in real world scenes. We\nshow our method is more robust than a deep learning approach while being more\naccurate than a method with an uninformative prior.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Constructing 3D representations of object geometry is critical for many\ndownstream manipulation tasks. These representations must be built from\npotentially noisy partial observations. In this work we focus on the problem of\nreconstructing a multi-object scene from a single RGBD image. Current deep\nlearning approaches to this problem can be brittle to noisy real world\nobservations and out-of-distribution objects. Other approaches that do not rely\non training data cannot accurately infer the backside of objects. We propose\nBRRP, a reconstruction method that can leverage preexisting mesh datasets to\nbuild an informative prior during robust probabilistic reconstruction. In order\nto make our method more efficient, we introduce the concept of\nretrieval-augmented prior, where we retrieve relevant components of our prior\ndistribution during inference. Our method produces a distribution over object\nshape that can be used for reconstruction or measuring uncertainty. We evaluate\nour method in both procedurally generated scenes and in real world scenes. We\nshow our method is more robust than a deep learning approach while being more\naccurate than a method with an uninformative prior."
                },
                "authors": [
                    {
                        "name": "Herbert Wright"
                    },
                    {
                        "name": "Weiming Zhi"
                    },
                    {
                        "name": "Matthew Johnson-Roberson"
                    },
                    {
                        "name": "Tucker Hermans"
                    }
                ],
                "author_detail": {
                    "name": "Tucker Hermans"
                },
                "author": "Tucker Hermans",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19461v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19461v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19459v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19459v1",
                "updated": "2024-11-29T04:09:13Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    4,
                    9,
                    13,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T04:09:13Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    4,
                    9,
                    13,
                    4,
                    334,
                    0
                ],
                "title": "Fleximo: Towards Flexible Text-to-Human Motion Video Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fleximo: Towards Flexible Text-to-Human Motion Video Generation"
                },
                "summary": "Current methods for generating human motion videos rely on extracting pose\nsequences from reference videos, which restricts flexibility and control.\nAdditionally, due to the limitations of pose detection techniques, the\nextracted pose sequences can sometimes be inaccurate, leading to low-quality\nvideo outputs. We introduce a novel task aimed at generating human motion\nvideos solely from reference images and natural language. This approach offers\ngreater flexibility and ease of use, as text is more accessible than the\ndesired guidance videos. However, training an end-to-end model for this task\nrequires millions of high-quality text and human motion video pairs, which are\nchallenging to obtain. To address this, we propose a new framework called\nFleximo, which leverages large-scale pre-trained text-to-3D motion models. This\napproach is not straightforward, as the text-generated skeletons may not\nconsistently match the scale of the reference image and may lack detailed\ninformation. To overcome these challenges, we introduce an anchor point based\nrescale method and design a skeleton adapter to fill in missing details and\nbridge the gap between text-to-motion and motion-to-video generation. We also\npropose a video refinement process to further enhance video quality. A large\nlanguage model (LLM) is employed to decompose natural language into discrete\nmotion sequences, enabling the generation of motion videos of any desired\nlength. To assess the performance of Fleximo, we introduce a new benchmark\ncalled MotionBench, which includes 400 videos across 20 identities and 20\nmotions. We also propose a new metric, MotionScore, to evaluate the accuracy of\nmotion following. Both qualitative and quantitative results demonstrate that\nour method outperforms existing text-conditioned image-to-video generation\nmethods. All code and model weights will be made publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current methods for generating human motion videos rely on extracting pose\nsequences from reference videos, which restricts flexibility and control.\nAdditionally, due to the limitations of pose detection techniques, the\nextracted pose sequences can sometimes be inaccurate, leading to low-quality\nvideo outputs. We introduce a novel task aimed at generating human motion\nvideos solely from reference images and natural language. This approach offers\ngreater flexibility and ease of use, as text is more accessible than the\ndesired guidance videos. However, training an end-to-end model for this task\nrequires millions of high-quality text and human motion video pairs, which are\nchallenging to obtain. To address this, we propose a new framework called\nFleximo, which leverages large-scale pre-trained text-to-3D motion models. This\napproach is not straightforward, as the text-generated skeletons may not\nconsistently match the scale of the reference image and may lack detailed\ninformation. To overcome these challenges, we introduce an anchor point based\nrescale method and design a skeleton adapter to fill in missing details and\nbridge the gap between text-to-motion and motion-to-video generation. We also\npropose a video refinement process to further enhance video quality. A large\nlanguage model (LLM) is employed to decompose natural language into discrete\nmotion sequences, enabling the generation of motion videos of any desired\nlength. To assess the performance of Fleximo, we introduce a new benchmark\ncalled MotionBench, which includes 400 videos across 20 identities and 20\nmotions. We also propose a new metric, MotionScore, to evaluate the accuracy of\nmotion following. Both qualitative and quantitative results demonstrate that\nour method outperforms existing text-conditioned image-to-video generation\nmethods. All code and model weights will be made publicly available."
                },
                "authors": [
                    {
                        "name": "Yuhang Zhang"
                    },
                    {
                        "name": "Yuan Zhou"
                    },
                    {
                        "name": "Zeyu Liu"
                    },
                    {
                        "name": "Yuxuan Cai"
                    },
                    {
                        "name": "Qiuyue Wang"
                    },
                    {
                        "name": "Aidong Men"
                    },
                    {
                        "name": "Huan Yang"
                    }
                ],
                "author_detail": {
                    "name": "Huan Yang"
                },
                "author": "Huan Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19459v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19459v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19456v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19456v1",
                "updated": "2024-11-29T03:57:26Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    3,
                    57,
                    26,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T03:57:26Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    3,
                    57,
                    26,
                    4,
                    334,
                    0
                ],
                "title": "Beyond Surface Structure: A Causal Assessment of LLMs' Comprehension\n  Ability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Surface Structure: A Causal Assessment of LLMs' Comprehension\n  Ability"
                },
                "summary": "Large language models (LLMs) have shown remarkable capability in natural\nlanguage tasks, yet debate persists on whether they truly comprehend deep\nstructure (i.e., core semantics) or merely rely on surface structure (e.g.,\npresentation format). Prior studies observe that LLMs' performance declines\nwhen intervening on surface structure, arguing their success relies on surface\nstructure recognition. However, surface structure sensitivity does not prevent\ndeep structure comprehension. Rigorously evaluating LLMs' capability requires\nanalyzing both, yet deep structure is often overlooked. To this end, we assess\nLLMs' comprehension ability using causal mediation analysis, aiming to fully\ndiscover the capability of using both deep and surface structures.\nSpecifically, we formulate the comprehension of deep structure as direct causal\neffect (DCE) and that of surface structure as indirect causal effect (ICE),\nrespectively. To address the non-estimability of original DCE and ICE --\nstemming from the infeasibility of isolating mutual influences of deep and\nsurface structures, we develop the corresponding quantifiable surrogates,\nincluding approximated DCE (ADCE) and approximated ICE (AICE). We further apply\nthe ADCE to evaluate a series of mainstream LLMs, showing that most of them\nexhibit deep structure comprehension ability, which grows along with the\nprediction accuracy. Comparing ADCE and AICE demonstrates closed-source LLMs\nrely more on deep structure, while open-source LLMs are more surface-sensitive,\nwhich decreases with model scale. Theoretically, ADCE is a bidirectional\nevaluation, which measures both the sufficiency and necessity of deep structure\nchanges in causing output variations, thus offering a more comprehensive\nassessment than accuracy, a common evaluation in LLMs. Our work provides new\ninsights into LLMs' deep structure comprehension and offers novel methods for\nLLMs evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown remarkable capability in natural\nlanguage tasks, yet debate persists on whether they truly comprehend deep\nstructure (i.e., core semantics) or merely rely on surface structure (e.g.,\npresentation format). Prior studies observe that LLMs' performance declines\nwhen intervening on surface structure, arguing their success relies on surface\nstructure recognition. However, surface structure sensitivity does not prevent\ndeep structure comprehension. Rigorously evaluating LLMs' capability requires\nanalyzing both, yet deep structure is often overlooked. To this end, we assess\nLLMs' comprehension ability using causal mediation analysis, aiming to fully\ndiscover the capability of using both deep and surface structures.\nSpecifically, we formulate the comprehension of deep structure as direct causal\neffect (DCE) and that of surface structure as indirect causal effect (ICE),\nrespectively. To address the non-estimability of original DCE and ICE --\nstemming from the infeasibility of isolating mutual influences of deep and\nsurface structures, we develop the corresponding quantifiable surrogates,\nincluding approximated DCE (ADCE) and approximated ICE (AICE). We further apply\nthe ADCE to evaluate a series of mainstream LLMs, showing that most of them\nexhibit deep structure comprehension ability, which grows along with the\nprediction accuracy. Comparing ADCE and AICE demonstrates closed-source LLMs\nrely more on deep structure, while open-source LLMs are more surface-sensitive,\nwhich decreases with model scale. Theoretically, ADCE is a bidirectional\nevaluation, which measures both the sufficiency and necessity of deep structure\nchanges in causing output variations, thus offering a more comprehensive\nassessment than accuracy, a common evaluation in LLMs. Our work provides new\ninsights into LLMs' deep structure comprehension and offers novel methods for\nLLMs evaluation."
                },
                "authors": [
                    {
                        "name": "Yujin Han"
                    },
                    {
                        "name": "Lei Xu"
                    },
                    {
                        "name": "Sirui Chen"
                    },
                    {
                        "name": "Difan Zou"
                    },
                    {
                        "name": "Chaochao Lu"
                    }
                ],
                "author_detail": {
                    "name": "Chaochao Lu"
                },
                "author": "Chaochao Lu",
                "arxiv_comment": "28 pages, 14 figures, 10 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19456v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19456v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.00774v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.00774v4",
                "updated": "2024-11-29T03:49:55Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    3,
                    49,
                    55,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-01T17:59:51Z",
                "published_parsed": [
                    2024,
                    11,
                    1,
                    17,
                    59,
                    51,
                    4,
                    306,
                    0
                ],
                "title": "Freeze-Omni: A Smart and Low Latency Speech-to-speech Dialogue Model\n  with Frozen LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Freeze-Omni: A Smart and Low Latency Speech-to-speech Dialogue Model\n  with Frozen LLM"
                },
                "summary": "Rapidly developing large language models (LLMs) have brought tremendous\nintelligent applications. Especially, the GPT-4o's excellent duplex speech\ninteraction ability has brought impressive experience to users. Researchers\nhave recently proposed several multi-modal LLMs in this direction that can\nachieve user-agent speech-to-speech conversations. This paper proposes a novel\nspeech-text multimodal LLM architecture called Freeze-Omni. Our main\ncontribution is that the speech input and output modalities can be easily\nconnected to a textual LLM while keeping the LLM's parameters frozen throughout\nthe training process. We design a three-stage training strategy for modeling\nboth the speech input and output, enabling Freeze-Omni to obtain\nspeech-to-speech conversation ability using text-speech paired data (such as\nASR and TTS data) and only 60,000 multi-round text Q&A data on 8 GPUs.\nMoreover, we can effectively ensure that the intelligence of the Freeze-Omni in\nthe speech modality is at the same level compared with that in the text\nmodality of its backbone LLM, while achieving low latency end-to-end spoken\nresponse. In addition, we also designed a method to achieve duplex dialogue\nability through multi-task training, giving Freeze-Omni a more natural style of\ndialogue ability between users and agents. In summary, Freeze-Omni holds great\npotential to conduct speech-to-speech dialogue based on a multimodal LLM under\nthe condition of a frozen LLM, avoiding the catastrophic forgetting problem\ncaused by limited data and training resources.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rapidly developing large language models (LLMs) have brought tremendous\nintelligent applications. Especially, the GPT-4o's excellent duplex speech\ninteraction ability has brought impressive experience to users. Researchers\nhave recently proposed several multi-modal LLMs in this direction that can\nachieve user-agent speech-to-speech conversations. This paper proposes a novel\nspeech-text multimodal LLM architecture called Freeze-Omni. Our main\ncontribution is that the speech input and output modalities can be easily\nconnected to a textual LLM while keeping the LLM's parameters frozen throughout\nthe training process. We design a three-stage training strategy for modeling\nboth the speech input and output, enabling Freeze-Omni to obtain\nspeech-to-speech conversation ability using text-speech paired data (such as\nASR and TTS data) and only 60,000 multi-round text Q&A data on 8 GPUs.\nMoreover, we can effectively ensure that the intelligence of the Freeze-Omni in\nthe speech modality is at the same level compared with that in the text\nmodality of its backbone LLM, while achieving low latency end-to-end spoken\nresponse. In addition, we also designed a method to achieve duplex dialogue\nability through multi-task training, giving Freeze-Omni a more natural style of\ndialogue ability between users and agents. In summary, Freeze-Omni holds great\npotential to conduct speech-to-speech dialogue based on a multimodal LLM under\nthe condition of a frozen LLM, avoiding the catastrophic forgetting problem\ncaused by limited data and training resources."
                },
                "authors": [
                    {
                        "name": "Xiong Wang"
                    },
                    {
                        "name": "Yangze Li"
                    },
                    {
                        "name": "Chaoyou Fu"
                    },
                    {
                        "name": "Yunhang Shen"
                    },
                    {
                        "name": "Lei Xie"
                    },
                    {
                        "name": "Ke Li"
                    },
                    {
                        "name": "Xing Sun"
                    },
                    {
                        "name": "Long Ma"
                    }
                ],
                "author_detail": {
                    "name": "Long Ma"
                },
                "author": "Long Ma",
                "arxiv_comment": "Project Page: https://freeze-omni.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.00774v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.00774v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2411.19951v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19951v1",
                "updated": "2024-11-29T18:59:54Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    18,
                    59,
                    54,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T18:59:54Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    18,
                    59,
                    54,
                    4,
                    334,
                    0
                ],
                "title": "T2Vid: Translating Long Text into Multi-Image is the Catalyst for\n  Video-LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "T2Vid: Translating Long Text into Multi-Image is the Catalyst for\n  Video-LLMs"
                },
                "summary": "The success of Multimodal Large Language Models (MLLMs) in the image domain\nhas garnered wide attention from the research community. Drawing on previous\nsuccessful experiences, researchers have recently explored extending the\nsuccess to the video understanding realms. Apart from training from scratch, an\nefficient way is to utilize the pre-trained image-LLMs, leading to two\nmainstream approaches, i.e. zero-shot inference and further fine-tuning with\nvideo data. In this work, our study of these approaches harvests an effective\ndata augmentation method. We first make a deeper inspection of the zero-shot\ninference way and identify two limitations, i.e. limited generalization and\nlack of temporal understanding capabilities. Thus, we further investigate the\nfine-tuning approach and find a low learning efficiency when simply using all\nthe video data samples, which can be attributed to a lack of instruction\ndiversity. Aiming at this issue, we develop a method called T2Vid to synthesize\nvideo-like samples to enrich the instruction diversity in the training corpus.\nIntegrating these data enables a simple and efficient training scheme, which\nachieves performance comparable to or even superior to using full video\ndatasets by training with just 15% the sample size. Meanwhile, we find that the\nproposed scheme can boost the performance of long video understanding without\ntraining with long video samples. We hope our study will spark more thinking\nabout using MLLMs for video understanding and curation of high-quality data.\nThe code is released at https://github.com/xjtupanda/T2Vid.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The success of Multimodal Large Language Models (MLLMs) in the image domain\nhas garnered wide attention from the research community. Drawing on previous\nsuccessful experiences, researchers have recently explored extending the\nsuccess to the video understanding realms. Apart from training from scratch, an\nefficient way is to utilize the pre-trained image-LLMs, leading to two\nmainstream approaches, i.e. zero-shot inference and further fine-tuning with\nvideo data. In this work, our study of these approaches harvests an effective\ndata augmentation method. We first make a deeper inspection of the zero-shot\ninference way and identify two limitations, i.e. limited generalization and\nlack of temporal understanding capabilities. Thus, we further investigate the\nfine-tuning approach and find a low learning efficiency when simply using all\nthe video data samples, which can be attributed to a lack of instruction\ndiversity. Aiming at this issue, we develop a method called T2Vid to synthesize\nvideo-like samples to enrich the instruction diversity in the training corpus.\nIntegrating these data enables a simple and efficient training scheme, which\nachieves performance comparable to or even superior to using full video\ndatasets by training with just 15% the sample size. Meanwhile, we find that the\nproposed scheme can boost the performance of long video understanding without\ntraining with long video samples. We hope our study will spark more thinking\nabout using MLLMs for video understanding and curation of high-quality data.\nThe code is released at https://github.com/xjtupanda/T2Vid."
                },
                "authors": [
                    {
                        "name": "Shukang Yin"
                    },
                    {
                        "name": "Chaoyou Fu"
                    },
                    {
                        "name": "Sirui Zhao"
                    },
                    {
                        "name": "Yunhang Shen"
                    },
                    {
                        "name": "Chunjiang Ge"
                    },
                    {
                        "name": "Yan Yang"
                    },
                    {
                        "name": "Zuwei Long"
                    },
                    {
                        "name": "Yuhan Dai"
                    },
                    {
                        "name": "Tong Xu"
                    },
                    {
                        "name": "Xing Sun"
                    },
                    {
                        "name": "Ran He"
                    },
                    {
                        "name": "Caifeng Shan"
                    },
                    {
                        "name": "Enhong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Enhong Chen"
                },
                "author": "Enhong Chen",
                "arxiv_comment": "13 pages, 9 figures, 5 tables. Project page:\n  https://github.com/xjtupanda/T2Vid",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19951v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19951v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19943v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19943v1",
                "updated": "2024-11-29T18:58:22Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    18,
                    58,
                    22,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T18:58:22Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    18,
                    58,
                    22,
                    4,
                    334,
                    0
                ],
                "title": "Critical Tokens Matter: Token-Level Contrastive Estimation Enhence LLM's\n  Reasoning Capability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Critical Tokens Matter: Token-Level Contrastive Estimation Enhence LLM's\n  Reasoning Capability"
                },
                "summary": "Large Language Models (LLMs) have exhibited remarkable performance on\nreasoning tasks. They utilize autoregressive token generation to construct\nreasoning trajectories, enabling the development of a coherent chain of\nthought. In this work, we explore the impact of individual tokens on the final\noutcomes of reasoning tasks. We identify the existence of ``critical tokens''\nthat lead to incorrect reasoning trajectories in LLMs. Specifically, we find\nthat LLMs tend to produce positive outcomes when forced to decode other tokens\ninstead of critical tokens. Motivated by this observation, we propose a novel\napproach - cDPO - designed to automatically recognize and conduct token-level\nrewards for the critical tokens during the alignment process. Specifically, we\ndevelop a contrastive estimation approach to automatically identify critical\ntokens. It is achieved by comparing the generation likelihood of positive and\nnegative models. To achieve this, we separately fine-tune the positive and\nnegative models on various reasoning trajectories, consequently, they are\ncapable of identifying identify critical tokens within incorrect trajectories\nthat contribute to erroneous outcomes. Moreover, to further align the model\nwith the critical token information during the alignment process, we extend the\nconventional DPO algorithms to token-level DPO and utilize the differential\nlikelihood from the aforementioned positive and negative model as important\nweight for token-level DPO learning.Experimental results on GSM8K and MATH500\nbenchmarks with two-widely used models Llama-3 (8B and 70B) and deepseek-math\n(7B) demonstrate the effectiveness of the propsoed approach cDPO.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have exhibited remarkable performance on\nreasoning tasks. They utilize autoregressive token generation to construct\nreasoning trajectories, enabling the development of a coherent chain of\nthought. In this work, we explore the impact of individual tokens on the final\noutcomes of reasoning tasks. We identify the existence of ``critical tokens''\nthat lead to incorrect reasoning trajectories in LLMs. Specifically, we find\nthat LLMs tend to produce positive outcomes when forced to decode other tokens\ninstead of critical tokens. Motivated by this observation, we propose a novel\napproach - cDPO - designed to automatically recognize and conduct token-level\nrewards for the critical tokens during the alignment process. Specifically, we\ndevelop a contrastive estimation approach to automatically identify critical\ntokens. It is achieved by comparing the generation likelihood of positive and\nnegative models. To achieve this, we separately fine-tune the positive and\nnegative models on various reasoning trajectories, consequently, they are\ncapable of identifying identify critical tokens within incorrect trajectories\nthat contribute to erroneous outcomes. Moreover, to further align the model\nwith the critical token information during the alignment process, we extend the\nconventional DPO algorithms to token-level DPO and utilize the differential\nlikelihood from the aforementioned positive and negative model as important\nweight for token-level DPO learning.Experimental results on GSM8K and MATH500\nbenchmarks with two-widely used models Llama-3 (8B and 70B) and deepseek-math\n(7B) demonstrate the effectiveness of the propsoed approach cDPO."
                },
                "authors": [
                    {
                        "name": "Zicheng Lin"
                    },
                    {
                        "name": "Tian Liang"
                    },
                    {
                        "name": "Jiahao Xu"
                    },
                    {
                        "name": "Xing Wang"
                    },
                    {
                        "name": "Ruilin Luo"
                    },
                    {
                        "name": "Chufan Shi"
                    },
                    {
                        "name": "Siheng Li"
                    },
                    {
                        "name": "Yujiu Yang"
                    },
                    {
                        "name": "Zhaopeng Tu"
                    }
                ],
                "author_detail": {
                    "name": "Zhaopeng Tu"
                },
                "author": "Zhaopeng Tu",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19943v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19943v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19921v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19921v1",
                "updated": "2024-11-29T18:36:15Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    18,
                    36,
                    15,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T18:36:15Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    18,
                    36,
                    15,
                    4,
                    334,
                    0
                ],
                "title": "SIMS: Simulating Human-Scene Interactions with Real World Script\n  Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SIMS: Simulating Human-Scene Interactions with Real World Script\n  Planning"
                },
                "summary": "Simulating long-term human-scene interaction is a challenging yet fascinating\ntask. Previous works have not effectively addressed the generation of long-term\nhuman scene interactions with detailed narratives for physics-based animation.\nThis paper introduces a novel framework for the planning and controlling of\nlong-horizon physical plausible human-scene interaction. On the one hand, films\nand shows with stylish human locomotions or interactions with scenes are\nabundantly available on the internet, providing a rich source of data for\nscript planning. On the other hand, Large Language Models (LLMs) can understand\nand generate logical storylines.\n  This motivates us to marry the two by using an LLM-based pipeline to extract\nscripts from videos, and then employ LLMs to imitate and create new scripts,\ncapturing complex, time-series human behaviors and interactions with\nenvironments. By leveraging this, we utilize a dual-aware policy that achieves\nboth language comprehension and scene understanding to guide character motions\nwithin contextual and spatial constraints. To facilitate training and\nevaluation, we contribute a comprehensive planning dataset containing diverse\nmotion sequences extracted from real-world videos and expand them with large\nlanguage models. We also collect and re-annotate motion clips from existing\nkinematic datasets to enable our policy learn diverse skills. Extensive\nexperiments demonstrate the effectiveness of our framework in versatile task\nexecution and its generalization ability to various scenarios, showing\nremarkably enhanced performance compared with existing methods. Our code and\ndata will be publicly available soon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulating long-term human-scene interaction is a challenging yet fascinating\ntask. Previous works have not effectively addressed the generation of long-term\nhuman scene interactions with detailed narratives for physics-based animation.\nThis paper introduces a novel framework for the planning and controlling of\nlong-horizon physical plausible human-scene interaction. On the one hand, films\nand shows with stylish human locomotions or interactions with scenes are\nabundantly available on the internet, providing a rich source of data for\nscript planning. On the other hand, Large Language Models (LLMs) can understand\nand generate logical storylines.\n  This motivates us to marry the two by using an LLM-based pipeline to extract\nscripts from videos, and then employ LLMs to imitate and create new scripts,\ncapturing complex, time-series human behaviors and interactions with\nenvironments. By leveraging this, we utilize a dual-aware policy that achieves\nboth language comprehension and scene understanding to guide character motions\nwithin contextual and spatial constraints. To facilitate training and\nevaluation, we contribute a comprehensive planning dataset containing diverse\nmotion sequences extracted from real-world videos and expand them with large\nlanguage models. We also collect and re-annotate motion clips from existing\nkinematic datasets to enable our policy learn diverse skills. Extensive\nexperiments demonstrate the effectiveness of our framework in versatile task\nexecution and its generalization ability to various scenarios, showing\nremarkably enhanced performance compared with existing methods. Our code and\ndata will be publicly available soon."
                },
                "authors": [
                    {
                        "name": "Wenjia Wang"
                    },
                    {
                        "name": "Liang Pan"
                    },
                    {
                        "name": "Zhiyang Dou"
                    },
                    {
                        "name": "Zhouyingcheng Liao"
                    },
                    {
                        "name": "Yuke Lou"
                    },
                    {
                        "name": "Lei Yang"
                    },
                    {
                        "name": "Jingbo Wang"
                    },
                    {
                        "name": "Taku Komura"
                    }
                ],
                "author_detail": {
                    "name": "Taku Komura"
                },
                "author": "Taku Komura",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19921v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19921v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19886v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19886v1",
                "updated": "2024-11-29T17:52:39Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    17,
                    52,
                    39,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T17:52:39Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    17,
                    52,
                    39,
                    4,
                    334,
                    0
                ],
                "title": "PDDLFuse: A Tool for Generating Diverse Planning Domains",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PDDLFuse: A Tool for Generating Diverse Planning Domains"
                },
                "summary": "Various real-world challenges require planning algorithms that can adapt to a\nbroad range of domains. Traditionally, the creation of planning domains has\nrelied heavily on human implementation, which limits the scale and diversity of\navailable domains. While recent advancements have leveraged generative AI\ntechnologies such as large language models (LLMs) for domain creation, these\nefforts have predominantly focused on translating existing domains from natural\nlanguage descriptions rather than generating novel ones. In contrast, the\nconcept of domain randomization, which has been highly effective in\nreinforcement learning, enhances performance and generalizability by training\non a diverse array of randomized new domains. Inspired by this success, our\ntool, PDDLFuse, aims to bridge this gap in Planning Domain Definition Language\n(PDDL). PDDLFuse is designed to generate new, diverse planning domains that can\nbe used to validate new planners or test foundational planning models. We have\ndeveloped methods to adjust the domain generators parameters to modulate the\ndifficulty of the domains it generates. This adaptability is crucial as\nexisting domain-independent planners often struggle with more complex problems.\nInitial tests indicate that PDDLFuse efficiently creates intricate and varied\ndomains, representing a significant advancement over traditional domain\ngeneration methods and making a contribution towards planning research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Various real-world challenges require planning algorithms that can adapt to a\nbroad range of domains. Traditionally, the creation of planning domains has\nrelied heavily on human implementation, which limits the scale and diversity of\navailable domains. While recent advancements have leveraged generative AI\ntechnologies such as large language models (LLMs) for domain creation, these\nefforts have predominantly focused on translating existing domains from natural\nlanguage descriptions rather than generating novel ones. In contrast, the\nconcept of domain randomization, which has been highly effective in\nreinforcement learning, enhances performance and generalizability by training\non a diverse array of randomized new domains. Inspired by this success, our\ntool, PDDLFuse, aims to bridge this gap in Planning Domain Definition Language\n(PDDL). PDDLFuse is designed to generate new, diverse planning domains that can\nbe used to validate new planners or test foundational planning models. We have\ndeveloped methods to adjust the domain generators parameters to modulate the\ndifficulty of the domains it generates. This adaptability is crucial as\nexisting domain-independent planners often struggle with more complex problems.\nInitial tests indicate that PDDLFuse efficiently creates intricate and varied\ndomains, representing a significant advancement over traditional domain\ngeneration methods and making a contribution towards planning research."
                },
                "authors": [
                    {
                        "name": "Vedant Khandelwal"
                    },
                    {
                        "name": "Amit Sheth"
                    },
                    {
                        "name": "Forest Agostinelli"
                    }
                ],
                "author_detail": {
                    "name": "Forest Agostinelli"
                },
                "author": "Forest Agostinelli",
                "arxiv_comment": "218 Tables, 3 Figures, 4 Algorithms",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19886v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19886v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19876v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19876v1",
                "updated": "2024-11-29T17:38:56Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    17,
                    38,
                    56,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T17:38:56Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    17,
                    38,
                    56,
                    4,
                    334,
                    0
                ],
                "title": "LUMIA: Linear probing for Unimodal and MultiModal Membership Inference\n  A!acks leveraging internal LLM states",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LUMIA: Linear probing for Unimodal and MultiModal Membership Inference\n  A!acks leveraging internal LLM states"
                },
                "summary": "Large Language Models (LLMs) are increasingly used in a variety of\napplications, but concerns around membership inference have grown in parallel.\nPrevious efforts focus on black-to-grey-box models, thus neglecting the\npotential benefit from internal LLM information. To address this, we propose\nthe use of Linear Probes (LPs) as a method to detect Membership Inference\nAttacks (MIAs) by examining internal activations of LLMs. Our approach, dubbed\nLUMIA, applies LPs layer-by-layer to get fine-grained data on the model inner\nworkings. We test this method across several model architectures, sizes and\ndatasets, including unimodal and multimodal tasks. In unimodal MIA, LUMIA\nachieves an average gain of 15.71 % in Area Under the Curve (AUC) over previous\ntechniques. Remarkably, LUMIA reaches AUC>60% in 65.33% of cases -- an\nincrement of 46.80% against the state of the art. Furthermore, our approach\nreveals key insights, such as the model layers where MIAs are most detectable.\nIn multimodal models, LPs indicate that visual inputs can significantly\ncontribute to detect MIAs -- AUC>60% is reached in 85.90% of experiments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly used in a variety of\napplications, but concerns around membership inference have grown in parallel.\nPrevious efforts focus on black-to-grey-box models, thus neglecting the\npotential benefit from internal LLM information. To address this, we propose\nthe use of Linear Probes (LPs) as a method to detect Membership Inference\nAttacks (MIAs) by examining internal activations of LLMs. Our approach, dubbed\nLUMIA, applies LPs layer-by-layer to get fine-grained data on the model inner\nworkings. We test this method across several model architectures, sizes and\ndatasets, including unimodal and multimodal tasks. In unimodal MIA, LUMIA\nachieves an average gain of 15.71 % in Area Under the Curve (AUC) over previous\ntechniques. Remarkably, LUMIA reaches AUC>60% in 65.33% of cases -- an\nincrement of 46.80% against the state of the art. Furthermore, our approach\nreveals key insights, such as the model layers where MIAs are most detectable.\nIn multimodal models, LPs indicate that visual inputs can significantly\ncontribute to detect MIAs -- AUC>60% is reached in 85.90% of experiments."
                },
                "authors": [
                    {
                        "name": "Luis Ibanez-Lissen"
                    },
                    {
                        "name": "Lorena Gonzalez-Manzano"
                    },
                    {
                        "name": "Jose Maria de Fuentes"
                    },
                    {
                        "name": "Nicolas Anciaux"
                    },
                    {
                        "name": "Joaquin Garcia-Alfaro"
                    }
                ],
                "author_detail": {
                    "name": "Joaquin Garcia-Alfaro"
                },
                "author": "Joaquin Garcia-Alfaro",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19876v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19876v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19869v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19869v1",
                "updated": "2024-11-29T17:31:42Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    17,
                    31,
                    42,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T17:31:42Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    17,
                    31,
                    42,
                    4,
                    334,
                    0
                ],
                "title": "AIDetx: a compression-based method for identification of\n  machine-learning generated text",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AIDetx: a compression-based method for identification of\n  machine-learning generated text"
                },
                "summary": "This paper introduces AIDetx, a novel method for detecting machine-generated\ntext using data compression techniques. Traditional approaches, such as deep\nlearning classifiers, often suffer from high computational costs and limited\ninterpretability. To address these limitations, we propose a compression-based\nclassification framework that leverages finite-context models (FCMs). AIDetx\nconstructs distinct compression models for human-written and AI-generated text,\nclassifying new inputs based on which model achieves a higher compression\nratio. We evaluated AIDetx on two benchmark datasets, achieving F1 scores\nexceeding 97% and 99%, respectively, highlighting its high accuracy. Compared\nto current methods, such as large language models (LLMs), AIDetx offers a more\ninterpretable and computationally efficient solution, significantly reducing\nboth training time and hardware requirements (e.g., no GPUs needed). The full\nimplementation is publicly available at https://github.com/AIDetx/AIDetx.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces AIDetx, a novel method for detecting machine-generated\ntext using data compression techniques. Traditional approaches, such as deep\nlearning classifiers, often suffer from high computational costs and limited\ninterpretability. To address these limitations, we propose a compression-based\nclassification framework that leverages finite-context models (FCMs). AIDetx\nconstructs distinct compression models for human-written and AI-generated text,\nclassifying new inputs based on which model achieves a higher compression\nratio. We evaluated AIDetx on two benchmark datasets, achieving F1 scores\nexceeding 97% and 99%, respectively, highlighting its high accuracy. Compared\nto current methods, such as large language models (LLMs), AIDetx offers a more\ninterpretable and computationally efficient solution, significantly reducing\nboth training time and hardware requirements (e.g., no GPUs needed). The full\nimplementation is publicly available at https://github.com/AIDetx/AIDetx."
                },
                "authors": [
                    {
                        "name": "Leonardo Almeida"
                    },
                    {
                        "name": "Pedro Rodrigues"
                    },
                    {
                        "name": "Diogo MagalhÃ£es"
                    },
                    {
                        "name": "Armando J. Pinho"
                    },
                    {
                        "name": "Diogo Pratas"
                    }
                ],
                "author_detail": {
                    "name": "Diogo Pratas"
                },
                "author": "Diogo Pratas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19869v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19869v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19865v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19865v1",
                "updated": "2024-11-29T17:27:05Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    17,
                    27,
                    5,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T17:27:05Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    17,
                    27,
                    5,
                    4,
                    334,
                    0
                ],
                "title": "Reverse Thinking Makes LLMs Stronger Reasoners",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reverse Thinking Makes LLMs Stronger Reasoners"
                },
                "summary": "Reverse thinking plays a crucial role in human reasoning. Humans can reason\nnot only from a problem to a solution but also in reverse, i.e., start from the\nsolution and reason towards the problem. This often enhances overall reasoning\nperformance as it enables consistency checks between their forward and backward\nthinking. To enable Large Language Models (LLMs) to perform reverse thinking,\nwe introduce Reverse-Enhanced Thinking (RevThink), a framework composed of data\naugmentation and learning objectives. In RevThink, we augment the dataset by\ncollecting structured forward-backward reasoning from a teacher model,\nconsisting of: (1) the original question, (2) forward reasoning, (3) backward\nquestion, and (4) backward reasoning. We then employ three objectives to train\na smaller student model in a multi-task learning fashion: (a) generate forward\nreasoning from a question, (b) generate a backward question from a question,\nand (c) generate backward reasoning from the backward question. Experiments\nacross 12 datasets covering commonsense, math, and logical reasoning show an\naverage 13.53% improvement over the student model's zero-shot performance and a\n6.84% improvement over the strongest knowledge distillation baselines.\nMoreover, our method demonstrates sample efficiency -- using only 10% of the\ncorrect forward reasoning from the training data, it outperforms a standard\nfine-tuning method trained on 10x more forward reasoning. RevThink also\nexhibits strong generalization to out-of-distribution held-out datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reverse thinking plays a crucial role in human reasoning. Humans can reason\nnot only from a problem to a solution but also in reverse, i.e., start from the\nsolution and reason towards the problem. This often enhances overall reasoning\nperformance as it enables consistency checks between their forward and backward\nthinking. To enable Large Language Models (LLMs) to perform reverse thinking,\nwe introduce Reverse-Enhanced Thinking (RevThink), a framework composed of data\naugmentation and learning objectives. In RevThink, we augment the dataset by\ncollecting structured forward-backward reasoning from a teacher model,\nconsisting of: (1) the original question, (2) forward reasoning, (3) backward\nquestion, and (4) backward reasoning. We then employ three objectives to train\na smaller student model in a multi-task learning fashion: (a) generate forward\nreasoning from a question, (b) generate a backward question from a question,\nand (c) generate backward reasoning from the backward question. Experiments\nacross 12 datasets covering commonsense, math, and logical reasoning show an\naverage 13.53% improvement over the student model's zero-shot performance and a\n6.84% improvement over the strongest knowledge distillation baselines.\nMoreover, our method demonstrates sample efficiency -- using only 10% of the\ncorrect forward reasoning from the training data, it outperforms a standard\nfine-tuning method trained on 10x more forward reasoning. RevThink also\nexhibits strong generalization to out-of-distribution held-out datasets."
                },
                "authors": [
                    {
                        "name": "Justin Chih-Yao Chen"
                    },
                    {
                        "name": "Zifeng Wang"
                    },
                    {
                        "name": "Hamid Palangi"
                    },
                    {
                        "name": "Rujun Han"
                    },
                    {
                        "name": "Sayna Ebrahimi"
                    },
                    {
                        "name": "Long Le"
                    },
                    {
                        "name": "Vincent Perot"
                    },
                    {
                        "name": "Swaroop Mishra"
                    },
                    {
                        "name": "Mohit Bansal"
                    },
                    {
                        "name": "Chen-Yu Lee"
                    },
                    {
                        "name": "Tomas Pfister"
                    }
                ],
                "author_detail": {
                    "name": "Tomas Pfister"
                },
                "author": "Tomas Pfister",
                "arxiv_comment": "20 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19865v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19865v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19862v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19862v1",
                "updated": "2024-11-29T17:25:00Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    17,
                    25,
                    0,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T17:25:00Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    17,
                    25,
                    0,
                    4,
                    334,
                    0
                ],
                "title": "Cross-Domain Recommendation Meets Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-Domain Recommendation Meets Large Language Models"
                },
                "summary": "Cross-domain recommendation (CDR) has emerged as a promising solution to the\ncold-start problem, faced by single-domain recommender systems. However,\nexisting CDR models rely on complex neural architectures, large datasets, and\nsignificant computational resources, making them less effective in data-scarce\nscenarios or when simplicity is crucial. In this work, we leverage the\nreasoning capabilities of large language models (LLMs) and explore their\nperformance in the CDR domain across multiple domain pairs. We introduce two\nnovel prompt designs tailored for CDR and demonstrate that LLMs, when prompted\neffectively, outperform state-of-the-art CDR baselines across various metrics\nand domain combinations in the rating prediction and ranking tasks. This work\nbridges the gap between LLMs and recommendation systems, showcasing their\npotential as effective cross-domain recommenders.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-domain recommendation (CDR) has emerged as a promising solution to the\ncold-start problem, faced by single-domain recommender systems. However,\nexisting CDR models rely on complex neural architectures, large datasets, and\nsignificant computational resources, making them less effective in data-scarce\nscenarios or when simplicity is crucial. In this work, we leverage the\nreasoning capabilities of large language models (LLMs) and explore their\nperformance in the CDR domain across multiple domain pairs. We introduce two\nnovel prompt designs tailored for CDR and demonstrate that LLMs, when prompted\neffectively, outperform state-of-the-art CDR baselines across various metrics\nand domain combinations in the rating prediction and ranking tasks. This work\nbridges the gap between LLMs and recommendation systems, showcasing their\npotential as effective cross-domain recommenders."
                },
                "authors": [
                    {
                        "name": "Ajay Krishna Vajjala"
                    },
                    {
                        "name": "Dipak Meher"
                    },
                    {
                        "name": "Ziwei Zhu"
                    },
                    {
                        "name": "David S. Rosenblum"
                    }
                ],
                "author_detail": {
                    "name": "David S. Rosenblum"
                },
                "author": "David S. Rosenblum",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19862v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19862v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15623v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15623v2",
                "updated": "2024-11-29T17:18:49Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    17,
                    18,
                    49,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-23T18:27:35Z",
                "published_parsed": [
                    2024,
                    11,
                    23,
                    18,
                    27,
                    35,
                    5,
                    328,
                    0
                ],
                "title": "Multi-label Sequential Sentence Classification via Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-label Sequential Sentence Classification via Large Language Model"
                },
                "summary": "Sequential sentence classification (SSC) in scientific publications is\ncrucial for supporting downstream tasks such as fine-grained information\nretrieval and extractive summarization. However, current SSC methods are\nconstrained by model size, sequence length, and single-label setting. To\naddress these limitations, this paper proposes LLM-SSC, a large language model\n(LLM)-based framework for both single- and multi-label SSC tasks. Unlike\nprevious approaches that employ small- or medium-sized language models, the\nproposed framework utilizes LLMs to generate SSC labels through designed\nprompts, which enhance task understanding by incorporating demonstrations and a\nquery to describe the prediction target. We also present a multi-label\ncontrastive learning loss with auto-weighting scheme, enabling the multi-label\nclassification task. To support our multi-label SSC analysis, we introduce and\nrelease a new dataset, biorc800, which mainly contains unstructured abstracts\nin the biomedical domain with manual annotations. Experiments demonstrate\nLLM-SSC's strong performance in SSC under both in-context learning and\ntask-specific tuning settings. We release biorc800 and our code at:\nhttps://github.com/ScienceNLP-Lab/LLM-SSC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequential sentence classification (SSC) in scientific publications is\ncrucial for supporting downstream tasks such as fine-grained information\nretrieval and extractive summarization. However, current SSC methods are\nconstrained by model size, sequence length, and single-label setting. To\naddress these limitations, this paper proposes LLM-SSC, a large language model\n(LLM)-based framework for both single- and multi-label SSC tasks. Unlike\nprevious approaches that employ small- or medium-sized language models, the\nproposed framework utilizes LLMs to generate SSC labels through designed\nprompts, which enhance task understanding by incorporating demonstrations and a\nquery to describe the prediction target. We also present a multi-label\ncontrastive learning loss with auto-weighting scheme, enabling the multi-label\nclassification task. To support our multi-label SSC analysis, we introduce and\nrelease a new dataset, biorc800, which mainly contains unstructured abstracts\nin the biomedical domain with manual annotations. Experiments demonstrate\nLLM-SSC's strong performance in SSC under both in-context learning and\ntask-specific tuning settings. We release biorc800 and our code at:\nhttps://github.com/ScienceNLP-Lab/LLM-SSC."
                },
                "authors": [
                    {
                        "name": "Mengfei Lan"
                    },
                    {
                        "name": "Lecheng Zheng"
                    },
                    {
                        "name": "Shufan Ming"
                    },
                    {
                        "name": "Halil Kilicoglu"
                    }
                ],
                "author_detail": {
                    "name": "Halil Kilicoglu"
                },
                "author": "Halil Kilicoglu",
                "arxiv_comment": "Accepted by EMNLP 2024 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15623v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15623v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19832v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19832v1",
                "updated": "2024-11-29T16:44:02Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    16,
                    44,
                    2,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T16:44:02Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    16,
                    44,
                    2,
                    4,
                    334,
                    0
                ],
                "title": "Sensitive Content Classification in Social Media: A Holistic Resource\n  and Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sensitive Content Classification in Social Media: A Holistic Resource\n  and Evaluation"
                },
                "summary": "The detection of sensitive content in large datasets is crucial for ensuring\nthat shared and analysed data is free from harmful material. However, current\nmoderation tools, such as external APIs, suffer from limitations in\ncustomisation, accuracy across diverse sensitive categories, and privacy\nconcerns. Additionally, existing datasets and open-source models focus\npredominantly on toxic language, leaving gaps in detecting other sensitive\ncategories such as substance abuse or self-harm. In this paper, we put forward\na unified dataset tailored for social media content moderation across six\nsensitive categories: conflictual language, profanity, sexually explicit\nmaterial, drug-related content, self-harm, and spam. By collecting and\nannotating data with consistent retrieval strategies and guidelines, we address\nthe shortcomings of previous focalised research. Our analysis demonstrates that\nfine-tuning large language models (LLMs) on this novel dataset yields\nsignificant improvements in detection performance compared to open\noff-the-shelf models such as LLaMA, and even proprietary OpenAI models, which\nunderperform by 10-15% overall. This limitation is even more pronounced on\npopular moderation APIs, which cannot be easily tailored to specific sensitive\ncontent categories, among others.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The detection of sensitive content in large datasets is crucial for ensuring\nthat shared and analysed data is free from harmful material. However, current\nmoderation tools, such as external APIs, suffer from limitations in\ncustomisation, accuracy across diverse sensitive categories, and privacy\nconcerns. Additionally, existing datasets and open-source models focus\npredominantly on toxic language, leaving gaps in detecting other sensitive\ncategories such as substance abuse or self-harm. In this paper, we put forward\na unified dataset tailored for social media content moderation across six\nsensitive categories: conflictual language, profanity, sexually explicit\nmaterial, drug-related content, self-harm, and spam. By collecting and\nannotating data with consistent retrieval strategies and guidelines, we address\nthe shortcomings of previous focalised research. Our analysis demonstrates that\nfine-tuning large language models (LLMs) on this novel dataset yields\nsignificant improvements in detection performance compared to open\noff-the-shelf models such as LLaMA, and even proprietary OpenAI models, which\nunderperform by 10-15% overall. This limitation is even more pronounced on\npopular moderation APIs, which cannot be easily tailored to specific sensitive\ncontent categories, among others."
                },
                "authors": [
                    {
                        "name": "Dimosthenis Antypas"
                    },
                    {
                        "name": "Indira Sen"
                    },
                    {
                        "name": "Carla Perez-Almendros"
                    },
                    {
                        "name": "Jose Camacho-Collados"
                    },
                    {
                        "name": "Francesco Barbieri"
                    }
                ],
                "author_detail": {
                    "name": "Francesco Barbieri"
                },
                "author": "Francesco Barbieri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19832v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19832v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.18653v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.18653v2",
                "updated": "2024-11-29T16:19:01Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    16,
                    19,
                    1,
                    4,
                    334,
                    0
                ],
                "published": "2024-05-28T23:32:46Z",
                "published_parsed": [
                    2024,
                    5,
                    28,
                    23,
                    32,
                    46,
                    1,
                    149,
                    0
                ],
                "title": "Recent Advances of Foundation Language Models-based Continual Learning:\n  A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent Advances of Foundation Language Models-based Continual Learning:\n  A Survey"
                },
                "summary": "Recently, foundation language models (LMs) have marked significant\nachievements in the domains of natural language processing (NLP) and computer\nvision (CV). Unlike traditional neural network models, foundation LMs obtain a\ngreat ability for transfer learning by acquiring rich commonsense knowledge\nthrough pre-training on extensive unsupervised datasets with a vast number of\nparameters. However, they still can not emulate human-like continuous learning\ndue to catastrophic forgetting. Consequently, various continual learning\n(CL)-based methodologies have been developed to refine LMs, enabling them to\nadapt to new tasks without forgetting previous knowledge. However, a systematic\ntaxonomy of existing approaches and a comparison of their performance are still\nlacking, which is the gap that our survey aims to fill. We delve into a\ncomprehensive review, summarization, and classification of the existing\nliterature on CL-based approaches applied to foundation language models, such\nas pre-trained language models (PLMs), large language models (LLMs) and\nvision-language models (VLMs). We divide these studies into offline CL and\nonline CL, which consist of traditional methods, parameter-efficient-based\nmethods, instruction tuning-based methods and continual pre-training methods.\nOffline CL encompasses domain-incremental learning, task-incremental learning,\nand class-incremental learning, while online CL is subdivided into hard task\nboundary and blurry task boundary settings. Additionally, we outline the\ntypical datasets and metrics employed in CL research and provide a detailed\nanalysis of the challenges and future work for LMs-based continual learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, foundation language models (LMs) have marked significant\nachievements in the domains of natural language processing (NLP) and computer\nvision (CV). Unlike traditional neural network models, foundation LMs obtain a\ngreat ability for transfer learning by acquiring rich commonsense knowledge\nthrough pre-training on extensive unsupervised datasets with a vast number of\nparameters. However, they still can not emulate human-like continuous learning\ndue to catastrophic forgetting. Consequently, various continual learning\n(CL)-based methodologies have been developed to refine LMs, enabling them to\nadapt to new tasks without forgetting previous knowledge. However, a systematic\ntaxonomy of existing approaches and a comparison of their performance are still\nlacking, which is the gap that our survey aims to fill. We delve into a\ncomprehensive review, summarization, and classification of the existing\nliterature on CL-based approaches applied to foundation language models, such\nas pre-trained language models (PLMs), large language models (LLMs) and\nvision-language models (VLMs). We divide these studies into offline CL and\nonline CL, which consist of traditional methods, parameter-efficient-based\nmethods, instruction tuning-based methods and continual pre-training methods.\nOffline CL encompasses domain-incremental learning, task-incremental learning,\nand class-incremental learning, while online CL is subdivided into hard task\nboundary and blurry task boundary settings. Additionally, we outline the\ntypical datasets and metrics employed in CL research and provide a detailed\nanalysis of the challenges and future work for LMs-based continual learning."
                },
                "authors": [
                    {
                        "name": "Yutao Yang"
                    },
                    {
                        "name": "Jie Zhou"
                    },
                    {
                        "name": "Xuanwen Ding"
                    },
                    {
                        "name": "Tianyu Huai"
                    },
                    {
                        "name": "Shunyu Liu"
                    },
                    {
                        "name": "Qin Chen"
                    },
                    {
                        "name": "Yuan Xie"
                    },
                    {
                        "name": "Liang He"
                    }
                ],
                "author_detail": {
                    "name": "Liang He"
                },
                "author": "Liang He",
                "arxiv_comment": "Accepted by ACM Computing Survey",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.18653v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.18653v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08130v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08130v2",
                "updated": "2024-11-29T16:18:29Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    16,
                    18,
                    29,
                    4,
                    334,
                    0
                ],
                "published": "2024-10-10T17:14:36Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    17,
                    14,
                    36,
                    3,
                    284,
                    0
                ],
                "title": "Think Beyond Size: Adaptive Prompting for More Effective Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Think Beyond Size: Adaptive Prompting for More Effective Reasoning"
                },
                "summary": "Pretrained large language models (LLMs) are increasingly utilized across a\nwide range of natural language processing (NLP) tasks due to their impressive\ncapabilities as few-shot learners. Recent techniques, such as chain-of-thought\n(CoT) prompting, have significantly advanced multi-step reasoning by\nintroducing step-by-step decomposition, achieving state-of-the-art results on\ncomplex reasoning benchmarks. However, these approaches often rely on static\nprompting templates that do not adapt to task complexity or errors during the\nreasoning process. In this work, we introduce Adaptive Prompting, a dynamic and\niterative framework designed to enhance reasoning by incorporating real-time\nadjustments to prompt structures and validation mechanisms.Experimental results\ndemonstrate that Adaptive Prompting significantly improves performance on\ndiverse reasoning benchmarks, including arithmetic reasoning (GSM8K,\nMultiArith), logical reasoning and commonsense tasks, achieving substantial\naccuracy gains compared to static prompting baselines. By integrating guided\nprompts, intermediate validation, and self-corrective steps, our approach\nenables smaller models to achieve competitive performance with larger\ncounterparts, such as GPT-4, while maintaining computational efficiency. The\nframework achieves this without requiring fine-tuning or task-specific training\ndata, highlighting the untapped potential of iterative reasoning methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pretrained large language models (LLMs) are increasingly utilized across a\nwide range of natural language processing (NLP) tasks due to their impressive\ncapabilities as few-shot learners. Recent techniques, such as chain-of-thought\n(CoT) prompting, have significantly advanced multi-step reasoning by\nintroducing step-by-step decomposition, achieving state-of-the-art results on\ncomplex reasoning benchmarks. However, these approaches often rely on static\nprompting templates that do not adapt to task complexity or errors during the\nreasoning process. In this work, we introduce Adaptive Prompting, a dynamic and\niterative framework designed to enhance reasoning by incorporating real-time\nadjustments to prompt structures and validation mechanisms.Experimental results\ndemonstrate that Adaptive Prompting significantly improves performance on\ndiverse reasoning benchmarks, including arithmetic reasoning (GSM8K,\nMultiArith), logical reasoning and commonsense tasks, achieving substantial\naccuracy gains compared to static prompting baselines. By integrating guided\nprompts, intermediate validation, and self-corrective steps, our approach\nenables smaller models to achieve competitive performance with larger\ncounterparts, such as GPT-4, while maintaining computational efficiency. The\nframework achieves this without requiring fine-tuning or task-specific training\ndata, highlighting the untapped potential of iterative reasoning methods."
                },
                "authors": [
                    {
                        "name": "Kamesh R"
                    }
                ],
                "author_detail": {
                    "name": "Kamesh R"
                },
                "author": "Kamesh R",
                "arxiv_comment": "Submitted to ICLR 2025. This is a preprint version. Future revisions\n  will include additional evaluations and refinements",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08130v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08130v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19804v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19804v1",
                "updated": "2024-11-29T16:09:43Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    16,
                    9,
                    43,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T16:09:43Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    16,
                    9,
                    43,
                    4,
                    334,
                    0
                ],
                "title": "Advanced System Integration: Analyzing OpenAPI Chunking for\n  Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advanced System Integration: Analyzing OpenAPI Chunking for\n  Retrieval-Augmented Generation"
                },
                "summary": "Integrating multiple (sub-)systems is essential to create advanced\nInformation Systems (ISs). Difficulties mainly arise when integrating dynamic\nenvironments across the IS lifecycle. A traditional approach is a registry that\nprovides the API documentation of the systems' endpoints. Large Language Models\n(LLMs) have shown to be capable of automatically creating system integrations\n(e.g., as service composition) based on this documentation but require concise\ninput due to input token limitations, especially regarding comprehensive API\ndescriptions. Currently, it is unknown how best to preprocess these API\ndescriptions. Within this work, we (i) analyze the usage of Retrieval Augmented\nGeneration (RAG) for endpoint discovery and the chunking, i.e., preprocessing,\nof OpenAPIs to reduce the input token length while preserving the most relevant\ninformation. To further reduce the input token length for the composition\nprompt and improve endpoint retrieval, we propose (ii) a Discovery Agent that\nonly receives a summary of the most relevant endpoints and retrieves details on\ndemand. We evaluate RAG for endpoint discovery using the RestBench benchmark,\nfirst, for the different chunking possibilities and parameters measuring the\nendpoint retrieval recall, precision, and F1 score. Then, we assess the\nDiscovery Agent using the same test set. With our prototype, we demonstrate how\nto successfully employ RAG for endpoint discovery to reduce the token count.\nWhile revealing high values for recall, precision, and F1, further research is\nnecessary to retrieve all requisite endpoints. Our experiments show that for\npreprocessing, LLM-based and format-specific approaches outperform na\\\"ive\nchunking methods. Relying on an agent further enhances these results as the\nagent splits the tasks into multiple fine granular subtasks, improving the\noverall RAG performance in the token count, precision, and F1 score.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating multiple (sub-)systems is essential to create advanced\nInformation Systems (ISs). Difficulties mainly arise when integrating dynamic\nenvironments across the IS lifecycle. A traditional approach is a registry that\nprovides the API documentation of the systems' endpoints. Large Language Models\n(LLMs) have shown to be capable of automatically creating system integrations\n(e.g., as service composition) based on this documentation but require concise\ninput due to input token limitations, especially regarding comprehensive API\ndescriptions. Currently, it is unknown how best to preprocess these API\ndescriptions. Within this work, we (i) analyze the usage of Retrieval Augmented\nGeneration (RAG) for endpoint discovery and the chunking, i.e., preprocessing,\nof OpenAPIs to reduce the input token length while preserving the most relevant\ninformation. To further reduce the input token length for the composition\nprompt and improve endpoint retrieval, we propose (ii) a Discovery Agent that\nonly receives a summary of the most relevant endpoints and retrieves details on\ndemand. We evaluate RAG for endpoint discovery using the RestBench benchmark,\nfirst, for the different chunking possibilities and parameters measuring the\nendpoint retrieval recall, precision, and F1 score. Then, we assess the\nDiscovery Agent using the same test set. With our prototype, we demonstrate how\nto successfully employ RAG for endpoint discovery to reduce the token count.\nWhile revealing high values for recall, precision, and F1, further research is\nnecessary to retrieve all requisite endpoints. Our experiments show that for\npreprocessing, LLM-based and format-specific approaches outperform na\\\"ive\nchunking methods. Relying on an agent further enhances these results as the\nagent splits the tasks into multiple fine granular subtasks, improving the\noverall RAG performance in the token count, precision, and F1 score."
                },
                "authors": [
                    {
                        "name": "Robin D. Pesl"
                    },
                    {
                        "name": "Jerin G. Mathew"
                    },
                    {
                        "name": "Massimo Mecella"
                    },
                    {
                        "name": "Marco Aiello"
                    }
                ],
                "author_detail": {
                    "name": "Marco Aiello"
                },
                "author": "Marco Aiello",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19804v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19804v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19799v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19799v1",
                "updated": "2024-11-29T16:03:14Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    16,
                    3,
                    14,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T16:03:14Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    16,
                    3,
                    14,
                    4,
                    334,
                    0
                ],
                "title": "INCLUDE: Evaluating Multilingual Language Understanding with Regional\n  Knowledge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "INCLUDE: Evaluating Multilingual Language Understanding with Regional\n  Knowledge"
                },
                "summary": "The performance differential of large language models (LLM) between languages\nhinders their effective deployment in many regions, inhibiting the potential\neconomic and societal value of generative AI tools in many communities.\nHowever, the development of functional LLMs in many languages (\\ie,\nmultilingual LLMs) is bottlenecked by the lack of high-quality evaluation\nresources in languages other than English. Moreover, current practices in\nmultilingual benchmark construction often translate English resources, ignoring\nthe regional and cultural knowledge of the environments in which multilingual\nsystems would be used. In this work, we construct an evaluation suite of\n197,243 QA pairs from local exam sources to measure the capabilities of\nmultilingual LLMs in a variety of regional contexts. Our novel resource,\nINCLUDE, is a comprehensive knowledge- and reasoning-centric benchmark across\n44 written languages that evaluates multilingual LLMs for performance in the\nactual language environments where they would be deployed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The performance differential of large language models (LLM) between languages\nhinders their effective deployment in many regions, inhibiting the potential\neconomic and societal value of generative AI tools in many communities.\nHowever, the development of functional LLMs in many languages (\\ie,\nmultilingual LLMs) is bottlenecked by the lack of high-quality evaluation\nresources in languages other than English. Moreover, current practices in\nmultilingual benchmark construction often translate English resources, ignoring\nthe regional and cultural knowledge of the environments in which multilingual\nsystems would be used. In this work, we construct an evaluation suite of\n197,243 QA pairs from local exam sources to measure the capabilities of\nmultilingual LLMs in a variety of regional contexts. Our novel resource,\nINCLUDE, is a comprehensive knowledge- and reasoning-centric benchmark across\n44 written languages that evaluates multilingual LLMs for performance in the\nactual language environments where they would be deployed."
                },
                "authors": [
                    {
                        "name": "Angelika Romanou"
                    },
                    {
                        "name": "Negar Foroutan"
                    },
                    {
                        "name": "Anna Sotnikova"
                    },
                    {
                        "name": "Zeming Chen"
                    },
                    {
                        "name": "Sree Harsha Nelaturu"
                    },
                    {
                        "name": "Shivalika Singh"
                    },
                    {
                        "name": "Rishabh Maheshwary"
                    },
                    {
                        "name": "Micol Altomare"
                    },
                    {
                        "name": "Mohamed A. Haggag"
                    },
                    {
                        "name": "Snegha A"
                    },
                    {
                        "name": "Alfonso Amayuelas"
                    },
                    {
                        "name": "Azril Hafizi Amirudin"
                    },
                    {
                        "name": "Viraat Aryabumi"
                    },
                    {
                        "name": "Danylo Boiko"
                    },
                    {
                        "name": "Michael Chang"
                    },
                    {
                        "name": "Jenny Chim"
                    },
                    {
                        "name": "Gal Cohen"
                    },
                    {
                        "name": "Aditya Kumar Dalmia"
                    },
                    {
                        "name": "Abraham Diress"
                    },
                    {
                        "name": "Sharad Duwal"
                    },
                    {
                        "name": "Daniil Dzenhaliou"
                    },
                    {
                        "name": "Daniel Fernando Erazo Florez"
                    },
                    {
                        "name": "Fabian Farestam"
                    },
                    {
                        "name": "Joseph Marvin Imperial"
                    },
                    {
                        "name": "Shayekh Bin Islam"
                    },
                    {
                        "name": "Perttu Isotalo"
                    },
                    {
                        "name": "Maral Jabbarishiviari"
                    },
                    {
                        "name": "BÃ¶rje F. Karlsson"
                    },
                    {
                        "name": "Eldar Khalilov"
                    },
                    {
                        "name": "Christopher Klamm"
                    },
                    {
                        "name": "Fajri Koto"
                    },
                    {
                        "name": "Dominik KrzemiÅski"
                    },
                    {
                        "name": "Gabriel Adriano de Melo"
                    },
                    {
                        "name": "Syrielle Montariol"
                    },
                    {
                        "name": "Yiyang Nan"
                    },
                    {
                        "name": "Joel Niklaus"
                    },
                    {
                        "name": "Jekaterina Novikova"
                    },
                    {
                        "name": "Johan Samir Obando Ceron"
                    },
                    {
                        "name": "Debjit Paul"
                    },
                    {
                        "name": "Esther Ploeger"
                    },
                    {
                        "name": "Jebish Purbey"
                    },
                    {
                        "name": "Swati Rajwal"
                    },
                    {
                        "name": "Selvan Sunitha Ravi"
                    },
                    {
                        "name": "Sara Rydell"
                    },
                    {
                        "name": "Roshan Santhosh"
                    },
                    {
                        "name": "Drishti Sharma"
                    },
                    {
                        "name": "Marjana Prifti Skenduli"
                    },
                    {
                        "name": "Arshia Soltani Moakhar"
                    },
                    {
                        "name": "Bardia Soltani Moakhar"
                    },
                    {
                        "name": "Ran Tamir"
                    },
                    {
                        "name": "Ayush Kumar Tarun"
                    },
                    {
                        "name": "Azmine Toushik Wasi"
                    },
                    {
                        "name": "Thenuka Ovin Weerasinghe"
                    },
                    {
                        "name": "Serhan Yilmaz"
                    },
                    {
                        "name": "Mike Zhang"
                    },
                    {
                        "name": "Imanol Schlag"
                    },
                    {
                        "name": "Marzieh Fadaee"
                    },
                    {
                        "name": "Sara Hooker"
                    },
                    {
                        "name": "Antoine Bosselut"
                    }
                ],
                "author_detail": {
                    "name": "Antoine Bosselut"
                },
                "author": "Antoine Bosselut",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19799v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19799v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2306.13549v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2306.13549v4",
                "updated": "2024-11-29T15:51:23Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    15,
                    51,
                    23,
                    4,
                    334,
                    0
                ],
                "published": "2023-06-23T15:21:52Z",
                "published_parsed": [
                    2023,
                    6,
                    23,
                    15,
                    21,
                    52,
                    4,
                    174,
                    0
                ],
                "title": "A Survey on Multimodal Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Multimodal Large Language Models"
                },
                "summary": "Recently, Multimodal Large Language Model (MLLM) represented by GPT-4V has\nbeen a new rising research hotspot, which uses powerful Large Language Models\n(LLMs) as a brain to perform multimodal tasks. The surprising emergent\ncapabilities of MLLM, such as writing stories based on images and OCR-free math\nreasoning, are rare in traditional multimodal methods, suggesting a potential\npath to artificial general intelligence. To this end, both academia and\nindustry have endeavored to develop MLLMs that can compete with or even better\nthan GPT-4V, pushing the limit of research at a surprising speed. In this\npaper, we aim to trace and summarize the recent progress of MLLMs. First of\nall, we present the basic formulation of MLLM and delineate its related\nconcepts, including architecture, training strategy and data, as well as\nevaluation. Then, we introduce research topics about how MLLMs can be extended\nto support more granularity, modalities, languages, and scenarios. We continue\nwith multimodal hallucination and extended techniques, including Multimodal ICL\n(M-ICL), Multimodal CoT (M-CoT), and LLM-Aided Visual Reasoning (LAVR). To\nconclude the paper, we discuss existing challenges and point out promising\nresearch directions. In light of the fact that the era of MLLM has only just\nbegun, we will keep updating this survey and hope it can inspire more research.\nAn associated GitHub link collecting the latest papers is available at\nhttps://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, Multimodal Large Language Model (MLLM) represented by GPT-4V has\nbeen a new rising research hotspot, which uses powerful Large Language Models\n(LLMs) as a brain to perform multimodal tasks. The surprising emergent\ncapabilities of MLLM, such as writing stories based on images and OCR-free math\nreasoning, are rare in traditional multimodal methods, suggesting a potential\npath to artificial general intelligence. To this end, both academia and\nindustry have endeavored to develop MLLMs that can compete with or even better\nthan GPT-4V, pushing the limit of research at a surprising speed. In this\npaper, we aim to trace and summarize the recent progress of MLLMs. First of\nall, we present the basic formulation of MLLM and delineate its related\nconcepts, including architecture, training strategy and data, as well as\nevaluation. Then, we introduce research topics about how MLLMs can be extended\nto support more granularity, modalities, languages, and scenarios. We continue\nwith multimodal hallucination and extended techniques, including Multimodal ICL\n(M-ICL), Multimodal CoT (M-CoT), and LLM-Aided Visual Reasoning (LAVR). To\nconclude the paper, we discuss existing challenges and point out promising\nresearch directions. In light of the fact that the era of MLLM has only just\nbegun, we will keep updating this survey and hope it can inspire more research.\nAn associated GitHub link collecting the latest papers is available at\nhttps://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models."
                },
                "authors": [
                    {
                        "name": "Shukang Yin"
                    },
                    {
                        "name": "Chaoyou Fu"
                    },
                    {
                        "name": "Sirui Zhao"
                    },
                    {
                        "name": "Ke Li"
                    },
                    {
                        "name": "Xing Sun"
                    },
                    {
                        "name": "Tong Xu"
                    },
                    {
                        "name": "Enhong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Enhong Chen"
                },
                "author": "Enhong Chen",
                "arxiv_doi": "10.1093/nsr/nwae403",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1093/nsr/nwae403",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2306.13549v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2306.13549v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted for publication in National Science Review. Project\n  page:https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.02837v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.02837v2",
                "updated": "2024-11-29T15:49:20Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    15,
                    49,
                    20,
                    4,
                    334,
                    0
                ],
                "published": "2024-04-03T16:16:31Z",
                "published_parsed": [
                    2024,
                    4,
                    3,
                    16,
                    16,
                    31,
                    2,
                    94,
                    0
                ],
                "title": "Cherry on Top: Parameter Heterogeneity and Quantization in Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cherry on Top: Parameter Heterogeneity and Quantization in Large\n  Language Models"
                },
                "summary": "This paper reveals the phenomenon of parameter heterogeneity in large\nlanguage models (LLMs). We find that a small subset of \"cherry\" parameters\nexhibit a disproportionately large influence on model performance, while the\nvast majority of parameters have minimal impact. This heterogeneity is found to\nbe prevalent across different model families, scales, and types. Motivated by\nthis observation, we propose CherryQ, a novel quantization method that unifies\nthe optimization of mixed-precision parameters. CherryQ identifies and\npreserves the critical cherry parameters in high precision while aggressively\nquantizing the remaining parameters to low precision. Extensive experiments\ndemonstrate the effectiveness of CherryQ. CherryQ outperforms existing\nquantization approaches in terms of perplexity and downstream task performance.\nNotably, our 3-bit quantized Vicuna-1.5 exhibits competitive performance\ncompared to their 16-bit counterparts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper reveals the phenomenon of parameter heterogeneity in large\nlanguage models (LLMs). We find that a small subset of \"cherry\" parameters\nexhibit a disproportionately large influence on model performance, while the\nvast majority of parameters have minimal impact. This heterogeneity is found to\nbe prevalent across different model families, scales, and types. Motivated by\nthis observation, we propose CherryQ, a novel quantization method that unifies\nthe optimization of mixed-precision parameters. CherryQ identifies and\npreserves the critical cherry parameters in high precision while aggressively\nquantizing the remaining parameters to low precision. Extensive experiments\ndemonstrate the effectiveness of CherryQ. CherryQ outperforms existing\nquantization approaches in terms of perplexity and downstream task performance.\nNotably, our 3-bit quantized Vicuna-1.5 exhibits competitive performance\ncompared to their 16-bit counterparts."
                },
                "authors": [
                    {
                        "name": "Wanyun Cui"
                    },
                    {
                        "name": "Qianle Wang"
                    }
                ],
                "author_detail": {
                    "name": "Qianle Wang"
                },
                "author": "Qianle Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.02837v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.02837v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19774v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19774v1",
                "updated": "2024-11-29T15:20:29Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    15,
                    20,
                    29,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T15:20:29Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    15,
                    20,
                    29,
                    4,
                    334,
                    0
                ],
                "title": "PerLA: Perceptive 3D Language Assistant",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PerLA: Perceptive 3D Language Assistant"
                },
                "summary": "Enabling Large Language Models (LLMs) to understand the 3D physical world is\nan emerging yet challenging research direction. Current strategies for\nprocessing point clouds typically downsample the scene or divide it into\nsmaller parts for separate analysis. However, both approaches risk losing key\nlocal details or global contextual information. In this paper, we introduce\nPerLA, a 3D language assistant designed to be more perceptive to both details\nand context, making visual representations more informative for the LLM. PerLA\ncaptures high-resolution (local) details in parallel from different point cloud\nareas and integrates them with (global) context obtained from a\nlower-resolution whole point cloud. We present a novel algorithm that preserves\npoint cloud locality through the Hilbert curve and effectively aggregates\nlocal-to-global information via cross-attention and a graph neural network.\nLastly, we introduce a novel loss for local representation consensus to promote\ntraining stability. PerLA outperforms state-of-the-art 3D language assistants,\nwith gains of up to +1.34 CiDEr on ScanQA for question answering, and +4.22 on\nScanRefer and +3.88 on Nr3D for dense\ncaptioning.\\url{https://gfmei.github.io/PerLA/}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling Large Language Models (LLMs) to understand the 3D physical world is\nan emerging yet challenging research direction. Current strategies for\nprocessing point clouds typically downsample the scene or divide it into\nsmaller parts for separate analysis. However, both approaches risk losing key\nlocal details or global contextual information. In this paper, we introduce\nPerLA, a 3D language assistant designed to be more perceptive to both details\nand context, making visual representations more informative for the LLM. PerLA\ncaptures high-resolution (local) details in parallel from different point cloud\nareas and integrates them with (global) context obtained from a\nlower-resolution whole point cloud. We present a novel algorithm that preserves\npoint cloud locality through the Hilbert curve and effectively aggregates\nlocal-to-global information via cross-attention and a graph neural network.\nLastly, we introduce a novel loss for local representation consensus to promote\ntraining stability. PerLA outperforms state-of-the-art 3D language assistants,\nwith gains of up to +1.34 CiDEr on ScanQA for question answering, and +4.22 on\nScanRefer and +3.88 on Nr3D for dense\ncaptioning.\\url{https://gfmei.github.io/PerLA/}"
                },
                "authors": [
                    {
                        "name": "Guofeng Mei"
                    },
                    {
                        "name": "Wei Lin"
                    },
                    {
                        "name": "Luigi Riz"
                    },
                    {
                        "name": "Yujiao Wu"
                    },
                    {
                        "name": "Fabio Poiesi"
                    },
                    {
                        "name": "Yiming Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yiming Wang"
                },
                "author": "Yiming Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19774v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19774v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19772v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19772v1",
                "updated": "2024-11-29T15:18:06Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    15,
                    18,
                    6,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T15:18:06Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    15,
                    18,
                    6,
                    4,
                    334,
                    0
                ],
                "title": "LongVALE: Vision-Audio-Language-Event Benchmark Towards Time-Aware\n  Omni-Modal Perception of Long Videos",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LongVALE: Vision-Audio-Language-Event Benchmark Towards Time-Aware\n  Omni-Modal Perception of Long Videos"
                },
                "summary": "Despite impressive advancements in video understanding, most efforts remain\nlimited to coarse-grained or visual-only video tasks. However, real-world\nvideos encompass omni-modal information (vision, audio, and speech) with a\nseries of events forming a cohesive storyline. The lack of multi-modal video\ndata with fine-grained event annotations and the high cost of manual labeling\nare major obstacles to comprehensive omni-modality video perception. To address\nthis gap, we propose an automatic pipeline consisting of high-quality\nmulti-modal video filtering, semantically coherent omni-modal event boundary\ndetection, and cross-modal correlation-aware event captioning. In this way, we\npresent LongVALE, the first-ever Vision-Audio-Language Event understanding\nbenchmark comprising 105K omni-modal events with precise temporal boundaries\nand detailed relation-aware captions within 8.4K high-quality long videos.\nFurther, we build a baseline that leverages LongVALE to enable video large\nlanguage models (LLMs) for omni-modality fine-grained temporal video\nunderstanding for the first time. Extensive experiments demonstrate the\neffectiveness and great potential of LongVALE in advancing comprehensive\nmulti-modal video understanding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite impressive advancements in video understanding, most efforts remain\nlimited to coarse-grained or visual-only video tasks. However, real-world\nvideos encompass omni-modal information (vision, audio, and speech) with a\nseries of events forming a cohesive storyline. The lack of multi-modal video\ndata with fine-grained event annotations and the high cost of manual labeling\nare major obstacles to comprehensive omni-modality video perception. To address\nthis gap, we propose an automatic pipeline consisting of high-quality\nmulti-modal video filtering, semantically coherent omni-modal event boundary\ndetection, and cross-modal correlation-aware event captioning. In this way, we\npresent LongVALE, the first-ever Vision-Audio-Language Event understanding\nbenchmark comprising 105K omni-modal events with precise temporal boundaries\nand detailed relation-aware captions within 8.4K high-quality long videos.\nFurther, we build a baseline that leverages LongVALE to enable video large\nlanguage models (LLMs) for omni-modality fine-grained temporal video\nunderstanding for the first time. Extensive experiments demonstrate the\neffectiveness and great potential of LongVALE in advancing comprehensive\nmulti-modal video understanding."
                },
                "authors": [
                    {
                        "name": "Tiantian Geng"
                    },
                    {
                        "name": "Jinrui Zhang"
                    },
                    {
                        "name": "Qingni Wang"
                    },
                    {
                        "name": "Teng Wang"
                    },
                    {
                        "name": "Jinming Duan"
                    },
                    {
                        "name": "Feng Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Feng Zheng"
                },
                "author": "Feng Zheng",
                "arxiv_comment": "18 pages, 15 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19772v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19772v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19757v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19757v1",
                "updated": "2024-11-29T15:01:25Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    15,
                    1,
                    25,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T15:01:25Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    15,
                    1,
                    25,
                    4,
                    334,
                    0
                ],
                "title": "Dual Risk Minimization: Towards Next-Level Robustness in Fine-tuning\n  Zero-Shot Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dual Risk Minimization: Towards Next-Level Robustness in Fine-tuning\n  Zero-Shot Models"
                },
                "summary": "Fine-tuning foundation models often compromises their robustness to\ndistribution shifts. To remedy this, most robust fine-tuning methods aim to\npreserve the pre-trained features. However, not all pre-trained features are\nrobust and those methods are largely indifferent to which ones to preserve. We\npropose dual risk minimization (DRM), which combines empirical risk\nminimization with worst-case risk minimization, to better preserve the core\nfeatures of downstream tasks. In particular, we utilize core-feature\ndescriptions generated by LLMs to induce core-based zero-shot predictions which\nthen serve as proxies to estimate the worst-case risk. DRM balances two crucial\naspects of model robustness: expected performance and worst-case performance,\nestablishing a new state of the art on various real-world benchmarks. DRM\nsignificantly improves the out-of-distribution performance of CLIP ViT-L/14@336\non ImageNet (75.9 to 77.1), WILDS-iWildCam (47.1 to 51.8), and WILDS-FMoW (50.7\nto 53.1); opening up new avenues for robust fine-tuning. Our code is available\nat https://github.com/vaynexie/DRM .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning foundation models often compromises their robustness to\ndistribution shifts. To remedy this, most robust fine-tuning methods aim to\npreserve the pre-trained features. However, not all pre-trained features are\nrobust and those methods are largely indifferent to which ones to preserve. We\npropose dual risk minimization (DRM), which combines empirical risk\nminimization with worst-case risk minimization, to better preserve the core\nfeatures of downstream tasks. In particular, we utilize core-feature\ndescriptions generated by LLMs to induce core-based zero-shot predictions which\nthen serve as proxies to estimate the worst-case risk. DRM balances two crucial\naspects of model robustness: expected performance and worst-case performance,\nestablishing a new state of the art on various real-world benchmarks. DRM\nsignificantly improves the out-of-distribution performance of CLIP ViT-L/14@336\non ImageNet (75.9 to 77.1), WILDS-iWildCam (47.1 to 51.8), and WILDS-FMoW (50.7\nto 53.1); opening up new avenues for robust fine-tuning. Our code is available\nat https://github.com/vaynexie/DRM ."
                },
                "authors": [
                    {
                        "name": "Kaican Li"
                    },
                    {
                        "name": "Weiyan Xie"
                    },
                    {
                        "name": "Yongxiang Huang"
                    },
                    {
                        "name": "Didan Deng"
                    },
                    {
                        "name": "Lanqing Hong"
                    },
                    {
                        "name": "Zhenguo Li"
                    },
                    {
                        "name": "Ricardo Silva"
                    },
                    {
                        "name": "Nevin L. Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Nevin L. Zhang"
                },
                "author": "Nevin L. Zhang",
                "arxiv_comment": "NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19757v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19757v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19746v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19746v1",
                "updated": "2024-11-29T14:46:37Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    14,
                    46,
                    37,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T14:46:37Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    14,
                    46,
                    37,
                    4,
                    334,
                    0
                ],
                "title": "HVAC-DPT: A Decision Pretrained Transformer for HVAC Control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HVAC-DPT: A Decision Pretrained Transformer for HVAC Control"
                },
                "summary": "Building operations consume approximately 40% of global energy, with Heating,\nVentilation, and Air Conditioning (HVAC) systems responsible for up to 50% of\nthis consumption. As HVAC energy demands are expected to rise, optimising\nsystem efficiency is crucial for reducing future energy use and mitigating\nclimate change. Existing control strategies lack generalisation and require\nextensive training and data, limiting their rapid deployment across diverse\nbuildings. This paper introduces HVAC-DPT, a Decision-Pretrained Transformer\nusing in-context Reinforcement Learning (RL) for multi-zone HVAC control.\nHVAC-DPT frames HVAC control as a sequential prediction task, training a causal\ntransformer on interaction histories generated by diverse RL agents. This\napproach enables HVAC-DPT to refine its policy in-context, without modifying\nnetwork parameters, allowing for deployment across different buildings without\nthe need for additional training or data collection. HVAC-DPT reduces energy\nconsumption in unseen buildings by 45% compared to the baseline controller,\noffering a scalable and effective approach to mitigating the increasing\nenvironmental impact of HVAC systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building operations consume approximately 40% of global energy, with Heating,\nVentilation, and Air Conditioning (HVAC) systems responsible for up to 50% of\nthis consumption. As HVAC energy demands are expected to rise, optimising\nsystem efficiency is crucial for reducing future energy use and mitigating\nclimate change. Existing control strategies lack generalisation and require\nextensive training and data, limiting their rapid deployment across diverse\nbuildings. This paper introduces HVAC-DPT, a Decision-Pretrained Transformer\nusing in-context Reinforcement Learning (RL) for multi-zone HVAC control.\nHVAC-DPT frames HVAC control as a sequential prediction task, training a causal\ntransformer on interaction histories generated by diverse RL agents. This\napproach enables HVAC-DPT to refine its policy in-context, without modifying\nnetwork parameters, allowing for deployment across different buildings without\nthe need for additional training or data collection. HVAC-DPT reduces energy\nconsumption in unseen buildings by 45% compared to the baseline controller,\noffering a scalable and effective approach to mitigating the increasing\nenvironmental impact of HVAC systems."
                },
                "authors": [
                    {
                        "name": "AnaÃ¯s Berkes"
                    }
                ],
                "author_detail": {
                    "name": "AnaÃ¯s Berkes"
                },
                "author": "AnaÃ¯s Berkes",
                "arxiv_comment": "7 pages, 3 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19746v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19746v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.08349v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.08349v3",
                "updated": "2024-11-29T14:44:27Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    14,
                    44,
                    27,
                    4,
                    334,
                    0
                ],
                "published": "2024-02-13T10:28:57Z",
                "published_parsed": [
                    2024,
                    2,
                    13,
                    10,
                    28,
                    57,
                    1,
                    44,
                    0
                ],
                "title": "Evaluating the Data Model Robustness of Text-to-SQL Systems Based on\n  Real User Queries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating the Data Model Robustness of Text-to-SQL Systems Based on\n  Real User Queries"
                },
                "summary": "Text-to-SQL systems (also known as NL-to-SQL systems) have become an\nincreasingly popular solution for bridging the gap between user capabilities\nand SQL-based data access. These systems translate user requests in natural\nlanguage to valid SQL statements for a specific database. Recent Text-to-SQL\nsystems have benefited from the rapid improvement of transformer-based language\nmodels. However, while Text-to-SQL systems that incorporate such models\ncontinuously reach new high scores on -- often synthetic -- benchmark datasets,\na systematic exploration of their robustness towards different data models in a\nreal-world, realistic scenario is notably missing. This paper provides the\nfirst in-depth evaluation of the data model robustness of Text-to-SQL systems\nin practice based on a multi-year international project focused on Text-to-SQL\ninterfaces. Our evaluation is based on a real-world deployment of FootballDB, a\nsystem that was deployed over a 9 month period in the context of the FIFA World\nCup 2022, during which about 6K natural language questions were asked and\nexecuted. All of our data is based on real user questions that were asked live\nto the system. We manually labeled and translated a subset of these questions\nfor three different data models. For each data model, we explore the\nperformance of representative Text-to-SQL systems and language models. We\nfurther quantify the impact of training data size, pre-, and post-processing\nsteps as well as language model inference time. Our comprehensive evaluation\nsheds light on the design choices of real-world Text-to-SQL systems and their\nimpact on moving from research prototypes to real deployments. Last, we provide\na new benchmark dataset to the community, which is the first to enable the\nevaluation of different data models for the same dataset and is substantially\nmore challenging than most previous datasets in terms of query complexity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-SQL systems (also known as NL-to-SQL systems) have become an\nincreasingly popular solution for bridging the gap between user capabilities\nand SQL-based data access. These systems translate user requests in natural\nlanguage to valid SQL statements for a specific database. Recent Text-to-SQL\nsystems have benefited from the rapid improvement of transformer-based language\nmodels. However, while Text-to-SQL systems that incorporate such models\ncontinuously reach new high scores on -- often synthetic -- benchmark datasets,\na systematic exploration of their robustness towards different data models in a\nreal-world, realistic scenario is notably missing. This paper provides the\nfirst in-depth evaluation of the data model robustness of Text-to-SQL systems\nin practice based on a multi-year international project focused on Text-to-SQL\ninterfaces. Our evaluation is based on a real-world deployment of FootballDB, a\nsystem that was deployed over a 9 month period in the context of the FIFA World\nCup 2022, during which about 6K natural language questions were asked and\nexecuted. All of our data is based on real user questions that were asked live\nto the system. We manually labeled and translated a subset of these questions\nfor three different data models. For each data model, we explore the\nperformance of representative Text-to-SQL systems and language models. We\nfurther quantify the impact of training data size, pre-, and post-processing\nsteps as well as language model inference time. Our comprehensive evaluation\nsheds light on the design choices of real-world Text-to-SQL systems and their\nimpact on moving from research prototypes to real deployments. Last, we provide\na new benchmark dataset to the community, which is the first to enable the\nevaluation of different data models for the same dataset and is substantially\nmore challenging than most previous datasets in terms of query complexity."
                },
                "authors": [
                    {
                        "name": "Jonathan FÃ¼rst"
                    },
                    {
                        "name": "Catherine Kosten"
                    },
                    {
                        "name": "Farhad Nooralahzadeh"
                    },
                    {
                        "name": "Yi Zhang"
                    },
                    {
                        "name": "Kurt Stockinger"
                    }
                ],
                "author_detail": {
                    "name": "Kurt Stockinger"
                },
                "author": "Kurt Stockinger",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.08349v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.08349v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19714v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19714v1",
                "updated": "2024-11-29T14:02:00Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    14,
                    2,
                    0,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T14:02:00Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    14,
                    2,
                    0,
                    4,
                    334,
                    0
                ],
                "title": "The Streetscape Application Services Stack (SASS): Towards a Distributed\n  Sensing Architecture for Urban Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Streetscape Application Services Stack (SASS): Towards a Distributed\n  Sensing Architecture for Urban Applications"
                },
                "summary": "As urban populations grow, cities are becoming more complex, driving the\ndeployment of interconnected sensing systems to realize the vision of smart\ncities. These systems aim to improve safety, mobility, and quality of life\nthrough applications that integrate diverse sensors with real-time\ndecision-making. Streetscape applications-focusing on challenges like\npedestrian safety and adaptive traffic management-depend on managing\ndistributed, heterogeneous sensor data, aligning information across time and\nspace, and enabling real-time processing. These tasks are inherently complex\nand often difficult to scale. The Streetscape Application Services Stack (SASS)\naddresses these challenges with three core services: multimodal data\nsynchronization, spatiotemporal data fusion, and distributed edge computing. By\nstructuring these capabilities as clear, composable abstractions with clear\nsemantics, SASS allows developers to scale streetscape applications efficiently\nwhile minimizing the complexity of multimodal integration.\n  We evaluated SASS in two real-world testbed environments: a controlled\nparking lot and an urban intersection in a major U.S. city. These testbeds\nallowed us to test SASS under diverse conditions, demonstrating its practical\napplicability. The Multimodal Data Synchronization service reduced temporal\nmisalignment errors by 88%, achieving synchronization accuracy within 50\nmilliseconds. Spatiotemporal Data Fusion service improved detection accuracy\nfor pedestrians and vehicles by over 10%, leveraging multicamera integration.\nThe Distributed Edge Computing service increased system throughput by more than\nan order of magnitude. Together, these results show how SASS provides the\nabstractions and performance needed to support real-time, scalable urban\napplications, bridging the gap between sensing infrastructure and actionable\nstreetscape intelligence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As urban populations grow, cities are becoming more complex, driving the\ndeployment of interconnected sensing systems to realize the vision of smart\ncities. These systems aim to improve safety, mobility, and quality of life\nthrough applications that integrate diverse sensors with real-time\ndecision-making. Streetscape applications-focusing on challenges like\npedestrian safety and adaptive traffic management-depend on managing\ndistributed, heterogeneous sensor data, aligning information across time and\nspace, and enabling real-time processing. These tasks are inherently complex\nand often difficult to scale. The Streetscape Application Services Stack (SASS)\naddresses these challenges with three core services: multimodal data\nsynchronization, spatiotemporal data fusion, and distributed edge computing. By\nstructuring these capabilities as clear, composable abstractions with clear\nsemantics, SASS allows developers to scale streetscape applications efficiently\nwhile minimizing the complexity of multimodal integration.\n  We evaluated SASS in two real-world testbed environments: a controlled\nparking lot and an urban intersection in a major U.S. city. These testbeds\nallowed us to test SASS under diverse conditions, demonstrating its practical\napplicability. The Multimodal Data Synchronization service reduced temporal\nmisalignment errors by 88%, achieving synchronization accuracy within 50\nmilliseconds. Spatiotemporal Data Fusion service improved detection accuracy\nfor pedestrians and vehicles by over 10%, leveraging multicamera integration.\nThe Distributed Edge Computing service increased system throughput by more than\nan order of magnitude. Together, these results show how SASS provides the\nabstractions and performance needed to support real-time, scalable urban\napplications, bridging the gap between sensing infrastructure and actionable\nstreetscape intelligence."
                },
                "authors": [
                    {
                        "name": "Navid Salami Pargoo"
                    },
                    {
                        "name": "Mahshid Ghasemi"
                    },
                    {
                        "name": "Shuren Xia"
                    },
                    {
                        "name": "Mehmet Kerem Turkcan"
                    },
                    {
                        "name": "Taqiya Ehsan"
                    },
                    {
                        "name": "Chengbo Zang"
                    },
                    {
                        "name": "Yuan Sun"
                    },
                    {
                        "name": "Javad Ghaderi"
                    },
                    {
                        "name": "Gil Zussman"
                    },
                    {
                        "name": "Zoran Kostic"
                    },
                    {
                        "name": "Jorge Ortiz"
                    }
                ],
                "author_detail": {
                    "name": "Jorge Ortiz"
                },
                "author": "Jorge Ortiz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19714v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19714v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19710v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19710v1",
                "updated": "2024-11-29T13:57:07Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    13,
                    57,
                    7,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T13:57:07Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    13,
                    57,
                    7,
                    4,
                    334,
                    0
                ],
                "title": "Know Your RAG: Dataset Taxonomy and Generation Strategies for Evaluating\n  RAG Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Know Your RAG: Dataset Taxonomy and Generation Strategies for Evaluating\n  RAG Systems"
                },
                "summary": "Retrieval Augmented Generation (RAG) systems are a widespread application of\nLarge Language Models (LLMs) in the industry. While many tools exist empowering\ndevelopers to build their own systems, measuring their performance locally,\nwith datasets reflective of the system's use cases, is a technological\nchallenge. Solutions to this problem range from non-specific and cheap (most\npublic datasets) to specific and costly (generating data from local documents).\nIn this paper, we show that using public question and answer (Q&A) datasets to\nassess retrieval performance can lead to non-optimal systems design, and that\ncommon tools for RAG dataset generation can lead to unbalanced data. We propose\nsolutions to these issues based on the characterization of RAG datasets through\nlabels and through label-targeted data generation. Finally, we show that\nfine-tuned small LLMs can efficiently generate Q&A datasets. We believe that\nthese observations are invaluable to the know-your-data step of RAG systems\ndevelopment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval Augmented Generation (RAG) systems are a widespread application of\nLarge Language Models (LLMs) in the industry. While many tools exist empowering\ndevelopers to build their own systems, measuring their performance locally,\nwith datasets reflective of the system's use cases, is a technological\nchallenge. Solutions to this problem range from non-specific and cheap (most\npublic datasets) to specific and costly (generating data from local documents).\nIn this paper, we show that using public question and answer (Q&A) datasets to\nassess retrieval performance can lead to non-optimal systems design, and that\ncommon tools for RAG dataset generation can lead to unbalanced data. We propose\nsolutions to these issues based on the characterization of RAG datasets through\nlabels and through label-targeted data generation. Finally, we show that\nfine-tuned small LLMs can efficiently generate Q&A datasets. We believe that\nthese observations are invaluable to the know-your-data step of RAG systems\ndevelopment."
                },
                "authors": [
                    {
                        "name": "Rafael Teixeira de Lima"
                    },
                    {
                        "name": "Shubham Gupta"
                    },
                    {
                        "name": "Cesar Berrospi"
                    },
                    {
                        "name": "Lokesh Mishra"
                    },
                    {
                        "name": "Michele Dolfi"
                    },
                    {
                        "name": "Peter Staar"
                    },
                    {
                        "name": "Panagiotis Vagenas"
                    }
                ],
                "author_detail": {
                    "name": "Panagiotis Vagenas"
                },
                "arxiv_affiliation": "IBM Research Zurich",
                "author": "Panagiotis Vagenas",
                "arxiv_comment": "to be published in the 31st International Conference on Computational\n  Linguistics (COLING 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19710v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19710v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19706v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19706v1",
                "updated": "2024-11-29T13:56:07Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    13,
                    56,
                    7,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T13:56:07Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    13,
                    56,
                    7,
                    4,
                    334,
                    0
                ],
                "title": "Challenges and Opportunities for Global Cellular Connectivity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Challenges and Opportunities for Global Cellular Connectivity"
                },
                "summary": "Traditional cellular service was designed for global connectivity, but\nbusiness and logistical constraints led to its fragmentation, with deployments\nlimited to individual countries and regions. Initiatives like Mobile Virtual\nNetwork Operators (MVNOs), Mobile Network Aggregators (MNAs), and regulations\nlike ''roam-like-at-home'' have partially restored global service potential,\nthough often at high costs in terms of user bills, application performance, and\ntraffic efficiency. This paper makes two key contributions: first, it surveys\nthe global cellular ecosystem, analyzing the strengths and weaknesses of major\nplayers using data from prior research, proprietary datasets, and public\nsources. Second, it argues that the technology for seamless global service\nexists in Local Breakout (LBO), a roaming architecture which allows user\ntraffic to be routed directly to the Internet through the visited network,\nbypassing the home network and/or third-party infrastructures. However, LBO\nadoption is hindered by issues such as policy enforcement, billing, and Quality\nof Service (QoS) guarantees, rooted in a lack of trust between operators. The\npaper concludes by exploring technological advances that could enable LBO, and\npave the way for truly global cellular connectivity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional cellular service was designed for global connectivity, but\nbusiness and logistical constraints led to its fragmentation, with deployments\nlimited to individual countries and regions. Initiatives like Mobile Virtual\nNetwork Operators (MVNOs), Mobile Network Aggregators (MNAs), and regulations\nlike ''roam-like-at-home'' have partially restored global service potential,\nthough often at high costs in terms of user bills, application performance, and\ntraffic efficiency. This paper makes two key contributions: first, it surveys\nthe global cellular ecosystem, analyzing the strengths and weaknesses of major\nplayers using data from prior research, proprietary datasets, and public\nsources. Second, it argues that the technology for seamless global service\nexists in Local Breakout (LBO), a roaming architecture which allows user\ntraffic to be routed directly to the Internet through the visited network,\nbypassing the home network and/or third-party infrastructures. However, LBO\nadoption is hindered by issues such as policy enforcement, billing, and Quality\nof Service (QoS) guarantees, rooted in a lack of trust between operators. The\npaper concludes by exploring technological advances that could enable LBO, and\npave the way for truly global cellular connectivity."
                },
                "authors": [
                    {
                        "name": "Viktoria Vomhoff"
                    },
                    {
                        "name": "Hyunseok Daniel Jang"
                    },
                    {
                        "name": "Matteo Varvello"
                    },
                    {
                        "name": "Stefan GeiÃler"
                    },
                    {
                        "name": "Yasir Zaki"
                    },
                    {
                        "name": "Tobias HoÃfeld"
                    },
                    {
                        "name": "Andra Lutu"
                    }
                ],
                "author_detail": {
                    "name": "Andra Lutu"
                },
                "author": "Andra Lutu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19706v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19706v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19689v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19689v1",
                "updated": "2024-11-29T13:24:10Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    13,
                    24,
                    10,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T13:24:10Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    13,
                    24,
                    10,
                    4,
                    334,
                    0
                ],
                "title": "MIMDE: Exploring the Use of Synthetic vs Human Data for Evaluating\n  Multi-Insight Multi-Document Extraction Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MIMDE: Exploring the Use of Synthetic vs Human Data for Evaluating\n  Multi-Insight Multi-Document Extraction Tasks"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable capabilities in\ntext analysis tasks, yet their evaluation on complex, real-world applications\nremains challenging. We define a set of tasks, Multi-Insight Multi-Document\nExtraction (MIMDE) tasks, which involves extracting an optimal set of insights\nfrom a document corpus and mapping these insights back to their source\ndocuments. This task is fundamental to many practical applications, from\nanalyzing survey responses to processing medical records, where identifying and\ntracing key insights across documents is crucial. We develop an evaluation\nframework for MIMDE and introduce a novel set of complementary human and\nsynthetic datasets to examine the potential of synthetic data for LLM\nevaluation. After establishing optimal metrics for comparing extracted\ninsights, we benchmark 20 state-of-the-art LLMs on both datasets. Our analysis\nreveals a strong correlation (0.71) between the ability of LLMs to extracts\ninsights on our two datasets but synthetic data fails to capture the complexity\nof document-level analysis. These findings offer crucial guidance for the use\nof synthetic data in evaluating text analysis systems, highlighting both its\npotential and limitations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable capabilities in\ntext analysis tasks, yet their evaluation on complex, real-world applications\nremains challenging. We define a set of tasks, Multi-Insight Multi-Document\nExtraction (MIMDE) tasks, which involves extracting an optimal set of insights\nfrom a document corpus and mapping these insights back to their source\ndocuments. This task is fundamental to many practical applications, from\nanalyzing survey responses to processing medical records, where identifying and\ntracing key insights across documents is crucial. We develop an evaluation\nframework for MIMDE and introduce a novel set of complementary human and\nsynthetic datasets to examine the potential of synthetic data for LLM\nevaluation. After establishing optimal metrics for comparing extracted\ninsights, we benchmark 20 state-of-the-art LLMs on both datasets. Our analysis\nreveals a strong correlation (0.71) between the ability of LLMs to extracts\ninsights on our two datasets but synthetic data fails to capture the complexity\nof document-level analysis. These findings offer crucial guidance for the use\nof synthetic data in evaluating text analysis systems, highlighting both its\npotential and limitations."
                },
                "authors": [
                    {
                        "name": "John Francis"
                    },
                    {
                        "name": "Saba Esnaashari"
                    },
                    {
                        "name": "Anton Poletaev"
                    },
                    {
                        "name": "Sukankana Chakraborty"
                    },
                    {
                        "name": "Youmna Hashem"
                    },
                    {
                        "name": "Jonathan Bright"
                    }
                ],
                "author_detail": {
                    "name": "Jonathan Bright"
                },
                "author": "Jonathan Bright",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19689v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19689v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19688v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19688v1",
                "updated": "2024-11-29T13:22:52Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    13,
                    22,
                    52,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T13:22:52Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    13,
                    22,
                    52,
                    4,
                    334,
                    0
                ],
                "title": "SURE-VQA: Systematic Understanding of Robustness Evaluation in Medical\n  VQA Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SURE-VQA: Systematic Understanding of Robustness Evaluation in Medical\n  VQA Tasks"
                },
                "summary": "Vision-Language Models (VLMs) have great potential in medical tasks, like\nVisual Question Answering (VQA), where they could act as interactive assistants\nfor both patients and clinicians. Yet their robustness to distribution shifts\non unseen data remains a critical concern for safe deployment. Evaluating such\nrobustness requires a controlled experimental setup that allows for systematic\ninsights into the model's behavior. However, we demonstrate that current setups\nfail to offer sufficiently thorough evaluations, limiting their ability to\naccurately assess model robustness. To address this gap, our work introduces a\nnovel framework, called SURE-VQA, centered around three key requirements to\novercome the current pitfalls and systematically analyze the robustness of\nVLMs: 1) Since robustness on synthetic shifts does not necessarily translate to\nreal-world shifts, robustness should be measured on real-world shifts that are\ninherent to the VQA data; 2) Traditional token-matching metrics often fail to\ncapture underlying semantics, necessitating the use of large language models\n(LLMs) for more accurate semantic evaluation; 3) Model performance often lacks\ninterpretability due to missing sanity baselines, thus meaningful baselines\nshould be reported that allow assessing the multimodal impact on the VLM. To\ndemonstrate the relevance of this framework, we conduct a study on the\nrobustness of various fine-tuning methods across three medical datasets with\nfour different types of distribution shifts. Our study reveals several\nimportant findings: 1) Sanity baselines that do not utilize image data can\nperform surprisingly well; 2) We confirm LoRA as the best-performing PEFT\nmethod; 3) No PEFT method consistently outperforms others in terms of\nrobustness to shifts. Code is provided at https://github.com/IML-DKFZ/sure-vqa.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language Models (VLMs) have great potential in medical tasks, like\nVisual Question Answering (VQA), where they could act as interactive assistants\nfor both patients and clinicians. Yet their robustness to distribution shifts\non unseen data remains a critical concern for safe deployment. Evaluating such\nrobustness requires a controlled experimental setup that allows for systematic\ninsights into the model's behavior. However, we demonstrate that current setups\nfail to offer sufficiently thorough evaluations, limiting their ability to\naccurately assess model robustness. To address this gap, our work introduces a\nnovel framework, called SURE-VQA, centered around three key requirements to\novercome the current pitfalls and systematically analyze the robustness of\nVLMs: 1) Since robustness on synthetic shifts does not necessarily translate to\nreal-world shifts, robustness should be measured on real-world shifts that are\ninherent to the VQA data; 2) Traditional token-matching metrics often fail to\ncapture underlying semantics, necessitating the use of large language models\n(LLMs) for more accurate semantic evaluation; 3) Model performance often lacks\ninterpretability due to missing sanity baselines, thus meaningful baselines\nshould be reported that allow assessing the multimodal impact on the VLM. To\ndemonstrate the relevance of this framework, we conduct a study on the\nrobustness of various fine-tuning methods across three medical datasets with\nfour different types of distribution shifts. Our study reveals several\nimportant findings: 1) Sanity baselines that do not utilize image data can\nperform surprisingly well; 2) We confirm LoRA as the best-performing PEFT\nmethod; 3) No PEFT method consistently outperforms others in terms of\nrobustness to shifts. Code is provided at https://github.com/IML-DKFZ/sure-vqa."
                },
                "authors": [
                    {
                        "name": "Kim-Celine Kahl"
                    },
                    {
                        "name": "Selen Erkan"
                    },
                    {
                        "name": "Jeremias Traub"
                    },
                    {
                        "name": "Carsten T. LÃ¼th"
                    },
                    {
                        "name": "Klaus Maier-Hein"
                    },
                    {
                        "name": "Lena Maier-Hein"
                    },
                    {
                        "name": "Paul F. Jaeger"
                    }
                ],
                "author_detail": {
                    "name": "Paul F. Jaeger"
                },
                "author": "Paul F. Jaeger",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19688v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19688v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19668v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19668v1",
                "updated": "2024-11-29T12:48:49Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    12,
                    48,
                    49,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T12:48:49Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    12,
                    48,
                    49,
                    4,
                    334,
                    0
                ],
                "title": "ChineseWebText 2.0: Large-Scale High-quality Chinese Web Text with\n  Multi-dimensional and fine-grained information",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChineseWebText 2.0: Large-Scale High-quality Chinese Web Text with\n  Multi-dimensional and fine-grained information"
                },
                "summary": "During the development of large language models (LLMs), pre-training data\nplay a critical role in shaping LLMs' capabilities. In recent years several\nlarge-scale and high-quality pre-training datasets have been released to\naccelerate the research of LLMs, including ChineseWebText1.0, C4, Pile,\nWanJuan, MAPCC and others. However, as LLMs continue to evolve, focus has\nincreasingly shifted to domain-specific capabilities and safety concerns,\nmaking those previous coarse-grained texts insufficient for meeting training\nrequirements. Furthermore, fine-grained information, such as quality, domain\nand toxicity, is becoming increasingly important in building powerful and\nreliable LLMs for various scenarios. To address these challenges, in this paper\nwe propose a new tool-chain called MDFG-tool for constructing large-scale and\nhigh-quality Chinese datasets with multi-dimensional and fine-grained\ninformation. First, we employ manually crafted rules to discard explicit noisy\ntexts from raw contents. Second, the quality evaluation model, domain\nclassifier, and toxicity evaluation model are well-designed to assess the\nremaining cleaned data respectively. Finally, we integrate these three types of\nfine-grained information for each text. With this approach, we release the\nlargest, high-quality and fine-grained Chinese text ChineseWebText2.0, which\nconsists of 3.8TB and each text is associated with a quality score, domain\nlabels, a toxicity label and a toxicity score, facilitating the LLM researchers\nto select data based on various types of fine-grained information. The data,\ncodes and the tool-chain are available on this website\nhttps://github.com/CASIA-LM/ChineseWebText-2.0",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "During the development of large language models (LLMs), pre-training data\nplay a critical role in shaping LLMs' capabilities. In recent years several\nlarge-scale and high-quality pre-training datasets have been released to\naccelerate the research of LLMs, including ChineseWebText1.0, C4, Pile,\nWanJuan, MAPCC and others. However, as LLMs continue to evolve, focus has\nincreasingly shifted to domain-specific capabilities and safety concerns,\nmaking those previous coarse-grained texts insufficient for meeting training\nrequirements. Furthermore, fine-grained information, such as quality, domain\nand toxicity, is becoming increasingly important in building powerful and\nreliable LLMs for various scenarios. To address these challenges, in this paper\nwe propose a new tool-chain called MDFG-tool for constructing large-scale and\nhigh-quality Chinese datasets with multi-dimensional and fine-grained\ninformation. First, we employ manually crafted rules to discard explicit noisy\ntexts from raw contents. Second, the quality evaluation model, domain\nclassifier, and toxicity evaluation model are well-designed to assess the\nremaining cleaned data respectively. Finally, we integrate these three types of\nfine-grained information for each text. With this approach, we release the\nlargest, high-quality and fine-grained Chinese text ChineseWebText2.0, which\nconsists of 3.8TB and each text is associated with a quality score, domain\nlabels, a toxicity label and a toxicity score, facilitating the LLM researchers\nto select data based on various types of fine-grained information. The data,\ncodes and the tool-chain are available on this website\nhttps://github.com/CASIA-LM/ChineseWebText-2.0"
                },
                "authors": [
                    {
                        "name": "Wanyue Zhang"
                    },
                    {
                        "name": "Ziyong Li"
                    },
                    {
                        "name": "Wen Yang"
                    },
                    {
                        "name": "Chunlin Leng"
                    },
                    {
                        "name": "Yinan Bai"
                    },
                    {
                        "name": "Qianlong Du"
                    },
                    {
                        "name": "Chengqing Zong"
                    },
                    {
                        "name": "Jiajun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jiajun Zhang"
                },
                "author": "Jiajun Zhang",
                "arxiv_comment": "ChineseWebTex2.0 dataset is available at\n  https://github.com/CASIA-LM/ChineseWebText-2.0",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19668v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19668v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00481v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00481v3",
                "updated": "2024-11-29T12:34:51Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    12,
                    34,
                    51,
                    4,
                    334,
                    0
                ],
                "published": "2024-08-31T15:26:57Z",
                "published_parsed": [
                    2024,
                    8,
                    31,
                    15,
                    26,
                    57,
                    5,
                    244,
                    0
                ],
                "title": "DCIM-AVSR : Efficient Audio-Visual Speech Recognition via Dual Conformer\n  Interaction Module",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DCIM-AVSR : Efficient Audio-Visual Speech Recognition via Dual Conformer\n  Interaction Module"
                },
                "summary": "Speech recognition is the technology that enables machines to interpret and\nprocess human speech, converting spoken language into text or commands. This\ntechnology is essential for applications such as virtual assistants,\ntranscription services, and communication tools. The Audio-Visual Speech\nRecognition (AVSR) model enhances traditional speech recognition, particularly\nin noisy environments, by incorporating visual modalities like lip movements\nand facial expressions. While traditional AVSR models trained on large-scale\ndatasets with numerous parameters can achieve remarkable accuracy, often\nsurpassing human performance, they also come with high training costs and\ndeployment challenges. To address these issues, we introduce an efficient AVSR\nmodel that reduces the number of parameters through the integration of a Dual\nConformer Interaction Module (DCIM). In addition, we propose a pre-training\nmethod that further optimizes model performance by selectively updating\nparameters, leading to significant improvements in efficiency. Unlike\nconventional models that require the system to independently learn the\nhierarchical relationship between audio and visual modalities, our approach\nincorporates this distinction directly into the model architecture. This design\nenhances both efficiency and performance, resulting in a more practical and\neffective solution for AVSR tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speech recognition is the technology that enables machines to interpret and\nprocess human speech, converting spoken language into text or commands. This\ntechnology is essential for applications such as virtual assistants,\ntranscription services, and communication tools. The Audio-Visual Speech\nRecognition (AVSR) model enhances traditional speech recognition, particularly\nin noisy environments, by incorporating visual modalities like lip movements\nand facial expressions. While traditional AVSR models trained on large-scale\ndatasets with numerous parameters can achieve remarkable accuracy, often\nsurpassing human performance, they also come with high training costs and\ndeployment challenges. To address these issues, we introduce an efficient AVSR\nmodel that reduces the number of parameters through the integration of a Dual\nConformer Interaction Module (DCIM). In addition, we propose a pre-training\nmethod that further optimizes model performance by selectively updating\nparameters, leading to significant improvements in efficiency. Unlike\nconventional models that require the system to independently learn the\nhierarchical relationship between audio and visual modalities, our approach\nincorporates this distinction directly into the model architecture. This design\nenhances both efficiency and performance, resulting in a more practical and\neffective solution for AVSR tasks."
                },
                "authors": [
                    {
                        "name": "Xinyu Wang"
                    },
                    {
                        "name": "Qian Wang"
                    },
                    {
                        "name": "Haotian Jiang"
                    },
                    {
                        "name": "Haolin Huang"
                    },
                    {
                        "name": "Yu Fang"
                    },
                    {
                        "name": "Mengjie Xu"
                    }
                ],
                "author_detail": {
                    "name": "Mengjie Xu"
                },
                "author": "Mengjie Xu",
                "arxiv_comment": "Submitted to ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00481v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00481v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.03157v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.03157v2",
                "updated": "2024-11-29T12:33:45Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    12,
                    33,
                    45,
                    4,
                    334,
                    0
                ],
                "published": "2023-11-06T14:52:30Z",
                "published_parsed": [
                    2023,
                    11,
                    6,
                    14,
                    52,
                    30,
                    0,
                    310,
                    0
                ],
                "title": "GPTuner: A Manual-Reading Database Tuning System via GPT-Guided Bayesian\n  Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPTuner: A Manual-Reading Database Tuning System via GPT-Guided Bayesian\n  Optimization"
                },
                "summary": "Modern database management systems (DBMS) expose hundreds of configurable\nknobs to control system behaviours. Determining the appropriate values for\nthese knobs to improve DBMS performance is a long-standing problem in the\ndatabase community. As there is an increasing number of knobs to tune and each\nknob could be in continuous or categorical values, manual tuning becomes\nimpractical. Recently, automatic tuning systems using machine learning methods\nhave shown great potentials. However, existing approaches still incur\nsignificant tuning costs or only yield sub-optimal performance. This is because\nthey either ignore the extensive domain knowledge available (e.g., DBMS manuals\nand forum discussions) and only rely on the runtime feedback of benchmark\nevaluations to guide the optimization, or they utilize the domain knowledge in\na limited way. Hence, we propose GPTuner, a manual-reading database tuning\nsystem. Firstly, we develop a Large Language Model (LLM)-based pipeline to\ncollect and refine heterogeneous knowledge, and propose a prompt ensemble\nalgorithm to unify a structured view of the refined knowledge. Secondly, using\nthe structured knowledge, we (1) design a workload-aware and training-free knob\nselection strategy, (2) develop a search space optimization technique\nconsidering the value range of each knob, and (3) propose a Coarse-to-Fine\nBayesian Optimization Framework to explore the optimized space. Finally, we\nevaluate GPTuner under different benchmarks (TPC-C and TPC-H), metrics\n(throughput and latency) as well as DBMS (PostgreSQL and MySQL). Compared to\nthe state-of-the-art approaches, GPTuner identifies better configurations in\n16x less time on average. Moreover, GPTuner achieves up to 30% performance\nimprovement (higher throughput or lower latency) over the best-performing\nalternative.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern database management systems (DBMS) expose hundreds of configurable\nknobs to control system behaviours. Determining the appropriate values for\nthese knobs to improve DBMS performance is a long-standing problem in the\ndatabase community. As there is an increasing number of knobs to tune and each\nknob could be in continuous or categorical values, manual tuning becomes\nimpractical. Recently, automatic tuning systems using machine learning methods\nhave shown great potentials. However, existing approaches still incur\nsignificant tuning costs or only yield sub-optimal performance. This is because\nthey either ignore the extensive domain knowledge available (e.g., DBMS manuals\nand forum discussions) and only rely on the runtime feedback of benchmark\nevaluations to guide the optimization, or they utilize the domain knowledge in\na limited way. Hence, we propose GPTuner, a manual-reading database tuning\nsystem. Firstly, we develop a Large Language Model (LLM)-based pipeline to\ncollect and refine heterogeneous knowledge, and propose a prompt ensemble\nalgorithm to unify a structured view of the refined knowledge. Secondly, using\nthe structured knowledge, we (1) design a workload-aware and training-free knob\nselection strategy, (2) develop a search space optimization technique\nconsidering the value range of each knob, and (3) propose a Coarse-to-Fine\nBayesian Optimization Framework to explore the optimized space. Finally, we\nevaluate GPTuner under different benchmarks (TPC-C and TPC-H), metrics\n(throughput and latency) as well as DBMS (PostgreSQL and MySQL). Compared to\nthe state-of-the-art approaches, GPTuner identifies better configurations in\n16x less time on average. Moreover, GPTuner achieves up to 30% performance\nimprovement (higher throughput or lower latency) over the best-performing\nalternative."
                },
                "authors": [
                    {
                        "name": "Jiale Lao"
                    },
                    {
                        "name": "Yibo Wang"
                    },
                    {
                        "name": "Yufei Li"
                    },
                    {
                        "name": "Jianping Wang"
                    },
                    {
                        "name": "Yunjia Zhang"
                    },
                    {
                        "name": "Zhiyuan Cheng"
                    },
                    {
                        "name": "Wanghu Chen"
                    },
                    {
                        "name": "Mingjie Tang"
                    },
                    {
                        "name": "Jianguo Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jianguo Wang"
                },
                "author": "Jianguo Wang",
                "arxiv_doi": "10.14778/3659437.3659449",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.14778/3659437.3659449",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2311.03157v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.03157v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by VLDB2024",
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19655v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19655v1",
                "updated": "2024-11-29T12:21:15Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    12,
                    21,
                    15,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T12:21:15Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    12,
                    21,
                    15,
                    4,
                    334,
                    0
                ],
                "title": "Truth or Mirage? Towards End-to-End Factuality Evaluation with LLM-OASIS",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Truth or Mirage? Towards End-to-End Factuality Evaluation with LLM-OASIS"
                },
                "summary": "After the introduction of Large Language Models (LLMs), there have been\nsubstantial improvements in the performance of Natural Language Generation\n(NLG) tasks, including Text Summarization and Machine Translation. However,\nLLMs still produce outputs containing hallucinations, that is, content not\ngrounded in factual information. Therefore, developing methods to assess the\nfactuality of LLMs has become urgent.\n  Indeed, resources for factuality evaluation have recently emerged. Although\nchallenging, these resources face one or more of the following limitations: (i)\nthey are tailored to a specific task or domain; (ii) they are limited in size,\nthereby preventing the training of new factuality evaluators; (iii) they are\ndesigned for simpler verification tasks, such as claim verification.\n  To address these issues, we introduce LLM-Oasis, to the best of our knowledge\nthe largest resource for training end-to-end factuality evaluators. LLM-Oasis\nis constructed by extracting claims from Wikipedia, falsifying a subset of\nthese claims, and generating pairs of factual and unfactual texts. We then rely\non human annotators to both validate the quality of our dataset and to create a\ngold standard test set for benchmarking factuality evaluation systems.\n  Our experiments demonstrate that LLM-Oasis presents a significant challenge\nfor state-of-the-art LLMs, with GPT-4o achieving up to 60% accuracy in our\nproposed end-to-end factuality evaluation task, highlighting its potential to\ndrive future research in the field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "After the introduction of Large Language Models (LLMs), there have been\nsubstantial improvements in the performance of Natural Language Generation\n(NLG) tasks, including Text Summarization and Machine Translation. However,\nLLMs still produce outputs containing hallucinations, that is, content not\ngrounded in factual information. Therefore, developing methods to assess the\nfactuality of LLMs has become urgent.\n  Indeed, resources for factuality evaluation have recently emerged. Although\nchallenging, these resources face one or more of the following limitations: (i)\nthey are tailored to a specific task or domain; (ii) they are limited in size,\nthereby preventing the training of new factuality evaluators; (iii) they are\ndesigned for simpler verification tasks, such as claim verification.\n  To address these issues, we introduce LLM-Oasis, to the best of our knowledge\nthe largest resource for training end-to-end factuality evaluators. LLM-Oasis\nis constructed by extracting claims from Wikipedia, falsifying a subset of\nthese claims, and generating pairs of factual and unfactual texts. We then rely\non human annotators to both validate the quality of our dataset and to create a\ngold standard test set for benchmarking factuality evaluation systems.\n  Our experiments demonstrate that LLM-Oasis presents a significant challenge\nfor state-of-the-art LLMs, with GPT-4o achieving up to 60% accuracy in our\nproposed end-to-end factuality evaluation task, highlighting its potential to\ndrive future research in the field."
                },
                "authors": [
                    {
                        "name": "Alessandro ScirÃ¨"
                    },
                    {
                        "name": "Andrei Stefan Bejgu"
                    },
                    {
                        "name": "Simone Tedeschi"
                    },
                    {
                        "name": "Karim Ghonim"
                    },
                    {
                        "name": "Federico Martelli"
                    },
                    {
                        "name": "Roberto Navigli"
                    }
                ],
                "author_detail": {
                    "name": "Roberto Navigli"
                },
                "author": "Roberto Navigli",
                "arxiv_comment": "15 pages. To be submitted to CL journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19655v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19655v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19648v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19648v1",
                "updated": "2024-11-29T12:02:28Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    12,
                    2,
                    28,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T12:02:28Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    12,
                    2,
                    28,
                    4,
                    334,
                    0
                ],
                "title": "Enhancing Security in Third-Party Library Reuse -- Comprehensive\n  Detection of 1-day Vulnerability through Code Patch Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Security in Third-Party Library Reuse -- Comprehensive\n  Detection of 1-day Vulnerability through Code Patch Analysis"
                },
                "summary": "Nowadays, software development progresses rapidly to incorporate new\nfeatures. To facilitate such growth and provide convenience for developers when\ncreating and updating software, reusing open-source software (i.e., thirdparty\nlibrary reuses) has become one of the most effective and efficient methods.\nUnfortunately, the practice of reusing third-party libraries (TPLs) can also\nintroduce vulnerabilities (known as 1-day vulnerabilities) because of the low\nmaintenance of TPLs, resulting in many vulnerable versions remaining in use. If\nthe software incorporating these TPLs fails to detect the introduced\nvulnerabilities and leads to delayed updates, it will exacerbate the security\nrisks. However, the complicated code dependencies and flexibility of TPL reuses\nmake the detection of 1-day vulnerability a challenging task. To support\ndevelopers in securely reusing TPLs during software development, we design and\nimplement VULTURE, an effective and efficient detection tool, aiming at\nidentifying 1-day vulnerabilities that arise from the reuse of vulnerable TPLs.\nIt first executes a database creation method, TPLFILTER, which leverages the\nLarge Language Model (LLM) to automatically build a unique database for the\ntargeted platform. Instead of relying on code-level similarity comparison,\nVULTURE employs hashing-based comparison to explore the dependencies among the\ncollected TPLs and identify the similarities between the TPLs and the target\nprojects. Recognizing that developers have the flexibility to reuse TPLs\nexactly or in a custom manner, VULTURE separately conducts version-based\ncomparison and chunk-based analysis to capture fine-grained semantic features\nat the function levels. We applied VULTURE to 10 real-world projects to assess\nits effectiveness and efficiency in detecting 1-day vulnerabilities. VULTURE\nsuccessfully identified 175 vulnerabilities from 178 reused TPLs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nowadays, software development progresses rapidly to incorporate new\nfeatures. To facilitate such growth and provide convenience for developers when\ncreating and updating software, reusing open-source software (i.e., thirdparty\nlibrary reuses) has become one of the most effective and efficient methods.\nUnfortunately, the practice of reusing third-party libraries (TPLs) can also\nintroduce vulnerabilities (known as 1-day vulnerabilities) because of the low\nmaintenance of TPLs, resulting in many vulnerable versions remaining in use. If\nthe software incorporating these TPLs fails to detect the introduced\nvulnerabilities and leads to delayed updates, it will exacerbate the security\nrisks. However, the complicated code dependencies and flexibility of TPL reuses\nmake the detection of 1-day vulnerability a challenging task. To support\ndevelopers in securely reusing TPLs during software development, we design and\nimplement VULTURE, an effective and efficient detection tool, aiming at\nidentifying 1-day vulnerabilities that arise from the reuse of vulnerable TPLs.\nIt first executes a database creation method, TPLFILTER, which leverages the\nLarge Language Model (LLM) to automatically build a unique database for the\ntargeted platform. Instead of relying on code-level similarity comparison,\nVULTURE employs hashing-based comparison to explore the dependencies among the\ncollected TPLs and identify the similarities between the TPLs and the target\nprojects. Recognizing that developers have the flexibility to reuse TPLs\nexactly or in a custom manner, VULTURE separately conducts version-based\ncomparison and chunk-based analysis to capture fine-grained semantic features\nat the function levels. We applied VULTURE to 10 real-world projects to assess\nits effectiveness and efficiency in detecting 1-day vulnerabilities. VULTURE\nsuccessfully identified 175 vulnerabilities from 178 reused TPLs."
                },
                "authors": [
                    {
                        "name": "Shangzhi Xu"
                    },
                    {
                        "name": "Jialiang Dong"
                    },
                    {
                        "name": "Weiting Cai"
                    },
                    {
                        "name": "Juanru Li"
                    },
                    {
                        "name": "Arash Shaghaghi"
                    },
                    {
                        "name": "Nan Sun"
                    },
                    {
                        "name": "Siqi Ma"
                    }
                ],
                "author_detail": {
                    "name": "Siqi Ma"
                },
                "author": "Siqi Ma",
                "arxiv_comment": "17 pages, NDSS 25'",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19648v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19648v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.16314v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.16314v2",
                "updated": "2024-11-29T11:52:31Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    11,
                    52,
                    31,
                    4,
                    334,
                    0
                ],
                "published": "2024-10-09T10:09:37Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    10,
                    9,
                    37,
                    2,
                    283,
                    0
                ],
                "title": "Steering Large Language Models using Conceptors: Improving\n  Addition-Based Activation Engineering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Steering Large Language Models using Conceptors: Improving\n  Addition-Based Activation Engineering"
                },
                "summary": "Large language models have transformed AI, yet reliably controlling their\noutputs remains a challenge. This paper explores activation engineering, where\noutputs of pre-trained LLMs are controlled by manipulating their activations at\ninference time. Unlike traditional methods using a single steering vector, we\nintroduce conceptors - mathematical constructs that represent sets of\nactivation vectors as ellipsoidal regions. Conceptors act as soft projection\nmatrices and offer more precise control over complex activation patterns. Our\nexperiments demonstrate that conceptors outperform traditional methods across\nmultiple steering tasks. We further use Boolean operations on conceptors for\ncombined steering goals that empirically outperform additively combining\nsteering vectors on a set of tasks. These results highlight conceptors as a\npromising tool for more effective steering of LLMs. Our code is available on\ngithub.com/jorispos/conceptorsteering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have transformed AI, yet reliably controlling their\noutputs remains a challenge. This paper explores activation engineering, where\noutputs of pre-trained LLMs are controlled by manipulating their activations at\ninference time. Unlike traditional methods using a single steering vector, we\nintroduce conceptors - mathematical constructs that represent sets of\nactivation vectors as ellipsoidal regions. Conceptors act as soft projection\nmatrices and offer more precise control over complex activation patterns. Our\nexperiments demonstrate that conceptors outperform traditional methods across\nmultiple steering tasks. We further use Boolean operations on conceptors for\ncombined steering goals that empirically outperform additively combining\nsteering vectors on a set of tasks. These results highlight conceptors as a\npromising tool for more effective steering of LLMs. Our code is available on\ngithub.com/jorispos/conceptorsteering."
                },
                "authors": [
                    {
                        "name": "Joris Postmus"
                    },
                    {
                        "name": "Steven Abreu"
                    }
                ],
                "author_detail": {
                    "name": "Steven Abreu"
                },
                "author": "Steven Abreu",
                "arxiv_comment": "Presented at the MINT workshop at NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.16314v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.16314v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.11295v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.11295v6",
                "updated": "2024-11-29T11:47:55Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    11,
                    47,
                    55,
                    4,
                    334,
                    0
                ],
                "published": "2024-02-17T14:26:57Z",
                "published_parsed": [
                    2024,
                    2,
                    17,
                    14,
                    26,
                    57,
                    5,
                    48,
                    0
                ],
                "title": "OneBit: Towards Extremely Low-bit Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OneBit: Towards Extremely Low-bit Large Language Models"
                },
                "summary": "Model quantification uses low bit-width values to represent the weight\nmatrices of existing models to be quantized, which is a promising approach to\nreduce both storage and computational overheads of deploying highly anticipated\nLLMs. However, current quantization methods suffer severe performance\ndegradation when the bit-width is extremely reduced, and thus focus on\nutilizing 4-bit or 8-bit values to quantize models. This paper boldly quantizes\nthe weight matrices of LLMs to 1-bit, paving the way for the extremely low\nbit-width deployment of LLMs. For this target, we introduce a 1-bit model\ncompressing framework named OneBit, including a novel 1-bit parameter\nrepresentation method to better quantize LLMs as well as an effective parameter\ninitialization method based on matrix decomposition to improve the convergence\nspeed of the quantization framework. Sufficient experimental results indicate\nthat OneBit achieves good performance (at least 81% of the non-quantized\nperformance on LLaMA models) with robust training processes when only using\n1-bit weight matrices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model quantification uses low bit-width values to represent the weight\nmatrices of existing models to be quantized, which is a promising approach to\nreduce both storage and computational overheads of deploying highly anticipated\nLLMs. However, current quantization methods suffer severe performance\ndegradation when the bit-width is extremely reduced, and thus focus on\nutilizing 4-bit or 8-bit values to quantize models. This paper boldly quantizes\nthe weight matrices of LLMs to 1-bit, paving the way for the extremely low\nbit-width deployment of LLMs. For this target, we introduce a 1-bit model\ncompressing framework named OneBit, including a novel 1-bit parameter\nrepresentation method to better quantize LLMs as well as an effective parameter\ninitialization method based on matrix decomposition to improve the convergence\nspeed of the quantization framework. Sufficient experimental results indicate\nthat OneBit achieves good performance (at least 81% of the non-quantized\nperformance on LLaMA models) with robust training processes when only using\n1-bit weight matrices."
                },
                "authors": [
                    {
                        "name": "Yuzhuang Xu"
                    },
                    {
                        "name": "Xu Han"
                    },
                    {
                        "name": "Zonghan Yang"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Qingfu Zhu"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Weidong Liu"
                    },
                    {
                        "name": "Wanxiang Che"
                    }
                ],
                "author_detail": {
                    "name": "Wanxiang Che"
                },
                "author": "Wanxiang Che",
                "arxiv_comment": "Accepted by NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.11295v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.11295v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19638v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19638v1",
                "updated": "2024-11-29T11:42:58Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    11,
                    42,
                    58,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T11:42:58Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    11,
                    42,
                    58,
                    4,
                    334,
                    0
                ],
                "title": "LLM Teacher-Student Framework for Text Classification With No Manually\n  Annotated Data: A Case Study in IPTC News Topic Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Teacher-Student Framework for Text Classification With No Manually\n  Annotated Data: A Case Study in IPTC News Topic Classification"
                },
                "summary": "With the ever-increasing number of news stories available online, classifying\nthem by topic, regardless of the language they are written in, has become\ncrucial for enhancing readers' access to relevant content. To address this\nchallenge, we propose a teacher-student framework based on large language\nmodels (LLMs) for developing multilingual news classification models of\nreasonable size with no need for manual data annotation. The framework employs\na Generative Pretrained Transformer (GPT) model as the teacher model to develop\nan IPTC Media Topic training dataset through automatic annotation of news\narticles in Slovenian, Croatian, Greek, and Catalan. The teacher model exhibits\na high zero-shot performance on all four languages. Its agreement with human\nannotators is comparable to that between the human annotators themselves. To\nmitigate the computational limitations associated with the requirement of\nprocessing millions of texts daily, smaller BERT-like student models are\nfine-tuned on the GPT-annotated dataset. These student models achieve high\nperformance comparable to the teacher model. Furthermore, we explore the impact\nof the training data size on the performance of the student models and\ninvestigate their monolingual, multilingual and zero-shot cross-lingual\ncapabilities. The findings indicate that student models can achieve high\nperformance with a relatively small number of training instances, and\ndemonstrate strong zero-shot cross-lingual abilities. Finally, we publish the\nbest-performing news topic classifier, enabling multilingual classification\nwith the top-level categories of the IPTC Media Topic schema.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the ever-increasing number of news stories available online, classifying\nthem by topic, regardless of the language they are written in, has become\ncrucial for enhancing readers' access to relevant content. To address this\nchallenge, we propose a teacher-student framework based on large language\nmodels (LLMs) for developing multilingual news classification models of\nreasonable size with no need for manual data annotation. The framework employs\na Generative Pretrained Transformer (GPT) model as the teacher model to develop\nan IPTC Media Topic training dataset through automatic annotation of news\narticles in Slovenian, Croatian, Greek, and Catalan. The teacher model exhibits\na high zero-shot performance on all four languages. Its agreement with human\nannotators is comparable to that between the human annotators themselves. To\nmitigate the computational limitations associated with the requirement of\nprocessing millions of texts daily, smaller BERT-like student models are\nfine-tuned on the GPT-annotated dataset. These student models achieve high\nperformance comparable to the teacher model. Furthermore, we explore the impact\nof the training data size on the performance of the student models and\ninvestigate their monolingual, multilingual and zero-shot cross-lingual\ncapabilities. The findings indicate that student models can achieve high\nperformance with a relatively small number of training instances, and\ndemonstrate strong zero-shot cross-lingual abilities. Finally, we publish the\nbest-performing news topic classifier, enabling multilingual classification\nwith the top-level categories of the IPTC Media Topic schema."
                },
                "authors": [
                    {
                        "name": "Taja Kuzman"
                    },
                    {
                        "name": "Nikola LjubeÅ¡iÄ"
                    }
                ],
                "author_detail": {
                    "name": "Nikola LjubeÅ¡iÄ"
                },
                "author": "Nikola LjubeÅ¡iÄ",
                "arxiv_comment": "This work has been submitted to the IEEE for possible publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19638v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19638v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19635v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19635v1",
                "updated": "2024-11-29T11:37:12Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    11,
                    37,
                    12,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T11:37:12Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    11,
                    37,
                    12,
                    4,
                    334,
                    0
                ],
                "title": "Build An Influential Bot In Social Media Simulations With Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Build An Influential Bot In Social Media Simulations With Large Language\n  Models"
                },
                "summary": "Understanding the dynamics of public opinion evolution on online social\nplatforms is critical for analyzing influence mechanisms. Traditional\napproaches to influencer analysis are typically divided into qualitative\nassessments of personal attributes and quantitative evaluations of influence\npower. In this study, we introduce a novel simulated environment that combines\nAgent-Based Modeling (ABM) with Large Language Models (LLMs), enabling agents\nto generate posts, form opinions, and update follower networks. This simulation\nallows for more detailed observations of how opinion leaders emerge.\nAdditionally, we present an innovative application of Reinforcement Learning\n(RL) to replicate the process of opinion leader formation. Our findings reveal\nthat limiting the action space and incorporating self-observation are key\nfactors for achieving stable opinion leader generation. The learning curves\ndemonstrate the model's capacity to identify optimal strategies and adapt to\ncomplex, unpredictable dynamics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the dynamics of public opinion evolution on online social\nplatforms is critical for analyzing influence mechanisms. Traditional\napproaches to influencer analysis are typically divided into qualitative\nassessments of personal attributes and quantitative evaluations of influence\npower. In this study, we introduce a novel simulated environment that combines\nAgent-Based Modeling (ABM) with Large Language Models (LLMs), enabling agents\nto generate posts, form opinions, and update follower networks. This simulation\nallows for more detailed observations of how opinion leaders emerge.\nAdditionally, we present an innovative application of Reinforcement Learning\n(RL) to replicate the process of opinion leader formation. Our findings reveal\nthat limiting the action space and incorporating self-observation are key\nfactors for achieving stable opinion leader generation. The learning curves\ndemonstrate the model's capacity to identify optimal strategies and adapt to\ncomplex, unpredictable dynamics."
                },
                "authors": [
                    {
                        "name": "Bailu Jin"
                    },
                    {
                        "name": "Weisi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Weisi Guo"
                },
                "author": "Weisi Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19635v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19635v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14569v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14569v2",
                "updated": "2024-11-29T11:30:49Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    11,
                    30,
                    49,
                    4,
                    334,
                    0
                ],
                "published": "2024-10-18T16:16:34Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    16,
                    16,
                    34,
                    4,
                    292,
                    0
                ],
                "title": "When LLMs Go Online: The Emerging Threat of Web-Enabled LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When LLMs Go Online: The Emerging Threat of Web-Enabled LLMs"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) have established them as\nagentic systems capable of planning and interacting with various tools. These\nLLM agents are often paired with web-based tools, enabling access to diverse\nsources and real-time information. Although these advancements offer\nsignificant benefits across various applications, they also increase the risk\nof malicious use, particularly in cyberattacks involving personal information.\nIn this work, we investigate the risks associated with misuse of LLM agents in\ncyberattacks involving personal data. Specifically, we aim to understand: 1)\nhow potent LLM agents can be when directed to conduct cyberattacks, 2) how\ncyberattacks are enhanced by web-based tools, and 3) how affordable and easy it\nbecomes to launch cyberattacks using LLM agents. We examine three attack\nscenarios: the collection of Personally Identifiable Information (PII), the\ngeneration of impersonation posts, and the creation of spear-phishing emails.\nOur experiments reveal the effectiveness of LLM agents in these attacks: LLM\nagents achieved a precision of up to 95.9% in collecting PII, up to 93.9% of\nimpersonation posts created by LLM agents were evaluated as authentic, and the\nclick rate for links in spear phishing emails created by LLM agents reached up\nto 46.67%. Additionally, our findings underscore the limitations of existing\nsafeguards in contemporary commercial LLMs, emphasizing the urgent need for\nmore robust security measures to prevent the misuse of LLM agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) have established them as\nagentic systems capable of planning and interacting with various tools. These\nLLM agents are often paired with web-based tools, enabling access to diverse\nsources and real-time information. Although these advancements offer\nsignificant benefits across various applications, they also increase the risk\nof malicious use, particularly in cyberattacks involving personal information.\nIn this work, we investigate the risks associated with misuse of LLM agents in\ncyberattacks involving personal data. Specifically, we aim to understand: 1)\nhow potent LLM agents can be when directed to conduct cyberattacks, 2) how\ncyberattacks are enhanced by web-based tools, and 3) how affordable and easy it\nbecomes to launch cyberattacks using LLM agents. We examine three attack\nscenarios: the collection of Personally Identifiable Information (PII), the\ngeneration of impersonation posts, and the creation of spear-phishing emails.\nOur experiments reveal the effectiveness of LLM agents in these attacks: LLM\nagents achieved a precision of up to 95.9% in collecting PII, up to 93.9% of\nimpersonation posts created by LLM agents were evaluated as authentic, and the\nclick rate for links in spear phishing emails created by LLM agents reached up\nto 46.67%. Additionally, our findings underscore the limitations of existing\nsafeguards in contemporary commercial LLMs, emphasizing the urgent need for\nmore robust security measures to prevent the misuse of LLM agents."
                },
                "authors": [
                    {
                        "name": "Hanna Kim"
                    },
                    {
                        "name": "Minkyoo Song"
                    },
                    {
                        "name": "Seung Ho Na"
                    },
                    {
                        "name": "Seungwon Shin"
                    },
                    {
                        "name": "Kimin Lee"
                    }
                ],
                "author_detail": {
                    "name": "Kimin Lee"
                },
                "author": "Kimin Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14569v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14569v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.13299v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.13299v2",
                "updated": "2024-11-29T11:21:29Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    11,
                    21,
                    29,
                    4,
                    334,
                    0
                ],
                "published": "2024-10-17T07:55:47Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    7,
                    55,
                    47,
                    3,
                    291,
                    0
                ],
                "title": "LLM-Rank: A Graph Theoretical Approach to Pruning Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Rank: A Graph Theoretical Approach to Pruning Large Language Models"
                },
                "summary": "The evolving capabilities of large language models are accompanied by growing\nsizes and deployment costs, necessitating effective inference optimisation\ntechniques. We propose a novel pruning method utilising centrality measures\nfrom graph theory, reducing both the computational requirements and the memory\nfootprint of these models. Specifically, we devise a method for creating a\nweighted directed acyclical graph representation of multilayer perceptrons to\nwhich we apply a modified version of the weighted PageRank centrality measure\nto compute node importance scores. In combination with uniform pruning this\nleads to structured sparsity. We call this pruning method MLPRank. Furthermore\nwe introduce an extension to decoder-only transformer models and call it\nLLMRank. For both variants we demonstrate a strong performance. With MLPRank on\naverage leading to 6.09 % higher accuracy retention than three popular\nbaselines and 13.42 % with LLMRank compared to two popular baselines. Code is\navailable at https://github.com/amazon-science/llm-rank-pruning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The evolving capabilities of large language models are accompanied by growing\nsizes and deployment costs, necessitating effective inference optimisation\ntechniques. We propose a novel pruning method utilising centrality measures\nfrom graph theory, reducing both the computational requirements and the memory\nfootprint of these models. Specifically, we devise a method for creating a\nweighted directed acyclical graph representation of multilayer perceptrons to\nwhich we apply a modified version of the weighted PageRank centrality measure\nto compute node importance scores. In combination with uniform pruning this\nleads to structured sparsity. We call this pruning method MLPRank. Furthermore\nwe introduce an extension to decoder-only transformer models and call it\nLLMRank. For both variants we demonstrate a strong performance. With MLPRank on\naverage leading to 6.09 % higher accuracy retention than three popular\nbaselines and 13.42 % with LLMRank compared to two popular baselines. Code is\navailable at https://github.com/amazon-science/llm-rank-pruning."
                },
                "authors": [
                    {
                        "name": "David Hoffmann"
                    },
                    {
                        "name": "Kailash Budhathoki"
                    },
                    {
                        "name": "Matthaeus Kleindessner"
                    }
                ],
                "author_detail": {
                    "name": "Matthaeus Kleindessner"
                },
                "author": "Matthaeus Kleindessner",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.13299v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.13299v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.12025v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.12025v3",
                "updated": "2024-11-29T10:46:22Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    10,
                    46,
                    22,
                    4,
                    334,
                    0
                ],
                "published": "2024-02-19T10:34:13Z",
                "published_parsed": [
                    2024,
                    2,
                    19,
                    10,
                    34,
                    13,
                    0,
                    50,
                    0
                ],
                "title": "Speech Translation with Speech Foundation Models and Large Language\n  Models: What is There and What is Missing?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speech Translation with Speech Foundation Models and Large Language\n  Models: What is There and What is Missing?"
                },
                "summary": "The field of natural language processing (NLP) has recently witnessed a\ntransformative shift with the emergence of foundation models, particularly\nLarge Language Models (LLMs) that have revolutionized text-based NLP. This\nparadigm has extended to other modalities, including speech, where researchers\nare actively exploring the combination of Speech Foundation Models (SFMs) and\nLLMs into single, unified models capable of addressing multimodal tasks. Among\nsuch tasks, this paper focuses on speech-to-text translation (ST). By examining\nthe published papers on the topic, we propose a unified view of the\narchitectural solutions and training strategies presented so far, highlighting\nsimilarities and differences among them. Based on this examination, we not only\norganize the lessons learned but also show how diverse settings and evaluation\napproaches hinder the identification of the best-performing solution for each\narchitectural building block and training choice. Lastly, we outline\nrecommendations for future works on the topic aimed at better understanding the\nstrengths and weaknesses of the SFM+LLM solutions for ST.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The field of natural language processing (NLP) has recently witnessed a\ntransformative shift with the emergence of foundation models, particularly\nLarge Language Models (LLMs) that have revolutionized text-based NLP. This\nparadigm has extended to other modalities, including speech, where researchers\nare actively exploring the combination of Speech Foundation Models (SFMs) and\nLLMs into single, unified models capable of addressing multimodal tasks. Among\nsuch tasks, this paper focuses on speech-to-text translation (ST). By examining\nthe published papers on the topic, we propose a unified view of the\narchitectural solutions and training strategies presented so far, highlighting\nsimilarities and differences among them. Based on this examination, we not only\norganize the lessons learned but also show how diverse settings and evaluation\napproaches hinder the identification of the best-performing solution for each\narchitectural building block and training choice. Lastly, we outline\nrecommendations for future works on the topic aimed at better understanding the\nstrengths and weaknesses of the SFM+LLM solutions for ST."
                },
                "authors": [
                    {
                        "name": "Marco Gaido"
                    },
                    {
                        "name": "Sara Papi"
                    },
                    {
                        "name": "Matteo Negri"
                    },
                    {
                        "name": "Luisa Bentivogli"
                    }
                ],
                "author_detail": {
                    "name": "Luisa Bentivogli"
                },
                "author": "Luisa Bentivogli",
                "arxiv_comment": "Outstanding paper at the ACL 2024 main conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.12025v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.12025v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.08367v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.08367v2",
                "updated": "2024-11-29T10:39:26Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    10,
                    39,
                    26,
                    4,
                    334,
                    0
                ],
                "published": "2023-10-12T14:38:25Z",
                "published_parsed": [
                    2023,
                    10,
                    12,
                    14,
                    38,
                    25,
                    3,
                    285,
                    0
                ],
                "title": "Towards Evaluating Generalist Agents: An Automated Benchmark in Open\n  World",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Evaluating Generalist Agents: An Automated Benchmark in Open\n  World"
                },
                "summary": "Evaluating generalist agents presents significant challenges due to their\nwide-ranging abilities and the limitations of current benchmarks in assessing\ntrue generalization. We introduce the Minecraft Universe (MCU), a fully\nautomated benchmarking framework set within the open-world game Minecraft. MCU\ndynamically generates and evaluates a broad spectrum of tasks, offering three\ncore components: 1) a task generation mechanism that provides high degrees of\nfreedom and variability, 2) an ever-expanding set of over 3K composable atomic\ntasks, and 3) a general evaluation framework that supports open-ended task\nassessment. By integrating large language models (LLMs), MCU dynamically\ncreates diverse environments for each evaluation, fostering agent\ngeneralization. The framework uses a vision-language model (VLM) to\nautomatically generate evaluation criteria, achieving over 90% agreement with\nhuman ratings across multi-dimensional assessments, which demonstrates that MCU\nis a scalable and explainable solution for evaluating generalist agents.\nAdditionally, we show that while state-of-the-art foundational models perform\nwell on specific tasks, they often struggle with increased task diversity and\ndifficulty.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating generalist agents presents significant challenges due to their\nwide-ranging abilities and the limitations of current benchmarks in assessing\ntrue generalization. We introduce the Minecraft Universe (MCU), a fully\nautomated benchmarking framework set within the open-world game Minecraft. MCU\ndynamically generates and evaluates a broad spectrum of tasks, offering three\ncore components: 1) a task generation mechanism that provides high degrees of\nfreedom and variability, 2) an ever-expanding set of over 3K composable atomic\ntasks, and 3) a general evaluation framework that supports open-ended task\nassessment. By integrating large language models (LLMs), MCU dynamically\ncreates diverse environments for each evaluation, fostering agent\ngeneralization. The framework uses a vision-language model (VLM) to\nautomatically generate evaluation criteria, achieving over 90% agreement with\nhuman ratings across multi-dimensional assessments, which demonstrates that MCU\nis a scalable and explainable solution for evaluating generalist agents.\nAdditionally, we show that while state-of-the-art foundational models perform\nwell on specific tasks, they often struggle with increased task diversity and\ndifficulty."
                },
                "authors": [
                    {
                        "name": "Xinyue Zheng"
                    },
                    {
                        "name": "Haowei Lin"
                    },
                    {
                        "name": "Kaichen He"
                    },
                    {
                        "name": "Zihao Wang"
                    },
                    {
                        "name": "Zilong Zheng"
                    },
                    {
                        "name": "Yitao Liang"
                    }
                ],
                "author_detail": {
                    "name": "Yitao Liang"
                },
                "author": "Yitao Liang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.08367v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.08367v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06622v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06622v2",
                "updated": "2024-11-29T10:30:11Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    10,
                    30,
                    11,
                    4,
                    334,
                    0
                ],
                "published": "2024-09-10T16:22:18Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    16,
                    22,
                    18,
                    1,
                    254,
                    0
                ],
                "title": "Exploring Italian sentence embeddings properties through multi-tasking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Italian sentence embeddings properties through multi-tasking"
                },
                "summary": "We investigate to what degree existing LLMs encode abstract linguistic\ninformation in Italian in a multi-task setting. We exploit curated synthetic\ndata on a large scale -- several Blackbird Language Matrices (BLMs) problems in\nItalian -- and use them to study how sentence representations built using\npre-trained language models encode specific syntactic and semantic information.\nWe use a two-level architecture to model separately a compression of the\nsentence embeddings into a representation that contains relevant information\nfor a task, and a BLM task. We then investigate whether we can obtain\ncompressed sentence representations that encode syntactic and semantic\ninformation relevant to several BLM tasks. While we expected that the sentence\nstructure -- in terms of sequence of phrases/chunks -- and chunk properties\ncould be shared across tasks, performance and error analysis show that the\nclues for the different tasks are encoded in different manners in the sentence\nembeddings, suggesting that abstract linguistic notions such as constituents or\nthematic roles does not seem to be present in the pretrained sentence\nembeddings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate to what degree existing LLMs encode abstract linguistic\ninformation in Italian in a multi-task setting. We exploit curated synthetic\ndata on a large scale -- several Blackbird Language Matrices (BLMs) problems in\nItalian -- and use them to study how sentence representations built using\npre-trained language models encode specific syntactic and semantic information.\nWe use a two-level architecture to model separately a compression of the\nsentence embeddings into a representation that contains relevant information\nfor a task, and a BLM task. We then investigate whether we can obtain\ncompressed sentence representations that encode syntactic and semantic\ninformation relevant to several BLM tasks. While we expected that the sentence\nstructure -- in terms of sequence of phrases/chunks -- and chunk properties\ncould be shared across tasks, performance and error analysis show that the\nclues for the different tasks are encoded in different manners in the sentence\nembeddings, suggesting that abstract linguistic notions such as constituents or\nthematic roles does not seem to be present in the pretrained sentence\nembeddings."
                },
                "authors": [
                    {
                        "name": "Vivi Nastase"
                    },
                    {
                        "name": "Giuseppe Samo"
                    },
                    {
                        "name": "Chunyang Jiang"
                    },
                    {
                        "name": "Paola Merlo"
                    }
                ],
                "author_detail": {
                    "name": "Paola Merlo"
                },
                "author": "Paola Merlo",
                "arxiv_comment": "11 pages, 6 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06622v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06622v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.02085v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.02085v4",
                "updated": "2024-11-29T10:10:43Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    10,
                    10,
                    43,
                    4,
                    334,
                    0
                ],
                "published": "2024-08-04T16:50:07Z",
                "published_parsed": [
                    2024,
                    8,
                    4,
                    16,
                    50,
                    7,
                    6,
                    217,
                    0
                ],
                "title": "Unleashing the Power of Data Tsunami: A Comprehensive Survey on Data\n  Assessment and Selection for Instruction Tuning of Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unleashing the Power of Data Tsunami: A Comprehensive Survey on Data\n  Assessment and Selection for Instruction Tuning of Language Models"
                },
                "summary": "Instruction tuning plays a critical role in aligning large language models\n(LLMs) with human preference. Despite the vast amount of open instruction\ndatasets, naively training a LLM on all existing instructions may not be\noptimal and practical. To pinpoint the most beneficial datapoints, data\nassessment and selection methods have been proposed in the fields of natural\nlanguage processing (NLP) and deep learning. However, under the context of\ninstruction tuning, there still exists a gap in knowledge on what kind of data\nevaluation metrics can be employed and how they can be integrated into the\nselection mechanism. To bridge this gap, we present a comprehensive review on\nexisting literature of data assessment and selection especially for instruction\ntuning of LLMs. We systematically categorize all applicable methods into\nquality-based, diversity-based, and importance-based ones where a unified,\nfine-grained taxonomy is structured. For each category, representative methods\nare elaborated to describe the landscape of relevant research. In addition,\ncomparison between the latest methods is conducted on their officially reported\nresults to provide in-depth discussions on their limitations. Finally, we\nsummarize the open challenges and propose the promosing avenues for future\nstudies. All related contents are available at\nhttps://github.com/yuleiqin/fantastic-data-engineering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction tuning plays a critical role in aligning large language models\n(LLMs) with human preference. Despite the vast amount of open instruction\ndatasets, naively training a LLM on all existing instructions may not be\noptimal and practical. To pinpoint the most beneficial datapoints, data\nassessment and selection methods have been proposed in the fields of natural\nlanguage processing (NLP) and deep learning. However, under the context of\ninstruction tuning, there still exists a gap in knowledge on what kind of data\nevaluation metrics can be employed and how they can be integrated into the\nselection mechanism. To bridge this gap, we present a comprehensive review on\nexisting literature of data assessment and selection especially for instruction\ntuning of LLMs. We systematically categorize all applicable methods into\nquality-based, diversity-based, and importance-based ones where a unified,\nfine-grained taxonomy is structured. For each category, representative methods\nare elaborated to describe the landscape of relevant research. In addition,\ncomparison between the latest methods is conducted on their officially reported\nresults to provide in-depth discussions on their limitations. Finally, we\nsummarize the open challenges and propose the promosing avenues for future\nstudies. All related contents are available at\nhttps://github.com/yuleiqin/fantastic-data-engineering."
                },
                "authors": [
                    {
                        "name": "Yulei Qin"
                    },
                    {
                        "name": "Yuncheng Yang"
                    },
                    {
                        "name": "Pengcheng Guo"
                    },
                    {
                        "name": "Gang Li"
                    },
                    {
                        "name": "Hang Shao"
                    },
                    {
                        "name": "Yuchen Shi"
                    },
                    {
                        "name": "Zihan Xu"
                    },
                    {
                        "name": "Yun Gu"
                    },
                    {
                        "name": "Ke Li"
                    },
                    {
                        "name": "Xing Sun"
                    }
                ],
                "author_detail": {
                    "name": "Xing Sun"
                },
                "author": "Xing Sun",
                "arxiv_comment": "review, survey, 37 pages, 5 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.02085v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.02085v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19589v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19589v1",
                "updated": "2024-11-29T10:10:16Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    10,
                    10,
                    16,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T10:10:16Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    10,
                    10,
                    16,
                    4,
                    334,
                    0
                ],
                "title": "Can Large Language Models Reason about the Region Connection Calculus?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Large Language Models Reason about the Region Connection Calculus?"
                },
                "summary": "Qualitative Spatial Reasoning is a well explored area of Knowledge\nRepresentation and Reasoning and has multiple applications ranging from\nGeographical Information Systems to Robotics and Computer Vision. Recently,\nmany claims have been made for the reasoning capabilities of Large Language\nModels (LLMs). Here, we investigate the extent to which a set of representative\nLLMs can perform classical qualitative spatial reasoning tasks on the\nmereotopological Region Connection Calculus, RCC-8. We conduct three pairs of\nexperiments (reconstruction of composition tables, alignment to human\ncomposition preferences, conceptual neighbourhood reconstruction) using\nstate-of-the-art LLMs; in each pair one experiment uses eponymous relations and\none, anonymous relations (to test the extent to which the LLM relies on\nknowledge about the relation names obtained during training). All instances are\nrepeated 30 times to measure the stochasticity of the LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Qualitative Spatial Reasoning is a well explored area of Knowledge\nRepresentation and Reasoning and has multiple applications ranging from\nGeographical Information Systems to Robotics and Computer Vision. Recently,\nmany claims have been made for the reasoning capabilities of Large Language\nModels (LLMs). Here, we investigate the extent to which a set of representative\nLLMs can perform classical qualitative spatial reasoning tasks on the\nmereotopological Region Connection Calculus, RCC-8. We conduct three pairs of\nexperiments (reconstruction of composition tables, alignment to human\ncomposition preferences, conceptual neighbourhood reconstruction) using\nstate-of-the-art LLMs; in each pair one experiment uses eponymous relations and\none, anonymous relations (to test the extent to which the LLM relies on\nknowledge about the relation names obtained during training). All instances are\nrepeated 30 times to measure the stochasticity of the LLMs."
                },
                "authors": [
                    {
                        "name": "Anthony G Cohn"
                    },
                    {
                        "name": "Robert E Blackwell"
                    }
                ],
                "author_detail": {
                    "name": "Robert E Blackwell"
                },
                "author": "Robert E Blackwell",
                "arxiv_comment": "13 pages. arXiv admin note: text overlap with arXiv:2309.15577",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19589v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19589v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19581v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19581v1",
                "updated": "2024-11-29T09:54:08Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    9,
                    54,
                    8,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T09:54:08Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    9,
                    54,
                    8,
                    4,
                    334,
                    0
                ],
                "title": "In-Context Learning with Noisy Labels",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-Context Learning with Noisy Labels"
                },
                "summary": "In-context learning refers to the emerging ability of large language models\n(LLMs) to perform a target task without additional training, utilizing\ndemonstrations of the task. Recent studies aim to enhance in-context learning\nperformance by selecting more useful demonstrations. However, they overlook the\npresence of inevitable noisy labels in task demonstrations that arise during\nthe labeling process in the real-world. In this paper, we propose a new task,\nin-context learning with noisy labels, which aims to solve real-world problems\nfor in-context learning where labels in task demonstrations would be corrupted.\nMoreover, we propose a new method and baseline methods for the new task,\ninspired by studies in learning with noisy labels. Through experiments, we\ndemonstrate that our proposed method can serve as a safeguard against\nperformance degradation in in-context learning caused by noisy labels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context learning refers to the emerging ability of large language models\n(LLMs) to perform a target task without additional training, utilizing\ndemonstrations of the task. Recent studies aim to enhance in-context learning\nperformance by selecting more useful demonstrations. However, they overlook the\npresence of inevitable noisy labels in task demonstrations that arise during\nthe labeling process in the real-world. In this paper, we propose a new task,\nin-context learning with noisy labels, which aims to solve real-world problems\nfor in-context learning where labels in task demonstrations would be corrupted.\nMoreover, we propose a new method and baseline methods for the new task,\ninspired by studies in learning with noisy labels. Through experiments, we\ndemonstrate that our proposed method can serve as a safeguard against\nperformance degradation in in-context learning caused by noisy labels."
                },
                "authors": [
                    {
                        "name": "Junyong Kang"
                    },
                    {
                        "name": "Donghyun Son"
                    },
                    {
                        "name": "Hwanjun Song"
                    },
                    {
                        "name": "Buru Chang"
                    }
                ],
                "author_detail": {
                    "name": "Buru Chang"
                },
                "author": "Buru Chang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19581v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19581v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11401v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11401v3",
                "updated": "2024-11-29T09:48:33Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    9,
                    48,
                    33,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-18T09:24:01Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    9,
                    24,
                    1,
                    0,
                    323,
                    0
                ],
                "title": "Deep Learning-based Code Reviews: A Paradigm Shift or a Double-Edged\n  Sword?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Learning-based Code Reviews: A Paradigm Shift or a Double-Edged\n  Sword?"
                },
                "summary": "Several techniques have been proposed to automate code review. Early support\nconsisted in recommending the most suited reviewer for a given change or in\nprioritizing the review tasks. With the advent of deep learning in software\nengineering, the level of automation has been pushed to new heights, with\napproaches able to provide feedback on source code in natural language as a\nhuman reviewer would do. Also, recent work documented open source projects\nadopting Large Language Models (LLMs) as co-reviewers. Although the research in\nthis field is very active, little is known about the actual impact of including\nautomatically generated code reviews in the code review process. While there\nare many aspects worth investigating, in this work we focus on three of them:\n(i) review quality, i.e., the reviewer's ability to identify issues in the\ncode; (ii) review cost, i.e., the time spent reviewing the code; and (iii)\nreviewer's confidence, i.e., how confident is the reviewer about the provided\nfeedback. We run a controlled experiment with 29 experts who reviewed different\nprograms with/without the support of an automatically generated code review.\nDuring the experiment we monitored the reviewers' activities, for over 50 hours\nof recorded code reviews. We show that reviewers consider valid most of the\nissues automatically identified by the LLM and that the availability of an\nautomated review as a starting point strongly influences their behavior:\nReviewers tend to focus on the code locations indicated by the LLM rather than\nsearching for additional issues in other parts of the code. The reviewers who\nstarted from an automated review identified a higher number of low-severity\nissues while, however, not identifying more high-severity issues as compared to\na completely manual process. Finally, the automated support did not result in\nsaved time and did not increase the reviewers' confidence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Several techniques have been proposed to automate code review. Early support\nconsisted in recommending the most suited reviewer for a given change or in\nprioritizing the review tasks. With the advent of deep learning in software\nengineering, the level of automation has been pushed to new heights, with\napproaches able to provide feedback on source code in natural language as a\nhuman reviewer would do. Also, recent work documented open source projects\nadopting Large Language Models (LLMs) as co-reviewers. Although the research in\nthis field is very active, little is known about the actual impact of including\nautomatically generated code reviews in the code review process. While there\nare many aspects worth investigating, in this work we focus on three of them:\n(i) review quality, i.e., the reviewer's ability to identify issues in the\ncode; (ii) review cost, i.e., the time spent reviewing the code; and (iii)\nreviewer's confidence, i.e., how confident is the reviewer about the provided\nfeedback. We run a controlled experiment with 29 experts who reviewed different\nprograms with/without the support of an automatically generated code review.\nDuring the experiment we monitored the reviewers' activities, for over 50 hours\nof recorded code reviews. We show that reviewers consider valid most of the\nissues automatically identified by the LLM and that the availability of an\nautomated review as a starting point strongly influences their behavior:\nReviewers tend to focus on the code locations indicated by the LLM rather than\nsearching for additional issues in other parts of the code. The reviewers who\nstarted from an automated review identified a higher number of low-severity\nissues while, however, not identifying more high-severity issues as compared to\na completely manual process. Finally, the automated support did not result in\nsaved time and did not increase the reviewers' confidence."
                },
                "authors": [
                    {
                        "name": "Rosalia Tufano"
                    },
                    {
                        "name": "Alberto Martin-Lopez"
                    },
                    {
                        "name": "Ahmad Tayeb"
                    },
                    {
                        "name": "Ozren DabiÄ"
                    },
                    {
                        "name": "Sonia Haiduc"
                    },
                    {
                        "name": "Gabriele Bavota"
                    }
                ],
                "author_detail": {
                    "name": "Gabriele Bavota"
                },
                "author": "Gabriele Bavota",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11401v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11401v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19576v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19576v1",
                "updated": "2024-11-29T09:47:32Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    9,
                    47,
                    32,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T09:47:32Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    9,
                    47,
                    32,
                    4,
                    334,
                    0
                ],
                "title": "A Review of LLM-based Explanations in Recommender Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Review of LLM-based Explanations in Recommender Systems"
                },
                "summary": "The rise of Large Language Models (LLMs), such as LLaMA and ChatGPT, has\nopened new opportunities for enhancing recommender systems through improved\nexplainability. This paper provides a systematic literature review focused on\nleveraging LLMs to generate explanations for recommendations -- a critical\naspect for fostering transparency and user trust. We conducted a comprehensive\nsearch within the ACM Guide to Computing Literature, covering publications from\nthe launch of ChatGPT (November 2022) to the present (November 2024). Our\nsearch yielded 232 articles, but after applying inclusion criteria, only six\nwere identified as directly addressing the use of LLMs in explaining\nrecommendations. This scarcity highlights that, despite the rise of LLMs, their\napplication in explainable recommender systems is still in an early stage. We\nanalyze these select studies to understand current methodologies, identify\nchallenges, and suggest directions for future research. Our findings underscore\nthe potential of LLMs improving explanations of recommender systems and\nencourage the development of more transparent and user-centric recommendation\nexplanation solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of Large Language Models (LLMs), such as LLaMA and ChatGPT, has\nopened new opportunities for enhancing recommender systems through improved\nexplainability. This paper provides a systematic literature review focused on\nleveraging LLMs to generate explanations for recommendations -- a critical\naspect for fostering transparency and user trust. We conducted a comprehensive\nsearch within the ACM Guide to Computing Literature, covering publications from\nthe launch of ChatGPT (November 2022) to the present (November 2024). Our\nsearch yielded 232 articles, but after applying inclusion criteria, only six\nwere identified as directly addressing the use of LLMs in explaining\nrecommendations. This scarcity highlights that, despite the rise of LLMs, their\napplication in explainable recommender systems is still in an early stage. We\nanalyze these select studies to understand current methodologies, identify\nchallenges, and suggest directions for future research. Our findings underscore\nthe potential of LLMs improving explanations of recommender systems and\nencourage the development of more transparent and user-centric recommendation\nexplanation solutions."
                },
                "authors": [
                    {
                        "name": "Alan Said"
                    }
                ],
                "author_detail": {
                    "name": "Alan Said"
                },
                "author": "Alan Said",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19576v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19576v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19563v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19563v1",
                "updated": "2024-11-29T09:18:32Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    9,
                    18,
                    32,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T09:18:32Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    9,
                    18,
                    32,
                    4,
                    334,
                    0
                ],
                "title": "Ensemble Watermarks for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ensemble Watermarks for Large Language Models"
                },
                "summary": "The rapid advancement of large language models (LLMs) has made it\nincreasingly difficult to distinguish between text written by humans and\nmachines. While watermarks already exist for LLMs, they often lack flexibility,\nand struggle with attacks such as paraphrasing. To address these issues, we\npropose a multi-feature method for generating watermarks that combines multiple\ndistinct watermark features into an ensemble watermark. Concretely, we combine\nacrostica and sensorimotor norms with the established red-green watermark to\nachieve a 98% detection rate. After a paraphrasing attack the performance\nremains high with 95% detection rate. The red-green feature alone as baseline\nachieves a detection rate of 49%. The evaluation of all feature combinations\nreveals that the ensemble of all three consistently has the highest detection\nrate across several LLMs and watermark strength settings. Due to the\nflexibility of combining features in the ensemble, various requirements and\ntrade-offs can be addressed. Additionally, for all ensemble configurations the\nsame detection function can be used without adaptations. This method is\nparticularly of interest to facilitate accountability and prevent societal\nharm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of large language models (LLMs) has made it\nincreasingly difficult to distinguish between text written by humans and\nmachines. While watermarks already exist for LLMs, they often lack flexibility,\nand struggle with attacks such as paraphrasing. To address these issues, we\npropose a multi-feature method for generating watermarks that combines multiple\ndistinct watermark features into an ensemble watermark. Concretely, we combine\nacrostica and sensorimotor norms with the established red-green watermark to\nachieve a 98% detection rate. After a paraphrasing attack the performance\nremains high with 95% detection rate. The red-green feature alone as baseline\nachieves a detection rate of 49%. The evaluation of all feature combinations\nreveals that the ensemble of all three consistently has the highest detection\nrate across several LLMs and watermark strength settings. Due to the\nflexibility of combining features in the ensemble, various requirements and\ntrade-offs can be addressed. Additionally, for all ensemble configurations the\nsame detection function can be used without adaptations. This method is\nparticularly of interest to facilitate accountability and prevent societal\nharm."
                },
                "authors": [
                    {
                        "name": "Georg Niess"
                    },
                    {
                        "name": "Roman Kern"
                    }
                ],
                "author_detail": {
                    "name": "Roman Kern"
                },
                "author": "Roman Kern",
                "arxiv_comment": "9 pages in the main body. Code is available at\n  http://github.com/CommodoreEU/master-generation. arXiv admin note:\n  substantial text overlap with arXiv:2405.08400",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19563v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19563v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06752v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06752v2",
                "updated": "2024-11-29T09:17:29Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    9,
                    17,
                    29,
                    4,
                    334,
                    0
                ],
                "published": "2024-08-13T09:19:21Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    9,
                    19,
                    21,
                    1,
                    226,
                    0
                ],
                "title": "Evaluating Research Quality with Large Language Models: An Analysis of\n  ChatGPT's Effectiveness with Different Settings and Inputs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Research Quality with Large Language Models: An Analysis of\n  ChatGPT's Effectiveness with Different Settings and Inputs"
                },
                "summary": "Evaluating the quality of academic journal articles is a time consuming but\ncritical task for national research evaluation exercises, appointments and\npromotion. It is therefore important to investigate whether Large Language\nModels (LLMs) can play a role in this process. This article assesses which\nChatGPT inputs (full text without tables, figures and references; title and\nabstract; title only) produce better quality score estimates, and the extent to\nwhich scores are affected by ChatGPT models and system prompts. The results\nshow that the optimal input is the article title and abstract, with average\nChatGPT scores based on these (30 iterations on a dataset of 51 papers)\ncorrelating at 0.67 with human scores, the highest ever reported. ChatGPT 4o is\nslightly better than 3.5-turbo (0.66), and 4o-mini (0.66). The results suggest\nthat article full texts might confuse LLM research quality evaluations, even\nthough complex system instructions for the task are more effective than simple\nones. Thus, whilst abstracts contain insufficient information for a thorough\nassessment of rigour, they may contain strong pointers about originality and\nsignificance. Finally, linear regression can be used to convert the model\nscores into the human scale scores, which is 31% more accurate than guessing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating the quality of academic journal articles is a time consuming but\ncritical task for national research evaluation exercises, appointments and\npromotion. It is therefore important to investigate whether Large Language\nModels (LLMs) can play a role in this process. This article assesses which\nChatGPT inputs (full text without tables, figures and references; title and\nabstract; title only) produce better quality score estimates, and the extent to\nwhich scores are affected by ChatGPT models and system prompts. The results\nshow that the optimal input is the article title and abstract, with average\nChatGPT scores based on these (30 iterations on a dataset of 51 papers)\ncorrelating at 0.67 with human scores, the highest ever reported. ChatGPT 4o is\nslightly better than 3.5-turbo (0.66), and 4o-mini (0.66). The results suggest\nthat article full texts might confuse LLM research quality evaluations, even\nthough complex system instructions for the task are more effective than simple\nones. Thus, whilst abstracts contain insufficient information for a thorough\nassessment of rigour, they may contain strong pointers about originality and\nsignificance. Finally, linear regression can be used to convert the model\nscores into the human scale scores, which is 31% more accurate than guessing."
                },
                "authors": [
                    {
                        "name": "Mike Thelwall"
                    }
                ],
                "author_detail": {
                    "name": "Mike Thelwall"
                },
                "author": "Mike Thelwall",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06752v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06752v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19557v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19557v1",
                "updated": "2024-11-29T09:10:30Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    9,
                    10,
                    30,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T09:10:30Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    9,
                    10,
                    30,
                    4,
                    334,
                    0
                ],
                "title": "Initialization using Update Approximation is a Silver Bullet for\n  Extremely Efficient Low-Rank Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Initialization using Update Approximation is a Silver Bullet for\n  Extremely Efficient Low-Rank Fine-Tuning"
                },
                "summary": "Low-rank adapters have become a standard approach for efficiently fine-tuning\nlarge language models (LLMs), but they often fall short of achieving the\nperformance of full fine-tuning. We propose a method, LoRA Silver Bullet or\nLoRA-SB, that approximates full fine-tuning within low-rank subspaces using a\ncarefully designed initialization strategy. We theoretically demonstrate that\nthe architecture of LoRA-XS, which inserts a trainable (r x r) matrix between B\nand A while keeping other matrices fixed, provides the precise conditions\nneeded for this approximation. We leverage its constrained update space to\nachieve optimal scaling for high-rank gradient updates while removing the need\nfor hyperparameter tuning. We prove that our initialization offers an optimal\nlow-rank approximation of the initial gradient and preserves update directions\nthroughout training. Extensive experiments across mathematical reasoning,\ncommonsense reasoning, and language understanding tasks demonstrate that our\napproach exceeds the performance of standard LoRA while using 27-90x fewer\nparameters, and comprehensively outperforms LoRA-XS. Our findings establish\nthat it is possible to simulate full fine-tuning in low-rank subspaces, and\nachieve significant efficiency gains without sacrificing performance. Our code\nis publicly available at https://github.com/RaghavSinghal10/lora-sb.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-rank adapters have become a standard approach for efficiently fine-tuning\nlarge language models (LLMs), but they often fall short of achieving the\nperformance of full fine-tuning. We propose a method, LoRA Silver Bullet or\nLoRA-SB, that approximates full fine-tuning within low-rank subspaces using a\ncarefully designed initialization strategy. We theoretically demonstrate that\nthe architecture of LoRA-XS, which inserts a trainable (r x r) matrix between B\nand A while keeping other matrices fixed, provides the precise conditions\nneeded for this approximation. We leverage its constrained update space to\nachieve optimal scaling for high-rank gradient updates while removing the need\nfor hyperparameter tuning. We prove that our initialization offers an optimal\nlow-rank approximation of the initial gradient and preserves update directions\nthroughout training. Extensive experiments across mathematical reasoning,\ncommonsense reasoning, and language understanding tasks demonstrate that our\napproach exceeds the performance of standard LoRA while using 27-90x fewer\nparameters, and comprehensively outperforms LoRA-XS. Our findings establish\nthat it is possible to simulate full fine-tuning in low-rank subspaces, and\nachieve significant efficiency gains without sacrificing performance. Our code\nis publicly available at https://github.com/RaghavSinghal10/lora-sb."
                },
                "authors": [
                    {
                        "name": "Kaustubh Ponkshe"
                    },
                    {
                        "name": "Raghav Singhal"
                    },
                    {
                        "name": "Eduard Gorbunov"
                    },
                    {
                        "name": "Alexey Tumanov"
                    },
                    {
                        "name": "Samuel Horvath"
                    },
                    {
                        "name": "Praneeth Vepakomma"
                    }
                ],
                "author_detail": {
                    "name": "Praneeth Vepakomma"
                },
                "author": "Praneeth Vepakomma",
                "arxiv_comment": "Kaustubh Ponkshe and Raghav Singhal contributed equally to this work",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19557v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19557v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19554v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19554v1",
                "updated": "2024-11-29T09:07:21Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    9,
                    7,
                    21,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T09:07:21Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    9,
                    7,
                    21,
                    4,
                    334,
                    0
                ],
                "title": "Unimib Assistant: designing a student-friendly RAG-based chatbot for all\n  their needs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unimib Assistant: designing a student-friendly RAG-based chatbot for all\n  their needs"
                },
                "summary": "Natural language processing skills of Large Language Models (LLMs) are\nunprecedented, having wide diffusion and application in different tasks. This\npilot study focuses on specializing ChatGPT behavior through a\nRetrieval-Augmented Generation (RAG) system using the OpenAI custom GPTs\nfeature. The purpose of our chatbot, called Unimib Assistant, is to provide\ninformation and solutions to the specific needs of University of Milano-Bicocca\n(Unimib) students through a question-answering approach. We provided the system\nwith a prompt highlighting its specific purpose and behavior, as well as\nuniversity-related documents and links obtained from an initial need-finding\nphase, interviewing six students. After a preliminary customization phase, a\nqualitative usability test was conducted with six other students to identify\nthe strengths and weaknesses of the chatbot, with the goal of improving it in a\nsubsequent redesign phase. While the chatbot was appreciated for its\nuser-friendly experience, perceived general reliability, well-structured\nresponses, and conversational tone, several significant technical and\nfunctional limitations emerged. In particular, the satisfaction and overall\nexperience of the users was impaired by the system's inability to always\nprovide fully accurate information. Moreover, it would often neglect to report\nrelevant information even if present in the materials uploaded and prompt\ngiven. Furthermore, it sometimes generated unclickable links, undermining its\ntrustworthiness, since providing the source of information was an important\naspect for our users. Further in-depth studies and feedback from other users as\nwell as implementation iterations are planned to refine our Unimib Assistant.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Natural language processing skills of Large Language Models (LLMs) are\nunprecedented, having wide diffusion and application in different tasks. This\npilot study focuses on specializing ChatGPT behavior through a\nRetrieval-Augmented Generation (RAG) system using the OpenAI custom GPTs\nfeature. The purpose of our chatbot, called Unimib Assistant, is to provide\ninformation and solutions to the specific needs of University of Milano-Bicocca\n(Unimib) students through a question-answering approach. We provided the system\nwith a prompt highlighting its specific purpose and behavior, as well as\nuniversity-related documents and links obtained from an initial need-finding\nphase, interviewing six students. After a preliminary customization phase, a\nqualitative usability test was conducted with six other students to identify\nthe strengths and weaknesses of the chatbot, with the goal of improving it in a\nsubsequent redesign phase. While the chatbot was appreciated for its\nuser-friendly experience, perceived general reliability, well-structured\nresponses, and conversational tone, several significant technical and\nfunctional limitations emerged. In particular, the satisfaction and overall\nexperience of the users was impaired by the system's inability to always\nprovide fully accurate information. Moreover, it would often neglect to report\nrelevant information even if present in the materials uploaded and prompt\ngiven. Furthermore, it sometimes generated unclickable links, undermining its\ntrustworthiness, since providing the source of information was an important\naspect for our users. Further in-depth studies and feedback from other users as\nwell as implementation iterations are planned to refine our Unimib Assistant."
                },
                "authors": [
                    {
                        "name": "Chiara Antico"
                    },
                    {
                        "name": "Stefano Giordano"
                    },
                    {
                        "name": "Cansu Koyuturk"
                    },
                    {
                        "name": "Dimitri Ognibene"
                    }
                ],
                "author_detail": {
                    "name": "Dimitri Ognibene"
                },
                "author": "Dimitri Ognibene",
                "arxiv_comment": "Accepted for Italian Workshop on Artificial Intelligence for Human\n  Machine Interaction (AIxHMI 2024), November 26, 2024, Bolzano, Italy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19554v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19554v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16205v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16205v3",
                "updated": "2024-11-29T08:48:17Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    8,
                    48,
                    17,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-25T09:05:36Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    9,
                    5,
                    36,
                    0,
                    330,
                    0
                ],
                "title": "MH-MoE: Multi-Head Mixture-of-Experts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MH-MoE: Multi-Head Mixture-of-Experts"
                },
                "summary": "Multi-Head Mixture-of-Experts (MH-MoE) demonstrates superior performance by\nusing the multi-head mechanism to collectively attend to information from\nvarious representation spaces within different experts. In this paper, we\npresent a novel implementation of MH-MoE that maintains both FLOPs and\nparameter parity with sparse Mixture of Experts models. Experimental results on\nlanguage models show that the new implementation yields quality improvements\nover both vanilla MoE and fine-grained MoE models. Additionally, our\nexperiments demonstrate that MH-MoE is compatible with 1-bit Large Language\nModels (LLMs) such as BitNet.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Head Mixture-of-Experts (MH-MoE) demonstrates superior performance by\nusing the multi-head mechanism to collectively attend to information from\nvarious representation spaces within different experts. In this paper, we\npresent a novel implementation of MH-MoE that maintains both FLOPs and\nparameter parity with sparse Mixture of Experts models. Experimental results on\nlanguage models show that the new implementation yields quality improvements\nover both vanilla MoE and fine-grained MoE models. Additionally, our\nexperiments demonstrate that MH-MoE is compatible with 1-bit Large Language\nModels (LLMs) such as BitNet."
                },
                "authors": [
                    {
                        "name": "Shaohan Huang"
                    },
                    {
                        "name": "Xun Wu"
                    },
                    {
                        "name": "Shuming Ma"
                    },
                    {
                        "name": "Furu Wei"
                    }
                ],
                "author_detail": {
                    "name": "Furu Wei"
                },
                "author": "Furu Wei",
                "arxiv_comment": "7 pages, 0 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16205v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16205v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19547v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19547v1",
                "updated": "2024-11-29T08:47:04Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    8,
                    47,
                    4,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T08:47:04Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    8,
                    47,
                    4,
                    4,
                    334,
                    0
                ],
                "title": "Training Agents with Weakly Supervised Feedback from Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training Agents with Weakly Supervised Feedback from Large Language\n  Models"
                },
                "summary": "Large Language Models (LLMs) offer a promising basis for creating agents that\ncan tackle complex tasks through iterative environmental interaction. Existing\nmethods either require these agents to mimic expert-provided trajectories or\nrely on definitive environmental feedback for reinforcement learning which\nlimits their application to specific scenarios like gaming or code generation.\nThis paper introduces a novel training method for LLM-based agents using weakly\nsupervised signals from a critic LLM, bypassing the need for expert\ntrajectories or definitive feedback. Our agents are trained in iterative\nmanner, where they initially generate trajectories through environmental\ninteraction. Subsequently, a critic LLM selects a subset of good trajectories,\nwhich are then used to update the agents, enabling them to generate improved\ntrajectories in the next iteration. Extensive tests on the API-bank dataset\nshow consistent improvement in our agents' capabilities and comparable\nperformance to GPT-4, despite using open-source models with much fewer\nparameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) offer a promising basis for creating agents that\ncan tackle complex tasks through iterative environmental interaction. Existing\nmethods either require these agents to mimic expert-provided trajectories or\nrely on definitive environmental feedback for reinforcement learning which\nlimits their application to specific scenarios like gaming or code generation.\nThis paper introduces a novel training method for LLM-based agents using weakly\nsupervised signals from a critic LLM, bypassing the need for expert\ntrajectories or definitive feedback. Our agents are trained in iterative\nmanner, where they initially generate trajectories through environmental\ninteraction. Subsequently, a critic LLM selects a subset of good trajectories,\nwhich are then used to update the agents, enabling them to generate improved\ntrajectories in the next iteration. Extensive tests on the API-bank dataset\nshow consistent improvement in our agents' capabilities and comparable\nperformance to GPT-4, despite using open-source models with much fewer\nparameters."
                },
                "authors": [
                    {
                        "name": "Dihong Gong"
                    },
                    {
                        "name": "Pu Lu"
                    },
                    {
                        "name": "Zelong Wang"
                    },
                    {
                        "name": "Meng Zhou"
                    },
                    {
                        "name": "Xiuqiang He"
                    }
                ],
                "author_detail": {
                    "name": "Xiuqiang He"
                },
                "author": "Xiuqiang He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19547v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19547v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19542v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19542v1",
                "updated": "2024-11-29T08:39:24Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    8,
                    39,
                    24,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T08:39:24Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    8,
                    39,
                    24,
                    4,
                    334,
                    0
                ],
                "title": "A dynamic parallel method for performance optimization on hybrid CPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A dynamic parallel method for performance optimization on hybrid CPUs"
                },
                "summary": "The AIPC concept is gaining popularity, and more and more hybrid CPUs will be\nrunning AI models on client devices. However, the current AI inference\nframework overlooks the imbalanced hardware capability of hybrid CPUs, leading\nto low inference performance. To address this issue, we have introduced a\ndynamic parallel method for hybrid CPUs, which significantly increases LLM\ninference performance by balancing the workload for each core of a hybrid CPU\nbefore the parallel work starts. This method has enabled Neural Speed to\nachieve more than 90% (on average) of memory bandwidth on two hybrid Intel\nCPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The AIPC concept is gaining popularity, and more and more hybrid CPUs will be\nrunning AI models on client devices. However, the current AI inference\nframework overlooks the imbalanced hardware capability of hybrid CPUs, leading\nto low inference performance. To address this issue, we have introduced a\ndynamic parallel method for hybrid CPUs, which significantly increases LLM\ninference performance by balancing the workload for each core of a hybrid CPU\nbefore the parallel work starts. This method has enabled Neural Speed to\nachieve more than 90% (on average) of memory bandwidth on two hybrid Intel\nCPUs."
                },
                "authors": [
                    {
                        "name": "Luo Yu"
                    },
                    {
                        "name": "Liu Yucheng"
                    },
                    {
                        "name": "Shen Haihao"
                    }
                ],
                "author_detail": {
                    "name": "Shen Haihao"
                },
                "author": "Shen Haihao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19542v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19542v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19539v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19539v1",
                "updated": "2024-11-29T08:34:07Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    8,
                    34,
                    7,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T08:34:07Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    8,
                    34,
                    7,
                    4,
                    334,
                    0
                ],
                "title": "Knowledge Management for Automobile Failure Analysis Using Graph RAG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Management for Automobile Failure Analysis Using Graph RAG"
                },
                "summary": "This paper presents a knowledge management system for automobile failure\nanalysis using retrieval-augmented generation (RAG) with large language models\n(LLMs) and knowledge graphs (KGs). In the automotive industry, there is a\ngrowing demand for knowledge transfer of failure analysis from experienced\nengineers to young engineers. However, failure events are phenomena that occur\nin a chain reaction, making them difficult for beginners to analyze them. While\nknowledge graphs, which can describe semantic relationships and structure\ninformation is effective in representing failure events, due to their\ncapability of representing the relationships between components, there is much\ninformation in KGs, so it is challenging for young engineers to extract and\nunderstand sub-graphs from the KG. On the other hand, there is increasing\ninterest in the use of Graph RAG, a type of RAG that combines LLMs and KGs for\nknowledge management. However, when using the current Graph RAG framework with\nan existing knowledge graph for automobile failures, several issues arise\nbecause it is difficult to generate executable queries for a knowledge graph\ndatabase which is not constructed by LLMs. To address this, we focused on\noptimizing the Graph RAG pipeline for existing knowledge graphs. Using an\noriginal Q&A dataset, the ROUGE F1 score of the sentences generated by the\nproposed method showed an average improvement of 157.6% compared to the current\nmethod. This highlights the effectiveness of the proposed method for automobile\nfailure analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a knowledge management system for automobile failure\nanalysis using retrieval-augmented generation (RAG) with large language models\n(LLMs) and knowledge graphs (KGs). In the automotive industry, there is a\ngrowing demand for knowledge transfer of failure analysis from experienced\nengineers to young engineers. However, failure events are phenomena that occur\nin a chain reaction, making them difficult for beginners to analyze them. While\nknowledge graphs, which can describe semantic relationships and structure\ninformation is effective in representing failure events, due to their\ncapability of representing the relationships between components, there is much\ninformation in KGs, so it is challenging for young engineers to extract and\nunderstand sub-graphs from the KG. On the other hand, there is increasing\ninterest in the use of Graph RAG, a type of RAG that combines LLMs and KGs for\nknowledge management. However, when using the current Graph RAG framework with\nan existing knowledge graph for automobile failures, several issues arise\nbecause it is difficult to generate executable queries for a knowledge graph\ndatabase which is not constructed by LLMs. To address this, we focused on\noptimizing the Graph RAG pipeline for existing knowledge graphs. Using an\noriginal Q&A dataset, the ROUGE F1 score of the sentences generated by the\nproposed method showed an average improvement of 157.6% compared to the current\nmethod. This highlights the effectiveness of the proposed method for automobile\nfailure analysis."
                },
                "authors": [
                    {
                        "name": "Yuta Ojima"
                    },
                    {
                        "name": "Hiroki Sakaji"
                    },
                    {
                        "name": "Tadashi Nakamura"
                    },
                    {
                        "name": "Hiroaki Sakata"
                    },
                    {
                        "name": "Kazuya Seki"
                    },
                    {
                        "name": "Yuu Teshigawara"
                    },
                    {
                        "name": "Masami Yamashita"
                    },
                    {
                        "name": "Kazuhiro Aoyama"
                    }
                ],
                "author_detail": {
                    "name": "Kazuhiro Aoyama"
                },
                "author": "Kazuhiro Aoyama",
                "arxiv_comment": "7 pages, 6 figures, to be published in 2024 IEEE International\n  Conference on Bid Data (BigData)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19539v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19539v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18191v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18191v2",
                "updated": "2024-11-29T08:33:49Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    8,
                    33,
                    49,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-27T10:14:38Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    10,
                    14,
                    38,
                    2,
                    332,
                    0
                ],
                "title": "InputSnatch: Stealing Input in LLM Services via Timing Side-Channel\n  Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InputSnatch: Stealing Input in LLM Services via Timing Side-Channel\n  Attacks"
                },
                "summary": "Large language models (LLMs) possess extensive knowledge and\nquestion-answering capabilities, having been widely deployed in\nprivacy-sensitive domains like finance and medical consultation. During LLM\ninferences, cache-sharing methods are commonly employed to enhance efficiency\nby reusing cached states or responses for the same or similar inference\nrequests. However, we identify that these cache mechanisms pose a risk of\nprivate input leakage, as the caching can result in observable variations in\nresponse times, making them a strong candidate for a timing-based attack hint.\n  In this study, we propose a novel timing-based side-channel attack to execute\ninput theft in LLMs inference. The cache-based attack faces the challenge of\nconstructing candidate inputs in a large search space to hit and steal cached\nuser queries. To address these challenges, we propose two primary components.\nThe input constructor employs machine learning techniques and LLM-based\napproaches for vocabulary correlation learning while implementing optimized\nsearch mechanisms for generalized input construction. The time analyzer\nimplements statistical time fitting with outlier elimination to identify cache\nhit patterns, continuously providing feedback to refine the constructor's\nsearch strategy. We conduct experiments across two cache mechanisms and the\nresults demonstrate that our approach consistently attains high attack success\nrates in various applications. Our work highlights the security vulnerabilities\nassociated with performance optimizations, underscoring the necessity of\nprioritizing privacy and security alongside enhancements in LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) possess extensive knowledge and\nquestion-answering capabilities, having been widely deployed in\nprivacy-sensitive domains like finance and medical consultation. During LLM\ninferences, cache-sharing methods are commonly employed to enhance efficiency\nby reusing cached states or responses for the same or similar inference\nrequests. However, we identify that these cache mechanisms pose a risk of\nprivate input leakage, as the caching can result in observable variations in\nresponse times, making them a strong candidate for a timing-based attack hint.\n  In this study, we propose a novel timing-based side-channel attack to execute\ninput theft in LLMs inference. The cache-based attack faces the challenge of\nconstructing candidate inputs in a large search space to hit and steal cached\nuser queries. To address these challenges, we propose two primary components.\nThe input constructor employs machine learning techniques and LLM-based\napproaches for vocabulary correlation learning while implementing optimized\nsearch mechanisms for generalized input construction. The time analyzer\nimplements statistical time fitting with outlier elimination to identify cache\nhit patterns, continuously providing feedback to refine the constructor's\nsearch strategy. We conduct experiments across two cache mechanisms and the\nresults demonstrate that our approach consistently attains high attack success\nrates in various applications. Our work highlights the security vulnerabilities\nassociated with performance optimizations, underscoring the necessity of\nprioritizing privacy and security alongside enhancements in LLM inference."
                },
                "authors": [
                    {
                        "name": "Xinyao Zheng"
                    },
                    {
                        "name": "Husheng Han"
                    },
                    {
                        "name": "Shangyi Shi"
                    },
                    {
                        "name": "Qiyan Fang"
                    },
                    {
                        "name": "Zidong Du"
                    },
                    {
                        "name": "Xing Hu"
                    },
                    {
                        "name": "Qi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Qi Guo"
                },
                "author": "Qi Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18191v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18191v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10666v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10666v2",
                "updated": "2024-11-29T08:16:29Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    8,
                    16,
                    29,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-16T02:02:49Z",
                "published_parsed": [
                    2024,
                    11,
                    16,
                    2,
                    2,
                    49,
                    5,
                    321,
                    0
                ],
                "title": "SAM Decoding: Speculative Decoding via Suffix Automaton",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SAM Decoding: Speculative Decoding via Suffix Automaton"
                },
                "summary": "Large Language Models (LLMs) have revolutionized natural language processing\nby unifying tasks into text generation, yet their large parameter sizes and\nautoregressive nature limit inference speed. SAM-Decoding addresses this by\nintroducing a novel retrieval-based speculative decoding method that uses a\nsuffix automaton for efficient and accurate draft generation. Unlike n-gram\nmatching used by the existing method, SAM-Decoding finds the longest suffix\nmatch in generating text and text corpuss, achieving an average time complexity\nof $O(1)$ per generation step. SAM-Decoding constructs static and dynamic\nsuffix automatons for the text corpus and input prompts, respectively, enabling\nfast and precise draft generation. Meanwhile, it is designed as an approach\nthat can be combined with existing methods, allowing SAM-Decoding to adaptively\nselect a draft generation strategy based on the matching length, thus\nincreasing the inference speed of the LLM. When combined with Token Recycling,\nevaluations show SAM-Decoding outperforms existing model-free methods,\nachieving a speedup of $2.27\\times$ over autoregressive decoding on Spec-Bench.\nWhen combined with EAGLE2, it reaches a speedup of $2.49\\times$, surpassing all\ncurrent approaches. Our code is available at\nhttps://github.com/hyx1999/SAM-Decoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized natural language processing\nby unifying tasks into text generation, yet their large parameter sizes and\nautoregressive nature limit inference speed. SAM-Decoding addresses this by\nintroducing a novel retrieval-based speculative decoding method that uses a\nsuffix automaton for efficient and accurate draft generation. Unlike n-gram\nmatching used by the existing method, SAM-Decoding finds the longest suffix\nmatch in generating text and text corpuss, achieving an average time complexity\nof $O(1)$ per generation step. SAM-Decoding constructs static and dynamic\nsuffix automatons for the text corpus and input prompts, respectively, enabling\nfast and precise draft generation. Meanwhile, it is designed as an approach\nthat can be combined with existing methods, allowing SAM-Decoding to adaptively\nselect a draft generation strategy based on the matching length, thus\nincreasing the inference speed of the LLM. When combined with Token Recycling,\nevaluations show SAM-Decoding outperforms existing model-free methods,\nachieving a speedup of $2.27\\times$ over autoregressive decoding on Spec-Bench.\nWhen combined with EAGLE2, it reaches a speedup of $2.49\\times$, surpassing all\ncurrent approaches. Our code is available at\nhttps://github.com/hyx1999/SAM-Decoding."
                },
                "authors": [
                    {
                        "name": "Yuxuan Hu"
                    },
                    {
                        "name": "Ke Wang"
                    },
                    {
                        "name": "Xiaokang Zhang"
                    },
                    {
                        "name": "Fanjin Zhang"
                    },
                    {
                        "name": "Cuiping Li"
                    },
                    {
                        "name": "Hong Chen"
                    },
                    {
                        "name": "Jing Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jing Zhang"
                },
                "author": "Jing Zhang",
                "arxiv_comment": "17 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10666v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10666v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19528v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19528v1",
                "updated": "2024-11-29T07:57:32Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    7,
                    57,
                    32,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T07:57:32Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    7,
                    57,
                    32,
                    4,
                    334,
                    0
                ],
                "title": "RAGDiffusion: Faithful Cloth Generation via External Knowledge\n  Assimilation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAGDiffusion: Faithful Cloth Generation via External Knowledge\n  Assimilation"
                },
                "summary": "Standard clothing asset generation involves creating forward-facing flat-lay\ngarment images displayed on a clear background by extracting clothing\ninformation from diverse real-world contexts, which presents significant\nchallenges due to highly standardized sampling distributions and precise\nstructural requirements in the generated images. Existing models have limited\nspatial perception and often exhibit structural hallucinations in this\nhigh-specification generative task. To address this issue, we propose a novel\nRetrieval-Augmented Generation (RAG) framework, termed RAGDiffusion, to enhance\nstructure determinacy and mitigate hallucinations by assimilating external\nknowledge from LLM and databases. RAGDiffusion consists of two core processes:\n(1) Retrieval-based structure aggregation, which employs contrastive learning\nand a Structure Locally Linear Embedding (SLLE) to derive global structure and\nspatial landmarks, providing both soft and hard guidance to counteract\nstructural ambiguities; and (2) Omni-level faithful garment generation, which\nintroduces a three-level alignment that ensures fidelity in structural,\npattern, and decoding components within the diffusing. Extensive experiments on\nchallenging real-world datasets demonstrate that RAGDiffusion synthesizes\nstructurally and detail-faithful clothing assets with significant performance\nimprovements, representing a pioneering effort in high-specification faithful\ngeneration with RAG to confront intrinsic hallucinations and enhance fidelity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Standard clothing asset generation involves creating forward-facing flat-lay\ngarment images displayed on a clear background by extracting clothing\ninformation from diverse real-world contexts, which presents significant\nchallenges due to highly standardized sampling distributions and precise\nstructural requirements in the generated images. Existing models have limited\nspatial perception and often exhibit structural hallucinations in this\nhigh-specification generative task. To address this issue, we propose a novel\nRetrieval-Augmented Generation (RAG) framework, termed RAGDiffusion, to enhance\nstructure determinacy and mitigate hallucinations by assimilating external\nknowledge from LLM and databases. RAGDiffusion consists of two core processes:\n(1) Retrieval-based structure aggregation, which employs contrastive learning\nand a Structure Locally Linear Embedding (SLLE) to derive global structure and\nspatial landmarks, providing both soft and hard guidance to counteract\nstructural ambiguities; and (2) Omni-level faithful garment generation, which\nintroduces a three-level alignment that ensures fidelity in structural,\npattern, and decoding components within the diffusing. Extensive experiments on\nchallenging real-world datasets demonstrate that RAGDiffusion synthesizes\nstructurally and detail-faithful clothing assets with significant performance\nimprovements, representing a pioneering effort in high-specification faithful\ngeneration with RAG to confront intrinsic hallucinations and enhance fidelity."
                },
                "authors": [
                    {
                        "name": "Xianfeng Tan"
                    },
                    {
                        "name": "Yuhan Li"
                    },
                    {
                        "name": "Wenxiang Shang"
                    },
                    {
                        "name": "Yubo Wu"
                    },
                    {
                        "name": "Jian Wang"
                    },
                    {
                        "name": "Xuanhong Chen"
                    },
                    {
                        "name": "Yi Zhang"
                    },
                    {
                        "name": "Ran Lin"
                    },
                    {
                        "name": "Bingbing Ni"
                    }
                ],
                "author_detail": {
                    "name": "Bingbing Ni"
                },
                "author": "Bingbing Ni",
                "arxiv_comment": "Project website: https://colorful-liyu.github.io/RAGDiffusion-page/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19528v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19528v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19515v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19515v1",
                "updated": "2024-11-29T07:18:50Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    7,
                    18,
                    50,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T07:18:50Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    7,
                    18,
                    50,
                    4,
                    334,
                    0
                ],
                "title": "Leveraging Large Language Models for Institutional Portfolio Management:\n  Persona-Based Ensembles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Large Language Models for Institutional Portfolio Management:\n  Persona-Based Ensembles"
                },
                "summary": "Large language models (LLMs) have demonstrated promising performance in\nvarious financial applications, though their potential in complex investment\nstrategies remains underexplored. To address this gap, we investigate how LLMs\ncan predict price movements in stock and bond portfolios using economic\nindicators, enabling portfolio adjustments akin to those employed by\ninstitutional investors. Additionally, we explore the impact of incorporating\ndifferent personas within LLMs, using an ensemble approach to leverage their\ndiverse predictions. Our findings show that LLM-based strategies, especially\nwhen combined with the mode ensemble, outperform the buy-and-hold strategy in\nterms of Sharpe ratio during periods of rising consumer price index (CPI).\nHowever, traditional strategies are more effective during declining CPI trends\nor sharp market downturns. These results suggest that while LLMs can enhance\nportfolio management, they may require complementary strategies to optimize\nperformance across varying market conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated promising performance in\nvarious financial applications, though their potential in complex investment\nstrategies remains underexplored. To address this gap, we investigate how LLMs\ncan predict price movements in stock and bond portfolios using economic\nindicators, enabling portfolio adjustments akin to those employed by\ninstitutional investors. Additionally, we explore the impact of incorporating\ndifferent personas within LLMs, using an ensemble approach to leverage their\ndiverse predictions. Our findings show that LLM-based strategies, especially\nwhen combined with the mode ensemble, outperform the buy-and-hold strategy in\nterms of Sharpe ratio during periods of rising consumer price index (CPI).\nHowever, traditional strategies are more effective during declining CPI trends\nor sharp market downturns. These results suggest that while LLMs can enhance\nportfolio management, they may require complementary strategies to optimize\nperformance across varying market conditions."
                },
                "authors": [
                    {
                        "name": "Yoshia Abe"
                    },
                    {
                        "name": "Shuhei Matsuo"
                    },
                    {
                        "name": "Ryoma Kondo"
                    },
                    {
                        "name": "Ryohei Hisano"
                    }
                ],
                "author_detail": {
                    "name": "Ryohei Hisano"
                },
                "author": "Ryohei Hisano",
                "arxiv_comment": "10 pages, 5 figures, submitted to The IEEE International Workshop on\n  Large Language Models for Finance 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19515v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19515v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19508v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19508v1",
                "updated": "2024-11-29T07:00:47Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    7,
                    0,
                    47,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T07:00:47Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    7,
                    0,
                    47,
                    4,
                    334,
                    0
                ],
                "title": "On the Adversarial Robustness of Instruction-Tuned Large Language Models\n  for Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Adversarial Robustness of Instruction-Tuned Large Language Models\n  for Code"
                },
                "summary": "The advent of instruction-tuned Large Language Models designed for coding\ntasks (Code LLMs) has transformed software engineering practices. However,\ntheir robustness against various input challenges remains a critical concern.\nThis study introduces DegradePrompter, a novel method designed to\nsystematically evaluate the robustness of instruction-tuned Code LLMs. We\nassess the impact of diverse input challenges on the functionality and\ncorrectness of generated code using rigorous metrics and established\nbenchmarks. Our comprehensive evaluation includes five state-of-the-art\nopen-source models and three production-grade closed-source models, revealing\nvarying degrees of robustness. Open-source models demonstrate an increased\nsusceptibility to input perturbations, resulting in declines in functional\ncorrectness ranging from 12% to 34%. In contrast, commercial models demonstrate\nrelatively greater resilience, with performance degradation ranging from 3% to\n24%. To enhance the robustness of the models against these vulnerabilities, we\ninvestigate a straightforward yet effective mitigation strategy. Our findings\nhighlight the need for robust defense mechanisms and comprehensive evaluations\nduring both the development and deployment phases to ensure the resilience and\nreliability of automated code generation systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advent of instruction-tuned Large Language Models designed for coding\ntasks (Code LLMs) has transformed software engineering practices. However,\ntheir robustness against various input challenges remains a critical concern.\nThis study introduces DegradePrompter, a novel method designed to\nsystematically evaluate the robustness of instruction-tuned Code LLMs. We\nassess the impact of diverse input challenges on the functionality and\ncorrectness of generated code using rigorous metrics and established\nbenchmarks. Our comprehensive evaluation includes five state-of-the-art\nopen-source models and three production-grade closed-source models, revealing\nvarying degrees of robustness. Open-source models demonstrate an increased\nsusceptibility to input perturbations, resulting in declines in functional\ncorrectness ranging from 12% to 34%. In contrast, commercial models demonstrate\nrelatively greater resilience, with performance degradation ranging from 3% to\n24%. To enhance the robustness of the models against these vulnerabilities, we\ninvestigate a straightforward yet effective mitigation strategy. Our findings\nhighlight the need for robust defense mechanisms and comprehensive evaluations\nduring both the development and deployment phases to ensure the resilience and\nreliability of automated code generation systems."
                },
                "authors": [
                    {
                        "name": "Md Imran Hossen"
                    },
                    {
                        "name": "Xiali Hei"
                    }
                ],
                "author_detail": {
                    "name": "Xiali Hei"
                },
                "author": "Xiali Hei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19508v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19508v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19506v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19506v1",
                "updated": "2024-11-29T06:56:42Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    6,
                    56,
                    42,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T06:56:42Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    6,
                    56,
                    42,
                    4,
                    334,
                    0
                ],
                "title": "Real-time Anomaly Detection at the L1 Trigger of CMS Experiment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time Anomaly Detection at the L1 Trigger of CMS Experiment"
                },
                "summary": "We present the preparation, deployment, and testing of an autoencoder trained\nfor unbiased detection of new physics signatures in the CMS experiment Global\nTrigger (GT) test crate FPGAs during LHC Run 3. The GT makes the final decision\nwhether to readout or discard the data from each LHC collision, which occur at\na rate of 40 MHz, within a 50 ns latency. The Neural Network makes a prediction\nfor each event within these constraints, which can be used to select anomalous\nevents for further analysis. The GT test crate is a copy of the main GT system,\nreceiving the same input data, but whose output is not used to trigger the\nreadout of CMS, providing a platform for thorough testing of new trigger\nalgorithms on live data, but without interrupting data taking. We describe the\nmethodology to achieve ultra low latency anomaly detection, and present the\nintegration of the DNN into the GT test crate, as well as the monitoring,\ntesting, and validation of the algorithm during proton collisions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present the preparation, deployment, and testing of an autoencoder trained\nfor unbiased detection of new physics signatures in the CMS experiment Global\nTrigger (GT) test crate FPGAs during LHC Run 3. The GT makes the final decision\nwhether to readout or discard the data from each LHC collision, which occur at\na rate of 40 MHz, within a 50 ns latency. The Neural Network makes a prediction\nfor each event within these constraints, which can be used to select anomalous\nevents for further analysis. The GT test crate is a copy of the main GT system,\nreceiving the same input data, but whose output is not used to trigger the\nreadout of CMS, providing a platform for thorough testing of new trigger\nalgorithms on live data, but without interrupting data taking. We describe the\nmethodology to achieve ultra low latency anomaly detection, and present the\nintegration of the DNN into the GT test crate, as well as the monitoring,\ntesting, and validation of the algorithm during proton collisions."
                },
                "authors": [
                    {
                        "name": "Abhijith Gandrakota"
                    }
                ],
                "author_detail": {
                    "name": "Abhijith Gandrakota"
                },
                "arxiv_affiliation": "on behalf of CMS collaboration",
                "author": "Abhijith Gandrakota",
                "arxiv_comment": "Contribution to 42nd International Conference on High Energy Physics\n  (ICHEP 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19506v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19506v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-ex",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.data-an",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19504v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19504v1",
                "updated": "2024-11-29T06:48:13Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    6,
                    48,
                    13,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T06:48:13Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    6,
                    48,
                    13,
                    4,
                    334,
                    0
                ],
                "title": "TQA-Bench: Evaluating LLMs for Multi-Table Question Answering with\n  Scalable Context and Symbolic Extension",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TQA-Bench: Evaluating LLMs for Multi-Table Question Answering with\n  Scalable Context and Symbolic Extension"
                },
                "summary": "The advent of large language models (LLMs) has unlocked great opportunities\nin complex data management tasks, particularly in question answering (QA) over\ncomplicated multi-table relational data. Despite significant progress,\nsystematically evaluating LLMs on multi-table QA remains a critical challenge\ndue to the inherent complexity of analyzing heterogeneous table structures and\npotential large scale of serialized relational data. Existing benchmarks\nprimarily focus on single-table QA, failing to capture the intricacies of\nreasoning across multiple relational tables, as required in real-world domains\nsuch as finance, healthcare, and e-commerce. To address this gap, we present\nTQA-Bench, a new multi-table QA benchmark designed to evaluate the capabilities\nof LLMs in tackling complex QA tasks over relational data. Our benchmark\nincorporates diverse relational database instances sourced from real-world\npublic datasets and introduces a flexible sampling mechanism to create tasks\nwith varying multi-table context lengths, ranging from 8K to 64K tokens. To\nensure robustness and reliability, we integrate symbolic extensions into the\nevaluation framework, enabling the assessment of LLM reasoning capabilities\nbeyond simple data retrieval or probabilistic pattern matching. We\nsystematically evaluate a range of LLMs, both open-source and closed-source,\nspanning model scales from 7 billion to 70 billion parameters. Our extensive\nexperiments reveal critical insights into the performance of LLMs in\nmulti-table QA, highlighting both challenges and opportunities for advancing\ntheir application in complex, data-driven environments. Our benchmark\nimplementation and results are available at\nhttps://github.com/Relaxed-System-Lab/TQA-Bench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advent of large language models (LLMs) has unlocked great opportunities\nin complex data management tasks, particularly in question answering (QA) over\ncomplicated multi-table relational data. Despite significant progress,\nsystematically evaluating LLMs on multi-table QA remains a critical challenge\ndue to the inherent complexity of analyzing heterogeneous table structures and\npotential large scale of serialized relational data. Existing benchmarks\nprimarily focus on single-table QA, failing to capture the intricacies of\nreasoning across multiple relational tables, as required in real-world domains\nsuch as finance, healthcare, and e-commerce. To address this gap, we present\nTQA-Bench, a new multi-table QA benchmark designed to evaluate the capabilities\nof LLMs in tackling complex QA tasks over relational data. Our benchmark\nincorporates diverse relational database instances sourced from real-world\npublic datasets and introduces a flexible sampling mechanism to create tasks\nwith varying multi-table context lengths, ranging from 8K to 64K tokens. To\nensure robustness and reliability, we integrate symbolic extensions into the\nevaluation framework, enabling the assessment of LLM reasoning capabilities\nbeyond simple data retrieval or probabilistic pattern matching. We\nsystematically evaluate a range of LLMs, both open-source and closed-source,\nspanning model scales from 7 billion to 70 billion parameters. Our extensive\nexperiments reveal critical insights into the performance of LLMs in\nmulti-table QA, highlighting both challenges and opportunities for advancing\ntheir application in complex, data-driven environments. Our benchmark\nimplementation and results are available at\nhttps://github.com/Relaxed-System-Lab/TQA-Bench."
                },
                "authors": [
                    {
                        "name": "Zipeng Qiu"
                    },
                    {
                        "name": "You Peng"
                    },
                    {
                        "name": "Guangxin He"
                    },
                    {
                        "name": "Binhang Yuan"
                    },
                    {
                        "name": "Chen Wang"
                    }
                ],
                "author_detail": {
                    "name": "Chen Wang"
                },
                "author": "Chen Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19504v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19504v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19500v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19500v1",
                "updated": "2024-11-29T06:37:13Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    6,
                    37,
                    13,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T06:37:13Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    6,
                    37,
                    13,
                    4,
                    334,
                    0
                ],
                "title": "COLD: Causal reasOning in cLosed Daily activities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "COLD: Causal reasOning in cLosed Daily activities"
                },
                "summary": "Large Language Models (LLMs) have shown state-of-the-art performance in a\nvariety of tasks, including arithmetic and reasoning; however, to gauge the\nintellectual capabilities of LLMs, causal reasoning has become a reliable proxy\nfor validating a general understanding of the mechanics and intricacies of the\nworld similar to humans. Previous works in natural language processing (NLP)\nhave either focused on open-ended causal reasoning via causal commonsense\nreasoning (CCR) or framed a symbolic representation-based question answering\nfor theoretically backed-up analysis via a causal inference engine. The former\nadds an advantage of real-world grounding but lacks theoretically backed-up\nanalysis/validation, whereas the latter is far from real-world grounding. In\nthis work, we bridge this gap by proposing the COLD (Causal reasOning in cLosed\nDaily activities) framework, which is built upon human understanding of daily\nreal-world activities to reason about the causal nature of events. We show that\nthe proposed framework facilitates the creation of enormous causal queries (~ 9\nmillion) and comes close to the mini-turing test, simulating causal reasoning\nto evaluate the understanding of a daily real-world task. We evaluate multiple\nLLMs on the created causal queries and find that causal reasoning is\nchallenging even for activities trivial to humans. We further explore (the\ncausal reasoning abilities of LLMs) using the backdoor criterion to determine\nthe causal strength between events.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown state-of-the-art performance in a\nvariety of tasks, including arithmetic and reasoning; however, to gauge the\nintellectual capabilities of LLMs, causal reasoning has become a reliable proxy\nfor validating a general understanding of the mechanics and intricacies of the\nworld similar to humans. Previous works in natural language processing (NLP)\nhave either focused on open-ended causal reasoning via causal commonsense\nreasoning (CCR) or framed a symbolic representation-based question answering\nfor theoretically backed-up analysis via a causal inference engine. The former\nadds an advantage of real-world grounding but lacks theoretically backed-up\nanalysis/validation, whereas the latter is far from real-world grounding. In\nthis work, we bridge this gap by proposing the COLD (Causal reasOning in cLosed\nDaily activities) framework, which is built upon human understanding of daily\nreal-world activities to reason about the causal nature of events. We show that\nthe proposed framework facilitates the creation of enormous causal queries (~ 9\nmillion) and comes close to the mini-turing test, simulating causal reasoning\nto evaluate the understanding of a daily real-world task. We evaluate multiple\nLLMs on the created causal queries and find that causal reasoning is\nchallenging even for activities trivial to humans. We further explore (the\ncausal reasoning abilities of LLMs) using the backdoor criterion to determine\nthe causal strength between events."
                },
                "authors": [
                    {
                        "name": "Abhinav Joshi"
                    },
                    {
                        "name": "Areeb Ahmad"
                    },
                    {
                        "name": "Ashutosh Modi"
                    }
                ],
                "author_detail": {
                    "name": "Ashutosh Modi"
                },
                "author": "Ashutosh Modi",
                "arxiv_comment": "Paper accepted at NeurIPS 2024; Total 37 Pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19500v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19500v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19488v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19488v1",
                "updated": "2024-11-29T06:06:35Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    6,
                    6,
                    35,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T06:06:35Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    6,
                    6,
                    35,
                    4,
                    334,
                    0
                ],
                "title": "Interleaved-Modal Chain-of-Thought",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interleaved-Modal Chain-of-Thought"
                },
                "summary": "Chain-of-Thought (CoT) prompting elicits large language models (LLMs) to\nproduce a series of intermediate reasoning steps before arriving at the final\nanswer. However, when transitioning to vision-language models (VLMs), their\ntext-only rationales struggle to express the fine-grained associations with the\noriginal image. In this paper, we propose an image-incorporated multimodal\nChain-of-Thought, named \\textbf{Interleaved-modal Chain-of-Thought (ICoT)},\nwhich generates sequential reasoning steps consisting of paired visual and\ntextual rationales to infer the final answer. Intuitively, the novel ICoT\nrequires VLMs to enable the generation of fine-grained interleaved-modal\ncontent, which is hard for current VLMs to fulfill. Considering that the\nrequired visual information is usually part of the input image, we propose\n\\textbf{Attention-driven Selection (ADS)} to realize ICoT over existing VLMs.\nADS intelligently inserts regions of the input image to generate the\ninterleaved-modal reasoning steps with ignorable additional latency. ADS relies\nsolely on the attention map of VLMs without the need for parameterization, and\ntherefore it is a plug-and-play strategy that can be generalized to a spectrum\nof VLMs. We apply ADS to realize ICoT on two popular VLMs of different\narchitectures. Extensive evaluations of three benchmarks have shown that ICoT\nprompting achieves substantial performance (up to 14\\%) and interpretability\nimprovements compared to existing multimodal CoT prompting methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-Thought (CoT) prompting elicits large language models (LLMs) to\nproduce a series of intermediate reasoning steps before arriving at the final\nanswer. However, when transitioning to vision-language models (VLMs), their\ntext-only rationales struggle to express the fine-grained associations with the\noriginal image. In this paper, we propose an image-incorporated multimodal\nChain-of-Thought, named \\textbf{Interleaved-modal Chain-of-Thought (ICoT)},\nwhich generates sequential reasoning steps consisting of paired visual and\ntextual rationales to infer the final answer. Intuitively, the novel ICoT\nrequires VLMs to enable the generation of fine-grained interleaved-modal\ncontent, which is hard for current VLMs to fulfill. Considering that the\nrequired visual information is usually part of the input image, we propose\n\\textbf{Attention-driven Selection (ADS)} to realize ICoT over existing VLMs.\nADS intelligently inserts regions of the input image to generate the\ninterleaved-modal reasoning steps with ignorable additional latency. ADS relies\nsolely on the attention map of VLMs without the need for parameterization, and\ntherefore it is a plug-and-play strategy that can be generalized to a spectrum\nof VLMs. We apply ADS to realize ICoT on two popular VLMs of different\narchitectures. Extensive evaluations of three benchmarks have shown that ICoT\nprompting achieves substantial performance (up to 14\\%) and interpretability\nimprovements compared to existing multimodal CoT prompting methods."
                },
                "authors": [
                    {
                        "name": "Jun Gao"
                    },
                    {
                        "name": "Yongqi Li"
                    },
                    {
                        "name": "Ziqiang Cao"
                    },
                    {
                        "name": "Wenjie Li"
                    }
                ],
                "author_detail": {
                    "name": "Wenjie Li"
                },
                "author": "Wenjie Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19488v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19488v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19485v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19485v1",
                "updated": "2024-11-29T05:54:41Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    5,
                    54,
                    41,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T05:54:41Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    5,
                    54,
                    41,
                    4,
                    334,
                    0
                ],
                "title": "Action Engine: An LLM-based Framework for Automatic FaaS Workflow\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Action Engine: An LLM-based Framework for Automatic FaaS Workflow\n  Generation"
                },
                "summary": "Function as a Service (FaaS) is poised to become the foundation of the next\ngeneration of cloud systems due to its inherent advantages in scalability,\ncost-efficiency, and ease of use. However, challenges such as the need for\nspecialized knowledge and difficulties in building function workflows persist\nfor cloud-native application developers. To overcome these challenges and\nmitigate the burden of developing FaaS-based applications, in this paper, we\npropose a mechanism called Action Engine, that makes use of Tool-Augmented\nLarge Language Models (LLMs) at its kernel to interpret human language queries\nand automates FaaS workflow generation, thereby, reducing the need for\nspecialized expertise and manual design. Action Engine includes modules to\nidentify relevant functions from the FaaS repository and seamlessly manage the\ndata dependency between them, ensuring that the developer's query is processed\nand resolved. Beyond that, Action Engine can execute the generated workflow by\nfeeding the user-provided parameters. Our evaluations show that Action Engine\ncan generate workflows with up to 20\\% higher correctness without developer\ninvolvement. We notice that Action Engine can unlock FaaS workflow generation\nfor non-cloud-savvy developers and expedite the development cycles of\ncloud-native applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Function as a Service (FaaS) is poised to become the foundation of the next\ngeneration of cloud systems due to its inherent advantages in scalability,\ncost-efficiency, and ease of use. However, challenges such as the need for\nspecialized knowledge and difficulties in building function workflows persist\nfor cloud-native application developers. To overcome these challenges and\nmitigate the burden of developing FaaS-based applications, in this paper, we\npropose a mechanism called Action Engine, that makes use of Tool-Augmented\nLarge Language Models (LLMs) at its kernel to interpret human language queries\nand automates FaaS workflow generation, thereby, reducing the need for\nspecialized expertise and manual design. Action Engine includes modules to\nidentify relevant functions from the FaaS repository and seamlessly manage the\ndata dependency between them, ensuring that the developer's query is processed\nand resolved. Beyond that, Action Engine can execute the generated workflow by\nfeeding the user-provided parameters. Our evaluations show that Action Engine\ncan generate workflows with up to 20\\% higher correctness without developer\ninvolvement. We notice that Action Engine can unlock FaaS workflow generation\nfor non-cloud-savvy developers and expedite the development cycles of\ncloud-native applications."
                },
                "authors": [
                    {
                        "name": "Akiharu Esashi"
                    },
                    {
                        "name": "Pawissanutt Lertpongrujikorn"
                    },
                    {
                        "name": "Mohsen Amini Salehi"
                    }
                ],
                "author_detail": {
                    "name": "Mohsen Amini Salehi"
                },
                "author": "Mohsen Amini Salehi",
                "arxiv_comment": "Accepted at Utility Cloud Computing (UCC '24) conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19485v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19485v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.00958v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.00958v4",
                "updated": "2024-11-29T05:50:09Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    5,
                    50,
                    9,
                    4,
                    334,
                    0
                ],
                "published": "2024-07-01T04:29:35Z",
                "published_parsed": [
                    2024,
                    7,
                    1,
                    4,
                    29,
                    35,
                    0,
                    183,
                    0
                ],
                "title": "Dynamic Universal Approximation Theory: The Basic Theory for\n  Transformer-based Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Universal Approximation Theory: The Basic Theory for\n  Transformer-based Large Language Models"
                },
                "summary": "Language models have emerged as a critical area of focus in artificial\nintelligence, particularly with the introduction of groundbreaking innovations\nlike ChatGPT. Large-scale Transformer networks have quickly become the leading\napproach for advancing natural language processing algorithms. Built on the\nTransformer architecture, these models enable interactions that closely mimic\nhuman communication and, equipped with extensive knowledge, can even assist in\nguiding human tasks. Despite their impressive capabilities and growing\ncomplexity, a key question remains-the theoretical foundations of large\nlanguage models (LLMs). What makes Transformer so effective for powering\nintelligent language applications, such as translation and coding? What\nunderlies LLMs' ability for In-Context Learning (ICL)? How does the LoRA scheme\nenhance the fine-tuning of LLMs? And what supports the practicality of pruning\nLLMs? To address these critical questions and explore the technological\nstrategies within LLMs, we leverage the Universal Approximation Theory (UAT) to\noffer a theoretical backdrop, shedding light on the mechanisms that underpin\nthese advancements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language models have emerged as a critical area of focus in artificial\nintelligence, particularly with the introduction of groundbreaking innovations\nlike ChatGPT. Large-scale Transformer networks have quickly become the leading\napproach for advancing natural language processing algorithms. Built on the\nTransformer architecture, these models enable interactions that closely mimic\nhuman communication and, equipped with extensive knowledge, can even assist in\nguiding human tasks. Despite their impressive capabilities and growing\ncomplexity, a key question remains-the theoretical foundations of large\nlanguage models (LLMs). What makes Transformer so effective for powering\nintelligent language applications, such as translation and coding? What\nunderlies LLMs' ability for In-Context Learning (ICL)? How does the LoRA scheme\nenhance the fine-tuning of LLMs? And what supports the practicality of pruning\nLLMs? To address these critical questions and explore the technological\nstrategies within LLMs, we leverage the Universal Approximation Theory (UAT) to\noffer a theoretical backdrop, shedding light on the mechanisms that underpin\nthese advancements."
                },
                "authors": [
                    {
                        "name": "Wei Wang"
                    },
                    {
                        "name": "Qing Li"
                    }
                ],
                "author_detail": {
                    "name": "Qing Li"
                },
                "author": "Qing Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.00958v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.00958v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19478v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19478v1",
                "updated": "2024-11-29T05:31:04Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    5,
                    31,
                    4,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T05:31:04Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    5,
                    31,
                    4,
                    4,
                    334,
                    0
                ],
                "title": "Zero-Indexing Internet Search Augmented Generation for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-Indexing Internet Search Augmented Generation for Large Language\n  Models"
                },
                "summary": "Retrieval augmented generation has emerged as an effective method to enhance\nlarge language model performance. This approach typically relies on an internal\nretrieval module that uses various indexing mechanisms to manage a static\npre-processed corpus. However, such a paradigm often falls short when it is\nnecessary to integrate the most up-to-date information that has not been\nupdated into the corpus during generative inference time. In this paper, we\nexplore an alternative approach that leverages standard search engine APIs to\ndynamically integrate the latest online information (without maintaining any\nindex for any fixed corpus), thereby improving the quality of generated\ncontent. We design a collaborative LLM-based paradigm, where we include: (i) a\nparser-LLM that determines if the Internet augmented generation is demanded and\nextracts the search keywords if so with a single inference; (ii) a mixed\nranking strategy that re-ranks the retrieved HTML files to eliminate bias\nintroduced from the search engine API; and (iii) an extractor-LLM that can\naccurately and efficiently extract relevant information from the fresh content\nin each HTML file. We conduct extensive empirical studies to evaluate the\nperformance of this Internet search augmented generation paradigm. The\nexperimental results demonstrate that our method generates content with\nsignificantly improved quality. Our system has been successfully deployed in a\nproduction environment to serve 01.AI's generative inference requests.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval augmented generation has emerged as an effective method to enhance\nlarge language model performance. This approach typically relies on an internal\nretrieval module that uses various indexing mechanisms to manage a static\npre-processed corpus. However, such a paradigm often falls short when it is\nnecessary to integrate the most up-to-date information that has not been\nupdated into the corpus during generative inference time. In this paper, we\nexplore an alternative approach that leverages standard search engine APIs to\ndynamically integrate the latest online information (without maintaining any\nindex for any fixed corpus), thereby improving the quality of generated\ncontent. We design a collaborative LLM-based paradigm, where we include: (i) a\nparser-LLM that determines if the Internet augmented generation is demanded and\nextracts the search keywords if so with a single inference; (ii) a mixed\nranking strategy that re-ranks the retrieved HTML files to eliminate bias\nintroduced from the search engine API; and (iii) an extractor-LLM that can\naccurately and efficiently extract relevant information from the fresh content\nin each HTML file. We conduct extensive empirical studies to evaluate the\nperformance of this Internet search augmented generation paradigm. The\nexperimental results demonstrate that our method generates content with\nsignificantly improved quality. Our system has been successfully deployed in a\nproduction environment to serve 01.AI's generative inference requests."
                },
                "authors": [
                    {
                        "name": "Guangxin He"
                    },
                    {
                        "name": "Zonghong Dai"
                    },
                    {
                        "name": "Jiangcheng Zhu"
                    },
                    {
                        "name": "Binqiang Zhao"
                    },
                    {
                        "name": "Chenyue Li"
                    },
                    {
                        "name": "You Peng"
                    },
                    {
                        "name": "Chen Wang"
                    },
                    {
                        "name": "Binhang Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Binhang Yuan"
                },
                "author": "Binhang Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19478v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19478v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19477v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19477v1",
                "updated": "2024-11-29T05:29:47Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    5,
                    29,
                    47,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T05:29:47Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    5,
                    29,
                    47,
                    4,
                    334,
                    0
                ],
                "title": "A Simple and Provable Scaling Law for the Test-Time Compute of Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Simple and Provable Scaling Law for the Test-Time Compute of Large\n  Language Models"
                },
                "summary": "We propose a general two-stage algorithm that enjoys a provable scaling law\nfor the test-time compute of large language models (LLMs). Given an input\nproblem, the proposed algorithm first generates $N$ candidate solutions, and\nthen chooses the best one via a multiple-round knockout tournament where each\npair of candidates are compared for $K$ times and only the winners move on to\nthe next round. In a minimalistic implementation, both stages can be executed\nwith a black-box LLM alone and nothing else (e.g., no external verifier or\nreward model), and a total of $N \\times (K + 1)$ highly parallelizable LLM\ncalls are needed for solving an input problem. Assuming that a generated\ncandidate solution is correct with probability $p_{\\text{gen}} > 0$ and a\ncomparison between a pair of correct and incorrect solutions identifies the\nright winner with probability $p_{\\text{comp}} > 0.5$ (i.e., better than a\nrandom guess), we prove theoretically that the failure probability of the\nproposed algorithm decays to zero exponentially with respect to $N$ and $K$:\n$$\\mathbb{P}(\\text{final output is incorrect}) \\le (1 - p_{\\text{gen}})^N +\n\\lceil \\log_2 N \\rceil e^{-2 K (p_{\\text{comp}} - 0.5)^2}.$$ Our empirical\nresults with the challenging MMLU-Pro benchmark validate the technical\nassumptions, as well as the efficacy of the proposed algorithm and the gains\nfrom scaling up its test-time compute.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a general two-stage algorithm that enjoys a provable scaling law\nfor the test-time compute of large language models (LLMs). Given an input\nproblem, the proposed algorithm first generates $N$ candidate solutions, and\nthen chooses the best one via a multiple-round knockout tournament where each\npair of candidates are compared for $K$ times and only the winners move on to\nthe next round. In a minimalistic implementation, both stages can be executed\nwith a black-box LLM alone and nothing else (e.g., no external verifier or\nreward model), and a total of $N \\times (K + 1)$ highly parallelizable LLM\ncalls are needed for solving an input problem. Assuming that a generated\ncandidate solution is correct with probability $p_{\\text{gen}} > 0$ and a\ncomparison between a pair of correct and incorrect solutions identifies the\nright winner with probability $p_{\\text{comp}} > 0.5$ (i.e., better than a\nrandom guess), we prove theoretically that the failure probability of the\nproposed algorithm decays to zero exponentially with respect to $N$ and $K$:\n$$\\mathbb{P}(\\text{final output is incorrect}) \\le (1 - p_{\\text{gen}})^N +\n\\lceil \\log_2 N \\rceil e^{-2 K (p_{\\text{comp}} - 0.5)^2}.$$ Our empirical\nresults with the challenging MMLU-Pro benchmark validate the technical\nassumptions, as well as the efficacy of the proposed algorithm and the gains\nfrom scaling up its test-time compute."
                },
                "authors": [
                    {
                        "name": "Yanxi Chen"
                    },
                    {
                        "name": "Xuchen Pan"
                    },
                    {
                        "name": "Yaliang Li"
                    },
                    {
                        "name": "Bolin Ding"
                    },
                    {
                        "name": "Jingren Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jingren Zhou"
                },
                "author": "Jingren Zhou",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19477v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19477v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.00627v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.00627v3",
                "updated": "2024-11-29T05:05:13Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    5,
                    5,
                    13,
                    4,
                    334,
                    0
                ],
                "published": "2024-06-02T06:09:56Z",
                "published_parsed": [
                    2024,
                    6,
                    2,
                    6,
                    9,
                    56,
                    6,
                    154,
                    0
                ],
                "title": "Prompt Framework for Role-playing: Generation and Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt Framework for Role-playing: Generation and Evaluation"
                },
                "summary": "Large language models (LLMs) exhibit impressive proficiency in natural\nlanguage generation, understanding user instructions, and emulating human-like\nlanguage use, which has led to significant interest in their application to\nrole-playing scenarios. However, the manual collection of role-specific script\ndata and the evaluation of model performance are resource-intensive processes.\nThis project introduces a prompt-based framework designed to leverage GPT's\ncapabilities for the generation of role-playing dialogue datasets and the\nevaluation of role-playing performance. To validate the effectiveness of the\nGPT-based generation and evaluation, we further incorporate the recall-oriented\nRouge-L metric, providing an additional quantitative measure of performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) exhibit impressive proficiency in natural\nlanguage generation, understanding user instructions, and emulating human-like\nlanguage use, which has led to significant interest in their application to\nrole-playing scenarios. However, the manual collection of role-specific script\ndata and the evaluation of model performance are resource-intensive processes.\nThis project introduces a prompt-based framework designed to leverage GPT's\ncapabilities for the generation of role-playing dialogue datasets and the\nevaluation of role-playing performance. To validate the effectiveness of the\nGPT-based generation and evaluation, we further incorporate the recall-oriented\nRouge-L metric, providing an additional quantitative measure of performance."
                },
                "authors": [
                    {
                        "name": "Xun Liu"
                    },
                    {
                        "name": "Zhengwei Ni"
                    }
                ],
                "author_detail": {
                    "name": "Zhengwei Ni"
                },
                "author": "Zhengwei Ni",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.00627v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.00627v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.06350v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.06350v2",
                "updated": "2024-11-29T04:50:35Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    4,
                    50,
                    35,
                    4,
                    334,
                    0
                ],
                "published": "2024-03-11T00:46:56Z",
                "published_parsed": [
                    2024,
                    3,
                    11,
                    0,
                    46,
                    56,
                    0,
                    71,
                    0
                ],
                "title": "IndicLLMSuite: A Blueprint for Creating Pre-training and Fine-Tuning\n  Datasets for Indian Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IndicLLMSuite: A Blueprint for Creating Pre-training and Fine-Tuning\n  Datasets for Indian Languages"
                },
                "summary": "Despite the considerable advancements in English LLMs, the progress in\nbuilding comparable models for other languages has been hindered due to the\nscarcity of tailored resources. Our work aims to bridge this divide by\nintroducing an expansive suite of resources specifically designed for the\ndevelopment of Indic LLMs, covering 22 languages, containing a total of 251B\ntokens and 74.8M instruction-response pairs. Recognizing the importance of both\ndata quality and quantity, our approach combines highly curated manually\nverified data, unverified yet valuable data, and synthetic data. We build a\nclean, open-source pipeline for curating pre-training data from diverse\nsources, including websites, PDFs, and videos, incorporating best practices for\ncrawling, cleaning, flagging, and deduplication. For instruction-fine tuning,\nwe amalgamate existing Indic datasets, translate/transliterate English datasets\ninto Indian languages, and utilize LLaMa2 and Mixtral models to create\nconversations grounded in articles from Indian Wikipedia and Wikihow.\nAdditionally, we address toxicity alignment by generating toxic prompts for\nmultiple scenarios and then generate non-toxic responses by feeding these toxic\nprompts to an aligned LLaMa2 model. We hope that the datasets, tools, and\nresources released as a part of this work will not only propel the research and\ndevelopment of Indic LLMs but also establish an open-source blueprint for\nextending such efforts to other languages. The data and other artifacts created\nas part of this work are released with permissive licenses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the considerable advancements in English LLMs, the progress in\nbuilding comparable models for other languages has been hindered due to the\nscarcity of tailored resources. Our work aims to bridge this divide by\nintroducing an expansive suite of resources specifically designed for the\ndevelopment of Indic LLMs, covering 22 languages, containing a total of 251B\ntokens and 74.8M instruction-response pairs. Recognizing the importance of both\ndata quality and quantity, our approach combines highly curated manually\nverified data, unverified yet valuable data, and synthetic data. We build a\nclean, open-source pipeline for curating pre-training data from diverse\nsources, including websites, PDFs, and videos, incorporating best practices for\ncrawling, cleaning, flagging, and deduplication. For instruction-fine tuning,\nwe amalgamate existing Indic datasets, translate/transliterate English datasets\ninto Indian languages, and utilize LLaMa2 and Mixtral models to create\nconversations grounded in articles from Indian Wikipedia and Wikihow.\nAdditionally, we address toxicity alignment by generating toxic prompts for\nmultiple scenarios and then generate non-toxic responses by feeding these toxic\nprompts to an aligned LLaMa2 model. We hope that the datasets, tools, and\nresources released as a part of this work will not only propel the research and\ndevelopment of Indic LLMs but also establish an open-source blueprint for\nextending such efforts to other languages. The data and other artifacts created\nas part of this work are released with permissive licenses."
                },
                "authors": [
                    {
                        "name": "Mohammed Safi Ur Rahman Khan"
                    },
                    {
                        "name": "Priyam Mehta"
                    },
                    {
                        "name": "Ananth Sankar"
                    },
                    {
                        "name": "Umashankar Kumaravelan"
                    },
                    {
                        "name": "Sumanth Doddapaneni"
                    },
                    {
                        "name": "Suriyaprasaad B"
                    },
                    {
                        "name": "Varun Balan G"
                    },
                    {
                        "name": "Sparsh Jain"
                    },
                    {
                        "name": "Anoop Kunchukuttan"
                    },
                    {
                        "name": "Pratyush Kumar"
                    },
                    {
                        "name": "Raj Dabre"
                    },
                    {
                        "name": "Mitesh M. Khapra"
                    }
                ],
                "author_detail": {
                    "name": "Mitesh M. Khapra"
                },
                "author": "Mitesh M. Khapra",
                "arxiv_doi": "10.18653/v1/2024.acl-long.843",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.18653/v1/2024.acl-long.843",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2403.06350v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.06350v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "ACL-2024 Outstanding Paper",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19463v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19463v1",
                "updated": "2024-11-29T04:25:31Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    4,
                    25,
                    31,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T04:25:31Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    4,
                    25,
                    31,
                    4,
                    334,
                    0
                ],
                "title": "Towards Understanding Retrieval Accuracy and Prompt Quality in RAG\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Understanding Retrieval Accuracy and Prompt Quality in RAG\n  Systems"
                },
                "summary": "Retrieval-Augmented Generation (RAG) is a pivotal technique for enhancing the\ncapability of large language models (LLMs) and has demonstrated promising\nefficacy across a diverse spectrum of tasks. While LLM-driven RAG systems show\nsuperior performance, they face unique challenges in stability and reliability.\nTheir complexity hinders developers' efforts to design, maintain, and optimize\neffective RAG systems. Therefore, it is crucial to understand how RAG's\nperformance is impacted by its design. In this work, we conduct an early\nexploratory study toward a better understanding of the mechanism of RAG\nsystems, covering three code datasets, three QA datasets, and two LLMs. We\nfocus on four design factors: retrieval document type, retrieval recall,\ndocument selection, and prompt techniques. Our study uncovers how each factor\nimpacts system correctness and confidence, providing valuable insights for\ndeveloping an accurate and reliable RAG system. Based on these findings, we\npresent nine actionable guidelines for detecting defects and optimizing the\nperformance of RAG systems. We hope our early exploration can inspire further\nadvancements in engineering, improving and maintaining LLM-driven intelligent\nsoftware systems for greater efficiency and reliability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) is a pivotal technique for enhancing the\ncapability of large language models (LLMs) and has demonstrated promising\nefficacy across a diverse spectrum of tasks. While LLM-driven RAG systems show\nsuperior performance, they face unique challenges in stability and reliability.\nTheir complexity hinders developers' efforts to design, maintain, and optimize\neffective RAG systems. Therefore, it is crucial to understand how RAG's\nperformance is impacted by its design. In this work, we conduct an early\nexploratory study toward a better understanding of the mechanism of RAG\nsystems, covering three code datasets, three QA datasets, and two LLMs. We\nfocus on four design factors: retrieval document type, retrieval recall,\ndocument selection, and prompt techniques. Our study uncovers how each factor\nimpacts system correctness and confidence, providing valuable insights for\ndeveloping an accurate and reliable RAG system. Based on these findings, we\npresent nine actionable guidelines for detecting defects and optimizing the\nperformance of RAG systems. We hope our early exploration can inspire further\nadvancements in engineering, improving and maintaining LLM-driven intelligent\nsoftware systems for greater efficiency and reliability."
                },
                "authors": [
                    {
                        "name": "Shengming Zhao"
                    },
                    {
                        "name": "Yuheng Huang"
                    },
                    {
                        "name": "Jiayang Song"
                    },
                    {
                        "name": "Zhijie Wang"
                    },
                    {
                        "name": "Chengcheng Wan"
                    },
                    {
                        "name": "Lei Ma"
                    }
                ],
                "author_detail": {
                    "name": "Lei Ma"
                },
                "author": "Lei Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19463v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19463v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19459v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19459v1",
                "updated": "2024-11-29T04:09:13Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    4,
                    9,
                    13,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T04:09:13Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    4,
                    9,
                    13,
                    4,
                    334,
                    0
                ],
                "title": "Fleximo: Towards Flexible Text-to-Human Motion Video Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fleximo: Towards Flexible Text-to-Human Motion Video Generation"
                },
                "summary": "Current methods for generating human motion videos rely on extracting pose\nsequences from reference videos, which restricts flexibility and control.\nAdditionally, due to the limitations of pose detection techniques, the\nextracted pose sequences can sometimes be inaccurate, leading to low-quality\nvideo outputs. We introduce a novel task aimed at generating human motion\nvideos solely from reference images and natural language. This approach offers\ngreater flexibility and ease of use, as text is more accessible than the\ndesired guidance videos. However, training an end-to-end model for this task\nrequires millions of high-quality text and human motion video pairs, which are\nchallenging to obtain. To address this, we propose a new framework called\nFleximo, which leverages large-scale pre-trained text-to-3D motion models. This\napproach is not straightforward, as the text-generated skeletons may not\nconsistently match the scale of the reference image and may lack detailed\ninformation. To overcome these challenges, we introduce an anchor point based\nrescale method and design a skeleton adapter to fill in missing details and\nbridge the gap between text-to-motion and motion-to-video generation. We also\npropose a video refinement process to further enhance video quality. A large\nlanguage model (LLM) is employed to decompose natural language into discrete\nmotion sequences, enabling the generation of motion videos of any desired\nlength. To assess the performance of Fleximo, we introduce a new benchmark\ncalled MotionBench, which includes 400 videos across 20 identities and 20\nmotions. We also propose a new metric, MotionScore, to evaluate the accuracy of\nmotion following. Both qualitative and quantitative results demonstrate that\nour method outperforms existing text-conditioned image-to-video generation\nmethods. All code and model weights will be made publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current methods for generating human motion videos rely on extracting pose\nsequences from reference videos, which restricts flexibility and control.\nAdditionally, due to the limitations of pose detection techniques, the\nextracted pose sequences can sometimes be inaccurate, leading to low-quality\nvideo outputs. We introduce a novel task aimed at generating human motion\nvideos solely from reference images and natural language. This approach offers\ngreater flexibility and ease of use, as text is more accessible than the\ndesired guidance videos. However, training an end-to-end model for this task\nrequires millions of high-quality text and human motion video pairs, which are\nchallenging to obtain. To address this, we propose a new framework called\nFleximo, which leverages large-scale pre-trained text-to-3D motion models. This\napproach is not straightforward, as the text-generated skeletons may not\nconsistently match the scale of the reference image and may lack detailed\ninformation. To overcome these challenges, we introduce an anchor point based\nrescale method and design a skeleton adapter to fill in missing details and\nbridge the gap between text-to-motion and motion-to-video generation. We also\npropose a video refinement process to further enhance video quality. A large\nlanguage model (LLM) is employed to decompose natural language into discrete\nmotion sequences, enabling the generation of motion videos of any desired\nlength. To assess the performance of Fleximo, we introduce a new benchmark\ncalled MotionBench, which includes 400 videos across 20 identities and 20\nmotions. We also propose a new metric, MotionScore, to evaluate the accuracy of\nmotion following. Both qualitative and quantitative results demonstrate that\nour method outperforms existing text-conditioned image-to-video generation\nmethods. All code and model weights will be made publicly available."
                },
                "authors": [
                    {
                        "name": "Yuhang Zhang"
                    },
                    {
                        "name": "Yuan Zhou"
                    },
                    {
                        "name": "Zeyu Liu"
                    },
                    {
                        "name": "Yuxuan Cai"
                    },
                    {
                        "name": "Qiuyue Wang"
                    },
                    {
                        "name": "Aidong Men"
                    },
                    {
                        "name": "Huan Yang"
                    }
                ],
                "author_detail": {
                    "name": "Huan Yang"
                },
                "author": "Huan Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19459v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19459v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19456v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19456v1",
                "updated": "2024-11-29T03:57:26Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    3,
                    57,
                    26,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T03:57:26Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    3,
                    57,
                    26,
                    4,
                    334,
                    0
                ],
                "title": "Beyond Surface Structure: A Causal Assessment of LLMs' Comprehension\n  Ability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Surface Structure: A Causal Assessment of LLMs' Comprehension\n  Ability"
                },
                "summary": "Large language models (LLMs) have shown remarkable capability in natural\nlanguage tasks, yet debate persists on whether they truly comprehend deep\nstructure (i.e., core semantics) or merely rely on surface structure (e.g.,\npresentation format). Prior studies observe that LLMs' performance declines\nwhen intervening on surface structure, arguing their success relies on surface\nstructure recognition. However, surface structure sensitivity does not prevent\ndeep structure comprehension. Rigorously evaluating LLMs' capability requires\nanalyzing both, yet deep structure is often overlooked. To this end, we assess\nLLMs' comprehension ability using causal mediation analysis, aiming to fully\ndiscover the capability of using both deep and surface structures.\nSpecifically, we formulate the comprehension of deep structure as direct causal\neffect (DCE) and that of surface structure as indirect causal effect (ICE),\nrespectively. To address the non-estimability of original DCE and ICE --\nstemming from the infeasibility of isolating mutual influences of deep and\nsurface structures, we develop the corresponding quantifiable surrogates,\nincluding approximated DCE (ADCE) and approximated ICE (AICE). We further apply\nthe ADCE to evaluate a series of mainstream LLMs, showing that most of them\nexhibit deep structure comprehension ability, which grows along with the\nprediction accuracy. Comparing ADCE and AICE demonstrates closed-source LLMs\nrely more on deep structure, while open-source LLMs are more surface-sensitive,\nwhich decreases with model scale. Theoretically, ADCE is a bidirectional\nevaluation, which measures both the sufficiency and necessity of deep structure\nchanges in causing output variations, thus offering a more comprehensive\nassessment than accuracy, a common evaluation in LLMs. Our work provides new\ninsights into LLMs' deep structure comprehension and offers novel methods for\nLLMs evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown remarkable capability in natural\nlanguage tasks, yet debate persists on whether they truly comprehend deep\nstructure (i.e., core semantics) or merely rely on surface structure (e.g.,\npresentation format). Prior studies observe that LLMs' performance declines\nwhen intervening on surface structure, arguing their success relies on surface\nstructure recognition. However, surface structure sensitivity does not prevent\ndeep structure comprehension. Rigorously evaluating LLMs' capability requires\nanalyzing both, yet deep structure is often overlooked. To this end, we assess\nLLMs' comprehension ability using causal mediation analysis, aiming to fully\ndiscover the capability of using both deep and surface structures.\nSpecifically, we formulate the comprehension of deep structure as direct causal\neffect (DCE) and that of surface structure as indirect causal effect (ICE),\nrespectively. To address the non-estimability of original DCE and ICE --\nstemming from the infeasibility of isolating mutual influences of deep and\nsurface structures, we develop the corresponding quantifiable surrogates,\nincluding approximated DCE (ADCE) and approximated ICE (AICE). We further apply\nthe ADCE to evaluate a series of mainstream LLMs, showing that most of them\nexhibit deep structure comprehension ability, which grows along with the\nprediction accuracy. Comparing ADCE and AICE demonstrates closed-source LLMs\nrely more on deep structure, while open-source LLMs are more surface-sensitive,\nwhich decreases with model scale. Theoretically, ADCE is a bidirectional\nevaluation, which measures both the sufficiency and necessity of deep structure\nchanges in causing output variations, thus offering a more comprehensive\nassessment than accuracy, a common evaluation in LLMs. Our work provides new\ninsights into LLMs' deep structure comprehension and offers novel methods for\nLLMs evaluation."
                },
                "authors": [
                    {
                        "name": "Yujin Han"
                    },
                    {
                        "name": "Lei Xu"
                    },
                    {
                        "name": "Sirui Chen"
                    },
                    {
                        "name": "Difan Zou"
                    },
                    {
                        "name": "Chaochao Lu"
                    }
                ],
                "author_detail": {
                    "name": "Chaochao Lu"
                },
                "author": "Chaochao Lu",
                "arxiv_comment": "28 pages, 14 figures, 10 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19456v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19456v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.00774v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.00774v4",
                "updated": "2024-11-29T03:49:55Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    3,
                    49,
                    55,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-01T17:59:51Z",
                "published_parsed": [
                    2024,
                    11,
                    1,
                    17,
                    59,
                    51,
                    4,
                    306,
                    0
                ],
                "title": "Freeze-Omni: A Smart and Low Latency Speech-to-speech Dialogue Model\n  with Frozen LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Freeze-Omni: A Smart and Low Latency Speech-to-speech Dialogue Model\n  with Frozen LLM"
                },
                "summary": "Rapidly developing large language models (LLMs) have brought tremendous\nintelligent applications. Especially, the GPT-4o's excellent duplex speech\ninteraction ability has brought impressive experience to users. Researchers\nhave recently proposed several multi-modal LLMs in this direction that can\nachieve user-agent speech-to-speech conversations. This paper proposes a novel\nspeech-text multimodal LLM architecture called Freeze-Omni. Our main\ncontribution is that the speech input and output modalities can be easily\nconnected to a textual LLM while keeping the LLM's parameters frozen throughout\nthe training process. We design a three-stage training strategy for modeling\nboth the speech input and output, enabling Freeze-Omni to obtain\nspeech-to-speech conversation ability using text-speech paired data (such as\nASR and TTS data) and only 60,000 multi-round text Q&A data on 8 GPUs.\nMoreover, we can effectively ensure that the intelligence of the Freeze-Omni in\nthe speech modality is at the same level compared with that in the text\nmodality of its backbone LLM, while achieving low latency end-to-end spoken\nresponse. In addition, we also designed a method to achieve duplex dialogue\nability through multi-task training, giving Freeze-Omni a more natural style of\ndialogue ability between users and agents. In summary, Freeze-Omni holds great\npotential to conduct speech-to-speech dialogue based on a multimodal LLM under\nthe condition of a frozen LLM, avoiding the catastrophic forgetting problem\ncaused by limited data and training resources.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rapidly developing large language models (LLMs) have brought tremendous\nintelligent applications. Especially, the GPT-4o's excellent duplex speech\ninteraction ability has brought impressive experience to users. Researchers\nhave recently proposed several multi-modal LLMs in this direction that can\nachieve user-agent speech-to-speech conversations. This paper proposes a novel\nspeech-text multimodal LLM architecture called Freeze-Omni. Our main\ncontribution is that the speech input and output modalities can be easily\nconnected to a textual LLM while keeping the LLM's parameters frozen throughout\nthe training process. We design a three-stage training strategy for modeling\nboth the speech input and output, enabling Freeze-Omni to obtain\nspeech-to-speech conversation ability using text-speech paired data (such as\nASR and TTS data) and only 60,000 multi-round text Q&A data on 8 GPUs.\nMoreover, we can effectively ensure that the intelligence of the Freeze-Omni in\nthe speech modality is at the same level compared with that in the text\nmodality of its backbone LLM, while achieving low latency end-to-end spoken\nresponse. In addition, we also designed a method to achieve duplex dialogue\nability through multi-task training, giving Freeze-Omni a more natural style of\ndialogue ability between users and agents. In summary, Freeze-Omni holds great\npotential to conduct speech-to-speech dialogue based on a multimodal LLM under\nthe condition of a frozen LLM, avoiding the catastrophic forgetting problem\ncaused by limited data and training resources."
                },
                "authors": [
                    {
                        "name": "Xiong Wang"
                    },
                    {
                        "name": "Yangze Li"
                    },
                    {
                        "name": "Chaoyou Fu"
                    },
                    {
                        "name": "Yunhang Shen"
                    },
                    {
                        "name": "Lei Xie"
                    },
                    {
                        "name": "Ke Li"
                    },
                    {
                        "name": "Xing Sun"
                    },
                    {
                        "name": "Long Ma"
                    }
                ],
                "author_detail": {
                    "name": "Long Ma"
                },
                "author": "Long Ma",
                "arxiv_comment": "Project Page: https://freeze-omni.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.00774v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.00774v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19443v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19443v1",
                "updated": "2024-11-29T03:01:05Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    3,
                    1,
                    5,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T03:01:05Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    3,
                    1,
                    5,
                    4,
                    334,
                    0
                ],
                "title": "Auto-RAG: Autonomous Retrieval-Augmented Generation for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auto-RAG: Autonomous Retrieval-Augmented Generation for Large Language\n  Models"
                },
                "summary": "Iterative retrieval refers to the process in which the model continuously\nqueries the retriever during generation to enhance the relevance of the\nretrieved knowledge, thereby improving the performance of Retrieval-Augmented\nGeneration (RAG). Existing work typically employs few-shot prompting or\nmanually constructed rules to implement iterative retrieval. This introduces\nadditional inference overhead and overlooks the remarkable reasoning\ncapabilities of Large Language Models (LLMs). In this paper, we introduce\nAuto-RAG, an autonomous iterative retrieval model centered on the LLM's\npowerful decision-making capabilities. Auto-RAG engages in multi-turn dialogues\nwith the retriever, systematically planning retrievals and refining queries to\nacquire valuable knowledge. This process continues until sufficient external\ninformation is gathered, at which point the results are presented to the user.\nTo this end, we develop a method for autonomously synthesizing reasoning-based\ndecision-making instructions in iterative retrieval and fine-tuned the latest\nopen-source LLMs. The experimental results indicate that Auto-RAG is capable of\nautonomous iterative interaction with the retriever, effectively leveraging the\nremarkable reasoning and decision-making abilities of LLMs, which lead to\noutstanding performance across six benchmarks. Further analysis reveals that\nAuto-RAG can autonomously adjust the number of iterations based on the\ndifficulty of the questions and the utility of the retrieved knowledge, without\nrequiring any human intervention. Moreover, Auto-RAG expresses the iterative\nretrieval process in natural language, enhancing interpretability while\nproviding users with a more intuitive experience\\footnote{Code is available at\n\\url{https://github.com/ictnlp/Auto-RAG}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Iterative retrieval refers to the process in which the model continuously\nqueries the retriever during generation to enhance the relevance of the\nretrieved knowledge, thereby improving the performance of Retrieval-Augmented\nGeneration (RAG). Existing work typically employs few-shot prompting or\nmanually constructed rules to implement iterative retrieval. This introduces\nadditional inference overhead and overlooks the remarkable reasoning\ncapabilities of Large Language Models (LLMs). In this paper, we introduce\nAuto-RAG, an autonomous iterative retrieval model centered on the LLM's\npowerful decision-making capabilities. Auto-RAG engages in multi-turn dialogues\nwith the retriever, systematically planning retrievals and refining queries to\nacquire valuable knowledge. This process continues until sufficient external\ninformation is gathered, at which point the results are presented to the user.\nTo this end, we develop a method for autonomously synthesizing reasoning-based\ndecision-making instructions in iterative retrieval and fine-tuned the latest\nopen-source LLMs. The experimental results indicate that Auto-RAG is capable of\nautonomous iterative interaction with the retriever, effectively leveraging the\nremarkable reasoning and decision-making abilities of LLMs, which lead to\noutstanding performance across six benchmarks. Further analysis reveals that\nAuto-RAG can autonomously adjust the number of iterations based on the\ndifficulty of the questions and the utility of the retrieved knowledge, without\nrequiring any human intervention. Moreover, Auto-RAG expresses the iterative\nretrieval process in natural language, enhancing interpretability while\nproviding users with a more intuitive experience\\footnote{Code is available at\n\\url{https://github.com/ictnlp/Auto-RAG}."
                },
                "authors": [
                    {
                        "name": "Tian Yu"
                    },
                    {
                        "name": "Shaolei Zhang"
                    },
                    {
                        "name": "Yang Feng"
                    }
                ],
                "author_detail": {
                    "name": "Yang Feng"
                },
                "author": "Yang Feng",
                "arxiv_comment": "Code is available at https://github.com/ictnlp/Auto-RAG",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19443v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19443v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.11217v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.11217v2",
                "updated": "2024-11-29T02:50:45Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    2,
                    50,
                    45,
                    4,
                    334,
                    0
                ],
                "published": "2024-02-17T08:04:23Z",
                "published_parsed": [
                    2024,
                    2,
                    17,
                    8,
                    4,
                    23,
                    5,
                    48,
                    0
                ],
                "title": "A Spectrum Evaluation Benchmark for Medical Multi-Modal Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Spectrum Evaluation Benchmark for Medical Multi-Modal Large Language\n  Models"
                },
                "summary": "The significant breakthroughs of Medical Multi-Modal Large Language Models\n(Med-MLLMs) renovate modern healthcare with robust information synthesis and\nmedical decision support. However, these models are often evaluated on\nbenchmarks that are unsuitable for the Med-MLLMs due to the complexity of\nreal-world diagnostics across diverse specialties. To address this gap, we\nintroduce Asclepius, a novel Med-MLLM benchmark that comprehensively assesses\nMed-MLLMs in terms of: distinct medical specialties (cardiovascular,\ngastroenterology, etc.) and different diagnostic capacities (perception,\ndisease analysis, etc.). Grounded in 3 proposed core principles, Asclepius\nensures a comprehensive evaluation by encompassing 15 medical specialties,\nstratifying into 3 main categories and 8 sub-categories of clinical tasks, and\nexempting overlap with existing VQA dataset. We further provide an in-depth\nanalysis of 6 Med-MLLMs and compare them with 3 human specialists, providing\ninsights into their competencies and limitations in various medical contexts.\nOur work not only advances the understanding of Med-MLLMs' capabilities but\nalso sets a precedent for future evaluations and the safe deployment of these\nmodels in clinical environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The significant breakthroughs of Medical Multi-Modal Large Language Models\n(Med-MLLMs) renovate modern healthcare with robust information synthesis and\nmedical decision support. However, these models are often evaluated on\nbenchmarks that are unsuitable for the Med-MLLMs due to the complexity of\nreal-world diagnostics across diverse specialties. To address this gap, we\nintroduce Asclepius, a novel Med-MLLM benchmark that comprehensively assesses\nMed-MLLMs in terms of: distinct medical specialties (cardiovascular,\ngastroenterology, etc.) and different diagnostic capacities (perception,\ndisease analysis, etc.). Grounded in 3 proposed core principles, Asclepius\nensures a comprehensive evaluation by encompassing 15 medical specialties,\nstratifying into 3 main categories and 8 sub-categories of clinical tasks, and\nexempting overlap with existing VQA dataset. We further provide an in-depth\nanalysis of 6 Med-MLLMs and compare them with 3 human specialists, providing\ninsights into their competencies and limitations in various medical contexts.\nOur work not only advances the understanding of Med-MLLMs' capabilities but\nalso sets a precedent for future evaluations and the safe deployment of these\nmodels in clinical environments."
                },
                "authors": [
                    {
                        "name": "Jie Liu"
                    },
                    {
                        "name": "Wenxuan Wang"
                    },
                    {
                        "name": "Yihang Su"
                    },
                    {
                        "name": "Jingyuan Huan"
                    },
                    {
                        "name": "Wenting Chen"
                    },
                    {
                        "name": "Yudi Zhang"
                    },
                    {
                        "name": "Cheng-Yi Li"
                    },
                    {
                        "name": "Kao-Jung Chang"
                    },
                    {
                        "name": "Xiaohan Xin"
                    },
                    {
                        "name": "Linlin Shen"
                    },
                    {
                        "name": "Michael R. Lyu"
                    }
                ],
                "author_detail": {
                    "name": "Michael R. Lyu"
                },
                "author": "Michael R. Lyu",
                "arxiv_comment": "20 pages, 15 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.11217v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.11217v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08660v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08660v3",
                "updated": "2024-11-29T02:35:47Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    2,
                    35,
                    47,
                    4,
                    334,
                    0
                ],
                "published": "2024-10-11T09:39:11Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    9,
                    39,
                    11,
                    4,
                    285,
                    0
                ],
                "title": "RePD: Defending Jailbreak Attack through a Retrieval-based Prompt\n  Decomposition Process",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RePD: Defending Jailbreak Attack through a Retrieval-based Prompt\n  Decomposition Process"
                },
                "summary": "In this study, we introduce RePD, an innovative attack Retrieval-based Prompt\nDecomposition framework designed to mitigate the risk of jailbreak attacks on\nlarge language models (LLMs). Despite rigorous pretraining and finetuning\nfocused on ethical alignment, LLMs are still susceptible to jailbreak exploits.\nRePD operates on a one-shot learning model, wherein it accesses a database of\npre-collected jailbreak prompt templates to identify and decompose harmful\ninquiries embedded within user prompts. This process involves integrating the\ndecomposition of the jailbreak prompt into the user's original query into a\none-shot learning example to effectively teach the LLM to discern and separate\nmalicious components. Consequently, the LLM is equipped to first neutralize any\npotentially harmful elements before addressing the user's prompt in a manner\nthat aligns with its ethical guidelines. RePD is versatile and compatible with\na variety of open-source LLMs acting as agents. Through comprehensive\nexperimentation with both harmful and benign prompts, we have demonstrated the\nefficacy of our proposed RePD in enhancing the resilience of LLMs against\njailbreak attacks, without compromising their performance in responding to\ntypical user requests.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this study, we introduce RePD, an innovative attack Retrieval-based Prompt\nDecomposition framework designed to mitigate the risk of jailbreak attacks on\nlarge language models (LLMs). Despite rigorous pretraining and finetuning\nfocused on ethical alignment, LLMs are still susceptible to jailbreak exploits.\nRePD operates on a one-shot learning model, wherein it accesses a database of\npre-collected jailbreak prompt templates to identify and decompose harmful\ninquiries embedded within user prompts. This process involves integrating the\ndecomposition of the jailbreak prompt into the user's original query into a\none-shot learning example to effectively teach the LLM to discern and separate\nmalicious components. Consequently, the LLM is equipped to first neutralize any\npotentially harmful elements before addressing the user's prompt in a manner\nthat aligns with its ethical guidelines. RePD is versatile and compatible with\na variety of open-source LLMs acting as agents. Through comprehensive\nexperimentation with both harmful and benign prompts, we have demonstrated the\nefficacy of our proposed RePD in enhancing the resilience of LLMs against\njailbreak attacks, without compromising their performance in responding to\ntypical user requests."
                },
                "authors": [
                    {
                        "name": "Peiran Wang"
                    },
                    {
                        "name": "Xiaogeng Liu"
                    },
                    {
                        "name": "Chaowei Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Chaowei Xiao"
                },
                "author": "Chaowei Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08660v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08660v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.01247v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.01247v3",
                "updated": "2024-11-29T01:47:20Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    1,
                    47,
                    20,
                    4,
                    334,
                    0
                ],
                "published": "2024-09-02T13:29:44Z",
                "published_parsed": [
                    2024,
                    9,
                    2,
                    13,
                    29,
                    44,
                    0,
                    246,
                    0
                ],
                "title": "Conversational Complexity for Assessing Risk in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conversational Complexity for Assessing Risk in Large Language Models"
                },
                "summary": "Large Language Models (LLMs) present a dual-use dilemma: they enable\nbeneficial applications while harboring potential for harm, particularly\nthrough conversational interactions. Despite various safeguards, advanced LLMs\nremain vulnerable. A watershed case in early 2023 involved journalist Kevin\nRoose's extended dialogue with Bing, an LLM-powered search engine, which\nrevealed harmful outputs after probing questions, highlighting vulnerabilities\nin the model's safeguards. This contrasts with simpler early jailbreaks, like\nthe \"Grandma Jailbreak,\" where users framed requests as innocent help for a\ngrandmother, easily eliciting similar content. This raises the question: How\nmuch conversational effort is needed to elicit harmful information from LLMs?\nWe propose two measures to quantify this effort: Conversational Length (CL),\nwhich measures the number of conversational turns needed to obtain a specific\nharmful response, and Conversational Complexity (CC), defined as the Kolmogorov\ncomplexity of the user's instruction sequence leading to the harmful response.\nTo address the incomputability of Kolmogorov complexity, we approximate CC\nusing a reference LLM to estimate the compressibility of the user instructions.\nApplying this approach to a large red-teaming dataset, we perform a\nquantitative analysis examining the statistical distribution of harmful and\nharmless conversational lengths and complexities. Our empirical findings\nsuggest that this distributional analysis and the minimization of CC serve as\nvaluable tools for understanding AI safety, offering insights into the\naccessibility of harmful information. This work establishes a foundation for a\nnew perspective on LLM safety, centered around the algorithmic complexity of\npathways to harm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) present a dual-use dilemma: they enable\nbeneficial applications while harboring potential for harm, particularly\nthrough conversational interactions. Despite various safeguards, advanced LLMs\nremain vulnerable. A watershed case in early 2023 involved journalist Kevin\nRoose's extended dialogue with Bing, an LLM-powered search engine, which\nrevealed harmful outputs after probing questions, highlighting vulnerabilities\nin the model's safeguards. This contrasts with simpler early jailbreaks, like\nthe \"Grandma Jailbreak,\" where users framed requests as innocent help for a\ngrandmother, easily eliciting similar content. This raises the question: How\nmuch conversational effort is needed to elicit harmful information from LLMs?\nWe propose two measures to quantify this effort: Conversational Length (CL),\nwhich measures the number of conversational turns needed to obtain a specific\nharmful response, and Conversational Complexity (CC), defined as the Kolmogorov\ncomplexity of the user's instruction sequence leading to the harmful response.\nTo address the incomputability of Kolmogorov complexity, we approximate CC\nusing a reference LLM to estimate the compressibility of the user instructions.\nApplying this approach to a large red-teaming dataset, we perform a\nquantitative analysis examining the statistical distribution of harmful and\nharmless conversational lengths and complexities. Our empirical findings\nsuggest that this distributional analysis and the minimization of CC serve as\nvaluable tools for understanding AI safety, offering insights into the\naccessibility of harmful information. This work establishes a foundation for a\nnew perspective on LLM safety, centered around the algorithmic complexity of\npathways to harm."
                },
                "authors": [
                    {
                        "name": "John Burden"
                    },
                    {
                        "name": "Manuel Cebrian"
                    },
                    {
                        "name": "Jose Hernandez-Orallo"
                    }
                ],
                "author_detail": {
                    "name": "Jose Hernandez-Orallo"
                },
                "author": "Jose Hernandez-Orallo",
                "arxiv_comment": "15 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.01247v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.01247v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19430v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19430v1",
                "updated": "2024-11-29T01:46:30Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    1,
                    46,
                    30,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T01:46:30Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    1,
                    46,
                    30,
                    4,
                    334,
                    0
                ],
                "title": "Core Placement Optimization of Many-core Brain-Inspired Near-Storage\n  Systems for Spiking Neural Network Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Core Placement Optimization of Many-core Brain-Inspired Near-Storage\n  Systems for Spiking Neural Network Training"
                },
                "summary": "With the increasing application scope of spiking neural networks (SNN), the\ncomplexity of SNN models has surged, leading to an exponential growth in demand\nfor AI computility. As the new generation computing architecture of the neural\nnetworks, the efficiency and power consumption of distributed storage and\nparallel computing in the many-core near-memory computing system have attracted\nmuch attention. Among them, the mapping problem from logical cores to physical\ncores is one of the research hotspots. In order to improve the computing\nparallelism and system throughput of the many-core near-memory computing\nsystem, and to reduce power consumption, we propose a SNN training many-core\ndeployment optimization method based on Off-policy Deterministic Actor-Critic.\nWe utilize deep reinforcement learning as a nonlinear optimizer, treating the\nmany-core topology as network graph features and using graph convolution to\ninput the many-core structure into the policy network. We update the parameters\nof the policy network through near-end policy optimization to achieve\ndeployment optimization of SNN models in the many-core near-memory computing\narchitecture to reduce chip power consumption. To handle large-dimensional\naction spaces, we use continuous values matching the number of cores as the\noutput of the policy network and then discretize them again to obtain new\ndeployment schemes. Furthermore, to further balance inter-core computation\nlatency and improve system throughput, we propose a model partitioning method\nwith a balanced storage and computation strategy. Our method overcomes the\nproblems such as uneven computation and storage loads between cores, and the\nformation of local communication hotspots, significantly reducing model\ntraining time, communication costs, and average flow load between cores in the\nmany-core near-memory computing architecture.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the increasing application scope of spiking neural networks (SNN), the\ncomplexity of SNN models has surged, leading to an exponential growth in demand\nfor AI computility. As the new generation computing architecture of the neural\nnetworks, the efficiency and power consumption of distributed storage and\nparallel computing in the many-core near-memory computing system have attracted\nmuch attention. Among them, the mapping problem from logical cores to physical\ncores is one of the research hotspots. In order to improve the computing\nparallelism and system throughput of the many-core near-memory computing\nsystem, and to reduce power consumption, we propose a SNN training many-core\ndeployment optimization method based on Off-policy Deterministic Actor-Critic.\nWe utilize deep reinforcement learning as a nonlinear optimizer, treating the\nmany-core topology as network graph features and using graph convolution to\ninput the many-core structure into the policy network. We update the parameters\nof the policy network through near-end policy optimization to achieve\ndeployment optimization of SNN models in the many-core near-memory computing\narchitecture to reduce chip power consumption. To handle large-dimensional\naction spaces, we use continuous values matching the number of cores as the\noutput of the policy network and then discretize them again to obtain new\ndeployment schemes. Furthermore, to further balance inter-core computation\nlatency and improve system throughput, we propose a model partitioning method\nwith a balanced storage and computation strategy. Our method overcomes the\nproblems such as uneven computation and storage loads between cores, and the\nformation of local communication hotspots, significantly reducing model\ntraining time, communication costs, and average flow load between cores in the\nmany-core near-memory computing architecture."
                },
                "authors": [
                    {
                        "name": "Xueke Zhu"
                    },
                    {
                        "name": "Wenjie Lin"
                    },
                    {
                        "name": "Yanyu Lin"
                    },
                    {
                        "name": "Wenxiang Cheng"
                    },
                    {
                        "name": "Zhengyu Ma"
                    },
                    {
                        "name": "Yonghong Tian"
                    },
                    {
                        "name": "Huihui Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Huihui Zhou"
                },
                "arxiv_affiliation": "Pengcheng Laboratory",
                "author": "Huihui Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19430v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19430v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20302v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20302v2",
                "updated": "2024-11-29T00:27:46Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    0,
                    27,
                    46,
                    4,
                    334,
                    0
                ],
                "published": "2024-10-27T00:50:30Z",
                "published_parsed": [
                    2024,
                    10,
                    27,
                    0,
                    50,
                    30,
                    6,
                    301,
                    0
                ],
                "title": "Sequential Large Language Model-Based Hyper-Parameter Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequential Large Language Model-Based Hyper-Parameter Optimization"
                },
                "summary": "This study introduces SLLMBO, an innovative framework that leverages Large\nLanguage Models (LLMs) for hyperparameter optimization (HPO), incorporating\ndynamic search space adaptability, enhanced parameter landscape exploitation,\nand a hybrid, novel LLM-Tree-structured Parzen Estimator (LLM-TPE) sampler. By\naddressing limitations in recent fully LLM-based methods and traditional\nBayesian Optimization (BO), SLLMBO achieves more robust optimization. This\ncomprehensive benchmarking evaluates multiple LLMs, including GPT-3.5-turbo,\nGPT-4o, Claude-Sonnet-3.5, and Gemini-1.5-flash, extending prior work beyond\nGPT-3.5 and GPT-4 and establishing SLLMBO as the first framework to benchmark a\ndiverse set of LLMs for HPO. By integrating LLMs' established strengths in\nparameter initialization with the exploitation abilities demonstrated in this\nstudy, alongside TPE's exploration capabilities, the LLM-TPE sampler achieves a\nbalanced exploration-exploitation trade-off, reduces API costs, and mitigates\npremature early stoppings for more effective parameter searches. Across 14\ntabular tasks in classification and regression, the LLM-TPE sampler\noutperformed fully LLM-based methods and achieved superior results over BO\nmethods in 9 tasks. Testing early stopping in budget-constrained scenarios\nfurther demonstrated competitive performance, indicating that LLM-based methods\ngenerally benefit from extended iterations for optimal results. This work lays\nthe foundation for future research exploring open-source LLMs, reproducibility\nof LLM results in HPO, and benchmarking SLLMBO on complex datasets, such as\nimage classification, segmentation, and machine translation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study introduces SLLMBO, an innovative framework that leverages Large\nLanguage Models (LLMs) for hyperparameter optimization (HPO), incorporating\ndynamic search space adaptability, enhanced parameter landscape exploitation,\nand a hybrid, novel LLM-Tree-structured Parzen Estimator (LLM-TPE) sampler. By\naddressing limitations in recent fully LLM-based methods and traditional\nBayesian Optimization (BO), SLLMBO achieves more robust optimization. This\ncomprehensive benchmarking evaluates multiple LLMs, including GPT-3.5-turbo,\nGPT-4o, Claude-Sonnet-3.5, and Gemini-1.5-flash, extending prior work beyond\nGPT-3.5 and GPT-4 and establishing SLLMBO as the first framework to benchmark a\ndiverse set of LLMs for HPO. By integrating LLMs' established strengths in\nparameter initialization with the exploitation abilities demonstrated in this\nstudy, alongside TPE's exploration capabilities, the LLM-TPE sampler achieves a\nbalanced exploration-exploitation trade-off, reduces API costs, and mitigates\npremature early stoppings for more effective parameter searches. Across 14\ntabular tasks in classification and regression, the LLM-TPE sampler\noutperformed fully LLM-based methods and achieved superior results over BO\nmethods in 9 tasks. Testing early stopping in budget-constrained scenarios\nfurther demonstrated competitive performance, indicating that LLM-based methods\ngenerally benefit from extended iterations for optimal results. This work lays\nthe foundation for future research exploring open-source LLMs, reproducibility\nof LLM results in HPO, and benchmarking SLLMBO on complex datasets, such as\nimage classification, segmentation, and machine translation."
                },
                "authors": [
                    {
                        "name": "Kanan Mahammadli"
                    },
                    {
                        "name": "Seyda Bolelli Ertekin"
                    }
                ],
                "author_detail": {
                    "name": "Seyda Bolelli Ertekin"
                },
                "author": "Seyda Bolelli Ertekin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20302v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20302v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23031v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23031v2",
                "updated": "2024-11-28T23:00:31Z",
                "updated_parsed": [
                    2024,
                    11,
                    28,
                    23,
                    0,
                    31,
                    3,
                    333,
                    0
                ],
                "published": "2024-10-30T14:01:31Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    14,
                    1,
                    31,
                    2,
                    304,
                    0
                ],
                "title": "Offline Reinforcement Learning and Sequence Modeling for Downlink Link\n  Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Offline Reinforcement Learning and Sequence Modeling for Downlink Link\n  Adaptation"
                },
                "summary": "Link adaptation (LA) is an essential function in modern wireless\ncommunication systems that dynamically adjusts the transmission rate of a\ncommunication link to match time- and frequency-varying radio link conditions.\nHowever, factors such as user mobility, fast fading, imperfect channel quality\ninformation, and aging of measurements make the modeling of LA challenging. To\nbypass the need for explicit modeling, recent research has introduced online\nreinforcement learning (RL) approaches as an alternative to the more commonly\nused rule-based algorithms. Yet, RL-based approaches face deployment\nchallenges, as training in live networks can potentially degrade real-time\nperformance. To address this challenge, this paper considers offline RL as a\ncandidate to learn LA policies with minimal effects on the network operation.\nWe propose three LA designs based on batch-constrained deep Q-learning,\nconservative Q-learning, and decision transformer. Our results show that\noffline RL algorithms can match the performance of state-of-the-art online RL\nmethods when data is collected with a proper behavioral policy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Link adaptation (LA) is an essential function in modern wireless\ncommunication systems that dynamically adjusts the transmission rate of a\ncommunication link to match time- and frequency-varying radio link conditions.\nHowever, factors such as user mobility, fast fading, imperfect channel quality\ninformation, and aging of measurements make the modeling of LA challenging. To\nbypass the need for explicit modeling, recent research has introduced online\nreinforcement learning (RL) approaches as an alternative to the more commonly\nused rule-based algorithms. Yet, RL-based approaches face deployment\nchallenges, as training in live networks can potentially degrade real-time\nperformance. To address this challenge, this paper considers offline RL as a\ncandidate to learn LA policies with minimal effects on the network operation.\nWe propose three LA designs based on batch-constrained deep Q-learning,\nconservative Q-learning, and decision transformer. Our results show that\noffline RL algorithms can match the performance of state-of-the-art online RL\nmethods when data is collected with a proper behavioral policy."
                },
                "authors": [
                    {
                        "name": "Samuele Peri"
                    },
                    {
                        "name": "Alessio Russo"
                    },
                    {
                        "name": "Gabor Fodor"
                    },
                    {
                        "name": "Pablo Soldati"
                    }
                ],
                "author_detail": {
                    "name": "Pablo Soldati"
                },
                "author": "Pablo Soldati",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23031v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23031v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.13879v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.13879v2",
                "updated": "2024-11-28T22:32:19Z",
                "updated_parsed": [
                    2024,
                    11,
                    28,
                    22,
                    32,
                    19,
                    3,
                    333,
                    0
                ],
                "published": "2023-09-25T05:10:50Z",
                "published_parsed": [
                    2023,
                    9,
                    25,
                    5,
                    10,
                    50,
                    0,
                    268,
                    0
                ],
                "title": "User Interaction Patterns and Breakdowns in Conversing with LLM-Powered\n  Voice Assistants",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "User Interaction Patterns and Breakdowns in Conversing with LLM-Powered\n  Voice Assistants"
                },
                "summary": "Conventional Voice Assistants (VAs) rely on traditional language models to\ndiscern user intent and respond to their queries, leading to interactions that\noften lack a broader contextual understanding, an area in which Large Language\nModels (LLMs) excel. However, current LLMs are largely designed for text-based\ninteractions, thus making it unclear how user interactions will evolve if their\nmodality is changed to voice. In this work, we investigate whether LLMs can\nenrich VA interactions via an exploratory study with participants (N=20) using\na ChatGPT-powered VA for three scenarios (medical self-diagnosis, creative\nplanning, and discussion) with varied constraints, stakes, and objectivity. We\nobserve that LLM-powered VA elicits richer interaction patterns that vary\nacross tasks, showing its versatility. Notably, LLMs absorb the majority of VA\nintent recognition failures. We additionally discuss the potential of\nharnessing LLMs for more resilient and fluid user-VA interactions and provide\ndesign guidelines for tailoring LLMs for voice assistance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conventional Voice Assistants (VAs) rely on traditional language models to\ndiscern user intent and respond to their queries, leading to interactions that\noften lack a broader contextual understanding, an area in which Large Language\nModels (LLMs) excel. However, current LLMs are largely designed for text-based\ninteractions, thus making it unclear how user interactions will evolve if their\nmodality is changed to voice. In this work, we investigate whether LLMs can\nenrich VA interactions via an exploratory study with participants (N=20) using\na ChatGPT-powered VA for three scenarios (medical self-diagnosis, creative\nplanning, and discussion) with varied constraints, stakes, and objectivity. We\nobserve that LLM-powered VA elicits richer interaction patterns that vary\nacross tasks, showing its versatility. Notably, LLMs absorb the majority of VA\nintent recognition failures. We additionally discuss the potential of\nharnessing LLMs for more resilient and fluid user-VA interactions and provide\ndesign guidelines for tailoring LLMs for voice assistance."
                },
                "authors": [
                    {
                        "name": "Amama Mahmood"
                    },
                    {
                        "name": "Junxiang Wang"
                    },
                    {
                        "name": "Bingsheng Yao"
                    },
                    {
                        "name": "Dakuo Wang"
                    },
                    {
                        "name": "Chien-Ming Huang"
                    }
                ],
                "author_detail": {
                    "name": "Chien-Ming Huang"
                },
                "author": "Chien-Ming Huang",
                "arxiv_doi": "10.1016/j.ijhcs.2024.103406",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.ijhcs.2024.103406",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2309.13879v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.13879v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06336v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06336v2",
                "updated": "2024-11-28T22:19:48Z",
                "updated_parsed": [
                    2024,
                    11,
                    28,
                    22,
                    19,
                    48,
                    3,
                    333,
                    0
                ],
                "published": "2024-09-10T08:47:23Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    8,
                    47,
                    23,
                    1,
                    254,
                    0
                ],
                "title": "Towards Agentic AI on Particle Accelerators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Agentic AI on Particle Accelerators"
                },
                "summary": "As particle accelerators grow in complexity, traditional control methods face\nincreasing challenges in achieving optimal performance. This paper envisions a\nparadigm shift: a decentralized multi-agent framework for accelerator control,\npowered by Large Language Models (LLMs) and distributed among autonomous\nagents. We present a proposition of a self-improving decentralized system where\nintelligent agents handle high-level tasks and communication and each agent is\nspecialized to control individual accelerator components.\n  This approach raises some questions: What are the future applications of AI\nin particle accelerators? How can we implement an autonomous complex system\nsuch as a particle accelerator where agents gradually improve through\nexperience and human feedback? What are the implications of integrating a\nhuman-in-the-loop component for labeling operational data and providing expert\nguidance? We show three examples, where we demonstrate the viability of such\narchitecture.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As particle accelerators grow in complexity, traditional control methods face\nincreasing challenges in achieving optimal performance. This paper envisions a\nparadigm shift: a decentralized multi-agent framework for accelerator control,\npowered by Large Language Models (LLMs) and distributed among autonomous\nagents. We present a proposition of a self-improving decentralized system where\nintelligent agents handle high-level tasks and communication and each agent is\nspecialized to control individual accelerator components.\n  This approach raises some questions: What are the future applications of AI\nin particle accelerators? How can we implement an autonomous complex system\nsuch as a particle accelerator where agents gradually improve through\nexperience and human feedback? What are the implications of integrating a\nhuman-in-the-loop component for labeling operational data and providing expert\nguidance? We show three examples, where we demonstrate the viability of such\narchitecture."
                },
                "authors": [
                    {
                        "name": "Antonin Sulc"
                    },
                    {
                        "name": "Thorsten Hellert"
                    },
                    {
                        "name": "Raimund Kammering"
                    },
                    {
                        "name": "Hayden Houscher"
                    },
                    {
                        "name": "Jason St. John"
                    }
                ],
                "author_detail": {
                    "name": "Jason St. John"
                },
                "author": "Jason St. John",
                "arxiv_comment": "5 pages, 3 figures, Machine Learning and the Physical Sciences at\n  Workshop at the 38th conference on Neural Information Processing Systems\n  (NeurIPS)",
                "arxiv_journal_ref": "Machine Learning and the Physical Sciences Workshop at the 38th\n  conference on Neural Information Processing Systems (NeurIPS) December 15,\n  2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06336v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06336v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.acc-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.acc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.00820v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.00820v2",
                "updated": "2024-11-28T22:01:57Z",
                "updated_parsed": [
                    2024,
                    11,
                    28,
                    22,
                    1,
                    57,
                    3,
                    333,
                    0
                ],
                "published": "2024-01-01T17:32:28Z",
                "published_parsed": [
                    2024,
                    1,
                    1,
                    17,
                    32,
                    28,
                    0,
                    1,
                    0
                ],
                "title": "A Computational Framework for Behavioral Assessment of LLM Therapists",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Computational Framework for Behavioral Assessment of LLM Therapists"
                },
                "summary": "The emergence of large language models (LLMs) like ChatGPT has increased\ninterest in their use as therapists to address mental health challenges and the\nwidespread lack of access to care. However, experts have emphasized the\ncritical need for systematic evaluation of LLM-based mental health\ninterventions to accurately assess their capabilities and limitations. Here, we\npropose BOLT, a proof-of-concept computational framework to systematically\nassess the conversational behavior of LLM therapists. We quantitatively measure\nLLM behavior across 13 psychotherapeutic approaches with in-context learning\nmethods. Then, we compare the behavior of LLMs against high- and low-quality\nhuman therapy. Our analysis based on Motivational Interviewing therapy reveals\nthat LLMs often resemble behaviors more commonly exhibited in low-quality\ntherapy rather than high-quality therapy, such as offering a higher degree of\nproblem-solving advice when clients share emotions. However, unlike low-quality\ntherapy, LLMs reflect significantly more upon clients' needs and strengths. Our\nfindings caution that LLM therapists still require further research for\nconsistent, high-quality care.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of large language models (LLMs) like ChatGPT has increased\ninterest in their use as therapists to address mental health challenges and the\nwidespread lack of access to care. However, experts have emphasized the\ncritical need for systematic evaluation of LLM-based mental health\ninterventions to accurately assess their capabilities and limitations. Here, we\npropose BOLT, a proof-of-concept computational framework to systematically\nassess the conversational behavior of LLM therapists. We quantitatively measure\nLLM behavior across 13 psychotherapeutic approaches with in-context learning\nmethods. Then, we compare the behavior of LLMs against high- and low-quality\nhuman therapy. Our analysis based on Motivational Interviewing therapy reveals\nthat LLMs often resemble behaviors more commonly exhibited in low-quality\ntherapy rather than high-quality therapy, such as offering a higher degree of\nproblem-solving advice when clients share emotions. However, unlike low-quality\ntherapy, LLMs reflect significantly more upon clients' needs and strengths. Our\nfindings caution that LLM therapists still require further research for\nconsistent, high-quality care."
                },
                "authors": [
                    {
                        "name": "Yu Ying Chiu"
                    },
                    {
                        "name": "Ashish Sharma"
                    },
                    {
                        "name": "Inna Wanyin Lin"
                    },
                    {
                        "name": "Tim Althoff"
                    }
                ],
                "author_detail": {
                    "name": "Tim Althoff"
                },
                "author": "Tim Althoff",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.00820v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.00820v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19379v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19379v1",
                "updated": "2024-11-28T21:10:20Z",
                "updated_parsed": [
                    2024,
                    11,
                    28,
                    21,
                    10,
                    20,
                    3,
                    333,
                    0
                ],
                "published": "2024-11-28T21:10:20Z",
                "published_parsed": [
                    2024,
                    11,
                    28,
                    21,
                    10,
                    20,
                    3,
                    333,
                    0
                ],
                "title": "Marconi: Prefix Caching for the Era of Hybrid LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Marconi: Prefix Caching for the Era of Hybrid LLMs"
                },
                "summary": "Hybrid models that combine the language modeling capabilities of Attention\nlayers with the efficiency of Recurrent layers (e.g., State Space Models) have\ngained traction in practically supporting long contexts in Large Language Model\nserving. Yet, the unique properties of these models complicate the usage of\ncomplementary efficiency optimizations such as prefix caching that skip\nredundant computations across requests. Most notably, their use of in-place\nstate updates for recurrent layers precludes rolling back cache entries for\npartial sequence overlaps, and instead mandates only exact-match cache hits;\nthe effect is a deluge of (large) cache entries per sequence, most of which\nyield minimal reuse opportunities. We present Marconi, the first system that\nsupports efficient prefix caching with Hybrid LLMs. Key to Marconi are its\nnovel admission and eviction policies that more judiciously assess potential\ncache entries based not only on recency, but also on (1) forecasts of their\nreuse likelihood across a taxonomy of different hit scenarios, and (2) the\ncompute savings that hits deliver relative to memory footprints. Across diverse\nworkloads and Hybrid models, Marconi achieves up to 34.4$\\times$ higher token\nhit rates (71.1% or 617 ms lower TTFT) compared to state-of-the-art prefix\ncaching systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid models that combine the language modeling capabilities of Attention\nlayers with the efficiency of Recurrent layers (e.g., State Space Models) have\ngained traction in practically supporting long contexts in Large Language Model\nserving. Yet, the unique properties of these models complicate the usage of\ncomplementary efficiency optimizations such as prefix caching that skip\nredundant computations across requests. Most notably, their use of in-place\nstate updates for recurrent layers precludes rolling back cache entries for\npartial sequence overlaps, and instead mandates only exact-match cache hits;\nthe effect is a deluge of (large) cache entries per sequence, most of which\nyield minimal reuse opportunities. We present Marconi, the first system that\nsupports efficient prefix caching with Hybrid LLMs. Key to Marconi are its\nnovel admission and eviction policies that more judiciously assess potential\ncache entries based not only on recency, but also on (1) forecasts of their\nreuse likelihood across a taxonomy of different hit scenarios, and (2) the\ncompute savings that hits deliver relative to memory footprints. Across diverse\nworkloads and Hybrid models, Marconi achieves up to 34.4$\\times$ higher token\nhit rates (71.1% or 617 ms lower TTFT) compared to state-of-the-art prefix\ncaching systems."
                },
                "authors": [
                    {
                        "name": "Rui Pan"
                    },
                    {
                        "name": "Zhuang Wang"
                    },
                    {
                        "name": "Zhen Jia"
                    },
                    {
                        "name": "Can Karakus"
                    },
                    {
                        "name": "Luca Zancato"
                    },
                    {
                        "name": "Tri Dao"
                    },
                    {
                        "name": "Ravi Netravali"
                    },
                    {
                        "name": "Yida Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yida Wang"
                },
                "author": "Yida Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19379v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19379v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18969v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18969v2",
                "updated": "2024-11-28T20:29:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    28,
                    20,
                    29,
                    44,
                    3,
                    333,
                    0
                ],
                "published": "2024-09-11T14:50:28Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    14,
                    50,
                    28,
                    2,
                    255,
                    0
                ],
                "title": "Integrating SPARQL and LLMs for Question Answering over Scholarly Data\n  Sources",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating SPARQL and LLMs for Question Answering over Scholarly Data\n  Sources"
                },
                "summary": "The Scholarly Hybrid Question Answering over Linked Data (QALD) Challenge at\nthe International Semantic Web Conference (ISWC) 2024 focuses on Question\nAnswering (QA) over diverse scholarly sources: DBLP, SemOpenAlex, and\nWikipedia-based texts. This paper describes a methodology that combines SPARQL\nqueries, divide and conquer algorithms, and a pre-trained extractive question\nanswering model. It starts with SPARQL queries to gather data, then applies\ndivide and conquer to manage various question types and sources, and uses the\nmodel to handle personal author questions. The approach, evaluated with Exact\nMatch and F-score metrics, shows promise for improving QA accuracy and\nefficiency in scholarly contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Scholarly Hybrid Question Answering over Linked Data (QALD) Challenge at\nthe International Semantic Web Conference (ISWC) 2024 focuses on Question\nAnswering (QA) over diverse scholarly sources: DBLP, SemOpenAlex, and\nWikipedia-based texts. This paper describes a methodology that combines SPARQL\nqueries, divide and conquer algorithms, and a pre-trained extractive question\nanswering model. It starts with SPARQL queries to gather data, then applies\ndivide and conquer to manage various question types and sources, and uses the\nmodel to handle personal author questions. The approach, evaluated with Exact\nMatch and F-score metrics, shows promise for improving QA accuracy and\nefficiency in scholarly contexts."
                },
                "authors": [
                    {
                        "name": "Fomubad Borista Fondi"
                    },
                    {
                        "name": "Azanzi Jiomekong Fidel"
                    },
                    {
                        "name": "Gaoussou Camara"
                    }
                ],
                "author_detail": {
                    "name": "Gaoussou Camara"
                },
                "author": "Gaoussou Camara",
                "arxiv_comment": "Scholarly Hybrid Question answering challenge from the International\n  Semantic Web Conference of 2024(ISWC), 7 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18969v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18969v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19134v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19134v2",
                "updated": "2024-11-28T20:20:23Z",
                "updated_parsed": [
                    2024,
                    11,
                    28,
                    20,
                    20,
                    23,
                    3,
                    333,
                    0
                ],
                "published": "2024-09-27T20:32:42Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    20,
                    32,
                    42,
                    4,
                    271,
                    0
                ],
                "title": "Confidential Prompting: Protecting User Prompts from Cloud LLM Providers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Confidential Prompting: Protecting User Prompts from Cloud LLM Providers"
                },
                "summary": "Our work tackles the challenge of securing user inputs in cloud-hosted large\nlanguage model (LLM) serving while ensuring output invariance, model\nconfidentiality, and compute efficiency. We introduce secure multi-party\ndecoding (SMD), which leverages confidential computing to confine user prompts\nto a trusted execution environment (TEE), namely a confidential virtual machine\n(CVM), while allowing service providers to generate tokens efficiently. We also\nintroduce a novel cryptographic method, prompt obfuscation (PO), to ensure\nrobustness against reconstruction attacks on SMD. We demonstrate that our\napproach preserves both prompt confidentiality and LLM serving efficiency. Our\nsolution can enable privacy-preserving cloud LLM serving that handles sensitive\nprompts, such as clinical records, financial data, and personal information.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Our work tackles the challenge of securing user inputs in cloud-hosted large\nlanguage model (LLM) serving while ensuring output invariance, model\nconfidentiality, and compute efficiency. We introduce secure multi-party\ndecoding (SMD), which leverages confidential computing to confine user prompts\nto a trusted execution environment (TEE), namely a confidential virtual machine\n(CVM), while allowing service providers to generate tokens efficiently. We also\nintroduce a novel cryptographic method, prompt obfuscation (PO), to ensure\nrobustness against reconstruction attacks on SMD. We demonstrate that our\napproach preserves both prompt confidentiality and LLM serving efficiency. Our\nsolution can enable privacy-preserving cloud LLM serving that handles sensitive\nprompts, such as clinical records, financial data, and personal information."
                },
                "authors": [
                    {
                        "name": "In Gim"
                    },
                    {
                        "name": "Caihua Li"
                    },
                    {
                        "name": "Lin Zhong"
                    }
                ],
                "author_detail": {
                    "name": "Lin Zhong"
                },
                "author": "Lin Zhong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19134v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19134v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19360v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19360v1",
                "updated": "2024-11-28T20:14:47Z",
                "updated_parsed": [
                    2024,
                    11,
                    28,
                    20,
                    14,
                    47,
                    3,
                    333,
                    0
                ],
                "published": "2024-11-28T20:14:47Z",
                "published_parsed": [
                    2024,
                    11,
                    28,
                    20,
                    14,
                    47,
                    3,
                    333,
                    0
                ],
                "title": "DENIAHL: In-Context Features Influence LLM Needle-In-A-Haystack\n  Abilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DENIAHL: In-Context Features Influence LLM Needle-In-A-Haystack\n  Abilities"
                },
                "summary": "The Needle-in-a-haystack (NIAH) test is a general task used to assess\nlanguage models' (LMs') abilities to recall particular information from long\ninput context. This framework however does not provide a means of analyzing\nwhat factors, beyond context length, contribute to LMs' abilities or\ninabilities to separate and recall needles from their haystacks. To provide a\nsystematic means of assessing what features contribute to LMs' NIAH\ncapabilities, we developed a synthetic benchmark called DENIAHL (Data-oriented\nEvaluation of NIAH for LLM's). Our work expands on previous NIAH studies by\nablating NIAH features beyond typical context length including data type, size,\nand patterns. We find stark differences between GPT-3.5 and LLaMA 2-7B's\nperformance on DENIAHL, and drops in recall performance when features like item\nsize are increased, and to some degree when data type is changed from numbers\nto letters. This has implications for increasingly large context models,\ndemonstrating factors beyond item-number impact NIAH capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Needle-in-a-haystack (NIAH) test is a general task used to assess\nlanguage models' (LMs') abilities to recall particular information from long\ninput context. This framework however does not provide a means of analyzing\nwhat factors, beyond context length, contribute to LMs' abilities or\ninabilities to separate and recall needles from their haystacks. To provide a\nsystematic means of assessing what features contribute to LMs' NIAH\ncapabilities, we developed a synthetic benchmark called DENIAHL (Data-oriented\nEvaluation of NIAH for LLM's). Our work expands on previous NIAH studies by\nablating NIAH features beyond typical context length including data type, size,\nand patterns. We find stark differences between GPT-3.5 and LLaMA 2-7B's\nperformance on DENIAHL, and drops in recall performance when features like item\nsize are increased, and to some degree when data type is changed from numbers\nto letters. This has implications for increasingly large context models,\ndemonstrating factors beyond item-number impact NIAH capabilities."
                },
                "authors": [
                    {
                        "name": "Hui Dai"
                    },
                    {
                        "name": "Dan Pechi"
                    },
                    {
                        "name": "Xinyi Yang"
                    },
                    {
                        "name": "Garvit Banga"
                    },
                    {
                        "name": "Raghav Mantri"
                    }
                ],
                "author_detail": {
                    "name": "Raghav Mantri"
                },
                "author": "Raghav Mantri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19360v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19360v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20739v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20739v3",
                "updated": "2024-11-28T19:54:09Z",
                "updated_parsed": [
                    2024,
                    11,
                    28,
                    19,
                    54,
                    9,
                    3,
                    333,
                    0
                ],
                "published": "2024-10-28T05:08:08Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    5,
                    8,
                    8,
                    0,
                    302,
                    0
                ],
                "title": "Gender Bias in LLM-generated Interview Responses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gender Bias in LLM-generated Interview Responses"
                },
                "summary": "LLMs have emerged as a promising tool for assisting individuals in diverse\ntext-generation tasks, including job-related texts. However, LLM-generated\nanswers have been increasingly found to exhibit gender bias. This study\nevaluates three LLMs (GPT-3.5, GPT-4, Claude) to conduct a multifaceted audit\nof LLM-generated interview responses across models, question types, and jobs,\nand their alignment with two gender stereotypes. Our findings reveal that\ngender bias is consistent, and closely aligned with gender stereotypes and the\ndominance of jobs. Overall, this study contributes to the systematic\nexamination of gender bias in LLM-generated interview responses, highlighting\nthe need for a mindful approach to mitigate such biases in related\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs have emerged as a promising tool for assisting individuals in diverse\ntext-generation tasks, including job-related texts. However, LLM-generated\nanswers have been increasingly found to exhibit gender bias. This study\nevaluates three LLMs (GPT-3.5, GPT-4, Claude) to conduct a multifaceted audit\nof LLM-generated interview responses across models, question types, and jobs,\nand their alignment with two gender stereotypes. Our findings reveal that\ngender bias is consistent, and closely aligned with gender stereotypes and the\ndominance of jobs. Overall, this study contributes to the systematic\nexamination of gender bias in LLM-generated interview responses, highlighting\nthe need for a mindful approach to mitigate such biases in related\napplications."
                },
                "authors": [
                    {
                        "name": "Haein Kong"
                    },
                    {
                        "name": "Yongsu Ahn"
                    },
                    {
                        "name": "Sangyub Lee"
                    },
                    {
                        "name": "Yunho Maeng"
                    }
                ],
                "author_detail": {
                    "name": "Yunho Maeng"
                },
                "author": "Yunho Maeng",
                "arxiv_comment": "Accepted to NeurlIPS 2024, SoLaR workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20739v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20739v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19352v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19352v1",
                "updated": "2024-11-28T19:53:39Z",
                "updated_parsed": [
                    2024,
                    11,
                    28,
                    19,
                    53,
                    39,
                    3,
                    333,
                    0
                ],
                "published": "2024-11-28T19:53:39Z",
                "published_parsed": [
                    2024,
                    11,
                    28,
                    19,
                    53,
                    39,
                    3,
                    333,
                    0
                ],
                "title": "OMuleT: Orchestrating Multiple Tools for Practicable Conversational\n  Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OMuleT: Orchestrating Multiple Tools for Practicable Conversational\n  Recommendation"
                },
                "summary": "In this paper, we present a systematic effort to design, evaluate, and\nimplement a realistic conversational recommender system (CRS). The objective of\nour system is to allow users to input free-form text to request\nrecommendations, and then receive a list of relevant and diverse items. While\nprevious work on synthetic queries augments large language models (LLMs) with\n1-3 tools, we argue that a more extensive toolbox is necessary to effectively\nhandle real user requests. As such, we propose a novel approach that equips\nLLMs with over 10 tools, providing them access to the internal knowledge base\nand API calls used in production. We evaluate our model on a dataset of real\nusers and show that it generates relevant, novel, and diverse recommendations\ncompared to vanilla LLMs. Furthermore, we conduct ablation studies to\ndemonstrate the effectiveness of using the full range of tools in our toolbox.\nWe share our designs and lessons learned from deploying the system for internal\nalpha release. Our contribution is the addressing of all four key aspects of a\npracticable CRS: (1) real user requests, (2) augmenting LLMs with a wide\nvariety of tools, (3) extensive evaluation, and (4) deployment insights.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present a systematic effort to design, evaluate, and\nimplement a realistic conversational recommender system (CRS). The objective of\nour system is to allow users to input free-form text to request\nrecommendations, and then receive a list of relevant and diverse items. While\nprevious work on synthetic queries augments large language models (LLMs) with\n1-3 tools, we argue that a more extensive toolbox is necessary to effectively\nhandle real user requests. As such, we propose a novel approach that equips\nLLMs with over 10 tools, providing them access to the internal knowledge base\nand API calls used in production. We evaluate our model on a dataset of real\nusers and show that it generates relevant, novel, and diverse recommendations\ncompared to vanilla LLMs. Furthermore, we conduct ablation studies to\ndemonstrate the effectiveness of using the full range of tools in our toolbox.\nWe share our designs and lessons learned from deploying the system for internal\nalpha release. Our contribution is the addressing of all four key aspects of a\npracticable CRS: (1) real user requests, (2) augmenting LLMs with a wide\nvariety of tools, (3) extensive evaluation, and (4) deployment insights."
                },
                "authors": [
                    {
                        "name": "Se-eun Yoon"
                    },
                    {
                        "name": "Xiaokai Wei"
                    },
                    {
                        "name": "Yexi Jiang"
                    },
                    {
                        "name": "Rachit Pareek"
                    },
                    {
                        "name": "Frank Ong"
                    },
                    {
                        "name": "Kevin Gao"
                    },
                    {
                        "name": "Julian McAuley"
                    },
                    {
                        "name": "Michelle Gong"
                    }
                ],
                "author_detail": {
                    "name": "Michelle Gong"
                },
                "author": "Michelle Gong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19352v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19352v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19346v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19346v1",
                "updated": "2024-11-28T19:48:54Z",
                "updated_parsed": [
                    2024,
                    11,
                    28,
                    19,
                    48,
                    54,
                    3,
                    333,
                    0
                ],
                "published": "2024-11-28T19:48:54Z",
                "published_parsed": [
                    2024,
                    11,
                    28,
                    19,
                    48,
                    54,
                    3,
                    333,
                    0
                ],
                "title": "CLIP meets DINO for Tuning Zero-Shot Classifier using Unlabeled Image\n  Collections",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CLIP meets DINO for Tuning Zero-Shot Classifier using Unlabeled Image\n  Collections"
                },
                "summary": "In the era of foundation models, CLIP has emerged as a powerful tool for\naligning text and visual modalities into a common embedding space. However, the\nalignment objective used to train CLIP often results in subpar visual features\nfor fine-grained tasks. In contrast, SSL-pretrained models like DINO excel at\nextracting rich visual features due to their specialized training paradigm.\nYet, these SSL models require an additional supervised linear probing step,\nwhich relies on fully labeled data which is often expensive and difficult to\nobtain at scale. In this paper, we propose a label-free prompt-tuning method\nthat leverages the rich visual features of self-supervised learning models\n(DINO) and the broad textual knowledge of large language models (LLMs) to\nlargely enhance CLIP-based image classification performance using unlabeled\nimages. Our approach unfolds in three key steps: (1) We generate robust textual\nfeature embeddings that more accurately represent object classes by leveraging\nclass-specific descriptions from LLMs, enabling more effective zero-shot\nclassification compared to CLIP's default name-specific prompts. (2) These\ntextual embeddings are then used to produce pseudo-labels to train an alignment\nmodule that integrates the complementary strengths of LLM description-based\ntextual embeddings and DINO's visual features. (3) Finally, we prompt-tune\nCLIP's vision encoder through DINO-assisted supervision using the trained\nalignment module. This three-step process allows us to harness the best of\nvisual and textual foundation models, resulting in a powerful and efficient\napproach that surpasses state-of-the-art label-free classification methods.\nNotably, our framework, NoLA (No Labels Attached), achieves an average absolute\ngain of 3.6% over the state-of-the-art LaFter across 11 diverse image\nclassification datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the era of foundation models, CLIP has emerged as a powerful tool for\naligning text and visual modalities into a common embedding space. However, the\nalignment objective used to train CLIP often results in subpar visual features\nfor fine-grained tasks. In contrast, SSL-pretrained models like DINO excel at\nextracting rich visual features due to their specialized training paradigm.\nYet, these SSL models require an additional supervised linear probing step,\nwhich relies on fully labeled data which is often expensive and difficult to\nobtain at scale. In this paper, we propose a label-free prompt-tuning method\nthat leverages the rich visual features of self-supervised learning models\n(DINO) and the broad textual knowledge of large language models (LLMs) to\nlargely enhance CLIP-based image classification performance using unlabeled\nimages. Our approach unfolds in three key steps: (1) We generate robust textual\nfeature embeddings that more accurately represent object classes by leveraging\nclass-specific descriptions from LLMs, enabling more effective zero-shot\nclassification compared to CLIP's default name-specific prompts. (2) These\ntextual embeddings are then used to produce pseudo-labels to train an alignment\nmodule that integrates the complementary strengths of LLM description-based\ntextual embeddings and DINO's visual features. (3) Finally, we prompt-tune\nCLIP's vision encoder through DINO-assisted supervision using the trained\nalignment module. This three-step process allows us to harness the best of\nvisual and textual foundation models, resulting in a powerful and efficient\napproach that surpasses state-of-the-art label-free classification methods.\nNotably, our framework, NoLA (No Labels Attached), achieves an average absolute\ngain of 3.6% over the state-of-the-art LaFter across 11 diverse image\nclassification datasets."
                },
                "authors": [
                    {
                        "name": "Mohamed Fazli Imam"
                    },
                    {
                        "name": "Rufael Fedaku Marew"
                    },
                    {
                        "name": "Jameel Hassan"
                    },
                    {
                        "name": "Mustansar Fiaz"
                    },
                    {
                        "name": "Alham Fikri Aji"
                    },
                    {
                        "name": "Hisham Cholakkal"
                    }
                ],
                "author_detail": {
                    "name": "Hisham Cholakkal"
                },
                "author": "Hisham Cholakkal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19346v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19346v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03136v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03136v2",
                "updated": "2024-11-28T19:47:26Z",
                "updated_parsed": [
                    2024,
                    11,
                    28,
                    19,
                    47,
                    26,
                    3,
                    333,
                    0
                ],
                "published": "2024-10-04T04:23:36Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    4,
                    23,
                    36,
                    4,
                    278,
                    0
                ],
                "title": "Deliberate Reasoning for LLMs as Structure-aware Planning with Accurate\n  World Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deliberate Reasoning for LLMs as Structure-aware Planning with Accurate\n  World Model"
                },
                "summary": "Enhancing the reasoning capabilities of large language models (LLMs) remains\na key challenge, especially for tasks that require complex, multi-step\ndecision-making. Humans excel at these tasks by leveraging deliberate planning\nwith an internal world model to simulate the potential outcomes of various\nactions. Inspired by this, we propose a novel multi-step reasoning framework\nfor LLMs, referred to as Structure-aware Planning with Accurate World Model\n(SWAP). Unlike previous approaches that rely solely on Chain-of-Thought (CoT)\nreasoning in natural language, SWAP incorporates structural information to\nguide the reasoning process via a world model and provides a soft verification\nmechanism over the steps. Moreover, SWAP overcomes the challenge of accurate\nworld state predictions in complex reasoning tasks by introducing a\nGenerator-Discriminator architecture, which enables more reliable world\nmodeling. Specifically, the generator predicts the next state, and the\ndiscriminator ensures alignment with the logical consistency required by the\nproblem context. SWAP also encourages the policy model to explore a broad range\nof potential actions to prevent premature convergence. By resolving the\nbottlenecks of generation diversity for both actions and states using\ndiversity-based modeling (DBM) and improving discrimination accuracy through\ncontrastive ranking (CR), SWAP significantly enhances the reasoning performance\nof LLMs. We evaluate SWAP across diverse reasoning-intensive benchmarks\nincluding math reasoning, logical reasoning, and coding tasks. Extensive\nexperiments demonstrate that SWAP achieves substantial improvements over the\nbaselines and consistently outperforms existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing the reasoning capabilities of large language models (LLMs) remains\na key challenge, especially for tasks that require complex, multi-step\ndecision-making. Humans excel at these tasks by leveraging deliberate planning\nwith an internal world model to simulate the potential outcomes of various\nactions. Inspired by this, we propose a novel multi-step reasoning framework\nfor LLMs, referred to as Structure-aware Planning with Accurate World Model\n(SWAP). Unlike previous approaches that rely solely on Chain-of-Thought (CoT)\nreasoning in natural language, SWAP incorporates structural information to\nguide the reasoning process via a world model and provides a soft verification\nmechanism over the steps. Moreover, SWAP overcomes the challenge of accurate\nworld state predictions in complex reasoning tasks by introducing a\nGenerator-Discriminator architecture, which enables more reliable world\nmodeling. Specifically, the generator predicts the next state, and the\ndiscriminator ensures alignment with the logical consistency required by the\nproblem context. SWAP also encourages the policy model to explore a broad range\nof potential actions to prevent premature convergence. By resolving the\nbottlenecks of generation diversity for both actions and states using\ndiversity-based modeling (DBM) and improving discrimination accuracy through\ncontrastive ranking (CR), SWAP significantly enhances the reasoning performance\nof LLMs. We evaluate SWAP across diverse reasoning-intensive benchmarks\nincluding math reasoning, logical reasoning, and coding tasks. Extensive\nexperiments demonstrate that SWAP achieves substantial improvements over the\nbaselines and consistently outperforms existing methods."
                },
                "authors": [
                    {
                        "name": "Siheng Xiong"
                    },
                    {
                        "name": "Ali Payani"
                    },
                    {
                        "name": "Yuan Yang"
                    },
                    {
                        "name": "Faramarz Fekri"
                    }
                ],
                "author_detail": {
                    "name": "Faramarz Fekri"
                },
                "author": "Faramarz Fekri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03136v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03136v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12227v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12227v3",
                "updated": "2024-11-28T18:26:28Z",
                "updated_parsed": [
                    2024,
                    11,
                    28,
                    18,
                    26,
                    28,
                    3,
                    333,
                    0
                ],
                "published": "2024-06-18T03:05:08Z",
                "published_parsed": [
                    2024,
                    6,
                    18,
                    3,
                    5,
                    8,
                    1,
                    170,
                    0
                ],
                "title": "Refine Large Language Model Fine-tuning via Instruction Vector",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Refine Large Language Model Fine-tuning via Instruction Vector"
                },
                "summary": "Fine-tuning large language models (LLMs) can cause them to lose their general\ncapabilities. However, the intrinsic mechanisms behind such forgetting remain\nunexplored. In this paper, we begin by examining this phenomenon by focusing on\nknowledge understanding and instruction following, with the latter identified\nas the main contributor to forgetting during fine-tuning. Consequently, we\npropose the Instruction Vector (IV) framework to capture model representations\nhighly related to specific instruction-following capabilities, thereby making\nit possible to understand model-intrinsic forgetting. Through the analysis of\nIV dynamics pre and post-training, we suggest that fine-tuning mostly adds\nspecialized reasoning patterns instead of erasing previous skills, which may\nappear as forgetting. Building on this insight, we develop IV-guided training,\nwhich aims to preserve original computation graph, thereby mitigating\ncatastrophic forgetting. Empirical tests on three benchmarks confirm the\nefficacy of this new approach, supporting the relationship between IVs and\nforgetting. Our code will be made available soon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning large language models (LLMs) can cause them to lose their general\ncapabilities. However, the intrinsic mechanisms behind such forgetting remain\nunexplored. In this paper, we begin by examining this phenomenon by focusing on\nknowledge understanding and instruction following, with the latter identified\nas the main contributor to forgetting during fine-tuning. Consequently, we\npropose the Instruction Vector (IV) framework to capture model representations\nhighly related to specific instruction-following capabilities, thereby making\nit possible to understand model-intrinsic forgetting. Through the analysis of\nIV dynamics pre and post-training, we suggest that fine-tuning mostly adds\nspecialized reasoning patterns instead of erasing previous skills, which may\nappear as forgetting. Building on this insight, we develop IV-guided training,\nwhich aims to preserve original computation graph, thereby mitigating\ncatastrophic forgetting. Empirical tests on three benchmarks confirm the\nefficacy of this new approach, supporting the relationship between IVs and\nforgetting. Our code will be made available soon."
                },
                "authors": [
                    {
                        "name": "Gangwei Jiang"
                    },
                    {
                        "name": "Zhaoyi Li"
                    },
                    {
                        "name": "Defu Lian"
                    },
                    {
                        "name": "Ying Wei"
                    }
                ],
                "author_detail": {
                    "name": "Ying Wei"
                },
                "author": "Ying Wei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12227v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12227v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19301v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19301v1",
                "updated": "2024-11-28T18:16:41Z",
                "updated_parsed": [
                    2024,
                    11,
                    28,
                    18,
                    16,
                    41,
                    3,
                    333,
                    0
                ],
                "published": "2024-11-28T18:16:41Z",
                "published_parsed": [
                    2024,
                    11,
                    28,
                    18,
                    16,
                    41,
                    3,
                    333,
                    0
                ],
                "title": "Structured Object Language Modeling (SoLM): Native Structured Objects\n  Generation Conforming to Complex Schemas with Self-Supervised Denoising",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Structured Object Language Modeling (SoLM): Native Structured Objects\n  Generation Conforming to Complex Schemas with Self-Supervised Denoising"
                },
                "summary": "In this paper, we study the problem of generating structured objects that\nconform to a complex schema, with intricate dependencies between the different\ncomponents (facets) of the object. The facets of the object (attributes,\nfields, columns, properties) can be a mix of short, structured,\ntype-constrained facts, or long natural-language descriptions. The object has\nto be self-consistent between the different facets in the redundant information\nit carries (relative consistency), while being grounded with respect to world\nknowledge (absolute consistency). We frame the problem as a Language Modeling\nproblem (Structured Object Language Modeling) and train an LLM to perform the\ntask natively, without requiring instructions or prompt-engineering. We propose\na self-supervised denoising method to train the model from an existing dataset\nof such objects. The input query can be the existing object itself, in which\ncase the model acts as a regenerator, completing, correcting, normalizing the\ninput, or any unstructured blurb to be structured. We show that the\nself-supervised denoising training provides a strong baseline, and that\nadditional supervised fine-tuning with small amount of human demonstrations\nleads to further improvement. Experimental results show that the proposed\nmethod matches or outperforms prompt-engineered general-purpose\nstate-of-the-art LLMs (Claude 3, Mixtral-8x7B), while being order-of-magnitude\nmore cost-efficient.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we study the problem of generating structured objects that\nconform to a complex schema, with intricate dependencies between the different\ncomponents (facets) of the object. The facets of the object (attributes,\nfields, columns, properties) can be a mix of short, structured,\ntype-constrained facts, or long natural-language descriptions. The object has\nto be self-consistent between the different facets in the redundant information\nit carries (relative consistency), while being grounded with respect to world\nknowledge (absolute consistency). We frame the problem as a Language Modeling\nproblem (Structured Object Language Modeling) and train an LLM to perform the\ntask natively, without requiring instructions or prompt-engineering. We propose\na self-supervised denoising method to train the model from an existing dataset\nof such objects. The input query can be the existing object itself, in which\ncase the model acts as a regenerator, completing, correcting, normalizing the\ninput, or any unstructured blurb to be structured. We show that the\nself-supervised denoising training provides a strong baseline, and that\nadditional supervised fine-tuning with small amount of human demonstrations\nleads to further improvement. Experimental results show that the proposed\nmethod matches or outperforms prompt-engineered general-purpose\nstate-of-the-art LLMs (Claude 3, Mixtral-8x7B), while being order-of-magnitude\nmore cost-efficient."
                },
                "authors": [
                    {
                        "name": "Amir Tavanaei"
                    },
                    {
                        "name": "Kee Kiat Koo"
                    },
                    {
                        "name": "Hayreddin Ceker"
                    },
                    {
                        "name": "Shaobai Jiang"
                    },
                    {
                        "name": "Qi Li"
                    },
                    {
                        "name": "Julien Han"
                    },
                    {
                        "name": "Karim Bouyarmane"
                    }
                ],
                "author_detail": {
                    "name": "Karim Bouyarmane"
                },
                "author": "Karim Bouyarmane",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19301v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19301v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19275v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19275v1",
                "updated": "2024-11-28T17:12:21Z",
                "updated_parsed": [
                    2024,
                    11,
                    28,
                    17,
                    12,
                    21,
                    3,
                    333,
                    0
                ],
                "published": "2024-11-28T17:12:21Z",
                "published_parsed": [
                    2024,
                    11,
                    28,
                    17,
                    12,
                    21,
                    3,
                    333,
                    0
                ],
                "title": "VeCoGen: Automating Generation of Formally Verified C Code with Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VeCoGen: Automating Generation of Formally Verified C Code with Large\n  Language Models"
                },
                "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities in\ngenerating code, yet they often produce programs with flaws or deviations from\nintended behavior, limiting their suitability for safety-critical applications.\nTo address this limitation, this paper introduces VeCoGen, a novel tool that\ncombines LLMs with formal verification to automate the generation of formally\nverified C programs. VeCoGen takes a formal specification in ANSI/ISO C\nSpecification Language (ACSL), a natural language specification, and a set of\ntest cases to attempt to generate a program. This program-generation process\nconsists of two steps. First, VeCoGen generates an initial set of candidate\nprograms. Secondly, the tool iteratively improves on previously generated\ncandidates. If a candidate program meets the formal specification, then we are\nsure the program is correct. We evaluate VeCoGen on 15 problems presented in\nCodeforces competitions. On these problems, VeCoGen solves 13 problems. This\nwork shows the potential of combining LLMs with formal verification to automate\nprogram generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated impressive capabilities in\ngenerating code, yet they often produce programs with flaws or deviations from\nintended behavior, limiting their suitability for safety-critical applications.\nTo address this limitation, this paper introduces VeCoGen, a novel tool that\ncombines LLMs with formal verification to automate the generation of formally\nverified C programs. VeCoGen takes a formal specification in ANSI/ISO C\nSpecification Language (ACSL), a natural language specification, and a set of\ntest cases to attempt to generate a program. This program-generation process\nconsists of two steps. First, VeCoGen generates an initial set of candidate\nprograms. Secondly, the tool iteratively improves on previously generated\ncandidates. If a candidate program meets the formal specification, then we are\nsure the program is correct. We evaluate VeCoGen on 15 problems presented in\nCodeforces competitions. On these problems, VeCoGen solves 13 problems. This\nwork shows the potential of combining LLMs with formal verification to automate\nprogram generation."
                },
                "authors": [
                    {
                        "name": "Merlijn Sevenhuijsen"
                    },
                    {
                        "name": "Khashayar Etemadi"
                    },
                    {
                        "name": "Mattias Nyberg"
                    }
                ],
                "author_detail": {
                    "name": "Mattias Nyberg"
                },
                "author": "Mattias Nyberg",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19275v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19275v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19240v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19240v1",
                "updated": "2024-11-28T16:20:25Z",
                "updated_parsed": [
                    2024,
                    11,
                    28,
                    16,
                    20,
                    25,
                    3,
                    333,
                    0
                ],
                "published": "2024-11-28T16:20:25Z",
                "published_parsed": [
                    2024,
                    11,
                    28,
                    16,
                    20,
                    25,
                    3,
                    333,
                    0
                ],
                "title": "How far can bias go? -- Tracing bias from pretraining data to alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How far can bias go? -- Tracing bias from pretraining data to alignment"
                },
                "summary": "As LLMs are increasingly integrated into user-facing applications, addressing\nbiases that perpetuate societal inequalities is crucial. While much work has\ngone into measuring or mitigating biases in these models, fewer studies have\ninvestigated their origins. Therefore, this study examines the correlation\nbetween gender-occupation bias in pre-training data and their manifestation in\nLLMs, focusing on the Dolma dataset and the OLMo model. Using zero-shot\nprompting and token co-occurrence analyses, we explore how biases in training\ndata influence model outputs. Our findings reveal that biases present in\npre-training data are amplified in model outputs. The study also examines the\neffects of prompt types, hyperparameters, and instruction-tuning on bias\nexpression, finding instruction-tuning partially alleviating representational\nbias while still maintaining overall stereotypical gender associations, whereas\nhyperparameters and prompting variation have a lesser effect on bias\nexpression. Our research traces bias throughout the LLM development pipeline\nand underscores the importance of mitigating bias at the pretraining stage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As LLMs are increasingly integrated into user-facing applications, addressing\nbiases that perpetuate societal inequalities is crucial. While much work has\ngone into measuring or mitigating biases in these models, fewer studies have\ninvestigated their origins. Therefore, this study examines the correlation\nbetween gender-occupation bias in pre-training data and their manifestation in\nLLMs, focusing on the Dolma dataset and the OLMo model. Using zero-shot\nprompting and token co-occurrence analyses, we explore how biases in training\ndata influence model outputs. Our findings reveal that biases present in\npre-training data are amplified in model outputs. The study also examines the\neffects of prompt types, hyperparameters, and instruction-tuning on bias\nexpression, finding instruction-tuning partially alleviating representational\nbias while still maintaining overall stereotypical gender associations, whereas\nhyperparameters and prompting variation have a lesser effect on bias\nexpression. Our research traces bias throughout the LLM development pipeline\nand underscores the importance of mitigating bias at the pretraining stage."
                },
                "authors": [
                    {
                        "name": "Marion Thaler"
                    },
                    {
                        "name": "Abdullatif KÃ¶ksal"
                    },
                    {
                        "name": "Alina Leidinger"
                    },
                    {
                        "name": "Anna Korhonen"
                    },
                    {
                        "name": "Hinrich SchÃ¼tze"
                    }
                ],
                "author_detail": {
                    "name": "Hinrich SchÃ¼tze"
                },
                "author": "Hinrich SchÃ¼tze",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19240v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19240v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19234v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19234v1",
                "updated": "2024-11-28T16:02:01Z",
                "updated_parsed": [
                    2024,
                    11,
                    28,
                    16,
                    2,
                    1,
                    3,
                    333,
                    0
                ],
                "published": "2024-11-28T16:02:01Z",
                "published_parsed": [
                    2024,
                    11,
                    28,
                    16,
                    2,
                    1,
                    3,
                    333,
                    0
                ],
                "title": "SmartLLMSentry: A Comprehensive LLM Based Smart Contract Vulnerability\n  Detection Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SmartLLMSentry: A Comprehensive LLM Based Smart Contract Vulnerability\n  Detection Framework"
                },
                "summary": "Smart contracts are essential for managing digital assets in blockchain\nnetworks, highlighting the need for effective security measures. This paper\nintroduces SmartLLMSentry, a novel framework that leverages large language\nmodels (LLMs), specifically ChatGPT with in-context training, to advance smart\ncontract vulnerability detection. Traditional rule-based frameworks have\nlimitations in integrating new detection rules efficiently. In contrast,\nSmartLLMSentry utilizes LLMs to streamline this process. We created a\nspecialized dataset of five randomly selected vulnerabilities for model\ntraining and evaluation. Our results show an exact match accuracy of 91.1% with\nsufficient data, although GPT-4 demonstrated reduced performance compared to\nGPT-3 in rule generation. This study illustrates that SmartLLMSentry\nsignificantly enhances the speed and accuracy of vulnerability detection\nthrough LLMdriven rule integration, offering a new approach to improving\nBlockchain security and addressing previously underexplored vulnerabilities in\nsmart contracts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Smart contracts are essential for managing digital assets in blockchain\nnetworks, highlighting the need for effective security measures. This paper\nintroduces SmartLLMSentry, a novel framework that leverages large language\nmodels (LLMs), specifically ChatGPT with in-context training, to advance smart\ncontract vulnerability detection. Traditional rule-based frameworks have\nlimitations in integrating new detection rules efficiently. In contrast,\nSmartLLMSentry utilizes LLMs to streamline this process. We created a\nspecialized dataset of five randomly selected vulnerabilities for model\ntraining and evaluation. Our results show an exact match accuracy of 91.1% with\nsufficient data, although GPT-4 demonstrated reduced performance compared to\nGPT-3 in rule generation. This study illustrates that SmartLLMSentry\nsignificantly enhances the speed and accuracy of vulnerability detection\nthrough LLMdriven rule integration, offering a new approach to improving\nBlockchain security and addressing previously underexplored vulnerabilities in\nsmart contracts."
                },
                "authors": [
                    {
                        "name": "Oualid Zaazaa"
                    },
                    {
                        "name": "Hanan El Bakkali"
                    }
                ],
                "author_detail": {
                    "name": "Hanan El Bakkali"
                },
                "author": "Hanan El Bakkali",
                "arxiv_doi": "10.57019/jmv.1489060",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.57019/jmv.1489060",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2411.19234v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19234v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Journal of Metaverse, Year 2024 Volume: 4 Issue: 2",
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17820v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17820v2",
                "updated": "2024-11-28T15:49:22Z",
                "updated_parsed": [
                    2024,
                    11,
                    28,
                    15,
                    49,
                    22,
                    3,
                    333,
                    0
                ],
                "published": "2024-11-26T19:02:20Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    19,
                    2,
                    20,
                    1,
                    331,
                    0
                ],
                "title": "CityWalker: Learning Embodied Urban Navigation from Web-Scale Videos",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CityWalker: Learning Embodied Urban Navigation from Web-Scale Videos"
                },
                "summary": "Navigating dynamic urban environments presents significant challenges for\nembodied agents, requiring advanced spatial reasoning and adherence to\ncommon-sense norms. Despite progress, existing visual navigation methods\nstruggle in map-free or off-street settings, limiting the deployment of\nautonomous agents like last-mile delivery robots. To overcome these obstacles,\nwe propose a scalable, data-driven approach for human-like urban navigation by\ntraining agents on thousands of hours of in-the-wild city walking and driving\nvideos sourced from the web. We introduce a simple and scalable data processing\npipeline that extracts action supervision from these videos, enabling\nlarge-scale imitation learning without costly annotations. Our model learns\nsophisticated navigation policies to handle diverse challenges and critical\nscenarios. Experimental results show that training on large-scale, diverse\ndatasets significantly enhances navigation performance, surpassing current\nmethods. This work shows the potential of using abundant online video data to\ndevelop robust navigation policies for embodied agents in dynamic urban\nsettings. Project homepage is at https://ai4ce.github.io/CityWalker/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Navigating dynamic urban environments presents significant challenges for\nembodied agents, requiring advanced spatial reasoning and adherence to\ncommon-sense norms. Despite progress, existing visual navigation methods\nstruggle in map-free or off-street settings, limiting the deployment of\nautonomous agents like last-mile delivery robots. To overcome these obstacles,\nwe propose a scalable, data-driven approach for human-like urban navigation by\ntraining agents on thousands of hours of in-the-wild city walking and driving\nvideos sourced from the web. We introduce a simple and scalable data processing\npipeline that extracts action supervision from these videos, enabling\nlarge-scale imitation learning without costly annotations. Our model learns\nsophisticated navigation policies to handle diverse challenges and critical\nscenarios. Experimental results show that training on large-scale, diverse\ndatasets significantly enhances navigation performance, surpassing current\nmethods. This work shows the potential of using abundant online video data to\ndevelop robust navigation policies for embodied agents in dynamic urban\nsettings. Project homepage is at https://ai4ce.github.io/CityWalker/."
                },
                "authors": [
                    {
                        "name": "Xinhao Liu"
                    },
                    {
                        "name": "Jintong Li"
                    },
                    {
                        "name": "Yicheng Jiang"
                    },
                    {
                        "name": "Niranjan Sujay"
                    },
                    {
                        "name": "Zhicheng Yang"
                    },
                    {
                        "name": "Juexiao Zhang"
                    },
                    {
                        "name": "John Abanes"
                    },
                    {
                        "name": "Jing Zhang"
                    },
                    {
                        "name": "Chen Feng"
                    }
                ],
                "author_detail": {
                    "name": "Chen Feng"
                },
                "author": "Chen Feng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17820v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17820v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19203v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19203v1",
                "updated": "2024-11-28T15:23:12Z",
                "updated_parsed": [
                    2024,
                    11,
                    28,
                    15,
                    23,
                    12,
                    3,
                    333,
                    0
                ],
                "published": "2024-11-28T15:23:12Z",
                "published_parsed": [
                    2024,
                    11,
                    28,
                    15,
                    23,
                    12,
                    3,
                    333,
                    0
                ],
                "title": "An Extensive Evaluation of Factual Consistency in Large Language Models\n  for Data-to-Text Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Extensive Evaluation of Factual Consistency in Large Language Models\n  for Data-to-Text Generation"
                },
                "summary": "Large Language Models (LLMs) have shown exceptional performance across\nvarious Data-to-Text Generation (DTG) tasks. However, generating factually\nconsistent text in DTG remains challenging for LLMs. Despite this, in-depth\nevaluations of LLM factual consistency for DTG remain missing in the current\nliterature. This paper addresses this gap by providing an extensive evaluation\nof factual consistency in LLMs for DTG. Our evaluation covers five widely used\nDTG datasets (E2E, ViGGo, WikiTableText, DART, and WebNLG) and five prominent\nLLM families (T5, BART, OPT, BLOOM, and Llama 2). To ensure a thorough\nevaluation of factual consistency, we use four state-of-the-art automatic\nmetrics and include essential human assessments. Our extensive evaluations\nreveals three key findings regarding factual consistency in LLMs for DTG.\nFirst, Llama 2 often excels in generating factually consistent text, although\nsmaller models like T5 and BART can achieve strong factual consistency on\nlarger, lexically less-diverse datasets. Second, the average rate of change\n(AROC) indicates that increasing model size (number of model trainable\nparameters) generally enhances factual consistency of LLMs in DTG. Third, we\nobserve that source-reference divergence (i.e., when the reference text\ndiverges semantically from the source) typically reduces the factual\nconsistency of LLMs in DTG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown exceptional performance across\nvarious Data-to-Text Generation (DTG) tasks. However, generating factually\nconsistent text in DTG remains challenging for LLMs. Despite this, in-depth\nevaluations of LLM factual consistency for DTG remain missing in the current\nliterature. This paper addresses this gap by providing an extensive evaluation\nof factual consistency in LLMs for DTG. Our evaluation covers five widely used\nDTG datasets (E2E, ViGGo, WikiTableText, DART, and WebNLG) and five prominent\nLLM families (T5, BART, OPT, BLOOM, and Llama 2). To ensure a thorough\nevaluation of factual consistency, we use four state-of-the-art automatic\nmetrics and include essential human assessments. Our extensive evaluations\nreveals three key findings regarding factual consistency in LLMs for DTG.\nFirst, Llama 2 often excels in generating factually consistent text, although\nsmaller models like T5 and BART can achieve strong factual consistency on\nlarger, lexically less-diverse datasets. Second, the average rate of change\n(AROC) indicates that increasing model size (number of model trainable\nparameters) generally enhances factual consistency of LLMs in DTG. Third, we\nobserve that source-reference divergence (i.e., when the reference text\ndiverges semantically from the source) typically reduces the factual\nconsistency of LLMs in DTG."
                },
                "authors": [
                    {
                        "name": "Joy Mahapatra"
                    },
                    {
                        "name": "Utpal Garain"
                    }
                ],
                "author_detail": {
                    "name": "Utpal Garain"
                },
                "author": "Utpal Garain",
                "arxiv_comment": "15 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19203v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19203v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.16236v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.16236v2",
                "updated": "2024-11-28T14:58:34Z",
                "updated_parsed": [
                    2024,
                    11,
                    28,
                    14,
                    58,
                    34,
                    3,
                    333,
                    0
                ],
                "published": "2024-05-25T13:54:05Z",
                "published_parsed": [
                    2024,
                    5,
                    25,
                    13,
                    54,
                    5,
                    5,
                    146,
                    0
                ],
                "title": "A transfer learning framework for weak-to-strong generalization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A transfer learning framework for weak-to-strong generalization"
                },
                "summary": "Modern large language model (LLM) alignment techniques rely on human\nfeedback, but it is unclear whether these techniques fundamentally limit the\ncapabilities of aligned LLMs. In particular, it is unknown if it is possible to\nalign (stronger) LLMs with superhuman capabilities with (weaker) human feedback\nwithout degrading their capabilities. This is an instance of the weak-to-strong\ngeneralization problem: using feedback from a weaker (less capable) model to\ntrain a stronger (more capable) model. We prove that weak-to-strong\ngeneralization is possible by eliciting latent knowledge from pre-trained LLMs.\nIn particular, we cast the weak-to-strong generalization problem as a transfer\nlearning problem in which we wish to transfer a latent concept prior from a\nweak model to a strong pre-trained model. We prove that a naive fine-tuning\napproach suffers from fundamental limitations, but an alternative\nrefinement-based approach suggested by the problem structure provably overcomes\nthe limitations of fine-tuning. Finally, we demonstrate the practical\napplicability of the refinement approach in multiple LLM alignment tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern large language model (LLM) alignment techniques rely on human\nfeedback, but it is unclear whether these techniques fundamentally limit the\ncapabilities of aligned LLMs. In particular, it is unknown if it is possible to\nalign (stronger) LLMs with superhuman capabilities with (weaker) human feedback\nwithout degrading their capabilities. This is an instance of the weak-to-strong\ngeneralization problem: using feedback from a weaker (less capable) model to\ntrain a stronger (more capable) model. We prove that weak-to-strong\ngeneralization is possible by eliciting latent knowledge from pre-trained LLMs.\nIn particular, we cast the weak-to-strong generalization problem as a transfer\nlearning problem in which we wish to transfer a latent concept prior from a\nweak model to a strong pre-trained model. We prove that a naive fine-tuning\napproach suffers from fundamental limitations, but an alternative\nrefinement-based approach suggested by the problem structure provably overcomes\nthe limitations of fine-tuning. Finally, we demonstrate the practical\napplicability of the refinement approach in multiple LLM alignment tasks."
                },
                "authors": [
                    {
                        "name": "Seamus Somerstep"
                    },
                    {
                        "name": "Felipe Maia Polo"
                    },
                    {
                        "name": "Moulinath Banerjee"
                    },
                    {
                        "name": "Ya'acov Ritov"
                    },
                    {
                        "name": "Mikhail Yurochkin"
                    },
                    {
                        "name": "Yuekai Sun"
                    }
                ],
                "author_detail": {
                    "name": "Yuekai Sun"
                },
                "author": "Yuekai Sun",
                "arxiv_comment": "v2: Major changes to set up, theory, and experiments",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.16236v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.16236v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19187v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19187v1",
                "updated": "2024-11-28T14:47:55Z",
                "updated_parsed": [
                    2024,
                    11,
                    28,
                    14,
                    47,
                    55,
                    3,
                    333,
                    0
                ],
                "published": "2024-11-28T14:47:55Z",
                "published_parsed": [
                    2024,
                    11,
                    28,
                    14,
                    47,
                    55,
                    3,
                    333,
                    0
                ],
                "title": "Beyond Logit Lens: Contextual Embeddings for Robust Hallucination\n  Detection & Grounding in VLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Logit Lens: Contextual Embeddings for Robust Hallucination\n  Detection & Grounding in VLMs"
                },
                "summary": "The rapid development of Large Multimodal Models (LMMs) has significantly\nadvanced multimodal understanding by harnessing the language abilities of Large\nLanguage Models (LLMs) and integrating modality-specific encoders. However,\nLMMs are plagued by hallucinations that limit their reliability and adoption.\nWhile traditional methods to detect and mitigate these hallucinations often\ninvolve costly training or rely heavily on external models, recent approaches\nutilizing internal model features present a promising alternative. In this\npaper, we critically assess the limitations of the state-of-the-art\ntraining-free technique, the logit lens, in handling generalized visual\nhallucinations. We introduce a refined method that leverages contextual token\nembeddings from middle layers of LMMs. This approach significantly improves\nhallucination detection and grounding across diverse categories, including\nactions and OCR, while also excelling in tasks requiring contextual\nunderstanding, such as spatial relations and attribute comparison. Our novel\ngrounding technique yields highly precise bounding boxes, facilitating a\ntransition from Zero-Shot Object Segmentation to Grounded Visual Question\nAnswering. Our contributions pave the way for more reliable and interpretable\nmultimodal models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development of Large Multimodal Models (LMMs) has significantly\nadvanced multimodal understanding by harnessing the language abilities of Large\nLanguage Models (LLMs) and integrating modality-specific encoders. However,\nLMMs are plagued by hallucinations that limit their reliability and adoption.\nWhile traditional methods to detect and mitigate these hallucinations often\ninvolve costly training or rely heavily on external models, recent approaches\nutilizing internal model features present a promising alternative. In this\npaper, we critically assess the limitations of the state-of-the-art\ntraining-free technique, the logit lens, in handling generalized visual\nhallucinations. We introduce a refined method that leverages contextual token\nembeddings from middle layers of LMMs. This approach significantly improves\nhallucination detection and grounding across diverse categories, including\nactions and OCR, while also excelling in tasks requiring contextual\nunderstanding, such as spatial relations and attribute comparison. Our novel\ngrounding technique yields highly precise bounding boxes, facilitating a\ntransition from Zero-Shot Object Segmentation to Grounded Visual Question\nAnswering. Our contributions pave the way for more reliable and interpretable\nmultimodal models."
                },
                "authors": [
                    {
                        "name": "Anirudh Phukan"
                    },
                    {
                        "name": "Divyansh"
                    },
                    {
                        "name": "Harshit Kumar Morj"
                    },
                    {
                        "name": "Vaishnavi"
                    },
                    {
                        "name": "Apoorv Saxena"
                    },
                    {
                        "name": "Koustava Goswami"
                    }
                ],
                "author_detail": {
                    "name": "Koustava Goswami"
                },
                "author": "Koustava Goswami",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19187v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19187v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.04885v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.04885v2",
                "updated": "2024-11-28T14:30:24Z",
                "updated_parsed": [
                    2024,
                    11,
                    28,
                    14,
                    30,
                    24,
                    3,
                    333,
                    0
                ],
                "published": "2024-04-07T09:05:09Z",
                "published_parsed": [
                    2024,
                    4,
                    7,
                    9,
                    5,
                    9,
                    6,
                    98,
                    0
                ],
                "title": "TimeGPT in Load Forecasting: A Large Time Series Model Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TimeGPT in Load Forecasting: A Large Time Series Model Perspective"
                },
                "summary": "Machine learning models have made significant progress in load forecasting,\nbut their forecast accuracy is limited in cases where historical load data is\nscarce. Inspired by the outstanding performance of large language models (LLMs)\nin computer vision and natural language processing, this paper aims to discuss\nthe potential of large time series models in load forecasting with scarce\nhistorical data. Specifically, the large time series model is constructed as a\ntime series generative pre-trained transformer (TimeGPT), which is trained on\nmassive and diverse time series datasets consisting of 100 billion data points\n(e.g., finance, transportation, banking, web traffic, weather, energy,\nhealthcare, etc.). Then, the scarce historical load data is used to fine-tune\nthe TimeGPT, which helps it to adapt to the data distribution and\ncharacteristics associated with load forecasting. Simulation results show that\nTimeGPT outperforms the benchmarks (e.g., popular machine learning models and\nstatistical models) for load forecasting on several real datasets with scarce\ntraining samples, particularly for short look-ahead times. However, it cannot\nbe guaranteed that TimeGPT is always superior to benchmarks for load\nforecasting with scarce data, since the performance of TimeGPT may be affected\nby the distribution differences between the load data and the training data. In\npractical applications, we can divide the historical data into a training set\nand a validation set, and then use the validation set loss to decide whether\nTimeGPT is the best choice for a specific dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning models have made significant progress in load forecasting,\nbut their forecast accuracy is limited in cases where historical load data is\nscarce. Inspired by the outstanding performance of large language models (LLMs)\nin computer vision and natural language processing, this paper aims to discuss\nthe potential of large time series models in load forecasting with scarce\nhistorical data. Specifically, the large time series model is constructed as a\ntime series generative pre-trained transformer (TimeGPT), which is trained on\nmassive and diverse time series datasets consisting of 100 billion data points\n(e.g., finance, transportation, banking, web traffic, weather, energy,\nhealthcare, etc.). Then, the scarce historical load data is used to fine-tune\nthe TimeGPT, which helps it to adapt to the data distribution and\ncharacteristics associated with load forecasting. Simulation results show that\nTimeGPT outperforms the benchmarks (e.g., popular machine learning models and\nstatistical models) for load forecasting on several real datasets with scarce\ntraining samples, particularly for short look-ahead times. However, it cannot\nbe guaranteed that TimeGPT is always superior to benchmarks for load\nforecasting with scarce data, since the performance of TimeGPT may be affected\nby the distribution differences between the load data and the training data. In\npractical applications, we can divide the historical data into a training set\nand a validation set, and then use the validation set loss to decide whether\nTimeGPT is the best choice for a specific dataset."
                },
                "authors": [
                    {
                        "name": "Wenlong Liao"
                    },
                    {
                        "name": "Fernando Porte-Agel"
                    },
                    {
                        "name": "Jiannong Fang"
                    },
                    {
                        "name": "Christian Rehtanz"
                    },
                    {
                        "name": "Shouxiang Wang"
                    },
                    {
                        "name": "Dechang Yang"
                    },
                    {
                        "name": "Zhe Yang"
                    }
                ],
                "author_detail": {
                    "name": "Zhe Yang"
                },
                "author": "Zhe Yang",
                "arxiv_doi": "10.1016/j.apenergy.2024.124973",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.apenergy.2024.124973",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2404.04885v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.04885v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "10 pages. It was published in Applied Energy",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]