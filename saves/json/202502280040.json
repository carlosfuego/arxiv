[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2408.15766v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15766v3",
                "updated": "2025-02-26T11:47:58Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    11,
                    47,
                    58,
                    2,
                    57,
                    0
                ],
                "published": "2024-08-28T12:59:12Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    12,
                    59,
                    12,
                    2,
                    241,
                    0
                ],
                "title": "Learning Harmonized Representations for Speculative Sampling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Harmonized Representations for Speculative Sampling"
                },
                "summary": "Speculative sampling is a promising approach to accelerate the decoding stage\nfor Large Language Models (LLMs). Recent advancements that leverage target\nLLM's contextual information, such as hidden states and KV cache, have shown\nsignificant practical improvements. However, these approaches suffer from\ninconsistent context between training and decoding. We also observe another\ndiscrepancy between the training and decoding objectives in existing\nspeculative sampling methods. In this work, we propose a solution named\nHArmonized Speculative Sampling (HASS) that learns harmonized representations\nto address these issues. HASS accelerates the decoding stage without adding\ninference overhead through harmonized objective distillation and harmonized\ncontext alignment. Experiments on four LLaMA models demonstrate that HASS\nachieves 2.81x-4.05x wall-clock time speedup ratio averaging across three\ndatasets, surpassing EAGLE-2 by 8%-20%. The code is available at\nhttps://github.com/HArmonizedSS/HASS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative sampling is a promising approach to accelerate the decoding stage\nfor Large Language Models (LLMs). Recent advancements that leverage target\nLLM's contextual information, such as hidden states and KV cache, have shown\nsignificant practical improvements. However, these approaches suffer from\ninconsistent context between training and decoding. We also observe another\ndiscrepancy between the training and decoding objectives in existing\nspeculative sampling methods. In this work, we propose a solution named\nHArmonized Speculative Sampling (HASS) that learns harmonized representations\nto address these issues. HASS accelerates the decoding stage without adding\ninference overhead through harmonized objective distillation and harmonized\ncontext alignment. Experiments on four LLaMA models demonstrate that HASS\nachieves 2.81x-4.05x wall-clock time speedup ratio averaging across three\ndatasets, surpassing EAGLE-2 by 8%-20%. The code is available at\nhttps://github.com/HArmonizedSS/HASS."
                },
                "authors": [
                    {
                        "name": "Lefan Zhang"
                    },
                    {
                        "name": "Xiaodan Wang"
                    },
                    {
                        "name": "Yanhua Huang"
                    },
                    {
                        "name": "Ruiwen Xu"
                    }
                ],
                "author_detail": {
                    "name": "Ruiwen Xu"
                },
                "author": "Ruiwen Xu",
                "arxiv_comment": "Published as a conference paper at ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15766v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15766v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.02747v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.02747v3",
                "updated": "2025-02-26T10:49:33Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    10,
                    49,
                    33,
                    2,
                    57,
                    0
                ],
                "published": "2024-04-03T13:44:41Z",
                "published_parsed": [
                    2024,
                    4,
                    3,
                    13,
                    44,
                    41,
                    2,
                    94,
                    0
                ],
                "title": "Faster Diffusion via Temporal Attention Decomposition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Faster Diffusion via Temporal Attention Decomposition"
                },
                "summary": "We explore the role of attention mechanism during inference in\ntext-conditional diffusion models. Empirical observations suggest that\ncross-attention outputs converge to a fixed point after several inference\nsteps. The convergence time naturally divides the entire inference process into\ntwo phases: an initial phase for planning text-oriented visual semantics, which\nare then translated into images in a subsequent fidelity-improving phase.\nCross-attention is essential in the initial phase but almost irrelevant\nthereafter. However, self-attention initially plays a minor role but becomes\ncrucial in the second phase. These findings yield a simple and training-free\nmethod known as temporally gating the attention (TGATE), which efficiently\ngenerates images by caching and reusing attention outputs at scheduled time\nsteps. Experimental results show when widely applied to various existing\ntext-conditional diffusion models, TGATE accelerates these models by 10%-50%.\nThe code of TGATE is available at https://github.com/HaozheLiu-ST/T-GATE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We explore the role of attention mechanism during inference in\ntext-conditional diffusion models. Empirical observations suggest that\ncross-attention outputs converge to a fixed point after several inference\nsteps. The convergence time naturally divides the entire inference process into\ntwo phases: an initial phase for planning text-oriented visual semantics, which\nare then translated into images in a subsequent fidelity-improving phase.\nCross-attention is essential in the initial phase but almost irrelevant\nthereafter. However, self-attention initially plays a minor role but becomes\ncrucial in the second phase. These findings yield a simple and training-free\nmethod known as temporally gating the attention (TGATE), which efficiently\ngenerates images by caching and reusing attention outputs at scheduled time\nsteps. Experimental results show when widely applied to various existing\ntext-conditional diffusion models, TGATE accelerates these models by 10%-50%.\nThe code of TGATE is available at https://github.com/HaozheLiu-ST/T-GATE."
                },
                "authors": [
                    {
                        "name": "Haozhe Liu"
                    },
                    {
                        "name": "Wentian Zhang"
                    },
                    {
                        "name": "Jinheng Xie"
                    },
                    {
                        "name": "Francesco Faccio"
                    },
                    {
                        "name": "Mengmeng Xu"
                    },
                    {
                        "name": "Tao Xiang"
                    },
                    {
                        "name": "Mike Zheng Shou"
                    },
                    {
                        "name": "Juan-Manuel Perez-Rua"
                    },
                    {
                        "name": "Jürgen Schmidhuber"
                    }
                ],
                "author_detail": {
                    "name": "Jürgen Schmidhuber"
                },
                "author": "Jürgen Schmidhuber",
                "arxiv_comment": "Accepted by TMLR: https://openreview.net/forum?id=xXs2GKXPnH",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.02747v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.02747v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18890v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18890v1",
                "updated": "2025-02-26T07:10:08Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    7,
                    10,
                    8,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-26T07:10:08Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    7,
                    10,
                    8,
                    2,
                    57,
                    0
                ],
                "title": "From Hours to Minutes: Lossless Acceleration of Ultra Long Sequence\n  Generation up to 100K Tokens",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Hours to Minutes: Lossless Acceleration of Ultra Long Sequence\n  Generation up to 100K Tokens"
                },
                "summary": "Generating ultra-long sequences with large language models (LLMs) has become\nincreasingly crucial but remains a highly time-intensive task, particularly for\nsequences up to 100K tokens. While traditional speculative decoding methods\nexist, simply extending their generation limits fails to accelerate the process\nand can be detrimental. Through an in-depth analysis, we identify three major\nchallenges hindering efficient generation: frequent model reloading, dynamic\nkey-value (KV) management and repetitive generation. To address these issues,\nwe introduce TOKENSWIFT, a novel framework designed to substantially accelerate\nthe generation process of ultra-long sequences while maintaining the target\nmodel's inherent quality. Experimental results demonstrate that TOKENSWIFT\nachieves over 3 times speedup across models of varying scales (1.5B, 7B, 8B,\n14B) and architectures (MHA, GQA). This acceleration translates to hours of\ntime savings for ultra-long sequence generation, establishing TOKENSWIFT as a\nscalable and effective solution at unprecedented lengths. Code can be found at\nhttps://github.com/bigai-nlco/TokenSwift.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating ultra-long sequences with large language models (LLMs) has become\nincreasingly crucial but remains a highly time-intensive task, particularly for\nsequences up to 100K tokens. While traditional speculative decoding methods\nexist, simply extending their generation limits fails to accelerate the process\nand can be detrimental. Through an in-depth analysis, we identify three major\nchallenges hindering efficient generation: frequent model reloading, dynamic\nkey-value (KV) management and repetitive generation. To address these issues,\nwe introduce TOKENSWIFT, a novel framework designed to substantially accelerate\nthe generation process of ultra-long sequences while maintaining the target\nmodel's inherent quality. Experimental results demonstrate that TOKENSWIFT\nachieves over 3 times speedup across models of varying scales (1.5B, 7B, 8B,\n14B) and architectures (MHA, GQA). This acceleration translates to hours of\ntime savings for ultra-long sequence generation, establishing TOKENSWIFT as a\nscalable and effective solution at unprecedented lengths. Code can be found at\nhttps://github.com/bigai-nlco/TokenSwift."
                },
                "authors": [
                    {
                        "name": "Tong Wu"
                    },
                    {
                        "name": "Junzhe Shen"
                    },
                    {
                        "name": "Zixia Jia"
                    },
                    {
                        "name": "Yuxuan Wang"
                    },
                    {
                        "name": "Zilong Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zilong Zheng"
                },
                "author": "Zilong Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18890v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18890v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04077v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04077v2",
                "updated": "2025-02-26T02:48:22Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    2,
                    48,
                    22,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-06T13:41:46Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    13,
                    41,
                    46,
                    3,
                    37,
                    0
                ],
                "title": "AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference"
                },
                "summary": "With the development of large language models (LLMs), efficient inference\nthrough Key-Value (KV) cache compression has attracted considerable attention,\nespecially for long-context generation. To compress the KV cache, recent\nmethods identify critical KV tokens through heuristic ranking with attention\nscores. However, these methods often struggle to accurately determine critical\ntokens as they neglect the \\textit{temporal patterns} in attention scores,\nresulting in a noticeable degradation in LLM performance. To address this\nchallenge, we propose AttentionPredictor, which is the first learning-based\ncritical token identification approach. Specifically, AttentionPredictor learns\na lightweight convolution model to capture spatiotemporal patterns and predict\nthe next-token attention score. An appealing feature of AttentionPredictor is\nthat it accurately predicts the attention score while consuming negligible\nmemory. Moreover, we propose a cross-token critical cache prefetching framework\nthat hides the token estimation time overhead to accelerate the decoding stage.\nBy retaining most of the attention information, AttentionPredictor achieves\n16$\\times$ KV cache compression with comparable LLM performance, significantly\noutperforming the state-of-the-art.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the development of large language models (LLMs), efficient inference\nthrough Key-Value (KV) cache compression has attracted considerable attention,\nespecially for long-context generation. To compress the KV cache, recent\nmethods identify critical KV tokens through heuristic ranking with attention\nscores. However, these methods often struggle to accurately determine critical\ntokens as they neglect the \\textit{temporal patterns} in attention scores,\nresulting in a noticeable degradation in LLM performance. To address this\nchallenge, we propose AttentionPredictor, which is the first learning-based\ncritical token identification approach. Specifically, AttentionPredictor learns\na lightweight convolution model to capture spatiotemporal patterns and predict\nthe next-token attention score. An appealing feature of AttentionPredictor is\nthat it accurately predicts the attention score while consuming negligible\nmemory. Moreover, we propose a cross-token critical cache prefetching framework\nthat hides the token estimation time overhead to accelerate the decoding stage.\nBy retaining most of the attention information, AttentionPredictor achieves\n16$\\times$ KV cache compression with comparable LLM performance, significantly\noutperforming the state-of-the-art."
                },
                "authors": [
                    {
                        "name": "Qingyue Yang"
                    },
                    {
                        "name": "Jie Wang"
                    },
                    {
                        "name": "Xing Li"
                    },
                    {
                        "name": "Zhihai Wang"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Lei Chen"
                    },
                    {
                        "name": "Xianzhi Yu"
                    },
                    {
                        "name": "Wulong Liu"
                    },
                    {
                        "name": "Jianye Hao"
                    },
                    {
                        "name": "Mingxuan Yuan"
                    },
                    {
                        "name": "Bin Li"
                    }
                ],
                "author_detail": {
                    "name": "Bin Li"
                },
                "author": "Bin Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04077v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04077v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18755v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18755v1",
                "updated": "2025-02-26T02:16:46Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    2,
                    16,
                    46,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-26T02:16:46Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    2,
                    16,
                    46,
                    2,
                    57,
                    0
                ],
                "title": "M-ANT: Efficient Low-bit Group Quantization for LLMs via Mathematically\n  Adaptive Numerical Type",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "M-ANT: Efficient Low-bit Group Quantization for LLMs via Mathematically\n  Adaptive Numerical Type"
                },
                "summary": "Large language models (LLMs) are one of the most important killer computer\napplications. The recent algorithmic advancement proposes a fine-grained\ngroup-wise quantization for LLMs, which treats a small set (e.g., 64) of values\nin a tensor as a compression unit. It effectively preserves the model accuracy\nwithout retraining, and has become the standard approach to efficiently deploy\nLLMs. On the other hand, there are works that propose various adaptive data\ntypes to better adapt to different distributions and further reduce the\nrequired bit length for LLMs. In this work, our detailed analysis unveils a key\nfinding that while different tensors exhibit similar distributions, small\ngroups can have markedly different distributions. As such, the group-level\ndiversity requires a new level of adaptivity for which existing adaptive data\ntypes fail to provide.\n  In this paper, we propose MANT, a mathematically adaptive numeric type,\nfeaturing a more flexible encoding paradigm with a wider range of data\ndistribution and more efficient decodingcomputation fusion mechanism to address\nthese challenges. Based on MANT, we develop a supporting framework to assign\nthe appropriate data type for each group adaptively. Meanwhile, the dynamically\ngenerated Key-Value (KV) caches in LLMs introduce further complexity for\nreal-time quantization. To tackle this, we propose an efficient real-time\nquantization mechanism. Besides, we implement a specific processing element\n(PE) to efficiently support MANT and incorporate a real-time quantization unit.\nBy integrating these components into a systolic array, MANT unifies the\ngroup-wise weight and KV cache quantization and addresses the associated\nchallenges. Our evaluation shows achieving, on average, 2.99x (up to 4.46x)\nspeedup and 2.81x (up to 4.10x) energy reduction to the state-of-the-art LLM\naccelerator.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are one of the most important killer computer\napplications. The recent algorithmic advancement proposes a fine-grained\ngroup-wise quantization for LLMs, which treats a small set (e.g., 64) of values\nin a tensor as a compression unit. It effectively preserves the model accuracy\nwithout retraining, and has become the standard approach to efficiently deploy\nLLMs. On the other hand, there are works that propose various adaptive data\ntypes to better adapt to different distributions and further reduce the\nrequired bit length for LLMs. In this work, our detailed analysis unveils a key\nfinding that while different tensors exhibit similar distributions, small\ngroups can have markedly different distributions. As such, the group-level\ndiversity requires a new level of adaptivity for which existing adaptive data\ntypes fail to provide.\n  In this paper, we propose MANT, a mathematically adaptive numeric type,\nfeaturing a more flexible encoding paradigm with a wider range of data\ndistribution and more efficient decodingcomputation fusion mechanism to address\nthese challenges. Based on MANT, we develop a supporting framework to assign\nthe appropriate data type for each group adaptively. Meanwhile, the dynamically\ngenerated Key-Value (KV) caches in LLMs introduce further complexity for\nreal-time quantization. To tackle this, we propose an efficient real-time\nquantization mechanism. Besides, we implement a specific processing element\n(PE) to efficiently support MANT and incorporate a real-time quantization unit.\nBy integrating these components into a systolic array, MANT unifies the\ngroup-wise weight and KV cache quantization and addresses the associated\nchallenges. Our evaluation shows achieving, on average, 2.99x (up to 4.46x)\nspeedup and 2.81x (up to 4.10x) energy reduction to the state-of-the-art LLM\naccelerator."
                },
                "authors": [
                    {
                        "name": "Weiming Hu"
                    },
                    {
                        "name": "Haoyan Zhang"
                    },
                    {
                        "name": "Cong Guo"
                    },
                    {
                        "name": "Yu Feng"
                    },
                    {
                        "name": "Renyang Guan"
                    },
                    {
                        "name": "Zhendong Hua"
                    },
                    {
                        "name": "Zihan Liu"
                    },
                    {
                        "name": "Yue Guan"
                    },
                    {
                        "name": "Minyi Guo"
                    },
                    {
                        "name": "Jingwen Leng"
                    }
                ],
                "author_detail": {
                    "name": "Jingwen Leng"
                },
                "author": "Jingwen Leng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18755v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18755v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2203.02550v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2203.02550v3",
                "updated": "2025-02-25T13:03:44Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    13,
                    3,
                    44,
                    1,
                    56,
                    0
                ],
                "published": "2022-03-04T19:56:56Z",
                "published_parsed": [
                    2022,
                    3,
                    4,
                    19,
                    56,
                    56,
                    4,
                    63,
                    0
                ],
                "title": "AgileWatts: An Energy-Efficient CPU Core Idle-State Architecture for\n  Latency-Sensitive Server Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AgileWatts: An Energy-Efficient CPU Core Idle-State Architecture for\n  Latency-Sensitive Server Applications"
                },
                "summary": "User-facing applications running in modern datacenters exhibit irregular\nrequest patterns and are implemented using a multitude of services with tight\nlatency requirements. These characteristics render ineffective existing energy\nconserving techniques when processors are idle due to the long transition time\nfrom a deep idle power state (C-state). While prior works propose management\ntechniques to mitigate this inefficiency, we tackle it at its root with\nAgileWatts (AW): a new deep C-state architecture optimized for datacenter\nserver processors targeting latency-sensitive applications. AW is based on\nthree key ideas. First, AW eliminates the latency overhead of saving/restoring\nthe core context (i.e., micro-architectural state) when powering-off/-on the\ncore in a deep idle power state by i) implementing medium-grained power-gates,\ncarefully distributed across the CPU core, and ii) retaining context in the\npower-ungated domain. Second, AW eliminates the flush latency overhead (several\ntens of microseconds) of the L1/L2 caches when entering a deep idle power state\nby keeping L1/L2 cache content power-ungated. A minimal control logic also\nremains power-ungated to serve cache coherence traffic (i.e., snoops)\nseamlessly. AW implements sleep-mode in caches to reduce caches leakage power\nconsumption and lowers a core voltage to the minimum operational voltage level\nto minimize the leakage power of the power-ungated domain. Third, using a\nstate-of-the-art power efficient all-digital phase-locked loop (ADPLL) clock\ngenerator, AW keeps the PLL active and locked during the idle state, further\ncutting precious microseconds of wake-up latency at a negligible power cost.\nOur evaluation with an accurate simulator calibrated against an Intel Skylake\nserver shows that AW reduces the energy consumption of Memcached by up to 71%\n(35% on average) with up to 1% performance degradation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "User-facing applications running in modern datacenters exhibit irregular\nrequest patterns and are implemented using a multitude of services with tight\nlatency requirements. These characteristics render ineffective existing energy\nconserving techniques when processors are idle due to the long transition time\nfrom a deep idle power state (C-state). While prior works propose management\ntechniques to mitigate this inefficiency, we tackle it at its root with\nAgileWatts (AW): a new deep C-state architecture optimized for datacenter\nserver processors targeting latency-sensitive applications. AW is based on\nthree key ideas. First, AW eliminates the latency overhead of saving/restoring\nthe core context (i.e., micro-architectural state) when powering-off/-on the\ncore in a deep idle power state by i) implementing medium-grained power-gates,\ncarefully distributed across the CPU core, and ii) retaining context in the\npower-ungated domain. Second, AW eliminates the flush latency overhead (several\ntens of microseconds) of the L1/L2 caches when entering a deep idle power state\nby keeping L1/L2 cache content power-ungated. A minimal control logic also\nremains power-ungated to serve cache coherence traffic (i.e., snoops)\nseamlessly. AW implements sleep-mode in caches to reduce caches leakage power\nconsumption and lowers a core voltage to the minimum operational voltage level\nto minimize the leakage power of the power-ungated domain. Third, using a\nstate-of-the-art power efficient all-digital phase-locked loop (ADPLL) clock\ngenerator, AW keeps the PLL active and locked during the idle state, further\ncutting precious microseconds of wake-up latency at a negligible power cost.\nOur evaluation with an accurate simulator calibrated against an Intel Skylake\nserver shows that AW reduces the energy consumption of Memcached by up to 71%\n(35% on average) with up to 1% performance degradation."
                },
                "authors": [
                    {
                        "name": "Jawad Haj Yahya"
                    },
                    {
                        "name": "Haris Volos"
                    },
                    {
                        "name": "Davide B. Bartolini"
                    },
                    {
                        "name": "Georgia Antoniou"
                    },
                    {
                        "name": "Jeremie S. Kim"
                    },
                    {
                        "name": "Zhe Wang"
                    },
                    {
                        "name": "Kleovoulos Kalaitzidis"
                    },
                    {
                        "name": "Tom Rollet"
                    },
                    {
                        "name": "Zhirui Chen"
                    },
                    {
                        "name": "Ye Geng"
                    },
                    {
                        "name": "Onur Mutlu"
                    },
                    {
                        "name": "Yiannakis Sazeides"
                    }
                ],
                "author_detail": {
                    "name": "Yiannakis Sazeides"
                },
                "author": "Yiannakis Sazeides",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2203.02550v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2203.02550v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18113v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18113v1",
                "updated": "2025-02-25T11:36:43Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    11,
                    36,
                    43,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-25T11:36:43Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    11,
                    36,
                    43,
                    1,
                    56,
                    0
                ],
                "title": "Accelerating Graph Indexing for ANNS on Modern CPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Graph Indexing for ANNS on Modern CPUs"
                },
                "summary": "In high-dimensional vector spaces, Approximate Nearest Neighbor Search (ANNS)\nis a key component in database and artificial intelligence infrastructures.\nGraph-based methods, particularly HNSW, have emerged as leading solutions among\nvarious ANNS approaches, offering an impressive trade-off between search\nefficiency and accuracy. Many modern vector databases utilize graph indexes as\ntheir core algorithms, benefiting from various optimizations to enhance search\nperformance. However, the high indexing time associated with graph algorithms\nposes a significant challenge, especially given the increasing volume of data,\nquery processing complexity, and dynamic index maintenance demand. This has\nrendered indexing time a critical performance metric for users. In this paper,\nwe comprehensively analyze the underlying causes of the low graph indexing\nefficiency on modern CPUs, identifying that distance computation dominates\nindexing time, primarily due to high memory access latency and suboptimal\narithmetic operation efficiency. We demonstrate that distance comparisons\nduring index construction can be effectively performed using compact vector\ncodes at an appropriate compression error. Drawing from insights gained through\nintegrating existing compact coding methods in the graph indexing process, we\npropose a novel compact coding strategy, named Flash, designed explicitly for\ngraph indexing and optimized for modern CPU architectures. By minimizing random\nmemory accesses and maximizing the utilization of SIMD (Single Instruction,\nMultiple Data) instructions, Flash significantly enhances cache hit rates and\narithmetic operations. Extensive experiments conducted on eight real-world\ndatasets, ranging from ten million to one billion vectors, exhibit that Flash\nachieves a speedup of 10.4$\\times$ to 22.9$\\times$ in index construction\nefficiency, while maintaining or improving search performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In high-dimensional vector spaces, Approximate Nearest Neighbor Search (ANNS)\nis a key component in database and artificial intelligence infrastructures.\nGraph-based methods, particularly HNSW, have emerged as leading solutions among\nvarious ANNS approaches, offering an impressive trade-off between search\nefficiency and accuracy. Many modern vector databases utilize graph indexes as\ntheir core algorithms, benefiting from various optimizations to enhance search\nperformance. However, the high indexing time associated with graph algorithms\nposes a significant challenge, especially given the increasing volume of data,\nquery processing complexity, and dynamic index maintenance demand. This has\nrendered indexing time a critical performance metric for users. In this paper,\nwe comprehensively analyze the underlying causes of the low graph indexing\nefficiency on modern CPUs, identifying that distance computation dominates\nindexing time, primarily due to high memory access latency and suboptimal\narithmetic operation efficiency. We demonstrate that distance comparisons\nduring index construction can be effectively performed using compact vector\ncodes at an appropriate compression error. Drawing from insights gained through\nintegrating existing compact coding methods in the graph indexing process, we\npropose a novel compact coding strategy, named Flash, designed explicitly for\ngraph indexing and optimized for modern CPU architectures. By minimizing random\nmemory accesses and maximizing the utilization of SIMD (Single Instruction,\nMultiple Data) instructions, Flash significantly enhances cache hit rates and\narithmetic operations. Extensive experiments conducted on eight real-world\ndatasets, ranging from ten million to one billion vectors, exhibit that Flash\nachieves a speedup of 10.4$\\times$ to 22.9$\\times$ in index construction\nefficiency, while maintaining or improving search performance."
                },
                "authors": [
                    {
                        "name": "Mengzhao Wang"
                    },
                    {
                        "name": "Haotian Wu"
                    },
                    {
                        "name": "Xiangyu Ke"
                    },
                    {
                        "name": "Yunjun Gao"
                    },
                    {
                        "name": "Yifan Zhu"
                    },
                    {
                        "name": "Wenchao Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Wenchao Zhou"
                },
                "author": "Wenchao Zhou",
                "arxiv_comment": "SIGMOD 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18113v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18113v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17363v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17363v2",
                "updated": "2025-02-25T09:42:11Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    9,
                    42,
                    11,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-24T17:40:09Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    17,
                    40,
                    9,
                    0,
                    55,
                    0
                ],
                "title": "KV-Edit: Training-Free Image Editing for Precise Background Preservation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV-Edit: Training-Free Image Editing for Precise Background Preservation"
                },
                "summary": "Background consistency remains a significant challenge in image editing\ntasks. Despite extensive developments, existing works still face a trade-off\nbetween maintaining similarity to the original image and generating content\nthat aligns with the target. Here, we propose KV-Edit, a training-free approach\nthat uses KV cache in DiTs to maintain background consistency, where background\ntokens are preserved rather than regenerated, eliminating the need for complex\nmechanisms or expensive training, ultimately generating new content that\nseamlessly integrates with the background within user-provided regions. We\nfurther explore the memory consumption of the KV cache during editing and\noptimize the space complexity to $O(1)$ using an inversion-free method. Our\napproach is compatible with any DiT-based generative model without additional\ntraining. Experiments demonstrate that KV-Edit significantly outperforms\nexisting approaches in terms of both background and image quality, even\nsurpassing training-based methods. Project webpage is available at\nhttps://xilluill.github.io/projectpages/KV-Edit",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Background consistency remains a significant challenge in image editing\ntasks. Despite extensive developments, existing works still face a trade-off\nbetween maintaining similarity to the original image and generating content\nthat aligns with the target. Here, we propose KV-Edit, a training-free approach\nthat uses KV cache in DiTs to maintain background consistency, where background\ntokens are preserved rather than regenerated, eliminating the need for complex\nmechanisms or expensive training, ultimately generating new content that\nseamlessly integrates with the background within user-provided regions. We\nfurther explore the memory consumption of the KV cache during editing and\noptimize the space complexity to $O(1)$ using an inversion-free method. Our\napproach is compatible with any DiT-based generative model without additional\ntraining. Experiments demonstrate that KV-Edit significantly outperforms\nexisting approaches in terms of both background and image quality, even\nsurpassing training-based methods. Project webpage is available at\nhttps://xilluill.github.io/projectpages/KV-Edit"
                },
                "authors": [
                    {
                        "name": "Tianrui Zhu"
                    },
                    {
                        "name": "Shiyi Zhang"
                    },
                    {
                        "name": "Jiawei Shao"
                    },
                    {
                        "name": "Yansong Tang"
                    }
                ],
                "author_detail": {
                    "name": "Yansong Tang"
                },
                "author": "Yansong Tang",
                "arxiv_comment": "Project webpage is available at\n  https://xilluill.github.io/projectpages/KV-Edit",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17363v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17363v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04420v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04420v3",
                "updated": "2025-02-25T03:42:15Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    3,
                    42,
                    15,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-06T15:26:26Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    15,
                    26,
                    26,
                    3,
                    37,
                    0
                ],
                "title": "KVTuner: Sensitivity-Aware Layer-wise Mixed Precision KV Cache\n  Quantization for Efficient and Nearly Lossless LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVTuner: Sensitivity-Aware Layer-wise Mixed Precision KV Cache\n  Quantization for Efficient and Nearly Lossless LLM Inference"
                },
                "summary": "KV cache quantization can improve Large Language Models (LLMs) inference\nthroughput and latency in long contexts and large batch-size scenarios while\npreserving LLMs effectiveness. However, current methods have three unsolved\nissues: overlooking layer-wise sensitivity to KV cache quantization, high\noverhead of online fine-grained decision-making, and low flexibility to\ndifferent LLMs and constraints. Therefore, we thoroughly analyze the inherent\ncorrelation of layer-wise transformer attention patterns to KV cache\nquantization errors and study why key cache is more important than value cache\nfor quantization error reduction. We further propose a simple yet effective\nframework KVTuner to adaptively search for the optimal hardware-friendly\nlayer-wise KV quantization precision pairs for coarse-grained KV cache with\nmulti-objective optimization and directly utilize the offline searched\nconfigurations during online inference. To reduce the computational cost of\noffline calibration, we utilize the intra-layer KV precision pair pruning and\ninter-layer clustering to reduce the search space. Experimental results show\nthat we can achieve nearly lossless 3.25-bit mixed precision KV cache\nquantization for LLMs like Llama-3.1-8B-Instruct and 4.0-bit for sensitive\nmodels like Qwen2.5-7B-Instruct on mathematical reasoning tasks. The maximum\ninference throughput can be improved by 38.3% compared with KV8 quantization\nover various context lengths. Our code and searched configurations are\navailable at https://github.com/cmd2001/KVTuner.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV cache quantization can improve Large Language Models (LLMs) inference\nthroughput and latency in long contexts and large batch-size scenarios while\npreserving LLMs effectiveness. However, current methods have three unsolved\nissues: overlooking layer-wise sensitivity to KV cache quantization, high\noverhead of online fine-grained decision-making, and low flexibility to\ndifferent LLMs and constraints. Therefore, we thoroughly analyze the inherent\ncorrelation of layer-wise transformer attention patterns to KV cache\nquantization errors and study why key cache is more important than value cache\nfor quantization error reduction. We further propose a simple yet effective\nframework KVTuner to adaptively search for the optimal hardware-friendly\nlayer-wise KV quantization precision pairs for coarse-grained KV cache with\nmulti-objective optimization and directly utilize the offline searched\nconfigurations during online inference. To reduce the computational cost of\noffline calibration, we utilize the intra-layer KV precision pair pruning and\ninter-layer clustering to reduce the search space. Experimental results show\nthat we can achieve nearly lossless 3.25-bit mixed precision KV cache\nquantization for LLMs like Llama-3.1-8B-Instruct and 4.0-bit for sensitive\nmodels like Qwen2.5-7B-Instruct on mathematical reasoning tasks. The maximum\ninference throughput can be improved by 38.3% compared with KV8 quantization\nover various context lengths. Our code and searched configurations are\navailable at https://github.com/cmd2001/KVTuner."
                },
                "authors": [
                    {
                        "name": "Xing Li"
                    },
                    {
                        "name": "Zeyu Xing"
                    },
                    {
                        "name": "Yiming Li"
                    },
                    {
                        "name": "Linping Qu"
                    },
                    {
                        "name": "Hui-Ling Zhen"
                    },
                    {
                        "name": "Wulong Liu"
                    },
                    {
                        "name": "Yiwu Yao"
                    },
                    {
                        "name": "Sinno Jialin Pan"
                    },
                    {
                        "name": "Mingxuan Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Mingxuan Yuan"
                },
                "author": "Mingxuan Yuan",
                "arxiv_comment": "36 pages. Code: https://github.com/cmd2001/KVTuner",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04420v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04420v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17606v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17606v1",
                "updated": "2025-02-24T19:48:48Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    19,
                    48,
                    48,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T19:48:48Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    19,
                    48,
                    48,
                    0,
                    55,
                    0
                ],
                "title": "ELMo-Tune-V2: LLM-Assisted Full-Cycle Auto-Tuning to Optimize LSM-Based\n  Key-Value Stores",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ELMo-Tune-V2: LLM-Assisted Full-Cycle Auto-Tuning to Optimize LSM-Based\n  Key-Value Stores"
                },
                "summary": "Log-Structured Merge-tree-based Key-Value Store (LSM-KVS) is a foundational\nstorage engine serving diverse modern workloads, systems, and applications. To\nsuit varying use cases, LSM-KVS allows a vast configuration space that controls\ncore parameters like compaction, flush, and cache sizes, each consuming a\nshared pool of CPU, Memory, and Storage resources. Navigating the LSM-KVS\nconfiguration space necessitates knowledge of the impact of each configuration\non the expected workload and underlying hardware. Beyond expensive and\ntime-intensive human-expert-based tuning, existing LSM-KVS tuning solutions\nfocus on tuning with specific workload expectations while limited to a narrow\nsubset of parameters.\n  This paper introduces ELMo-Tune-V2, a framework that integrates Large\nLanguage Models (LLMs) at its foundation to demonstrate the potential of\napplying modern LLMs in data system optimization problems. ELMo-Tune-V2\nleverages the contextual reasoning, cross-domain, and generative capabilities\nof LLMs to perform 1) self-navigated characterization and modeling of LSM-KVS\nworkloads, 2) automatic tuning across a broad parameter space using\ncross-domain knowledge, and 3) real-time dynamic configuration adjustments for\nLSM-KVS. ELMo-Tune-V2 integrates three innovations: LLM-based workload\nsynthesis for adaptive benchmark generation, feedback-driven iterative\nfine-tuning for configuration refinement, and real-time tuning to handle\nevolving workloads. Through detailed evaluation using RocksDB under several\nreal-world applications across diverse scenarios, ELMo-Tune-V2 achieves\nperformance improvements up to ~14X our YCSB benchmarks compared against\ndefault RocksDB configurations, and our end-to-end tests with upper-level\napplications, NebulaGraph and Kvrocks, demonstrate performance gains of 34% and\n26%, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Log-Structured Merge-tree-based Key-Value Store (LSM-KVS) is a foundational\nstorage engine serving diverse modern workloads, systems, and applications. To\nsuit varying use cases, LSM-KVS allows a vast configuration space that controls\ncore parameters like compaction, flush, and cache sizes, each consuming a\nshared pool of CPU, Memory, and Storage resources. Navigating the LSM-KVS\nconfiguration space necessitates knowledge of the impact of each configuration\non the expected workload and underlying hardware. Beyond expensive and\ntime-intensive human-expert-based tuning, existing LSM-KVS tuning solutions\nfocus on tuning with specific workload expectations while limited to a narrow\nsubset of parameters.\n  This paper introduces ELMo-Tune-V2, a framework that integrates Large\nLanguage Models (LLMs) at its foundation to demonstrate the potential of\napplying modern LLMs in data system optimization problems. ELMo-Tune-V2\nleverages the contextual reasoning, cross-domain, and generative capabilities\nof LLMs to perform 1) self-navigated characterization and modeling of LSM-KVS\nworkloads, 2) automatic tuning across a broad parameter space using\ncross-domain knowledge, and 3) real-time dynamic configuration adjustments for\nLSM-KVS. ELMo-Tune-V2 integrates three innovations: LLM-based workload\nsynthesis for adaptive benchmark generation, feedback-driven iterative\nfine-tuning for configuration refinement, and real-time tuning to handle\nevolving workloads. Through detailed evaluation using RocksDB under several\nreal-world applications across diverse scenarios, ELMo-Tune-V2 achieves\nperformance improvements up to ~14X our YCSB benchmarks compared against\ndefault RocksDB configurations, and our end-to-end tests with upper-level\napplications, NebulaGraph and Kvrocks, demonstrate performance gains of 34% and\n26%, respectively."
                },
                "authors": [
                    {
                        "name": "Viraj Thakkar"
                    },
                    {
                        "name": "Qi Lin"
                    },
                    {
                        "name": "Kenanya Keandra Adriel Prasetyo"
                    },
                    {
                        "name": "Raden Haryosatyo Wisjnunandono"
                    },
                    {
                        "name": "Achmad Imam Kistijantoro"
                    },
                    {
                        "name": "Reza Fuad Rachmadi"
                    },
                    {
                        "name": "Zhichao Cao"
                    }
                ],
                "author_detail": {
                    "name": "Zhichao Cao"
                },
                "author": "Zhichao Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17606v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17606v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17599v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17599v1",
                "updated": "2025-02-24T19:34:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    19,
                    34,
                    52,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T19:34:52Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    19,
                    34,
                    52,
                    0,
                    55,
                    0
                ],
                "title": "MEDA: Dynamic KV Cache Allocation for Efficient Multimodal Long-Context\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MEDA: Dynamic KV Cache Allocation for Efficient Multimodal Long-Context\n  Inference"
                },
                "summary": "Long-context Multimodal Large Language Models (MLLMs) that incorporate long\ntext-image and text-video modalities, demand substantial resources as their\nmultimodal Key-Value (KV) caches grow with increasing input lengths,\nchallenging inference efficiency. Existing methods for KV cache compression, in\nboth text-only and multimodal LLMs, have neglected attention density variations\nacross layers, thus often adopting uniform or progressive reduction strategies\nfor layer-wise cache allocation. In this work, we propose MEDA, a dynamic\nlayer-wise KV cache allocation method for efficient multimodal long-context\ninference. As its core, MEDA utilizes cross-modal attention entropy to\ndetermine the KV cache size at each MLLMs layer. Given the dynamically\nallocated KV cache size at each layer, MEDA also employs a KV pair selection\nscheme to identify which KV pairs to select and a KV pair merging strategy that\nmerges the selected and non-selected ones to preserve information from the\nentire context. MEDA achieves up to 72% KV cache memory reduction and 2.82\ntimes faster decoding speed, while maintaining or enhancing performance on\nvarious multimodal tasks in long-context settings, including multi-images and\nlong-video scenarios. Our code is released at\nhttps://github.com/AIoT-MLSys-Lab/MEDA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context Multimodal Large Language Models (MLLMs) that incorporate long\ntext-image and text-video modalities, demand substantial resources as their\nmultimodal Key-Value (KV) caches grow with increasing input lengths,\nchallenging inference efficiency. Existing methods for KV cache compression, in\nboth text-only and multimodal LLMs, have neglected attention density variations\nacross layers, thus often adopting uniform or progressive reduction strategies\nfor layer-wise cache allocation. In this work, we propose MEDA, a dynamic\nlayer-wise KV cache allocation method for efficient multimodal long-context\ninference. As its core, MEDA utilizes cross-modal attention entropy to\ndetermine the KV cache size at each MLLMs layer. Given the dynamically\nallocated KV cache size at each layer, MEDA also employs a KV pair selection\nscheme to identify which KV pairs to select and a KV pair merging strategy that\nmerges the selected and non-selected ones to preserve information from the\nentire context. MEDA achieves up to 72% KV cache memory reduction and 2.82\ntimes faster decoding speed, while maintaining or enhancing performance on\nvarious multimodal tasks in long-context settings, including multi-images and\nlong-video scenarios. Our code is released at\nhttps://github.com/AIoT-MLSys-Lab/MEDA."
                },
                "authors": [
                    {
                        "name": "Zhongwei Wan"
                    },
                    {
                        "name": "Hui Shen"
                    },
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Che Liu"
                    },
                    {
                        "name": "Zheda Mai"
                    },
                    {
                        "name": "Mi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Mi Zhang"
                },
                "author": "Mi Zhang",
                "arxiv_comment": "NAACL 2025 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17599v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17599v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17421v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17421v1",
                "updated": "2025-02-24T18:53:31Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    53,
                    31,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T18:53:31Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    53,
                    31,
                    0,
                    55,
                    0
                ],
                "title": "LongSpec: Long-Context Speculative Decoding with Efficient Drafting and\n  Verification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LongSpec: Long-Context Speculative Decoding with Efficient Drafting and\n  Verification"
                },
                "summary": "Speculative decoding has become a promising technique to mitigate the high\ninference latency of autoregressive decoding in Large Language Models (LLMs).\nDespite its promise, the effective application of speculative decoding in LLMs\nstill confronts three key challenges: the increasing memory demands of the\ndraft model, the distribution shift between the short-training corpora and\nlong-context inference, and inefficiencies in attention implementation. In this\nwork, we enhance the performance of speculative decoding in long-context\nsettings by addressing these challenges. First, we propose a memory-efficient\ndraft model with a constant-sized Key-Value (KV) cache. Second, we introduce\nnovel position indices for short-training data, enabling seamless adaptation\nfrom short-context training to long-context inference. Finally, we present an\ninnovative attention aggregation method that combines fast implementations for\nprefix computation with standard attention for tree mask handling, effectively\nresolving the latency and memory inefficiencies of tree decoding. Our approach\nachieves strong results on various long-context tasks, including\nrepository-level code completion, long-context summarization, and o1-like long\nreasoning tasks, demonstrating significant improvements in latency reduction.\nThe code is available at https://github.com/sail-sg/LongSpec.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding has become a promising technique to mitigate the high\ninference latency of autoregressive decoding in Large Language Models (LLMs).\nDespite its promise, the effective application of speculative decoding in LLMs\nstill confronts three key challenges: the increasing memory demands of the\ndraft model, the distribution shift between the short-training corpora and\nlong-context inference, and inefficiencies in attention implementation. In this\nwork, we enhance the performance of speculative decoding in long-context\nsettings by addressing these challenges. First, we propose a memory-efficient\ndraft model with a constant-sized Key-Value (KV) cache. Second, we introduce\nnovel position indices for short-training data, enabling seamless adaptation\nfrom short-context training to long-context inference. Finally, we present an\ninnovative attention aggregation method that combines fast implementations for\nprefix computation with standard attention for tree mask handling, effectively\nresolving the latency and memory inefficiencies of tree decoding. Our approach\nachieves strong results on various long-context tasks, including\nrepository-level code completion, long-context summarization, and o1-like long\nreasoning tasks, demonstrating significant improvements in latency reduction.\nThe code is available at https://github.com/sail-sg/LongSpec."
                },
                "authors": [
                    {
                        "name": "Penghui Yang"
                    },
                    {
                        "name": "Cunxiao Du"
                    },
                    {
                        "name": "Fengzhuo Zhang"
                    },
                    {
                        "name": "Haonan Wang"
                    },
                    {
                        "name": "Tianyu Pang"
                    },
                    {
                        "name": "Chao Du"
                    },
                    {
                        "name": "Bo An"
                    }
                ],
                "author_detail": {
                    "name": "Bo An"
                },
                "author": "Bo An",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17421v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17421v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.01418v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.01418v2",
                "updated": "2025-02-24T18:51:48Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    51,
                    48,
                    0,
                    55,
                    0
                ],
                "published": "2024-05-02T16:08:03Z",
                "published_parsed": [
                    2024,
                    5,
                    2,
                    16,
                    8,
                    3,
                    3,
                    123,
                    0
                ],
                "title": "GTX: A Write-Optimized Latch-free Graph Data System with Transactional\n  Support -- Extended Version",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GTX: A Write-Optimized Latch-free Graph Data System with Transactional\n  Support -- Extended Version"
                },
                "summary": "This paper introduces GTX, a standalone main-memory write-optimized graph\ndata system that specializes in structural and graph property updates while\nenabling concurrent reads and graph analytics through ACID transactions. Recent\ngraph systems target concurrent read and write support while guaranteeing\ntransaction semantics. However, their performance suffers from updates with\nreal-world temporal locality over the same vertices and edges due to\nvertex-centric lock contentions. GTX has an adaptive delta-chain locking\nprotocol on top of a carefully designed latch-free graph storage. It eliminates\nvertex-level locking contention, and adapts to real-life workloads while\nmaintaining sequential access to the graph's adjacency lists storage. GTX's\ntransactions further support cache-friendly block level concurrency control,\nand cooperative group commit and garbage collection. This combination of\nfeatures ensures high update throughput and provides low-latency graph\nanalytics. Based on experimental evaluation, in addition to not sacrificing the\nperformance of read-heavy analytical workloads, and having competitive\nperformance similar to state-of-the-art systems, GTX has high read-write\ntransaction throughput. For write-heavy transactional workloads, GTX achieves\nup to 11x better transaction throughput than the best-performing\nstate-of-the-art system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces GTX, a standalone main-memory write-optimized graph\ndata system that specializes in structural and graph property updates while\nenabling concurrent reads and graph analytics through ACID transactions. Recent\ngraph systems target concurrent read and write support while guaranteeing\ntransaction semantics. However, their performance suffers from updates with\nreal-world temporal locality over the same vertices and edges due to\nvertex-centric lock contentions. GTX has an adaptive delta-chain locking\nprotocol on top of a carefully designed latch-free graph storage. It eliminates\nvertex-level locking contention, and adapts to real-life workloads while\nmaintaining sequential access to the graph's adjacency lists storage. GTX's\ntransactions further support cache-friendly block level concurrency control,\nand cooperative group commit and garbage collection. This combination of\nfeatures ensures high update throughput and provides low-latency graph\nanalytics. Based on experimental evaluation, in addition to not sacrificing the\nperformance of read-heavy analytical workloads, and having competitive\nperformance similar to state-of-the-art systems, GTX has high read-write\ntransaction throughput. For write-heavy transactional workloads, GTX achieves\nup to 11x better transaction throughput than the best-performing\nstate-of-the-art system."
                },
                "authors": [
                    {
                        "name": "Libin Zhou"
                    },
                    {
                        "name": "Lu Xing"
                    },
                    {
                        "name": "Yeasir Rayhan"
                    },
                    {
                        "name": "Walid. G. Aref"
                    }
                ],
                "author_detail": {
                    "name": "Walid. G. Aref"
                },
                "author": "Walid. G. Aref",
                "arxiv_comment": "technical report for our main paper GTX: A Write-Optimized Latch-free\n  Graph Data System with Transactional Support",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.01418v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.01418v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.2.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17398v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17398v1",
                "updated": "2025-02-24T18:26:22Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    26,
                    22,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T18:26:22Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    26,
                    22,
                    0,
                    55,
                    0
                ],
                "title": "Evaluating IOMMU-Based Shared Virtual Addressing for RISC-V Embedded\n  Heterogeneous SoCs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating IOMMU-Based Shared Virtual Addressing for RISC-V Embedded\n  Heterogeneous SoCs"
                },
                "summary": "Embedded heterogeneous systems-on-chip (SoCs) rely on domain-specific\nhardware accelerators to improve performance and energy efficiency. In\nparticular, programmable multi-core accelerators feature a cluster of\nprocessing elements and tightly coupled scratchpad memories to balance\nperformance, energy efficiency, and flexibility. In embedded systems running a\ngeneral-purpose OS, accelerators access data via dedicated, physically\naddressed memory regions. This negatively impacts memory utilization and\nperformance by requiring a copy from the virtual host address to the physical\naccelerator address space. Input-Output Memory Management Units (IOMMUs)\novercome this limitation by allowing devices and hosts to use a shared virtual\npaged address space. However, resolving IO virtual addresses can be\nparticularly costly on high-latency memory systems as it requires up to three\nsequential memory accesses on IOTLB miss. In this work, we present a\nquantitative evaluation of shared virtual addressing in RISC-V heterogeneous\nembedded systems. We integrate an IOMMU in an open-source heterogeneous RISC-V\nSoC consisting of a 64-bit host with a 32-bit accelerator cluster. We evaluated\nthe system performance by emulating the design on FPGA and implementing compute\nkernels from the RajaPERF benchmark suite using heterogeneous OpenMP\nprogramming. We measure the transfers and computation time on the host and\naccelerators for systems with different DRAM access latencies. We first show\nthat IO virtual address translation can account for 4.2% up to 17.6% of the\naccelerator's runtime for gemm (General Matrix Multiplication) at low and high\nmemory bandwidth. Then, we show that in systems containing a last-level cache,\nthis IO address translation cost falls to 0.4% and 0.7% under the same\nconditions, making shared virtual addressing and zero-copy offloading suitable\nfor such RISC-V heterogeneous SoCs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embedded heterogeneous systems-on-chip (SoCs) rely on domain-specific\nhardware accelerators to improve performance and energy efficiency. In\nparticular, programmable multi-core accelerators feature a cluster of\nprocessing elements and tightly coupled scratchpad memories to balance\nperformance, energy efficiency, and flexibility. In embedded systems running a\ngeneral-purpose OS, accelerators access data via dedicated, physically\naddressed memory regions. This negatively impacts memory utilization and\nperformance by requiring a copy from the virtual host address to the physical\naccelerator address space. Input-Output Memory Management Units (IOMMUs)\novercome this limitation by allowing devices and hosts to use a shared virtual\npaged address space. However, resolving IO virtual addresses can be\nparticularly costly on high-latency memory systems as it requires up to three\nsequential memory accesses on IOTLB miss. In this work, we present a\nquantitative evaluation of shared virtual addressing in RISC-V heterogeneous\nembedded systems. We integrate an IOMMU in an open-source heterogeneous RISC-V\nSoC consisting of a 64-bit host with a 32-bit accelerator cluster. We evaluated\nthe system performance by emulating the design on FPGA and implementing compute\nkernels from the RajaPERF benchmark suite using heterogeneous OpenMP\nprogramming. We measure the transfers and computation time on the host and\naccelerators for systems with different DRAM access latencies. We first show\nthat IO virtual address translation can account for 4.2% up to 17.6% of the\naccelerator's runtime for gemm (General Matrix Multiplication) at low and high\nmemory bandwidth. Then, we show that in systems containing a last-level cache,\nthis IO address translation cost falls to 0.4% and 0.7% under the same\nconditions, making shared virtual addressing and zero-copy offloading suitable\nfor such RISC-V heterogeneous SoCs."
                },
                "authors": [
                    {
                        "name": "Cyril Koenig"
                    },
                    {
                        "name": "Enrico Zelioli"
                    },
                    {
                        "name": "Luca Benini"
                    }
                ],
                "author_detail": {
                    "name": "Luca Benini"
                },
                "author": "Luca Benini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17398v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17398v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12094v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12094v5",
                "updated": "2025-02-24T15:42:59Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    15,
                    42,
                    59,
                    0,
                    55,
                    0
                ],
                "published": "2024-12-16T18:58:57Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    18,
                    58,
                    57,
                    0,
                    351,
                    0
                ],
                "title": "SepLLM: Accelerate Large Language Models by Compressing One Segment into\n  One Separator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SepLLM: Accelerate Large Language Models by Compressing One Segment into\n  One Separator"
                },
                "summary": "Large Language Models (LLMs) have exhibited exceptional performance across a\nspectrum of natural language processing tasks. However, their substantial sizes\npose considerable challenges, particularly in computational demands and\ninference speed, due to their quadratic complexity. In this work, we have\nidentified a key pattern: certain seemingly meaningless separator tokens (i.e.,\npunctuations) contribute disproportionately to attention scores compared to\nsemantically meaningful tokens. This observation suggests that information of\nthe segments between these separator tokens can be effectively condensed into\nthe separator tokens themselves without significant information loss. Guided by\nthis insight, we introduce SepLLM, a plug-and-play framework that accelerates\ninference by compressing these segments and eliminating redundant tokens.\nAdditionally, we implement efficient kernels for training acceleration.\nExperimental results across training-free, training-from-scratch, and\npost-training settings demonstrate SepLLM's effectiveness. Notably, using the\nLlama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the\nGSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in\nstreaming settings, SepLLM effectively processes sequences of up to 4 million\ntokens or more while maintaining consistent language modeling capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have exhibited exceptional performance across a\nspectrum of natural language processing tasks. However, their substantial sizes\npose considerable challenges, particularly in computational demands and\ninference speed, due to their quadratic complexity. In this work, we have\nidentified a key pattern: certain seemingly meaningless separator tokens (i.e.,\npunctuations) contribute disproportionately to attention scores compared to\nsemantically meaningful tokens. This observation suggests that information of\nthe segments between these separator tokens can be effectively condensed into\nthe separator tokens themselves without significant information loss. Guided by\nthis insight, we introduce SepLLM, a plug-and-play framework that accelerates\ninference by compressing these segments and eliminating redundant tokens.\nAdditionally, we implement efficient kernels for training acceleration.\nExperimental results across training-free, training-from-scratch, and\npost-training settings demonstrate SepLLM's effectiveness. Notably, using the\nLlama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the\nGSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in\nstreaming settings, SepLLM effectively processes sequences of up to 4 million\ntokens or more while maintaining consistent language modeling capabilities."
                },
                "authors": [
                    {
                        "name": "Guoxuan Chen"
                    },
                    {
                        "name": "Han Shi"
                    },
                    {
                        "name": "Jiawei Li"
                    },
                    {
                        "name": "Yihang Gao"
                    },
                    {
                        "name": "Xiaozhe Ren"
                    },
                    {
                        "name": "Yimeng Chen"
                    },
                    {
                        "name": "Xin Jiang"
                    },
                    {
                        "name": "Zhenguo Li"
                    },
                    {
                        "name": "Weiyang Liu"
                    },
                    {
                        "name": "Chao Huang"
                    }
                ],
                "author_detail": {
                    "name": "Chao Huang"
                },
                "author": "Chao Huang",
                "arxiv_comment": "We have made our code publicly available at sepllm.github.io. Our\n  codebase supports efficient multi-node distributed training with accelerated\n  attention module Sep-Attention and also supports numerous existing Fusion\n  Operators to accelerate the training process, such as fused rope, etc. If you\n  find our code helpful, please kindly consider giving us a **star** on GitHub\n  ^_^ Thank you very much!",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12094v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12094v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17535v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17535v1",
                "updated": "2025-02-24T15:39:35Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    15,
                    39,
                    35,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T15:39:35Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    15,
                    39,
                    35,
                    0,
                    55,
                    0
                ],
                "title": "The Lottery LLM Hypothesis, Rethinking What Abilities Should LLM\n  Compression Preserve?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Lottery LLM Hypothesis, Rethinking What Abilities Should LLM\n  Compression Preserve?"
                },
                "summary": "Motivated by reducing the computational and storage costs of LLMs, model\ncompression and KV cache compression have attracted much attention from\nresearchers. However, current methods predominantly emphasize maintaining the\nperformance of compressed LLMs, as measured by perplexity or simple accuracy on\ntasks of common sense knowledge QA and basic arithmetic reasoning. In this\nblog, we present a brief review of recent advancements in LLMs related to\nretrieval-augmented generation, multi-step reasoning, external tools, and\ncomputational expressivity, all of which substantially enhance LLM performance.\nThen, we propose a lottery LLM hypothesis suggesting that for a given LLM and\ntask, there exists a smaller lottery LLM capable of producing the same\nperformance as the original LLM with the assistance of multi-step reasoning and\nexternal tools. Based on the review of current progress in LLMs, we discuss and\nsummarize the essential capabilities that the lottery LLM and KV cache\ncompression must possess, which are currently overlooked in existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Motivated by reducing the computational and storage costs of LLMs, model\ncompression and KV cache compression have attracted much attention from\nresearchers. However, current methods predominantly emphasize maintaining the\nperformance of compressed LLMs, as measured by perplexity or simple accuracy on\ntasks of common sense knowledge QA and basic arithmetic reasoning. In this\nblog, we present a brief review of recent advancements in LLMs related to\nretrieval-augmented generation, multi-step reasoning, external tools, and\ncomputational expressivity, all of which substantially enhance LLM performance.\nThen, we propose a lottery LLM hypothesis suggesting that for a given LLM and\ntask, there exists a smaller lottery LLM capable of producing the same\nperformance as the original LLM with the assistance of multi-step reasoning and\nexternal tools. Based on the review of current progress in LLMs, we discuss and\nsummarize the essential capabilities that the lottery LLM and KV cache\ncompression must possess, which are currently overlooked in existing methods."
                },
                "authors": [
                    {
                        "name": "Zhenheng Tang"
                    },
                    {
                        "name": "Xiang Liu"
                    },
                    {
                        "name": "Qian Wang"
                    },
                    {
                        "name": "Peijie Dong"
                    },
                    {
                        "name": "Bingsheng He"
                    },
                    {
                        "name": "Xiaowen Chu"
                    },
                    {
                        "name": "Bo Li"
                    }
                ],
                "author_detail": {
                    "name": "Bo Li"
                },
                "author": "Bo Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17535v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17535v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.FL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15294v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15294v2",
                "updated": "2025-02-24T13:35:18Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    13,
                    35,
                    18,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-21T08:40:07Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    8,
                    40,
                    7,
                    4,
                    52,
                    0
                ],
                "title": "Round Attention: A Novel Round-Level Attention Mechanism to Accelerate\n  LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Round Attention: A Novel Round-Level Attention Mechanism to Accelerate\n  LLM Inference"
                },
                "summary": "The increasing context window size in large language models (LLMs) has\nimproved their ability to handle complex, long-text tasks. However, as the\nconversation rounds continue, it is required to store a large amount of KV\ncache in GPU memory, which significantly affects the efficiency and even\navailability of the model serving systems. This paper analyzes dialogue data\nfrom real users and discovers that the LLM inference manifests a watershed\nlayer, after which the distribution of round-level attention shows notable\nsimilarity. We propose Round Attention, a novel round-level attention mechanism\nthat only recalls and computes the KV cache of the most relevant rounds. The\nexperiments show that our method saves 55\\% memory usage without compromising\nmodel performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing context window size in large language models (LLMs) has\nimproved their ability to handle complex, long-text tasks. However, as the\nconversation rounds continue, it is required to store a large amount of KV\ncache in GPU memory, which significantly affects the efficiency and even\navailability of the model serving systems. This paper analyzes dialogue data\nfrom real users and discovers that the LLM inference manifests a watershed\nlayer, after which the distribution of round-level attention shows notable\nsimilarity. We propose Round Attention, a novel round-level attention mechanism\nthat only recalls and computes the KV cache of the most relevant rounds. The\nexperiments show that our method saves 55\\% memory usage without compromising\nmodel performance."
                },
                "authors": [
                    {
                        "name": "Yaohua Tang"
                    },
                    {
                        "name": "Zhicheng Hu"
                    },
                    {
                        "name": "Kun Cheng"
                    },
                    {
                        "name": "Fan Mo"
                    },
                    {
                        "name": "Qiheng Lv"
                    },
                    {
                        "name": "Hua Wang"
                    },
                    {
                        "name": "Zhi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Chen"
                },
                "author": "Zhi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15294v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15294v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17139v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17139v1",
                "updated": "2025-02-24T13:30:30Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    13,
                    30,
                    30,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T13:30:30Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    13,
                    30,
                    30,
                    0,
                    55,
                    0
                ],
                "title": "CodeSwift: Accelerating LLM Inference for Efficient Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CodeSwift: Accelerating LLM Inference for Efficient Code Generation"
                },
                "summary": "Code generation is a latency-sensitive task that demands high timeliness, but\nthe autoregressive decoding mechanism of Large Language Models (LLMs) leads to\npoor inference efficiency. Existing LLM inference acceleration methods mainly\nfocus on standalone functions using only built-in components. Moreover, they\ntreat code like natural language sequences, ignoring its unique syntax and\nsemantic characteristics. As a result, the effectiveness of these approaches in\ncode generation tasks remains limited and fails to align with real-world\nprogramming scenarios. To alleviate this issue, we propose CodeSwift, a simple\nyet highly efficient inference acceleration approach specifically designed for\ncode generation, without comprising the quality of the output. CodeSwift\nconstructs a multi-source datastore, providing access to both general and\nproject-specific knowledge, facilitating the retrieval of high-quality draft\nsequences. Moreover, CodeSwift reduces retrieval cost by controlling retrieval\ntiming, and enhances efficiency through parallel retrieval and a context- and\nLLM preference-aware cache. Experimental results show that CodeSwift can reach\nup to 2.53x and 2.54x speedup compared to autoregressive decoding in\nrepository-level and standalone code generation tasks, respectively,\noutperforming state-of-the-art inference acceleration approaches by up to 88%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code generation is a latency-sensitive task that demands high timeliness, but\nthe autoregressive decoding mechanism of Large Language Models (LLMs) leads to\npoor inference efficiency. Existing LLM inference acceleration methods mainly\nfocus on standalone functions using only built-in components. Moreover, they\ntreat code like natural language sequences, ignoring its unique syntax and\nsemantic characteristics. As a result, the effectiveness of these approaches in\ncode generation tasks remains limited and fails to align with real-world\nprogramming scenarios. To alleviate this issue, we propose CodeSwift, a simple\nyet highly efficient inference acceleration approach specifically designed for\ncode generation, without comprising the quality of the output. CodeSwift\nconstructs a multi-source datastore, providing access to both general and\nproject-specific knowledge, facilitating the retrieval of high-quality draft\nsequences. Moreover, CodeSwift reduces retrieval cost by controlling retrieval\ntiming, and enhances efficiency through parallel retrieval and a context- and\nLLM preference-aware cache. Experimental results show that CodeSwift can reach\nup to 2.53x and 2.54x speedup compared to autoregressive decoding in\nrepository-level and standalone code generation tasks, respectively,\noutperforming state-of-the-art inference acceleration approaches by up to 88%."
                },
                "authors": [
                    {
                        "name": "Qianhui Zhao"
                    },
                    {
                        "name": "Li Zhang"
                    },
                    {
                        "name": "Fang Liu"
                    },
                    {
                        "name": "Xiaoli Lian"
                    },
                    {
                        "name": "Qiaoyuanhe Meng"
                    },
                    {
                        "name": "Ziqian Jiao"
                    },
                    {
                        "name": "Zetong Zhou"
                    },
                    {
                        "name": "Borui Zhang"
                    },
                    {
                        "name": "Runlin Guo"
                    },
                    {
                        "name": "Jia Li"
                    }
                ],
                "author_detail": {
                    "name": "Jia Li"
                },
                "author": "Jia Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17139v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17139v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.16886v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.16886v1",
                "updated": "2025-02-24T06:33:39Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    6,
                    33,
                    39,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T06:33:39Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    6,
                    33,
                    39,
                    0,
                    55,
                    0
                ],
                "title": "DBudgetKV: Dynamic Budget in KV Cache Compression for Ensuring Optimal\n  Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DBudgetKV: Dynamic Budget in KV Cache Compression for Ensuring Optimal\n  Performance"
                },
                "summary": "To alleviate memory burden during inference of large language models (LLMs),\nnumerous studies have focused on compressing the KV cache by exploring aspects\nsuch as attention sparsity. However, these techniques often require a\npre-defined cache budget; as the optimal budget varies with different input\nlengths and task types, it limits their practical deployment accepting\nopen-domain instructions. To address this limitation, we propose a new KV cache\ncompression objective: to always ensure the full-cache performance regardless\nof specific inputs, while maximizing KV cache pruning as much as possible. To\nachieve this goal, we introduce a novel KV cache compression method dubbed\nDBudgetKV, which features an attention-based metric to signal when the\nremaining KV cache is unlikely to match the full-cache performance, then\nhalting the pruning process. Empirical evaluation spanning diverse context\nlengths, task types, and model sizes suggests that our method achieves lossless\nKV pruning effectively and robustly, exceeding 25% compression ratio on\naverage. Furthermore, our method is easy to integrate within LLM inference, not\nonly optimizing memory space, but also showing reduced inference time compared\nto existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To alleviate memory burden during inference of large language models (LLMs),\nnumerous studies have focused on compressing the KV cache by exploring aspects\nsuch as attention sparsity. However, these techniques often require a\npre-defined cache budget; as the optimal budget varies with different input\nlengths and task types, it limits their practical deployment accepting\nopen-domain instructions. To address this limitation, we propose a new KV cache\ncompression objective: to always ensure the full-cache performance regardless\nof specific inputs, while maximizing KV cache pruning as much as possible. To\nachieve this goal, we introduce a novel KV cache compression method dubbed\nDBudgetKV, which features an attention-based metric to signal when the\nremaining KV cache is unlikely to match the full-cache performance, then\nhalting the pruning process. Empirical evaluation spanning diverse context\nlengths, task types, and model sizes suggests that our method achieves lossless\nKV pruning effectively and robustly, exceeding 25% compression ratio on\naverage. Furthermore, our method is easy to integrate within LLM inference, not\nonly optimizing memory space, but also showing reduced inference time compared\nto existing methods."
                },
                "authors": [
                    {
                        "name": "Xuanfan Ni"
                    },
                    {
                        "name": "Liyan Xu"
                    },
                    {
                        "name": "Chenyang Lyu"
                    },
                    {
                        "name": "Longyue Wang"
                    },
                    {
                        "name": "Mo Yu"
                    },
                    {
                        "name": "Lemao Liu"
                    },
                    {
                        "name": "Fandong Meng"
                    },
                    {
                        "name": "Jie Zhou"
                    },
                    {
                        "name": "Piji Li"
                    }
                ],
                "author_detail": {
                    "name": "Piji Li"
                },
                "author": "Piji Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.16886v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.16886v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13176v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13176v2",
                "updated": "2025-02-24T01:28:27Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    1,
                    28,
                    27,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-18T04:08:29Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    4,
                    8,
                    29,
                    1,
                    49,
                    0
                ],
                "title": "BaKlaVa -- Budgeted Allocation of KV cache for Long-context Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BaKlaVa -- Budgeted Allocation of KV cache for Long-context Inference"
                },
                "summary": "In Large Language Model (LLM) inference, Key-Value (KV) caches (KV-caches)\nare essential for reducing time complexity. However, they result in a linear\nincrease in GPU memory as the context length grows. While recent work explores\nKV-cache eviction and compression policies to reduce memory usage, they often\nconsider uniform KV-caches across all attention heads, leading to suboptimal\nperformance. We introduce BaKlaVa, a method to allocate optimal memory for\nindividual KV-caches across the model by estimating the importance of each\nKV-cache. Our empirical analysis demonstrates that not all KV-caches are\nequally critical for LLM performance. Using a one-time profiling approach,\nBaKlaVa assigns optimal memory budgets to each KV-cache. We evaluated our\nmethod on LLaMA-3-8B, and Qwen2.5-7B models, achieving up to a 70\\% compression\nratio while keeping baseline performance and delivering up to an\norder-of-magnitude accuracy improvement at higher compression levels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In Large Language Model (LLM) inference, Key-Value (KV) caches (KV-caches)\nare essential for reducing time complexity. However, they result in a linear\nincrease in GPU memory as the context length grows. While recent work explores\nKV-cache eviction and compression policies to reduce memory usage, they often\nconsider uniform KV-caches across all attention heads, leading to suboptimal\nperformance. We introduce BaKlaVa, a method to allocate optimal memory for\nindividual KV-caches across the model by estimating the importance of each\nKV-cache. Our empirical analysis demonstrates that not all KV-caches are\nequally critical for LLM performance. Using a one-time profiling approach,\nBaKlaVa assigns optimal memory budgets to each KV-cache. We evaluated our\nmethod on LLaMA-3-8B, and Qwen2.5-7B models, achieving up to a 70\\% compression\nratio while keeping baseline performance and delivering up to an\norder-of-magnitude accuracy improvement at higher compression levels."
                },
                "authors": [
                    {
                        "name": "Ahmed Burak Gulhan"
                    },
                    {
                        "name": "Krishna Teja Chitty-Venkata"
                    },
                    {
                        "name": "Murali Emani"
                    },
                    {
                        "name": "Mahmut Kandemir"
                    },
                    {
                        "name": "Venkatram Vishwanath"
                    }
                ],
                "author_detail": {
                    "name": "Venkatram Vishwanath"
                },
                "author": "Venkatram Vishwanath",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13176v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13176v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15605v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15605v2",
                "updated": "2025-02-23T19:48:12Z",
                "updated_parsed": [
                    2025,
                    2,
                    23,
                    19,
                    48,
                    12,
                    6,
                    54,
                    0
                ],
                "published": "2024-12-20T06:58:32Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    6,
                    58,
                    32,
                    4,
                    355,
                    0
                ],
                "title": "Don't Do RAG: When Cache-Augmented Generation is All You Need for\n  Knowledge Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Don't Do RAG: When Cache-Augmented Generation is All You Need for\n  Knowledge Tasks"
                },
                "summary": "Retrieval-augmented generation (RAG) has gained traction as a powerful\napproach for enhancing language models by integrating external knowledge\nsources. However, RAG introduces challenges such as retrieval latency,\npotential errors in document selection, and increased system complexity. With\nthe advent of large language models (LLMs) featuring significantly extended\ncontext windows, this paper proposes an alternative paradigm, cache-augmented\ngeneration (CAG) that bypasses real-time retrieval. Our method involves\npreloading all relevant resources, especially when the documents or knowledge\nfor retrieval are of a limited and manageable size, into the LLM's extended\ncontext and caching its runtime parameters. During inference, the model\nutilizes these preloaded parameters to answer queries without additional\nretrieval steps. Comparative analyses reveal that CAG eliminates retrieval\nlatency and minimizes retrieval errors while maintaining context relevance.\nPerformance evaluations across multiple benchmarks highlight scenarios where\nlong-context LLMs either outperform or complement traditional RAG pipelines.\nThese findings suggest that, for certain applications, particularly those with\na constrained knowledge base, CAG provide a streamlined and efficient\nalternative to RAG, achieving comparable or superior results with reduced\ncomplexity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) has gained traction as a powerful\napproach for enhancing language models by integrating external knowledge\nsources. However, RAG introduces challenges such as retrieval latency,\npotential errors in document selection, and increased system complexity. With\nthe advent of large language models (LLMs) featuring significantly extended\ncontext windows, this paper proposes an alternative paradigm, cache-augmented\ngeneration (CAG) that bypasses real-time retrieval. Our method involves\npreloading all relevant resources, especially when the documents or knowledge\nfor retrieval are of a limited and manageable size, into the LLM's extended\ncontext and caching its runtime parameters. During inference, the model\nutilizes these preloaded parameters to answer queries without additional\nretrieval steps. Comparative analyses reveal that CAG eliminates retrieval\nlatency and minimizes retrieval errors while maintaining context relevance.\nPerformance evaluations across multiple benchmarks highlight scenarios where\nlong-context LLMs either outperform or complement traditional RAG pipelines.\nThese findings suggest that, for certain applications, particularly those with\na constrained knowledge base, CAG provide a streamlined and efficient\nalternative to RAG, achieving comparable or superior results with reduced\ncomplexity."
                },
                "authors": [
                    {
                        "name": "Brian J Chan"
                    },
                    {
                        "name": "Chao-Ting Chen"
                    },
                    {
                        "name": "Jui-Hung Cheng"
                    },
                    {
                        "name": "Hen-Hsen Huang"
                    }
                ],
                "author_detail": {
                    "name": "Hen-Hsen Huang"
                },
                "author": "Hen-Hsen Huang",
                "arxiv_doi": "10.1145/3701716.3715490",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3701716.3715490",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.15605v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15605v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "5 pages, accepted by the Web Conference 2025 (WWW '25) as a short\n  paper",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.16632v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.16632v1",
                "updated": "2025-02-23T16:17:34Z",
                "updated_parsed": [
                    2025,
                    2,
                    23,
                    16,
                    17,
                    34,
                    6,
                    54,
                    0
                ],
                "published": "2025-02-23T16:17:34Z",
                "published_parsed": [
                    2025,
                    2,
                    23,
                    16,
                    17,
                    34,
                    6,
                    54,
                    0
                ],
                "title": "Simultaneously Transmitting And Reflecting Surfaces (STARS) for\n  Multi-Functional 6G",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simultaneously Transmitting And Reflecting Surfaces (STARS) for\n  Multi-Functional 6G"
                },
                "summary": "Simultaneously transmitting and reflecting surface (STARS) empowered\nmulti-functional 6G wireless networks are investigated. Starting with the\ncommunication functionality, various types of STARS are introduced in terms of\npower amplification capabilities, reciprocity features, and spatial density of\nelements. Then, three STARS-empowered wireless sensing architectures are\nproposed, namely STARS-aided monostatic sensing, STARS-enabled bistatic\nsensing, and sensing with target-mounted STARS, where the representative\nbenefits and application challenges are identified. Furthermore, promising\napplications of STARS for computing and caching functionalities are explored to\nimprove the computation efficiency and reduce the content delivery latency.\nFinally, recent standardization progress for reconfigurable intelligent\nsurfaces is presented for motivating the employment of STARS in\nmulti-functional 6G.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simultaneously transmitting and reflecting surface (STARS) empowered\nmulti-functional 6G wireless networks are investigated. Starting with the\ncommunication functionality, various types of STARS are introduced in terms of\npower amplification capabilities, reciprocity features, and spatial density of\nelements. Then, three STARS-empowered wireless sensing architectures are\nproposed, namely STARS-aided monostatic sensing, STARS-enabled bistatic\nsensing, and sensing with target-mounted STARS, where the representative\nbenefits and application challenges are identified. Furthermore, promising\napplications of STARS for computing and caching functionalities are explored to\nimprove the computation efficiency and reduce the content delivery latency.\nFinally, recent standardization progress for reconfigurable intelligent\nsurfaces is presented for motivating the employment of STARS in\nmulti-functional 6G."
                },
                "authors": [
                    {
                        "name": "Xidong Mu"
                    },
                    {
                        "name": "Zhaolin Wang"
                    },
                    {
                        "name": "Yuanwei Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yuanwei Liu"
                },
                "author": "Yuanwei Liu",
                "arxiv_doi": "10.1109/MNET.2024.3481293",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/MNET.2024.3481293",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.16632v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.16632v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "6 figures, 8 pages, published in IEEE Network",
                "arxiv_journal_ref": "in IEEE Network, vol. 39, no. 1, pp. 47-55, Jan. 2025",
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11855v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11855v3",
                "updated": "2025-02-23T11:52:45Z",
                "updated_parsed": [
                    2025,
                    2,
                    23,
                    11,
                    52,
                    45,
                    6,
                    54,
                    0
                ],
                "published": "2025-01-21T03:13:21Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    3,
                    13,
                    21,
                    1,
                    21,
                    0
                ],
                "title": "A New Construction Structure on Coded Caching with Linear\n  Subpacketization: Non-Half-Sum Disjoint Packing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A New Construction Structure on Coded Caching with Linear\n  Subpacketization: Non-Half-Sum Disjoint Packing"
                },
                "summary": "Coded caching is a promising technique to effectively reduce peak traffic by\nusing local caches and the multicast gains generated by these local caches. We\nprefer to design a coded caching scheme with the subpacketization $F$ and\ntransmission load $R$ as small as possible since these are the key metrics for\nevaluating the implementation complexity and transmission efficiency of the\nscheme, respectively. However, most of the existing coded caching schemes have\nlarge subpacketizations which grow exponentially with the number of users $K$,\nand there are a few schemes with linear subpacketizations which have large\ntransmission loads. In this paper, we focus on studying the linear\nsubpacketization, i.e., $K=F$, coded caching scheme with low transmission load.\nSpecifically, we first introduce a new combinatorial structure called\nnon-half-sum disjoint packing (NHSDP) which can be used to generate a coded\ncaching scheme with $K=F$. Then a class of new schemes is obtained by\nconstructing NHSDP. Theoretical and numerical comparisons show that (i)\ncompared to the existing schemes with linear subpacketization (to the number of\nusers), the proposed scheme achieves a lower load; (ii) compared to some\nexisting schemes with polynomial subpacketization, the proposed scheme can also\nachieve a lower load in some cases; (iii) compared to some existing schemes\nwith exponential subpacketization, the proposed scheme has loads close to those\nof these schemes in some cases. Moreover, the new concept of NHSDP is closely\nrelated to the classical combinatorial structures such as cyclic difference\npacking (CDP), non-three-term arithmetic progressions (NTAP), and perfect hash\nfamily (PHF). These connections indicate that NHSDP is an important\ncombinatorial structure in the field of combinatorial design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coded caching is a promising technique to effectively reduce peak traffic by\nusing local caches and the multicast gains generated by these local caches. We\nprefer to design a coded caching scheme with the subpacketization $F$ and\ntransmission load $R$ as small as possible since these are the key metrics for\nevaluating the implementation complexity and transmission efficiency of the\nscheme, respectively. However, most of the existing coded caching schemes have\nlarge subpacketizations which grow exponentially with the number of users $K$,\nand there are a few schemes with linear subpacketizations which have large\ntransmission loads. In this paper, we focus on studying the linear\nsubpacketization, i.e., $K=F$, coded caching scheme with low transmission load.\nSpecifically, we first introduce a new combinatorial structure called\nnon-half-sum disjoint packing (NHSDP) which can be used to generate a coded\ncaching scheme with $K=F$. Then a class of new schemes is obtained by\nconstructing NHSDP. Theoretical and numerical comparisons show that (i)\ncompared to the existing schemes with linear subpacketization (to the number of\nusers), the proposed scheme achieves a lower load; (ii) compared to some\nexisting schemes with polynomial subpacketization, the proposed scheme can also\nachieve a lower load in some cases; (iii) compared to some existing schemes\nwith exponential subpacketization, the proposed scheme has loads close to those\nof these schemes in some cases. Moreover, the new concept of NHSDP is closely\nrelated to the classical combinatorial structures such as cyclic difference\npacking (CDP), non-three-term arithmetic progressions (NTAP), and perfect hash\nfamily (PHF). These connections indicate that NHSDP is an important\ncombinatorial structure in the field of combinatorial design."
                },
                "authors": [
                    {
                        "name": "Minquan Cheng"
                    },
                    {
                        "name": "Huimei Wei"
                    },
                    {
                        "name": "Kai Wan"
                    },
                    {
                        "name": "Giuseppe Caire"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Caire"
                },
                "author": "Giuseppe Caire",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11855v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11855v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02088v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02088v4",
                "updated": "2025-02-23T03:27:01Z",
                "updated_parsed": [
                    2025,
                    2,
                    23,
                    3,
                    27,
                    1,
                    6,
                    54,
                    0
                ],
                "published": "2024-09-03T17:40:24Z",
                "published_parsed": [
                    2024,
                    9,
                    3,
                    17,
                    40,
                    24,
                    1,
                    247,
                    0
                ],
                "title": "Cache Coherence Over Disaggregated Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache Coherence Over Disaggregated Memory"
                },
                "summary": "Disaggregating memory from compute offers the opportunity to better utilize\nstranded memory in cloud data centers. It is important to cache data in the\ncompute nodes and maintain cache coherence across multiple compute nodes.\nHowever, the limited computing power on disaggregated memory servers makes\ntraditional cache coherence protocols suboptimal, particularly in the case of\nstranded memory. This paper introduces SELCC; a Shared-Exclusive Latch Cache\nCoherence protocol that maintains cache coherence without imposing any\ncomputational burden on the remote memory side. It aligns the state machine of\nthe shared-exclusive latch protocol with the MSI protocol , thereby ensuring\nboth atomicity of data access and cache coherence with sequential consistency.\nSELCC embeds cache-ownership metadata directly into the RDMA latch word,\nenabling efficient cache ownership management via RDMA atomic operations. SELCC\ncan serve as an abstraction layer over disaggregated memory with APIs that\nresemble main-memory accesses. A concurrent B-tree and three transaction\nconcurrency control algorithms are realized using SELCC's abstraction layer.\nExperimental results show that SELCC significantly outperforms\nRemote-Procedure-Call-based protocols for cache coherence under limited remote\ncomputing power. Applications on SELCC achieve comparable or superior\nperformance over disaggregated memory compared to competitors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disaggregating memory from compute offers the opportunity to better utilize\nstranded memory in cloud data centers. It is important to cache data in the\ncompute nodes and maintain cache coherence across multiple compute nodes.\nHowever, the limited computing power on disaggregated memory servers makes\ntraditional cache coherence protocols suboptimal, particularly in the case of\nstranded memory. This paper introduces SELCC; a Shared-Exclusive Latch Cache\nCoherence protocol that maintains cache coherence without imposing any\ncomputational burden on the remote memory side. It aligns the state machine of\nthe shared-exclusive latch protocol with the MSI protocol , thereby ensuring\nboth atomicity of data access and cache coherence with sequential consistency.\nSELCC embeds cache-ownership metadata directly into the RDMA latch word,\nenabling efficient cache ownership management via RDMA atomic operations. SELCC\ncan serve as an abstraction layer over disaggregated memory with APIs that\nresemble main-memory accesses. A concurrent B-tree and three transaction\nconcurrency control algorithms are realized using SELCC's abstraction layer.\nExperimental results show that SELCC significantly outperforms\nRemote-Procedure-Call-based protocols for cache coherence under limited remote\ncomputing power. Applications on SELCC achieve comparable or superior\nperformance over disaggregated memory compared to competitors."
                },
                "authors": [
                    {
                        "name": "Ruihong Wang"
                    },
                    {
                        "name": "Jianguo Wang"
                    },
                    {
                        "name": "Walid G. Aref"
                    }
                ],
                "author_detail": {
                    "name": "Walid G. Aref"
                },
                "author": "Walid G. Aref",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02088v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02088v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13502v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13502v2",
                "updated": "2025-02-22T22:32:08Z",
                "updated_parsed": [
                    2025,
                    2,
                    22,
                    22,
                    32,
                    8,
                    5,
                    53,
                    0
                ],
                "published": "2025-02-19T07:43:36Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    7,
                    43,
                    36,
                    2,
                    50,
                    0
                ],
                "title": "PLDR-LLMs Learn A Generalizable Tensor Operator That Can Replace Its Own\n  Deep Neural Net At Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PLDR-LLMs Learn A Generalizable Tensor Operator That Can Replace Its Own\n  Deep Neural Net At Inference"
                },
                "summary": "We show that Large Language Model from Power Law Decoder Representations\n(PLDR-LLM) is a foundational model whose deductive outputs are invariant\ntensors up to a small perturbation. PLDR-LLM learns a singularity condition for\nthe deductive outputs that enable the once-inferred energy-curvature tensor\n$\\mathbf{G}_{LM}$ to replace the deep neural network of power law graph\nattention (PLGA) generating the deductive outputs at inference. We demonstrate\nthat a cache for $\\mathbf{G}_{LM}$ (G-cache) and KV-cache can be implemented in\na straightforward manner to improve the inference time. The invariance and\ngeneralizable nature of deductive outputs is at a very high fidelity where\ndeductive outputs have same RMSE and determinant values up to 15 decimal places\nafter caching, and zero-shot benchmark scores remain unchanged. Ablation\nstudies show that learned deductive outputs have distinct loss and accuracy\ncharacteristics from models pretrained with transferred, randomly initialized\nor identity tensors as a constant tensor operator and an LLM with scaled-dot\nproduct attention (SDPA) is a special case of PLDR-LLM where $\\mathbf{G}_{LM}$\nis predefined as identity. The observed invariance characteristic introduces a\nnovel asymmetry between training and inference phases with caching. We outline\nobserved common characteristics of the deductive outputs for the learned\nsingularity condition. We provide an implementation of a training and inference\nframework for PLDR-LLM with KV-cache and G-cache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We show that Large Language Model from Power Law Decoder Representations\n(PLDR-LLM) is a foundational model whose deductive outputs are invariant\ntensors up to a small perturbation. PLDR-LLM learns a singularity condition for\nthe deductive outputs that enable the once-inferred energy-curvature tensor\n$\\mathbf{G}_{LM}$ to replace the deep neural network of power law graph\nattention (PLGA) generating the deductive outputs at inference. We demonstrate\nthat a cache for $\\mathbf{G}_{LM}$ (G-cache) and KV-cache can be implemented in\na straightforward manner to improve the inference time. The invariance and\ngeneralizable nature of deductive outputs is at a very high fidelity where\ndeductive outputs have same RMSE and determinant values up to 15 decimal places\nafter caching, and zero-shot benchmark scores remain unchanged. Ablation\nstudies show that learned deductive outputs have distinct loss and accuracy\ncharacteristics from models pretrained with transferred, randomly initialized\nor identity tensors as a constant tensor operator and an LLM with scaled-dot\nproduct attention (SDPA) is a special case of PLDR-LLM where $\\mathbf{G}_{LM}$\nis predefined as identity. The observed invariance characteristic introduces a\nnovel asymmetry between training and inference phases with caching. We outline\nobserved common characteristics of the deductive outputs for the learned\nsingularity condition. We provide an implementation of a training and inference\nframework for PLDR-LLM with KV-cache and G-cache."
                },
                "authors": [
                    {
                        "name": "Burc Gokden"
                    }
                ],
                "author_detail": {
                    "name": "Burc Gokden"
                },
                "author": "Burc Gokden",
                "arxiv_comment": "15 pages, 1 figure, 12 tables, more ablation data included",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13502v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13502v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.16235v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.16235v1",
                "updated": "2025-02-22T14:13:37Z",
                "updated_parsed": [
                    2025,
                    2,
                    22,
                    14,
                    13,
                    37,
                    5,
                    53,
                    0
                ],
                "published": "2025-02-22T14:13:37Z",
                "published_parsed": [
                    2025,
                    2,
                    22,
                    14,
                    13,
                    37,
                    5,
                    53,
                    0
                ],
                "title": "Dynamic Parallel Tree Search for Efficient LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Parallel Tree Search for Efficient LLM Reasoning"
                },
                "summary": "Tree of Thoughts (ToT) enhances Large Language Model (LLM) reasoning by\nstructuring problem-solving as a spanning tree. However, recent methods focus\non search accuracy while overlooking computational efficiency. The challenges\nof accelerating the ToT lie in the frequent switching of reasoning focus, and\nthe redundant exploration of suboptimal solutions. To alleviate this dilemma,\nwe propose Dynamic Parallel Tree Search (DPTS), a novel parallelism framework\nthat aims to dynamically optimize the reasoning path in inference. It includes\nthe Parallelism Streamline in the generation phase to build up a flexible and\nadaptive parallelism with arbitrary paths by fine-grained cache management and\nalignment. Meanwhile, the Search and Transition Mechanism filters potential\ncandidates to dynamically maintain the reasoning focus on more possible\nsolutions and have less redundancy. Experiments on Qwen-2.5 and Llama-3 with\nMath500 and GSM8K datasets show that DPTS significantly improves efficiency by\n2-4x on average while maintaining or even surpassing existing reasoning\nalgorithms in accuracy, making ToT-based reasoning more scalable and\ncomputationally efficient.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tree of Thoughts (ToT) enhances Large Language Model (LLM) reasoning by\nstructuring problem-solving as a spanning tree. However, recent methods focus\non search accuracy while overlooking computational efficiency. The challenges\nof accelerating the ToT lie in the frequent switching of reasoning focus, and\nthe redundant exploration of suboptimal solutions. To alleviate this dilemma,\nwe propose Dynamic Parallel Tree Search (DPTS), a novel parallelism framework\nthat aims to dynamically optimize the reasoning path in inference. It includes\nthe Parallelism Streamline in the generation phase to build up a flexible and\nadaptive parallelism with arbitrary paths by fine-grained cache management and\nalignment. Meanwhile, the Search and Transition Mechanism filters potential\ncandidates to dynamically maintain the reasoning focus on more possible\nsolutions and have less redundancy. Experiments on Qwen-2.5 and Llama-3 with\nMath500 and GSM8K datasets show that DPTS significantly improves efficiency by\n2-4x on average while maintaining or even surpassing existing reasoning\nalgorithms in accuracy, making ToT-based reasoning more scalable and\ncomputationally efficient."
                },
                "authors": [
                    {
                        "name": "Yifu Ding"
                    },
                    {
                        "name": "Wentao Jiang"
                    },
                    {
                        "name": "Shunyu Liu"
                    },
                    {
                        "name": "Yongcheng Jing"
                    },
                    {
                        "name": "Jinyang Guo"
                    },
                    {
                        "name": "Yingjie Wang"
                    },
                    {
                        "name": "Jing Zhang"
                    },
                    {
                        "name": "Zengmao Wang"
                    },
                    {
                        "name": "Ziwei Liu"
                    },
                    {
                        "name": "Bo Du"
                    },
                    {
                        "name": "Xianglong Liu"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "arxiv_comment": "17 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.16235v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.16235v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.15197v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.15197v3",
                "updated": "2025-02-22T10:31:51Z",
                "updated_parsed": [
                    2025,
                    2,
                    22,
                    10,
                    31,
                    51,
                    5,
                    53,
                    0
                ],
                "published": "2024-05-24T04:00:04Z",
                "published_parsed": [
                    2024,
                    5,
                    24,
                    4,
                    0,
                    4,
                    4,
                    145,
                    0
                ],
                "title": "Warp-centric GPU meta-meshing and fast triangulation of billion-scale\n  lattice structures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Warp-centric GPU meta-meshing and fast triangulation of billion-scale\n  lattice structures"
                },
                "summary": "Lattice structures have been widely used in applications due to their\nsuperior mechanical properties. To fabricate such structures, a geometric\nprocessing step called triangulation is often employed to transform them into\nthe STL format before sending them to 3D printers. Because lattice structures\ntend to have high geometric complexity, this step usually generates a large\namount of triangles, a memory and compute-intensive task. This problem\nmanifests itself clearly through large-scale lattice structures that have\nmillions or billions of struts. To address this problem, this paper proposes to\ntransform a lattice structure into an intermediate model called meta-mesh\nbefore undergoing real triangulation. Compared to triangular meshes,\nmeta-meshes are very lightweight and much less compute-demanding. The meta-mesh\ncan also work as a base mesh reusable for conveniently and efficiently\ntriangulating lattice structures with arbitrary resolutions. A CPU+GPU\nasynchronous meta-meshing pipeline has been developed to efficiently generate\nmeta-meshes from lattice structures. It shifts from the thread-centric GPU\nalgorithm design paradigm commonly used in CAD to the recent warp-centric\ndesign paradigm to achieve high performance. This is achieved by a new data\ncompression method, a GPU cache-aware data structure, and a workload-balanced\nscheduling method that can significantly reduce memory divergence and branch\ndivergence. Experimenting with various billion-scale lattice structures, the\nproposed method is seen to be two orders of magnitude faster than previously\nachievable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lattice structures have been widely used in applications due to their\nsuperior mechanical properties. To fabricate such structures, a geometric\nprocessing step called triangulation is often employed to transform them into\nthe STL format before sending them to 3D printers. Because lattice structures\ntend to have high geometric complexity, this step usually generates a large\namount of triangles, a memory and compute-intensive task. This problem\nmanifests itself clearly through large-scale lattice structures that have\nmillions or billions of struts. To address this problem, this paper proposes to\ntransform a lattice structure into an intermediate model called meta-mesh\nbefore undergoing real triangulation. Compared to triangular meshes,\nmeta-meshes are very lightweight and much less compute-demanding. The meta-mesh\ncan also work as a base mesh reusable for conveniently and efficiently\ntriangulating lattice structures with arbitrary resolutions. A CPU+GPU\nasynchronous meta-meshing pipeline has been developed to efficiently generate\nmeta-meshes from lattice structures. It shifts from the thread-centric GPU\nalgorithm design paradigm commonly used in CAD to the recent warp-centric\ndesign paradigm to achieve high performance. This is achieved by a new data\ncompression method, a GPU cache-aware data structure, and a workload-balanced\nscheduling method that can significantly reduce memory divergence and branch\ndivergence. Experimenting with various billion-scale lattice structures, the\nproposed method is seen to be two orders of magnitude faster than previously\nachievable."
                },
                "authors": [
                    {
                        "name": "Qiang Zou"
                    },
                    {
                        "name": "Yunzhu Gao"
                    }
                ],
                "author_detail": {
                    "name": "Yunzhu Gao"
                },
                "author": "Yunzhu Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.15197v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.15197v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.16002v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.16002v1",
                "updated": "2025-02-21T23:34:29Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    23,
                    34,
                    29,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T23:34:29Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    23,
                    34,
                    29,
                    4,
                    52,
                    0
                ],
                "title": "KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse"
                },
                "summary": "We describe KVLink, an approach for efficient key-value (KV) cache reuse in\nlarge language models (LLMs). In many LLM applications, different inputs can\nshare overlapping context, such as the same retrieved document appearing in\nmultiple queries. However, the LLMs still need to encode the entire context for\neach query, leading to redundant computation. In this paper, we propose a new\nstrategy to eliminate such inefficiency, where the KV cache of each document is\nprecomputed independently. During inference, the KV caches of retrieved\ndocuments are concatenated, allowing the model to reuse cached representations\ninstead of recomputing them. To mitigate the performance degradation of LLMs\nwhen using KV caches computed independently for each document, KVLink\nintroduces three key components: adjusting positional embeddings of the KV\ncache at inference to match the global position after concatenation, using\ntrainable special tokens to restore self-attention across independently encoded\ndocuments, and applying mixed-data fine-tuning to enhance performance while\npreserving the model's original capabilities. Experiments across 7 datasets\ndemonstrate that KVLink improves question answering accuracy by an average of\n4% over state-of-the-art methods. Furthermore, by leveraging precomputed KV\ncaches, our approach reduces time-to-first-token by up to 90% compared to\nstandard LLM inference, making it a scalable and efficient solution for context\nreuse.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We describe KVLink, an approach for efficient key-value (KV) cache reuse in\nlarge language models (LLMs). In many LLM applications, different inputs can\nshare overlapping context, such as the same retrieved document appearing in\nmultiple queries. However, the LLMs still need to encode the entire context for\neach query, leading to redundant computation. In this paper, we propose a new\nstrategy to eliminate such inefficiency, where the KV cache of each document is\nprecomputed independently. During inference, the KV caches of retrieved\ndocuments are concatenated, allowing the model to reuse cached representations\ninstead of recomputing them. To mitigate the performance degradation of LLMs\nwhen using KV caches computed independently for each document, KVLink\nintroduces three key components: adjusting positional embeddings of the KV\ncache at inference to match the global position after concatenation, using\ntrainable special tokens to restore self-attention across independently encoded\ndocuments, and applying mixed-data fine-tuning to enhance performance while\npreserving the model's original capabilities. Experiments across 7 datasets\ndemonstrate that KVLink improves question answering accuracy by an average of\n4% over state-of-the-art methods. Furthermore, by leveraging precomputed KV\ncaches, our approach reduces time-to-first-token by up to 90% compared to\nstandard LLM inference, making it a scalable and efficient solution for context\nreuse."
                },
                "authors": [
                    {
                        "name": "Jingbo Yang"
                    },
                    {
                        "name": "Bairu Hou"
                    },
                    {
                        "name": "Wei Wei"
                    },
                    {
                        "name": "Yujia Bao"
                    },
                    {
                        "name": "Shiyu Chang"
                    }
                ],
                "author_detail": {
                    "name": "Shiyu Chang"
                },
                "author": "Shiyu Chang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.16002v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.16002v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15955v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15955v1",
                "updated": "2025-02-21T21:37:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    21,
                    37,
                    52,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T21:37:52Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    21,
                    37,
                    52,
                    4,
                    52,
                    0
                ],
                "title": "Compression Barriers for Autoregressive Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compression Barriers for Autoregressive Transformers"
                },
                "summary": "A key limitation of autoregressive Transformers is the large memory needed at\ninference-time to cache all previous key-value (KV) embeddings. Prior works\naddress this by compressing the KV cache, but often assume specific structural\nproperties of the embeddings. This raises the following natural question: Can\ntruly sublinear space utilization be achieved without such assumptions? In this\nwork, we answer this question in the negative. Any algorithm for\nattention-based token generation must use $\\Theta(nd)$ space, where $n$ is the\nnumber of tokens generated so far and $d = \\Omega(\\log n)$ is the dimension of\nthe KV embeddings. Our proof involves a reduction from a classic communication\ncomplexity problem and uses a randomized construction that leverages properties\nof projections in the spirit of the Johnson-Linderstrauss lemma. For the\nlow-dimensional regime $d = o(\\log n)$, we show that any algorithm requires\n$\\Omega(d\\cdot e^d)$ space and prove, using tight bounds on covering numbers,\nthat SubGen, proposed by Zandieh, Han, Mirrokni and Karbasi, matches this\nbound. Further, we investigate how sparsity assumptions enable token generation\nin truly sublinear space, presenting impossibility results and proposing a new\nKV cache compression algorithm for sliding window attention when the value\ncache outside the window is unmasked. Finally, we analyze token generation's\ntime complexity, using an indistinguishability argument to prove that no\nnon-adaptive algorithm can compute attention online in sublinear time for all\ntokens.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A key limitation of autoregressive Transformers is the large memory needed at\ninference-time to cache all previous key-value (KV) embeddings. Prior works\naddress this by compressing the KV cache, but often assume specific structural\nproperties of the embeddings. This raises the following natural question: Can\ntruly sublinear space utilization be achieved without such assumptions? In this\nwork, we answer this question in the negative. Any algorithm for\nattention-based token generation must use $\\Theta(nd)$ space, where $n$ is the\nnumber of tokens generated so far and $d = \\Omega(\\log n)$ is the dimension of\nthe KV embeddings. Our proof involves a reduction from a classic communication\ncomplexity problem and uses a randomized construction that leverages properties\nof projections in the spirit of the Johnson-Linderstrauss lemma. For the\nlow-dimensional regime $d = o(\\log n)$, we show that any algorithm requires\n$\\Omega(d\\cdot e^d)$ space and prove, using tight bounds on covering numbers,\nthat SubGen, proposed by Zandieh, Han, Mirrokni and Karbasi, matches this\nbound. Further, we investigate how sparsity assumptions enable token generation\nin truly sublinear space, presenting impossibility results and proposing a new\nKV cache compression algorithm for sliding window attention when the value\ncache outside the window is unmasked. Finally, we analyze token generation's\ntime complexity, using an indistinguishability argument to prove that no\nnon-adaptive algorithm can compute attention online in sublinear time for all\ntokens."
                },
                "authors": [
                    {
                        "name": "Themistoklis Haris"
                    },
                    {
                        "name": "Krzysztof Onak"
                    }
                ],
                "author_detail": {
                    "name": "Krzysztof Onak"
                },
                "author": "Krzysztof Onak",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15955v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15955v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14488v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14488v2",
                "updated": "2025-02-21T13:35:43Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    13,
                    35,
                    43,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-20T12:09:34Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    12,
                    9,
                    34,
                    3,
                    51,
                    0
                ],
                "title": "U-index: A Universal Indexing Framework for Matching Long Patterns",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "U-index: A Universal Indexing Framework for Matching Long Patterns"
                },
                "summary": "Text indexing is a fundamental and well-studied problem. Classic solutions\neither replace the original text with a compressed representation, e.g., the\nFM-index and its variants, or keep it uncompressed but attach some redundancy -\nan index - to accelerate matching. The former solutions thus retain excellent\ncompressed space, but areslow in practice. The latter approaches, like the\nsuffix array, instead sacrifice space for speed.\n  We show that efficient text indexing can be achieved using just a small extra\nspace on top of the original text, provided that the query patterns are\nsufficiently long. More specifically, we develop a new indexing paradigm in\nwhich a sketch of a query pattern is first matched against a sketch of the\ntext. Once candidate matches are retrieved, they are verified using the\noriginal text. This paradigm is thus universal in the sense that it allows us\nto use any solution to index the sketched text, like a suffix array, FM-index,\nor r-index.\n  We explore both the theory and the practice of this universal framework. With\nan extensive experimental analysis, we show that, surprisingly, universal\nindexes can be constructed much faster than their unsketched counterparts and\ntake a fraction of the space, as a direct consequence of (i) having a lower\nbound on the length of patterns and (ii) working in sketch space. Furthermore,\nthese data structures have the potential of retaining or even improving query\ntime, because matching against the sketched text is faster and verifying\ncandidates can be theoretically done in constant time per occurrence (or, in\npractice, by short and cache-friendly scans of the text). Finally, we discuss\nsome important applications of this novel indexing paradigm to computational\nbiology. We hypothesize that such indexes will be particularly effective when\nthe queries are sufficiently long, and so demonstrate applications in long-read\nmapping.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text indexing is a fundamental and well-studied problem. Classic solutions\neither replace the original text with a compressed representation, e.g., the\nFM-index and its variants, or keep it uncompressed but attach some redundancy -\nan index - to accelerate matching. The former solutions thus retain excellent\ncompressed space, but areslow in practice. The latter approaches, like the\nsuffix array, instead sacrifice space for speed.\n  We show that efficient text indexing can be achieved using just a small extra\nspace on top of the original text, provided that the query patterns are\nsufficiently long. More specifically, we develop a new indexing paradigm in\nwhich a sketch of a query pattern is first matched against a sketch of the\ntext. Once candidate matches are retrieved, they are verified using the\noriginal text. This paradigm is thus universal in the sense that it allows us\nto use any solution to index the sketched text, like a suffix array, FM-index,\nor r-index.\n  We explore both the theory and the practice of this universal framework. With\nan extensive experimental analysis, we show that, surprisingly, universal\nindexes can be constructed much faster than their unsketched counterparts and\ntake a fraction of the space, as a direct consequence of (i) having a lower\nbound on the length of patterns and (ii) working in sketch space. Furthermore,\nthese data structures have the potential of retaining or even improving query\ntime, because matching against the sketched text is faster and verifying\ncandidates can be theoretically done in constant time per occurrence (or, in\npractice, by short and cache-friendly scans of the text). Finally, we discuss\nsome important applications of this novel indexing paradigm to computational\nbiology. We hypothesize that such indexes will be particularly effective when\nthe queries are sufficiently long, and so demonstrate applications in long-read\nmapping."
                },
                "authors": [
                    {
                        "name": "Lorraine A. K. Ayad"
                    },
                    {
                        "name": "Gabriele Fici"
                    },
                    {
                        "name": "Ragnar Groot Koerkamp"
                    },
                    {
                        "name": "Grigorios Loukides"
                    },
                    {
                        "name": "Rob Patro"
                    },
                    {
                        "name": "Giulio Ermanno Pibiri"
                    },
                    {
                        "name": "Solon P. Pissis"
                    }
                ],
                "author_detail": {
                    "name": "Solon P. Pissis"
                },
                "author": "Solon P. Pissis",
                "arxiv_comment": "18 pages, 6 figures, code available at\n  https://github.com/u-index/u-index-rs",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14488v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14488v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "F.2.2; J.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17501v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17501v1",
                "updated": "2025-02-21T12:03:07Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    12,
                    3,
                    7,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T12:03:07Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    12,
                    3,
                    7,
                    4,
                    52,
                    0
                ],
                "title": "CoKV: Optimizing KV Cache Allocation via Cooperative Game",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoKV: Optimizing KV Cache Allocation via Cooperative Game"
                },
                "summary": "Large language models (LLMs) have achieved remarkable success on various\naspects of human life. However, one of the major challenges in deploying these\nmodels is the substantial memory consumption required to store key-value pairs\n(KV), which imposes significant resource demands. Recent research has focused\non KV cache budget allocation, with several approaches proposing head-level\nbudget distribution by evaluating the importance of individual attention heads.\nThese methods, however, assess the importance of heads independently,\noverlooking their cooperative contributions within the model, which may result\nin a deviation from their true impact on model performance. In light of this\nlimitation, we propose CoKV, a novel method that models the cooperation between\nheads in model inference as a cooperative game. By evaluating the contribution\nof each head within the cooperative game, CoKV can allocate the cache budget\nmore effectively. Extensive experiments show that CoKV achieves\nstate-of-the-art performance on the LongBench benchmark using\nLLama-3-8B-Instruct and Mistral-7B models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved remarkable success on various\naspects of human life. However, one of the major challenges in deploying these\nmodels is the substantial memory consumption required to store key-value pairs\n(KV), which imposes significant resource demands. Recent research has focused\non KV cache budget allocation, with several approaches proposing head-level\nbudget distribution by evaluating the importance of individual attention heads.\nThese methods, however, assess the importance of heads independently,\noverlooking their cooperative contributions within the model, which may result\nin a deviation from their true impact on model performance. In light of this\nlimitation, we propose CoKV, a novel method that models the cooperation between\nheads in model inference as a cooperative game. By evaluating the contribution\nof each head within the cooperative game, CoKV can allocate the cache budget\nmore effectively. Extensive experiments show that CoKV achieves\nstate-of-the-art performance on the LongBench benchmark using\nLLama-3-8B-Instruct and Mistral-7B models."
                },
                "authors": [
                    {
                        "name": "Qiheng Sun"
                    },
                    {
                        "name": "Hongwei Zhang"
                    },
                    {
                        "name": "Haocheng Xia"
                    },
                    {
                        "name": "Jiayao Zhang"
                    },
                    {
                        "name": "Jinfei Liu"
                    },
                    {
                        "name": "Kui Ren"
                    }
                ],
                "author_detail": {
                    "name": "Kui Ren"
                },
                "author": "Kui Ren",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17501v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17501v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15304v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15304v1",
                "updated": "2025-02-21T08:55:21Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    8,
                    55,
                    21,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T08:55:21Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    8,
                    55,
                    21,
                    4,
                    52,
                    0
                ],
                "title": "SVDq: 1.25-bit and 410x Key Cache Compression for LLM Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SVDq: 1.25-bit and 410x Key Cache Compression for LLM Attention"
                },
                "summary": "For the efficient inference of Large Language Models (LLMs), the effective\ncompression of key-value (KV) cache is essential. Three main types of KV cache\ncompression techniques, namely sparsity, channel compression, and quantization,\nhave been identified. This study presents SVDq, a Singular Value Decomposition\n(SVD) - based mixed precision quantization method for K cache. Initially, K\ncache is transformed into latent channels using SVD basis representations.\nSince the values in latent channels decay rapidly and become negligible after\nonly a few latent channels, our method then incorporates importance-aware\nquantization and compression for latent channels. This enables the effective\nallocation of higher precision to more significant channels. Theoretically, we\nprove that SVDq results in quantization errors (x0.1 or even lower) that are\nmuch lower than those of per-channel key quantization in the original space.\nOur findings based on RULER and LongBench benchmarks demonstrate that SVDq can\nachieve an equivalent key cache precision as low as 1.25-bit. When combined\nwith key sparsity, it can reach a key compression ratio of up to 410x for\nattention computation, all while maintaining comparable model performance.\nNotably, our method is nearly lossless for LongBench datasets. This indicates\nthat SVDq enables high-precision low-bit quantization, providing a more\nefficient solution for KV cache compression in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "For the efficient inference of Large Language Models (LLMs), the effective\ncompression of key-value (KV) cache is essential. Three main types of KV cache\ncompression techniques, namely sparsity, channel compression, and quantization,\nhave been identified. This study presents SVDq, a Singular Value Decomposition\n(SVD) - based mixed precision quantization method for K cache. Initially, K\ncache is transformed into latent channels using SVD basis representations.\nSince the values in latent channels decay rapidly and become negligible after\nonly a few latent channels, our method then incorporates importance-aware\nquantization and compression for latent channels. This enables the effective\nallocation of higher precision to more significant channels. Theoretically, we\nprove that SVDq results in quantization errors (x0.1 or even lower) that are\nmuch lower than those of per-channel key quantization in the original space.\nOur findings based on RULER and LongBench benchmarks demonstrate that SVDq can\nachieve an equivalent key cache precision as low as 1.25-bit. When combined\nwith key sparsity, it can reach a key compression ratio of up to 410x for\nattention computation, all while maintaining comparable model performance.\nNotably, our method is nearly lossless for LongBench datasets. This indicates\nthat SVDq enables high-precision low-bit quantization, providing a more\nefficient solution for KV cache compression in LLMs."
                },
                "authors": [
                    {
                        "name": "Hong Yankun"
                    },
                    {
                        "name": "Li Xing"
                    },
                    {
                        "name": "Zhen Hui-Ling"
                    },
                    {
                        "name": "Yu Xianzhi"
                    },
                    {
                        "name": "Liu Wulong"
                    },
                    {
                        "name": "Yuan Mingxuan"
                    }
                ],
                "author_detail": {
                    "name": "Yuan Mingxuan"
                },
                "author": "Yuan Mingxuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15304v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15304v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15192v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15192v1",
                "updated": "2025-02-21T04:07:00Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    4,
                    7,
                    0,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T04:07:00Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    4,
                    7,
                    0,
                    4,
                    52,
                    0
                ],
                "title": "SAAP: Spatial awareness and Association based Prefetching of Virtual\n  Objects in Augmented Reality at the Edge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SAAP: Spatial awareness and Association based Prefetching of Virtual\n  Objects in Augmented Reality at the Edge"
                },
                "summary": "Mobile Augmented Reality (MAR) applications face performance challenges due\nto their high computational demands and need for low-latency responses.\nTraditional approaches like on-device storage or reactive data fetching from\nthe cloud often result in limited AR experiences or unacceptable lag. Edge\ncaching, which caches AR objects closer to the user, provides a promising\nsolution. However, existing edge caching approaches do not consider AR-specific\nfeatures such as AR object sizes, user interactions, and physical location.\nThis paper investigates how to further optimize edge caching by employing\nAR-aware prefetching techniques. We present SAAP, a Spatial Awareness and\nAssociation-based Prefetching policy specifically designed for MAR Caches. SAAP\nintelligently prioritizes the caching of virtual objects based on their\nassociation with other similar objects and the user's proximity to them. It\nalso considers the recency of associations and uses a lazy fetching strategy to\nefficiently manage edge resources and maximize Quality of Experience (QoE).\n  Through extensive evaluation using both synthetic and real-world workloads,\nwe demonstrate that SAAP significantly improves cache hit rates compared to\nstandard caching algorithms, achieving gains ranging from 3\\% to 40\\% while\nreducing the need for on-demand data retrieval from the cloud. Further, we\npresent an adaptive tuning algorithm that automatically tunes SAAP parameters\nto achieve optimal performance. Our findings demonstrate the potential of SAAP\nto substantially enhance the user experience in MAR applications by ensuring\nthe timely availability of virtual objects.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mobile Augmented Reality (MAR) applications face performance challenges due\nto their high computational demands and need for low-latency responses.\nTraditional approaches like on-device storage or reactive data fetching from\nthe cloud often result in limited AR experiences or unacceptable lag. Edge\ncaching, which caches AR objects closer to the user, provides a promising\nsolution. However, existing edge caching approaches do not consider AR-specific\nfeatures such as AR object sizes, user interactions, and physical location.\nThis paper investigates how to further optimize edge caching by employing\nAR-aware prefetching techniques. We present SAAP, a Spatial Awareness and\nAssociation-based Prefetching policy specifically designed for MAR Caches. SAAP\nintelligently prioritizes the caching of virtual objects based on their\nassociation with other similar objects and the user's proximity to them. It\nalso considers the recency of associations and uses a lazy fetching strategy to\nefficiently manage edge resources and maximize Quality of Experience (QoE).\n  Through extensive evaluation using both synthetic and real-world workloads,\nwe demonstrate that SAAP significantly improves cache hit rates compared to\nstandard caching algorithms, achieving gains ranging from 3\\% to 40\\% while\nreducing the need for on-demand data retrieval from the cloud. Further, we\npresent an adaptive tuning algorithm that automatically tunes SAAP parameters\nto achieve optimal performance. Our findings demonstrate the potential of SAAP\nto substantially enhance the user experience in MAR applications by ensuring\nthe timely availability of virtual objects."
                },
                "authors": [
                    {
                        "name": "Nikhil Sreekumar"
                    },
                    {
                        "name": "Abhishek Chandra"
                    },
                    {
                        "name": "Jon Weissman"
                    }
                ],
                "author_detail": {
                    "name": "Jon Weissman"
                },
                "author": "Jon Weissman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15192v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15192v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.ET",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03065v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03065v2",
                "updated": "2025-02-20T23:28:01Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    23,
                    28,
                    1,
                    3,
                    51,
                    0
                ],
                "published": "2024-10-04T01:11:09Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    1,
                    11,
                    9,
                    4,
                    278,
                    0
                ],
                "title": "Compute Or Load KV Cache? Why Not Both?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compute Or Load KV Cache? Why Not Both?"
                },
                "summary": "Large Language Models (LLMs) are increasingly deployed in large-scale online\nservices, enabling sophisticated applications. However, the computational\noverhead of generating key-value (KV) caches in the prefill stage presents a\nmajor bottleneck, particularly for long-context inputs. Prefix caching\nmitigates this issue by storing KV caches for reuse, reducing redundant\ncomputation. Despite its advantages, prefix caching suffers from high latency\ndue to the limited I/O bandwidth of storage devices, constraining inference\nefficiency. To address this challenge, we introduce Cake, a novel KV cache\nloading system that optimally utilizes both computational and I/O resources in\nparallel. Cake employs a bidirectional scheduling strategy that dynamically\nbalances KV cache computation and loading, ensuring efficient resource\nutilization. Additionally, Cake incorporates an adaptive scheduling mechanism\nthat seamlessly integrates with non-prefix caching requests, improving system\nthroughput and adapting to fluctuating resource availabilty. Through extensive\nevaluations across various hardware configurations, datasets, and storage\nconditions, Cake achieves on average 2.6x reduction in Time to First Token\n(TTFT) compared to compute-only and I/O-only methods. Our findings highlight\nCake as an effective and practical solution for optimizing long-context LLM\ninference, bridging the gap between computation and I/O efficiency in\nlarge-scale AI deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly deployed in large-scale online\nservices, enabling sophisticated applications. However, the computational\noverhead of generating key-value (KV) caches in the prefill stage presents a\nmajor bottleneck, particularly for long-context inputs. Prefix caching\nmitigates this issue by storing KV caches for reuse, reducing redundant\ncomputation. Despite its advantages, prefix caching suffers from high latency\ndue to the limited I/O bandwidth of storage devices, constraining inference\nefficiency. To address this challenge, we introduce Cake, a novel KV cache\nloading system that optimally utilizes both computational and I/O resources in\nparallel. Cake employs a bidirectional scheduling strategy that dynamically\nbalances KV cache computation and loading, ensuring efficient resource\nutilization. Additionally, Cake incorporates an adaptive scheduling mechanism\nthat seamlessly integrates with non-prefix caching requests, improving system\nthroughput and adapting to fluctuating resource availabilty. Through extensive\nevaluations across various hardware configurations, datasets, and storage\nconditions, Cake achieves on average 2.6x reduction in Time to First Token\n(TTFT) compared to compute-only and I/O-only methods. Our findings highlight\nCake as an effective and practical solution for optimizing long-context LLM\ninference, bridging the gap between computation and I/O efficiency in\nlarge-scale AI deployments."
                },
                "authors": [
                    {
                        "name": "Shuowei Jin"
                    },
                    {
                        "name": "Xueshen Liu"
                    },
                    {
                        "name": "Qingzhao Zhang"
                    },
                    {
                        "name": "Z. Morley Mao"
                    }
                ],
                "author_detail": {
                    "name": "Z. Morley Mao"
                },
                "author": "Z. Morley Mao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03065v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03065v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15075v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15075v1",
                "updated": "2025-02-20T22:24:27Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    22,
                    24,
                    27,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T22:24:27Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    22,
                    24,
                    27,
                    3,
                    51,
                    0
                ],
                "title": "More for Keys, Less for Values: Adaptive KV Cache Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "More for Keys, Less for Values: Adaptive KV Cache Quantization"
                },
                "summary": "This paper introduces an information-aware quantization framework that\nadaptively compresses the key-value (KV) cache in large language models (LLMs).\nAlthough prior work has underscored the distinct roles of key and value cache\nduring inference, our systematic analysis -- examining singular value\ndistributions, spectral norms, and Frobenius norms -- reveals, for the first\ntime, that key matrices consistently exhibit higher norm values and are more\nsensitive to quantization than value matrices. Furthermore, our theoretical\nanalysis shows that matrices with higher spectral norms amplify quantization\nerrors more significantly. Motivated by these insights, we propose a\nmixed-precision quantization strategy, KV-AdaQuant, which allocates more\nbit-width for keys and fewer for values since key matrices have higher norm\nvalues. With the same total KV bit budget, this approach effectively mitigates\nerror propagation across transformer layers while achieving significant memory\nsavings. Our extensive experiments on multiple LLMs (1B--70B) demonstrate that\nour mixed-precision quantization scheme maintains high model accuracy even\nunder aggressive compression. For instance, using 4-bit for Key and 2-bit for\nValue achieves an accuracy of 75.2%, whereas reversing the assignment (2-bit\nfor Key and 4-bit for Value) yields only 54.7% accuracy. The code is available\nat https://tinyurl.com/kv-adaquant",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces an information-aware quantization framework that\nadaptively compresses the key-value (KV) cache in large language models (LLMs).\nAlthough prior work has underscored the distinct roles of key and value cache\nduring inference, our systematic analysis -- examining singular value\ndistributions, spectral norms, and Frobenius norms -- reveals, for the first\ntime, that key matrices consistently exhibit higher norm values and are more\nsensitive to quantization than value matrices. Furthermore, our theoretical\nanalysis shows that matrices with higher spectral norms amplify quantization\nerrors more significantly. Motivated by these insights, we propose a\nmixed-precision quantization strategy, KV-AdaQuant, which allocates more\nbit-width for keys and fewer for values since key matrices have higher norm\nvalues. With the same total KV bit budget, this approach effectively mitigates\nerror propagation across transformer layers while achieving significant memory\nsavings. Our extensive experiments on multiple LLMs (1B--70B) demonstrate that\nour mixed-precision quantization scheme maintains high model accuracy even\nunder aggressive compression. For instance, using 4-bit for Key and 2-bit for\nValue achieves an accuracy of 75.2%, whereas reversing the assignment (2-bit\nfor Key and 4-bit for Value) yields only 54.7% accuracy. The code is available\nat https://tinyurl.com/kv-adaquant"
                },
                "authors": [
                    {
                        "name": "Mohsen Hariri"
                    },
                    {
                        "name": "Lam Nguyen"
                    },
                    {
                        "name": "Sixu Chen"
                    },
                    {
                        "name": "Shaochen Zhong"
                    },
                    {
                        "name": "Qifan Wang"
                    },
                    {
                        "name": "Xia Hu"
                    },
                    {
                        "name": "Xiaotian Han"
                    },
                    {
                        "name": "Vipin Chaudhary"
                    }
                ],
                "author_detail": {
                    "name": "Vipin Chaudhary"
                },
                "author": "Vipin Chaudhary",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15075v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15075v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14866v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14866v1",
                "updated": "2025-02-20T18:59:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    59,
                    52,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T18:59:52Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    59,
                    52,
                    3,
                    51,
                    0
                ],
                "title": "LServe: Efficient Long-sequence LLM Serving with Unified Sparse\n  Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LServe: Efficient Long-sequence LLM Serving with Unified Sparse\n  Attention"
                },
                "summary": "Large language models (LLMs) have shown remarkable potential in processing\nlong sequences, yet efficiently serving these long-context models remains\nchallenging due to the quadratic computational complexity of attention in the\nprefilling stage and the large memory footprint of the KV cache in the decoding\nstage. To address these issues, we introduce LServe, an efficient system that\naccelerates long-sequence LLM serving via hybrid sparse attention. This method\nunifies different hardware-friendly, structured sparsity patterns for both\nprefilling and decoding attention into a single framework, where computations\non less important tokens are skipped block-wise. LServe demonstrates the\ncompatibility of static and dynamic sparsity in long-context LLM attention.\nThis design enables multiplicative speedups by combining these optimizations.\nSpecifically, we convert half of the attention heads to nearly free streaming\nheads in both the prefilling and decoding stages. Additionally, we find that\nonly a constant number of KV pages is required to preserve long-context\ncapabilities, irrespective of context length. We then design a hierarchical KV\npage selection policy that dynamically prunes KV pages based on query-centric\nsimilarity. On average, LServe accelerates LLM prefilling by up to 2.9x and\ndecoding by 1.3-2.1x over vLLM, maintaining long-context accuracy. Code is\nreleased at https://github.com/mit-han-lab/omniserve.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown remarkable potential in processing\nlong sequences, yet efficiently serving these long-context models remains\nchallenging due to the quadratic computational complexity of attention in the\nprefilling stage and the large memory footprint of the KV cache in the decoding\nstage. To address these issues, we introduce LServe, an efficient system that\naccelerates long-sequence LLM serving via hybrid sparse attention. This method\nunifies different hardware-friendly, structured sparsity patterns for both\nprefilling and decoding attention into a single framework, where computations\non less important tokens are skipped block-wise. LServe demonstrates the\ncompatibility of static and dynamic sparsity in long-context LLM attention.\nThis design enables multiplicative speedups by combining these optimizations.\nSpecifically, we convert half of the attention heads to nearly free streaming\nheads in both the prefilling and decoding stages. Additionally, we find that\nonly a constant number of KV pages is required to preserve long-context\ncapabilities, irrespective of context length. We then design a hierarchical KV\npage selection policy that dynamically prunes KV pages based on query-centric\nsimilarity. On average, LServe accelerates LLM prefilling by up to 2.9x and\ndecoding by 1.3-2.1x over vLLM, maintaining long-context accuracy. Code is\nreleased at https://github.com/mit-han-lab/omniserve."
                },
                "authors": [
                    {
                        "name": "Shang Yang"
                    },
                    {
                        "name": "Junxian Guo"
                    },
                    {
                        "name": "Haotian Tang"
                    },
                    {
                        "name": "Qinghao Hu"
                    },
                    {
                        "name": "Guangxuan Xiao"
                    },
                    {
                        "name": "Jiaming Tang"
                    },
                    {
                        "name": "Yujun Lin"
                    },
                    {
                        "name": "Zhijian Liu"
                    },
                    {
                        "name": "Yao Lu"
                    },
                    {
                        "name": "Song Han"
                    }
                ],
                "author_detail": {
                    "name": "Song Han"
                },
                "author": "Song Han",
                "arxiv_comment": "Accepted by MLSys 2025. Code available at:\n  https://github.com/mit-han-lab/omniserve",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14866v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14866v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14837v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14837v1",
                "updated": "2025-02-20T18:50:42Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    50,
                    42,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T18:50:42Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    50,
                    42,
                    3,
                    51,
                    0
                ],
                "title": "Towards Economical Inference: Enabling DeepSeek's Multi-Head Latent\n  Attention in Any Transformer-based LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Economical Inference: Enabling DeepSeek's Multi-Head Latent\n  Attention in Any Transformer-based LLMs"
                },
                "summary": "Multi-head Latent Attention (MLA) is an innovative architecture proposed by\nDeepSeek, designed to ensure efficient and economical inference by\nsignificantly compressing the Key-Value (KV) cache into a latent vector.\nCompared to MLA, standard LLMs employing Multi-Head Attention (MHA) and its\nvariants such as Grouped-Query Attention (GQA) exhibit significant cost\ndisadvantages. Enabling well-trained LLMs (e.g., Llama) to rapidly adapt to MLA\nwithout pre-training from scratch is both meaningful and challenging. This\npaper proposes the first data-efficient fine-tuning method for transitioning\nfrom MHA to MLA (MHA2MLA), which includes two key components: for partial-RoPE,\nwe remove RoPE from dimensions of queries and keys that contribute less to the\nattention scores, for low-rank approximation, we introduce joint SVD\napproximations based on the pre-trained parameters of keys and values. These\ncarefully designed strategies enable MHA2MLA to recover performance using only\na small fraction (0.3% to 0.6%) of the data, significantly reducing inference\ncosts while seamlessly integrating with compression techniques such as KV cache\nquantization. For example, the KV cache size of Llama2-7B is reduced by 92.19%,\nwith only a 0.5% drop in LongBench performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-head Latent Attention (MLA) is an innovative architecture proposed by\nDeepSeek, designed to ensure efficient and economical inference by\nsignificantly compressing the Key-Value (KV) cache into a latent vector.\nCompared to MLA, standard LLMs employing Multi-Head Attention (MHA) and its\nvariants such as Grouped-Query Attention (GQA) exhibit significant cost\ndisadvantages. Enabling well-trained LLMs (e.g., Llama) to rapidly adapt to MLA\nwithout pre-training from scratch is both meaningful and challenging. This\npaper proposes the first data-efficient fine-tuning method for transitioning\nfrom MHA to MLA (MHA2MLA), which includes two key components: for partial-RoPE,\nwe remove RoPE from dimensions of queries and keys that contribute less to the\nattention scores, for low-rank approximation, we introduce joint SVD\napproximations based on the pre-trained parameters of keys and values. These\ncarefully designed strategies enable MHA2MLA to recover performance using only\na small fraction (0.3% to 0.6%) of the data, significantly reducing inference\ncosts while seamlessly integrating with compression techniques such as KV cache\nquantization. For example, the KV cache size of Llama2-7B is reduced by 92.19%,\nwith only a 0.5% drop in LongBench performance."
                },
                "authors": [
                    {
                        "name": "Tao Ji"
                    },
                    {
                        "name": "Bin Guo"
                    },
                    {
                        "name": "Yuanbin Wu"
                    },
                    {
                        "name": "Qipeng Guo"
                    },
                    {
                        "name": "Lixing Shen"
                    },
                    {
                        "name": "Zhan Chen"
                    },
                    {
                        "name": "Xipeng Qiu"
                    },
                    {
                        "name": "Qi Zhang"
                    },
                    {
                        "name": "Tao Gui"
                    }
                ],
                "author_detail": {
                    "name": "Tao Gui"
                },
                "author": "Tao Gui",
                "arxiv_comment": "16 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14837v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14837v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19392v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19392v3",
                "updated": "2025-02-20T16:01:34Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    16,
                    1,
                    34,
                    3,
                    51,
                    0
                ],
                "published": "2025-01-31T18:47:42Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    18,
                    47,
                    42,
                    4,
                    31,
                    0
                ],
                "title": "Cache Me If You Must: Adaptive Key-Value Quantization for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache Me If You Must: Adaptive Key-Value Quantization for Large Language\n  Models"
                },
                "summary": "Efficient real-world deployments of large language models (LLMs) rely on\nKey-Value (KV) caching for processing and generating long outputs, reducing the\nneed for repetitive computation. For large contexts, Key-Value caches can take\nup tens of gigabytes of device memory, as they store vector representations for\neach token and layer. Recent work has shown that the cached vectors can be\ncompressed through quantization, pruning or merging, but these techniques often\ncompromise quality towards higher compression rates. In this work, we aim to\nimprove Key & Value compression by exploiting two observations: 1) the inherent\ndependencies between keys and values across different layers, and 2)\nhigh-compression mechanisms for internal network states. We propose AQUA-KV, an\nadaptive quantization for Key-Value caches that relies on compact adapters to\nexploit existing dependencies between Keys and Values, and aims to \"optimally\"\ncompress the information that cannot be predicted. AQUA-KV significantly\nimproves compression rates, while maintaining high accuracy on state-of-the-art\nLLM families. On Llama 3.2 LLMs, we achieve near-lossless inference at 2-2.5\nbits per value with under $1\\%$ relative error in perplexity and LongBench\nscores. AQUA-KV is one-shot, simple, and efficient: it can be calibrated on a\nsingle GPU within 1-6 hours, even for 70B models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient real-world deployments of large language models (LLMs) rely on\nKey-Value (KV) caching for processing and generating long outputs, reducing the\nneed for repetitive computation. For large contexts, Key-Value caches can take\nup tens of gigabytes of device memory, as they store vector representations for\neach token and layer. Recent work has shown that the cached vectors can be\ncompressed through quantization, pruning or merging, but these techniques often\ncompromise quality towards higher compression rates. In this work, we aim to\nimprove Key & Value compression by exploiting two observations: 1) the inherent\ndependencies between keys and values across different layers, and 2)\nhigh-compression mechanisms for internal network states. We propose AQUA-KV, an\nadaptive quantization for Key-Value caches that relies on compact adapters to\nexploit existing dependencies between Keys and Values, and aims to \"optimally\"\ncompress the information that cannot be predicted. AQUA-KV significantly\nimproves compression rates, while maintaining high accuracy on state-of-the-art\nLLM families. On Llama 3.2 LLMs, we achieve near-lossless inference at 2-2.5\nbits per value with under $1\\%$ relative error in perplexity and LongBench\nscores. AQUA-KV is one-shot, simple, and efficient: it can be calibrated on a\nsingle GPU within 1-6 hours, even for 70B models."
                },
                "authors": [
                    {
                        "name": "Alina Shutova"
                    },
                    {
                        "name": "Vladimir Malinovskii"
                    },
                    {
                        "name": "Vage Egiazarian"
                    },
                    {
                        "name": "Denis Kuznedelev"
                    },
                    {
                        "name": "Denis Mazur"
                    },
                    {
                        "name": "Nikita Surkov"
                    },
                    {
                        "name": "Ivan Ermakov"
                    },
                    {
                        "name": "Dan Alistarh"
                    }
                ],
                "author_detail": {
                    "name": "Dan Alistarh"
                },
                "author": "Dan Alistarh",
                "arxiv_comment": "Preprint, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19392v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19392v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14938v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14938v1",
                "updated": "2025-02-20T14:01:17Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    14,
                    1,
                    17,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T14:01:17Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    14,
                    1,
                    17,
                    3,
                    51,
                    0
                ],
                "title": "GS-Cache: A GS-Cache Inference Framework for Large-scale Gaussian\n  Splatting Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GS-Cache: A GS-Cache Inference Framework for Large-scale Gaussian\n  Splatting Models"
                },
                "summary": "Rendering large-scale 3D Gaussian Splatting (3DGS) model faces significant\nchallenges in achieving real-time, high-fidelity performance on consumer-grade\ndevices. Fully realizing the potential of 3DGS in applications such as virtual\nreality (VR) requires addressing critical system-level challenges to support\nreal-time, immersive experiences. We propose GS-Cache, an end-to-end framework\nthat seamlessly integrates 3DGS's advanced representation with a highly\noptimized rendering system. GS-Cache introduces a cache-centric pipeline to\neliminate redundant computations, an efficiency-aware scheduler for elastic\nmulti-GPU rendering, and optimized CUDA kernels to overcome computational\nbottlenecks. This synergy between 3DGS and system design enables GS-Cache to\nachieve up to 5.35x performance improvement, 35% latency reduction, and 42%\nlower GPU memory usage, supporting 2K binocular rendering at over 120 FPS with\nhigh visual quality. By bridging the gap between 3DGS's representation power\nand the demands of VR systems, GS-Cache establishes a scalable and efficient\nframework for real-time neural rendering in immersive environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rendering large-scale 3D Gaussian Splatting (3DGS) model faces significant\nchallenges in achieving real-time, high-fidelity performance on consumer-grade\ndevices. Fully realizing the potential of 3DGS in applications such as virtual\nreality (VR) requires addressing critical system-level challenges to support\nreal-time, immersive experiences. We propose GS-Cache, an end-to-end framework\nthat seamlessly integrates 3DGS's advanced representation with a highly\noptimized rendering system. GS-Cache introduces a cache-centric pipeline to\neliminate redundant computations, an efficiency-aware scheduler for elastic\nmulti-GPU rendering, and optimized CUDA kernels to overcome computational\nbottlenecks. This synergy between 3DGS and system design enables GS-Cache to\nachieve up to 5.35x performance improvement, 35% latency reduction, and 42%\nlower GPU memory usage, supporting 2K binocular rendering at over 120 FPS with\nhigh visual quality. By bridging the gap between 3DGS's representation power\nand the demands of VR systems, GS-Cache establishes a scalable and efficient\nframework for real-time neural rendering in immersive environments."
                },
                "authors": [
                    {
                        "name": "Miao Tao"
                    },
                    {
                        "name": "Yuanzhen Zhou"
                    },
                    {
                        "name": "Haoran Xu"
                    },
                    {
                        "name": "Zeyu He"
                    },
                    {
                        "name": "Zhenyu Yang"
                    },
                    {
                        "name": "Yuchang Zhang"
                    },
                    {
                        "name": "Zhongling Su"
                    },
                    {
                        "name": "Linning Xu"
                    },
                    {
                        "name": "Zhenxiang Ma"
                    },
                    {
                        "name": "Rong Fu"
                    },
                    {
                        "name": "Hengjie Li"
                    },
                    {
                        "name": "Xingcheng Zhang"
                    },
                    {
                        "name": "Jidong Zhai"
                    }
                ],
                "author_detail": {
                    "name": "Jidong Zhai"
                },
                "author": "Jidong Zhai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14938v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14938v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14504v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14504v1",
                "updated": "2025-02-20T12:31:31Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    12,
                    31,
                    31,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T12:31:31Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    12,
                    31,
                    31,
                    3,
                    51,
                    0
                ],
                "title": "PLPHP: Per-Layer Per-Head Vision Token Pruning for Efficient Large\n  Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PLPHP: Per-Layer Per-Head Vision Token Pruning for Efficient Large\n  Vision-Language Models"
                },
                "summary": "Large Vision-Language Models (LVLMs) have demonstrated remarkable\ncapabilities across a range of multimodal tasks. However, their inference\nefficiency is constrained by the large number of visual tokens processed during\ndecoding. To address this challenge, we propose Per-Layer Per-Head Vision Token\nPruning (PLPHP), a two-level fine-grained pruning method including Layer-Level\nRetention Rate Allocation and Head-Level Vision Token Pruning. Motivated by the\nVision Token Re-attention phenomenon across decoder layers, we dynamically\nadjust token retention rates layer by layer. Layers that exhibit stronger\nattention to visual information preserve more vision tokens, while layers with\nlower vision attention are aggressively pruned. Furthermore, PLPHP applies\npruning at the attention head level, enabling different heads within the same\nlayer to independently retain critical context. Experiments on multiple\nbenchmarks demonstrate that PLPHP delivers an 18% faster decoding speed and\nreduces the Key-Value Cache (KV Cache) size by over 50%, all at the cost of\n0.46% average performance drop, while also achieving notable performance\nimprovements in multi-image tasks. These results highlight the effectiveness of\nfine-grained token pruning and contribute to advancing the efficiency and\nscalability of LVLMs. Our source code will be made publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Vision-Language Models (LVLMs) have demonstrated remarkable\ncapabilities across a range of multimodal tasks. However, their inference\nefficiency is constrained by the large number of visual tokens processed during\ndecoding. To address this challenge, we propose Per-Layer Per-Head Vision Token\nPruning (PLPHP), a two-level fine-grained pruning method including Layer-Level\nRetention Rate Allocation and Head-Level Vision Token Pruning. Motivated by the\nVision Token Re-attention phenomenon across decoder layers, we dynamically\nadjust token retention rates layer by layer. Layers that exhibit stronger\nattention to visual information preserve more vision tokens, while layers with\nlower vision attention are aggressively pruned. Furthermore, PLPHP applies\npruning at the attention head level, enabling different heads within the same\nlayer to independently retain critical context. Experiments on multiple\nbenchmarks demonstrate that PLPHP delivers an 18% faster decoding speed and\nreduces the Key-Value Cache (KV Cache) size by over 50%, all at the cost of\n0.46% average performance drop, while also achieving notable performance\nimprovements in multi-image tasks. These results highlight the effectiveness of\nfine-grained token pruning and contribute to advancing the efficiency and\nscalability of LVLMs. Our source code will be made publicly available."
                },
                "authors": [
                    {
                        "name": "Yu Meng"
                    },
                    {
                        "name": "Kaiyuan Li"
                    },
                    {
                        "name": "Chenran Huang"
                    },
                    {
                        "name": "Chen Gao"
                    },
                    {
                        "name": "Xinlei Chen"
                    },
                    {
                        "name": "Yong Li"
                    },
                    {
                        "name": "Xiaoping Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoping Zhang"
                },
                "author": "Xiaoping Zhang",
                "arxiv_comment": "12 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14504v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14504v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12706v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12706v2",
                "updated": "2025-02-20T12:14:49Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    12,
                    14,
                    49,
                    3,
                    51,
                    0
                ],
                "published": "2024-12-17T09:20:31Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    9,
                    20,
                    31,
                    1,
                    352,
                    0
                ],
                "title": "More Tokens, Lower Precision: Towards the Optimal Token-Precision\n  Trade-off in KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "More Tokens, Lower Precision: Towards the Optimal Token-Precision\n  Trade-off in KV Cache Compression"
                },
                "summary": "As large language models (LLMs) process increasing context windows, the\nmemory usage of KV cache has become a critical bottleneck during inference. The\nmainstream KV compression methods, including KV pruning and KV quantization,\nprimarily focus on either token or precision dimension separately. However,\nthese works leaving the trade-off between these two orthogonal dimensions\nlargely under-explored. In this paper, we comprehensively investigate the\ntoken-precision trade-off in KV cache compression.Experiments demonstrate that\nstoring more tokens in the KV cache with lower precision,a strategy we term\nquantized pruning, can significantly enhance the long-context performance of\nLLMs. In-depth analysis of the token-precision trade-off across key aspects\ndemonstrates that, quantized pruning achieves substantial improvements in\nretrieval-related tasks and consistently performs well across varying input\nlengths. Furthermore, quantized pruning demonstrates notable stability and\neffectiveness across different KV pruning methods, quantization strategies, and\nmodel scales. These findings offer valuable insights into optimizing KV cache\ncompression through balanced token-precision trade-off strategies. Our code is\navailable at https://github.com/zhzihao/QPruningKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) process increasing context windows, the\nmemory usage of KV cache has become a critical bottleneck during inference. The\nmainstream KV compression methods, including KV pruning and KV quantization,\nprimarily focus on either token or precision dimension separately. However,\nthese works leaving the trade-off between these two orthogonal dimensions\nlargely under-explored. In this paper, we comprehensively investigate the\ntoken-precision trade-off in KV cache compression.Experiments demonstrate that\nstoring more tokens in the KV cache with lower precision,a strategy we term\nquantized pruning, can significantly enhance the long-context performance of\nLLMs. In-depth analysis of the token-precision trade-off across key aspects\ndemonstrates that, quantized pruning achieves substantial improvements in\nretrieval-related tasks and consistently performs well across varying input\nlengths. Furthermore, quantized pruning demonstrates notable stability and\neffectiveness across different KV pruning methods, quantization strategies, and\nmodel scales. These findings offer valuable insights into optimizing KV cache\ncompression through balanced token-precision trade-off strategies. Our code is\navailable at https://github.com/zhzihao/QPruningKV."
                },
                "authors": [
                    {
                        "name": "Jiebin Zhang"
                    },
                    {
                        "name": "Dawei Zhu"
                    },
                    {
                        "name": "Yifan Song"
                    },
                    {
                        "name": "Wenhao Wu"
                    },
                    {
                        "name": "Chuqiao Kuang"
                    },
                    {
                        "name": "Xiaoguang Li"
                    },
                    {
                        "name": "Lifeng Shang"
                    },
                    {
                        "name": "Qun Liu"
                    },
                    {
                        "name": "Sujian Li"
                    }
                ],
                "author_detail": {
                    "name": "Sujian Li"
                },
                "author": "Sujian Li",
                "arxiv_comment": "13pages,9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12706v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12706v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13251v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13251v2",
                "updated": "2025-02-20T09:03:05Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    9,
                    3,
                    5,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-18T19:22:44Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    19,
                    22,
                    44,
                    1,
                    49,
                    0
                ],
                "title": "Neural Attention Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Attention Search"
                },
                "summary": "We present Neural Attention Search (NAtS), a framework that automatically\nevaluates the importance of each token within a sequence and determines if the\ncorresponding token can be dropped after several steps. This approach can\nefficiently reduce the KV cache sizes required by transformer-based models\nduring inference and thus reduce inference costs. In this paper, we design a\nsearch space that contains three token types: (i) Global Tokens will be\npreserved and queried by all the following tokens. (ii) Local Tokens survive\nuntil the next global token appears. (iii) Sliding Window Tokens have an impact\non the inference of a fixed size of the next following tokens. Similar to the\nOne-Shot Neural Architecture Search approach, this token-type information can\nbe learned jointly with the architecture weights via a learnable attention\nmask. Experiments on both training a new transformer from scratch and\nfine-tuning existing large language models show that NAtS can efficiently\nreduce the KV cache size required for the models while maintaining the models'\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Neural Attention Search (NAtS), a framework that automatically\nevaluates the importance of each token within a sequence and determines if the\ncorresponding token can be dropped after several steps. This approach can\nefficiently reduce the KV cache sizes required by transformer-based models\nduring inference and thus reduce inference costs. In this paper, we design a\nsearch space that contains three token types: (i) Global Tokens will be\npreserved and queried by all the following tokens. (ii) Local Tokens survive\nuntil the next global token appears. (iii) Sliding Window Tokens have an impact\non the inference of a fixed size of the next following tokens. Similar to the\nOne-Shot Neural Architecture Search approach, this token-type information can\nbe learned jointly with the architecture weights via a learnable attention\nmask. Experiments on both training a new transformer from scratch and\nfine-tuning existing large language models show that NAtS can efficiently\nreduce the KV cache size required for the models while maintaining the models'\nperformance."
                },
                "authors": [
                    {
                        "name": "Difan Deng"
                    },
                    {
                        "name": "Marius Lindauer"
                    }
                ],
                "author_detail": {
                    "name": "Marius Lindauer"
                },
                "author": "Marius Lindauer",
                "arxiv_comment": "18 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13251v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13251v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14347v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14347v1",
                "updated": "2025-02-20T08:00:25Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    8,
                    0,
                    25,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T08:00:25Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    8,
                    0,
                    25,
                    3,
                    51,
                    0
                ],
                "title": "Discovery of a new phase in thin flakes of KV$_{3}$Sb$_{5}$ under\n  pressure",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Discovery of a new phase in thin flakes of KV$_{3}$Sb$_{5}$ under\n  pressure"
                },
                "summary": "We report results of magnetotransport measurements on KV$_3$Sb$_5$ thin\nflakes under pressure. Our zero-field electrical resistance reveals an\nadditional anomaly emerging under pressure ($p$), marking a previously\nunidentified phase boundary $T^{\\rm \\ast}$($p$). Together with the established\n$T_{\\rm CDW}(p)$ and $T_c(p)$, denoting the charge-density-wave transition and\na superconducting transition, respectively, the temperature-pressure phase\ndiagram of KV$_3$Sb$_5$ features a rich interplay among multiple phases. The\nHall coefficient evolves reasonably smoothly when crossing the $T^{\\rm \\ast}$\nphase boundary compared with the variation when crossing $T_{\\rm CDW}$,\nindicating the preservation of the pristine electronic structure. The mobility\nspectrum analysis provides further insights into distinguishing different\nphases. Finally, our high-pressure quantum oscillation studies up to 31 T\ncombined with density functional theory calculations further demonstrate that\nthe new phase does not reconstruct the Fermi surface, confirming that the\ntranslational symmetry of the pristine metallic state is preserved.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We report results of magnetotransport measurements on KV$_3$Sb$_5$ thin\nflakes under pressure. Our zero-field electrical resistance reveals an\nadditional anomaly emerging under pressure ($p$), marking a previously\nunidentified phase boundary $T^{\\rm \\ast}$($p$). Together with the established\n$T_{\\rm CDW}(p)$ and $T_c(p)$, denoting the charge-density-wave transition and\na superconducting transition, respectively, the temperature-pressure phase\ndiagram of KV$_3$Sb$_5$ features a rich interplay among multiple phases. The\nHall coefficient evolves reasonably smoothly when crossing the $T^{\\rm \\ast}$\nphase boundary compared with the variation when crossing $T_{\\rm CDW}$,\nindicating the preservation of the pristine electronic structure. The mobility\nspectrum analysis provides further insights into distinguishing different\nphases. Finally, our high-pressure quantum oscillation studies up to 31 T\ncombined with density functional theory calculations further demonstrate that\nthe new phase does not reconstruct the Fermi surface, confirming that the\ntranslational symmetry of the pristine metallic state is preserved."
                },
                "authors": [
                    {
                        "name": "Zheyu Wang"
                    },
                    {
                        "name": "Lingfei Wang"
                    },
                    {
                        "name": "King Yau Yip"
                    },
                    {
                        "name": "Ying Kit Tsui"
                    },
                    {
                        "name": "Tsz Fung Poon"
                    },
                    {
                        "name": "Wenyan Wang"
                    },
                    {
                        "name": "Chun Wai Tsang"
                    },
                    {
                        "name": "Shanmin Wang"
                    },
                    {
                        "name": "David Graf"
                    },
                    {
                        "name": "Alexandre Pourret"
                    },
                    {
                        "name": "Gabriel Seyfarth"
                    },
                    {
                        "name": "Georg Knebel"
                    },
                    {
                        "name": "Kwing To Lai"
                    },
                    {
                        "name": "Wing Chi Yu"
                    },
                    {
                        "name": "Wei Zhang"
                    },
                    {
                        "name": "Swee K. Goh"
                    }
                ],
                "author_detail": {
                    "name": "Swee K. Goh"
                },
                "author": "Swee K. Goh",
                "arxiv_comment": "10 pages, 5 figures. Advanced Science (2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14347v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14347v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.supr-con",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.supr-con",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.str-el",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14317v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14317v1",
                "updated": "2025-02-20T07:10:43Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    7,
                    10,
                    43,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T07:10:43Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    7,
                    10,
                    43,
                    3,
                    51,
                    0
                ],
                "title": "ParallelComp: Parallel Long-Context Compressor for Length Extrapolation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ParallelComp: Parallel Long-Context Compressor for Length Extrapolation"
                },
                "summary": "Efficiently handling long contexts is crucial for large language models\n(LLMs). While rotary position embeddings (RoPEs) enhance length generalization,\neffective length extrapolation remains challenging and often requires costly\nfine-tuning. In contrast, recent training-free approaches suffer from the\nattention sink phenomenon, leading to severe performance degradation. In this\npaper, we introduce ParallelComp, a novel training-free method for long-context\nextrapolation that extends LLMs' context length from 4K to 128K while\nmaintaining high throughput and preserving perplexity, and integrates\nseamlessly with Flash Attention. Our analysis offers new insights into\nattention biases in parallel attention mechanisms and provides practical\nsolutions to tackle these challenges. To mitigate the attention sink issue, we\npropose an attention calibration strategy that reduces biases, ensuring more\nstable long-range attention. Additionally, we introduce a chunk eviction\nstrategy to efficiently manage ultra-long contexts on a single A100 80GB GPU.\nTo further enhance efficiency, we propose a parallel KV cache eviction\ntechnique, which improves chunk throughput by 1.76x, thereby achieving a 23.50x\nacceleration in the prefilling stage with negligible performance loss due to\nattention calibration. Furthermore, ParallelComp achieves 91.17% of GPT-4's\nperformance on long-context tasks using an 8B model trained on 8K-length\ncontext, outperforming powerful closed-source models such as Claude-2 and\nKimi-Chat.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficiently handling long contexts is crucial for large language models\n(LLMs). While rotary position embeddings (RoPEs) enhance length generalization,\neffective length extrapolation remains challenging and often requires costly\nfine-tuning. In contrast, recent training-free approaches suffer from the\nattention sink phenomenon, leading to severe performance degradation. In this\npaper, we introduce ParallelComp, a novel training-free method for long-context\nextrapolation that extends LLMs' context length from 4K to 128K while\nmaintaining high throughput and preserving perplexity, and integrates\nseamlessly with Flash Attention. Our analysis offers new insights into\nattention biases in parallel attention mechanisms and provides practical\nsolutions to tackle these challenges. To mitigate the attention sink issue, we\npropose an attention calibration strategy that reduces biases, ensuring more\nstable long-range attention. Additionally, we introduce a chunk eviction\nstrategy to efficiently manage ultra-long contexts on a single A100 80GB GPU.\nTo further enhance efficiency, we propose a parallel KV cache eviction\ntechnique, which improves chunk throughput by 1.76x, thereby achieving a 23.50x\nacceleration in the prefilling stage with negligible performance loss due to\nattention calibration. Furthermore, ParallelComp achieves 91.17% of GPT-4's\nperformance on long-context tasks using an 8B model trained on 8K-length\ncontext, outperforming powerful closed-source models such as Claude-2 and\nKimi-Chat."
                },
                "authors": [
                    {
                        "name": "Jing Xiong"
                    },
                    {
                        "name": "Jianghan Shen"
                    },
                    {
                        "name": "Chuanyang Zheng"
                    },
                    {
                        "name": "Zhongwei Wan"
                    },
                    {
                        "name": "Chenyang Zhao"
                    },
                    {
                        "name": "Chiwun Yang"
                    },
                    {
                        "name": "Fanghua Ye"
                    },
                    {
                        "name": "Hongxia Yang"
                    },
                    {
                        "name": "Lingpeng Kong"
                    },
                    {
                        "name": "Ngai Wong"
                    }
                ],
                "author_detail": {
                    "name": "Ngai Wong"
                },
                "author": "Ngai Wong",
                "arxiv_comment": "We will release the code soon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14317v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14317v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14307v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14307v1",
                "updated": "2025-02-20T06:42:03Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    6,
                    42,
                    3,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T06:42:03Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    6,
                    42,
                    3,
                    3,
                    51,
                    0
                ],
                "title": "μRL: Discovering Transient Execution Vulnerabilities Using\n  Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "μRL: Discovering Transient Execution Vulnerabilities Using\n  Reinforcement Learning"
                },
                "summary": "We propose using reinforcement learning to address the challenges of\ndiscovering microarchitectural vulnerabilities, such as Spectre and Meltdown,\nwhich exploit subtle interactions in modern processors. Traditional methods\nlike random fuzzing fail to efficiently explore the vast instruction space and\noften miss vulnerabilities that manifest under specific conditions. To overcome\nthis, we introduce an intelligent, feedback-driven approach using RL. Our RL\nagents interact with the processor, learning from real-time feedback to\nprioritize instruction sequences more likely to reveal vulnerabilities,\nsignificantly improving the efficiency of the discovery process.\n  We also demonstrate that RL systems adapt effectively to various\nmicroarchitectures, providing a scalable solution across processor generations.\nBy automating the exploration process, we reduce the need for human\nintervention, enabling continuous learning that uncovers hidden\nvulnerabilities. Additionally, our approach detects subtle signals, such as\ntiming anomalies or unusual cache behavior, that may indicate\nmicroarchitectural weaknesses. This proposal advances hardware security testing\nby introducing a more efficient, adaptive, and systematic framework for\nprotecting modern processors.\n  When unleashed on Intel Skylake-X and Raptor Lake microarchitectures, our RL\nagent was indeed able to generate instruction sequences that cause significant\nobservable byte leakages through transient execution without generating any\n$\\mu$code assists, faults or interrupts. The newly identified leaky sequences\nstem from a variety of Intel instructions, e.g. including SERIALIZE, VERR/VERW,\nCLMUL, MMX-x87 transitions, LSL+RDSCP and LAR. These initial results give\ncredence to the proposed approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose using reinforcement learning to address the challenges of\ndiscovering microarchitectural vulnerabilities, such as Spectre and Meltdown,\nwhich exploit subtle interactions in modern processors. Traditional methods\nlike random fuzzing fail to efficiently explore the vast instruction space and\noften miss vulnerabilities that manifest under specific conditions. To overcome\nthis, we introduce an intelligent, feedback-driven approach using RL. Our RL\nagents interact with the processor, learning from real-time feedback to\nprioritize instruction sequences more likely to reveal vulnerabilities,\nsignificantly improving the efficiency of the discovery process.\n  We also demonstrate that RL systems adapt effectively to various\nmicroarchitectures, providing a scalable solution across processor generations.\nBy automating the exploration process, we reduce the need for human\nintervention, enabling continuous learning that uncovers hidden\nvulnerabilities. Additionally, our approach detects subtle signals, such as\ntiming anomalies or unusual cache behavior, that may indicate\nmicroarchitectural weaknesses. This proposal advances hardware security testing\nby introducing a more efficient, adaptive, and systematic framework for\nprotecting modern processors.\n  When unleashed on Intel Skylake-X and Raptor Lake microarchitectures, our RL\nagent was indeed able to generate instruction sequences that cause significant\nobservable byte leakages through transient execution without generating any\n$\\mu$code assists, faults or interrupts. The newly identified leaky sequences\nstem from a variety of Intel instructions, e.g. including SERIALIZE, VERR/VERW,\nCLMUL, MMX-x87 transitions, LSL+RDSCP and LAR. These initial results give\ncredence to the proposed approach."
                },
                "authors": [
                    {
                        "name": "M. Caner Tol"
                    },
                    {
                        "name": "Kemal Derya"
                    },
                    {
                        "name": "Berk Sunar"
                    }
                ],
                "author_detail": {
                    "name": "Berk Sunar"
                },
                "author": "Berk Sunar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14307v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14307v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.16406v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.16406v4",
                "updated": "2025-02-20T06:07:00Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    6,
                    7,
                    0,
                    3,
                    51,
                    0
                ],
                "published": "2024-05-26T02:15:49Z",
                "published_parsed": [
                    2024,
                    5,
                    26,
                    2,
                    15,
                    49,
                    6,
                    147,
                    0
                ],
                "title": "SpinQuant: LLM quantization with learned rotations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpinQuant: LLM quantization with learned rotations"
                },
                "summary": "Post-training quantization (PTQ) techniques applied to weights, activations,\nand the KV cache greatly reduce memory usage, latency, and power consumption of\nLarge Language Models (LLMs), but may lead to large quantization errors when\noutliers are present. Rotating activation or weight matrices helps remove\noutliers and benefits quantization. In this work, we identify a collection of\napplicable rotation parameterizations that lead to identical outputs in\nfull-precision Transformer architectures while enhancing quantization accuracy.\nIn addition, we find that some random rotations lead to much better\nquantization than others, with an up to 13 points difference in downstream\nzero-shot reasoning performance. As a result, we propose SpinQuant, a novel\napproach that incorporates learned rotation matrices for optimal quantized\nnetwork accuracy. With 4-bit quantization of weight, activation, and KV-cache,\nSpinQuant narrows the accuracy gap on zero-shot reasoning tasks with full\nprecision to merely 2.9 points on the LLaMA-2 7B model, surpassing LLM-QAT by\n19.1 points and SmoothQuant by 25.0 points. Furthermore, SpinQuant also\noutperforms concurrent work QuaRot, which applies random rotations to remove\noutliers. In particular, for LLaMA-3 8B models that are hard to quantize,\nSpinQuant reduces the gap to full precision by up to 45.1% relative to QuaRot.\nCode is available at https://github.com/facebookresearch/SpinQuant.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-training quantization (PTQ) techniques applied to weights, activations,\nand the KV cache greatly reduce memory usage, latency, and power consumption of\nLarge Language Models (LLMs), but may lead to large quantization errors when\noutliers are present. Rotating activation or weight matrices helps remove\noutliers and benefits quantization. In this work, we identify a collection of\napplicable rotation parameterizations that lead to identical outputs in\nfull-precision Transformer architectures while enhancing quantization accuracy.\nIn addition, we find that some random rotations lead to much better\nquantization than others, with an up to 13 points difference in downstream\nzero-shot reasoning performance. As a result, we propose SpinQuant, a novel\napproach that incorporates learned rotation matrices for optimal quantized\nnetwork accuracy. With 4-bit quantization of weight, activation, and KV-cache,\nSpinQuant narrows the accuracy gap on zero-shot reasoning tasks with full\nprecision to merely 2.9 points on the LLaMA-2 7B model, surpassing LLM-QAT by\n19.1 points and SmoothQuant by 25.0 points. Furthermore, SpinQuant also\noutperforms concurrent work QuaRot, which applies random rotations to remove\noutliers. In particular, for LLaMA-3 8B models that are hard to quantize,\nSpinQuant reduces the gap to full precision by up to 45.1% relative to QuaRot.\nCode is available at https://github.com/facebookresearch/SpinQuant."
                },
                "authors": [
                    {
                        "name": "Zechun Liu"
                    },
                    {
                        "name": "Changsheng Zhao"
                    },
                    {
                        "name": "Igor Fedorov"
                    },
                    {
                        "name": "Bilge Soran"
                    },
                    {
                        "name": "Dhruv Choudhary"
                    },
                    {
                        "name": "Raghuraman Krishnamoorthi"
                    },
                    {
                        "name": "Vikas Chandra"
                    },
                    {
                        "name": "Yuandong Tian"
                    },
                    {
                        "name": "Tijmen Blankevoort"
                    }
                ],
                "author_detail": {
                    "name": "Tijmen Blankevoort"
                },
                "author": "Tijmen Blankevoort",
                "arxiv_comment": "ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.16406v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.16406v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14280v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14280v1",
                "updated": "2025-02-20T05:41:15Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    5,
                    41,
                    15,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T05:41:15Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    5,
                    41,
                    15,
                    3,
                    51,
                    0
                ],
                "title": "EpMAN: Episodic Memory AttentioN for Generalizing to Longer Contexts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EpMAN: Episodic Memory AttentioN for Generalizing to Longer Contexts"
                },
                "summary": "Recent advances in Large Language Models (LLMs) have yielded impressive\nsuccesses on many language tasks. However, efficient processing of long\ncontexts using LLMs remains a significant challenge. We introduce\n\\textbf{EpMAN} -- a method for processing long contexts in an \\textit{episodic\nmemory} module while \\textit{holistically attending to} semantically relevant\ncontext chunks. The output of \\textit{episodic attention} is then used to\nreweigh the decoder's self-attention to the stored KV cache of the context\nduring training and generation. When an LLM decoder is trained using\n\\textbf{EpMAN}, its performance on multiple challenging single-hop long-context\nrecall and question-answering benchmarks is found to be stronger and more\nrobust across the range from 16k to 256k tokens than baseline decoders trained\nwith self-attention, and popular retrieval-augmented generation frameworks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Large Language Models (LLMs) have yielded impressive\nsuccesses on many language tasks. However, efficient processing of long\ncontexts using LLMs remains a significant challenge. We introduce\n\\textbf{EpMAN} -- a method for processing long contexts in an \\textit{episodic\nmemory} module while \\textit{holistically attending to} semantically relevant\ncontext chunks. The output of \\textit{episodic attention} is then used to\nreweigh the decoder's self-attention to the stored KV cache of the context\nduring training and generation. When an LLM decoder is trained using\n\\textbf{EpMAN}, its performance on multiple challenging single-hop long-context\nrecall and question-answering benchmarks is found to be stronger and more\nrobust across the range from 16k to 256k tokens than baseline decoders trained\nwith self-attention, and popular retrieval-augmented generation frameworks."
                },
                "authors": [
                    {
                        "name": "Subhajit Chaudhury"
                    },
                    {
                        "name": "Payel Das"
                    },
                    {
                        "name": "Sarathkrishna Swaminathan"
                    },
                    {
                        "name": "Georgios Kollias"
                    },
                    {
                        "name": "Elliot Nelson"
                    },
                    {
                        "name": "Khushbu Pahwa"
                    },
                    {
                        "name": "Tejaswini Pedapati"
                    },
                    {
                        "name": "Igor Melnyk"
                    },
                    {
                        "name": "Matthew Riemer"
                    }
                ],
                "author_detail": {
                    "name": "Matthew Riemer"
                },
                "author": "Matthew Riemer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14280v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14280v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14220v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14220v1",
                "updated": "2025-02-20T03:27:00Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    3,
                    27,
                    0,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T03:27:00Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    3,
                    27,
                    0,
                    3,
                    51,
                    0
                ],
                "title": "NDPage: Efficient Address Translation for Near-Data Processing\n  Architectures via Tailored Page Table",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NDPage: Efficient Address Translation for Near-Data Processing\n  Architectures via Tailored Page Table"
                },
                "summary": "Near-Data Processing (NDP) has been a promising architectural paradigm to\naddress the memory wall problem for data-intensive applications. Practical\nimplementation of NDP architectures calls for system support for better\nprogrammability, where having virtual memory (VM) is critical. Modern computing\nsystems incorporate a 4-level page table design to support address translation\nin VM. However, simply adopting an existing 4-level page table in NDP systems\ncauses significant address translation overhead because (1) NDP applications\ngenerate a lot of address translations, and (2) the limited L1 cache in NDP\nsystems cannot cover the accesses to page table entries (PTEs). We extensively\nanalyze the 4-level page table design in the NDP scenario and observe that (1)\nthe memory access to page table entries is highly irregular, thus cannot\nbenefit from the L1 cache, and (2) the last two levels of page tables are\nnearly fully occupied. Based on our observations, we propose NDPage, an\nefficient page table design tailored for NDP systems. The key mechanisms of\nNDPage are (1) an L1 cache bypass mechanism for PTEs that not only accelerates\nthe memory accesses of PTEs but also prevents the pollution of PTEs in the\ncache system, and (2) a flattened page table design that merges the last two\nlevels of page tables, allowing the page table to enjoy the flexibility of a\n4KB page while reducing the number of PTE accesses. We evaluate NDPage using a\nvariety of data-intensive workloads. Our evaluation shows that in a single-core\nNDP system, NDPage improves the end-to-end performance over the\nstate-of-the-art address translation mechanism of 14.3\\%; in 4-core and 8-core\nNDP systems, NDPage enhances the performance of 9.8\\% and 30.5\\%, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Near-Data Processing (NDP) has been a promising architectural paradigm to\naddress the memory wall problem for data-intensive applications. Practical\nimplementation of NDP architectures calls for system support for better\nprogrammability, where having virtual memory (VM) is critical. Modern computing\nsystems incorporate a 4-level page table design to support address translation\nin VM. However, simply adopting an existing 4-level page table in NDP systems\ncauses significant address translation overhead because (1) NDP applications\ngenerate a lot of address translations, and (2) the limited L1 cache in NDP\nsystems cannot cover the accesses to page table entries (PTEs). We extensively\nanalyze the 4-level page table design in the NDP scenario and observe that (1)\nthe memory access to page table entries is highly irregular, thus cannot\nbenefit from the L1 cache, and (2) the last two levels of page tables are\nnearly fully occupied. Based on our observations, we propose NDPage, an\nefficient page table design tailored for NDP systems. The key mechanisms of\nNDPage are (1) an L1 cache bypass mechanism for PTEs that not only accelerates\nthe memory accesses of PTEs but also prevents the pollution of PTEs in the\ncache system, and (2) a flattened page table design that merges the last two\nlevels of page tables, allowing the page table to enjoy the flexibility of a\n4KB page while reducing the number of PTE accesses. We evaluate NDPage using a\nvariety of data-intensive workloads. Our evaluation shows that in a single-core\nNDP system, NDPage improves the end-to-end performance over the\nstate-of-the-art address translation mechanism of 14.3\\%; in 4-core and 8-core\nNDP systems, NDPage enhances the performance of 9.8\\% and 30.5\\%, respectively."
                },
                "authors": [
                    {
                        "name": "Qingcai Jiang"
                    },
                    {
                        "name": "Buxin Tu"
                    },
                    {
                        "name": "Hong An"
                    }
                ],
                "author_detail": {
                    "name": "Hong An"
                },
                "author": "Hong An",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14220v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14220v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14051v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14051v1",
                "updated": "2025-02-19T19:12:46Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    19,
                    12,
                    46,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T19:12:46Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    19,
                    12,
                    46,
                    2,
                    50,
                    0
                ],
                "title": "RocketKV: Accelerating Long-Context LLM Inference via Two-Stage KV Cache\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RocketKV: Accelerating Long-Context LLM Inference via Two-Stage KV Cache\n  Compression"
                },
                "summary": "Transformer-based Large Language Models rely critically on KV cache to\nefficiently handle extended contexts during the decode phase. Yet, the size of\nthe KV cache grows proportionally with the input length, burdening both memory\nbandwidth and capacity as decoding progresses. To address this challenge, we\npresent RocketKV, a training-free KV cache compression strategy designed\nspecifically to reduce both memory bandwidth and capacity demand of KV cache\nduring the decode phase. RocketKV contains two consecutive stages. In the first\nstage, it performs coarse-grain KV cache eviction on the input sequence tokens\nwith SnapKV++, a method improved upon SnapKV by introducing adaptive pooling\nsize and full compatibility with grouped-query attention. In the second stage,\nit adopts a hybrid attention method to conduct fine-grain top-k sparse\nattention, approximating the attention scores by leveraging both head and\nsequence dimensional reductions. Combining these two stages, RocketKV achieves\nsignificant KV cache fetching bandwidth and storage savings while maintaining\ncomparable accuracy to full KV cache attention. We show that RocketKV provides\nend-to-end speedup by up to 3$\\times$ as well as peak memory reduction by up to\n31% in the decode phase on an NVIDIA H100 GPU compared to the full KV cache\nbaseline, while achieving negligible accuracy loss on a variety of long-context\ntasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based Large Language Models rely critically on KV cache to\nefficiently handle extended contexts during the decode phase. Yet, the size of\nthe KV cache grows proportionally with the input length, burdening both memory\nbandwidth and capacity as decoding progresses. To address this challenge, we\npresent RocketKV, a training-free KV cache compression strategy designed\nspecifically to reduce both memory bandwidth and capacity demand of KV cache\nduring the decode phase. RocketKV contains two consecutive stages. In the first\nstage, it performs coarse-grain KV cache eviction on the input sequence tokens\nwith SnapKV++, a method improved upon SnapKV by introducing adaptive pooling\nsize and full compatibility with grouped-query attention. In the second stage,\nit adopts a hybrid attention method to conduct fine-grain top-k sparse\nattention, approximating the attention scores by leveraging both head and\nsequence dimensional reductions. Combining these two stages, RocketKV achieves\nsignificant KV cache fetching bandwidth and storage savings while maintaining\ncomparable accuracy to full KV cache attention. We show that RocketKV provides\nend-to-end speedup by up to 3$\\times$ as well as peak memory reduction by up to\n31% in the decode phase on an NVIDIA H100 GPU compared to the full KV cache\nbaseline, while achieving negligible accuracy loss on a variety of long-context\ntasks."
                },
                "authors": [
                    {
                        "name": "Payman Behnam"
                    },
                    {
                        "name": "Yaosheng Fu"
                    },
                    {
                        "name": "Ritchie Zhao"
                    },
                    {
                        "name": "Po-An Tsai"
                    },
                    {
                        "name": "Zhiding Yu"
                    },
                    {
                        "name": "Alexey Tumanov"
                    }
                ],
                "author_detail": {
                    "name": "Alexey Tumanov"
                },
                "author": "Alexey Tumanov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14051v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14051v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17897v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17897v4",
                "updated": "2025-02-19T17:53:11Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    17,
                    53,
                    11,
                    2,
                    50,
                    0
                ],
                "published": "2024-10-23T14:15:07Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    14,
                    15,
                    7,
                    2,
                    297,
                    0
                ],
                "title": "Value Residual Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Value Residual Learning"
                },
                "summary": "While Transformer models have achieved remarkable success in various domains,\nthe effectiveness of information propagation through deep networks remains a\ncritical challenge. Standard hidden state residuals often fail to adequately\npreserve initial token-level information in deeper layers. This paper\nintroduces ResFormer, a novel architecture that enhances information flow by\nincorporating value residual connections in addition to hidden state residuals.\nAnd a variant is the SVFormer, where all layers share the first layer's value\nembedding. Comprehensive empirical evidence demonstrates ResFormer achieves\nequivalent validation loss with 13.3\\% fewer model parameters and 15.4\\% less\ntraining data compared to Transformer, while maintaining similar memory usage\nand computational cost. Besides, SVFormer reduces KV cache size by nearly half\nwith only a small performance penalty and can be integrated with other\nKV-efficient methods, yielding further reductions in KV cache, with performance\ninfluenced by sequence length and cumulative learning rate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Transformer models have achieved remarkable success in various domains,\nthe effectiveness of information propagation through deep networks remains a\ncritical challenge. Standard hidden state residuals often fail to adequately\npreserve initial token-level information in deeper layers. This paper\nintroduces ResFormer, a novel architecture that enhances information flow by\nincorporating value residual connections in addition to hidden state residuals.\nAnd a variant is the SVFormer, where all layers share the first layer's value\nembedding. Comprehensive empirical evidence demonstrates ResFormer achieves\nequivalent validation loss with 13.3\\% fewer model parameters and 15.4\\% less\ntraining data compared to Transformer, while maintaining similar memory usage\nand computational cost. Besides, SVFormer reduces KV cache size by nearly half\nwith only a small performance penalty and can be integrated with other\nKV-efficient methods, yielding further reductions in KV cache, with performance\ninfluenced by sequence length and cumulative learning rate."
                },
                "authors": [
                    {
                        "name": "Zhanchao Zhou"
                    },
                    {
                        "name": "Tianyi Wu"
                    },
                    {
                        "name": "Zhiyun Jiang"
                    },
                    {
                        "name": "Fares Obeid"
                    },
                    {
                        "name": "Zhenzhong Lan"
                    }
                ],
                "author_detail": {
                    "name": "Zhenzhong Lan"
                },
                "author": "Zhenzhong Lan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17897v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17897v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13873v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13873v1",
                "updated": "2025-02-19T16:54:58Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    16,
                    54,
                    58,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T16:54:58Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    16,
                    54,
                    58,
                    2,
                    50,
                    0
                ],
                "title": "NVR: Vector Runahead on NPUs for Sparse Memory Access",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NVR: Vector Runahead on NPUs for Sparse Memory Access"
                },
                "summary": "Deep Neural Networks are increasingly leveraging sparsity to reduce the\nscaling up of model parameter size. However, reducing wall-clock time through\nsparsity and pruning remains challenging due to irregular memory access\npatterns, leading to frequent cache misses. In this paper, we present NPU\nVector Runahead (NVR), a prefetching mechanism tailored for NPUs to address\ncache miss problems in sparse DNN workloads. Rather than optimising memory\npatterns with high overhead and poor portability, NVR adapts runahead execution\nto the unique architecture of NPUs. NVR provides a general micro-architectural\nsolution for sparse DNN workloads without requiring compiler or algorithmic\nsupport, operating as a decoupled, speculative, lightweight hardware sub-thread\nalongside the NPU, with minimal hardware overhead (under 5%). NVR achieves an\naverage 90% reduction in cache misses compared to SOTA prefetching in\ngeneral-purpose processors, delivering 4x average speedup on sparse workloads\nversus NPUs without prefetching. Moreover, we investigate the advantages of\nincorporating a small cache (16KB) into the NPU combined with NVR. Our\nevaluation shows that expanding this modest cache delivers 5x higher\nperformance benefits than increasing the L2 cache size by the same amount.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Neural Networks are increasingly leveraging sparsity to reduce the\nscaling up of model parameter size. However, reducing wall-clock time through\nsparsity and pruning remains challenging due to irregular memory access\npatterns, leading to frequent cache misses. In this paper, we present NPU\nVector Runahead (NVR), a prefetching mechanism tailored for NPUs to address\ncache miss problems in sparse DNN workloads. Rather than optimising memory\npatterns with high overhead and poor portability, NVR adapts runahead execution\nto the unique architecture of NPUs. NVR provides a general micro-architectural\nsolution for sparse DNN workloads without requiring compiler or algorithmic\nsupport, operating as a decoupled, speculative, lightweight hardware sub-thread\nalongside the NPU, with minimal hardware overhead (under 5%). NVR achieves an\naverage 90% reduction in cache misses compared to SOTA prefetching in\ngeneral-purpose processors, delivering 4x average speedup on sparse workloads\nversus NPUs without prefetching. Moreover, we investigate the advantages of\nincorporating a small cache (16KB) into the NPU combined with NVR. Our\nevaluation shows that expanding this modest cache delivers 5x higher\nperformance benefits than increasing the L2 cache size by the same amount."
                },
                "authors": [
                    {
                        "name": "Hui Wang"
                    },
                    {
                        "name": "Zhengpeng Zhao"
                    },
                    {
                        "name": "Jing Wang"
                    },
                    {
                        "name": "Yushu Du"
                    },
                    {
                        "name": "Yuan Cheng"
                    },
                    {
                        "name": "Bing Guo"
                    },
                    {
                        "name": "He Xiao"
                    },
                    {
                        "name": "Chenhao Ma"
                    },
                    {
                        "name": "Xiaomeng Han"
                    },
                    {
                        "name": "Dean You"
                    },
                    {
                        "name": "Jiapeng Guan"
                    },
                    {
                        "name": "Ran Wei"
                    },
                    {
                        "name": "Dawei Yang"
                    },
                    {
                        "name": "Zhe Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Zhe Jiang"
                },
                "author": "Zhe Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13873v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13873v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22118v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22118v2",
                "updated": "2025-02-19T11:10:09Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    11,
                    10,
                    9,
                    2,
                    50,
                    0
                ],
                "published": "2024-10-29T15:19:13Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    19,
                    13,
                    1,
                    303,
                    0
                ],
                "title": "The Impact of Inference Acceleration on Bias of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Impact of Inference Acceleration on Bias of LLMs"
                },
                "summary": "Last few years have seen unprecedented advances in capabilities of Large\nLanguage Models (LLMs). These advancements promise to benefit a vast array of\napplication domains. However, due to their immense size, performing inference\nwith LLMs is both costly and slow. Consequently, a plethora of recent work has\nproposed strategies to enhance inference efficiency, e.g., quantization,\npruning, and caching. These acceleration strategies reduce the inference cost\nand latency, often by several factors, while maintaining much of the predictive\nperformance measured via common benchmarks. In this work, we explore another\ncritical aspect of LLM performance: demographic bias in model generations due\nto inference acceleration optimizations. Using a wide range of metrics, we\nprobe bias in model outputs from a number of angles. Analysis of outputs before\nand after inference acceleration shows significant change in bias. Worryingly,\nthese bias effects are complex and unpredictable. A combination of an\nacceleration strategy and bias type may show little bias change in one model\nbut may lead to a large effect in another. Our results highlight a need for\nin-depth and case-by-case evaluation of model bias after it has been modified\nto accelerate inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Last few years have seen unprecedented advances in capabilities of Large\nLanguage Models (LLMs). These advancements promise to benefit a vast array of\napplication domains. However, due to their immense size, performing inference\nwith LLMs is both costly and slow. Consequently, a plethora of recent work has\nproposed strategies to enhance inference efficiency, e.g., quantization,\npruning, and caching. These acceleration strategies reduce the inference cost\nand latency, often by several factors, while maintaining much of the predictive\nperformance measured via common benchmarks. In this work, we explore another\ncritical aspect of LLM performance: demographic bias in model generations due\nto inference acceleration optimizations. Using a wide range of metrics, we\nprobe bias in model outputs from a number of angles. Analysis of outputs before\nand after inference acceleration shows significant change in bias. Worryingly,\nthese bias effects are complex and unpredictable. A combination of an\nacceleration strategy and bias type may show little bias change in one model\nbut may lead to a large effect in another. Our results highlight a need for\nin-depth and case-by-case evaluation of model bias after it has been modified\nto accelerate inference."
                },
                "authors": [
                    {
                        "name": "Elisabeth Kirsten"
                    },
                    {
                        "name": "Ivan Habernal"
                    },
                    {
                        "name": "Vedant Nanda"
                    },
                    {
                        "name": "Muhammad Bilal Zafar"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Bilal Zafar"
                },
                "author": "Muhammad Bilal Zafar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22118v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22118v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05317v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05317v4",
                "updated": "2025-02-19T10:39:58Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    10,
                    39,
                    58,
                    2,
                    50,
                    0
                ],
                "published": "2024-10-05T03:47:06Z",
                "published_parsed": [
                    2024,
                    10,
                    5,
                    3,
                    47,
                    6,
                    5,
                    279,
                    0
                ],
                "title": "Accelerating Diffusion Transformers with Token-wise Feature Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Transformers with Token-wise Feature Caching"
                },
                "summary": "Diffusion transformers have shown significant effectiveness in both image and\nvideo synthesis at the expense of huge computation costs. To address this\nproblem, feature caching methods have been introduced to accelerate diffusion\ntransformers by caching the features in previous timesteps and reusing them in\nthe following timesteps. However, previous caching methods ignore that\ndifferent tokens exhibit different sensitivities to feature caching, and\nfeature caching on some tokens may lead to 10$\\times$ more destruction to the\noverall generation quality compared with other tokens. In this paper, we\nintroduce token-wise feature caching, allowing us to adaptively select the most\nsuitable tokens for caching, and further enable us to apply different caching\nratios to neural layers in different types and depths. Extensive experiments on\nPixArt-$\\alpha$, OpenSora, and DiT demonstrate our effectiveness in both image\nand video generation with no requirements for training. For instance,\n2.36$\\times$ and 1.93$\\times$ acceleration are achieved on OpenSora and\nPixArt-$\\alpha$ with almost no drop in generation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion transformers have shown significant effectiveness in both image and\nvideo synthesis at the expense of huge computation costs. To address this\nproblem, feature caching methods have been introduced to accelerate diffusion\ntransformers by caching the features in previous timesteps and reusing them in\nthe following timesteps. However, previous caching methods ignore that\ndifferent tokens exhibit different sensitivities to feature caching, and\nfeature caching on some tokens may lead to 10$\\times$ more destruction to the\noverall generation quality compared with other tokens. In this paper, we\nintroduce token-wise feature caching, allowing us to adaptively select the most\nsuitable tokens for caching, and further enable us to apply different caching\nratios to neural layers in different types and depths. Extensive experiments on\nPixArt-$\\alpha$, OpenSora, and DiT demonstrate our effectiveness in both image\nand video generation with no requirements for training. For instance,\n2.36$\\times$ and 1.93$\\times$ acceleration are achieved on OpenSora and\nPixArt-$\\alpha$ with almost no drop in generation quality."
                },
                "authors": [
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Xuyang Liu"
                    },
                    {
                        "name": "Ting Liu"
                    },
                    {
                        "name": "Siteng Huang"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "ToCa is honored to be accepted by ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05317v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05317v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13575v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13575v1",
                "updated": "2025-02-19T09:30:38Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    9,
                    30,
                    38,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T09:30:38Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    9,
                    30,
                    38,
                    2,
                    50,
                    0
                ],
                "title": "ETS: Efficient Tree Search for Inference-Time Scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ETS: Efficient Tree Search for Inference-Time Scaling"
                },
                "summary": "Test-time compute scaling has emerged as a new axis along which to improve\nmodel accuracy, where additional computation is used at inference time to allow\nthe model to think longer for more challenging problems. One promising approach\nfor test-time compute scaling is search against a process reward model, where a\nmodel generates multiple potential candidates at each step of the search, and\nthese partial trajectories are then scored by a separate reward model in order\nto guide the search process. The diversity of trajectories in the tree search\nprocess affects the accuracy of the search, since increasing diversity promotes\nmore exploration. However, this diversity comes at a cost, as divergent\ntrajectories have less KV sharing, which means they consume more memory and\nslow down the search process. Previous search methods either do not perform\nsufficient exploration, or else explore diverse trajectories but have high\nlatency. We address this challenge by proposing Efficient Tree Search (ETS),\nwhich promotes KV sharing by pruning redundant trajectories while maintaining\nnecessary diverse trajectories. ETS incorporates a linear programming cost\nmodel to promote KV cache sharing by penalizing the number of nodes retained,\nwhile incorporating a semantic coverage term into the cost model to ensure that\nwe retain trajectories which are semantically different. We demonstrate how ETS\ncan achieve 1.8$\\times$ reduction in average KV cache size during the search\nprocess, leading to 1.4$\\times$ increased throughput relative to prior\nstate-of-the-art methods, with minimal accuracy degradation and without\nrequiring any custom kernel implementation. Code is available at:\nhttps://github.com/SqueezeAILab/ETS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time compute scaling has emerged as a new axis along which to improve\nmodel accuracy, where additional computation is used at inference time to allow\nthe model to think longer for more challenging problems. One promising approach\nfor test-time compute scaling is search against a process reward model, where a\nmodel generates multiple potential candidates at each step of the search, and\nthese partial trajectories are then scored by a separate reward model in order\nto guide the search process. The diversity of trajectories in the tree search\nprocess affects the accuracy of the search, since increasing diversity promotes\nmore exploration. However, this diversity comes at a cost, as divergent\ntrajectories have less KV sharing, which means they consume more memory and\nslow down the search process. Previous search methods either do not perform\nsufficient exploration, or else explore diverse trajectories but have high\nlatency. We address this challenge by proposing Efficient Tree Search (ETS),\nwhich promotes KV sharing by pruning redundant trajectories while maintaining\nnecessary diverse trajectories. ETS incorporates a linear programming cost\nmodel to promote KV cache sharing by penalizing the number of nodes retained,\nwhile incorporating a semantic coverage term into the cost model to ensure that\nwe retain trajectories which are semantically different. We demonstrate how ETS\ncan achieve 1.8$\\times$ reduction in average KV cache size during the search\nprocess, leading to 1.4$\\times$ increased throughput relative to prior\nstate-of-the-art methods, with minimal accuracy degradation and without\nrequiring any custom kernel implementation. Code is available at:\nhttps://github.com/SqueezeAILab/ETS."
                },
                "authors": [
                    {
                        "name": "Coleman Hooper"
                    },
                    {
                        "name": "Sehoon Kim"
                    },
                    {
                        "name": "Suhong Moon"
                    },
                    {
                        "name": "Kerem Dilmen"
                    },
                    {
                        "name": "Monishwaran Maheswaran"
                    },
                    {
                        "name": "Nicholas Lee"
                    },
                    {
                        "name": "Michael W. Mahoney"
                    },
                    {
                        "name": "Sophia Shao"
                    },
                    {
                        "name": "Kurt Keutzer"
                    },
                    {
                        "name": "Amir Gholami"
                    }
                ],
                "author_detail": {
                    "name": "Amir Gholami"
                },
                "author": "Amir Gholami",
                "arxiv_comment": "11 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13575v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13575v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13542v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13542v1",
                "updated": "2025-02-19T08:50:44Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    8,
                    50,
                    44,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T08:50:44Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    8,
                    50,
                    44,
                    2,
                    50,
                    0
                ],
                "title": "Activation-aware Probe-Query: Effective Key-Value Retrieval for\n  Long-Context LLMs Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Activation-aware Probe-Query: Effective Key-Value Retrieval for\n  Long-Context LLMs Inference"
                },
                "summary": "Recent advances in large language models (LLMs) have showcased exceptional\nperformance in long-context tasks, while facing significant inference\nefficiency challenges with limited GPU memory. Existing solutions first\nproposed the sliding-window approach to accumulate a set of historical\n\\textbf{key-value} (KV) pairs for reuse, then further improvements selectively\nretain its subsets at each step. However, due to the sparse attention\ndistribution across a long context, it is hard to identify and recall relevant\nKV pairs, as the attention is distracted by massive candidate pairs.\nAdditionally, we found it promising to select representative tokens as\nprobe-Query in each sliding window to effectively represent the entire context,\nwhich is an approach overlooked by existing methods. Thus, we propose\n\\textbf{ActQKV}, a training-free, \\textbf{Act}ivation-aware approach that\ndynamically determines probe-\\textbf{Q}uery and leverages it to retrieve the\nrelevant \\textbf{KV} pairs for inference. Specifically, ActQKV monitors a\ntoken-level indicator, Activation Bias, within each context window, enabling\nthe proper construction of probe-Query for retrieval at pre-filling stage. To\naccurately recall the relevant KV pairs and minimize the irrelevant ones, we\ndesign a dynamic KV cut-off mechanism guided by information density across\nlayers at the decoding stage. Experiments on the Long-Bench and $\\infty$\nBenchmarks demonstrate its state-of-the-art performance with competitive\ninference quality and resource efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have showcased exceptional\nperformance in long-context tasks, while facing significant inference\nefficiency challenges with limited GPU memory. Existing solutions first\nproposed the sliding-window approach to accumulate a set of historical\n\\textbf{key-value} (KV) pairs for reuse, then further improvements selectively\nretain its subsets at each step. However, due to the sparse attention\ndistribution across a long context, it is hard to identify and recall relevant\nKV pairs, as the attention is distracted by massive candidate pairs.\nAdditionally, we found it promising to select representative tokens as\nprobe-Query in each sliding window to effectively represent the entire context,\nwhich is an approach overlooked by existing methods. Thus, we propose\n\\textbf{ActQKV}, a training-free, \\textbf{Act}ivation-aware approach that\ndynamically determines probe-\\textbf{Q}uery and leverages it to retrieve the\nrelevant \\textbf{KV} pairs for inference. Specifically, ActQKV monitors a\ntoken-level indicator, Activation Bias, within each context window, enabling\nthe proper construction of probe-Query for retrieval at pre-filling stage. To\naccurately recall the relevant KV pairs and minimize the irrelevant ones, we\ndesign a dynamic KV cut-off mechanism guided by information density across\nlayers at the decoding stage. Experiments on the Long-Bench and $\\infty$\nBenchmarks demonstrate its state-of-the-art performance with competitive\ninference quality and resource efficiency."
                },
                "authors": [
                    {
                        "name": "Qingfa Xiao"
                    },
                    {
                        "name": "Jiachuan Wang"
                    },
                    {
                        "name": "Haoyang Li"
                    },
                    {
                        "name": "Cheng Deng"
                    },
                    {
                        "name": "Jiaqi Tang"
                    },
                    {
                        "name": "Shuangyin Li"
                    },
                    {
                        "name": "Yongqi Zhang"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Lei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Lei Chen"
                },
                "author": "Lei Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13542v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13542v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15804v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15804v1",
                "updated": "2025-02-19T06:14:27Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    6,
                    14,
                    27,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T06:14:27Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    6,
                    14,
                    27,
                    2,
                    50,
                    0
                ],
                "title": "FairKV: Balancing Per-Head KV Cache for Fast Multi-GPU Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FairKV: Balancing Per-Head KV Cache for Fast Multi-GPU Inference"
                },
                "summary": "KV cache techniques in Transformer models aim to reduce redundant\ncomputations at the expense of substantially increased memory usage, making KV\ncache compression an important and popular research topic. Recently,\nstate-of-the-art KV cache compression methods implement imbalanced, per-head\nallocation algorithms that dynamically adjust the KV cache budget for each\nattention head, achieving excellent performance in single-GPU scenarios.\nHowever, we observe that such imbalanced compression leads to significant load\nimbalance when deploying multi-GPU inference, as some GPUs become overburdened\nwhile others remain underutilized. In this paper, we propose FairKV, a method\ndesigned to ensure fair memory usage among attention heads in systems employing\nimbalanced KV cache compression. The core technique of FairKV is Fair-Copying,\nwhich replicates a small subset of memory-intensive attention heads across GPUs\nusing data parallelism to mitigate load imbalance. Our experiments on popular\nmodels, including LLaMA 70b and Mistral 24b model, demonstrate that FairKV\nincreases throughput by 1.66x compared to standard tensor parallelism\ninference. Our code will be released as open source upon acceptance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV cache techniques in Transformer models aim to reduce redundant\ncomputations at the expense of substantially increased memory usage, making KV\ncache compression an important and popular research topic. Recently,\nstate-of-the-art KV cache compression methods implement imbalanced, per-head\nallocation algorithms that dynamically adjust the KV cache budget for each\nattention head, achieving excellent performance in single-GPU scenarios.\nHowever, we observe that such imbalanced compression leads to significant load\nimbalance when deploying multi-GPU inference, as some GPUs become overburdened\nwhile others remain underutilized. In this paper, we propose FairKV, a method\ndesigned to ensure fair memory usage among attention heads in systems employing\nimbalanced KV cache compression. The core technique of FairKV is Fair-Copying,\nwhich replicates a small subset of memory-intensive attention heads across GPUs\nusing data parallelism to mitigate load imbalance. Our experiments on popular\nmodels, including LLaMA 70b and Mistral 24b model, demonstrate that FairKV\nincreases throughput by 1.66x compared to standard tensor parallelism\ninference. Our code will be released as open source upon acceptance."
                },
                "authors": [
                    {
                        "name": "Bingzhe Zhao"
                    },
                    {
                        "name": "Ke Cheng"
                    },
                    {
                        "name": "Aomufei Yuan"
                    },
                    {
                        "name": "Yuxuan Tian"
                    },
                    {
                        "name": "Ruiguang Zhong"
                    },
                    {
                        "name": "Chengchen Hu"
                    },
                    {
                        "name": "Tong Yang"
                    },
                    {
                        "name": "Lian Yu"
                    }
                ],
                "author_detail": {
                    "name": "Lian Yu"
                },
                "author": "Lian Yu",
                "arxiv_comment": "11 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15804v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15804v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13145v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13145v1",
                "updated": "2025-02-18T18:59:57Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    18,
                    59,
                    57,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T18:59:57Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    18,
                    59,
                    57,
                    1,
                    49,
                    0
                ],
                "title": "Multimodal Mamba: Decoder-only Multimodal State Space Model via\n  Quadratic to Linear Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Mamba: Decoder-only Multimodal State Space Model via\n  Quadratic to Linear Distillation"
                },
                "summary": "Recent Multimodal Large Language Models (MLLMs) have achieved remarkable\nperformance but face deployment challenges due to their quadratic computational\ncomplexity, growing Key-Value cache requirements, and reliance on separate\nvision encoders. We propose mmMamba, a framework for developing\nlinear-complexity native multimodal state space models through progressive\ndistillation from existing MLLMs using moderate academic computational\nresources. Our approach enables the direct conversion of trained decoder-only\nMLLMs to linear-complexity architectures without requiring pre-trained\nRNN-based LLM or vision encoders. We propose an seeding strategy to carve Mamba\nfrom trained Transformer and a three-stage distillation recipe, which can\neffectively transfer the knowledge from Transformer to Mamba while preserving\nmultimodal capabilities. Our method also supports flexible hybrid architectures\nthat combine Transformer and Mamba layers for customizable\nefficiency-performance trade-offs. Distilled from the Transformer-based\ndecoder-only HoVLE, mmMamba-linear achieves competitive performance against\nexisting linear and quadratic-complexity VLMs, while mmMamba-hybrid further\nimproves performance significantly, approaching HoVLE's capabilities. At 103K\ntokens, mmMamba-linear demonstrates 20.6$\\times$ speedup and 75.8% GPU memory\nreduction compared to HoVLE, while mmMamba-hybrid achieves 13.5$\\times$ speedup\nand 60.2% memory savings. Code and models are released at\nhttps://github.com/hustvl/mmMamba",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent Multimodal Large Language Models (MLLMs) have achieved remarkable\nperformance but face deployment challenges due to their quadratic computational\ncomplexity, growing Key-Value cache requirements, and reliance on separate\nvision encoders. We propose mmMamba, a framework for developing\nlinear-complexity native multimodal state space models through progressive\ndistillation from existing MLLMs using moderate academic computational\nresources. Our approach enables the direct conversion of trained decoder-only\nMLLMs to linear-complexity architectures without requiring pre-trained\nRNN-based LLM or vision encoders. We propose an seeding strategy to carve Mamba\nfrom trained Transformer and a three-stage distillation recipe, which can\neffectively transfer the knowledge from Transformer to Mamba while preserving\nmultimodal capabilities. Our method also supports flexible hybrid architectures\nthat combine Transformer and Mamba layers for customizable\nefficiency-performance trade-offs. Distilled from the Transformer-based\ndecoder-only HoVLE, mmMamba-linear achieves competitive performance against\nexisting linear and quadratic-complexity VLMs, while mmMamba-hybrid further\nimproves performance significantly, approaching HoVLE's capabilities. At 103K\ntokens, mmMamba-linear demonstrates 20.6$\\times$ speedup and 75.8% GPU memory\nreduction compared to HoVLE, while mmMamba-hybrid achieves 13.5$\\times$ speedup\nand 60.2% memory savings. Code and models are released at\nhttps://github.com/hustvl/mmMamba"
                },
                "authors": [
                    {
                        "name": "Bencheng Liao"
                    },
                    {
                        "name": "Hongyuan Tao"
                    },
                    {
                        "name": "Qian Zhang"
                    },
                    {
                        "name": "Tianheng Cheng"
                    },
                    {
                        "name": "Yingyue Li"
                    },
                    {
                        "name": "Haoran Yin"
                    },
                    {
                        "name": "Wenyu Liu"
                    },
                    {
                        "name": "Xinggang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xinggang Wang"
                },
                "author": "Xinggang Wang",
                "arxiv_comment": "Code and model are available at https://github.com/hustvl/mmMamba",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13145v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13145v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13063v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13063v1",
                "updated": "2025-02-18T17:08:45Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    17,
                    8,
                    45,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T17:08:45Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    17,
                    8,
                    45,
                    1,
                    49,
                    0
                ],
                "title": "Cramming 1568 Tokens into a Single Vector and Back Again: Exploring the\n  Limits of Embedding Space Capacity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cramming 1568 Tokens into a Single Vector and Back Again: Exploring the\n  Limits of Embedding Space Capacity"
                },
                "summary": "A range of recent works addresses the problem of compression of sequence of\ntokens into a shorter sequence of real-valued vectors to be used as inputs\ninstead of token embeddings or key-value cache. These approaches allow to\nreduce the amount of compute in existing language models. Despite relying on\npowerful models as encoders, the maximum attainable lossless compression ratio\nis typically not higher than x10. This fact is highly intriguing because, in\ntheory, the maximum information capacity of large real-valued vectors is far\nbeyond the presented rates even for 16-bit precision and a modest vector size.\nIn this work, we explore the limits of compression by replacing the encoder\nwith a per-sample optimization procedure. We show that vectors with compression\nratios up to x1500 exist, which highlights two orders of magnitude gap between\nexisting and practically attainable solutions. Furthermore, we empirically show\nthat the compression limits are determined not by the length of the input but\nby the amount of uncertainty to be reduced, namely, the cross-entropy loss on\nthis sequence without any conditioning. The obtained limits highlight the\nsubstantial gap between the theoretical capacity of input embeddings and their\npractical utilization, suggesting significant room for optimization in model\ndesign.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A range of recent works addresses the problem of compression of sequence of\ntokens into a shorter sequence of real-valued vectors to be used as inputs\ninstead of token embeddings or key-value cache. These approaches allow to\nreduce the amount of compute in existing language models. Despite relying on\npowerful models as encoders, the maximum attainable lossless compression ratio\nis typically not higher than x10. This fact is highly intriguing because, in\ntheory, the maximum information capacity of large real-valued vectors is far\nbeyond the presented rates even for 16-bit precision and a modest vector size.\nIn this work, we explore the limits of compression by replacing the encoder\nwith a per-sample optimization procedure. We show that vectors with compression\nratios up to x1500 exist, which highlights two orders of magnitude gap between\nexisting and practically attainable solutions. Furthermore, we empirically show\nthat the compression limits are determined not by the length of the input but\nby the amount of uncertainty to be reduced, namely, the cross-entropy loss on\nthis sequence without any conditioning. The obtained limits highlight the\nsubstantial gap between the theoretical capacity of input embeddings and their\npractical utilization, suggesting significant room for optimization in model\ndesign."
                },
                "authors": [
                    {
                        "name": "Yuri Kuratov"
                    },
                    {
                        "name": "Mikhail Arkhipov"
                    },
                    {
                        "name": "Aydar Bulatov"
                    },
                    {
                        "name": "Mikhail Burtsev"
                    }
                ],
                "author_detail": {
                    "name": "Mikhail Burtsev"
                },
                "author": "Mikhail Burtsev",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13063v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13063v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12875v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12875v1",
                "updated": "2025-02-18T14:05:12Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    14,
                    5,
                    12,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T14:05:12Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    14,
                    5,
                    12,
                    1,
                    49,
                    0
                ],
                "title": "A Survey on DRL based UAV Communications and Networking: DRL\n  Fundamentals, Applications and Implementations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on DRL based UAV Communications and Networking: DRL\n  Fundamentals, Applications and Implementations"
                },
                "summary": "Unmanned aerial vehicles (UAVs) are playing an increasingly pivotal role in\nmodern communication networks,offering flexibility and enhanced coverage for a\nvariety of applica-tions. However, UAV networks pose significant challenges due\nto their dynamic and distributed nature, particularly when dealing with tasks\nsuch as power allocation, channel assignment, caching,and task offloading.\nTraditional optimization techniques often struggle to handle the complexity and\nunpredictability of these environments, leading to suboptimal performance. This\nsurvey provides a comprehensive examination of how deep reinforcement learning\n(DRL) can be applied to solve these mathematical optimization problems in UAV\ncommunications and networking.Rather than simply introducing DRL methods, the\nfocus is on demonstrating how these methods can be utilized to solve complex\nmathematical models of the underlying problems. We begin by reviewing the\nfundamental concepts of DRL, including value-based, policy-based, and\nactor-critic approaches. Then,we illustrate how DRL algorithms are applied to\nspecific UAV network tasks by discussing from problem formulations to DRL\nimplementation. By framing UAV communication challenges as optimization\nproblems, this survey emphasizes the practical value of DRL in dynamic and\nuncertain environments. We also explore the strengths of DRL in handling\nlarge-scale network scenarios and the ability to continuously adapt to changes\nin the environment. In addition, future research directions are outlined,\nhighlighting the potential for DRL to further enhance UAV communications and\nexpand its applicability to more complex,multi-agent settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unmanned aerial vehicles (UAVs) are playing an increasingly pivotal role in\nmodern communication networks,offering flexibility and enhanced coverage for a\nvariety of applica-tions. However, UAV networks pose significant challenges due\nto their dynamic and distributed nature, particularly when dealing with tasks\nsuch as power allocation, channel assignment, caching,and task offloading.\nTraditional optimization techniques often struggle to handle the complexity and\nunpredictability of these environments, leading to suboptimal performance. This\nsurvey provides a comprehensive examination of how deep reinforcement learning\n(DRL) can be applied to solve these mathematical optimization problems in UAV\ncommunications and networking.Rather than simply introducing DRL methods, the\nfocus is on demonstrating how these methods can be utilized to solve complex\nmathematical models of the underlying problems. We begin by reviewing the\nfundamental concepts of DRL, including value-based, policy-based, and\nactor-critic approaches. Then,we illustrate how DRL algorithms are applied to\nspecific UAV network tasks by discussing from problem formulations to DRL\nimplementation. By framing UAV communication challenges as optimization\nproblems, this survey emphasizes the practical value of DRL in dynamic and\nuncertain environments. We also explore the strengths of DRL in handling\nlarge-scale network scenarios and the ability to continuously adapt to changes\nin the environment. In addition, future research directions are outlined,\nhighlighting the potential for DRL to further enhance UAV communications and\nexpand its applicability to more complex,multi-agent settings."
                },
                "authors": [
                    {
                        "name": "Wei Zhao"
                    },
                    {
                        "name": "Shaoxin Cui"
                    },
                    {
                        "name": "Wen Qiu"
                    },
                    {
                        "name": "Zhiqiang He"
                    },
                    {
                        "name": "Zhi Liu"
                    },
                    {
                        "name": "Xiao Zheng"
                    },
                    {
                        "name": "Bomin Mao"
                    },
                    {
                        "name": "Nei Kato"
                    }
                ],
                "author_detail": {
                    "name": "Nei Kato"
                },
                "author": "Nei Kato",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12875v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12875v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12665v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12665v1",
                "updated": "2025-02-18T09:11:51Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    9,
                    11,
                    51,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T09:11:51Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    9,
                    11,
                    51,
                    1,
                    49,
                    0
                ],
                "title": "A$^2$ATS: Retrieval-Based KV Cache Reduction via Windowed Rotary\n  Position Embedding and Query-Aware Vector Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A$^2$ATS: Retrieval-Based KV Cache Reduction via Windowed Rotary\n  Position Embedding and Query-Aware Vector Quantization"
                },
                "summary": "Long context large language models (LLMs) pose significant challenges for\nefficient serving due to the large memory footprint and high access overhead of\nKV cache. Retrieval-based KV cache reduction methods can mitigate these\nchallenges, typically by offloading the complete KV cache to CPU and retrieving\nnecessary tokens on demand during inference. However, these methods still\nsuffer from unsatisfactory accuracy degradation and extra retrieval overhead.\nTo address these limitations, this paper proposes A$^2$ATS, a novel\nretrieval-based KV cache reduction method. A$^2$ATS aims to obtain an accurate\napproximation of attention scores by applying the vector quantization technique\nto key states, thereby enabling efficient and precise retrieval of the top-K\ntokens. First, we propose Windowed Rotary Position Embedding, which decouples\nthe positional dependency from query and key states after position embedding.\nThen, we propose query-aware vector quantization that optimizes the objective\nof attention score approximation directly. Finally, we design the heterogeneous\ninference architecture for KV cache offloading, enabling long context serving\nwith larger batch sizes. Experimental results demonstrate that A$^2$ATS can\nachieve a lower performance degradation with similar or lower overhead compared\nto existing methods, thereby increasing long context serving throughput by up\nto $2.7 \\times$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long context large language models (LLMs) pose significant challenges for\nefficient serving due to the large memory footprint and high access overhead of\nKV cache. Retrieval-based KV cache reduction methods can mitigate these\nchallenges, typically by offloading the complete KV cache to CPU and retrieving\nnecessary tokens on demand during inference. However, these methods still\nsuffer from unsatisfactory accuracy degradation and extra retrieval overhead.\nTo address these limitations, this paper proposes A$^2$ATS, a novel\nretrieval-based KV cache reduction method. A$^2$ATS aims to obtain an accurate\napproximation of attention scores by applying the vector quantization technique\nto key states, thereby enabling efficient and precise retrieval of the top-K\ntokens. First, we propose Windowed Rotary Position Embedding, which decouples\nthe positional dependency from query and key states after position embedding.\nThen, we propose query-aware vector quantization that optimizes the objective\nof attention score approximation directly. Finally, we design the heterogeneous\ninference architecture for KV cache offloading, enabling long context serving\nwith larger batch sizes. Experimental results demonstrate that A$^2$ATS can\nachieve a lower performance degradation with similar or lower overhead compared\nto existing methods, thereby increasing long context serving throughput by up\nto $2.7 \\times$."
                },
                "authors": [
                    {
                        "name": "Junhui He"
                    },
                    {
                        "name": "Junna Xing"
                    },
                    {
                        "name": "Nan Wang"
                    },
                    {
                        "name": "Rui Xu"
                    },
                    {
                        "name": "Shangyu Wu"
                    },
                    {
                        "name": "Peng Zhou"
                    },
                    {
                        "name": "Qiang Liu"
                    },
                    {
                        "name": "Chun Jason Xue"
                    },
                    {
                        "name": "Qingan Li"
                    }
                ],
                "author_detail": {
                    "name": "Qingan Li"
                },
                "author": "Qingan Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12665v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12665v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05996v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05996v2",
                "updated": "2025-02-18T07:58:29Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    7,
                    58,
                    29,
                    1,
                    49,
                    0
                ],
                "published": "2024-08-12T08:46:30Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    8,
                    46,
                    30,
                    0,
                    225,
                    0
                ],
                "title": "Value-based Proactive Caching for Sensing Data in Vehicular Networks: An\n  Operator's Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Value-based Proactive Caching for Sensing Data in Vehicular Networks: An\n  Operator's Perspective"
                },
                "summary": "Access to sensing data (SD) is crucial for vehicular networks to ensure safe\nand efficient transportation services. Given the vast volume of data involved,\nproactive caching required SD is a pivotal strategy for alleviating network\ncongestion and improving data accessibility. Despite merits, existing studies\npredominantly address SD caching within a single slot. Therefore, these\napproaches lack scalability for scenarios involving multi-slots and are not\nwell-suited for network operators who manage resources within a long-term cost\nbudget. Moreover, the oversight of service capacity at caching nodes may result\nin substantial queuing delays for SD reception. To tackle these limitations, we\njointly consider the problem of anchoring SD caching and allocating from an\noperator's perspective. A value model incorporating both temporal and spacial\ncharacteristics is given to estimate the significance of various caching\ndecisions. Subsequently, a stochastic programming model is proposed to optimize\nthe long-term system performance, which is converted into a series of online\noptimization problem by leveraging the Lyapunov method and linearized via\nintroducing auxiliary variables. To expedite the solution, we provide a binary\nquantum particle swarm optimization based algorithm with quadratic time\ncomplexity. Numerical investigations demonstrate the superiority of proposed\nalgorithms compared with other schemes in terms of energy consumption, response\nlatency, and cache-hit ratio.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Access to sensing data (SD) is crucial for vehicular networks to ensure safe\nand efficient transportation services. Given the vast volume of data involved,\nproactive caching required SD is a pivotal strategy for alleviating network\ncongestion and improving data accessibility. Despite merits, existing studies\npredominantly address SD caching within a single slot. Therefore, these\napproaches lack scalability for scenarios involving multi-slots and are not\nwell-suited for network operators who manage resources within a long-term cost\nbudget. Moreover, the oversight of service capacity at caching nodes may result\nin substantial queuing delays for SD reception. To tackle these limitations, we\njointly consider the problem of anchoring SD caching and allocating from an\noperator's perspective. A value model incorporating both temporal and spacial\ncharacteristics is given to estimate the significance of various caching\ndecisions. Subsequently, a stochastic programming model is proposed to optimize\nthe long-term system performance, which is converted into a series of online\noptimization problem by leveraging the Lyapunov method and linearized via\nintroducing auxiliary variables. To expedite the solution, we provide a binary\nquantum particle swarm optimization based algorithm with quadratic time\ncomplexity. Numerical investigations demonstrate the superiority of proposed\nalgorithms compared with other schemes in terms of energy consumption, response\nlatency, and cache-hit ratio."
                },
                "authors": [
                    {
                        "name": "Yantong Wang"
                    },
                    {
                        "name": "Ke Liu"
                    },
                    {
                        "name": "Hui Ji"
                    },
                    {
                        "name": "Jiande Sun"
                    }
                ],
                "author_detail": {
                    "name": "Jiande Sun"
                },
                "author": "Jiande Sun",
                "arxiv_comment": "14 pages,10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05996v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05996v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12574v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12574v1",
                "updated": "2025-02-18T06:26:05Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    6,
                    26,
                    5,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T06:26:05Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    6,
                    26,
                    5,
                    1,
                    49,
                    0
                ],
                "title": "HeadInfer: Memory-Efficient LLM Inference by Head-wise Offloading",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HeadInfer: Memory-Efficient LLM Inference by Head-wise Offloading"
                },
                "summary": "Transformer-based large language models (LLMs) demonstrate impressive\nperformance in long context generation. Extending the context length has\ndisproportionately shifted the memory footprint of LLMs during inference to the\nkey-value cache (KV cache). In this paper, we propose HEADINFER, which offloads\nthe KV cache to CPU RAM while avoiding the need to fully store the KV cache for\nany transformer layer on the GPU. HEADINFER employs a fine-grained, head-wise\noffloading strategy, maintaining only selective attention heads KV cache on the\nGPU while computing attention output dynamically. Through roofline analysis, we\ndemonstrate that HEADINFER maintains computational efficiency while\nsignificantly reducing memory footprint. We evaluate HEADINFER on the\nLlama-3-8B model with a 1-million-token sequence, reducing the GPU memory\nfootprint of the KV cache from 128 GB to 1 GB and the total GPU memory usage\nfrom 207 GB to 17 GB, achieving a 92% reduction compared to BF16 baseline\ninference. Notably, HEADINFER enables 4-million-token inference with an 8B\nmodel on a single consumer GPU with 24GB memory (e.g., NVIDIA RTX 4090) without\napproximation methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models (LLMs) demonstrate impressive\nperformance in long context generation. Extending the context length has\ndisproportionately shifted the memory footprint of LLMs during inference to the\nkey-value cache (KV cache). In this paper, we propose HEADINFER, which offloads\nthe KV cache to CPU RAM while avoiding the need to fully store the KV cache for\nany transformer layer on the GPU. HEADINFER employs a fine-grained, head-wise\noffloading strategy, maintaining only selective attention heads KV cache on the\nGPU while computing attention output dynamically. Through roofline analysis, we\ndemonstrate that HEADINFER maintains computational efficiency while\nsignificantly reducing memory footprint. We evaluate HEADINFER on the\nLlama-3-8B model with a 1-million-token sequence, reducing the GPU memory\nfootprint of the KV cache from 128 GB to 1 GB and the total GPU memory usage\nfrom 207 GB to 17 GB, achieving a 92% reduction compared to BF16 baseline\ninference. Notably, HEADINFER enables 4-million-token inference with an 8B\nmodel on a single consumer GPU with 24GB memory (e.g., NVIDIA RTX 4090) without\napproximation methods."
                },
                "authors": [
                    {
                        "name": "Cheng Luo"
                    },
                    {
                        "name": "Zefan Cai"
                    },
                    {
                        "name": "Hanshi Sun"
                    },
                    {
                        "name": "Jinqi Xiao"
                    },
                    {
                        "name": "Bo Yuan"
                    },
                    {
                        "name": "Wen Xiao"
                    },
                    {
                        "name": "Junjie Hu"
                    },
                    {
                        "name": "Jiawei Zhao"
                    },
                    {
                        "name": "Beidi Chen"
                    },
                    {
                        "name": "Anima Anandkumar"
                    }
                ],
                "author_detail": {
                    "name": "Anima Anandkumar"
                },
                "author": "Anima Anandkumar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12574v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12574v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12224v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12224v1",
                "updated": "2025-02-17T14:54:14Z",
                "updated_parsed": [
                    2025,
                    2,
                    17,
                    14,
                    54,
                    14,
                    0,
                    48,
                    0
                ],
                "published": "2025-02-17T14:54:14Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    14,
                    54,
                    14,
                    0,
                    48,
                    0
                ],
                "title": "Accurate Expert Predictions in MoE Inference via Cross-Layer Gate",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate Expert Predictions in MoE Inference via Cross-Layer Gate"
                },
                "summary": "Large Language Models (LLMs) have demonstrated impressive performance across\nvarious tasks, and their application in edge scenarios has attracted\nsignificant attention. However, sparse-activated Mixture-of-Experts (MoE)\nmodels, which are well suited for edge scenarios, have received relatively\nlittle attention due to their high memory demands. Offload-based methods have\nbeen proposed to address this challenge, but they face difficulties with expert\nprediction. Inaccurate expert predictions can result in prolonged inference\ndelays. To promote the application of MoE models in edge scenarios, we propose\nFate, an offloading system designed for MoE models to enable efficient\ninference in resource-constrained environments. The key insight behind Fate is\nthat gate inputs from adjacent layers can be effectively used for expert\nprefetching, achieving high prediction accuracy without additional GPU\noverhead. Furthermore, Fate employs a shallow-favoring expert caching strategy\nthat increases the expert hit rate to 99\\%. Additionally, Fate integrates\ntailored quantization strategies for cache optimization and IO efficiency.\nExperimental results show that, compared to Load on Demand and Expert\nActivation Path-based method, Fate achieves up to 4.5x and 1.9x speedups in\nprefill speed and up to 4.1x and 2.2x speedups in decoding speed, respectively,\nwhile maintaining inference quality. Moreover, Fate's performance improvements\nare scalable across different memory budgets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated impressive performance across\nvarious tasks, and their application in edge scenarios has attracted\nsignificant attention. However, sparse-activated Mixture-of-Experts (MoE)\nmodels, which are well suited for edge scenarios, have received relatively\nlittle attention due to their high memory demands. Offload-based methods have\nbeen proposed to address this challenge, but they face difficulties with expert\nprediction. Inaccurate expert predictions can result in prolonged inference\ndelays. To promote the application of MoE models in edge scenarios, we propose\nFate, an offloading system designed for MoE models to enable efficient\ninference in resource-constrained environments. The key insight behind Fate is\nthat gate inputs from adjacent layers can be effectively used for expert\nprefetching, achieving high prediction accuracy without additional GPU\noverhead. Furthermore, Fate employs a shallow-favoring expert caching strategy\nthat increases the expert hit rate to 99\\%. Additionally, Fate integrates\ntailored quantization strategies for cache optimization and IO efficiency.\nExperimental results show that, compared to Load on Demand and Expert\nActivation Path-based method, Fate achieves up to 4.5x and 1.9x speedups in\nprefill speed and up to 4.1x and 2.2x speedups in decoding speed, respectively,\nwhile maintaining inference quality. Moreover, Fate's performance improvements\nare scalable across different memory budgets."
                },
                "authors": [
                    {
                        "name": "Zhiyuan Fang"
                    },
                    {
                        "name": "Zicong Hong"
                    },
                    {
                        "name": "Yuegui Huang"
                    },
                    {
                        "name": "Yufeng Lyu"
                    },
                    {
                        "name": "Wuhui Chen"
                    },
                    {
                        "name": "Yue Yu"
                    },
                    {
                        "name": "Fan Yu"
                    },
                    {
                        "name": "Zibin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zibin Zheng"
                },
                "author": "Zibin Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12224v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12224v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14838v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14838v2",
                "updated": "2025-02-17T14:34:58Z",
                "updated_parsed": [
                    2025,
                    2,
                    17,
                    14,
                    34,
                    58,
                    0,
                    48,
                    0
                ],
                "published": "2024-12-19T13:28:42Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    13,
                    28,
                    42,
                    3,
                    354,
                    0
                ],
                "title": "DynamicKV: Task-Aware Adaptive KV Cache Compression for Long Context\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DynamicKV: Task-Aware Adaptive KV Cache Compression for Long Context\n  LLMs"
                },
                "summary": "Efficient KV cache management in LLMs is crucial for long-context tasks like\nRAG and summarization. Existing KV cache compression methods enforce a fixed\npattern, neglecting task-specific characteristics and reducing the retention of\nessential information. However, we observe distinct activation patterns across\nlayers in various tasks, highlighting the need for adaptive strategies tailored\nto each task's unique demands. Based on this insight, we propose DynamicKV, a\nmethod that dynamically optimizes token retention by adjusting the number of\ntokens retained at each layer to adapt to the specific task. DynamicKV\nestablishes global and per-layer maximum KV cache budgets, temporarily\nretaining the maximum budget for the current layer, and periodically updating\nthe KV cache sizes of all preceding layers during inference. Our method retains\nonly 1.7% of the KV cache size while achieving ~85% of the Full KV cache\nperformance on LongBench. Notably, even under extreme compression (0.9%),\nDynamicKV surpasses state-of-the-art (SOTA) methods by 11% in the\nNeedle-in-a-Haystack test using Mistral-7B-Instruct-v0.2. The code will be\nreleased.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient KV cache management in LLMs is crucial for long-context tasks like\nRAG and summarization. Existing KV cache compression methods enforce a fixed\npattern, neglecting task-specific characteristics and reducing the retention of\nessential information. However, we observe distinct activation patterns across\nlayers in various tasks, highlighting the need for adaptive strategies tailored\nto each task's unique demands. Based on this insight, we propose DynamicKV, a\nmethod that dynamically optimizes token retention by adjusting the number of\ntokens retained at each layer to adapt to the specific task. DynamicKV\nestablishes global and per-layer maximum KV cache budgets, temporarily\nretaining the maximum budget for the current layer, and periodically updating\nthe KV cache sizes of all preceding layers during inference. Our method retains\nonly 1.7% of the KV cache size while achieving ~85% of the Full KV cache\nperformance on LongBench. Notably, even under extreme compression (0.9%),\nDynamicKV surpasses state-of-the-art (SOTA) methods by 11% in the\nNeedle-in-a-Haystack test using Mistral-7B-Instruct-v0.2. The code will be\nreleased."
                },
                "authors": [
                    {
                        "name": "Xiabin Zhou"
                    },
                    {
                        "name": "Wenbin Wang"
                    },
                    {
                        "name": "Minyan Zeng"
                    },
                    {
                        "name": "Jiaxian Guo"
                    },
                    {
                        "name": "Xuebo Liu"
                    },
                    {
                        "name": "Li Shen"
                    },
                    {
                        "name": "Min Zhang"
                    },
                    {
                        "name": "Liang Ding"
                    }
                ],
                "author_detail": {
                    "name": "Liang Ding"
                },
                "author": "Liang Ding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14838v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14838v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12216v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12216v1",
                "updated": "2025-02-17T08:39:43Z",
                "updated_parsed": [
                    2025,
                    2,
                    17,
                    8,
                    39,
                    43,
                    0,
                    48,
                    0
                ],
                "published": "2025-02-17T08:39:43Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    8,
                    39,
                    43,
                    0,
                    48,
                    0
                ],
                "title": "Tactic: Adaptive Sparse Attention with Clustering and Distribution\n  Fitting for Long-Context LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tactic: Adaptive Sparse Attention with Clustering and Distribution\n  Fitting for Long-Context LLMs"
                },
                "summary": "Long-context models are essential for many applications but face\ninefficiencies in loading large KV caches during decoding. Prior methods\nenforce fixed token budgets for sparse attention, assuming a set number of\ntokens can approximate full attention. However, these methods overlook\nvariations in the importance of attention across heads, layers, and contexts.\nTo address these limitations, we propose Tactic, a sparsity-adaptive and\ncalibration-free sparse attention mechanism that dynamically selects tokens\nbased on their cumulative attention scores rather than a fixed token budget. By\nsetting a target fraction of total attention scores, Tactic ensures that token\nselection naturally adapts to variations in attention sparsity. To efficiently\napproximate this selection, Tactic leverages clustering-based sorting and\ndistribution fitting, allowing it to accurately estimate token importance with\nminimal computational overhead. We show that Tactic outperforms existing sparse\nattention algorithms, achieving superior accuracy and up to 7.29x decode\nattention speedup. This improvement translates to an overall 1.58x end-to-end\ninference speedup, making Tactic a practical and effective solution for\nlong-context LLM inference in accuracy-sensitive applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context models are essential for many applications but face\ninefficiencies in loading large KV caches during decoding. Prior methods\nenforce fixed token budgets for sparse attention, assuming a set number of\ntokens can approximate full attention. However, these methods overlook\nvariations in the importance of attention across heads, layers, and contexts.\nTo address these limitations, we propose Tactic, a sparsity-adaptive and\ncalibration-free sparse attention mechanism that dynamically selects tokens\nbased on their cumulative attention scores rather than a fixed token budget. By\nsetting a target fraction of total attention scores, Tactic ensures that token\nselection naturally adapts to variations in attention sparsity. To efficiently\napproximate this selection, Tactic leverages clustering-based sorting and\ndistribution fitting, allowing it to accurately estimate token importance with\nminimal computational overhead. We show that Tactic outperforms existing sparse\nattention algorithms, achieving superior accuracy and up to 7.29x decode\nattention speedup. This improvement translates to an overall 1.58x end-to-end\ninference speedup, making Tactic a practical and effective solution for\nlong-context LLM inference in accuracy-sensitive applications."
                },
                "authors": [
                    {
                        "name": "Kan Zhu"
                    },
                    {
                        "name": "Tian Tang"
                    },
                    {
                        "name": "Qinyu Xu"
                    },
                    {
                        "name": "Yile Gu"
                    },
                    {
                        "name": "Zhichen Zeng"
                    },
                    {
                        "name": "Rohan Kadekodi"
                    },
                    {
                        "name": "Liangyu Zhao"
                    },
                    {
                        "name": "Ang Li"
                    },
                    {
                        "name": "Arvind Krishnamurthy"
                    },
                    {
                        "name": "Baris Kasikci"
                    }
                ],
                "author_detail": {
                    "name": "Baris Kasikci"
                },
                "author": "Baris Kasikci",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12216v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12216v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15779v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15779v1",
                "updated": "2025-02-17T08:12:34Z",
                "updated_parsed": [
                    2025,
                    2,
                    17,
                    8,
                    12,
                    34,
                    0,
                    48,
                    0
                ],
                "published": "2025-02-17T08:12:34Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    8,
                    12,
                    34,
                    0,
                    48,
                    0
                ],
                "title": "Rotate, Clip, and Partition: Towards W2A4KV4 Quantization by Integrating\n  Rotation and Learnable Non-uniform Quantizer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rotate, Clip, and Partition: Towards W2A4KV4 Quantization by Integrating\n  Rotation and Learnable Non-uniform Quantizer"
                },
                "summary": "We propose Rotate, Clip, and Partition (RCP), a quantization-aware training\n(QAT) approach that first realizes extreme compression of LLMs with\nW2A4KV4(2-bit weight, 4-bit activation, and 4-bit KV cache) configuration. RCP\nintegrates recent rotation techniques with a novel non-uniform weight quantizer\ndesign, by quantitatively analyzing the impact of random rotation on 2-bit\nweight quantization. Our weight quantizer features Learnable Direct\nPartitioning (LDP), which introduces learnable parameters to directly learn\nnon-uniform intervals jointly with LLM weights. We also present a specialized\nGPU kernel that supports GEMV on non-uniform W2A4. Experiments show that RCP\ncan compress LLaMA-2-7B to W2A4KV4 with a loss of only 2.84 WikiText2 ppl and\n5.29 times reduced memory footprint. Furthermore, RCP can quantize challenging\nmobile-targeted LLaMA-3.2 models and domain-specific WizardCoder-7B and\nMetaMath-7B with no critical problems such as convergence failure and\nrepetition. Code will be made available at blind_review.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose Rotate, Clip, and Partition (RCP), a quantization-aware training\n(QAT) approach that first realizes extreme compression of LLMs with\nW2A4KV4(2-bit weight, 4-bit activation, and 4-bit KV cache) configuration. RCP\nintegrates recent rotation techniques with a novel non-uniform weight quantizer\ndesign, by quantitatively analyzing the impact of random rotation on 2-bit\nweight quantization. Our weight quantizer features Learnable Direct\nPartitioning (LDP), which introduces learnable parameters to directly learn\nnon-uniform intervals jointly with LLM weights. We also present a specialized\nGPU kernel that supports GEMV on non-uniform W2A4. Experiments show that RCP\ncan compress LLaMA-2-7B to W2A4KV4 with a loss of only 2.84 WikiText2 ppl and\n5.29 times reduced memory footprint. Furthermore, RCP can quantize challenging\nmobile-targeted LLaMA-3.2 models and domain-specific WizardCoder-7B and\nMetaMath-7B with no critical problems such as convergence failure and\nrepetition. Code will be made available at blind_review."
                },
                "authors": [
                    {
                        "name": "Euntae Choi"
                    },
                    {
                        "name": "Sumin Song"
                    },
                    {
                        "name": "Woosang Lim"
                    },
                    {
                        "name": "Sungjoo Yoo"
                    }
                ],
                "author_detail": {
                    "name": "Sungjoo Yoo"
                },
                "author": "Sungjoo Yoo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15779v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15779v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11501v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11501v1",
                "updated": "2025-02-17T07:05:36Z",
                "updated_parsed": [
                    2025,
                    2,
                    17,
                    7,
                    5,
                    36,
                    0,
                    48,
                    0
                ],
                "published": "2025-02-17T07:05:36Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    7,
                    5,
                    36,
                    0,
                    48,
                    0
                ],
                "title": "Token Pruning in Multimodal Large Language Models: Are We Solving the\n  Right Problem?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Token Pruning in Multimodal Large Language Models: Are We Solving the\n  Right Problem?"
                },
                "summary": "Multimodal large language models (MLLMs) have shown remarkable performance\nfor cross-modal understanding and generation, yet still suffer from severe\ninference costs. Recently, abundant works have been proposed to solve this\nproblem with token pruning, which identifies the redundant tokens in MLLMs and\nthen prunes them to reduce the computation and KV storage costs, leading to\nsignificant acceleration without training. While these methods claim efficiency\ngains, critical questions about their fundamental design and evaluation remain\nunanswered: Why do many existing approaches underperform even compared to naive\nrandom token selection? Are attention-based scoring sufficient for reliably\nidentifying redundant tokens? Is language information really helpful during\ntoken pruning? What makes a good trade-off between token importance and\nduplication? Are current evaluation protocols comprehensive and unbiased? The\nignorance of previous research on these problems hinders the long-term\ndevelopment of token pruning. In this paper, we answer these questions one by\none, providing insights into the design of future token pruning methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (MLLMs) have shown remarkable performance\nfor cross-modal understanding and generation, yet still suffer from severe\ninference costs. Recently, abundant works have been proposed to solve this\nproblem with token pruning, which identifies the redundant tokens in MLLMs and\nthen prunes them to reduce the computation and KV storage costs, leading to\nsignificant acceleration without training. While these methods claim efficiency\ngains, critical questions about their fundamental design and evaluation remain\nunanswered: Why do many existing approaches underperform even compared to naive\nrandom token selection? Are attention-based scoring sufficient for reliably\nidentifying redundant tokens? Is language information really helpful during\ntoken pruning? What makes a good trade-off between token importance and\nduplication? Are current evaluation protocols comprehensive and unbiased? The\nignorance of previous research on these problems hinders the long-term\ndevelopment of token pruning. In this paper, we answer these questions one by\none, providing insights into the design of future token pruning methods."
                },
                "authors": [
                    {
                        "name": "Zichen Wen"
                    },
                    {
                        "name": "Yifeng Gao"
                    },
                    {
                        "name": "Weijia Li"
                    },
                    {
                        "name": "Conghui He"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "12 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11501v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11501v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11444v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11444v1",
                "updated": "2025-02-17T05:02:25Z",
                "updated_parsed": [
                    2025,
                    2,
                    17,
                    5,
                    2,
                    25,
                    0,
                    48,
                    0
                ],
                "published": "2025-02-17T05:02:25Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    5,
                    2,
                    25,
                    0,
                    48,
                    0
                ],
                "title": "Does RAG Really Perform Bad For Long-Context Processing?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Does RAG Really Perform Bad For Long-Context Processing?"
                },
                "summary": "The efficient processing of long context poses a serious challenge for large\nlanguage models (LLMs). Recently, retrieval-augmented generation (RAG) has\nemerged as a promising strategy for this problem, as it enables LLMs to make\nselective use of the long context for efficient computation. However, existing\nRAG approaches lag behind other long-context processing methods due to inherent\nlimitations on inaccurate retrieval and fragmented contexts. To address these\nchallenges, we introduce RetroLM, a novel RAG framework for long-context\nprocessing. Unlike traditional methods, RetroLM employs KV-level retrieval\naugmentation, where it partitions the LLM's KV cache into contiguous pages and\nretrieves the most crucial ones for efficient computation. This approach\nenhances robustness to retrieval inaccuracy, facilitates effective utilization\nof fragmented contexts, and saves the cost from repeated computation. Building\non this framework, we further develop a specialized retriever for precise\nretrieval of critical pages and conduct unsupervised post-training to optimize\nthe model's ability to leverage retrieved information. We conduct comprehensive\nevaluations with a variety of benchmarks, including LongBench, InfiniteBench,\nand RULER, where RetroLM significantly outperforms existing long-context LLMs\nand efficient long-context processing methods, particularly in tasks requiring\nintensive reasoning or extremely long-context comprehension.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The efficient processing of long context poses a serious challenge for large\nlanguage models (LLMs). Recently, retrieval-augmented generation (RAG) has\nemerged as a promising strategy for this problem, as it enables LLMs to make\nselective use of the long context for efficient computation. However, existing\nRAG approaches lag behind other long-context processing methods due to inherent\nlimitations on inaccurate retrieval and fragmented contexts. To address these\nchallenges, we introduce RetroLM, a novel RAG framework for long-context\nprocessing. Unlike traditional methods, RetroLM employs KV-level retrieval\naugmentation, where it partitions the LLM's KV cache into contiguous pages and\nretrieves the most crucial ones for efficient computation. This approach\nenhances robustness to retrieval inaccuracy, facilitates effective utilization\nof fragmented contexts, and saves the cost from repeated computation. Building\non this framework, we further develop a specialized retriever for precise\nretrieval of critical pages and conduct unsupervised post-training to optimize\nthe model's ability to leverage retrieved information. We conduct comprehensive\nevaluations with a variety of benchmarks, including LongBench, InfiniteBench,\nand RULER, where RetroLM significantly outperforms existing long-context LLMs\nand efficient long-context processing methods, particularly in tasks requiring\nintensive reasoning or extremely long-context comprehension."
                },
                "authors": [
                    {
                        "name": "Kun Luo"
                    },
                    {
                        "name": "Zheng Liu"
                    },
                    {
                        "name": "Peitian Zhang"
                    },
                    {
                        "name": "Hongjin Qian"
                    },
                    {
                        "name": "Jun Zhao"
                    },
                    {
                        "name": "Kang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Kang Liu"
                },
                "author": "Kang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11444v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11444v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09383v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09383v2",
                "updated": "2025-02-16T18:31:10Z",
                "updated_parsed": [
                    2025,
                    2,
                    16,
                    18,
                    31,
                    10,
                    6,
                    47,
                    0
                ],
                "published": "2025-02-13T14:59:03Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    14,
                    59,
                    3,
                    3,
                    44,
                    0
                ],
                "title": "Capitalizing on a Crisis: A Computational Analysis of all Five Million\n  British Firms During the Covid-19 Pandemic",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Capitalizing on a Crisis: A Computational Analysis of all Five Million\n  British Firms During the Covid-19 Pandemic"
                },
                "summary": "The Covid-19 pandemic brought unprecedented changes to business ownership in\nthe UK which affects a generation of entrepreneurs and their employees.\nNonetheless, the impact remains poorly understood. This is because research on\ncapital accumulation has typically lacked high-quality, individualized,\npopulation-level data. We overcome these barriers to examine who benefits from\neconomic crises through a computationally orientated lens of firm creation.\nLeveraging a comprehensive cache of administrative data on every UK firm and\nall nine million people running them, combined with probabilistic algorithms,\nwe conduct individual-level analyses to understand who became Covid\nentrepreneurs. Using these techniques, we explore characteristics of\nentrepreneurs--such as age, gender, region, business experience, and\nindustry--which potentially predict Covid entrepreneurship. By employing an\nautomated time series model selection procedure to generate counterfactuals, we\nshow that Covid entrepreneurs were typically aged 35-49 (40.4%), men (73.1%),\nand had previously held roles in existing firms (59.4%). For most industries,\ngrowth was disproportionately concentrated around London. It was therefore\nexisting corporate elites who were most able to capitalize on the Covid crisis\nand not, as some hypothesized, young entrepreneurs who were setting up their\nfirst businesses. In this respect, the pandemic will likely impact future\nwealth inequalities. Our work offers methodological guidance for future\npolicymakers during economic crises and highlights the long-term consequences\nfor capital and wealth inequality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Covid-19 pandemic brought unprecedented changes to business ownership in\nthe UK which affects a generation of entrepreneurs and their employees.\nNonetheless, the impact remains poorly understood. This is because research on\ncapital accumulation has typically lacked high-quality, individualized,\npopulation-level data. We overcome these barriers to examine who benefits from\neconomic crises through a computationally orientated lens of firm creation.\nLeveraging a comprehensive cache of administrative data on every UK firm and\nall nine million people running them, combined with probabilistic algorithms,\nwe conduct individual-level analyses to understand who became Covid\nentrepreneurs. Using these techniques, we explore characteristics of\nentrepreneurs--such as age, gender, region, business experience, and\nindustry--which potentially predict Covid entrepreneurship. By employing an\nautomated time series model selection procedure to generate counterfactuals, we\nshow that Covid entrepreneurs were typically aged 35-49 (40.4%), men (73.1%),\nand had previously held roles in existing firms (59.4%). For most industries,\ngrowth was disproportionately concentrated around London. It was therefore\nexisting corporate elites who were most able to capitalize on the Covid crisis\nand not, as some hypothesized, young entrepreneurs who were setting up their\nfirst businesses. In this respect, the pandemic will likely impact future\nwealth inequalities. Our work offers methodological guidance for future\npolicymakers during economic crises and highlights the long-term consequences\nfor capital and wealth inequality."
                },
                "authors": [
                    {
                        "name": "Naomi Muggleton"
                    },
                    {
                        "name": "Charles Rahal"
                    },
                    {
                        "name": "Aaron Reeves"
                    }
                ],
                "author_detail": {
                    "name": "Aaron Reeves"
                },
                "author": "Aaron Reeves",
                "arxiv_doi": "10.1007/s42001-025-00360-4",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/s42001-025-00360-4",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.09383v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09383v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Journal of Computational Social Science, 8(2), 1-29 (2025)",
                "arxiv_primary_category": {
                    "term": "econ.GN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07627v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07627v2",
                "updated": "2025-02-16T16:41:43Z",
                "updated_parsed": [
                    2025,
                    2,
                    16,
                    16,
                    41,
                    43,
                    6,
                    47,
                    0
                ],
                "published": "2024-11-12T08:17:15Z",
                "published_parsed": [
                    2024,
                    11,
                    12,
                    8,
                    17,
                    15,
                    1,
                    317,
                    0
                ],
                "title": "Leveraging Previous Steps: A Training-free Fast Solver for Flow\n  Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Previous Steps: A Training-free Fast Solver for Flow\n  Diffusion"
                },
                "summary": "Flow diffusion models (FDMs) have recently shown potential in generation\ntasks due to the high generation quality. However, the current ordinary\ndifferential equation (ODE) solver for FDMs, e.g., the Euler solver, still\nsuffers from slow generation since ODE solvers need many number function\nevaluations (NFE) to keep high-quality generation. In this paper, we propose a\nnovel training-free flow-solver to reduce NFE while maintaining high-quality\ngeneration. The key insight for the flow-solver is to leverage the previous\nsteps to reduce the NFE, where a cache is created to reuse these results from\nthe previous steps. Specifically, the Taylor expansion is first used to\napproximate the ODE. To calculate the high-order derivatives of Taylor\nexpansion, the flow-solver proposes to use the previous steps and a polynomial\ninterpolation to approximate it, where the number of orders we could\napproximate equals the number of previous steps we cached. We also prove that\nthe flow-solver has a more minor approximation error and faster generation\nspeed. Experimental results on the CIFAR-10, CelebA-HQ, LSUN-Bedroom,\nLSUN-Church, ImageNet, and real text-to-image generation prove the efficiency\nof the flow-solver. Specifically, the flow-solver improves the FID-30K from\n13.79 to 6.75, from 46.64 to 19.49 with $\\text{NFE}=10$ on CIFAR-10 and\nLSUN-Church, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flow diffusion models (FDMs) have recently shown potential in generation\ntasks due to the high generation quality. However, the current ordinary\ndifferential equation (ODE) solver for FDMs, e.g., the Euler solver, still\nsuffers from slow generation since ODE solvers need many number function\nevaluations (NFE) to keep high-quality generation. In this paper, we propose a\nnovel training-free flow-solver to reduce NFE while maintaining high-quality\ngeneration. The key insight for the flow-solver is to leverage the previous\nsteps to reduce the NFE, where a cache is created to reuse these results from\nthe previous steps. Specifically, the Taylor expansion is first used to\napproximate the ODE. To calculate the high-order derivatives of Taylor\nexpansion, the flow-solver proposes to use the previous steps and a polynomial\ninterpolation to approximate it, where the number of orders we could\napproximate equals the number of previous steps we cached. We also prove that\nthe flow-solver has a more minor approximation error and faster generation\nspeed. Experimental results on the CIFAR-10, CelebA-HQ, LSUN-Bedroom,\nLSUN-Church, ImageNet, and real text-to-image generation prove the efficiency\nof the flow-solver. Specifically, the flow-solver improves the FID-30K from\n13.79 to 6.75, from 46.64 to 19.49 with $\\text{NFE}=10$ on CIFAR-10 and\nLSUN-Church, respectively."
                },
                "authors": [
                    {
                        "name": "Kaiyu Song"
                    },
                    {
                        "name": "Hanjiang Lai"
                    }
                ],
                "author_detail": {
                    "name": "Hanjiang Lai"
                },
                "author": "Hanjiang Lai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07627v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07627v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15332v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15332v2",
                "updated": "2025-02-16T14:50:00Z",
                "updated_parsed": [
                    2025,
                    2,
                    16,
                    14,
                    50,
                    0,
                    6,
                    47,
                    0
                ],
                "published": "2024-10-20T08:42:29Z",
                "published_parsed": [
                    2024,
                    10,
                    20,
                    8,
                    42,
                    29,
                    6,
                    294,
                    0
                ],
                "title": "EPIC: Efficient Position-Independent Context Caching for Serving Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EPIC: Efficient Position-Independent Context Caching for Serving Large\n  Language Models"
                },
                "summary": "Large Language Models (LLMs) are critical for a wide range of applications,\nbut serving them efficiently becomes increasingly challenging as inputs become\nmore complex. Context caching improves serving performance by exploiting\ninter-request dependency and reusing key-value (KV) cache across requests, thus\nimproving time-to-first-token (TTFT). However, existing prefix-based context\ncaching requires exact token prefix matches, limiting cache reuse in few-shot\nlearning, multi-document QA, or retrieval-augmented generation, where prefixes\nmay vary. In this paper, we present EPIC, an LLM serving system that introduces\nposition-independent context caching (PIC), enabling modular KV cache reuse\nregardless of token chunk position (or prefix). EPIC features two key designs:\nAttnLink, which leverages static attention sparsity to minimize recomputation\nfor accuracy recovery, and KVSplit, a customizable chunking method that\npreserves semantic coherence. Our experiments demonstrate that Epic delivers up\nto 8x improvements in TTFT and 7x throughput over existing systems, with\nnegligible or no accuracy loss. By addressing the limitations of traditional\ncaching approaches, Epic enables more scalable and efficient LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are critical for a wide range of applications,\nbut serving them efficiently becomes increasingly challenging as inputs become\nmore complex. Context caching improves serving performance by exploiting\ninter-request dependency and reusing key-value (KV) cache across requests, thus\nimproving time-to-first-token (TTFT). However, existing prefix-based context\ncaching requires exact token prefix matches, limiting cache reuse in few-shot\nlearning, multi-document QA, or retrieval-augmented generation, where prefixes\nmay vary. In this paper, we present EPIC, an LLM serving system that introduces\nposition-independent context caching (PIC), enabling modular KV cache reuse\nregardless of token chunk position (or prefix). EPIC features two key designs:\nAttnLink, which leverages static attention sparsity to minimize recomputation\nfor accuracy recovery, and KVSplit, a customizable chunking method that\npreserves semantic coherence. Our experiments demonstrate that Epic delivers up\nto 8x improvements in TTFT and 7x throughput over existing systems, with\nnegligible or no accuracy loss. By addressing the limitations of traditional\ncaching approaches, Epic enables more scalable and efficient LLM inference."
                },
                "authors": [
                    {
                        "name": "Junhao Hu"
                    },
                    {
                        "name": "Wenrui Huang"
                    },
                    {
                        "name": "Haoyi Wang"
                    },
                    {
                        "name": "Weidong Wang"
                    },
                    {
                        "name": "Tiancheng Hu"
                    },
                    {
                        "name": "Qin Zhang"
                    },
                    {
                        "name": "Hao Feng"
                    },
                    {
                        "name": "Xusheng Chen"
                    },
                    {
                        "name": "Yizhou Shan"
                    },
                    {
                        "name": "Tao Xie"
                    }
                ],
                "author_detail": {
                    "name": "Tao Xie"
                },
                "author": "Tao Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15332v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15332v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11147v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11147v1",
                "updated": "2025-02-16T14:28:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    16,
                    14,
                    28,
                    52,
                    6,
                    47,
                    0
                ],
                "published": "2025-02-16T14:28:52Z",
                "published_parsed": [
                    2025,
                    2,
                    16,
                    14,
                    28,
                    52,
                    6,
                    47,
                    0
                ],
                "title": "Efficient Long-Decoding Inference with Reasoning-Aware Attention\n  Sparsity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Long-Decoding Inference with Reasoning-Aware Attention\n  Sparsity"
                },
                "summary": "Large Language Models (LLMs) have demonstrated strong capabilities across\nvarious domains, with recent advancements in challenging reasoning tasks such\nas mathematics and programming. However, solving reasoning tasks often requires\nlong decoding chains (of thoughts), which incur $O(N)$ time and memory\nconsumption, where $N$ is the chain length. To mitigate $O(N)$ time and memory\nconsumption, existing sparsity-based algorithms propose retaining only the most\ncritical token's intermediate data (i.e., key-value cache) and discarding the\nrest. However, these existing algorithms struggle with the ``impossible\ntrinity'' of accuracy, time, and memory. For example, the state-of-the-art\nalgorithm, Quest, achieves high accuracy with $O(L)$ time but $O(N)$ memory\n($L$ is the cache budget, $L \\ll N$). To address this issue, in this paper, we\nidentify a new attention pattern during the decode stage of reasoning tasks,\nwhere milestone tokens (analogous to lemmas in mathematical proofs) emerge, are\nutilized, and then become unimportant afterward. Based on this pattern, we\npropose a new algorithm named RaaS that identifies and retains milestone tokens\nonly until they are no longer needed, achieving high accuracy with $O(L)$ time\nand $O(L)$ memory complexity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated strong capabilities across\nvarious domains, with recent advancements in challenging reasoning tasks such\nas mathematics and programming. However, solving reasoning tasks often requires\nlong decoding chains (of thoughts), which incur $O(N)$ time and memory\nconsumption, where $N$ is the chain length. To mitigate $O(N)$ time and memory\nconsumption, existing sparsity-based algorithms propose retaining only the most\ncritical token's intermediate data (i.e., key-value cache) and discarding the\nrest. However, these existing algorithms struggle with the ``impossible\ntrinity'' of accuracy, time, and memory. For example, the state-of-the-art\nalgorithm, Quest, achieves high accuracy with $O(L)$ time but $O(N)$ memory\n($L$ is the cache budget, $L \\ll N$). To address this issue, in this paper, we\nidentify a new attention pattern during the decode stage of reasoning tasks,\nwhere milestone tokens (analogous to lemmas in mathematical proofs) emerge, are\nutilized, and then become unimportant afterward. Based on this pattern, we\npropose a new algorithm named RaaS that identifies and retains milestone tokens\nonly until they are no longer needed, achieving high accuracy with $O(L)$ time\nand $O(L)$ memory complexity."
                },
                "authors": [
                    {
                        "name": "Junhao Hu"
                    },
                    {
                        "name": "Wenrui Huang"
                    },
                    {
                        "name": "Weidong Wang"
                    },
                    {
                        "name": "Zhenwen Li"
                    },
                    {
                        "name": "Tiancheng Hu"
                    },
                    {
                        "name": "Zhixia Liu"
                    },
                    {
                        "name": "Xusheng Chen"
                    },
                    {
                        "name": "Tao Xie"
                    },
                    {
                        "name": "Yizhou Shan"
                    }
                ],
                "author_detail": {
                    "name": "Yizhou Shan"
                },
                "author": "Yizhou Shan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11147v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11147v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11101v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11101v1",
                "updated": "2025-02-16T12:33:16Z",
                "updated_parsed": [
                    2025,
                    2,
                    16,
                    12,
                    33,
                    16,
                    6,
                    47,
                    0
                ],
                "published": "2025-02-16T12:33:16Z",
                "published_parsed": [
                    2025,
                    2,
                    16,
                    12,
                    33,
                    16,
                    6,
                    47,
                    0
                ],
                "title": "CacheFocus: Dynamic Cache Re-Positioning for Efficient\n  Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CacheFocus: Dynamic Cache Re-Positioning for Efficient\n  Retrieval-Augmented Generation"
                },
                "summary": "Large Language Models (LLMs) excel across a variety of language tasks yet are\nconstrained by limited input lengths and high computational costs. Existing\napproaches\\textemdash such as relative positional encodings (e.g., RoPE, ALiBi)\nand sliding window mechanisms\\textemdash partially alleviate these issues but\noften require additional training or suffer from performance degradation with\nlonger inputs. In this paper, we introduce \\textbf{\\textit{CacheFocus}}, a\nmethod that enhances length normalization and reduces inference latency without\nany further training. Our approach leverages query-independent, offline caching\nto efficiently reuse a Context KV Cache Store. We address the amplification of\nabnormal token distributions problem by re-positioning cached keys and\nintroducing Layer-Adaptive Cache Pruning to discard low-relevance caches during\npre-filling. Additionally, our Adaptive Positional Allocation Strategy\ndynamically reassigns cache positions to maximize the use of the available\npositional encoding range. Experiments on the Natural Questions and TriviaQA\ndatasets demonstrate that CacheFocus outperforms alternative methods even when\ninputs exceed the $4$K limit of the \\texttt{LLaMA-2} model, emphasizing its\npractical effectiveness for long-context LLMs. Moreover, even with large\nmaximum input length of \\texttt{Qwen2}, the performance of CacheFocus shows\nthat it maintains consistent performance even as the number of documents\nincreases, effectively managing long-text generation without degradation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) excel across a variety of language tasks yet are\nconstrained by limited input lengths and high computational costs. Existing\napproaches\\textemdash such as relative positional encodings (e.g., RoPE, ALiBi)\nand sliding window mechanisms\\textemdash partially alleviate these issues but\noften require additional training or suffer from performance degradation with\nlonger inputs. In this paper, we introduce \\textbf{\\textit{CacheFocus}}, a\nmethod that enhances length normalization and reduces inference latency without\nany further training. Our approach leverages query-independent, offline caching\nto efficiently reuse a Context KV Cache Store. We address the amplification of\nabnormal token distributions problem by re-positioning cached keys and\nintroducing Layer-Adaptive Cache Pruning to discard low-relevance caches during\npre-filling. Additionally, our Adaptive Positional Allocation Strategy\ndynamically reassigns cache positions to maximize the use of the available\npositional encoding range. Experiments on the Natural Questions and TriviaQA\ndatasets demonstrate that CacheFocus outperforms alternative methods even when\ninputs exceed the $4$K limit of the \\texttt{LLaMA-2} model, emphasizing its\npractical effectiveness for long-context LLMs. Moreover, even with large\nmaximum input length of \\texttt{Qwen2}, the performance of CacheFocus shows\nthat it maintains consistent performance even as the number of documents\nincreases, effectively managing long-text generation without degradation."
                },
                "authors": [
                    {
                        "name": "Kun-Hui Lee"
                    },
                    {
                        "name": "Eunhwan Park"
                    },
                    {
                        "name": "Donghoon Han"
                    },
                    {
                        "name": "Seung-Hoon Na"
                    }
                ],
                "author_detail": {
                    "name": "Seung-Hoon Na"
                },
                "author": "Seung-Hoon Na",
                "arxiv_comment": "11 pages (Work in progress)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11101v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11101v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11083v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11083v1",
                "updated": "2025-02-16T11:37:14Z",
                "updated_parsed": [
                    2025,
                    2,
                    16,
                    11,
                    37,
                    14,
                    6,
                    47,
                    0
                ],
                "published": "2025-02-16T11:37:14Z",
                "published_parsed": [
                    2025,
                    2,
                    16,
                    11,
                    37,
                    14,
                    6,
                    47,
                    0
                ],
                "title": "Streamlining the Collaborative Chain of Models into A Single Forward\n  Pass in Generation-Based Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Streamlining the Collaborative Chain of Models into A Single Forward\n  Pass in Generation-Based Tasks"
                },
                "summary": "In Retrieval-Augmented Generation (RAG) and agent-based frameworks, the\n\"Chain of Models\" approach is widely used, where multiple specialized models\nwork sequentially on distinct sub-tasks. This approach is effective but\nincreases resource demands as each model must be deployed separately. Recent\nadvancements attempt to address this by applying prompt tuning, which allows a\nshared base model to adapt to multiple tasks with minimal parameter changes.\nHowever, a key challenge remains: intermediate outputs, passed between models\nas plain text, require recomputation of hidden states (i.e., Key and Value (KV)\nstates in Transformers) during inference. In this paper, we introduce FTHSS, a\nnovel prompt-tuning method that enables models to share KV hidden states,\neliminating redundant forward passes and reducing KV cache storage. By\nmodifying input and attention masks during training, FTHSS allows models to\neffectively utilize KV hidden states from prior models in both single- and\nmulti-round scenarios. Empirical results on four tasks show that FTHSS matches\nthe performance of traditional model chains while improving inference\nefficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In Retrieval-Augmented Generation (RAG) and agent-based frameworks, the\n\"Chain of Models\" approach is widely used, where multiple specialized models\nwork sequentially on distinct sub-tasks. This approach is effective but\nincreases resource demands as each model must be deployed separately. Recent\nadvancements attempt to address this by applying prompt tuning, which allows a\nshared base model to adapt to multiple tasks with minimal parameter changes.\nHowever, a key challenge remains: intermediate outputs, passed between models\nas plain text, require recomputation of hidden states (i.e., Key and Value (KV)\nstates in Transformers) during inference. In this paper, we introduce FTHSS, a\nnovel prompt-tuning method that enables models to share KV hidden states,\neliminating redundant forward passes and reducing KV cache storage. By\nmodifying input and attention masks during training, FTHSS allows models to\neffectively utilize KV hidden states from prior models in both single- and\nmulti-round scenarios. Empirical results on four tasks show that FTHSS matches\nthe performance of traditional model chains while improving inference\nefficiency."
                },
                "authors": [
                    {
                        "name": "Yuanjie Lyu"
                    },
                    {
                        "name": "Chao Zhang"
                    },
                    {
                        "name": "Yuhao Chen"
                    },
                    {
                        "name": "Yong Chen"
                    },
                    {
                        "name": "Tong Xu"
                    }
                ],
                "author_detail": {
                    "name": "Tong Xu"
                },
                "author": "Tong Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11083v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11083v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11046v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11046v1",
                "updated": "2025-02-16T09:08:36Z",
                "updated_parsed": [
                    2025,
                    2,
                    16,
                    9,
                    8,
                    36,
                    6,
                    47,
                    0
                ],
                "published": "2025-02-16T09:08:36Z",
                "published_parsed": [
                    2025,
                    2,
                    16,
                    9,
                    8,
                    36,
                    6,
                    47,
                    0
                ],
                "title": "Enabling Efficient Transaction Processing on CXL-Based Memory Sharing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling Efficient Transaction Processing on CXL-Based Memory Sharing"
                },
                "summary": "Transaction processing systems are the crux for modern data-center\napplications, yet current multi-node systems are slow due to network overheads.\nThis paper advocates for Compute Express Link (CXL) as a network alternative,\nwhich enables low-latency and cache-coherent shared memory accesses. However,\ndirectly adopting standard CXL primitives leads to performance degradation due\nto the high cost of maintaining cross-node cache coherence. To address the CXL\nchallenges, this paper introduces CtXnL, a software-hardware co-designed system\nthat implements a novel hybrid coherence primitive tailored to the loosely\ncoherent nature of transactional data. The core innovation of CtXnL is\nempowering transaction system developers with the ability to selectively\nachieve data coherence. Our evaluations on OLTP workloads demonstrate that\nCtXnL enhances performance, outperforming current network-based systems and\nachieves with up to 2.08x greater throughput than vanilla CXL memory sharing\narchitectures across universal transaction processing policies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transaction processing systems are the crux for modern data-center\napplications, yet current multi-node systems are slow due to network overheads.\nThis paper advocates for Compute Express Link (CXL) as a network alternative,\nwhich enables low-latency and cache-coherent shared memory accesses. However,\ndirectly adopting standard CXL primitives leads to performance degradation due\nto the high cost of maintaining cross-node cache coherence. To address the CXL\nchallenges, this paper introduces CtXnL, a software-hardware co-designed system\nthat implements a novel hybrid coherence primitive tailored to the loosely\ncoherent nature of transactional data. The core innovation of CtXnL is\nempowering transaction system developers with the ability to selectively\nachieve data coherence. Our evaluations on OLTP workloads demonstrate that\nCtXnL enhances performance, outperforming current network-based systems and\nachieves with up to 2.08x greater throughput than vanilla CXL memory sharing\narchitectures across universal transaction processing policies."
                },
                "authors": [
                    {
                        "name": "Zhao Wang"
                    },
                    {
                        "name": "Yiqi Chen"
                    },
                    {
                        "name": "Cong Li"
                    },
                    {
                        "name": "Dimin Niu"
                    },
                    {
                        "name": "Tianchan Guan"
                    },
                    {
                        "name": "Zhaoyang Du"
                    },
                    {
                        "name": "Xingda Wei"
                    },
                    {
                        "name": "Guangyu Sun"
                    }
                ],
                "author_detail": {
                    "name": "Guangyu Sun"
                },
                "author": "Guangyu Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11046v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11046v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.05231v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.05231v2",
                "updated": "2025-02-15T23:54:38Z",
                "updated_parsed": [
                    2025,
                    2,
                    15,
                    23,
                    54,
                    38,
                    5,
                    46,
                    0
                ],
                "published": "2024-05-08T17:27:11Z",
                "published_parsed": [
                    2024,
                    5,
                    8,
                    17,
                    27,
                    11,
                    2,
                    129,
                    0
                ],
                "title": "DiskGNN: Bridging I/O Efficiency and Model Accuracy for Out-of-Core GNN\n  Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiskGNN: Bridging I/O Efficiency and Model Accuracy for Out-of-Core GNN\n  Training"
                },
                "summary": "Graph neural networks (GNNs) are machine learning models specialized for\ngraph data and widely used in many applications. To train GNNs on large graphs\nthat exceed CPU memory, several systems store data on disk and conduct\nout-of-core processing. However, these systems suffer from either read\namplification when reading node features that are usually smaller than a disk\npage or degraded model accuracy by treating the graph as disconnected\npartitions. To close this gap, we build a system called DiskGNN, which achieves\nhigh I/O efficiency and thus fast training without hurting model accuracy. The\nkey technique used by DiskGNN is offline sampling, which helps decouple graph\nsampling from model computation. In particular, by conducting graph sampling\nbeforehand, DiskGNN acquires the node features that will be accessed by model\ncomputation, and such information is utilized to pack the target node features\ncontiguously on disk to avoid read amplification. Besides, \\name{} also adopts\ndesigns including four-level feature store to fully utilize the memory\nhierarchy to cache node features and reduce disk access, batched packing to\naccelerate the feature packing process, and pipelined training to overlap disk\naccess with other operations. We compare DiskGNN with Ginex and MariusGNN,\nwhich are state-of-the-art systems for out-of-core GNN training. The results\nshow that DiskGNN can speed up the baselines by over 8x while matching their\nbest model accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph neural networks (GNNs) are machine learning models specialized for\ngraph data and widely used in many applications. To train GNNs on large graphs\nthat exceed CPU memory, several systems store data on disk and conduct\nout-of-core processing. However, these systems suffer from either read\namplification when reading node features that are usually smaller than a disk\npage or degraded model accuracy by treating the graph as disconnected\npartitions. To close this gap, we build a system called DiskGNN, which achieves\nhigh I/O efficiency and thus fast training without hurting model accuracy. The\nkey technique used by DiskGNN is offline sampling, which helps decouple graph\nsampling from model computation. In particular, by conducting graph sampling\nbeforehand, DiskGNN acquires the node features that will be accessed by model\ncomputation, and such information is utilized to pack the target node features\ncontiguously on disk to avoid read amplification. Besides, \\name{} also adopts\ndesigns including four-level feature store to fully utilize the memory\nhierarchy to cache node features and reduce disk access, batched packing to\naccelerate the feature packing process, and pipelined training to overlap disk\naccess with other operations. We compare DiskGNN with Ginex and MariusGNN,\nwhich are state-of-the-art systems for out-of-core GNN training. The results\nshow that DiskGNN can speed up the baselines by over 8x while matching their\nbest model accuracy."
                },
                "authors": [
                    {
                        "name": "Renjie Liu"
                    },
                    {
                        "name": "Yichuan Wang"
                    },
                    {
                        "name": "Xiao Yan"
                    },
                    {
                        "name": "Haitian Jiang"
                    },
                    {
                        "name": "Zhenkun Cai"
                    },
                    {
                        "name": "Minjie Wang"
                    },
                    {
                        "name": "Bo Tang"
                    },
                    {
                        "name": "Jinyang Li"
                    }
                ],
                "author_detail": {
                    "name": "Jinyang Li"
                },
                "author": "Jinyang Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.05231v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.05231v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.01939v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.01939v2",
                "updated": "2025-02-15T18:09:50Z",
                "updated_parsed": [
                    2025,
                    2,
                    15,
                    18,
                    9,
                    50,
                    5,
                    46,
                    0
                ],
                "published": "2024-06-04T03:48:08Z",
                "published_parsed": [
                    2024,
                    6,
                    4,
                    3,
                    48,
                    8,
                    1,
                    156,
                    0
                ],
                "title": "Speeding up Policy Simulation in Supply Chain RL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speeding up Policy Simulation in Supply Chain RL"
                },
                "summary": "Simulating a single trajectory of a dynamical system under some\nstate-dependent policy is a core bottleneck in policy optimization (PO)\nalgorithms. The many inherently serial policy evaluations that must be\nperformed in a single simulation constitute the bulk of this bottleneck. In\napplying PO to supply chain optimization (SCO) problems, simulating a single\nsample path corresponding to one month of a supply chain can take several\nhours. We present an iterative algorithm to accelerate policy simulation,\ndubbed Picard Iteration. This scheme carefully assigns policy evaluation tasks\nto independent processes. Within an iteration, any given process evaluates the\npolicy only on its assigned tasks while assuming a certain \"cached\" evaluation\nfor other tasks; the cache is updated at the end of the iteration. Implemented\non GPUs, this scheme admits batched evaluation of the policy across a single\ntrajectory. We prove that the structure afforded by many SCO problems allows\nconvergence in a small number of iterations independent of the horizon. We\ndemonstrate practical speedups of 400x on large-scale SCO problems even with a\nsingle GPU, and also demonstrate practical efficacy in other RL environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulating a single trajectory of a dynamical system under some\nstate-dependent policy is a core bottleneck in policy optimization (PO)\nalgorithms. The many inherently serial policy evaluations that must be\nperformed in a single simulation constitute the bulk of this bottleneck. In\napplying PO to supply chain optimization (SCO) problems, simulating a single\nsample path corresponding to one month of a supply chain can take several\nhours. We present an iterative algorithm to accelerate policy simulation,\ndubbed Picard Iteration. This scheme carefully assigns policy evaluation tasks\nto independent processes. Within an iteration, any given process evaluates the\npolicy only on its assigned tasks while assuming a certain \"cached\" evaluation\nfor other tasks; the cache is updated at the end of the iteration. Implemented\non GPUs, this scheme admits batched evaluation of the policy across a single\ntrajectory. We prove that the structure afforded by many SCO problems allows\nconvergence in a small number of iterations independent of the horizon. We\ndemonstrate practical speedups of 400x on large-scale SCO problems even with a\nsingle GPU, and also demonstrate practical efficacy in other RL environments."
                },
                "authors": [
                    {
                        "name": "Vivek Farias"
                    },
                    {
                        "name": "Joren Gijsbrechts"
                    },
                    {
                        "name": "Aryan Khojandi"
                    },
                    {
                        "name": "Tianyi Peng"
                    },
                    {
                        "name": "Andrew Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Zheng"
                },
                "author": "Andrew Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.01939v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.01939v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14882v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14882v1",
                "updated": "2025-02-15T05:08:01Z",
                "updated_parsed": [
                    2025,
                    2,
                    15,
                    5,
                    8,
                    1,
                    5,
                    46,
                    0
                ],
                "published": "2025-02-15T05:08:01Z",
                "published_parsed": [
                    2025,
                    2,
                    15,
                    5,
                    8,
                    1,
                    5,
                    46,
                    0
                ],
                "title": "From 16-Bit to 1-Bit: Visual KV Cache Quantization for Memory-Efficient\n  Multimodal Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From 16-Bit to 1-Bit: Visual KV Cache Quantization for Memory-Efficient\n  Multimodal Large Language Models"
                },
                "summary": "Multimodal Large Language Models (MLLMs) have achieved remarkable success\nacross various applications, yet their computational overhead during deployment\nremains a critical challenge. While Key-Value (KV) caching improves inference\nefficiency by trading memory for computation, the growing memory footprint from\nstoring extensive KV caches reduces throughput and limits long-term execution\non devices with constrained GPU memory. Existing approaches primarily focus on\ndropping unimportant tokens to reduce the KV cache size, mitigating memory\nconstraints at the cost of potential information loss. In contrast, we propose\na simple yet effective visual quantization strategy that preserves all visual\ntokens while significantly reducing memory consumption. To achieve an extreme\nquantization ratio, i.e., 1-bit quantization, we propose group-specific\nquantization and quantile-based quantization approaches, motivated by the\ninherent patterns of the KV cache. Our method is plug-and-play, enabling\nseamless integration into various MLLMs to improve memory efficiency without\narchitectural modifications. Extensive experiments demonstrate that our\napproach effectively reduces memory overhead while maintaining computational\nefficiency and preserving multimodal performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) have achieved remarkable success\nacross various applications, yet their computational overhead during deployment\nremains a critical challenge. While Key-Value (KV) caching improves inference\nefficiency by trading memory for computation, the growing memory footprint from\nstoring extensive KV caches reduces throughput and limits long-term execution\non devices with constrained GPU memory. Existing approaches primarily focus on\ndropping unimportant tokens to reduce the KV cache size, mitigating memory\nconstraints at the cost of potential information loss. In contrast, we propose\na simple yet effective visual quantization strategy that preserves all visual\ntokens while significantly reducing memory consumption. To achieve an extreme\nquantization ratio, i.e., 1-bit quantization, we propose group-specific\nquantization and quantile-based quantization approaches, motivated by the\ninherent patterns of the KV cache. Our method is plug-and-play, enabling\nseamless integration into various MLLMs to improve memory efficiency without\narchitectural modifications. Extensive experiments demonstrate that our\napproach effectively reduces memory overhead while maintaining computational\nefficiency and preserving multimodal performance."
                },
                "authors": [
                    {
                        "name": "Zeliang Zhang"
                    },
                    {
                        "name": "Yifan Zhu"
                    },
                    {
                        "name": "Susan Liang"
                    },
                    {
                        "name": "Zhiyuan Wang"
                    },
                    {
                        "name": "Jiani Liu"
                    },
                    {
                        "name": "Haiting Lin"
                    },
                    {
                        "name": "Mingjie Zhao"
                    },
                    {
                        "name": "Chenliang Xu"
                    },
                    {
                        "name": "Kun Wan"
                    },
                    {
                        "name": "Wentian Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Wentian Zhao"
                },
                "author": "Wentian Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14882v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14882v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10659v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10659v1",
                "updated": "2025-02-15T03:56:22Z",
                "updated_parsed": [
                    2025,
                    2,
                    15,
                    3,
                    56,
                    22,
                    5,
                    46,
                    0
                ],
                "published": "2025-02-15T03:56:22Z",
                "published_parsed": [
                    2025,
                    2,
                    15,
                    3,
                    56,
                    22,
                    5,
                    46,
                    0
                ],
                "title": "Pushing up to the Limit of Memory Bandwidth and Capacity Utilization for\n  Efficient LLM Decoding on Embedded FPGA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pushing up to the Limit of Memory Bandwidth and Capacity Utilization for\n  Efficient LLM Decoding on Embedded FPGA"
                },
                "summary": "The extremely high computational and storage demands of large language models\nhave excluded most edge devices, which were widely used for efficient machine\nlearning, from being viable options. A typical edge device usually only has 4GB\nof memory capacity and a bandwidth of less than 20GB/s, while a large language\nmodel quantized to 4-bit precision with 7B parameters already requires 3.5GB of\ncapacity, and its decoding process is purely bandwidth-bound. In this paper, we\naim to explore these limits by proposing a hardware accelerator for large\nlanguage model (LLM) inference on the Zynq-based KV260 platform, equipped with\n4GB of 64-bit 2400Mbps DDR4 memory. We successfully deploy a LLaMA2-7B model,\nachieving a decoding speed of around 5 token/s, utilizing 93.3% of the memory\ncapacity and reaching 85% decoding speed of the theoretical memory bandwidth\nlimit. To fully reserve the memory capacity for model weights and key-value\ncache, we develop the system in a bare-metal environment without an operating\nsystem. To fully reserve the bandwidth for model weight transfers, we implement\na customized dataflow with an operator fusion pipeline and propose a data\narrangement format that can maximize the data transaction efficiency. This\nresearch marks the first attempt to deploy a 7B level LLM on a standalone\nembedded field programmable gate array (FPGA) device. It provides key insights\ninto efficient LLM inference on embedded FPGA devices and provides guidelines\nfor future architecture design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The extremely high computational and storage demands of large language models\nhave excluded most edge devices, which were widely used for efficient machine\nlearning, from being viable options. A typical edge device usually only has 4GB\nof memory capacity and a bandwidth of less than 20GB/s, while a large language\nmodel quantized to 4-bit precision with 7B parameters already requires 3.5GB of\ncapacity, and its decoding process is purely bandwidth-bound. In this paper, we\naim to explore these limits by proposing a hardware accelerator for large\nlanguage model (LLM) inference on the Zynq-based KV260 platform, equipped with\n4GB of 64-bit 2400Mbps DDR4 memory. We successfully deploy a LLaMA2-7B model,\nachieving a decoding speed of around 5 token/s, utilizing 93.3% of the memory\ncapacity and reaching 85% decoding speed of the theoretical memory bandwidth\nlimit. To fully reserve the memory capacity for model weights and key-value\ncache, we develop the system in a bare-metal environment without an operating\nsystem. To fully reserve the bandwidth for model weight transfers, we implement\na customized dataflow with an operator fusion pipeline and propose a data\narrangement format that can maximize the data transaction efficiency. This\nresearch marks the first attempt to deploy a 7B level LLM on a standalone\nembedded field programmable gate array (FPGA) device. It provides key insights\ninto efficient LLM inference on embedded FPGA devices and provides guidelines\nfor future architecture design."
                },
                "authors": [
                    {
                        "name": "Jindong Li"
                    },
                    {
                        "name": "Tenglong Li"
                    },
                    {
                        "name": "Guobin Shen"
                    },
                    {
                        "name": "Dongcheng Zhao"
                    },
                    {
                        "name": "Qian Zhang"
                    },
                    {
                        "name": "Yi Zeng"
                    }
                ],
                "author_detail": {
                    "name": "Yi Zeng"
                },
                "author": "Yi Zeng",
                "arxiv_comment": "Accepted by DATE2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10659v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10659v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10389v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10389v1",
                "updated": "2025-02-14T18:59:36Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    18,
                    59,
                    36,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-14T18:59:36Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    18,
                    59,
                    36,
                    4,
                    45,
                    0
                ],
                "title": "Region-Adaptive Sampling for Diffusion Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Region-Adaptive Sampling for Diffusion Transformers"
                },
                "summary": "Diffusion models (DMs) have become the leading choice for generative tasks\nacross diverse domains. However, their reliance on multiple sequential forward\npasses significantly limits real-time performance. Previous acceleration\nmethods have primarily focused on reducing the number of sampling steps or\nreusing intermediate results, failing to leverage variations across spatial\nregions within the image due to the constraints of convolutional U-Net\nstructures. By harnessing the flexibility of Diffusion Transformers (DiTs) in\nhandling variable number of tokens, we introduce RAS, a novel, training-free\nsampling strategy that dynamically assigns different sampling ratios to regions\nwithin an image based on the focus of the DiT model. Our key observation is\nthat during each sampling step, the model concentrates on semantically\nmeaningful regions, and these areas of focus exhibit strong continuity across\nconsecutive steps. Leveraging this insight, RAS updates only the regions\ncurrently in focus, while other regions are updated using cached noise from the\nprevious step. The model's focus is determined based on the output from the\npreceding step, capitalizing on the temporal consistency we observed. We\nevaluate RAS on Stable Diffusion 3 and Lumina-Next-T2I, achieving speedups up\nto 2.36x and 2.51x, respectively, with minimal degradation in generation\nquality. Additionally, a user study reveals that RAS delivers comparable\nqualities under human evaluation while achieving a 1.6x speedup. Our approach\nmakes a significant step towards more efficient diffusion transformers,\nenhancing their potential for real-time applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models (DMs) have become the leading choice for generative tasks\nacross diverse domains. However, their reliance on multiple sequential forward\npasses significantly limits real-time performance. Previous acceleration\nmethods have primarily focused on reducing the number of sampling steps or\nreusing intermediate results, failing to leverage variations across spatial\nregions within the image due to the constraints of convolutional U-Net\nstructures. By harnessing the flexibility of Diffusion Transformers (DiTs) in\nhandling variable number of tokens, we introduce RAS, a novel, training-free\nsampling strategy that dynamically assigns different sampling ratios to regions\nwithin an image based on the focus of the DiT model. Our key observation is\nthat during each sampling step, the model concentrates on semantically\nmeaningful regions, and these areas of focus exhibit strong continuity across\nconsecutive steps. Leveraging this insight, RAS updates only the regions\ncurrently in focus, while other regions are updated using cached noise from the\nprevious step. The model's focus is determined based on the output from the\npreceding step, capitalizing on the temporal consistency we observed. We\nevaluate RAS on Stable Diffusion 3 and Lumina-Next-T2I, achieving speedups up\nto 2.36x and 2.51x, respectively, with minimal degradation in generation\nquality. Additionally, a user study reveals that RAS delivers comparable\nqualities under human evaluation while achieving a 1.6x speedup. Our approach\nmakes a significant step towards more efficient diffusion transformers,\nenhancing their potential for real-time applications."
                },
                "authors": [
                    {
                        "name": "Ziming Liu"
                    },
                    {
                        "name": "Yifan Yang"
                    },
                    {
                        "name": "Chengruidong Zhang"
                    },
                    {
                        "name": "Yiqi Zhang"
                    },
                    {
                        "name": "Lili Qiu"
                    },
                    {
                        "name": "Yang You"
                    },
                    {
                        "name": "Yuqing Yang"
                    }
                ],
                "author_detail": {
                    "name": "Yuqing Yang"
                },
                "author": "Yuqing Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10389v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10389v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09057v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09057v2",
                "updated": "2025-02-14T17:17:20Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    17,
                    17,
                    20,
                    4,
                    45,
                    0
                ],
                "published": "2024-12-12T08:33:39Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    8,
                    33,
                    39,
                    3,
                    347,
                    0
                ],
                "title": "PhishIntel: Toward Practical Deployment of Reference-Based Phishing\n  Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PhishIntel: Toward Practical Deployment of Reference-Based Phishing\n  Detection"
                },
                "summary": "Phishing is a critical cyber threat, exploiting deceptive tactics to\ncompromise victims and cause significant financial losses. While\nreference-based phishing detectors (RBPDs) have achieved notable advancements\nin detection accuracy, their real-world deployment is hindered by challenges\nsuch as high latency and inefficiency in URL analysis. To address these\nlimitations, we present PhishIntel, an end-to-end phishing detection system for\nreal-world deployment. PhishIntel intelligently determines whether a URL can be\nprocessed immediately or not, segmenting the detection process into two\ndistinct tasks: a fast task that checks against local blacklists and result\ncache, and a slow task that conducts online blacklist verification, URL\ncrawling, and webpage analysis using an RBPD. This fast-slow task system\narchitecture ensures low response latency while retaining the robust detection\ncapabilities of RBPDs for zero-day phishing threats. Furthermore, we develop\ntwo downstream applications based on PhishIntel: a phishing intelligence\nplatform and a phishing email detection plugin for Microsoft Outlook,\ndemonstrating its practical efficacy and utility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Phishing is a critical cyber threat, exploiting deceptive tactics to\ncompromise victims and cause significant financial losses. While\nreference-based phishing detectors (RBPDs) have achieved notable advancements\nin detection accuracy, their real-world deployment is hindered by challenges\nsuch as high latency and inefficiency in URL analysis. To address these\nlimitations, we present PhishIntel, an end-to-end phishing detection system for\nreal-world deployment. PhishIntel intelligently determines whether a URL can be\nprocessed immediately or not, segmenting the detection process into two\ndistinct tasks: a fast task that checks against local blacklists and result\ncache, and a slow task that conducts online blacklist verification, URL\ncrawling, and webpage analysis using an RBPD. This fast-slow task system\narchitecture ensures low response latency while retaining the robust detection\ncapabilities of RBPDs for zero-day phishing threats. Furthermore, we develop\ntwo downstream applications based on PhishIntel: a phishing intelligence\nplatform and a phishing email detection plugin for Microsoft Outlook,\ndemonstrating its practical efficacy and utility."
                },
                "authors": [
                    {
                        "name": "Yuexin Li"
                    },
                    {
                        "name": "Hiok Kuek Tan"
                    },
                    {
                        "name": "Qiaoran Meng"
                    },
                    {
                        "name": "Mei Lin Lock"
                    },
                    {
                        "name": "Tri Cao"
                    },
                    {
                        "name": "Shumin Deng"
                    },
                    {
                        "name": "Nay Oo"
                    },
                    {
                        "name": "Hoon Wei Lim"
                    },
                    {
                        "name": "Bryan Hooi"
                    }
                ],
                "author_detail": {
                    "name": "Bryan Hooi"
                },
                "author": "Bryan Hooi",
                "arxiv_doi": "10.1145/3701716.3715192",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3701716.3715192",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.09057v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09057v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by WWW 2025 (Demo Track)",
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10220v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10220v1",
                "updated": "2025-02-14T15:14:53Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    15,
                    14,
                    53,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-14T15:14:53Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    15,
                    14,
                    53,
                    4,
                    45,
                    0
                ],
                "title": "Optimal and Coordinated Voltage Control: Case Study on a 132 kV\n  Norwegian Grid Subsystem",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimal and Coordinated Voltage Control: Case Study on a 132 kV\n  Norwegian Grid Subsystem"
                },
                "summary": "This work presents a framework for dynamic performance assessment of the\nhigher layers in the hierarchical voltage regulation scheme, with case studies\napplied to specific areas of the Norwegian grid. Unlike the primary (PVR)\nlevel, the secondary (SVR) and tertiary (TVR) levels are not tuned to a single\ndevice at a time, handling instead several reactive power resources available\nwithin a control zone including generator units, static VAr compensators and\nothers. Proper SVR-TVR coordination for realistic transmission systems is a\nchallenging topic at the core of many ongoing discussions in voltage control\nliterature. Special focus is placed on practical considerations from the system\noperator perspective, since this research is also aimed at simplifying daily\ncontrol centre routines. Dynamic simulation results concern a 21-bus equivalent\nof a 132 kV network model that accurately represents a Norwegian grid\nsubsystem. Case studies address daily grid operation with real-life load demand\nand wind power generation profiles, showing that the proposed strategy is\neffective not only to minimize total active power losses as much as possible\nwithin system-wide limitations, but also to maintain adequate voltage profiles\nand reactive power flows. Findings pertaining to this work showcase the\nbenefits of applying hierarchical voltage regulation layers as an asset to\nday-to-day control center management of a realistic transmission network.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work presents a framework for dynamic performance assessment of the\nhigher layers in the hierarchical voltage regulation scheme, with case studies\napplied to specific areas of the Norwegian grid. Unlike the primary (PVR)\nlevel, the secondary (SVR) and tertiary (TVR) levels are not tuned to a single\ndevice at a time, handling instead several reactive power resources available\nwithin a control zone including generator units, static VAr compensators and\nothers. Proper SVR-TVR coordination for realistic transmission systems is a\nchallenging topic at the core of many ongoing discussions in voltage control\nliterature. Special focus is placed on practical considerations from the system\noperator perspective, since this research is also aimed at simplifying daily\ncontrol centre routines. Dynamic simulation results concern a 21-bus equivalent\nof a 132 kV network model that accurately represents a Norwegian grid\nsubsystem. Case studies address daily grid operation with real-life load demand\nand wind power generation profiles, showing that the proposed strategy is\neffective not only to minimize total active power losses as much as possible\nwithin system-wide limitations, but also to maintain adequate voltage profiles\nand reactive power flows. Findings pertaining to this work showcase the\nbenefits of applying hierarchical voltage regulation layers as an asset to\nday-to-day control center management of a realistic transmission network."
                },
                "authors": [
                    {
                        "name": "Hugo Rodrigues de Brito"
                    },
                    {
                        "name": "Daniel Simon Baltensperger"
                    },
                    {
                        "name": "Kjetil Obstfelder Uhlen"
                    }
                ],
                "author_detail": {
                    "name": "Kjetil Obstfelder Uhlen"
                },
                "author": "Kjetil Obstfelder Uhlen",
                "arxiv_comment": "11 pages, 8 figures, CIGRE Symposium 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10220v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10220v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10167v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10167v1",
                "updated": "2025-02-14T13:55:01Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    13,
                    55,
                    1,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-14T13:55:01Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    13,
                    55,
                    1,
                    4,
                    45,
                    0
                ],
                "title": "Modeling and Simulating Emerging Memory Technologies: A Tutorial",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling and Simulating Emerging Memory Technologies: A Tutorial"
                },
                "summary": "Non-volatile Memory (NVM) technologies present a promising alternative to\ntraditional volatile memories such as SRAM and DRAM. Due to the limited\navailability of real NVM devices, simulators play a crucial role in\narchitectural exploration and hardware-software co-design. This tutorial\npresents a simulation toolchain through four detailed case studies, showcasing\nits applicability to various domains of system design, including hybrid\nmain-memory and cache, compute-in-memory, and wear-leveling design. These case\nstudies provide the reader with practical insights on customizing the toolchain\nfor their specific research needs. The source code is open-sourced.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Non-volatile Memory (NVM) technologies present a promising alternative to\ntraditional volatile memories such as SRAM and DRAM. Due to the limited\navailability of real NVM devices, simulators play a crucial role in\narchitectural exploration and hardware-software co-design. This tutorial\npresents a simulation toolchain through four detailed case studies, showcasing\nits applicability to various domains of system design, including hybrid\nmain-memory and cache, compute-in-memory, and wear-leveling design. These case\nstudies provide the reader with practical insights on customizing the toolchain\nfor their specific research needs. The source code is open-sourced."
                },
                "authors": [
                    {
                        "name": "Yun-Chih Chen"
                    },
                    {
                        "name": "Tristan Seidl"
                    },
                    {
                        "name": "Nils Hölscher"
                    },
                    {
                        "name": "Christian Hakert"
                    },
                    {
                        "name": "Minh Duy Truong"
                    },
                    {
                        "name": "Jian-Jia Chen"
                    },
                    {
                        "name": "João Paulo C. de Lima"
                    },
                    {
                        "name": "Asif Ali Khan"
                    },
                    {
                        "name": "Jeronimo Castrillon"
                    },
                    {
                        "name": "Ali Nezhadi"
                    },
                    {
                        "name": "Lokesh Siddhu"
                    },
                    {
                        "name": "Hassan Nassar"
                    },
                    {
                        "name": "Mahta Mayahinia"
                    },
                    {
                        "name": "Mehdi Baradaran Tahoori"
                    },
                    {
                        "name": "Jörg Henkel"
                    },
                    {
                        "name": "Nils Wilbert"
                    },
                    {
                        "name": "Stefan Wildermann"
                    },
                    {
                        "name": "Jürgen Teich"
                    }
                ],
                "author_detail": {
                    "name": "Jürgen Teich"
                },
                "author": "Jürgen Teich",
                "arxiv_comment": "DFG Priority Program 2377 - Disruptive Memory Technologies",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10167v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10167v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09921v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09921v1",
                "updated": "2025-02-14T05:19:46Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    5,
                    19,
                    46,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-14T05:19:46Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    5,
                    19,
                    46,
                    4,
                    45,
                    0
                ],
                "title": "INF^2: High-Throughput Generative Inference of Large Language Models\n  using Near-Storage Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "INF^2: High-Throughput Generative Inference of Large Language Models\n  using Near-Storage Processing"
                },
                "summary": "The growing memory and computational demands of large language models (LLMs)\nfor generative inference present significant challenges for practical\ndeployment. One promising solution to address these challenges is\noffloading-based batched inference, which leverages host memory and disk as an\nextended memory hierarchy for GPUs. While the approach cost-effectively enables\nLLM inference, its performance is limited by substantial I/O overhead,\nprimarily due to the large key-value (KV) cache sizes, which increase with\nbatch size and LLM context window length.\n  In this paper, we introduce INFerence-INFinity (INF^2), a framework that\nboosts generative inference throughput using computational storage devices\n(CSDs). The core of INF^2 is attention-near storage, which offloads\nmemory-intensive self-attention operations to near-storage accelerators,\nsignificantly reducing traffic through the system interconnect. We also propose\ndelayed KV cache writeback to hide storage write latency by delaying newly\ngenerated KV cache writes until the cache reaches sufficient size in system\nmemory. Additionally, we introduce cooperative X-cache, a technique designed to\nfurther trade off the remaining memory capacity for storage bandwidth. Our\nmethods effectively minimize idle time for computation, improving the overall\nthroughput.\n  To demonstrate the effectiveness of our approach, \\thiswork has been\nimplemented on PyTorch and evaluated on a real system. Our experiments show\nthat INF^2 achieves up to 3.46$\\times$ throughput improvement compared to\nstate-of-the-art baselines. We will open-source INF^2 to facilitate broader\nadoption.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing memory and computational demands of large language models (LLMs)\nfor generative inference present significant challenges for practical\ndeployment. One promising solution to address these challenges is\noffloading-based batched inference, which leverages host memory and disk as an\nextended memory hierarchy for GPUs. While the approach cost-effectively enables\nLLM inference, its performance is limited by substantial I/O overhead,\nprimarily due to the large key-value (KV) cache sizes, which increase with\nbatch size and LLM context window length.\n  In this paper, we introduce INFerence-INFinity (INF^2), a framework that\nboosts generative inference throughput using computational storage devices\n(CSDs). The core of INF^2 is attention-near storage, which offloads\nmemory-intensive self-attention operations to near-storage accelerators,\nsignificantly reducing traffic through the system interconnect. We also propose\ndelayed KV cache writeback to hide storage write latency by delaying newly\ngenerated KV cache writes until the cache reaches sufficient size in system\nmemory. Additionally, we introduce cooperative X-cache, a technique designed to\nfurther trade off the remaining memory capacity for storage bandwidth. Our\nmethods effectively minimize idle time for computation, improving the overall\nthroughput.\n  To demonstrate the effectiveness of our approach, \\thiswork has been\nimplemented on PyTorch and evaluated on a real system. Our experiments show\nthat INF^2 achieves up to 3.46$\\times$ throughput improvement compared to\nstate-of-the-art baselines. We will open-source INF^2 to facilitate broader\nadoption."
                },
                "authors": [
                    {
                        "name": "Hongsun Jang"
                    },
                    {
                        "name": "Siung Noh"
                    },
                    {
                        "name": "Changmin Shin"
                    },
                    {
                        "name": "Jaewon Jung"
                    },
                    {
                        "name": "Jaeyong Song"
                    },
                    {
                        "name": "Jinho Lee"
                    }
                ],
                "author_detail": {
                    "name": "Jinho Lee"
                },
                "author": "Jinho Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09921v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09921v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09888v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09888v1",
                "updated": "2025-02-14T03:25:09Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    3,
                    25,
                    9,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-14T03:25:09Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    3,
                    25,
                    9,
                    4,
                    45,
                    0
                ],
                "title": "An Efficient Large Recommendation Model: Towards a Resource-Optimal\n  Scaling Law",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Efficient Large Recommendation Model: Towards a Resource-Optimal\n  Scaling Law"
                },
                "summary": "The pursuit of scaling up recommendation models confronts intrinsic tensions\nbetween expanding model capacity and preserving computational tractability.\nWhile prior studies have explored scaling laws for recommendation systems,\ntheir resource-intensive paradigms -- often requiring tens of thousands of A100\nGPU hours -- remain impractical for most industrial applications. This work\naddresses a critical gap: achieving sustainable model scaling under strict\ncomputational budgets. We propose Climber, a resource-efficient recommendation\nframework comprising two synergistic components: the ASTRO model architecture\nfor algorithmic innovation and the TURBO acceleration framework for engineering\noptimization. ASTRO (Adaptive Scalable Transformer for RecOmmendation) adopts\ntwo core innovations: (1) multi-scale sequence partitioning that reduces\nattention complexity from O(n^2d) to O(n^2d/Nb) via hierarchical blocks,\nenabling more efficient scaling with sequence length; (2) dynamic temperature\nmodulation that adaptively adjusts attention scores for multimodal\ndistributions arising from inherent multi-scenario and multi-behavior\ninteractions. Complemented by TURBO (Two-stage Unified Ranking with Batched\nOutput), a co-designed acceleration framework integrating gradient-aware\nfeature compression and memory-efficient Key-Value caching, Climber achieves\n5.15x throughput gains without performance degradation. Comprehensive offline\nexperiments on multiple datasets validate that Climber exhibits a more ideal\nscaling curve. To our knowledge, this is the first publicly documented\nframework where controlled model scaling drives continuous online metric growth\n(12.19% overall lift) without prohibitive resource costs. Climber has been\nsuccessfully deployed on Netease Cloud Music, one of China's largest music\nstreaming platforms, serving tens of millions of users daily.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The pursuit of scaling up recommendation models confronts intrinsic tensions\nbetween expanding model capacity and preserving computational tractability.\nWhile prior studies have explored scaling laws for recommendation systems,\ntheir resource-intensive paradigms -- often requiring tens of thousands of A100\nGPU hours -- remain impractical for most industrial applications. This work\naddresses a critical gap: achieving sustainable model scaling under strict\ncomputational budgets. We propose Climber, a resource-efficient recommendation\nframework comprising two synergistic components: the ASTRO model architecture\nfor algorithmic innovation and the TURBO acceleration framework for engineering\noptimization. ASTRO (Adaptive Scalable Transformer for RecOmmendation) adopts\ntwo core innovations: (1) multi-scale sequence partitioning that reduces\nattention complexity from O(n^2d) to O(n^2d/Nb) via hierarchical blocks,\nenabling more efficient scaling with sequence length; (2) dynamic temperature\nmodulation that adaptively adjusts attention scores for multimodal\ndistributions arising from inherent multi-scenario and multi-behavior\ninteractions. Complemented by TURBO (Two-stage Unified Ranking with Batched\nOutput), a co-designed acceleration framework integrating gradient-aware\nfeature compression and memory-efficient Key-Value caching, Climber achieves\n5.15x throughput gains without performance degradation. Comprehensive offline\nexperiments on multiple datasets validate that Climber exhibits a more ideal\nscaling curve. To our knowledge, this is the first publicly documented\nframework where controlled model scaling drives continuous online metric growth\n(12.19% overall lift) without prohibitive resource costs. Climber has been\nsuccessfully deployed on Netease Cloud Music, one of China's largest music\nstreaming platforms, serving tens of millions of users daily."
                },
                "authors": [
                    {
                        "name": "Songpei Xu"
                    },
                    {
                        "name": "Shijia Wang"
                    },
                    {
                        "name": "Da Guo"
                    },
                    {
                        "name": "Xianwen Guo"
                    },
                    {
                        "name": "Qiang Xiao"
                    },
                    {
                        "name": "Fangjian Li"
                    },
                    {
                        "name": "Chuanjiang Luo"
                    }
                ],
                "author_detail": {
                    "name": "Chuanjiang Luo"
                },
                "author": "Chuanjiang Luo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09888v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09888v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09726v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09726v1",
                "updated": "2025-02-13T19:16:39Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    19,
                    16,
                    39,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T19:16:39Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    19,
                    16,
                    39,
                    3,
                    44,
                    0
                ],
                "title": "Analysis of Robust and Secure DNS Protocols for IoT Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analysis of Robust and Secure DNS Protocols for IoT Devices"
                },
                "summary": "The DNS (Domain Name System) protocol has been in use since the early days of\nthe Internet. Although DNS as a de facto networking protocol had no security\nconsiderations in its early years, there have been many security enhancements,\nsuch as DNSSec (Domain Name System Security Extensions), DoT (DNS over\nTransport Layer Security), DoH (DNS over HTTPS) and DoQ (DNS over QUIC). With\nall these security improvements, it is not yet clear what resource-constrained\nInternet-of-Things (IoT) devices should be used for robustness. In this paper,\nwe investigate different DNS security approaches using an edge DNS resolver\nimplemented as a Virtual Network Function (VNF) to replicate the impact of the\nprotocol from an IoT perspective and compare their performances under different\nconditions. We present our results for cache-based and non-cached responses and\nevaluate the corresponding security benefits. Our results and framework can\ngreatly help consumers, manufacturers, and the research community decide and\nimplement their DNS protocols depending on the given dynamic network conditions\nand enable robust Internet access via DNS for different devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The DNS (Domain Name System) protocol has been in use since the early days of\nthe Internet. Although DNS as a de facto networking protocol had no security\nconsiderations in its early years, there have been many security enhancements,\nsuch as DNSSec (Domain Name System Security Extensions), DoT (DNS over\nTransport Layer Security), DoH (DNS over HTTPS) and DoQ (DNS over QUIC). With\nall these security improvements, it is not yet clear what resource-constrained\nInternet-of-Things (IoT) devices should be used for robustness. In this paper,\nwe investigate different DNS security approaches using an edge DNS resolver\nimplemented as a Virtual Network Function (VNF) to replicate the impact of the\nprotocol from an IoT perspective and compare their performances under different\nconditions. We present our results for cache-based and non-cached responses and\nevaluate the corresponding security benefits. Our results and framework can\ngreatly help consumers, manufacturers, and the research community decide and\nimplement their DNS protocols depending on the given dynamic network conditions\nand enable robust Internet access via DNS for different devices."
                },
                "authors": [
                    {
                        "name": "Abdullah Aydeger"
                    },
                    {
                        "name": "Sanzida Hoque"
                    },
                    {
                        "name": "Engin Zeydan"
                    },
                    {
                        "name": "Kapal Dev"
                    }
                ],
                "author_detail": {
                    "name": "Kapal Dev"
                },
                "author": "Kapal Dev",
                "arxiv_comment": "6 pages, 2 tables, 2 figures. This paper has been accepted in the\n  2025 IEEE International Conference on Communications (ICC): SAC Cloud\n  Computing, Networking, and Storage Track. The final version will be published\n  in the IEEE Xplore",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09726v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09726v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09720v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09720v1",
                "updated": "2025-02-13T19:11:40Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    19,
                    11,
                    40,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T19:11:40Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    19,
                    11,
                    40,
                    3,
                    44,
                    0
                ],
                "title": "NestQuant: Nested Lattice Quantization for Matrix Products and LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NestQuant: Nested Lattice Quantization for Matrix Products and LLMs"
                },
                "summary": "Post-training quantization (PTQ) has emerged as a critical technique for\nefficient deployment of large language models (LLMs). This work proposes\nNestQuant, a novel PTQ scheme for weights and activations that is based on\nself-similar nested lattices. Recent work have mathematically shown such\nquantizers to be information-theoretically optimal for low-precision matrix\nmultiplication. We implement a practical low-complexity version of NestQuant\nbased on Gosset lattice, making it a drop-in quantizer for any matrix\nmultiplication step (e.g., in self-attention, MLP etc). For example, NestQuant\nquantizes weights, KV-cache, and activations of Llama-3-8B to 4 bits, achieving\nperplexity of 6.6 on wikitext2. This represents more than 55% reduction in\nperplexity gap with respect to unquantized model (perplexity of 6.14) compared\nto state-of-the-art Meta's SpinQuant (perplexity 7.3). Comparisons on various\nLLM evaluation benchmarks also show a reduction in performance degradation\ninduced by quantization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-training quantization (PTQ) has emerged as a critical technique for\nefficient deployment of large language models (LLMs). This work proposes\nNestQuant, a novel PTQ scheme for weights and activations that is based on\nself-similar nested lattices. Recent work have mathematically shown such\nquantizers to be information-theoretically optimal for low-precision matrix\nmultiplication. We implement a practical low-complexity version of NestQuant\nbased on Gosset lattice, making it a drop-in quantizer for any matrix\nmultiplication step (e.g., in self-attention, MLP etc). For example, NestQuant\nquantizes weights, KV-cache, and activations of Llama-3-8B to 4 bits, achieving\nperplexity of 6.6 on wikitext2. This represents more than 55% reduction in\nperplexity gap with respect to unquantized model (perplexity of 6.14) compared\nto state-of-the-art Meta's SpinQuant (perplexity 7.3). Comparisons on various\nLLM evaluation benchmarks also show a reduction in performance degradation\ninduced by quantization."
                },
                "authors": [
                    {
                        "name": "Semyon Savkin"
                    },
                    {
                        "name": "Eitan Porat"
                    },
                    {
                        "name": "Or Ordentlich"
                    },
                    {
                        "name": "Yury Polyanskiy"
                    }
                ],
                "author_detail": {
                    "name": "Yury Polyanskiy"
                },
                "author": "Yury Polyanskiy",
                "arxiv_comment": "16 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09720v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09720v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07864v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07864v2",
                "updated": "2025-02-13T18:07:04Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    18,
                    7,
                    4,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-11T18:20:18Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    18,
                    20,
                    18,
                    1,
                    42,
                    0
                ],
                "title": "TransMLA: Multi-Head Latent Attention Is All You Need",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TransMLA: Multi-Head Latent Attention Is All You Need"
                },
                "summary": "Modern large language models (LLMs) often encounter communication bottlenecks\non current hardware, rather than purely computational constraints. Multi-head\nLatent Attention (MLA) tackles this challenge by using low-rank matrices in the\nkey-value (KV) layers, thereby allowing compressed latent KV states to be\ncached. This approach significantly reduces the KV cache size relative to\ntraditional multi-head attention, leading to faster inference. Moreover, MLA\nemploys an up-projection matrix to increase expressiveness, trading additional\ncomputation for reduced communication overhead. Although MLA has demonstrated\nefficiency and effectiveness in Deepseek V2/V3/R1, many major model providers\nstill rely on Group Query Attention (GQA) and have not announced any plans to\nadopt MLA. In this paper, we show that GQA can always be represented by MLA\nwhile maintaining the same KV cache overhead, but the converse does not hold.\nTo encourage broader use of MLA, we introduce TransMLA, a post-training method\nthat converts widely used GQA-based pre-trained models (e.g., LLaMA, Qwen,\nMixtral) into MLA-based models. After conversion, the model can undergo\nadditional training to boost expressiveness without increasing the KV cache\nsize. Furthermore, we plan to develop MLA-specific inference acceleration\ntechniques to preserve low latency in transformed models, thus enabling more\nefficient distillation of Deepseek R1.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern large language models (LLMs) often encounter communication bottlenecks\non current hardware, rather than purely computational constraints. Multi-head\nLatent Attention (MLA) tackles this challenge by using low-rank matrices in the\nkey-value (KV) layers, thereby allowing compressed latent KV states to be\ncached. This approach significantly reduces the KV cache size relative to\ntraditional multi-head attention, leading to faster inference. Moreover, MLA\nemploys an up-projection matrix to increase expressiveness, trading additional\ncomputation for reduced communication overhead. Although MLA has demonstrated\nefficiency and effectiveness in Deepseek V2/V3/R1, many major model providers\nstill rely on Group Query Attention (GQA) and have not announced any plans to\nadopt MLA. In this paper, we show that GQA can always be represented by MLA\nwhile maintaining the same KV cache overhead, but the converse does not hold.\nTo encourage broader use of MLA, we introduce TransMLA, a post-training method\nthat converts widely used GQA-based pre-trained models (e.g., LLaMA, Qwen,\nMixtral) into MLA-based models. After conversion, the model can undergo\nadditional training to boost expressiveness without increasing the KV cache\nsize. Furthermore, we plan to develop MLA-specific inference acceleration\ntechniques to preserve low latency in transformed models, thus enabling more\nefficient distillation of Deepseek R1."
                },
                "authors": [
                    {
                        "name": "Fanxu Meng"
                    },
                    {
                        "name": "Zengwei Yao"
                    },
                    {
                        "name": "Muhan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Muhan Zhang"
                },
                "author": "Muhan Zhang",
                "arxiv_comment": "https://github.com/fxmeng/TransMLA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07864v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07864v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09541v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09541v1",
                "updated": "2025-02-13T17:57:05Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    17,
                    57,
                    5,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T17:57:05Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    17,
                    57,
                    5,
                    3,
                    44,
                    0
                ],
                "title": "Vortex: Overcoming Memory Capacity Limitations in GPU-Accelerated\n  Large-Scale Data Analytics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vortex: Overcoming Memory Capacity Limitations in GPU-Accelerated\n  Large-Scale Data Analytics"
                },
                "summary": "Despite the high computational throughput of GPUs, limited memory capacity\nand bandwidth-limited CPU-GPU communication via PCIe links remain significant\nbottlenecks for accelerating large-scale data analytics workloads. This paper\nintroduces Vortex, a GPU-accelerated framework designed for data analytics\nworkloads that exceed GPU memory capacity. A key aspect of our framework is an\noptimized IO primitive that leverages all available PCIe links in multi-GPU\nsystems for the IO demand of a single target GPU. It routes data through other\nGPUs to such target GPU that handles IO-intensive analytics tasks. This\napproach is advantageous when other GPUs are occupied with compute-bound\nworkloads, such as popular AI applications that typically underutilize IO\nresources. We also introduce a novel programming model that separates GPU\nkernel development from IO scheduling, reducing programmer burden and enabling\nGPU code reuse. Additionally, we present the design of certain important query\noperators and discuss a late materialization technique based on GPU's zero-copy\nmemory access. Without caching any data in GPU memory, Vortex improves the\nperformance of the state-of-the-art GPU baseline, Proteus, by 5.7$\\times$ on\naverage and enhances price performance by 2.5$\\times$ compared to a CPU-based\nDuckDB baseline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the high computational throughput of GPUs, limited memory capacity\nand bandwidth-limited CPU-GPU communication via PCIe links remain significant\nbottlenecks for accelerating large-scale data analytics workloads. This paper\nintroduces Vortex, a GPU-accelerated framework designed for data analytics\nworkloads that exceed GPU memory capacity. A key aspect of our framework is an\noptimized IO primitive that leverages all available PCIe links in multi-GPU\nsystems for the IO demand of a single target GPU. It routes data through other\nGPUs to such target GPU that handles IO-intensive analytics tasks. This\napproach is advantageous when other GPUs are occupied with compute-bound\nworkloads, such as popular AI applications that typically underutilize IO\nresources. We also introduce a novel programming model that separates GPU\nkernel development from IO scheduling, reducing programmer burden and enabling\nGPU code reuse. Additionally, we present the design of certain important query\noperators and discuss a late materialization technique based on GPU's zero-copy\nmemory access. Without caching any data in GPU memory, Vortex improves the\nperformance of the state-of-the-art GPU baseline, Proteus, by 5.7$\\times$ on\naverage and enhances price performance by 2.5$\\times$ compared to a CPU-based\nDuckDB baseline."
                },
                "authors": [
                    {
                        "name": "Yichao Yuan"
                    },
                    {
                        "name": "Advait Iyer"
                    },
                    {
                        "name": "Lin Ma"
                    },
                    {
                        "name": "Nishil Talati"
                    }
                ],
                "author_detail": {
                    "name": "Nishil Talati"
                },
                "author": "Nishil Talati",
                "arxiv_comment": "VLDB 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09541v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09541v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07115v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07115v2",
                "updated": "2025-02-13T12:54:36Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    12,
                    54,
                    36,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-10T23:11:44Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    23,
                    11,
                    44,
                    0,
                    41,
                    0
                ],
                "title": "Online Scheduling for LLM Inference with KV Cache Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online Scheduling for LLM Inference with KV Cache Constraints"
                },
                "summary": "Large Language Model (LLM) inference, where a trained model generates text\none word at a time in response to user prompts, is a computationally intensive\nprocess requiring efficient scheduling to optimize latency and resource\nutilization. A key challenge in LLM inference is the management of the\nKey-Value (KV) cache, which reduces redundant computations but introduces\nmemory constraints. In this work, we model LLM inference with KV cache\nconstraints theoretically and propose novel batching and scheduling algorithms\nthat minimize inference latency while effectively managing the KV cache's\nmemory.\n  We analyze both semi-online and fully online scheduling models, and our\nresults are threefold. First, we provide a polynomial-time algorithm that\nachieves exact optimality in terms of average latency in the semi-online prompt\narrival model. Second, in the fully online case with a stochastic prompt\narrival, we introduce an efficient online scheduling algorithm with constant\nregret. Third, we prove that no algorithm (deterministic or randomized) can\nachieve a constant competitive ratio in fully online adversarial settings. Our\nempirical evaluations on a public LLM inference dataset, using the Llama-70B\nmodel on A100 GPUs, show that our approach significantly outperforms benchmark\nalgorithms used currently in practice, achieving lower latency while reducing\nenergy consumption. Overall, our results offer a path toward more sustainable\nand cost-effective LLM deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) inference, where a trained model generates text\none word at a time in response to user prompts, is a computationally intensive\nprocess requiring efficient scheduling to optimize latency and resource\nutilization. A key challenge in LLM inference is the management of the\nKey-Value (KV) cache, which reduces redundant computations but introduces\nmemory constraints. In this work, we model LLM inference with KV cache\nconstraints theoretically and propose novel batching and scheduling algorithms\nthat minimize inference latency while effectively managing the KV cache's\nmemory.\n  We analyze both semi-online and fully online scheduling models, and our\nresults are threefold. First, we provide a polynomial-time algorithm that\nachieves exact optimality in terms of average latency in the semi-online prompt\narrival model. Second, in the fully online case with a stochastic prompt\narrival, we introduce an efficient online scheduling algorithm with constant\nregret. Third, we prove that no algorithm (deterministic or randomized) can\nachieve a constant competitive ratio in fully online adversarial settings. Our\nempirical evaluations on a public LLM inference dataset, using the Llama-70B\nmodel on A100 GPUs, show that our approach significantly outperforms benchmark\nalgorithms used currently in practice, achieving lower latency while reducing\nenergy consumption. Overall, our results offer a path toward more sustainable\nand cost-effective LLM deployment."
                },
                "authors": [
                    {
                        "name": "Patrick Jaillet"
                    },
                    {
                        "name": "Jiashuo Jiang"
                    },
                    {
                        "name": "Chara Podimata"
                    },
                    {
                        "name": "Zijie Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Zijie Zhou"
                },
                "author": "Zijie Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07115v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07115v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09003v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09003v1",
                "updated": "2025-02-13T06:44:33Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    6,
                    44,
                    33,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T06:44:33Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    6,
                    44,
                    33,
                    3,
                    44,
                    0
                ],
                "title": "RoSTE: An Efficient Quantization-Aware Supervised Fine-Tuning Approach\n  for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RoSTE: An Efficient Quantization-Aware Supervised Fine-Tuning Approach\n  for Large Language Models"
                },
                "summary": "Supervised fine-tuning is a standard method for adapting pre-trained large\nlanguage models (LLMs) to downstream tasks. Quantization has been recently\nstudied as a post-training technique for efficient LLM deployment. To obtain\nquantized fine-tuned LLMs, conventional pipelines would first fine-tune the\npre-trained models, followed by post-training quantization. This often yields\nsuboptimal performance as it fails to leverage the synergy between fine-tuning\nand quantization. To effectively realize low-bit quantization of weights,\nactivations, and KV caches in LLMs, we propose an algorithm named Rotated\nStraight-Through-Estimator (RoSTE), which combines quantization-aware\nsupervised fine-tuning (QA-SFT) with an adaptive rotation strategy that\nidentifies an effective rotation configuration to reduce activation outliers.\nWe provide theoretical insights on RoSTE by analyzing its prediction error when\napplied to an overparameterized least square quantized training problem. Our\nfindings reveal that the prediction error is directly proportional to the\nquantization error of the converged weights, which can be effectively managed\nthrough an optimized rotation configuration. Experiments on Pythia and Llama\nmodels of different sizes demonstrate the effectiveness of RoSTE. Compared to\nexisting post-SFT quantization baselines, our method consistently achieves\nsuperior performances across various tasks and different LLM architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Supervised fine-tuning is a standard method for adapting pre-trained large\nlanguage models (LLMs) to downstream tasks. Quantization has been recently\nstudied as a post-training technique for efficient LLM deployment. To obtain\nquantized fine-tuned LLMs, conventional pipelines would first fine-tune the\npre-trained models, followed by post-training quantization. This often yields\nsuboptimal performance as it fails to leverage the synergy between fine-tuning\nand quantization. To effectively realize low-bit quantization of weights,\nactivations, and KV caches in LLMs, we propose an algorithm named Rotated\nStraight-Through-Estimator (RoSTE), which combines quantization-aware\nsupervised fine-tuning (QA-SFT) with an adaptive rotation strategy that\nidentifies an effective rotation configuration to reduce activation outliers.\nWe provide theoretical insights on RoSTE by analyzing its prediction error when\napplied to an overparameterized least square quantized training problem. Our\nfindings reveal that the prediction error is directly proportional to the\nquantization error of the converged weights, which can be effectively managed\nthrough an optimized rotation configuration. Experiments on Pythia and Llama\nmodels of different sizes demonstrate the effectiveness of RoSTE. Compared to\nexisting post-SFT quantization baselines, our method consistently achieves\nsuperior performances across various tasks and different LLM architectures."
                },
                "authors": [
                    {
                        "name": "Quan Wei"
                    },
                    {
                        "name": "Chung-Yiu Yau"
                    },
                    {
                        "name": "Hoi-To Wai"
                    },
                    {
                        "name": "Yang"
                    },
                    {
                        "name": "Zhao"
                    },
                    {
                        "name": "Dongyeop Kang"
                    },
                    {
                        "name": "Youngsuk Park"
                    },
                    {
                        "name": "Mingyi Hong"
                    }
                ],
                "author_detail": {
                    "name": "Mingyi Hong"
                },
                "arxiv_affiliation": "Katie",
                "author": "Mingyi Hong",
                "arxiv_comment": "18 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09003v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09003v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08982v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08982v1",
                "updated": "2025-02-13T05:40:28Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    5,
                    40,
                    28,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T05:40:28Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    5,
                    40,
                    28,
                    3,
                    44,
                    0
                ],
                "title": "Outback: Fast and Communication-efficient Index for Key-Value Store on\n  Disaggregated Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Outback: Fast and Communication-efficient Index for Key-Value Store on\n  Disaggregated Memory"
                },
                "summary": "Disaggregated memory systems achieve resource utilization efficiency and\nsystem scalability by distributing computation and memory resources into\ndistinct pools of nodes. RDMA is an attractive solution to support\nhigh-throughput communication between different disaggregated resource pools.\nHowever, existing RDMA solutions face a dilemma: one-sided RDMA completely\nbypasses computation at memory nodes, but its communication takes multiple\nround trips; two-sided RDMA achieves one-round-trip communication but requires\nnon-trivial computation for index lookups at memory nodes, which violates the\nprinciple of disaggregated memory. This work presents Outback, a novel indexing\nsolution for key-value stores with a one-round-trip RDMA-based network that\ndoes not incur computation-heavy tasks at memory nodes. Outback is the first to\nutilize dynamic minimal perfect hashing and separates its index into two\ncomponents: one memory-efficient and compute-heavy component at compute nodes\nand the other memory-heavy and compute-efficient component at memory nodes. We\nimplement a prototype of Outback and evaluate its performance in a public\ncloud. The experimental results show that Outback achieves higher throughput\nthan both the state-of-the-art one-sided RDMA and two-sided RDMA-based\nin-memory KVS by 1.06-5.03x, due to the unique strength of applying a separated\nperfect hashing index.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disaggregated memory systems achieve resource utilization efficiency and\nsystem scalability by distributing computation and memory resources into\ndistinct pools of nodes. RDMA is an attractive solution to support\nhigh-throughput communication between different disaggregated resource pools.\nHowever, existing RDMA solutions face a dilemma: one-sided RDMA completely\nbypasses computation at memory nodes, but its communication takes multiple\nround trips; two-sided RDMA achieves one-round-trip communication but requires\nnon-trivial computation for index lookups at memory nodes, which violates the\nprinciple of disaggregated memory. This work presents Outback, a novel indexing\nsolution for key-value stores with a one-round-trip RDMA-based network that\ndoes not incur computation-heavy tasks at memory nodes. Outback is the first to\nutilize dynamic minimal perfect hashing and separates its index into two\ncomponents: one memory-efficient and compute-heavy component at compute nodes\nand the other memory-heavy and compute-efficient component at memory nodes. We\nimplement a prototype of Outback and evaluate its performance in a public\ncloud. The experimental results show that Outback achieves higher throughput\nthan both the state-of-the-art one-sided RDMA and two-sided RDMA-based\nin-memory KVS by 1.06-5.03x, due to the unique strength of applying a separated\nperfect hashing index."
                },
                "authors": [
                    {
                        "name": "Yi Liu"
                    },
                    {
                        "name": "Minghao Xie"
                    },
                    {
                        "name": "Shouqian Shi"
                    },
                    {
                        "name": "Yuanchao Xu"
                    },
                    {
                        "name": "Heiner Litz"
                    },
                    {
                        "name": "Chen Qian"
                    }
                ],
                "author_detail": {
                    "name": "Chen Qian"
                },
                "author": "Chen Qian",
                "arxiv_doi": "10.14778/3705829.3705849",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.14778/3705829.3705849",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.08982v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08982v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "PVLDB, 18(2): 335-348, 2024",
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08910v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08910v1",
                "updated": "2025-02-13T02:52:01Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    2,
                    52,
                    1,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T02:52:01Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    2,
                    52,
                    1,
                    3,
                    44,
                    0
                ],
                "title": "InfiniteHiP: Extending Language Model Context Up to 3 Million Tokens on\n  a Single GPU",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InfiniteHiP: Extending Language Model Context Up to 3 Million Tokens on\n  a Single GPU"
                },
                "summary": "In modern large language models (LLMs), handling very long context lengths\npresents significant challenges as it causes slower inference speeds and\nincreased memory costs. Additionally, most existing pre-trained LLMs fail to\ngeneralize beyond their original training sequence lengths. To enable efficient\nand practical long-context utilization, we introduce InfiniteHiP, a novel, and\npractical LLM inference framework that accelerates processing by dynamically\neliminating irrelevant context tokens through a modular hierarchical token\npruning algorithm. Our method also allows generalization to longer sequences by\nselectively applying various RoPE adjustment methods according to the internal\nattention patterns within LLMs. Furthermore, we offload the key-value cache to\nhost memory during inference, significantly reducing GPU memory pressure. As a\nresult, InfiniteHiP enables the processing of up to 3 million tokens on a\nsingle L40s 48GB GPU -- 3x larger -- without any permanent loss of context\ninformation. Our framework achieves an 18.95x speedup in attention decoding for\na 1 million token context without requiring additional training. We implement\nour method in the SGLang framework and demonstrate its effectiveness and\npracticality through extensive evaluations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In modern large language models (LLMs), handling very long context lengths\npresents significant challenges as it causes slower inference speeds and\nincreased memory costs. Additionally, most existing pre-trained LLMs fail to\ngeneralize beyond their original training sequence lengths. To enable efficient\nand practical long-context utilization, we introduce InfiniteHiP, a novel, and\npractical LLM inference framework that accelerates processing by dynamically\neliminating irrelevant context tokens through a modular hierarchical token\npruning algorithm. Our method also allows generalization to longer sequences by\nselectively applying various RoPE adjustment methods according to the internal\nattention patterns within LLMs. Furthermore, we offload the key-value cache to\nhost memory during inference, significantly reducing GPU memory pressure. As a\nresult, InfiniteHiP enables the processing of up to 3 million tokens on a\nsingle L40s 48GB GPU -- 3x larger -- without any permanent loss of context\ninformation. Our framework achieves an 18.95x speedup in attention decoding for\na 1 million token context without requiring additional training. We implement\nour method in the SGLang framework and demonstrate its effectiveness and\npracticality through extensive evaluations."
                },
                "authors": [
                    {
                        "name": "Heejun Lee"
                    },
                    {
                        "name": "Geon Park"
                    },
                    {
                        "name": "Jaduk Suh"
                    },
                    {
                        "name": "Sung Ju Hwang"
                    }
                ],
                "author_detail": {
                    "name": "Sung Ju Hwang"
                },
                "author": "Sung Ju Hwang",
                "arxiv_comment": "21 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08910v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08910v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.02690v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.02690v2",
                "updated": "2025-02-12T14:32:46Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    14,
                    32,
                    46,
                    2,
                    43,
                    0
                ],
                "published": "2024-04-03T12:37:34Z",
                "published_parsed": [
                    2024,
                    4,
                    3,
                    12,
                    37,
                    34,
                    2,
                    94,
                    0
                ],
                "title": "How Sparse Attention Approximates Exact Attention? Your Attention is\n  Naturally $n^C$-Sparse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Sparse Attention Approximates Exact Attention? Your Attention is\n  Naturally $n^C$-Sparse"
                },
                "summary": "Sparse Attention is a technique that approximates standard attention\ncomputation with sub-quadratic complexity. This is achieved by selectively\nignoring smaller entries in the attention matrix during the softmax function\ncomputation. Variations of this technique, such as pruning KV cache,\nsparsity-based fast attention, and Sparse Transformer, have been extensively\nutilized for efficient Large Language Models (LLMs) deployment. Despite its\nwidespread use, a theoretical understanding of the conditions under which\nsparse attention performs on par with traditional attention remains elusive.\nThis work aims to $\\textbf{bridge this gap by examining the inherent sparsity\nof standard attention processes}$. Our theoretical framework reveals several\nbrand-new key insights:\n  $\\bullet$ Attention is $n^{C}$-sparse, implying that considering only the\nlargest $\\Omega(n^{C})$ entries out of all $n$ entries is sufficient for sparse\nattention to approximate the exact attention matrix with decreasing loss. Here,\n$n$ represents the input length and $C \\in (0, 1)$ is a constant.\n  $\\bullet$ Stable $o(\\log(n))$-sparse attention, which approximates attention\ncomputation with $\\log(n)$ or fewer entries, may not be feasible since the\nerror will persist at a minimum of $O(1)$.\n  $\\bullet$ An adaptive strategy ($\\alpha \\cdot n^C, \\alpha \\in \\mathbb{R}$)\nfor the window size of efficient attention methods rather than a fixed one is\nguaranteed to perform more accurately and efficiently in a task for inference\non flexible context lengths.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse Attention is a technique that approximates standard attention\ncomputation with sub-quadratic complexity. This is achieved by selectively\nignoring smaller entries in the attention matrix during the softmax function\ncomputation. Variations of this technique, such as pruning KV cache,\nsparsity-based fast attention, and Sparse Transformer, have been extensively\nutilized for efficient Large Language Models (LLMs) deployment. Despite its\nwidespread use, a theoretical understanding of the conditions under which\nsparse attention performs on par with traditional attention remains elusive.\nThis work aims to $\\textbf{bridge this gap by examining the inherent sparsity\nof standard attention processes}$. Our theoretical framework reveals several\nbrand-new key insights:\n  $\\bullet$ Attention is $n^{C}$-sparse, implying that considering only the\nlargest $\\Omega(n^{C})$ entries out of all $n$ entries is sufficient for sparse\nattention to approximate the exact attention matrix with decreasing loss. Here,\n$n$ represents the input length and $C \\in (0, 1)$ is a constant.\n  $\\bullet$ Stable $o(\\log(n))$-sparse attention, which approximates attention\ncomputation with $\\log(n)$ or fewer entries, may not be feasible since the\nerror will persist at a minimum of $O(1)$.\n  $\\bullet$ An adaptive strategy ($\\alpha \\cdot n^C, \\alpha \\in \\mathbb{R}$)\nfor the window size of efficient attention methods rather than a fixed one is\nguaranteed to perform more accurately and efficiently in a task for inference\non flexible context lengths."
                },
                "authors": [
                    {
                        "name": "Yichuan Deng"
                    },
                    {
                        "name": "Zhao Song"
                    },
                    {
                        "name": "Jing Xiong"
                    },
                    {
                        "name": "Chiwun Yang"
                    }
                ],
                "author_detail": {
                    "name": "Chiwun Yang"
                },
                "author": "Chiwun Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.02690v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.02690v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05431v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05431v2",
                "updated": "2025-02-12T13:54:01Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    13,
                    54,
                    1,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-08T03:41:16Z",
                "published_parsed": [
                    2025,
                    2,
                    8,
                    3,
                    41,
                    16,
                    5,
                    39,
                    0
                ],
                "title": "APE: Faster and Longer Context-Augmented Generation via Adaptive\n  Parallel Encoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "APE: Faster and Longer Context-Augmented Generation via Adaptive\n  Parallel Encoding"
                },
                "summary": "Context-augmented generation (CAG) techniques, including RAG and ICL, require\nthe efficient combination of multiple contexts to generate responses to user\nqueries. Directly inputting these contexts as a sequence introduces a\nconsiderable computational burden by re-encoding the combined selection of\ncontexts for every request. To address this, we explore the promising potential\nof parallel encoding to independently pre-compute and cache each context's KV\nstates. This approach enables the direct loading of cached states during\ninference while accommodating more contexts through position reuse across\ncontexts. However, due to misalignments in attention distribution, directly\napplying parallel encoding results in a significant performance drop. To enable\neffective and efficient CAG, we propose Adaptive Parallel Encoding\n($\\textbf{APE}$), which brings shared prefix, attention temperature, and\nscaling factor to align the distribution of parallel encoding with sequential\nencoding. Results on RAG and ICL tasks demonstrate that APE can preserve 98%\nand 93% sequential encoding performance using the same inputs while\noutperforming parallel encoding by 3.6% and 7.9%, respectively. It also scales\nto many-shot CAG, effectively encoding hundreds of contexts in parallel.\nEfficiency evaluation shows that APE can achieve an end-to-end 4.5$\\times$\nspeedup by reducing 28$\\times$ prefilling time for a 128K-length context.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context-augmented generation (CAG) techniques, including RAG and ICL, require\nthe efficient combination of multiple contexts to generate responses to user\nqueries. Directly inputting these contexts as a sequence introduces a\nconsiderable computational burden by re-encoding the combined selection of\ncontexts for every request. To address this, we explore the promising potential\nof parallel encoding to independently pre-compute and cache each context's KV\nstates. This approach enables the direct loading of cached states during\ninference while accommodating more contexts through position reuse across\ncontexts. However, due to misalignments in attention distribution, directly\napplying parallel encoding results in a significant performance drop. To enable\neffective and efficient CAG, we propose Adaptive Parallel Encoding\n($\\textbf{APE}$), which brings shared prefix, attention temperature, and\nscaling factor to align the distribution of parallel encoding with sequential\nencoding. Results on RAG and ICL tasks demonstrate that APE can preserve 98%\nand 93% sequential encoding performance using the same inputs while\noutperforming parallel encoding by 3.6% and 7.9%, respectively. It also scales\nto many-shot CAG, effectively encoding hundreds of contexts in parallel.\nEfficiency evaluation shows that APE can achieve an end-to-end 4.5$\\times$\nspeedup by reducing 28$\\times$ prefilling time for a 128K-length context."
                },
                "authors": [
                    {
                        "name": "Xinyu Yang"
                    },
                    {
                        "name": "Tianqi Chen"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "arxiv_comment": "ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05431v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05431v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08363v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08363v1",
                "updated": "2025-02-12T12:50:15Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    12,
                    50,
                    15,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-12T12:50:15Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    12,
                    50,
                    15,
                    2,
                    43,
                    0
                ],
                "title": "Top-Theta Attention: Sparsifying Transformers by Compensated\n  Thresholding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Top-Theta Attention: Sparsifying Transformers by Compensated\n  Thresholding"
                },
                "summary": "The attention mechanism is essential for the impressive capabilities of\ntransformer-based Large Language Models (LLMs). However, calculating attention\nis computationally intensive due to its quadratic dependency on the sequence\nlength. We introduce a novel approach called Top-Theta Attention, or simply\nTop-$\\theta$, which selectively prunes less essential attention elements by\ncomparing them against carefully calibrated thresholds. This method greatly\nimproves the efficiency of self-attention matrix multiplication while\npreserving model accuracy, reducing the number of required V cache rows by 3x\nduring generative decoding and the number of attention elements by 10x during\nthe prefill phase. Our method does not require model retraining; instead, it\nrequires only a brief calibration phase to be resilient to distribution shifts,\nthus not requiring the thresholds for different datasets to be recalibrated.\nUnlike top-k attention, Top-$\\theta$ eliminates full-vector dependency, making\nit suitable for tiling and scale-out and avoiding costly top-k search. A key\ninnovation of our approach is the development of efficient numerical\ncompensation techniques, which help preserve model accuracy even under\naggressive pruning of attention scores.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The attention mechanism is essential for the impressive capabilities of\ntransformer-based Large Language Models (LLMs). However, calculating attention\nis computationally intensive due to its quadratic dependency on the sequence\nlength. We introduce a novel approach called Top-Theta Attention, or simply\nTop-$\\theta$, which selectively prunes less essential attention elements by\ncomparing them against carefully calibrated thresholds. This method greatly\nimproves the efficiency of self-attention matrix multiplication while\npreserving model accuracy, reducing the number of required V cache rows by 3x\nduring generative decoding and the number of attention elements by 10x during\nthe prefill phase. Our method does not require model retraining; instead, it\nrequires only a brief calibration phase to be resilient to distribution shifts,\nthus not requiring the thresholds for different datasets to be recalibrated.\nUnlike top-k attention, Top-$\\theta$ eliminates full-vector dependency, making\nit suitable for tiling and scale-out and avoiding costly top-k search. A key\ninnovation of our approach is the development of efficient numerical\ncompensation techniques, which help preserve model accuracy even under\naggressive pruning of attention scores."
                },
                "authors": [
                    {
                        "name": "Konstantin Berestizshevsky"
                    },
                    {
                        "name": "Renzo Andri"
                    },
                    {
                        "name": "Lukas Cavigelli"
                    }
                ],
                "author_detail": {
                    "name": "Lukas Cavigelli"
                },
                "author": "Lukas Cavigelli",
                "arxiv_comment": "8 pages, 11 figures, work under submission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08363v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08363v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T01",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16909v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16909v2",
                "updated": "2025-02-12T11:05:05Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    11,
                    5,
                    5,
                    2,
                    43,
                    0
                ],
                "published": "2025-01-28T12:57:53Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    12,
                    57,
                    53,
                    1,
                    28,
                    0
                ],
                "title": "Measuring GPU utilization one level deeper",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measuring GPU utilization one level deeper"
                },
                "summary": "GPU hardware is vastly underutilized. Even resource-intensive AI applications\nhave diverse resource profiles that often leave parts of GPUs idle. While\ncolocating applications can improve utilization, current spatial sharing\nsystems lack performance guarantees. Providing predictable performance\nguarantees requires a deep understanding of how applications contend for shared\nGPU resources such as block schedulers, compute units, L1/L2 caches, and memory\nbandwidth. We propose a methodology to profile resource interference of GPU\nkernels across these dimensions and discuss how to build GPU schedulers that\nprovide strict performance guarantees while colocating applications to minimize\ncost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPU hardware is vastly underutilized. Even resource-intensive AI applications\nhave diverse resource profiles that often leave parts of GPUs idle. While\ncolocating applications can improve utilization, current spatial sharing\nsystems lack performance guarantees. Providing predictable performance\nguarantees requires a deep understanding of how applications contend for shared\nGPU resources such as block schedulers, compute units, L1/L2 caches, and memory\nbandwidth. We propose a methodology to profile resource interference of GPU\nkernels across these dimensions and discuss how to build GPU schedulers that\nprovide strict performance guarantees while colocating applications to minimize\ncost."
                },
                "authors": [
                    {
                        "name": "Paul Elvinger"
                    },
                    {
                        "name": "Foteini Strati"
                    },
                    {
                        "name": "Natalie Enright Jerger"
                    },
                    {
                        "name": "Ana Klimovic"
                    }
                ],
                "author_detail": {
                    "name": "Ana Klimovic"
                },
                "author": "Ana Klimovic",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16909v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16909v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20002v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20002v3",
                "updated": "2025-02-12T07:02:06Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    7,
                    2,
                    6,
                    2,
                    43,
                    0
                ],
                "published": "2024-09-30T06:55:00Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    6,
                    55,
                    0,
                    0,
                    274,
                    0
                ],
                "title": "The Early Bird Catches the Leak: Unveiling Timing Side Channels in LLM\n  Serving Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Early Bird Catches the Leak: Unveiling Timing Side Channels in LLM\n  Serving Systems"
                },
                "summary": "The wide deployment of Large Language Models (LLMs) has given rise to strong\ndemands for optimizing their inference performance. Today's techniques serving\nthis purpose primarily focus on reducing latency and improving throughput\nthrough algorithmic and hardware enhancements, while largely overlooking their\nprivacy side effects, particularly in a multi-user environment. In our\nresearch, for the first time, we discovered a set of new timing side channels\nin LLM systems, arising from shared caches and GPU memory allocations, which\ncan be exploited to infer both confidential system prompts and those issued by\nother users. These vulnerabilities echo security challenges observed in\ntraditional computing systems, highlighting an urgent need to address potential\ninformation leakage in LLM serving infrastructures. In this paper, we report\nnovel attack strategies designed to exploit such timing side channels inherent\nin LLM deployments, specifically targeting the Key-Value (KV) cache and\nsemantic cache widely used to enhance LLM inference performance. Our approach\nleverages timing measurements and classification models to detect cache hits,\nallowing an adversary to infer private prompts with high accuracy. We also\npropose a token-by-token search algorithm to efficiently recover shared prompt\nprefixes in the caches, showing the feasibility of stealing system prompts and\nthose produced by peer users. Our experimental studies on black-box testing of\npopular online LLM services demonstrate that such privacy risks are completely\nrealistic, with significant consequences. Our findings underscore the need for\nrobust mitigation to protect LLM systems against such emerging threats.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The wide deployment of Large Language Models (LLMs) has given rise to strong\ndemands for optimizing their inference performance. Today's techniques serving\nthis purpose primarily focus on reducing latency and improving throughput\nthrough algorithmic and hardware enhancements, while largely overlooking their\nprivacy side effects, particularly in a multi-user environment. In our\nresearch, for the first time, we discovered a set of new timing side channels\nin LLM systems, arising from shared caches and GPU memory allocations, which\ncan be exploited to infer both confidential system prompts and those issued by\nother users. These vulnerabilities echo security challenges observed in\ntraditional computing systems, highlighting an urgent need to address potential\ninformation leakage in LLM serving infrastructures. In this paper, we report\nnovel attack strategies designed to exploit such timing side channels inherent\nin LLM deployments, specifically targeting the Key-Value (KV) cache and\nsemantic cache widely used to enhance LLM inference performance. Our approach\nleverages timing measurements and classification models to detect cache hits,\nallowing an adversary to infer private prompts with high accuracy. We also\npropose a token-by-token search algorithm to efficiently recover shared prompt\nprefixes in the caches, showing the feasibility of stealing system prompts and\nthose produced by peer users. Our experimental studies on black-box testing of\npopular online LLM services demonstrate that such privacy risks are completely\nrealistic, with significant consequences. Our findings underscore the need for\nrobust mitigation to protect LLM systems against such emerging threats."
                },
                "authors": [
                    {
                        "name": "Linke Song"
                    },
                    {
                        "name": "Zixuan Pang"
                    },
                    {
                        "name": "Wenhao Wang"
                    },
                    {
                        "name": "Zihao Wang"
                    },
                    {
                        "name": "XiaoFeng Wang"
                    },
                    {
                        "name": "Hongbo Chen"
                    },
                    {
                        "name": "Wei Song"
                    },
                    {
                        "name": "Yier Jin"
                    },
                    {
                        "name": "Dan Meng"
                    },
                    {
                        "name": "Rui Hou"
                    }
                ],
                "author_detail": {
                    "name": "Rui Hou"
                },
                "author": "Rui Hou",
                "arxiv_comment": "This work was submitted for review on Sept. 5, 2024, and the initial\n  version was uploaded to Arxiv on Sept. 30, 2024. The latest version reflects\n  the up-to-date experimental results",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20002v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20002v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07903v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07903v1",
                "updated": "2025-02-11T19:17:35Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    19,
                    17,
                    35,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T19:17:35Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    19,
                    17,
                    35,
                    1,
                    42,
                    0
                ],
                "title": "HexGen-2: Disaggregated Generative Inference of LLMs in Heterogeneous\n  Environment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HexGen-2: Disaggregated Generative Inference of LLMs in Heterogeneous\n  Environment"
                },
                "summary": "Disaggregating the prefill and decoding phases represents an effective new\nparadigm for generative inference of large language models (LLM), which\neliminates prefill-decoding interference and optimizes resource allocation.\nHowever, it is still an open problem about how to deploy the disaggregated\ninference paradigm across a group of heterogeneous GPUs, which can be an\neconomical alternative to deployment over homogeneous high-performance GPUs.\nTowards this end, we introduce HexGen-2, a distributed system for efficient and\neconomical LLM serving on heterogeneous GPUs following the disaggregated\nparadigm. Built on top of HexGen, the core component of HexGen-2 is a\nscheduling algorithm that formalizes the allocation of disaggregated LLM\ninference computations and communications over heterogeneous GPUs and network\nconnections as a constraint optimization problem. We leverage the graph\npartitioning and max-flow algorithms to co-optimize resource allocation,\nparallel strategies for distinct inference phases, and the efficiency of\ninter-phase key-value (KV) cache communications. We conduct extensive\nexperiments to evaluate HexGen-2, i.e., on OPT (30B) and Llama-2 (70B) models\nin various real-world settings, the results reveal that HexGen-2 delivers up to\na 2.0 times and on average a 1.3 times improvement in serving throughput,\nreduces the average inference latency by 1.5 times compared with\nstate-of-the-art systems given the same price budget, and achieves comparable\ninference performance with a 30% lower price budget.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disaggregating the prefill and decoding phases represents an effective new\nparadigm for generative inference of large language models (LLM), which\neliminates prefill-decoding interference and optimizes resource allocation.\nHowever, it is still an open problem about how to deploy the disaggregated\ninference paradigm across a group of heterogeneous GPUs, which can be an\neconomical alternative to deployment over homogeneous high-performance GPUs.\nTowards this end, we introduce HexGen-2, a distributed system for efficient and\neconomical LLM serving on heterogeneous GPUs following the disaggregated\nparadigm. Built on top of HexGen, the core component of HexGen-2 is a\nscheduling algorithm that formalizes the allocation of disaggregated LLM\ninference computations and communications over heterogeneous GPUs and network\nconnections as a constraint optimization problem. We leverage the graph\npartitioning and max-flow algorithms to co-optimize resource allocation,\nparallel strategies for distinct inference phases, and the efficiency of\ninter-phase key-value (KV) cache communications. We conduct extensive\nexperiments to evaluate HexGen-2, i.e., on OPT (30B) and Llama-2 (70B) models\nin various real-world settings, the results reveal that HexGen-2 delivers up to\na 2.0 times and on average a 1.3 times improvement in serving throughput,\nreduces the average inference latency by 1.5 times compared with\nstate-of-the-art systems given the same price budget, and achieves comparable\ninference performance with a 30% lower price budget."
                },
                "authors": [
                    {
                        "name": "Youhe Jiang"
                    },
                    {
                        "name": "Ran Yan"
                    },
                    {
                        "name": "Binhang Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Binhang Yuan"
                },
                "author": "Binhang Yuan",
                "arxiv_comment": "ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07903v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07903v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07776v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07776v1",
                "updated": "2025-02-11T18:58:04Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    18,
                    58,
                    4,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T18:58:04Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    18,
                    58,
                    4,
                    1,
                    42,
                    0
                ],
                "title": "Auditing Prompt Caching in Language Model APIs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auditing Prompt Caching in Language Model APIs"
                },
                "summary": "Prompt caching in large language models (LLMs) results in data-dependent\ntiming variations: cached prompts are processed faster than non-cached prompts.\nThese timing differences introduce the risk of side-channel timing attacks. For\nexample, if the cache is shared across users, an attacker could identify cached\nprompts from fast API response times to learn information about other users'\nprompts. Because prompt caching may cause privacy leakage, transparency around\nthe caching policies of API providers is important. To this end, we develop and\nconduct statistical audits to detect prompt caching in real-world LLM API\nproviders. We detect global cache sharing across users in seven API providers,\nincluding OpenAI, resulting in potential privacy leakage about users' prompts.\nTiming variations due to prompt caching can also result in leakage of\ninformation about model architecture. Namely, we find evidence that OpenAI's\nembedding model is a decoder-only Transformer, which was previously not\npublicly known.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt caching in large language models (LLMs) results in data-dependent\ntiming variations: cached prompts are processed faster than non-cached prompts.\nThese timing differences introduce the risk of side-channel timing attacks. For\nexample, if the cache is shared across users, an attacker could identify cached\nprompts from fast API response times to learn information about other users'\nprompts. Because prompt caching may cause privacy leakage, transparency around\nthe caching policies of API providers is important. To this end, we develop and\nconduct statistical audits to detect prompt caching in real-world LLM API\nproviders. We detect global cache sharing across users in seven API providers,\nincluding OpenAI, resulting in potential privacy leakage about users' prompts.\nTiming variations due to prompt caching can also result in leakage of\ninformation about model architecture. Namely, we find evidence that OpenAI's\nembedding model is a decoder-only Transformer, which was previously not\npublicly known."
                },
                "authors": [
                    {
                        "name": "Chenchen Gu"
                    },
                    {
                        "name": "Xiang Lisa Li"
                    },
                    {
                        "name": "Rohith Kuditipudi"
                    },
                    {
                        "name": "Percy Liang"
                    },
                    {
                        "name": "Tatsunori Hashimoto"
                    }
                ],
                "author_detail": {
                    "name": "Tatsunori Hashimoto"
                },
                "author": "Tatsunori Hashimoto",
                "arxiv_comment": "20 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07776v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07776v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2406.06608v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.06608v6",
                "updated": "2025-02-26T18:59:01Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    18,
                    59,
                    1,
                    2,
                    57,
                    0
                ],
                "published": "2024-06-06T18:10:11Z",
                "published_parsed": [
                    2024,
                    6,
                    6,
                    18,
                    10,
                    11,
                    3,
                    158,
                    0
                ],
                "title": "The Prompt Report: A Systematic Survey of Prompt Engineering Techniques",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Prompt Report: A Systematic Survey of Prompt Engineering Techniques"
                },
                "summary": "Generative Artificial Intelligence (GenAI) systems are increasingly being\ndeployed across diverse industries and research domains. Developers and\nend-users interact with these systems through the use of prompting and prompt\nengineering. Although prompt engineering is a widely adopted and extensively\nresearched area, it suffers from conflicting terminology and a fragmented\nontological understanding of what constitutes an effective prompt due to its\nrelatively recent emergence. We establish a structured understanding of prompt\nengineering by assembling a taxonomy of prompting techniques and analyzing\ntheir applications. We present a detailed vocabulary of 33 vocabulary terms, a\ntaxonomy of 58 LLM prompting techniques, and 40 techniques for other\nmodalities. Additionally, we provide best practices and guidelines for prompt\nengineering, including advice for prompting state-of-the-art (SOTA) LLMs such\nas ChatGPT. We further present a meta-analysis of the entire literature on\nnatural language prefix-prompting. As a culmination of these efforts, this\npaper presents the most comprehensive survey on prompt engineering to date.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative Artificial Intelligence (GenAI) systems are increasingly being\ndeployed across diverse industries and research domains. Developers and\nend-users interact with these systems through the use of prompting and prompt\nengineering. Although prompt engineering is a widely adopted and extensively\nresearched area, it suffers from conflicting terminology and a fragmented\nontological understanding of what constitutes an effective prompt due to its\nrelatively recent emergence. We establish a structured understanding of prompt\nengineering by assembling a taxonomy of prompting techniques and analyzing\ntheir applications. We present a detailed vocabulary of 33 vocabulary terms, a\ntaxonomy of 58 LLM prompting techniques, and 40 techniques for other\nmodalities. Additionally, we provide best practices and guidelines for prompt\nengineering, including advice for prompting state-of-the-art (SOTA) LLMs such\nas ChatGPT. We further present a meta-analysis of the entire literature on\nnatural language prefix-prompting. As a culmination of these efforts, this\npaper presents the most comprehensive survey on prompt engineering to date."
                },
                "authors": [
                    {
                        "name": "Sander Schulhoff"
                    },
                    {
                        "name": "Michael Ilie"
                    },
                    {
                        "name": "Nishant Balepur"
                    },
                    {
                        "name": "Konstantine Kahadze"
                    },
                    {
                        "name": "Amanda Liu"
                    },
                    {
                        "name": "Chenglei Si"
                    },
                    {
                        "name": "Yinheng Li"
                    },
                    {
                        "name": "Aayush Gupta"
                    },
                    {
                        "name": "HyoJung Han"
                    },
                    {
                        "name": "Sevien Schulhoff"
                    },
                    {
                        "name": "Pranav Sandeep Dulepet"
                    },
                    {
                        "name": "Saurav Vidyadhara"
                    },
                    {
                        "name": "Dayeon Ki"
                    },
                    {
                        "name": "Sweta Agrawal"
                    },
                    {
                        "name": "Chau Pham"
                    },
                    {
                        "name": "Gerson Kroiz"
                    },
                    {
                        "name": "Feileen Li"
                    },
                    {
                        "name": "Hudson Tao"
                    },
                    {
                        "name": "Ashay Srivastava"
                    },
                    {
                        "name": "Hevander Da Costa"
                    },
                    {
                        "name": "Saloni Gupta"
                    },
                    {
                        "name": "Megan L. Rogers"
                    },
                    {
                        "name": "Inna Goncearenco"
                    },
                    {
                        "name": "Giuseppe Sarli"
                    },
                    {
                        "name": "Igor Galynker"
                    },
                    {
                        "name": "Denis Peskoff"
                    },
                    {
                        "name": "Marine Carpuat"
                    },
                    {
                        "name": "Jules White"
                    },
                    {
                        "name": "Shyamal Anadkat"
                    },
                    {
                        "name": "Alexander Hoyle"
                    },
                    {
                        "name": "Philip Resnik"
                    }
                ],
                "author_detail": {
                    "name": "Philip Resnik"
                },
                "author": "Philip Resnik",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.06608v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.06608v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19416v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19416v1",
                "updated": "2025-02-26T18:58:30Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    18,
                    58,
                    30,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-26T18:58:30Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    18,
                    58,
                    30,
                    2,
                    57,
                    0
                ],
                "title": "Norm Growth and Stability Challenges in Localized Sequential Knowledge\n  Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Norm Growth and Stability Challenges in Localized Sequential Knowledge\n  Editing"
                },
                "summary": "This study investigates the impact of localized updates to large language\nmodels (LLMs), specifically in the context of knowledge editing - a task aimed\nat incorporating or modifying specific facts without altering broader model\ncapabilities. We first show that across different post-training interventions\nlike continuous pre-training, full fine-tuning and LORA-based fine-tuning, the\nFrobenius norm of the updated matrices always increases. This increasing norm\nis especially detrimental for localized knowledge editing, where only a subset\nof matrices are updated in a model . We reveal a consistent phenomenon across\nvarious editing techniques, including fine-tuning, hypernetwork-based\napproaches, and locate-and-edit methods: the norm of the updated matrix\ninvariably increases with successive updates. Such growth disrupts model\nbalance, particularly when isolated matrices are updated while the rest of the\nmodel remains static, leading to potential instability and degradation of\ndownstream performance. Upon deeper investigations of the intermediate\nactivation vectors, we find that the norm of internal activations decreases and\nis accompanied by shifts in the subspaces occupied by these activations, which\nshows that these activation vectors now occupy completely different regions in\nthe representation space compared to the unedited model. With our paper, we\nhighlight the technical challenges with continuous and localized sequential\nknowledge editing and their implications for maintaining model stability and\nutility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study investigates the impact of localized updates to large language\nmodels (LLMs), specifically in the context of knowledge editing - a task aimed\nat incorporating or modifying specific facts without altering broader model\ncapabilities. We first show that across different post-training interventions\nlike continuous pre-training, full fine-tuning and LORA-based fine-tuning, the\nFrobenius norm of the updated matrices always increases. This increasing norm\nis especially detrimental for localized knowledge editing, where only a subset\nof matrices are updated in a model . We reveal a consistent phenomenon across\nvarious editing techniques, including fine-tuning, hypernetwork-based\napproaches, and locate-and-edit methods: the norm of the updated matrix\ninvariably increases with successive updates. Such growth disrupts model\nbalance, particularly when isolated matrices are updated while the rest of the\nmodel remains static, leading to potential instability and degradation of\ndownstream performance. Upon deeper investigations of the intermediate\nactivation vectors, we find that the norm of internal activations decreases and\nis accompanied by shifts in the subspaces occupied by these activations, which\nshows that these activation vectors now occupy completely different regions in\nthe representation space compared to the unedited model. With our paper, we\nhighlight the technical challenges with continuous and localized sequential\nknowledge editing and their implications for maintaining model stability and\nutility."
                },
                "authors": [
                    {
                        "name": "Akshat Gupta"
                    },
                    {
                        "name": "Christine Fang"
                    },
                    {
                        "name": "Atahan Ozdemir"
                    },
                    {
                        "name": "Maochuan Lu"
                    },
                    {
                        "name": "Ahmed Alaa"
                    },
                    {
                        "name": "Thomas Hartvigsen"
                    },
                    {
                        "name": "Gopala Anumanchipalli"
                    }
                ],
                "author_detail": {
                    "name": "Gopala Anumanchipalli"
                },
                "author": "Gopala Anumanchipalli",
                "arxiv_comment": "Accepted for Oral Presentation at KnowFM @ AAAI 2025. arXiv admin\n  note: text overlap with arXiv:2502.01636",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19416v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19416v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19413v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19413v1",
                "updated": "2025-02-26T18:56:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    18,
                    56,
                    52,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-26T18:56:52Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    18,
                    56,
                    52,
                    2,
                    57,
                    0
                ],
                "title": "Project Alexandria: Towards Freeing Scientific Knowledge from Copyright\n  Burdens via LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Project Alexandria: Towards Freeing Scientific Knowledge from Copyright\n  Burdens via LLMs"
                },
                "summary": "Paywalls, licenses and copyright rules often restrict the broad dissemination\nand reuse of scientific knowledge. We take the position that it is both legally\nand technically feasible to extract the scientific knowledge in scholarly\ntexts. Current methods, like text embeddings, fail to reliably preserve factual\ncontent, and simple paraphrasing may not be legally sound. We urge the\ncommunity to adopt a new idea: convert scholarly documents into Knowledge Units\nusing LLMs. These units use structured data capturing entities, attributes and\nrelationships without stylistic content. We provide evidence that Knowledge\nUnits: (1) form a legally defensible framework for sharing knowledge from\ncopyrighted research texts, based on legal analyses of German copyright law and\nU.S. Fair Use doctrine, and (2) preserve most (~95%) factual knowledge from\noriginal text, measured by MCQ performance on facts from the original\ncopyrighted text across four research domains. Freeing scientific knowledge\nfrom copyright promises transformative benefits for scientific research and\neducation by allowing language models to reuse important facts from copyrighted\ntext. To support this, we share open-source tools for converting research\ndocuments into Knowledge Units. Overall, our work posits the feasibility of\ndemocratizing access to scientific knowledge while respecting copyright.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Paywalls, licenses and copyright rules often restrict the broad dissemination\nand reuse of scientific knowledge. We take the position that it is both legally\nand technically feasible to extract the scientific knowledge in scholarly\ntexts. Current methods, like text embeddings, fail to reliably preserve factual\ncontent, and simple paraphrasing may not be legally sound. We urge the\ncommunity to adopt a new idea: convert scholarly documents into Knowledge Units\nusing LLMs. These units use structured data capturing entities, attributes and\nrelationships without stylistic content. We provide evidence that Knowledge\nUnits: (1) form a legally defensible framework for sharing knowledge from\ncopyrighted research texts, based on legal analyses of German copyright law and\nU.S. Fair Use doctrine, and (2) preserve most (~95%) factual knowledge from\noriginal text, measured by MCQ performance on facts from the original\ncopyrighted text across four research domains. Freeing scientific knowledge\nfrom copyright promises transformative benefits for scientific research and\neducation by allowing language models to reuse important facts from copyrighted\ntext. To support this, we share open-source tools for converting research\ndocuments into Knowledge Units. Overall, our work posits the feasibility of\ndemocratizing access to scientific knowledge while respecting copyright."
                },
                "authors": [
                    {
                        "name": "Christoph Schuhmann"
                    },
                    {
                        "name": "Gollam Rabby"
                    },
                    {
                        "name": "Ameya Prabhu"
                    },
                    {
                        "name": "Tawsif Ahmed"
                    },
                    {
                        "name": "Andreas Hochlehnert"
                    },
                    {
                        "name": "Huu Nguyen"
                    },
                    {
                        "name": "Nick Akinci Heidrich"
                    },
                    {
                        "name": "Ludwig Schmidt"
                    },
                    {
                        "name": "Robert Kaczmarczyk"
                    },
                    {
                        "name": "Sören Auer"
                    },
                    {
                        "name": "Jenia Jitsev"
                    },
                    {
                        "name": "Matthias Bethge"
                    }
                ],
                "author_detail": {
                    "name": "Matthias Bethge"
                },
                "author": "Matthias Bethge",
                "arxiv_comment": "Technical Report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19413v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19413v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04697v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04697v2",
                "updated": "2025-02-26T18:55:54Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    18,
                    55,
                    54,
                    2,
                    57,
                    0
                ],
                "published": "2024-12-06T01:20:16Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    1,
                    20,
                    16,
                    4,
                    341,
                    0
                ],
                "title": "Privacy-Preserving Retrieval-Augmented Generation with Differential\n  Privacy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Privacy-Preserving Retrieval-Augmented Generation with Differential\n  Privacy"
                },
                "summary": "With the recent remarkable advancement of large language models (LLMs), there\nhas been a growing interest in utilizing them in the domains with highly\nsensitive data that lies outside their training data. For this purpose,\nretrieval-augmented generation (RAG) is particularly effective -- it assists\nLLMs by directly providing relevant information from the external knowledge\nsources. However, without extra privacy safeguards, RAG outputs risk leaking\nsensitive information from the external data source. In this work, we explore\nRAG under differential privacy (DP), a formal guarantee of data privacy. The\nmain challenge with differentially private RAG is how to generate long accurate\nanswers within a moderate privacy budget. We address this by proposing an\nalgorithm that smartly spends privacy budget only for the tokens that require\nthe sensitive information and uses the non-private LLM for other tokens. Our\nextensive empirical evaluations reveal that our algorithm outperforms the\nnon-RAG baseline under a reasonable privacy budget of $\\epsilon\\approx 10$\nacross different models and datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the recent remarkable advancement of large language models (LLMs), there\nhas been a growing interest in utilizing them in the domains with highly\nsensitive data that lies outside their training data. For this purpose,\nretrieval-augmented generation (RAG) is particularly effective -- it assists\nLLMs by directly providing relevant information from the external knowledge\nsources. However, without extra privacy safeguards, RAG outputs risk leaking\nsensitive information from the external data source. In this work, we explore\nRAG under differential privacy (DP), a formal guarantee of data privacy. The\nmain challenge with differentially private RAG is how to generate long accurate\nanswers within a moderate privacy budget. We address this by proposing an\nalgorithm that smartly spends privacy budget only for the tokens that require\nthe sensitive information and uses the non-private LLM for other tokens. Our\nextensive empirical evaluations reveal that our algorithm outperforms the\nnon-RAG baseline under a reasonable privacy budget of $\\epsilon\\approx 10$\nacross different models and datasets."
                },
                "authors": [
                    {
                        "name": "Tatsuki Koga"
                    },
                    {
                        "name": "Ruihan Wu"
                    },
                    {
                        "name": "Kamalika Chaudhuri"
                    }
                ],
                "author_detail": {
                    "name": "Kamalika Chaudhuri"
                },
                "author": "Kamalika Chaudhuri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04697v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04697v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19411v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19411v1",
                "updated": "2025-02-26T18:55:42Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    18,
                    55,
                    42,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-26T18:55:42Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    18,
                    55,
                    42,
                    2,
                    57,
                    0
                ],
                "title": "Code to Think, Think to Code: A Survey on Code-Enhanced Reasoning and\n  Reasoning-Driven Code Intelligence in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code to Think, Think to Code: A Survey on Code-Enhanced Reasoning and\n  Reasoning-Driven Code Intelligence in LLMs"
                },
                "summary": "In large language models (LLMs), code and reasoning reinforce each other:\ncode offers an abstract, modular, and logic-driven structure that supports\nreasoning, while reasoning translates high-level goals into smaller, executable\nsteps that drive more advanced code intelligence. In this study, we examine how\ncode serves as a structured medium for enhancing reasoning: it provides\nverifiable execution paths, enforces logical decomposition, and enables runtime\nvalidation. We also explore how improvements in reasoning have transformed code\nintelligence from basic completion to advanced capabilities, enabling models to\naddress complex software engineering tasks through planning and debugging.\nFinally, we identify key challenges and propose future research directions to\nstrengthen this synergy, ultimately improving LLM's performance in both areas.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In large language models (LLMs), code and reasoning reinforce each other:\ncode offers an abstract, modular, and logic-driven structure that supports\nreasoning, while reasoning translates high-level goals into smaller, executable\nsteps that drive more advanced code intelligence. In this study, we examine how\ncode serves as a structured medium for enhancing reasoning: it provides\nverifiable execution paths, enforces logical decomposition, and enables runtime\nvalidation. We also explore how improvements in reasoning have transformed code\nintelligence from basic completion to advanced capabilities, enabling models to\naddress complex software engineering tasks through planning and debugging.\nFinally, we identify key challenges and propose future research directions to\nstrengthen this synergy, ultimately improving LLM's performance in both areas."
                },
                "authors": [
                    {
                        "name": "Dayu Yang"
                    },
                    {
                        "name": "Tianyang Liu"
                    },
                    {
                        "name": "Daoan Zhang"
                    },
                    {
                        "name": "Antoine Simoulin"
                    },
                    {
                        "name": "Xiaoyi Liu"
                    },
                    {
                        "name": "Yuwei Cao"
                    },
                    {
                        "name": "Zhaopu Teng"
                    },
                    {
                        "name": "Xin Qian"
                    },
                    {
                        "name": "Grey Yang"
                    },
                    {
                        "name": "Jiebo Luo"
                    },
                    {
                        "name": "Julian McAuley"
                    }
                ],
                "author_detail": {
                    "name": "Julian McAuley"
                },
                "author": "Julian McAuley",
                "arxiv_comment": "Project Repo: https://github.com/dayuyang1999/Awesome-Code-Reasoning",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19411v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19411v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19410v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19410v1",
                "updated": "2025-02-26T18:55:26Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    18,
                    55,
                    26,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-26T18:55:26Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    18,
                    55,
                    26,
                    2,
                    57,
                    0
                ],
                "title": "Less or More: Towards Glanceable Explanations for LLM Recommendations\n  Using Ultra-Small Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Less or More: Towards Glanceable Explanations for LLM Recommendations\n  Using Ultra-Small Devices"
                },
                "summary": "Large Language Models (LLMs) have shown remarkable potential in recommending\neveryday actions as personal AI assistants, while Explainable AI (XAI)\ntechniques are being increasingly utilized to help users understand why a\nrecommendation is given. Personal AI assistants today are often located on\nultra-small devices such as smartwatches, which have limited screen space. The\nverbosity of LLM-generated explanations, however, makes it challenging to\ndeliver glanceable LLM explanations on such ultra-small devices. To address\nthis, we explored 1) spatially structuring an LLM's explanation text using\ndefined contextual components during prompting and 2) presenting temporally\nadaptive explanations to users based on confidence levels. We conducted a user\nstudy to understand how these approaches impacted user experiences when\ninteracting with LLM recommendations and explanations on ultra-small devices.\nThe results showed that structured explanations reduced users' time to action\nand cognitive load when reading an explanation. Always-on structured\nexplanations increased users' acceptance of AI recommendations. However, users\nwere less satisfied with structured explanations compared to unstructured ones\ndue to their lack of sufficient, readable details. Additionally, adaptively\npresenting structured explanations was less effective at improving user\nperceptions of the AI compared to the always-on structured explanations.\nTogether with users' interview feedback, the results led to design implications\nto be mindful of when personalizing the content and timing of LLM explanations\nthat are displayed on ultra-small devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown remarkable potential in recommending\neveryday actions as personal AI assistants, while Explainable AI (XAI)\ntechniques are being increasingly utilized to help users understand why a\nrecommendation is given. Personal AI assistants today are often located on\nultra-small devices such as smartwatches, which have limited screen space. The\nverbosity of LLM-generated explanations, however, makes it challenging to\ndeliver glanceable LLM explanations on such ultra-small devices. To address\nthis, we explored 1) spatially structuring an LLM's explanation text using\ndefined contextual components during prompting and 2) presenting temporally\nadaptive explanations to users based on confidence levels. We conducted a user\nstudy to understand how these approaches impacted user experiences when\ninteracting with LLM recommendations and explanations on ultra-small devices.\nThe results showed that structured explanations reduced users' time to action\nand cognitive load when reading an explanation. Always-on structured\nexplanations increased users' acceptance of AI recommendations. However, users\nwere less satisfied with structured explanations compared to unstructured ones\ndue to their lack of sufficient, readable details. Additionally, adaptively\npresenting structured explanations was less effective at improving user\nperceptions of the AI compared to the always-on structured explanations.\nTogether with users' interview feedback, the results led to design implications\nto be mindful of when personalizing the content and timing of LLM explanations\nthat are displayed on ultra-small devices."
                },
                "authors": [
                    {
                        "name": "Xinru Wang"
                    },
                    {
                        "name": "Mengjie Yu"
                    },
                    {
                        "name": "Hannah Nguyen"
                    },
                    {
                        "name": "Michael Iuzzolino"
                    },
                    {
                        "name": "Tianyi Wang"
                    },
                    {
                        "name": "Peiqi Tang"
                    },
                    {
                        "name": "Natasha Lynova"
                    },
                    {
                        "name": "Co Tran"
                    },
                    {
                        "name": "Ting Zhang"
                    },
                    {
                        "name": "Naveen Sendhilnathan"
                    },
                    {
                        "name": "Hrvoje Benko"
                    },
                    {
                        "name": "Haijun Xia"
                    },
                    {
                        "name": "Tanya Jonker"
                    }
                ],
                "author_detail": {
                    "name": "Tanya Jonker"
                },
                "author": "Tanya Jonker",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19410v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19410v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19407v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19407v1",
                "updated": "2025-02-26T18:54:39Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    18,
                    54,
                    39,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-26T18:54:39Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    18,
                    54,
                    39,
                    2,
                    57,
                    0
                ],
                "title": "Learning Code-Edit Embedding to Model Student Debugging Behavior",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Code-Edit Embedding to Model Student Debugging Behavior"
                },
                "summary": "Providing effective feedback for programming assignments in computer science\neducation can be challenging: students solve problems by iteratively submitting\ncode, executing it, and using limited feedback from the compiler or the\nauto-grader to debug. Analyzing student debugging behavior in this process may\nreveal important insights into their knowledge and inform better personalized\nsupport tools. In this work, we propose an encoder-decoder-based model that\nlearns meaningful code-edit embeddings between consecutive student code\nsubmissions, to capture their debugging behavior. Our model leverages\ninformation on whether a student code submission passes each test case to\nfine-tune large language models (LLMs) to learn code editing representations.\nIt enables personalized next-step code suggestions that maintain the student's\ncoding style while improving test case correctness. Our model also enables us\nto analyze student code-editing patterns to uncover common student errors and\ndebugging behaviors, using clustering techniques. Experimental results on a\nreal-world student code submission dataset demonstrate that our model excels at\ncode reconstruction and personalized code suggestion while revealing\ninteresting patterns in student debugging behavior.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Providing effective feedback for programming assignments in computer science\neducation can be challenging: students solve problems by iteratively submitting\ncode, executing it, and using limited feedback from the compiler or the\nauto-grader to debug. Analyzing student debugging behavior in this process may\nreveal important insights into their knowledge and inform better personalized\nsupport tools. In this work, we propose an encoder-decoder-based model that\nlearns meaningful code-edit embeddings between consecutive student code\nsubmissions, to capture their debugging behavior. Our model leverages\ninformation on whether a student code submission passes each test case to\nfine-tune large language models (LLMs) to learn code editing representations.\nIt enables personalized next-step code suggestions that maintain the student's\ncoding style while improving test case correctness. Our model also enables us\nto analyze student code-editing patterns to uncover common student errors and\ndebugging behaviors, using clustering techniques. Experimental results on a\nreal-world student code submission dataset demonstrate that our model excels at\ncode reconstruction and personalized code suggestion while revealing\ninteresting patterns in student debugging behavior."
                },
                "authors": [
                    {
                        "name": "Hasnain Heickal"
                    },
                    {
                        "name": "Andrew Lan"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Lan"
                },
                "author": "Andrew Lan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19407v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19407v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19405v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19405v1",
                "updated": "2025-02-26T18:53:31Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    18,
                    53,
                    31,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-26T18:53:31Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    18,
                    53,
                    31,
                    2,
                    57,
                    0
                ],
                "title": "Verde: Verification via Refereed Delegation for Machine Learning\n  Programs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Verde: Verification via Refereed Delegation for Machine Learning\n  Programs"
                },
                "summary": "Machine learning programs, such as those performing inference, fine-tuning,\nand training of LLMs, are commonly delegated to untrusted compute providers. To\nprovide correctness guarantees for the client, we propose adapting the\ncryptographic notion of refereed delegation to the machine learning setting.\nThis approach enables a computationally limited client to delegate a program to\nmultiple untrusted compute providers, with a guarantee of obtaining the correct\nresult if at least one of them is honest. Refereed delegation of ML programs\nposes two technical hurdles: (1) an arbitration protocol to resolve disputes\nwhen compute providers disagree on the output, and (2) the ability to bitwise\nreproduce ML programs across different hardware setups, For (1), we design\nVerde, a dispute arbitration protocol that efficiently handles the large scale\nand graph-based computational model of modern ML programs. For (2), we build\nRepOps (Reproducible Operators), a library that eliminates hardware\n\"non-determinism\" by controlling the order of floating point operations\nperformed on all hardware. Our implementation shows that refereed delegation\nachieves both strong guarantees for clients and practical overheads for compute\nproviders.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning programs, such as those performing inference, fine-tuning,\nand training of LLMs, are commonly delegated to untrusted compute providers. To\nprovide correctness guarantees for the client, we propose adapting the\ncryptographic notion of refereed delegation to the machine learning setting.\nThis approach enables a computationally limited client to delegate a program to\nmultiple untrusted compute providers, with a guarantee of obtaining the correct\nresult if at least one of them is honest. Refereed delegation of ML programs\nposes two technical hurdles: (1) an arbitration protocol to resolve disputes\nwhen compute providers disagree on the output, and (2) the ability to bitwise\nreproduce ML programs across different hardware setups, For (1), we design\nVerde, a dispute arbitration protocol that efficiently handles the large scale\nand graph-based computational model of modern ML programs. For (2), we build\nRepOps (Reproducible Operators), a library that eliminates hardware\n\"non-determinism\" by controlling the order of floating point operations\nperformed on all hardware. Our implementation shows that refereed delegation\nachieves both strong guarantees for clients and practical overheads for compute\nproviders."
                },
                "authors": [
                    {
                        "name": "Arasu Arun"
                    },
                    {
                        "name": "Adam St. Arnaud"
                    },
                    {
                        "name": "Alexey Titov"
                    },
                    {
                        "name": "Brian Wilcox"
                    },
                    {
                        "name": "Viktor Kolobaric"
                    },
                    {
                        "name": "Marc Brinkmann"
                    },
                    {
                        "name": "Oguzhan Ersoy"
                    },
                    {
                        "name": "Ben Fielding"
                    },
                    {
                        "name": "Joseph Bonneau"
                    }
                ],
                "author_detail": {
                    "name": "Joseph Bonneau"
                },
                "author": "Joseph Bonneau",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19405v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19405v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19402v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19402v1",
                "updated": "2025-02-26T18:51:12Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    18,
                    51,
                    12,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-26T18:51:12Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    18,
                    51,
                    12,
                    2,
                    57,
                    0
                ],
                "title": "General Reasoning Requires Learning to Reason from the Get-go",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "General Reasoning Requires Learning to Reason from the Get-go"
                },
                "summary": "Large Language Models (LLMs) have demonstrated impressive real-world utility,\nexemplifying artificial useful intelligence (AUI). However, their ability to\nreason adaptively and robustly -- the hallmarks of artificial general\nintelligence (AGI) -- remains fragile. While LLMs seemingly succeed in\ncommonsense reasoning, programming, and mathematics, they struggle to\ngeneralize algorithmic understanding across novel contexts. Our experiments\nwith algorithmic tasks in esoteric programming languages reveal that LLM's\nreasoning overfits to the training data and is limited in its transferability.\nWe hypothesize that the core issue underlying such limited transferability is\nthe coupling of reasoning and knowledge in LLMs.\n  To transition from AUI to AGI, we propose disentangling knowledge and\nreasoning through three key directions: (1) pretaining to reason using RL from\nscratch as an alternative to the widely used next-token prediction pretraining,\n(2) using a curriculum of synthetic tasks to ease the learning of a\n\\textit{reasoning prior} for RL that can then be transferred to natural\nlanguage tasks, and (3) learning more generalizable reasoning functions using a\nsmall context window to reduce exploiting spurious correlations between tokens.\nSuch a reasoning system coupled with a trained retrieval system and a large\nexternal memory bank as a knowledge store can overcome several limitations of\nexisting architectures at learning to reason in novel scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated impressive real-world utility,\nexemplifying artificial useful intelligence (AUI). However, their ability to\nreason adaptively and robustly -- the hallmarks of artificial general\nintelligence (AGI) -- remains fragile. While LLMs seemingly succeed in\ncommonsense reasoning, programming, and mathematics, they struggle to\ngeneralize algorithmic understanding across novel contexts. Our experiments\nwith algorithmic tasks in esoteric programming languages reveal that LLM's\nreasoning overfits to the training data and is limited in its transferability.\nWe hypothesize that the core issue underlying such limited transferability is\nthe coupling of reasoning and knowledge in LLMs.\n  To transition from AUI to AGI, we propose disentangling knowledge and\nreasoning through three key directions: (1) pretaining to reason using RL from\nscratch as an alternative to the widely used next-token prediction pretraining,\n(2) using a curriculum of synthetic tasks to ease the learning of a\n\\textit{reasoning prior} for RL that can then be transferred to natural\nlanguage tasks, and (3) learning more generalizable reasoning functions using a\nsmall context window to reduce exploiting spurious correlations between tokens.\nSuch a reasoning system coupled with a trained retrieval system and a large\nexternal memory bank as a knowledge store can overcome several limitations of\nexisting architectures at learning to reason in novel scenarios."
                },
                "authors": [
                    {
                        "name": "Seungwook Han"
                    },
                    {
                        "name": "Jyothish Pari"
                    },
                    {
                        "name": "Samuel J. Gershman"
                    },
                    {
                        "name": "Pulkit Agrawal"
                    }
                ],
                "author_detail": {
                    "name": "Pulkit Agrawal"
                },
                "author": "Pulkit Agrawal",
                "arxiv_comment": "11 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19402v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19402v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19400v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19400v1",
                "updated": "2025-02-26T18:50:09Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    18,
                    50,
                    9,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-26T18:50:09Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    18,
                    50,
                    9,
                    2,
                    57,
                    0
                ],
                "title": "TheoremExplainAgent: Towards Multimodal Explanations for LLM Theorem\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TheoremExplainAgent: Towards Multimodal Explanations for LLM Theorem\n  Understanding"
                },
                "summary": "Understanding domain-specific theorems often requires more than just\ntext-based reasoning; effective communication through structured visual\nexplanations is crucial for deeper comprehension. While large language models\n(LLMs) demonstrate strong performance in text-based theorem reasoning, their\nability to generate coherent and pedagogically meaningful visual explanations\nremains an open challenge. In this work, we introduce TheoremExplainAgent, an\nagentic approach for generating long-form theorem explanation videos (over 5\nminutes) using Manim animations. To systematically evaluate multimodal theorem\nexplanations, we propose TheoremExplainBench, a benchmark covering 240 theorems\nacross multiple STEM disciplines, along with 5 automated evaluation metrics.\nOur results reveal that agentic planning is essential for generating detailed\nlong-form videos, and the o3-mini agent achieves a success rate of 93.8% and an\noverall score of 0.77. However, our quantitative and qualitative studies show\nthat most of the videos produced exhibit minor issues with visual element\nlayout. Furthermore, multimodal explanations expose deeper reasoning flaws that\ntext-based explanations fail to reveal, highlighting the importance of\nmultimodal explanations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding domain-specific theorems often requires more than just\ntext-based reasoning; effective communication through structured visual\nexplanations is crucial for deeper comprehension. While large language models\n(LLMs) demonstrate strong performance in text-based theorem reasoning, their\nability to generate coherent and pedagogically meaningful visual explanations\nremains an open challenge. In this work, we introduce TheoremExplainAgent, an\nagentic approach for generating long-form theorem explanation videos (over 5\nminutes) using Manim animations. To systematically evaluate multimodal theorem\nexplanations, we propose TheoremExplainBench, a benchmark covering 240 theorems\nacross multiple STEM disciplines, along with 5 automated evaluation metrics.\nOur results reveal that agentic planning is essential for generating detailed\nlong-form videos, and the o3-mini agent achieves a success rate of 93.8% and an\noverall score of 0.77. However, our quantitative and qualitative studies show\nthat most of the videos produced exhibit minor issues with visual element\nlayout. Furthermore, multimodal explanations expose deeper reasoning flaws that\ntext-based explanations fail to reveal, highlighting the importance of\nmultimodal explanations."
                },
                "authors": [
                    {
                        "name": "Max Ku"
                    },
                    {
                        "name": "Thomas Chong"
                    },
                    {
                        "name": "Jonathan Leung"
                    },
                    {
                        "name": "Krish Shah"
                    },
                    {
                        "name": "Alvin Yu"
                    },
                    {
                        "name": "Wenhu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Wenhu Chen"
                },
                "author": "Wenhu Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19400v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19400v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19399v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19399v1",
                "updated": "2025-02-26T18:47:53Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    18,
                    47,
                    53,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-26T18:47:53Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    18,
                    47,
                    53,
                    2,
                    57,
                    0
                ],
                "title": "DROID: Discrete-Time Simulation for Ring-Oscillator-Based Ising Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DROID: Discrete-Time Simulation for Ring-Oscillator-Based Ising Design"
                },
                "summary": "Many combinatorial problems can be mapped to Ising machines, i.e., networks\nof coupled oscillators that settle to a minimum-energy ground state, from which\nthe problem solution is inferred. This work proposes DROID, a novel\nevent-driven method for simulating the evolution of a CMOS Ising machine to its\nground state. The approach is accurate under general delay-phase relations that\ninclude the effects of the transistor nonlinearities and is computationally\nefficient. On a realistic-size all-to-all coupled ring oscillator array, DROID\nis nearly four orders of magnitude faster than a traditional HSPICE simulation\nin predicting the evolution of a coupled oscillator system and is demonstrated\nto attain a similar distribution of solutions as the hardware.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many combinatorial problems can be mapped to Ising machines, i.e., networks\nof coupled oscillators that settle to a minimum-energy ground state, from which\nthe problem solution is inferred. This work proposes DROID, a novel\nevent-driven method for simulating the evolution of a CMOS Ising machine to its\nground state. The approach is accurate under general delay-phase relations that\ninclude the effects of the transistor nonlinearities and is computationally\nefficient. On a realistic-size all-to-all coupled ring oscillator array, DROID\nis nearly four orders of magnitude faster than a traditional HSPICE simulation\nin predicting the evolution of a coupled oscillator system and is demonstrated\nto attain a similar distribution of solutions as the hardware."
                },
                "authors": [
                    {
                        "name": "Abhimanyu Kumar"
                    },
                    {
                        "name": "Ramprasath S."
                    },
                    {
                        "name": "Chris H. Kim"
                    },
                    {
                        "name": "Ulya R. Karpuzcu"
                    },
                    {
                        "name": "Sachin S. Sapatnekar"
                    }
                ],
                "author_detail": {
                    "name": "Sachin S. Sapatnekar"
                },
                "author": "Sachin S. Sapatnekar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19399v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19399v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.ET",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11414v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11414v2",
                "updated": "2025-02-26T18:47:31Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    18,
                    47,
                    31,
                    2,
                    57,
                    0
                ],
                "published": "2024-08-21T08:19:55Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    8,
                    19,
                    55,
                    2,
                    234,
                    0
                ],
                "title": "emPDF: Inferring the Milky Way mass with data-driven distribution\n  function in phase space",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "emPDF: Inferring the Milky Way mass with data-driven distribution\n  function in phase space"
                },
                "summary": "We introduce the emPDF (Empirical Distribution Function), a novel dynamical\nmodeling method that infers the gravitational potential from kinematic tracers\nwith optimal statistical efficiency under the minimal assumption of steady\nstate. emPDF determines the best-fit potential by maximizing the similarity\nbetween instantaneous kinematics and the time-averaged phase-space distribution\nfunction (DF), which is empirically constructed from observation upon the\ntheoretical foundation of oPDF (Han et al. 2016). This approach eliminates the\nneed for presumed functional forms of DFs or orbit libraries required by\nconventional DF- or orbit-based methods. emPDF stands out for its flexibility,\nefficiency, and capability in handling observational effects, making it\npreferable to the popular Jeans equation or other minimal assumption methods,\nespecially for the Milky Way (MW) outer halo where tracers often have limited\nsample size and poor data quality. We apply emPDF to infer the MW mass profile\nusing Gaia DR3 data of satellite galaxies and globular clusters, obtaining\nenclosed masses of $M(<r)=26\\pm8, 46\\pm8, 90\\pm13$, and $149\\pm40 \\times\n10^{10}M_\\odot$ at r=30, 50, 100, and 200 kpc, respectively. These are\nconsistent with the updated constraints from simulation-informed DF fitting (Li\net al. 2020). While the simulation-informed DF offers superior precision owing\nto the additional information extracted from simulations, emPDF is independent\nof such supplementary knowledge and applicable to general tracer populations.\nemPDF is currently implemented for tracers with complete 6D kinematics within\nspherical potentials, but it can potentially be extended to address more\ngeneral problems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce the emPDF (Empirical Distribution Function), a novel dynamical\nmodeling method that infers the gravitational potential from kinematic tracers\nwith optimal statistical efficiency under the minimal assumption of steady\nstate. emPDF determines the best-fit potential by maximizing the similarity\nbetween instantaneous kinematics and the time-averaged phase-space distribution\nfunction (DF), which is empirically constructed from observation upon the\ntheoretical foundation of oPDF (Han et al. 2016). This approach eliminates the\nneed for presumed functional forms of DFs or orbit libraries required by\nconventional DF- or orbit-based methods. emPDF stands out for its flexibility,\nefficiency, and capability in handling observational effects, making it\npreferable to the popular Jeans equation or other minimal assumption methods,\nespecially for the Milky Way (MW) outer halo where tracers often have limited\nsample size and poor data quality. We apply emPDF to infer the MW mass profile\nusing Gaia DR3 data of satellite galaxies and globular clusters, obtaining\nenclosed masses of $M(<r)=26\\pm8, 46\\pm8, 90\\pm13$, and $149\\pm40 \\times\n10^{10}M_\\odot$ at r=30, 50, 100, and 200 kpc, respectively. These are\nconsistent with the updated constraints from simulation-informed DF fitting (Li\net al. 2020). While the simulation-informed DF offers superior precision owing\nto the additional information extracted from simulations, emPDF is independent\nof such supplementary knowledge and applicable to general tracer populations.\nemPDF is currently implemented for tracers with complete 6D kinematics within\nspherical potentials, but it can potentially be extended to address more\ngeneral problems."
                },
                "authors": [
                    {
                        "name": "Zhaozhou Li"
                    },
                    {
                        "name": "Jiaxin Han"
                    },
                    {
                        "name": "Wenting Wang"
                    },
                    {
                        "name": "Yong-Zhong Qian"
                    },
                    {
                        "name": "Qingyang Li"
                    },
                    {
                        "name": "Yipeng Jing"
                    },
                    {
                        "name": "Ting S. Li"
                    }
                ],
                "author_detail": {
                    "name": "Ting S. Li"
                },
                "author": "Ting S. Li",
                "arxiv_comment": "19 pages, 10 figures. Accepted for publication in MNRAS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11414v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11414v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11261v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11261v2",
                "updated": "2025-02-26T18:44:29Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    18,
                    44,
                    29,
                    2,
                    57,
                    0
                ],
                "published": "2024-10-15T04:35:56Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    4,
                    35,
                    56,
                    1,
                    289,
                    0
                ],
                "title": "Beyond Linear Approximations: A Novel Pruning Approach for Attention\n  Matrix",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Linear Approximations: A Novel Pruning Approach for Attention\n  Matrix"
                },
                "summary": "Large Language Models (LLMs) have shown immense potential in enhancing\nvarious aspects of our daily lives, from conversational AI to search and AI\nassistants. However, their growing capabilities come at the cost of extremely\nlarge model sizes, making deployment on edge devices challenging due to memory\nand computational constraints. This paper introduces a novel approach to LLM\nweight pruning that directly optimizes for approximating the attention matrix,\na core component of transformer architectures. Unlike existing methods that\nfocus on linear approximations, our approach accounts for the non-linear nature\nof the Softmax attention mechanism. We provide theoretical guarantees for the\nconvergence of our Gradient Descent-based optimization method to a near-optimal\npruning mask solution. Our empirical results demonstrate the effectiveness of\nour non-linear pruning approach in maintaining model performance while\nsignificantly reducing computational costs, which is beyond the current\nstate-of-the-art methods, i.e., SparseGPT and Wanda, by a large margin. This\nwork establishes a new theoretical foundation for pruning algorithm design in\nLLMs, potentially paving the way for more efficient LLM inference on\nresource-constrained devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown immense potential in enhancing\nvarious aspects of our daily lives, from conversational AI to search and AI\nassistants. However, their growing capabilities come at the cost of extremely\nlarge model sizes, making deployment on edge devices challenging due to memory\nand computational constraints. This paper introduces a novel approach to LLM\nweight pruning that directly optimizes for approximating the attention matrix,\na core component of transformer architectures. Unlike existing methods that\nfocus on linear approximations, our approach accounts for the non-linear nature\nof the Softmax attention mechanism. We provide theoretical guarantees for the\nconvergence of our Gradient Descent-based optimization method to a near-optimal\npruning mask solution. Our empirical results demonstrate the effectiveness of\nour non-linear pruning approach in maintaining model performance while\nsignificantly reducing computational costs, which is beyond the current\nstate-of-the-art methods, i.e., SparseGPT and Wanda, by a large margin. This\nwork establishes a new theoretical foundation for pruning algorithm design in\nLLMs, potentially paving the way for more efficient LLM inference on\nresource-constrained devices."
                },
                "authors": [
                    {
                        "name": "Yingyu Liang"
                    },
                    {
                        "name": "Jiangxuan Long"
                    },
                    {
                        "name": "Zhenmei Shi"
                    },
                    {
                        "name": "Zhao Song"
                    },
                    {
                        "name": "Yufa Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Yufa Zhou"
                },
                "author": "Yufa Zhou",
                "arxiv_comment": "ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11261v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11261v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19385v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19385v1",
                "updated": "2025-02-26T18:30:49Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    18,
                    30,
                    49,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-26T18:30:49Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    18,
                    30,
                    49,
                    2,
                    57,
                    0
                ],
                "title": "HDEE: Heterogeneous Domain Expert Ensemble",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HDEE: Heterogeneous Domain Expert Ensemble"
                },
                "summary": "Training dense LLMs requires enormous amounts of data and centralized\ncompute, which introduces fundamental bottlenecks and ever-growing costs for\nlarge models. Several studies aim to reduce this dependency on centralization\nby reducing the communication overhead of training dense models. Taking this\nidea of reducing communication overhead to a natural extreme, by training\nembarrassingly parallelizable ensembles of small independent experts, has been\nshown to outperform large dense models trained in traditional centralized\nsettings. However, existing studies do not take into account underlying\ndifferences amongst data domains and treat them as monolithic, regardless of\ntheir underlying complexity, size, or distribution. In this paper, we explore\nthe effects of introducing heterogeneity to these ensembles of domain expert\nmodels. Specifically, by allowing models within the ensemble to vary in\nsize--as well as the number of training steps taken depending on the training\ndata's domain--we study the effect heterogeneity has on these ensembles when\nevaluated against domains included in, and excluded from, the training set. We\nuse the same compute budget to train heterogeneous ensembles and homogeneous\nbaselines for comparison. We show that the heterogeneous ensembles achieve the\nlowest perplexity scores in $20$ out of the $21$ data domains used in the\nevaluation. Our code is available at https://github.com/gensyn-ai/hdee.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training dense LLMs requires enormous amounts of data and centralized\ncompute, which introduces fundamental bottlenecks and ever-growing costs for\nlarge models. Several studies aim to reduce this dependency on centralization\nby reducing the communication overhead of training dense models. Taking this\nidea of reducing communication overhead to a natural extreme, by training\nembarrassingly parallelizable ensembles of small independent experts, has been\nshown to outperform large dense models trained in traditional centralized\nsettings. However, existing studies do not take into account underlying\ndifferences amongst data domains and treat them as monolithic, regardless of\ntheir underlying complexity, size, or distribution. In this paper, we explore\nthe effects of introducing heterogeneity to these ensembles of domain expert\nmodels. Specifically, by allowing models within the ensemble to vary in\nsize--as well as the number of training steps taken depending on the training\ndata's domain--we study the effect heterogeneity has on these ensembles when\nevaluated against domains included in, and excluded from, the training set. We\nuse the same compute budget to train heterogeneous ensembles and homogeneous\nbaselines for comparison. We show that the heterogeneous ensembles achieve the\nlowest perplexity scores in $20$ out of the $21$ data domains used in the\nevaluation. Our code is available at https://github.com/gensyn-ai/hdee."
                },
                "authors": [
                    {
                        "name": "Oğuzhan Ersoy"
                    },
                    {
                        "name": "Jari Kolehmainen"
                    },
                    {
                        "name": "Gabriel Passamani Andrade"
                    }
                ],
                "author_detail": {
                    "name": "Gabriel Passamani Andrade"
                },
                "author": "Gabriel Passamani Andrade",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19385v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19385v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19384v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19384v1",
                "updated": "2025-02-26T18:30:20Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    18,
                    30,
                    20,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-26T18:30:20Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    18,
                    30,
                    20,
                    2,
                    57,
                    0
                ],
                "title": "Towards a robust approach to infer causality in molecular systems\n  satisfying detailed balance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards a robust approach to infer causality in molecular systems\n  satisfying detailed balance"
                },
                "summary": "The ability to distinguish between correlation and causation of variables in\nmolecular systems remains an interesting and open area of investigation. In\nthis work, we probe causality in a molecular system using two independent\ncomputational methods that infer the causal direction through the language of\ninformation transfer. Specifically, we demonstrate that a molecular dynamics\nsimulation involving a single Tryptophan in liquid water displays asymmetric\ninformation transfer between specific collective variables, such as solute and\nsolvent coordinates. Analyzing a discrete Markov-state and Langevin dynamics on\na 2D free energy surface, we show that the same kind of asymmetries can emerge\neven in extremely simple systems, undergoing equilibrium and time-reversible\ndynamics. We use these model systems to rationalize the unidirectional\ninformation transfer in the molecular system in terms of asymmetries in the\nunderlying free energy landscape and/or relaxation dynamics of the relevant\ncoordinates. Finally, we propose a computational experiment that allows one to\ndecide if an asymmetric information transfer between two variables corresponds\nto a genuine causal link.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ability to distinguish between correlation and causation of variables in\nmolecular systems remains an interesting and open area of investigation. In\nthis work, we probe causality in a molecular system using two independent\ncomputational methods that infer the causal direction through the language of\ninformation transfer. Specifically, we demonstrate that a molecular dynamics\nsimulation involving a single Tryptophan in liquid water displays asymmetric\ninformation transfer between specific collective variables, such as solute and\nsolvent coordinates. Analyzing a discrete Markov-state and Langevin dynamics on\na 2D free energy surface, we show that the same kind of asymmetries can emerge\neven in extremely simple systems, undergoing equilibrium and time-reversible\ndynamics. We use these model systems to rationalize the unidirectional\ninformation transfer in the molecular system in terms of asymmetries in the\nunderlying free energy landscape and/or relaxation dynamics of the relevant\ncoordinates. Finally, we propose a computational experiment that allows one to\ndecide if an asymmetric information transfer between two variables corresponds\nto a genuine causal link."
                },
                "authors": [
                    {
                        "name": "Vittorio Del Tatto"
                    },
                    {
                        "name": "Debarshi Banerjee"
                    },
                    {
                        "name": "Ali Hassanali"
                    },
                    {
                        "name": "Alessandro Laio"
                    }
                ],
                "author_detail": {
                    "name": "Alessandro Laio"
                },
                "author": "Alessandro Laio",
                "arxiv_comment": "34 pages, 17 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19384v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19384v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.chem-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.chem-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.stat-mech",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.bio-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03758v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03758v3",
                "updated": "2025-02-26T18:16:15Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    18,
                    16,
                    15,
                    2,
                    57,
                    0
                ],
                "published": "2024-12-04T22:53:56Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    22,
                    53,
                    56,
                    2,
                    339,
                    0
                ],
                "title": "ARCON: Advancing Auto-Regressive Continuation for Driving Videos",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ARCON: Advancing Auto-Regressive Continuation for Driving Videos"
                },
                "summary": "Recent advancements in auto-regressive large language models (LLMs) have led\nto their application in video generation. This paper explores the use of Large\nVision Models (LVMs) for video continuation, a task essential for building\nworld models and predicting future frames. We introduce ARCON, a scheme that\nalternates between generating semantic and RGB tokens, allowing the LVM to\nexplicitly learn high-level structural video information. We find high\nconsistency in the RGB images and semantic maps generated without special\ndesign. Moreover, we employ an optical flow-based texture stitching method to\nenhance visual quality. Experiments in autonomous driving scenarios show that\nour model can consistently generate long videos.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in auto-regressive large language models (LLMs) have led\nto their application in video generation. This paper explores the use of Large\nVision Models (LVMs) for video continuation, a task essential for building\nworld models and predicting future frames. We introduce ARCON, a scheme that\nalternates between generating semantic and RGB tokens, allowing the LVM to\nexplicitly learn high-level structural video information. We find high\nconsistency in the RGB images and semantic maps generated without special\ndesign. Moreover, we employ an optical flow-based texture stitching method to\nenhance visual quality. Experiments in autonomous driving scenarios show that\nour model can consistently generate long videos."
                },
                "authors": [
                    {
                        "name": "Ruibo Ming"
                    },
                    {
                        "name": "Jingwei Wu"
                    },
                    {
                        "name": "Zhewei Huang"
                    },
                    {
                        "name": "Zhuoxuan Ju"
                    },
                    {
                        "name": "Jianming HU"
                    },
                    {
                        "name": "Lihui Peng"
                    },
                    {
                        "name": "Shuchang Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Shuchang Zhou"
                },
                "author": "Shuchang Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03758v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03758v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15823v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15823v2",
                "updated": "2025-02-26T18:13:06Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    18,
                    13,
                    6,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-20T03:48:00Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    3,
                    48,
                    0,
                    3,
                    51,
                    0
                ],
                "title": "InductionBench: LLMs Fail in the Simplest Complexity Class",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InductionBench: LLMs Fail in the Simplest Complexity Class"
                },
                "summary": "Large language models (LLMs) have shown remarkable improvements in reasoning\nand many existing benchmarks have been addressed by models such as o1 and o3\neither fully or partially. However, a majority of these benchmarks emphasize\ndeductive reasoning, including mathematical and coding tasks in which rules\nsuch as mathematical axioms or programming syntax are clearly defined, based on\nwhich LLMs can plan and apply these rules to arrive at a solution. In contrast,\ninductive reasoning, where one infers the underlying rules from observed data,\nremains less explored. Such inductive processes lie at the heart of scientific\ndiscovery, as they enable researchers to extract general principles from\nempirical observations. To assess whether LLMs possess this capacity, we\nintroduce InductionBench, a new benchmark designed to evaluate the inductive\nreasoning ability of LLMs. Our experimental findings reveal that even the most\nadvanced models available struggle to master the simplest complexity classes\nwithin the subregular hierarchy of functions, highlighting a notable deficiency\nin current LLMs' inductive reasoning capabilities. Coda and data are available\nhttps://github.com/Wenyueh/inductive_reasoning_benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown remarkable improvements in reasoning\nand many existing benchmarks have been addressed by models such as o1 and o3\neither fully or partially. However, a majority of these benchmarks emphasize\ndeductive reasoning, including mathematical and coding tasks in which rules\nsuch as mathematical axioms or programming syntax are clearly defined, based on\nwhich LLMs can plan and apply these rules to arrive at a solution. In contrast,\ninductive reasoning, where one infers the underlying rules from observed data,\nremains less explored. Such inductive processes lie at the heart of scientific\ndiscovery, as they enable researchers to extract general principles from\nempirical observations. To assess whether LLMs possess this capacity, we\nintroduce InductionBench, a new benchmark designed to evaluate the inductive\nreasoning ability of LLMs. Our experimental findings reveal that even the most\nadvanced models available struggle to master the simplest complexity classes\nwithin the subregular hierarchy of functions, highlighting a notable deficiency\nin current LLMs' inductive reasoning capabilities. Coda and data are available\nhttps://github.com/Wenyueh/inductive_reasoning_benchmark."
                },
                "authors": [
                    {
                        "name": "Wenyue Hua"
                    },
                    {
                        "name": "Tyler Wong"
                    },
                    {
                        "name": "Sun Fei"
                    },
                    {
                        "name": "Liangming Pan"
                    },
                    {
                        "name": "Adam Jardine"
                    },
                    {
                        "name": "William Yang Wang"
                    }
                ],
                "author_detail": {
                    "name": "William Yang Wang"
                },
                "author": "William Yang Wang",
                "arxiv_comment": "24 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15823v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15823v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.FL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.06015v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.06015v2",
                "updated": "2025-02-26T18:10:09Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    18,
                    10,
                    9,
                    2,
                    57,
                    0
                ],
                "published": "2023-11-10T11:59:41Z",
                "published_parsed": [
                    2023,
                    11,
                    10,
                    11,
                    59,
                    41,
                    4,
                    314,
                    0
                ],
                "title": "Unlock Reliable Skill Inference for Quadruped Adaptive Behavior by Skill\n  Graph",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlock Reliable Skill Inference for Quadruped Adaptive Behavior by Skill\n  Graph"
                },
                "summary": "Developing robotic intelligent systems that can adapt quickly to unseen wild\nsituations is one of the critical challenges in pursuing autonomous robotics.\nAlthough some impressive progress has been made in walking stability and skill\nlearning in the field of legged robots, their ability for fast adaptation is\nstill inferior to that of animals in nature. Animals are born with a massive\nset of skills needed to survive, and can quickly acquire new ones, by composing\nfundamental skills with limited experience. Inspired by this, we propose a\nnovel framework, named Robot Skill Graph (RSG) for organizing a massive set of\nfundamental skills of robots and dexterously reusing them for fast adaptation.\nBearing a structure similar to the Knowledge Graph (KG), RSG is composed of\nmassive dynamic behavioral skills instead of static knowledge in KG and enables\ndiscovering implicit relations that exist in between the learning context and\nacquired skills of robots, serving as a starting point for understanding subtle\npatterns existing in robots' skill learning. Extensive experimental results\ndemonstrate that RSG can provide reliable skill inference upon new tasks and\nenvironments, and enable quadruped robots to adapt to new scenarios and quickly\nlearn new skills.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Developing robotic intelligent systems that can adapt quickly to unseen wild\nsituations is one of the critical challenges in pursuing autonomous robotics.\nAlthough some impressive progress has been made in walking stability and skill\nlearning in the field of legged robots, their ability for fast adaptation is\nstill inferior to that of animals in nature. Animals are born with a massive\nset of skills needed to survive, and can quickly acquire new ones, by composing\nfundamental skills with limited experience. Inspired by this, we propose a\nnovel framework, named Robot Skill Graph (RSG) for organizing a massive set of\nfundamental skills of robots and dexterously reusing them for fast adaptation.\nBearing a structure similar to the Knowledge Graph (KG), RSG is composed of\nmassive dynamic behavioral skills instead of static knowledge in KG and enables\ndiscovering implicit relations that exist in between the learning context and\nacquired skills of robots, serving as a starting point for understanding subtle\npatterns existing in robots' skill learning. Extensive experimental results\ndemonstrate that RSG can provide reliable skill inference upon new tasks and\nenvironments, and enable quadruped robots to adapt to new scenarios and quickly\nlearn new skills."
                },
                "authors": [
                    {
                        "name": "Hongyin Zhang"
                    },
                    {
                        "name": "Diyuan Shi"
                    },
                    {
                        "name": "Zifeng Zhuang"
                    },
                    {
                        "name": "Han Zhao"
                    },
                    {
                        "name": "Zhenyu Wei"
                    },
                    {
                        "name": "Feng Zhao"
                    },
                    {
                        "name": "Sibo Gai"
                    },
                    {
                        "name": "Shangke Lyu"
                    },
                    {
                        "name": "Donglin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Donglin Wang"
                },
                "author": "Donglin Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.06015v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.06015v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07133v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07133v3",
                "updated": "2025-02-26T18:10:05Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    18,
                    10,
                    5,
                    2,
                    57,
                    0
                ],
                "published": "2024-11-11T17:06:48Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    17,
                    6,
                    48,
                    0,
                    316,
                    0
                ],
                "title": "Stronger Models are NOT Stronger Teachers for Instruction Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stronger Models are NOT Stronger Teachers for Instruction Tuning"
                },
                "summary": "Instruction tuning has been widely adopted to ensure large language models\n(LLMs) follow user instructions effectively. The resulting\ninstruction-following capabilities of LLMs heavily rely on the instruction\ndatasets used for tuning. Recently, synthetic instruction datasets have emerged\nas an economically viable solution to provide LLMs diverse and high-quality\ninstructions. However, existing approaches typically assume that larger or\nstronger models are stronger teachers for instruction tuning, and hence simply\nadopt these models as response generators to the synthetic instructions. In\nthis paper, we challenge this commonly-adopted assumption. Our extensive\nexperiments across five base models and twenty response generators reveal that\nlarger and stronger models are not necessarily stronger teachers of smaller\nmodels. We refer to this phenomenon as the Larger Models' Paradox. We observe\nthat existing metrics cannot precisely predict the effectiveness of response\ngenerators since they ignore the compatibility between teachers and base models\nbeing fine-tuned. We thus develop a novel metric, named as\nCompatibility-Adjusted Reward (CAR) to measure the effectiveness of response\ngenerators. Our experiments across five base models demonstrate that CAR\noutperforms almost all baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction tuning has been widely adopted to ensure large language models\n(LLMs) follow user instructions effectively. The resulting\ninstruction-following capabilities of LLMs heavily rely on the instruction\ndatasets used for tuning. Recently, synthetic instruction datasets have emerged\nas an economically viable solution to provide LLMs diverse and high-quality\ninstructions. However, existing approaches typically assume that larger or\nstronger models are stronger teachers for instruction tuning, and hence simply\nadopt these models as response generators to the synthetic instructions. In\nthis paper, we challenge this commonly-adopted assumption. Our extensive\nexperiments across five base models and twenty response generators reveal that\nlarger and stronger models are not necessarily stronger teachers of smaller\nmodels. We refer to this phenomenon as the Larger Models' Paradox. We observe\nthat existing metrics cannot precisely predict the effectiveness of response\ngenerators since they ignore the compatibility between teachers and base models\nbeing fine-tuned. We thus develop a novel metric, named as\nCompatibility-Adjusted Reward (CAR) to measure the effectiveness of response\ngenerators. Our experiments across five base models demonstrate that CAR\noutperforms almost all baselines."
                },
                "authors": [
                    {
                        "name": "Zhangchen Xu"
                    },
                    {
                        "name": "Fengqing Jiang"
                    },
                    {
                        "name": "Luyao Niu"
                    },
                    {
                        "name": "Bill Yuchen Lin"
                    },
                    {
                        "name": "Radha Poovendran"
                    }
                ],
                "author_detail": {
                    "name": "Radha Poovendran"
                },
                "author": "Radha Poovendran",
                "arxiv_comment": "This is paper is accepted at NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07133v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07133v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01619v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01619v2",
                "updated": "2025-02-26T18:03:54Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    18,
                    3,
                    54,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-03T18:51:43Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    18,
                    51,
                    43,
                    0,
                    34,
                    0
                ],
                "title": "Learning to Generate Unit Tests for Automated Debugging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to Generate Unit Tests for Automated Debugging"
                },
                "summary": "Unit tests (UTs) play an instrumental role in assessing code correctness as\nwell as providing feedback to large language models (LLMs), motivating\nautomated test generation. However, we uncover a trade-off between generating\nunit test inputs that reveal errors when given a faulty code and correctly\npredicting the unit test output without access to the gold solution. To address\nthis trade-off, we propose UTGen, which teaches LLMs to generate unit test\ninputs that reveal errors along with their correct expected outputs based on\ntask descriptions. Since model-generated tests can provide noisy signals (e.g.,\nfrom incorrectly predicted outputs), we propose UTDebug that (i) scales UTGen\nvia test-time compute to improve UT output prediction, and (ii) validates and\nbacktracks edits based on multiple generated UTs to avoid overfitting, and\nhelps LLMs debug effectively. We show that UTGen outperforms other LLM-based\nbaselines by 7.59% based on a metric measuring the presence of both\nerror-revealing UT inputs and correct UT outputs. When used with UTDebug, we\nfind that feedback from UTGen's unit tests improves pass@1 accuracy of Qwen2.5\n32B on HumanEvalFix and our own harder debugging split of MBPP+ by over 3.17%\nand 12.35% (respectively) over other LLM-based UT generation baselines. Lastly,\nwe demonstrate that UTGen is a better judge for code correctness, outperforming\na state-of-the-art trained 8B reward model by 4.43% on HumanEval+ with\nbest-of-10 sampling using Qwen2.5 7B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unit tests (UTs) play an instrumental role in assessing code correctness as\nwell as providing feedback to large language models (LLMs), motivating\nautomated test generation. However, we uncover a trade-off between generating\nunit test inputs that reveal errors when given a faulty code and correctly\npredicting the unit test output without access to the gold solution. To address\nthis trade-off, we propose UTGen, which teaches LLMs to generate unit test\ninputs that reveal errors along with their correct expected outputs based on\ntask descriptions. Since model-generated tests can provide noisy signals (e.g.,\nfrom incorrectly predicted outputs), we propose UTDebug that (i) scales UTGen\nvia test-time compute to improve UT output prediction, and (ii) validates and\nbacktracks edits based on multiple generated UTs to avoid overfitting, and\nhelps LLMs debug effectively. We show that UTGen outperforms other LLM-based\nbaselines by 7.59% based on a metric measuring the presence of both\nerror-revealing UT inputs and correct UT outputs. When used with UTDebug, we\nfind that feedback from UTGen's unit tests improves pass@1 accuracy of Qwen2.5\n32B on HumanEvalFix and our own harder debugging split of MBPP+ by over 3.17%\nand 12.35% (respectively) over other LLM-based UT generation baselines. Lastly,\nwe demonstrate that UTGen is a better judge for code correctness, outperforming\na state-of-the-art trained 8B reward model by 4.43% on HumanEval+ with\nbest-of-10 sampling using Qwen2.5 7B."
                },
                "authors": [
                    {
                        "name": "Archiki Prasad"
                    },
                    {
                        "name": "Elias Stengel-Eskin"
                    },
                    {
                        "name": "Justin Chih-Yao Chen"
                    },
                    {
                        "name": "Zaid Khan"
                    },
                    {
                        "name": "Mohit Bansal"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Bansal"
                },
                "author": "Mohit Bansal",
                "arxiv_comment": "First two authors contributed equally. Dataset and Code:\n  https://github.com/archiki/UTGenDebug",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01619v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01619v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19363v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19363v1",
                "updated": "2025-02-26T18:01:19Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    18,
                    1,
                    19,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-26T18:01:19Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    18,
                    1,
                    19,
                    2,
                    57,
                    0
                ],
                "title": "DataMan: Data Manager for Pre-training Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DataMan: Data Manager for Pre-training Large Language Models"
                },
                "summary": "The performance emergence of large language models (LLMs) driven by data\nscaling laws makes the selection of pre-training data increasingly important.\nHowever, existing methods rely on limited heuristics and human intuition,\nlacking comprehensive and clear guidelines. To address this, we are inspired by\n``reverse thinking'' -- prompting LLMs to self-identify which criteria benefit\nits performance. As its pre-training capabilities are related to perplexity\n(PPL), we derive 14 quality criteria from the causes of text perplexity\nanomalies and introduce 15 common application domains to support domain mixing.\nIn this paper, we train a Data Manager (DataMan) to learn quality ratings and\ndomain recognition from pointwise rating, and use it to annotate a 447B token\npre-training corpus with 14 quality ratings and domain type. Our experiments\nvalidate our approach, using DataMan to select 30B tokens to train a\n1.3B-parameter language model, demonstrating significant improvements in\nin-context learning (ICL), perplexity, and instruction-following ability over\nthe state-of-the-art baseline. The best-performing model, based on the Overall\nScore l=5 surpasses a model trained with 50% more data using uniform sampling.\nWe continue pre-training with high-rated, domain-specific data annotated by\nDataMan to enhance domain-specific ICL performance and thus verify DataMan's\ndomain mixing ability. Our findings emphasize the importance of quality\nranking, the complementary nature of quality criteria, and their low\ncorrelation with perplexity, analyzing misalignment between PPL and ICL\nperformance. We also thoroughly analyzed our pre-training dataset, examining\nits composition, the distribution of quality ratings, and the original document\nsources.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The performance emergence of large language models (LLMs) driven by data\nscaling laws makes the selection of pre-training data increasingly important.\nHowever, existing methods rely on limited heuristics and human intuition,\nlacking comprehensive and clear guidelines. To address this, we are inspired by\n``reverse thinking'' -- prompting LLMs to self-identify which criteria benefit\nits performance. As its pre-training capabilities are related to perplexity\n(PPL), we derive 14 quality criteria from the causes of text perplexity\nanomalies and introduce 15 common application domains to support domain mixing.\nIn this paper, we train a Data Manager (DataMan) to learn quality ratings and\ndomain recognition from pointwise rating, and use it to annotate a 447B token\npre-training corpus with 14 quality ratings and domain type. Our experiments\nvalidate our approach, using DataMan to select 30B tokens to train a\n1.3B-parameter language model, demonstrating significant improvements in\nin-context learning (ICL), perplexity, and instruction-following ability over\nthe state-of-the-art baseline. The best-performing model, based on the Overall\nScore l=5 surpasses a model trained with 50% more data using uniform sampling.\nWe continue pre-training with high-rated, domain-specific data annotated by\nDataMan to enhance domain-specific ICL performance and thus verify DataMan's\ndomain mixing ability. Our findings emphasize the importance of quality\nranking, the complementary nature of quality criteria, and their low\ncorrelation with perplexity, analyzing misalignment between PPL and ICL\nperformance. We also thoroughly analyzed our pre-training dataset, examining\nits composition, the distribution of quality ratings, and the original document\nsources."
                },
                "authors": [
                    {
                        "name": "Ru Peng"
                    },
                    {
                        "name": "Kexin Yang"
                    },
                    {
                        "name": "Yawen Zeng"
                    },
                    {
                        "name": "Junyang Lin"
                    },
                    {
                        "name": "Dayiheng Liu"
                    },
                    {
                        "name": "Junbo Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Junbo Zhao"
                },
                "author": "Junbo Zhao",
                "arxiv_comment": "ICLR2025 paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19363v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19363v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19361v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19361v1",
                "updated": "2025-02-26T17:59:27Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    17,
                    59,
                    27,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-26T17:59:27Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    17,
                    59,
                    27,
                    2,
                    57,
                    0
                ],
                "title": "Can Large Language Models Detect Errors in Long Chain-of-Thought\n  Reasoning?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Large Language Models Detect Errors in Long Chain-of-Thought\n  Reasoning?"
                },
                "summary": "Recently, o1-like models have drawn significant attention, where these models\nproduce the long Chain-of-Thought (CoT) reasoning steps to improve the\nreasoning abilities of existing Large Language Models (LLMs). In this paper, to\nunderstand the qualities of these long CoTs and measure the critique abilities\nof existing LLMs on these long CoTs, we introduce the DeltaBench, including the\ngenerated long CoTs from different o1-like models (e.g., QwQ, DeepSeek-R1) for\ndifferent reasoning tasks (e.g., Math, Code, General Reasoning), to measure the\nability to detect errors in long CoT reasoning. Based on DeltaBench, we first\nperform fine-grained analysis of the generated long CoTs to discover the\neffectiveness and efficiency of different o1-like models. Then, we conduct\nextensive evaluations of existing process reward models (PRMs) and critic\nmodels to detect the errors of each annotated process, which aims to\ninvestigate the boundaries and limitations of existing PRMs and critic models.\nFinally, we hope that DeltaBench could guide developers to better understand\nthe long CoT reasoning abilities of their models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, o1-like models have drawn significant attention, where these models\nproduce the long Chain-of-Thought (CoT) reasoning steps to improve the\nreasoning abilities of existing Large Language Models (LLMs). In this paper, to\nunderstand the qualities of these long CoTs and measure the critique abilities\nof existing LLMs on these long CoTs, we introduce the DeltaBench, including the\ngenerated long CoTs from different o1-like models (e.g., QwQ, DeepSeek-R1) for\ndifferent reasoning tasks (e.g., Math, Code, General Reasoning), to measure the\nability to detect errors in long CoT reasoning. Based on DeltaBench, we first\nperform fine-grained analysis of the generated long CoTs to discover the\neffectiveness and efficiency of different o1-like models. Then, we conduct\nextensive evaluations of existing process reward models (PRMs) and critic\nmodels to detect the errors of each annotated process, which aims to\ninvestigate the boundaries and limitations of existing PRMs and critic models.\nFinally, we hope that DeltaBench could guide developers to better understand\nthe long CoT reasoning abilities of their models."
                },
                "authors": [
                    {
                        "name": "Yancheng He"
                    },
                    {
                        "name": "Shilong Li"
                    },
                    {
                        "name": "Jiaheng Liu"
                    },
                    {
                        "name": "Weixun Wang"
                    },
                    {
                        "name": "Xingyuan Bu"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Zhongyuan Peng"
                    },
                    {
                        "name": "Zhaoxiang Zhang"
                    },
                    {
                        "name": "Wenbo Su"
                    },
                    {
                        "name": "Bo Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Bo Zheng"
                },
                "author": "Bo Zheng",
                "arxiv_comment": "The first three authors contributed equally, 27 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19361v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19361v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.08056v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.08056v2",
                "updated": "2025-02-26T17:56:57Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    17,
                    56,
                    57,
                    2,
                    57,
                    0
                ],
                "published": "2024-07-10T21:25:51Z",
                "published_parsed": [
                    2024,
                    7,
                    10,
                    21,
                    25,
                    51,
                    2,
                    192,
                    0
                ],
                "title": "Pareto Low-Rank Adapters: Efficient Multi-Task Learning with Preferences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pareto Low-Rank Adapters: Efficient Multi-Task Learning with Preferences"
                },
                "summary": "Multi-task trade-offs in machine learning can be addressed via Pareto Front\nLearning (PFL) methods that parameterize the Pareto Front (PF) with a single\nmodel. PFL permits to select the desired operational point during inference,\ncontrary to traditional Multi-Task Learning (MTL) that optimizes for a single\ntrade-off decided prior to training. However, recent PFL methodologies suffer\nfrom limited scalability, slow convergence, and excessive memory requirements,\nwhile exhibiting inconsistent mappings from preference to objective space. We\nintroduce PaLoRA, a novel parameter-efficient method that addresses these\nlimitations in two ways. First, we augment any neural network architecture with\ntask-specific low-rank adapters and continuously parameterize the PF in their\nconvex hull. Our approach steers the original model and the adapters towards\nlearning general and task-specific features, respectively. Second, we propose a\ndeterministic sampling schedule of preference vectors that reinforces this\ndivision of labor, enabling faster convergence and strengthening the validity\nof the mapping from preference to objective space throughout training. Our\nexperiments show that PaLoRA outperforms state-of-the-art MTL and PFL baselines\nacross various datasets, scales to large networks, reducing the memory overhead\n$23.8-31.7$ times compared with competing PFL baselines in scene understanding\nbenchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-task trade-offs in machine learning can be addressed via Pareto Front\nLearning (PFL) methods that parameterize the Pareto Front (PF) with a single\nmodel. PFL permits to select the desired operational point during inference,\ncontrary to traditional Multi-Task Learning (MTL) that optimizes for a single\ntrade-off decided prior to training. However, recent PFL methodologies suffer\nfrom limited scalability, slow convergence, and excessive memory requirements,\nwhile exhibiting inconsistent mappings from preference to objective space. We\nintroduce PaLoRA, a novel parameter-efficient method that addresses these\nlimitations in two ways. First, we augment any neural network architecture with\ntask-specific low-rank adapters and continuously parameterize the PF in their\nconvex hull. Our approach steers the original model and the adapters towards\nlearning general and task-specific features, respectively. Second, we propose a\ndeterministic sampling schedule of preference vectors that reinforces this\ndivision of labor, enabling faster convergence and strengthening the validity\nof the mapping from preference to objective space throughout training. Our\nexperiments show that PaLoRA outperforms state-of-the-art MTL and PFL baselines\nacross various datasets, scales to large networks, reducing the memory overhead\n$23.8-31.7$ times compared with competing PFL baselines in scene understanding\nbenchmarks."
                },
                "authors": [
                    {
                        "name": "Nikolaos Dimitriadis"
                    },
                    {
                        "name": "Pascal Frossard"
                    },
                    {
                        "name": "Francois Fleuret"
                    }
                ],
                "author_detail": {
                    "name": "Francois Fleuret"
                },
                "author": "Francois Fleuret",
                "arxiv_comment": "Accepted at ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.08056v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.08056v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10733v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10733v6",
                "updated": "2025-02-26T17:56:06Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    17,
                    56,
                    6,
                    2,
                    57,
                    0
                ],
                "published": "2024-10-14T17:15:07Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    15,
                    7,
                    0,
                    288,
                    0
                ],
                "title": "Deep Compression Autoencoder for Efficient High-Resolution Diffusion\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Compression Autoencoder for Efficient High-Resolution Diffusion\n  Models"
                },
                "summary": "We present Deep Compression Autoencoder (DC-AE), a new family of autoencoder\nmodels for accelerating high-resolution diffusion models. Existing autoencoder\nmodels have demonstrated impressive results at a moderate spatial compression\nratio (e.g., 8x), but fail to maintain satisfactory reconstruction accuracy for\nhigh spatial compression ratios (e.g., 64x). We address this challenge by\nintroducing two key techniques: (1) Residual Autoencoding, where we design our\nmodels to learn residuals based on the space-to-channel transformed features to\nalleviate the optimization difficulty of high spatial-compression autoencoders;\n(2) Decoupled High-Resolution Adaptation, an efficient decoupled three-phases\ntraining strategy for mitigating the generalization penalty of high\nspatial-compression autoencoders. With these designs, we improve the\nautoencoder's spatial compression ratio up to 128 while maintaining the\nreconstruction quality. Applying our DC-AE to latent diffusion models, we\nachieve significant speedup without accuracy drop. For example, on ImageNet\n512x512, our DC-AE provides 19.1x inference speedup and 17.9x training speedup\non H100 GPU for UViT-H while achieving a better FID, compared with the widely\nused SD-VAE-f8 autoencoder. Our code is available at\nhttps://github.com/mit-han-lab/efficientvit.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Deep Compression Autoencoder (DC-AE), a new family of autoencoder\nmodels for accelerating high-resolution diffusion models. Existing autoencoder\nmodels have demonstrated impressive results at a moderate spatial compression\nratio (e.g., 8x), but fail to maintain satisfactory reconstruction accuracy for\nhigh spatial compression ratios (e.g., 64x). We address this challenge by\nintroducing two key techniques: (1) Residual Autoencoding, where we design our\nmodels to learn residuals based on the space-to-channel transformed features to\nalleviate the optimization difficulty of high spatial-compression autoencoders;\n(2) Decoupled High-Resolution Adaptation, an efficient decoupled three-phases\ntraining strategy for mitigating the generalization penalty of high\nspatial-compression autoencoders. With these designs, we improve the\nautoencoder's spatial compression ratio up to 128 while maintaining the\nreconstruction quality. Applying our DC-AE to latent diffusion models, we\nachieve significant speedup without accuracy drop. For example, on ImageNet\n512x512, our DC-AE provides 19.1x inference speedup and 17.9x training speedup\non H100 GPU for UViT-H while achieving a better FID, compared with the widely\nused SD-VAE-f8 autoencoder. Our code is available at\nhttps://github.com/mit-han-lab/efficientvit."
                },
                "authors": [
                    {
                        "name": "Junyu Chen"
                    },
                    {
                        "name": "Han Cai"
                    },
                    {
                        "name": "Junsong Chen"
                    },
                    {
                        "name": "Enze Xie"
                    },
                    {
                        "name": "Shang Yang"
                    },
                    {
                        "name": "Haotian Tang"
                    },
                    {
                        "name": "Muyang Li"
                    },
                    {
                        "name": "Yao Lu"
                    },
                    {
                        "name": "Song Han"
                    }
                ],
                "author_detail": {
                    "name": "Song Han"
                },
                "author": "Song Han",
                "arxiv_comment": "ICLR 2025. First two authors contributed equally to this work",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10733v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10733v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19353v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19353v1",
                "updated": "2025-02-26T17:53:27Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    17,
                    53,
                    27,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-26T17:53:27Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    17,
                    53,
                    27,
                    2,
                    57,
                    0
                ],
                "title": "An emulation-based model for the projected correlation function",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An emulation-based model for the projected correlation function"
                },
                "summary": "Data from the ongoing \\textit{Euclid} survey will map out billions of\ngalaxies in the Universe, covering more than a third of the sky. This data will\nprovide a wealth of information about the large-scale structure (LSS) of the\nUniverse and will have a significant impact on cosmology in the coming years.\nIn this paper, we introduce an emulator-based halo model approach to forward\nmodel the relationship between cosmological parameters and the projected\ngalaxy-galaxy two-point correlation function (2PCF). Utilizing the large\n\\textsc{AbacusSummit} simulation suite, we emulate the 2PCF by generating\nmock-galaxy catalogues within the Halo Occupation Distribution (HOD) framework.\nOur emulator is designed to predict the 2PCF over scales $0.1 \\leq r /\n(h^{-1}\\text{Mpc}) \\leq 105$, from which we derive the projected correlation\nfunction, independent of redshift space distortions. We demonstrate that the\nemulator accurately predicts the projected correlation function over scales\n$0.5 \\leq r_\\perp/(h^{-1}\\text{Mpc}) \\leq 40$, given a set of cosmological and\nHOD parameters. This model is then employed in a parameter inference analysis,\nshowcasing its ability to constrain cosmological parameters. Our findings\nindicate that while the projected correlation function places weak constraints\non several cosmological parameters due to its intrinsic lack of information,\nadditional clustering statistics are necessary to better probe the underlying\ncosmology. Despite the simplified covariance matrix used in the likelihood\nmodel, the posterior distributions of several cosmological parameters remain\nbroad, underscoring the need for a more comprehensive approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data from the ongoing \\textit{Euclid} survey will map out billions of\ngalaxies in the Universe, covering more than a third of the sky. This data will\nprovide a wealth of information about the large-scale structure (LSS) of the\nUniverse and will have a significant impact on cosmology in the coming years.\nIn this paper, we introduce an emulator-based halo model approach to forward\nmodel the relationship between cosmological parameters and the projected\ngalaxy-galaxy two-point correlation function (2PCF). Utilizing the large\n\\textsc{AbacusSummit} simulation suite, we emulate the 2PCF by generating\nmock-galaxy catalogues within the Halo Occupation Distribution (HOD) framework.\nOur emulator is designed to predict the 2PCF over scales $0.1 \\leq r /\n(h^{-1}\\text{Mpc}) \\leq 105$, from which we derive the projected correlation\nfunction, independent of redshift space distortions. We demonstrate that the\nemulator accurately predicts the projected correlation function over scales\n$0.5 \\leq r_\\perp/(h^{-1}\\text{Mpc}) \\leq 40$, given a set of cosmological and\nHOD parameters. This model is then employed in a parameter inference analysis,\nshowcasing its ability to constrain cosmological parameters. Our findings\nindicate that while the projected correlation function places weak constraints\non several cosmological parameters due to its intrinsic lack of information,\nadditional clustering statistics are necessary to better probe the underlying\ncosmology. Despite the simplified covariance matrix used in the likelihood\nmodel, the posterior distributions of several cosmological parameters remain\nbroad, underscoring the need for a more comprehensive approach."
                },
                "authors": [
                    {
                        "name": "Vetle A. Vikenes"
                    },
                    {
                        "name": "Cheng-Zong Ruan"
                    },
                    {
                        "name": "David F. Mota"
                    }
                ],
                "author_detail": {
                    "name": "David F. Mota"
                },
                "author": "David F. Mota",
                "arxiv_comment": "10 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19353v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19353v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04318v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04318v2",
                "updated": "2025-02-26T17:51:31Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    17,
                    51,
                    31,
                    2,
                    57,
                    0
                ],
                "published": "2024-12-05T16:34:20Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    16,
                    34,
                    20,
                    3,
                    340,
                    0
                ],
                "title": "The Hyperfitting Phenomenon: Sharpening and Stabilizing LLMs for\n  Open-Ended Text Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Hyperfitting Phenomenon: Sharpening and Stabilizing LLMs for\n  Open-Ended Text Generation"
                },
                "summary": "This paper introduces the counter-intuitive generalization results of\noverfitting pre-trained large language models (LLMs) on very small datasets. In\nthe setting of open-ended text generation, it is well-documented that LLMs tend\nto generate repetitive and dull sequences, a phenomenon that is especially\napparent when generating using greedy decoding. This issue persists even with\nstate-of-the-art LLMs containing billions of parameters, trained via next-token\nprediction on large datasets. We find that by further fine-tuning these models\nto achieve a near-zero training loss on a small set of samples -- a process we\nrefer to as hyperfitting -- the long-sequence generative capabilities are\ngreatly enhanced. Greedy decoding with these Hyperfitted models even outperform\nTop-P sampling over long-sequences, both in terms of diversity and human\npreferences. This phenomenon extends to LLMs of various sizes, different\ndomains, and even autoregressive image generation. We further find this\nphenomena to be distinctly different from that of Grokking and double descent.\nSurprisingly, our experiments indicate that hyperfitted models rarely fall into\nrepeating sequences they were trained on, and even explicitly blocking these\nsequences results in high-quality output. All hyperfitted models produce\nextremely low-entropy predictions, often allocating nearly all probability to a\nsingle token.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces the counter-intuitive generalization results of\noverfitting pre-trained large language models (LLMs) on very small datasets. In\nthe setting of open-ended text generation, it is well-documented that LLMs tend\nto generate repetitive and dull sequences, a phenomenon that is especially\napparent when generating using greedy decoding. This issue persists even with\nstate-of-the-art LLMs containing billions of parameters, trained via next-token\nprediction on large datasets. We find that by further fine-tuning these models\nto achieve a near-zero training loss on a small set of samples -- a process we\nrefer to as hyperfitting -- the long-sequence generative capabilities are\ngreatly enhanced. Greedy decoding with these Hyperfitted models even outperform\nTop-P sampling over long-sequences, both in terms of diversity and human\npreferences. This phenomenon extends to LLMs of various sizes, different\ndomains, and even autoregressive image generation. We further find this\nphenomena to be distinctly different from that of Grokking and double descent.\nSurprisingly, our experiments indicate that hyperfitted models rarely fall into\nrepeating sequences they were trained on, and even explicitly blocking these\nsequences results in high-quality output. All hyperfitted models produce\nextremely low-entropy predictions, often allocating nearly all probability to a\nsingle token."
                },
                "authors": [
                    {
                        "name": "Fredrik Carlsson"
                    },
                    {
                        "name": "Fangyu Liu"
                    },
                    {
                        "name": "Daniel Ward"
                    },
                    {
                        "name": "Murathan Kurfali"
                    },
                    {
                        "name": "Joakim Nivre"
                    }
                ],
                "author_detail": {
                    "name": "Joakim Nivre"
                },
                "author": "Joakim Nivre",
                "arxiv_comment": "Under review at ICLR",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04318v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04318v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17170v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17170v2",
                "updated": "2025-02-26T17:40:00Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    17,
                    40,
                    0,
                    2,
                    57,
                    0
                ],
                "published": "2024-10-22T16:50:00Z",
                "published_parsed": [
                    2024,
                    10,
                    22,
                    16,
                    50,
                    0,
                    1,
                    296,
                    0
                ],
                "title": "Self-calibration for Language Model Quantization and Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-calibration for Language Model Quantization and Pruning"
                },
                "summary": "Quantization and pruning are fundamental approaches for model compression,\nenabling efficient inference for language models. In a post-training setting,\nstate-of-the-art quantization and pruning methods require calibration data, a\nsmall set of unlabeled examples. Conventionally, this is randomly sampled web\ntext, aiming to reflect the model training data. However, this poses two key\nproblems: (1) unrepresentative calibration examples can harm model performance,\nand (2) organizations increasingly avoid releasing model training data. In this\npaper, we propose self-calibration as a solution. Our approach requires no\nexternal data, instead leveraging the model itself to generate synthetic\ncalibration data, with a view to better approximating the pre-training data\ndistribution. We extensively compare the performance of self-calibration with\nseveral baselines, across a variety of models, compression methods, and tasks.\nOur approach proves consistently competitive in maximizing downstream task\nperformance, frequently outperforming even using real data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization and pruning are fundamental approaches for model compression,\nenabling efficient inference for language models. In a post-training setting,\nstate-of-the-art quantization and pruning methods require calibration data, a\nsmall set of unlabeled examples. Conventionally, this is randomly sampled web\ntext, aiming to reflect the model training data. However, this poses two key\nproblems: (1) unrepresentative calibration examples can harm model performance,\nand (2) organizations increasingly avoid releasing model training data. In this\npaper, we propose self-calibration as a solution. Our approach requires no\nexternal data, instead leveraging the model itself to generate synthetic\ncalibration data, with a view to better approximating the pre-training data\ndistribution. We extensively compare the performance of self-calibration with\nseveral baselines, across a variety of models, compression methods, and tasks.\nOur approach proves consistently competitive in maximizing downstream task\nperformance, frequently outperforming even using real data."
                },
                "authors": [
                    {
                        "name": "Miles Williams"
                    },
                    {
                        "name": "George Chrysostomou"
                    },
                    {
                        "name": "Nikolaos Aletras"
                    }
                ],
                "author_detail": {
                    "name": "Nikolaos Aletras"
                },
                "author": "Nikolaos Aletras",
                "arxiv_comment": "NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17170v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17170v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.16627v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.16627v2",
                "updated": "2025-02-26T17:39:46Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    17,
                    39,
                    46,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-23T16:04:56Z",
                "published_parsed": [
                    2025,
                    2,
                    23,
                    16,
                    4,
                    56,
                    6,
                    54,
                    0
                ],
                "title": "Energy-Efficient Transformer Inference: Optimization Strategies for Time\n  Series Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Energy-Efficient Transformer Inference: Optimization Strategies for Time\n  Series Classification"
                },
                "summary": "The increasing computational demands of transformer models in time series\nclassification necessitate effective optimization strategies for\nenergy-efficient deployment. This paper presents a systematic investigation of\noptimization techniques, focusing on structured pruning and quantization\nmethods for transformer architectures. Through extensive experimentation on\nthree distinct datasets (RefrigerationDevices, ElectricDevices, and PLAID), we\nquantitatively evaluate model performance and energy efficiency across\ndifferent transformer configurations. Our experimental results demonstrate that\nstatic quantization reduces energy consumption by 29.14% while maintaining\nclassification performance, and L1 pruning achieves a 63% improvement in\ninference speed with minimal accuracy degradation. These findings provide\nvaluable insights into the effectiveness of optimization strategies for\ntransformer-based time series classification, establishing a foundation for\nefficient model deployment in resource-constrained environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing computational demands of transformer models in time series\nclassification necessitate effective optimization strategies for\nenergy-efficient deployment. This paper presents a systematic investigation of\noptimization techniques, focusing on structured pruning and quantization\nmethods for transformer architectures. Through extensive experimentation on\nthree distinct datasets (RefrigerationDevices, ElectricDevices, and PLAID), we\nquantitatively evaluate model performance and energy efficiency across\ndifferent transformer configurations. Our experimental results demonstrate that\nstatic quantization reduces energy consumption by 29.14% while maintaining\nclassification performance, and L1 pruning achieves a 63% improvement in\ninference speed with minimal accuracy degradation. These findings provide\nvaluable insights into the effectiveness of optimization strategies for\ntransformer-based time series classification, establishing a foundation for\nefficient model deployment in resource-constrained environments."
                },
                "authors": [
                    {
                        "name": "Arshia Kermani"
                    },
                    {
                        "name": "Ehsan Zeraatkar"
                    },
                    {
                        "name": "Habib Irani"
                    }
                ],
                "author_detail": {
                    "name": "Habib Irani"
                },
                "author": "Habib Irani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.16627v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.16627v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19347v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19347v1",
                "updated": "2025-02-26T17:38:58Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    17,
                    38,
                    58,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-26T17:38:58Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    17,
                    38,
                    58,
                    2,
                    57,
                    0
                ],
                "title": "Controlled Diversity: Length-optimized Natural Language Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Controlled Diversity: Length-optimized Natural Language Generation"
                },
                "summary": "LLMs are not generally able to adjust the length of their outputs based on\nstrict length requirements, a capability that would improve their usefulness in\napplications that require adherence to diverse user and system requirements. We\npresent an approach to train LLMs to acquire this capability by augmenting\nexisting data and applying existing fine-tuning techniques, which we compare\nbased on the trained models' adherence to the length requirement and overall\nresponse quality relative to the baseline model. Our results demonstrate that\nthese techniques can be successfully applied to train LLMs to adhere to length\nrequirements, with the trained models generating texts which better align to\nthe length requirements. Our results indicate that our method may change the\nresponse quality when using training data that was not generated by the\nbaseline model. This allows simultaneous alignment to another training\nobjective in certain scenarios, but is undesirable otherwise. Training on a\ndataset containing the model's own responses eliminates this issue.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs are not generally able to adjust the length of their outputs based on\nstrict length requirements, a capability that would improve their usefulness in\napplications that require adherence to diverse user and system requirements. We\npresent an approach to train LLMs to acquire this capability by augmenting\nexisting data and applying existing fine-tuning techniques, which we compare\nbased on the trained models' adherence to the length requirement and overall\nresponse quality relative to the baseline model. Our results demonstrate that\nthese techniques can be successfully applied to train LLMs to adhere to length\nrequirements, with the trained models generating texts which better align to\nthe length requirements. Our results indicate that our method may change the\nresponse quality when using training data that was not generated by the\nbaseline model. This allows simultaneous alignment to another training\nobjective in certain scenarios, but is undesirable otherwise. Training on a\ndataset containing the model's own responses eliminates this issue."
                },
                "authors": [
                    {
                        "name": "Diana Marie Schenke"
                    },
                    {
                        "name": "Timo Baumann"
                    }
                ],
                "author_detail": {
                    "name": "Timo Baumann"
                },
                "author": "Timo Baumann",
                "arxiv_comment": "ISCA/ITG Workshop on Diversity in Large Speech and Language Models",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19347v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19347v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.15756v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.15756v4",
                "updated": "2025-02-26T17:32:10Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    17,
                    32,
                    10,
                    2,
                    57,
                    0
                ],
                "published": "2024-05-24T17:51:39Z",
                "published_parsed": [
                    2024,
                    5,
                    24,
                    17,
                    51,
                    39,
                    4,
                    145,
                    0
                ],
                "title": "Wasserstein Distances, Neuronal Entanglement, and Sparsity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wasserstein Distances, Neuronal Entanglement, and Sparsity"
                },
                "summary": "Disentangling polysemantic neurons is at the core of many current approaches\nto interpretability of large language models. Here we attempt to study how\ndisentanglement can be used to understand performance, particularly under\nweight sparsity, a leading post-training optimization technique. We suggest a\nnovel measure for estimating neuronal entanglement: the Wasserstein distance of\na neuron's output distribution to a Gaussian. Moreover, we show the existence\nof a small number of highly entangled \"Wasserstein Neurons\" in each linear\nlayer of an LLM, characterized by their highly non-Gaussian output\ndistributions, their role in mapping similar inputs to dissimilar outputs, and\ntheir significant impact on model accuracy. To study these phenomena, we\npropose a new experimental framework for disentangling polysemantic neurons.\nOur framework separates each layer's inputs to create a mixture of experts\nwhere each neuron's output is computed by a mixture of neurons of lower\nWasserstein distance, each better at maintaining accuracy when sparsified\nwithout retraining. We provide strong evidence that this is because the mixture\nof sparse experts is effectively disentangling the input-output relationship of\nindividual neurons, in particular the difficult Wasserstein neurons.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disentangling polysemantic neurons is at the core of many current approaches\nto interpretability of large language models. Here we attempt to study how\ndisentanglement can be used to understand performance, particularly under\nweight sparsity, a leading post-training optimization technique. We suggest a\nnovel measure for estimating neuronal entanglement: the Wasserstein distance of\na neuron's output distribution to a Gaussian. Moreover, we show the existence\nof a small number of highly entangled \"Wasserstein Neurons\" in each linear\nlayer of an LLM, characterized by their highly non-Gaussian output\ndistributions, their role in mapping similar inputs to dissimilar outputs, and\ntheir significant impact on model accuracy. To study these phenomena, we\npropose a new experimental framework for disentangling polysemantic neurons.\nOur framework separates each layer's inputs to create a mixture of experts\nwhere each neuron's output is computed by a mixture of neurons of lower\nWasserstein distance, each better at maintaining accuracy when sparsified\nwithout retraining. We provide strong evidence that this is because the mixture\nof sparse experts is effectively disentangling the input-output relationship of\nindividual neurons, in particular the difficult Wasserstein neurons."
                },
                "authors": [
                    {
                        "name": "Shashata Sawmya"
                    },
                    {
                        "name": "Linghao Kong"
                    },
                    {
                        "name": "Ilia Markov"
                    },
                    {
                        "name": "Dan Alistarh"
                    },
                    {
                        "name": "Nir Shavit"
                    }
                ],
                "author_detail": {
                    "name": "Nir Shavit"
                },
                "author": "Nir Shavit",
                "arxiv_comment": "10 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.15756v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.15756v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19339v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19339v1",
                "updated": "2025-02-26T17:32:07Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    17,
                    32,
                    7,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-26T17:32:07Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    17,
                    32,
                    7,
                    2,
                    57,
                    0
                ],
                "title": "Evaluating LLMs and Pre-trained Models for Text Summarization Across\n  Diverse Datasets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating LLMs and Pre-trained Models for Text Summarization Across\n  Diverse Datasets"
                },
                "summary": "Text summarization plays a crucial role in natural language processing by\ncondensing large volumes of text into concise and coherent summaries. As\ndigital content continues to grow rapidly and the demand for effective\ninformation retrieval increases, text summarization has become a focal point of\nresearch in recent years. This study offers a thorough evaluation of four\nleading pre-trained and open-source large language models: BART, FLAN-T5,\nLLaMA-3-8B, and Gemma-7B, across five diverse datasets CNN/DM, Gigaword, News\nSummary, XSum, and BBC News. The evaluation employs widely recognized automatic\nmetrics, including ROUGE-1, ROUGE-2, ROUGE-L, BERTScore, and METEOR, to assess\nthe models' capabilities in generating coherent and informative summaries. The\nresults reveal the comparative strengths and limitations of these models in\nprocessing various text types.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text summarization plays a crucial role in natural language processing by\ncondensing large volumes of text into concise and coherent summaries. As\ndigital content continues to grow rapidly and the demand for effective\ninformation retrieval increases, text summarization has become a focal point of\nresearch in recent years. This study offers a thorough evaluation of four\nleading pre-trained and open-source large language models: BART, FLAN-T5,\nLLaMA-3-8B, and Gemma-7B, across five diverse datasets CNN/DM, Gigaword, News\nSummary, XSum, and BBC News. The evaluation employs widely recognized automatic\nmetrics, including ROUGE-1, ROUGE-2, ROUGE-L, BERTScore, and METEOR, to assess\nthe models' capabilities in generating coherent and informative summaries. The\nresults reveal the comparative strengths and limitations of these models in\nprocessing various text types."
                },
                "authors": [
                    {
                        "name": "Tohida Rehman"
                    },
                    {
                        "name": "Soumabha Ghosh"
                    },
                    {
                        "name": "Kuntal Das"
                    },
                    {
                        "name": "Souvik Bhattacharjee"
                    },
                    {
                        "name": "Debarshi Kumar Sanyal"
                    },
                    {
                        "name": "Samiran Chattopadhyay"
                    }
                ],
                "author_detail": {
                    "name": "Samiran Chattopadhyay"
                },
                "author": "Samiran Chattopadhyay",
                "arxiv_comment": "5 pages, 2 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19339v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19339v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19328v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19328v1",
                "updated": "2025-02-26T17:19:12Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    17,
                    19,
                    12,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-26T17:19:12Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    17,
                    19,
                    12,
                    2,
                    57,
                    0
                ],
                "title": "Agentic Reward Modeling: Integrating Human Preferences with Verifiable\n  Correctness Signals for Reliable Reward Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentic Reward Modeling: Integrating Human Preferences with Verifiable\n  Correctness Signals for Reliable Reward Systems"
                },
                "summary": "Reward models (RMs) are crucial for the training and inference-time scaling\nup of large language models (LLMs). However, existing reward models primarily\nfocus on human preferences, neglecting verifiable correctness signals which\nhave shown strong potential in training LLMs. In this paper, we propose agentic\nreward modeling, a reward system that combines reward models with verifiable\ncorrectness signals from different aspects to provide reliable rewards. We\nempirically implement a reward agent, named RewardAgent, that combines human\npreference rewards with two verifiable signals: factuality and instruction\nfollowing, to provide more reliable rewards. We conduct comprehensive\nexperiments on existing reward model benchmarks and inference time best-of-n\nsearches on real-world downstream tasks. RewardAgent significantly outperforms\nvanilla reward models, demonstrating its effectiveness. We further construct\ntraining preference pairs using RewardAgent and train an LLM with the DPO\nobjective, achieving superior performance on various NLP benchmarks compared to\nconventional reward models. Our codes are publicly released to facilitate\nfurther research (https://github.com/THU-KEG/Agentic-Reward-Modeling).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reward models (RMs) are crucial for the training and inference-time scaling\nup of large language models (LLMs). However, existing reward models primarily\nfocus on human preferences, neglecting verifiable correctness signals which\nhave shown strong potential in training LLMs. In this paper, we propose agentic\nreward modeling, a reward system that combines reward models with verifiable\ncorrectness signals from different aspects to provide reliable rewards. We\nempirically implement a reward agent, named RewardAgent, that combines human\npreference rewards with two verifiable signals: factuality and instruction\nfollowing, to provide more reliable rewards. We conduct comprehensive\nexperiments on existing reward model benchmarks and inference time best-of-n\nsearches on real-world downstream tasks. RewardAgent significantly outperforms\nvanilla reward models, demonstrating its effectiveness. We further construct\ntraining preference pairs using RewardAgent and train an LLM with the DPO\nobjective, achieving superior performance on various NLP benchmarks compared to\nconventional reward models. Our codes are publicly released to facilitate\nfurther research (https://github.com/THU-KEG/Agentic-Reward-Modeling)."
                },
                "authors": [
                    {
                        "name": "Hao Peng"
                    },
                    {
                        "name": "Yunjia Qi"
                    },
                    {
                        "name": "Xiaozhi Wang"
                    },
                    {
                        "name": "Zijun Yao"
                    },
                    {
                        "name": "Bin Xu"
                    },
                    {
                        "name": "Lei Hou"
                    },
                    {
                        "name": "Juanzi Li"
                    }
                ],
                "author_detail": {
                    "name": "Juanzi Li"
                },
                "author": "Juanzi Li",
                "arxiv_comment": "16 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19328v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19328v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19320v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19320v1",
                "updated": "2025-02-26T17:13:19Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    17,
                    13,
                    19,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-26T17:13:19Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    17,
                    13,
                    19,
                    2,
                    57,
                    0
                ],
                "title": "Shh, don't say that! Domain Certification in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shh, don't say that! Domain Certification in LLMs"
                },
                "summary": "Large language models (LLMs) are often deployed to perform constrained tasks,\nwith narrow domains. For example, customer support bots can be built on top of\nLLMs, relying on their broad language understanding and capabilities to enhance\nperformance. However, these LLMs are adversarially susceptible, potentially\ngenerating outputs outside the intended domain. To formalize, assess, and\nmitigate this risk, we introduce domain certification; a guarantee that\naccurately characterizes the out-of-domain behavior of language models. We then\npropose a simple yet effective approach, which we call VALID that provides\nadversarial bounds as a certificate. Finally, we evaluate our method across a\ndiverse set of datasets, demonstrating that it yields meaningful certificates,\nwhich bound the probability of out-of-domain samples tightly with minimum\npenalty to refusal behavior.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are often deployed to perform constrained tasks,\nwith narrow domains. For example, customer support bots can be built on top of\nLLMs, relying on their broad language understanding and capabilities to enhance\nperformance. However, these LLMs are adversarially susceptible, potentially\ngenerating outputs outside the intended domain. To formalize, assess, and\nmitigate this risk, we introduce domain certification; a guarantee that\naccurately characterizes the out-of-domain behavior of language models. We then\npropose a simple yet effective approach, which we call VALID that provides\nadversarial bounds as a certificate. Finally, we evaluate our method across a\ndiverse set of datasets, demonstrating that it yields meaningful certificates,\nwhich bound the probability of out-of-domain samples tightly with minimum\npenalty to refusal behavior."
                },
                "authors": [
                    {
                        "name": "Cornelius Emde"
                    },
                    {
                        "name": "Alasdair Paren"
                    },
                    {
                        "name": "Preetham Arvind"
                    },
                    {
                        "name": "Maxime Kayser"
                    },
                    {
                        "name": "Tom Rainforth"
                    },
                    {
                        "name": "Thomas Lukasiewicz"
                    },
                    {
                        "name": "Bernard Ghanem"
                    },
                    {
                        "name": "Philip H. S. Torr"
                    },
                    {
                        "name": "Adel Bibi"
                    }
                ],
                "author_detail": {
                    "name": "Adel Bibi"
                },
                "author": "Adel Bibi",
                "arxiv_comment": "10 pages, includes appendix Published in International Conference on\n  Learning Representations (ICLR) 2025",
                "arxiv_journal_ref": "International Conference on Learning Representations (ICLR) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19320v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19320v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.01383v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.01383v2",
                "updated": "2025-02-26T17:09:20Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    17,
                    9,
                    20,
                    2,
                    57,
                    0
                ],
                "published": "2024-09-02T17:26:00Z",
                "published_parsed": [
                    2024,
                    9,
                    2,
                    17,
                    26,
                    0,
                    0,
                    246,
                    0
                ],
                "title": "First Measurement of Missing Energy Due to Nuclear Effects in\n  Monoenergetic Neutrino Charged Current Interactions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "First Measurement of Missing Energy Due to Nuclear Effects in\n  Monoenergetic Neutrino Charged Current Interactions"
                },
                "summary": "We present the first measurement of the missing energy due to nuclear effects\nin monoenergetic, muon neutrino charged-current interactions on carbon,\noriginating from $K^+ \\rightarrow \\mu^+ \\nu_\\mu$ decay at rest\n($E_{\\nu_\\mu}=235.5$ MeV), performed with the J-PARC Sterile Neutrino Search at\nthe J-PARC Spallation Neutron Source liquid scintillator based experiment.\nToward characterizing the neutrino interaction, ostensibly $\\nu_\\mu n\n\\rightarrow \\mu^- p$ or $\\nu_\\mu$$^{12}\\mathrm{C}$ $\\rightarrow\n\\mu^-$$^{12}\\mathrm{N}$, we define the missing energy as the energy transferred\nto the nucleus ($\\omega$) minus the kinetic energy of the outgoing proton(s),\n$E_{m} \\equiv\\omega-\\sum T_p$, and relate this to visible energy in the\ndetector, $E_{m}=E_{\\nu_\\mu} (235.5 \\mathrm{MeV})-m_\\mu (105.7 \\mathrm{MeV}) +\n[m_n-m_p (1.3 \\mathrm{MeV})] - E_{\\mathrm{vis}}$. The missing energy, which is\nnaively expected to be zero in the absence of nuclear effects (e.g. nucleon\nseparation energy, Fermi momenta, and final-state interactions), is uniquely\nsensitive to many aspects of the interaction, and has previously been\ninaccessible with neutrinos. The shape-only, differential cross section\nmeasurement reported, based on a $(77\\pm3)$% pure double-coincidence kaon\ndecay-at-rest signal (621 total events), provides detailed insight into\nneutrino-nucleus interactions, allowing even the nuclear orbital shell of the\nstruck nucleon to be inferred. The measurement provides an important benchmark\nfor models and event generators at hundreds of MeV neutrino energies,\ncharacterized by the difficult-to-model transition region between\nneutrino-nucleus and neutrino-nucleon scattering, and relevant for applications\nin nuclear physics, neutrino oscillation measurements,and Type-II supernova\nstudies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present the first measurement of the missing energy due to nuclear effects\nin monoenergetic, muon neutrino charged-current interactions on carbon,\noriginating from $K^+ \\rightarrow \\mu^+ \\nu_\\mu$ decay at rest\n($E_{\\nu_\\mu}=235.5$ MeV), performed with the J-PARC Sterile Neutrino Search at\nthe J-PARC Spallation Neutron Source liquid scintillator based experiment.\nToward characterizing the neutrino interaction, ostensibly $\\nu_\\mu n\n\\rightarrow \\mu^- p$ or $\\nu_\\mu$$^{12}\\mathrm{C}$ $\\rightarrow\n\\mu^-$$^{12}\\mathrm{N}$, we define the missing energy as the energy transferred\nto the nucleus ($\\omega$) minus the kinetic energy of the outgoing proton(s),\n$E_{m} \\equiv\\omega-\\sum T_p$, and relate this to visible energy in the\ndetector, $E_{m}=E_{\\nu_\\mu} (235.5 \\mathrm{MeV})-m_\\mu (105.7 \\mathrm{MeV}) +\n[m_n-m_p (1.3 \\mathrm{MeV})] - E_{\\mathrm{vis}}$. The missing energy, which is\nnaively expected to be zero in the absence of nuclear effects (e.g. nucleon\nseparation energy, Fermi momenta, and final-state interactions), is uniquely\nsensitive to many aspects of the interaction, and has previously been\ninaccessible with neutrinos. The shape-only, differential cross section\nmeasurement reported, based on a $(77\\pm3)$% pure double-coincidence kaon\ndecay-at-rest signal (621 total events), provides detailed insight into\nneutrino-nucleus interactions, allowing even the nuclear orbital shell of the\nstruck nucleon to be inferred. The measurement provides an important benchmark\nfor models and event generators at hundreds of MeV neutrino energies,\ncharacterized by the difficult-to-model transition region between\nneutrino-nucleus and neutrino-nucleon scattering, and relevant for applications\nin nuclear physics, neutrino oscillation measurements,and Type-II supernova\nstudies."
                },
                "authors": [
                    {
                        "name": "E. Marzec"
                    },
                    {
                        "name": "S. Ajimura"
                    },
                    {
                        "name": "A. Antonakis"
                    },
                    {
                        "name": "M. Botran"
                    },
                    {
                        "name": "M. K. Cheoun"
                    },
                    {
                        "name": "J. H. Choi"
                    },
                    {
                        "name": "J. W. Choi"
                    },
                    {
                        "name": "J. Y. Choi"
                    },
                    {
                        "name": "T. Dodo"
                    },
                    {
                        "name": "H. Furuta"
                    },
                    {
                        "name": "J. H. Goh"
                    },
                    {
                        "name": "K. Haga"
                    },
                    {
                        "name": "M. Harada"
                    },
                    {
                        "name": "S. Hasegawa"
                    },
                    {
                        "name": "Y. Hino"
                    },
                    {
                        "name": "T. Hiraiwa"
                    },
                    {
                        "name": "W. Hwang"
                    },
                    {
                        "name": "T. Iida"
                    },
                    {
                        "name": "E. Iwai"
                    },
                    {
                        "name": "S. Iwata"
                    },
                    {
                        "name": "H. I. Jang"
                    },
                    {
                        "name": "J. S. Jang"
                    },
                    {
                        "name": "M. C. Jang"
                    },
                    {
                        "name": "H. K. Jeon"
                    },
                    {
                        "name": "S. H. Jeon"
                    },
                    {
                        "name": "K. K. Joo"
                    },
                    {
                        "name": "D. E. Jung"
                    },
                    {
                        "name": "S. K. Kang"
                    },
                    {
                        "name": "Y. Kasugai"
                    },
                    {
                        "name": "T. Kawasaki"
                    },
                    {
                        "name": "E. J. Kim"
                    },
                    {
                        "name": "J. Y. Kim"
                    },
                    {
                        "name": "E. M. Kim"
                    },
                    {
                        "name": "S. Y. Kim"
                    },
                    {
                        "name": "W. Kim"
                    },
                    {
                        "name": "S. B. Kim"
                    },
                    {
                        "name": "H. Kinoshita"
                    },
                    {
                        "name": "T. Konno"
                    },
                    {
                        "name": "K. Kuwata"
                    },
                    {
                        "name": "D. H. Lee"
                    },
                    {
                        "name": "S. Lee"
                    },
                    {
                        "name": "I. T. Lim"
                    },
                    {
                        "name": "C. Little"
                    },
                    {
                        "name": "T. Maruyama"
                    },
                    {
                        "name": "S. Masuda"
                    },
                    {
                        "name": "S. Meigo"
                    },
                    {
                        "name": "S. Monjushiro"
                    },
                    {
                        "name": "D. H. Moon"
                    },
                    {
                        "name": "T. Nakano"
                    },
                    {
                        "name": "M. Niiyama"
                    },
                    {
                        "name": "K. Nishikawa"
                    },
                    {
                        "name": "M. Noumachi"
                    },
                    {
                        "name": "M. Y. Pac"
                    },
                    {
                        "name": "B. J. Park"
                    },
                    {
                        "name": "H. W. Park"
                    },
                    {
                        "name": "J. B. Park"
                    },
                    {
                        "name": "J. S. Park"
                    },
                    {
                        "name": "J. S. Park"
                    },
                    {
                        "name": "R. G. Park"
                    },
                    {
                        "name": "S. J. M. Peeters"
                    },
                    {
                        "name": "G. Roellinghoff"
                    },
                    {
                        "name": "C. Rott"
                    },
                    {
                        "name": "J. W. Ryu"
                    },
                    {
                        "name": "K. Sakai"
                    },
                    {
                        "name": "S. Sakamoto"
                    },
                    {
                        "name": "T. Shima"
                    },
                    {
                        "name": "C. D. Shin"
                    },
                    {
                        "name": "J. Spitz"
                    },
                    {
                        "name": "I. Stancu"
                    },
                    {
                        "name": "F. Suekane"
                    },
                    {
                        "name": "Y. Sugaya"
                    },
                    {
                        "name": "K. Suzuya"
                    },
                    {
                        "name": "M. Taira"
                    },
                    {
                        "name": "Y. Takeuchi"
                    },
                    {
                        "name": "W. Wang"
                    },
                    {
                        "name": "J. Waterfield"
                    },
                    {
                        "name": "W. Wei"
                    },
                    {
                        "name": "R. White"
                    },
                    {
                        "name": "Y. Yamaguchi"
                    },
                    {
                        "name": "M. Yeh"
                    },
                    {
                        "name": "I. S. Yeo"
                    },
                    {
                        "name": "C. Yoo"
                    },
                    {
                        "name": "I. Yu"
                    },
                    {
                        "name": "A. Zohaib"
                    }
                ],
                "author_detail": {
                    "name": "A. Zohaib"
                },
                "author": "A. Zohaib",
                "arxiv_doi": "10.1103/PhysRevLett.134.081801",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1103/PhysRevLett.134.081801",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.01383v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.01383v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Phys. Rev. Lett. 134, 081801, 2025",
                "arxiv_primary_category": {
                    "term": "hep-ex",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19312v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19312v1",
                "updated": "2025-02-26T17:08:46Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    17,
                    8,
                    46,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-26T17:08:46Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    17,
                    8,
                    46,
                    2,
                    57,
                    0
                ],
                "title": "FSPO: Few-Shot Preference Optimization of Synthetic Preference Data in\n  LLMs Elicits Effective Personalization to Real Users",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FSPO: Few-Shot Preference Optimization of Synthetic Preference Data in\n  LLMs Elicits Effective Personalization to Real Users"
                },
                "summary": "Effective personalization of LLMs is critical for a broad range of\nuser-interfacing applications such as virtual assistants and content curation.\nInspired by the strong in-context learning capabilities of LLMs, we propose\nFew-Shot Preference Optimization (FSPO), which reframes reward modeling as a\nmeta-learning problem. Under this framework, an LLM learns to quickly adapt to\na user via a few labeled preferences from that user, constructing a\npersonalized reward function for them. Additionally, since real-world\npreference data is scarce and challenging to collect at scale, we propose\ncareful design choices to construct synthetic preference datasets for\npersonalization, generating over 1M synthetic personalized preferences using\npublicly available LLMs. In particular, to successfully transfer from synthetic\ndata to real users, we find it crucial for the data to exhibit both high\ndiversity and coherent, self-consistent structure. We evaluate FSPO on\npersonalized open-ended generation for up to 1,500 synthetic users across\nacross three domains: movie reviews, pedagogical adaptation based on\neducational background, and general question answering, along with a controlled\nhuman study. Overall, FSPO achieves an 87% Alpaca Eval winrate on average in\ngenerating responses that are personalized to synthetic users and a 72% winrate\nwith real human users in open-ended question answering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective personalization of LLMs is critical for a broad range of\nuser-interfacing applications such as virtual assistants and content curation.\nInspired by the strong in-context learning capabilities of LLMs, we propose\nFew-Shot Preference Optimization (FSPO), which reframes reward modeling as a\nmeta-learning problem. Under this framework, an LLM learns to quickly adapt to\na user via a few labeled preferences from that user, constructing a\npersonalized reward function for them. Additionally, since real-world\npreference data is scarce and challenging to collect at scale, we propose\ncareful design choices to construct synthetic preference datasets for\npersonalization, generating over 1M synthetic personalized preferences using\npublicly available LLMs. In particular, to successfully transfer from synthetic\ndata to real users, we find it crucial for the data to exhibit both high\ndiversity and coherent, self-consistent structure. We evaluate FSPO on\npersonalized open-ended generation for up to 1,500 synthetic users across\nacross three domains: movie reviews, pedagogical adaptation based on\neducational background, and general question answering, along with a controlled\nhuman study. Overall, FSPO achieves an 87% Alpaca Eval winrate on average in\ngenerating responses that are personalized to synthetic users and a 72% winrate\nwith real human users in open-ended question answering."
                },
                "authors": [
                    {
                        "name": "Anikait Singh"
                    },
                    {
                        "name": "Sheryl Hsu"
                    },
                    {
                        "name": "Kyle Hsu"
                    },
                    {
                        "name": "Eric Mitchell"
                    },
                    {
                        "name": "Stefano Ermon"
                    },
                    {
                        "name": "Tatsunori Hashimoto"
                    },
                    {
                        "name": "Archit Sharma"
                    },
                    {
                        "name": "Chelsea Finn"
                    }
                ],
                "author_detail": {
                    "name": "Chelsea Finn"
                },
                "author": "Chelsea Finn",
                "arxiv_comment": "Website: https://fewshot-preference-optimization.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19312v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19312v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19301v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19301v1",
                "updated": "2025-02-26T16:59:21Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    16,
                    59,
                    21,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-26T16:59:21Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    16,
                    59,
                    21,
                    2,
                    57,
                    0
                ],
                "title": "Rethinking LLM Unlearning Objectives: A Gradient Perspective and Go\n  Beyond",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking LLM Unlearning Objectives: A Gradient Perspective and Go\n  Beyond"
                },
                "summary": "Large language models (LLMs) should undergo rigorous audits to identify\npotential risks, such as copyright and privacy infringements. Once these risks\nemerge, timely updates are crucial to remove undesirable responses, ensuring\nlegal and safe model usage. It has spurred recent research into LLM unlearning,\nfocusing on erasing targeted undesirable knowledge without compromising the\nintegrity of other, non-targeted responses. Existing studies have introduced\nvarious unlearning objectives to pursue LLM unlearning without necessitating\ncomplete retraining. However, each of these objectives has unique properties,\nand no unified framework is currently available to comprehend them thoroughly.\nTo fill the gap, we propose a toolkit of the gradient effect (G-effect),\nquantifying the impacts of unlearning objectives on model performance from a\ngradient perspective. A notable advantage is its broad ability to detail the\nunlearning impacts from various aspects across instances, updating steps, and\nLLM layers. Accordingly, the G-effect offers new insights into identifying\ndrawbacks of existing unlearning objectives, further motivating us to explore a\nseries of new solutions for their mitigation and improvements. Finally, we\noutline promising directions that merit further studies, aiming at contributing\nto the community to advance this important field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) should undergo rigorous audits to identify\npotential risks, such as copyright and privacy infringements. Once these risks\nemerge, timely updates are crucial to remove undesirable responses, ensuring\nlegal and safe model usage. It has spurred recent research into LLM unlearning,\nfocusing on erasing targeted undesirable knowledge without compromising the\nintegrity of other, non-targeted responses. Existing studies have introduced\nvarious unlearning objectives to pursue LLM unlearning without necessitating\ncomplete retraining. However, each of these objectives has unique properties,\nand no unified framework is currently available to comprehend them thoroughly.\nTo fill the gap, we propose a toolkit of the gradient effect (G-effect),\nquantifying the impacts of unlearning objectives on model performance from a\ngradient perspective. A notable advantage is its broad ability to detail the\nunlearning impacts from various aspects across instances, updating steps, and\nLLM layers. Accordingly, the G-effect offers new insights into identifying\ndrawbacks of existing unlearning objectives, further motivating us to explore a\nseries of new solutions for their mitigation and improvements. Finally, we\noutline promising directions that merit further studies, aiming at contributing\nto the community to advance this important field."
                },
                "authors": [
                    {
                        "name": "Qizhou Wang"
                    },
                    {
                        "name": "Jin Peng Zhou"
                    },
                    {
                        "name": "Zhanke Zhou"
                    },
                    {
                        "name": "Saebyeol Shin"
                    },
                    {
                        "name": "Bo Han"
                    },
                    {
                        "name": "Kilian Q. Weinberger"
                    }
                ],
                "author_detail": {
                    "name": "Kilian Q. Weinberger"
                },
                "author": "Kilian Q. Weinberger",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19301v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19301v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19298v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19298v1",
                "updated": "2025-02-26T16:56:19Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    16,
                    56,
                    19,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-26T16:56:19Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    16,
                    56,
                    19,
                    2,
                    57,
                    0
                ],
                "title": "Agent-centric Information Access",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agent-centric Information Access"
                },
                "summary": "As large language models (LLMs) become more specialized, we envision a future\nwhere millions of expert LLMs exist, each trained on proprietary data and\nexcelling in specific domains. In such a system, answering a query requires\nselecting a small subset of relevant models, querying them efficiently, and\nsynthesizing their responses. This paper introduces a framework for\nagent-centric information access, where LLMs function as knowledge agents that\nare dynamically ranked and queried based on their demonstrated expertise.\nUnlike traditional document retrieval, this approach requires inferring\nexpertise on the fly, rather than relying on static metadata or predefined\nmodel descriptions. This shift introduces several challenges, including\nefficient expert selection, cost-effective querying, response aggregation\nacross multiple models, and robustness against adversarial manipulation. To\naddress these issues, we propose a scalable evaluation framework that leverages\nretrieval-augmented generation and clustering techniques to construct and\nassess thousands of specialized models, with the potential to scale toward\nmillions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) become more specialized, we envision a future\nwhere millions of expert LLMs exist, each trained on proprietary data and\nexcelling in specific domains. In such a system, answering a query requires\nselecting a small subset of relevant models, querying them efficiently, and\nsynthesizing their responses. This paper introduces a framework for\nagent-centric information access, where LLMs function as knowledge agents that\nare dynamically ranked and queried based on their demonstrated expertise.\nUnlike traditional document retrieval, this approach requires inferring\nexpertise on the fly, rather than relying on static metadata or predefined\nmodel descriptions. This shift introduces several challenges, including\nefficient expert selection, cost-effective querying, response aggregation\nacross multiple models, and robustness against adversarial manipulation. To\naddress these issues, we propose a scalable evaluation framework that leverages\nretrieval-augmented generation and clustering techniques to construct and\nassess thousands of specialized models, with the potential to scale toward\nmillions."
                },
                "authors": [
                    {
                        "name": "Evangelos Kanoulas"
                    },
                    {
                        "name": "Panagiotis Eustratiadis"
                    },
                    {
                        "name": "Yongkang Li"
                    },
                    {
                        "name": "Yougang Lyu"
                    },
                    {
                        "name": "Vaishali Pal"
                    },
                    {
                        "name": "Gabrielle Poerwawinata"
                    },
                    {
                        "name": "Jingfen Qiao"
                    },
                    {
                        "name": "Zihan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zihan Wang"
                },
                "author": "Zihan Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19298v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19298v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19295v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19295v1",
                "updated": "2025-02-26T16:52:31Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    16,
                    52,
                    31,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-26T16:52:31Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    16,
                    52,
                    31,
                    2,
                    57,
                    0
                ],
                "title": "Complex LLM Planning via Automated Heuristics Discovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Complex LLM Planning via Automated Heuristics Discovery"
                },
                "summary": "We consider enhancing large language models (LLMs) for complex planning\ntasks. While existing methods allow LLMs to explore intermediate steps to make\nplans, they either depend on unreliable self-verification or external verifiers\nto evaluate these steps, which demand significant data and computations. Here,\nwe propose automated heuristics discovery (AutoHD), a novel approach that\nenables LLMs to explicitly generate heuristic functions to guide inference-time\nsearch, allowing accurate evaluation of intermediate states. These heuristic\nfunctions are further refined through a heuristic evolution process, improving\ntheir robustness and effectiveness. Our proposed method requires no additional\nmodel training or fine-tuning, and the explicit definition of heuristic\nfunctions generated by the LLMs provides interpretability and insights into the\nreasoning process. Extensive experiments across diverse benchmarks demonstrate\nsignificant gains over multiple baselines, including nearly twice the accuracy\non some datasets, establishing our approach as a reliable and interpretable\nsolution for complex planning tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider enhancing large language models (LLMs) for complex planning\ntasks. While existing methods allow LLMs to explore intermediate steps to make\nplans, they either depend on unreliable self-verification or external verifiers\nto evaluate these steps, which demand significant data and computations. Here,\nwe propose automated heuristics discovery (AutoHD), a novel approach that\nenables LLMs to explicitly generate heuristic functions to guide inference-time\nsearch, allowing accurate evaluation of intermediate states. These heuristic\nfunctions are further refined through a heuristic evolution process, improving\ntheir robustness and effectiveness. Our proposed method requires no additional\nmodel training or fine-tuning, and the explicit definition of heuristic\nfunctions generated by the LLMs provides interpretability and insights into the\nreasoning process. Extensive experiments across diverse benchmarks demonstrate\nsignificant gains over multiple baselines, including nearly twice the accuracy\non some datasets, establishing our approach as a reliable and interpretable\nsolution for complex planning tasks."
                },
                "authors": [
                    {
                        "name": "Hongyi Ling"
                    },
                    {
                        "name": "Shubham Parashar"
                    },
                    {
                        "name": "Sambhav Khurana"
                    },
                    {
                        "name": "Blake Olson"
                    },
                    {
                        "name": "Anwesha Basu"
                    },
                    {
                        "name": "Gaurangi Sinha"
                    },
                    {
                        "name": "Zhengzhong Tu"
                    },
                    {
                        "name": "James Caverlee"
                    },
                    {
                        "name": "Shuiwang Ji"
                    }
                ],
                "author_detail": {
                    "name": "Shuiwang Ji"
                },
                "author": "Shuiwang Ji",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19295v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19295v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02764v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02764v2",
                "updated": "2025-02-26T16:52:21Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    16,
                    52,
                    21,
                    2,
                    57,
                    0
                ],
                "published": "2024-12-03T19:05:37Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    19,
                    5,
                    37,
                    1,
                    338,
                    0
                ],
                "title": "Drawing Pandas: A Benchmark for LLMs in Generating Plotting Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Drawing Pandas: A Benchmark for LLMs in Generating Plotting Code"
                },
                "summary": "This paper introduces the human-curated PandasPlotBench dataset, designed to\nevaluate language models' effectiveness as assistants in visual data\nexploration. Our benchmark focuses on generating code for visualizing tabular\ndata - such as a Pandas DataFrame - based on natural language instructions,\ncomplementing current evaluation tools and expanding their scope. The dataset\nincludes 175 unique tasks. Our experiments assess several leading Large\nLanguage Models (LLMs) across three visualization libraries: Matplotlib,\nSeaborn, and Plotly. We show that the shortening of tasks has a minimal effect\non plotting capabilities, allowing for the user interface that accommodates\nconcise user input without sacrificing functionality or accuracy. Another of\nour findings reveals that while LLMs perform well with popular libraries like\nMatplotlib and Seaborn, challenges persist with Plotly, highlighting areas for\nimprovement. We hope that the modular design of our benchmark will broaden the\ncurrent studies on generating visualizations. Our dataset and benchmark code\nare available online:\nhttps://huggingface.co/datasets/JetBrains-Research/PandasPlotBench;\nhttps://github.com/JetBrains-Research/PandasPlotBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces the human-curated PandasPlotBench dataset, designed to\nevaluate language models' effectiveness as assistants in visual data\nexploration. Our benchmark focuses on generating code for visualizing tabular\ndata - such as a Pandas DataFrame - based on natural language instructions,\ncomplementing current evaluation tools and expanding their scope. The dataset\nincludes 175 unique tasks. Our experiments assess several leading Large\nLanguage Models (LLMs) across three visualization libraries: Matplotlib,\nSeaborn, and Plotly. We show that the shortening of tasks has a minimal effect\non plotting capabilities, allowing for the user interface that accommodates\nconcise user input without sacrificing functionality or accuracy. Another of\nour findings reveals that while LLMs perform well with popular libraries like\nMatplotlib and Seaborn, challenges persist with Plotly, highlighting areas for\nimprovement. We hope that the modular design of our benchmark will broaden the\ncurrent studies on generating visualizations. Our dataset and benchmark code\nare available online:\nhttps://huggingface.co/datasets/JetBrains-Research/PandasPlotBench;\nhttps://github.com/JetBrains-Research/PandasPlotBench."
                },
                "authors": [
                    {
                        "name": "Timur Galimzyanov"
                    },
                    {
                        "name": "Sergey Titov"
                    },
                    {
                        "name": "Yaroslav Golubev"
                    },
                    {
                        "name": "Egor Bogomolov"
                    }
                ],
                "author_detail": {
                    "name": "Egor Bogomolov"
                },
                "author": "Egor Bogomolov",
                "arxiv_comment": "5 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02764v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02764v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16502v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16502v2",
                "updated": "2025-02-26T16:46:25Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    16,
                    46,
                    25,
                    2,
                    57,
                    0
                ],
                "published": "2024-11-25T15:37:27Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    15,
                    37,
                    27,
                    0,
                    330,
                    0
                ],
                "title": "Interpreting Language Reward Models via Contrastive Explanations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interpreting Language Reward Models via Contrastive Explanations"
                },
                "summary": "Reward models (RMs) are a crucial component in the alignment of large\nlanguage models' (LLMs) outputs with human values. RMs approximate human\npreferences over possible LLM responses to the same prompt by predicting and\ncomparing reward scores. However, as they are typically modified versions of\nLLMs with scalar output heads, RMs are large black boxes whose predictions are\nnot explainable. More transparent RMs would enable improved trust in the\nalignment of LLMs. In this work, we propose to use contrastive explanations to\nexplain any binary response comparison made by an RM. Specifically, we generate\na diverse set of new comparisons similar to the original one to characterise\nthe RM's local behaviour. The perturbed responses forming the new comparisons\nare generated to explicitly modify manually specified high-level evaluation\nattributes, on which analyses of RM behaviour are grounded. In quantitative\nexperiments, we validate the effectiveness of our method for finding\nhigh-quality contrastive explanations. We then showcase the qualitative\nusefulness of our method for investigating global sensitivity of RMs to each\nevaluation attribute, and demonstrate how representative examples can be\nautomatically extracted to explain and compare behaviours of different RMs. We\nsee our method as a flexible framework for RM explanation, providing a basis\nfor more interpretable and trustworthy LLM alignment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reward models (RMs) are a crucial component in the alignment of large\nlanguage models' (LLMs) outputs with human values. RMs approximate human\npreferences over possible LLM responses to the same prompt by predicting and\ncomparing reward scores. However, as they are typically modified versions of\nLLMs with scalar output heads, RMs are large black boxes whose predictions are\nnot explainable. More transparent RMs would enable improved trust in the\nalignment of LLMs. In this work, we propose to use contrastive explanations to\nexplain any binary response comparison made by an RM. Specifically, we generate\na diverse set of new comparisons similar to the original one to characterise\nthe RM's local behaviour. The perturbed responses forming the new comparisons\nare generated to explicitly modify manually specified high-level evaluation\nattributes, on which analyses of RM behaviour are grounded. In quantitative\nexperiments, we validate the effectiveness of our method for finding\nhigh-quality contrastive explanations. We then showcase the qualitative\nusefulness of our method for investigating global sensitivity of RMs to each\nevaluation attribute, and demonstrate how representative examples can be\nautomatically extracted to explain and compare behaviours of different RMs. We\nsee our method as a flexible framework for RM explanation, providing a basis\nfor more interpretable and trustworthy LLM alignment."
                },
                "authors": [
                    {
                        "name": "Junqi Jiang"
                    },
                    {
                        "name": "Tom Bewley"
                    },
                    {
                        "name": "Saumitra Mishra"
                    },
                    {
                        "name": "Freddy Lecue"
                    },
                    {
                        "name": "Manuela Veloso"
                    }
                ],
                "author_detail": {
                    "name": "Manuela Veloso"
                },
                "author": "Manuela Veloso",
                "arxiv_comment": "Accepted at ICLR 2025 conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16502v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16502v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19285v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19285v1",
                "updated": "2025-02-26T16:45:09Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    16,
                    45,
                    9,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-26T16:45:09Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    16,
                    45,
                    9,
                    2,
                    57,
                    0
                ],
                "title": "On the Importance of Text Preprocessing for Multimodal Representation\n  Learning and Pathology Report Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Importance of Text Preprocessing for Multimodal Representation\n  Learning and Pathology Report Generation"
                },
                "summary": "Vision-language models in pathology enable multimodal case retrieval and\nautomated report generation. Many of the models developed so far, however, have\nbeen trained on pathology reports that include information which cannot be\ninferred from paired whole slide images (e.g., patient history), potentially\nleading to hallucinated sentences in generated reports. To this end, we\ninvestigate how the selection of information from pathology reports for\nvision-language modeling affects the quality of the multimodal representations\nand generated reports. More concretely, we compare a model trained on full\nreports against a model trained on preprocessed reports that only include\nsentences describing the cell and tissue appearances based on the H&E-stained\nslides. For the experiments, we built upon the BLIP-2 framework and used a\ncutaneous melanocytic lesion dataset of 42,433 H&E-stained whole slide images\nand 19,636 corresponding pathology reports. Model performance was assessed\nusing image-to-text and text-to-image retrieval, as well as qualitative\nevaluation of the generated reports by an expert pathologist. Our results\ndemonstrate that text preprocessing prevents hallucination in report\ngeneration. Despite the improvement in the quality of the generated reports,\ntraining the vision-language model on full reports showed better cross-modal\nretrieval performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language models in pathology enable multimodal case retrieval and\nautomated report generation. Many of the models developed so far, however, have\nbeen trained on pathology reports that include information which cannot be\ninferred from paired whole slide images (e.g., patient history), potentially\nleading to hallucinated sentences in generated reports. To this end, we\ninvestigate how the selection of information from pathology reports for\nvision-language modeling affects the quality of the multimodal representations\nand generated reports. More concretely, we compare a model trained on full\nreports against a model trained on preprocessed reports that only include\nsentences describing the cell and tissue appearances based on the H&E-stained\nslides. For the experiments, we built upon the BLIP-2 framework and used a\ncutaneous melanocytic lesion dataset of 42,433 H&E-stained whole slide images\nand 19,636 corresponding pathology reports. Model performance was assessed\nusing image-to-text and text-to-image retrieval, as well as qualitative\nevaluation of the generated reports by an expert pathologist. Our results\ndemonstrate that text preprocessing prevents hallucination in report\ngeneration. Despite the improvement in the quality of the generated reports,\ntraining the vision-language model on full reports showed better cross-modal\nretrieval performance."
                },
                "authors": [
                    {
                        "name": "Ruben T. Lucassen"
                    },
                    {
                        "name": "Tijn van de Luijtgaarden"
                    },
                    {
                        "name": "Sander P. J. Moonemans"
                    },
                    {
                        "name": "Gerben E. Breimer"
                    },
                    {
                        "name": "Willeke A. M. Blokx"
                    },
                    {
                        "name": "Mitko Veta"
                    }
                ],
                "author_detail": {
                    "name": "Mitko Veta"
                },
                "author": "Mitko Veta",
                "arxiv_comment": "11 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19285v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19285v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06689v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06689v3",
                "updated": "2025-02-26T16:36:55Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    16,
                    36,
                    55,
                    2,
                    57,
                    0
                ],
                "published": "2025-01-12T02:43:59Z",
                "published_parsed": [
                    2025,
                    1,
                    12,
                    2,
                    43,
                    59,
                    6,
                    12,
                    0
                ],
                "title": "TAPO: Task-Referenced Adaptation for Prompt Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TAPO: Task-Referenced Adaptation for Prompt Optimization"
                },
                "summary": "Prompt engineering can significantly improve the performance of large\nlanguage models (LLMs), with automated prompt optimization (APO) gaining\nsignificant attention due to the time-consuming and laborious nature of manual\nprompt design. However, much of the existing work in APO overlooks\ntask-specific characteristics, resulting in prompts that lack domain\nspecificity and are not well-suited for task-specific optimization. In this\npaper, we introduce TAPO, a multitask-aware prompt optimization framework\ncomposed of three key modules. First, a task-aware metric selection module is\nproposed to enhance task-specific prompt generation capabilities. Second, we\npresent a multi-metrics evaluation module to jointly evaluate prompts from\nmultiple perspectives. Third, an evolution-based optimization framework is\nintroduced for automatic prompt refinement, which improves adaptability across\nvarious tasks. Extensive experiments on six datasets demonstrate the\neffectiveness of our approach, and our code is publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt engineering can significantly improve the performance of large\nlanguage models (LLMs), with automated prompt optimization (APO) gaining\nsignificant attention due to the time-consuming and laborious nature of manual\nprompt design. However, much of the existing work in APO overlooks\ntask-specific characteristics, resulting in prompts that lack domain\nspecificity and are not well-suited for task-specific optimization. In this\npaper, we introduce TAPO, a multitask-aware prompt optimization framework\ncomposed of three key modules. First, a task-aware metric selection module is\nproposed to enhance task-specific prompt generation capabilities. Second, we\npresent a multi-metrics evaluation module to jointly evaluate prompts from\nmultiple perspectives. Third, an evolution-based optimization framework is\nintroduced for automatic prompt refinement, which improves adaptability across\nvarious tasks. Extensive experiments on six datasets demonstrate the\neffectiveness of our approach, and our code is publicly available."
                },
                "authors": [
                    {
                        "name": "Wenxin Luo"
                    },
                    {
                        "name": "Weirui Wang"
                    },
                    {
                        "name": "Xiaopeng Li"
                    },
                    {
                        "name": "Weibo Zhou"
                    },
                    {
                        "name": "Pengyue Jia"
                    },
                    {
                        "name": "Xiangyu Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyu Zhao"
                },
                "author": "Xiangyu Zhao",
                "arxiv_comment": "Accepted to ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06689v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06689v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19280v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19280v1",
                "updated": "2025-02-26T16:36:24Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    16,
                    36,
                    24,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-26T16:36:24Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    16,
                    36,
                    24,
                    2,
                    57,
                    0
                ],
                "title": "Efficient Federated Search for Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Federated Search for Retrieval-Augmented Generation"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable capabilities across\nvarious domains but remain susceptible to hallucinations and inconsistencies,\nlimiting their reliability. Retrieval-augmented generation (RAG) mitigates\nthese issues by grounding model responses in external knowledge sources.\nExisting RAG workflows often leverage a single vector database, which is\nimpractical in the common setting where information is distributed across\nmultiple repositories. We introduce RAGRoute, a novel mechanism for federated\nRAG search. RAGRoute dynamically selects relevant data sources at query time\nusing a lightweight neural network classifier. By not querying every data\nsource, this approach significantly reduces query overhead, improves retrieval\nefficiency, and minimizes the retrieval of irrelevant information. We evaluate\nRAGRoute using the MIRAGE and MMLU benchmarks and demonstrate its effectiveness\nin retrieving relevant documents while reducing the number of queries. RAGRoute\nreduces the total number of queries up to 77.5% and communication volume up to\n76.2%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable capabilities across\nvarious domains but remain susceptible to hallucinations and inconsistencies,\nlimiting their reliability. Retrieval-augmented generation (RAG) mitigates\nthese issues by grounding model responses in external knowledge sources.\nExisting RAG workflows often leverage a single vector database, which is\nimpractical in the common setting where information is distributed across\nmultiple repositories. We introduce RAGRoute, a novel mechanism for federated\nRAG search. RAGRoute dynamically selects relevant data sources at query time\nusing a lightweight neural network classifier. By not querying every data\nsource, this approach significantly reduces query overhead, improves retrieval\nefficiency, and minimizes the retrieval of irrelevant information. We evaluate\nRAGRoute using the MIRAGE and MMLU benchmarks and demonstrate its effectiveness\nin retrieving relevant documents while reducing the number of queries. RAGRoute\nreduces the total number of queries up to 77.5% and communication volume up to\n76.2%."
                },
                "authors": [
                    {
                        "name": "Rachid Guerraoui"
                    },
                    {
                        "name": "Anne-Marie Kermarrec"
                    },
                    {
                        "name": "Diana Petrescu"
                    },
                    {
                        "name": "Rafael Pires"
                    },
                    {
                        "name": "Mathis Randl"
                    },
                    {
                        "name": "Martijn de Vos"
                    }
                ],
                "author_detail": {
                    "name": "Martijn de Vos"
                },
                "author": "Martijn de Vos",
                "arxiv_comment": "To appear in the proceedings of EuroMLSys'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19280v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19280v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19263v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19263v1",
                "updated": "2025-02-26T16:17:15Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    16,
                    17,
                    15,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-26T16:17:15Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    16,
                    17,
                    15,
                    2,
                    57,
                    0
                ],
                "title": "ArtInsight: Enabling AI-Powered Artwork Engagement for Mixed\n  Visual-Ability Families",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ArtInsight: Enabling AI-Powered Artwork Engagement for Mixed\n  Visual-Ability Families"
                },
                "summary": "We introduce ArtInsight, a novel AI-powered system to facilitate deeper\nengagement with child-created artwork in mixed visual-ability families.\nArtInsight leverages large language models (LLMs) to craft a respectful and\nthorough initial description of a child's artwork, and provides: creative\nAI-generated descriptions for a vivid overview, audio recording to capture the\nchild's own description of their artwork, and a set of AI-generated questions\nto facilitate discussion between blind or low-vision (BLV) family members and\ntheir children. Alongside ArtInsight, we also contribute a new rubric to score\nAI-generated descriptions of child-created artwork and an assessment of\nstate-of-the-art LLMs. We evaluated ArtInsight with five groups of BLV family\nmembers and their children, and as a case study with one BLV child therapist.\nOur findings highlight a preference for ArtInsight's longer,\nartistically-tailored descriptions over those generated by existing BLV AI\ntools. Participants highlighted the creative description and audio recording\ncomponents as most beneficial, with the former helping ``bring a picture to\nlife'' and the latter centering the child's narrative to generate context-aware\nAI responses. Our findings reveal different ways that AI can be used to support\nart engagement, including before, during, and after interaction with the child\nartist, as well as expectations that BLV adults and their sighted children have\nabout AI-powered tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce ArtInsight, a novel AI-powered system to facilitate deeper\nengagement with child-created artwork in mixed visual-ability families.\nArtInsight leverages large language models (LLMs) to craft a respectful and\nthorough initial description of a child's artwork, and provides: creative\nAI-generated descriptions for a vivid overview, audio recording to capture the\nchild's own description of their artwork, and a set of AI-generated questions\nto facilitate discussion between blind or low-vision (BLV) family members and\ntheir children. Alongside ArtInsight, we also contribute a new rubric to score\nAI-generated descriptions of child-created artwork and an assessment of\nstate-of-the-art LLMs. We evaluated ArtInsight with five groups of BLV family\nmembers and their children, and as a case study with one BLV child therapist.\nOur findings highlight a preference for ArtInsight's longer,\nartistically-tailored descriptions over those generated by existing BLV AI\ntools. Participants highlighted the creative description and audio recording\ncomponents as most beneficial, with the former helping ``bring a picture to\nlife'' and the latter centering the child's narrative to generate context-aware\nAI responses. Our findings reveal different ways that AI can be used to support\nart engagement, including before, during, and after interaction with the child\nartist, as well as expectations that BLV adults and their sighted children have\nabout AI-powered tools."
                },
                "authors": [
                    {
                        "name": "Arnavi Chheda-Kothary"
                    },
                    {
                        "name": "Ritesh Kanchi"
                    },
                    {
                        "name": "Chris Sanders"
                    },
                    {
                        "name": "Kevin Xiao"
                    },
                    {
                        "name": "Aditya Sengupta"
                    },
                    {
                        "name": "Melanie Kneitmix"
                    },
                    {
                        "name": "Jacob O. Wobbrock"
                    },
                    {
                        "name": "Jon E. Froehlich"
                    }
                ],
                "author_detail": {
                    "name": "Jon E. Froehlich"
                },
                "author": "Jon E. Froehlich",
                "arxiv_doi": "10.1145/3708359.3712082",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3708359.3712082",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.19263v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19263v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "21 pages, 30th International Conference on Intelligent User\n  Interfaces (IUI 2025)",
                "arxiv_journal_ref": "30th International Conference on Intelligent User Interfaces 2025",
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17533v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17533v2",
                "updated": "2025-02-26T16:13:37Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    16,
                    13,
                    37,
                    2,
                    57,
                    0
                ],
                "published": "2024-11-26T15:48:35Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    15,
                    48,
                    35,
                    1,
                    331,
                    0
                ],
                "title": "Simplifying Causal Mediation Analysis for Time-to-Event Outcomes using\n  Pseudo-Values",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simplifying Causal Mediation Analysis for Time-to-Event Outcomes using\n  Pseudo-Values"
                },
                "summary": "Mediation analysis for survival outcomes is challenging. Most existing\nmethods quantify the treatment effect using the hazard ratio (HR) and attempt\nto decompose the HR into the direct effect of treatment plus an indirect, or\nmediated, effect. However, the HR is not expressible as an expectation, which\ncomplicates this decomposition, both in terms of estimation and interpretation.\nHere, we present an alternative approach which leverages pseudo-values to\nsimplify estimation and inference. Pseudo-values take censoring into account\nduring their construction, and once derived, can be modeled in the same way as\nany continuous outcome. Thus, pseudo-values enable mediation analysis for a\nsurvival outcome to fit seamlessly into standard mediation software (e.g.\nCMAverse in R). Pseudo-values are easy to calculate via a\nleave-one-observation-out procedure (i.e. jackknifing) and the calculation can\nbe accelerated when the influence function of the estimator is known. Mediation\nanalysis for causal effects defined by survival probabilities, restricted mean\nsurvival time, and cumulative incidence functions - in the presence of\ncompeting risks - can all be performed within this framework. Extensive\nsimulation studies demonstrate that the method is unbiased across 324\nscenarios/estimands and controls the type-I error at the nominal level under\nthe null of no mediation. We illustrate the approach using data from the\nPARADIGMS clinical trial for the treatment of pediatric multiple sclerosis\nusing fingolimod. In particular, we evaluate whether an imaging biomarker lies\non the causal path between treatment and time-to-relapse, which aids in\njustifying this biomarker as a surrogate outcome. Our approach greatly\nsimplifies mediation analysis for survival data and provides a decomposition of\nthe total effect that is both intuitive and interpretable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mediation analysis for survival outcomes is challenging. Most existing\nmethods quantify the treatment effect using the hazard ratio (HR) and attempt\nto decompose the HR into the direct effect of treatment plus an indirect, or\nmediated, effect. However, the HR is not expressible as an expectation, which\ncomplicates this decomposition, both in terms of estimation and interpretation.\nHere, we present an alternative approach which leverages pseudo-values to\nsimplify estimation and inference. Pseudo-values take censoring into account\nduring their construction, and once derived, can be modeled in the same way as\nany continuous outcome. Thus, pseudo-values enable mediation analysis for a\nsurvival outcome to fit seamlessly into standard mediation software (e.g.\nCMAverse in R). Pseudo-values are easy to calculate via a\nleave-one-observation-out procedure (i.e. jackknifing) and the calculation can\nbe accelerated when the influence function of the estimator is known. Mediation\nanalysis for causal effects defined by survival probabilities, restricted mean\nsurvival time, and cumulative incidence functions - in the presence of\ncompeting risks - can all be performed within this framework. Extensive\nsimulation studies demonstrate that the method is unbiased across 324\nscenarios/estimands and controls the type-I error at the nominal level under\nthe null of no mediation. We illustrate the approach using data from the\nPARADIGMS clinical trial for the treatment of pediatric multiple sclerosis\nusing fingolimod. In particular, we evaluate whether an imaging biomarker lies\non the causal path between treatment and time-to-relapse, which aids in\njustifying this biomarker as a surrogate outcome. Our approach greatly\nsimplifies mediation analysis for survival data and provides a decomposition of\nthe total effect that is both intuitive and interpretable."
                },
                "authors": [
                    {
                        "name": "Alex Ocampo"
                    },
                    {
                        "name": "Enrico Giudice"
                    },
                    {
                        "name": "Dieter A. Häring"
                    },
                    {
                        "name": "Baldur Magnusson"
                    },
                    {
                        "name": "Theis Lange"
                    },
                    {
                        "name": "Zachary R. McCaw"
                    }
                ],
                "author_detail": {
                    "name": "Zachary R. McCaw"
                },
                "author": "Zachary R. McCaw",
                "arxiv_comment": "Mediation, Pseudo-Values, Time-to-event, Survival Analysis,\n  Restricted Mean Survival Time, Competing Risks",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17533v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17533v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19261v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19261v1",
                "updated": "2025-02-26T16:06:36Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    16,
                    6,
                    36,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-26T16:06:36Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    16,
                    6,
                    36,
                    2,
                    57,
                    0
                ],
                "title": "Drop-Upcycling: Training Sparse Mixture of Experts with Partial\n  Re-initialization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Drop-Upcycling: Training Sparse Mixture of Experts with Partial\n  Re-initialization"
                },
                "summary": "The Mixture of Experts (MoE) architecture reduces the training and inference\ncost significantly compared to a dense model of equivalent capacity. Upcycling\nis an approach that initializes and trains an MoE model using a pre-trained\ndense model. While upcycling leads to initial performance gains, the training\nprogresses slower than when trained from scratch, leading to suboptimal\nperformance in the long term. We propose Drop-Upcycling - a method that\neffectively addresses this problem. Drop-Upcycling combines two seemingly\ncontradictory approaches: utilizing the knowledge of pre-trained dense models\nwhile statistically re-initializing some parts of the weights. This approach\nstrategically promotes expert specialization, significantly enhancing the MoE\nmodel's efficiency in knowledge acquisition. Extensive large-scale experiments\ndemonstrate that Drop-Upcycling significantly outperforms previous MoE\nconstruction methods in the long term, specifically when training on hundreds\nof billions of tokens or more. As a result, our MoE model with 5.9B active\nparameters achieves comparable performance to a 13B dense model in the same\nmodel family, while requiring approximately 1/4 of the training FLOPs. All\nexperimental resources, including source code, training data, model checkpoints\nand logs, are publicly available to promote reproducibility and future research\non MoE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Mixture of Experts (MoE) architecture reduces the training and inference\ncost significantly compared to a dense model of equivalent capacity. Upcycling\nis an approach that initializes and trains an MoE model using a pre-trained\ndense model. While upcycling leads to initial performance gains, the training\nprogresses slower than when trained from scratch, leading to suboptimal\nperformance in the long term. We propose Drop-Upcycling - a method that\neffectively addresses this problem. Drop-Upcycling combines two seemingly\ncontradictory approaches: utilizing the knowledge of pre-trained dense models\nwhile statistically re-initializing some parts of the weights. This approach\nstrategically promotes expert specialization, significantly enhancing the MoE\nmodel's efficiency in knowledge acquisition. Extensive large-scale experiments\ndemonstrate that Drop-Upcycling significantly outperforms previous MoE\nconstruction methods in the long term, specifically when training on hundreds\nof billions of tokens or more. As a result, our MoE model with 5.9B active\nparameters achieves comparable performance to a 13B dense model in the same\nmodel family, while requiring approximately 1/4 of the training FLOPs. All\nexperimental resources, including source code, training data, model checkpoints\nand logs, are publicly available to promote reproducibility and future research\non MoE."
                },
                "authors": [
                    {
                        "name": "Taishi Nakamura"
                    },
                    {
                        "name": "Takuya Akiba"
                    },
                    {
                        "name": "Kazuki Fujii"
                    },
                    {
                        "name": "Yusuke Oda"
                    },
                    {
                        "name": "Rio Yokota"
                    },
                    {
                        "name": "Jun Suzuki"
                    }
                ],
                "author_detail": {
                    "name": "Jun Suzuki"
                },
                "author": "Jun Suzuki",
                "arxiv_comment": "To appear at the 13th International Conference on Learning\n  Representations (ICLR 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19261v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19261v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.13720v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.13720v2",
                "updated": "2025-02-26T16:05:55Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    16,
                    5,
                    55,
                    2,
                    57,
                    0
                ],
                "published": "2024-10-17T16:22:46Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    16,
                    22,
                    46,
                    3,
                    291,
                    0
                ],
                "title": "Movie Gen: A Cast of Media Foundation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Movie Gen: A Cast of Media Foundation Models"
                },
                "summary": "We present Movie Gen, a cast of foundation models that generates\nhigh-quality, 1080p HD videos with different aspect ratios and synchronized\naudio. We also show additional capabilities such as precise instruction-based\nvideo editing and generation of personalized videos based on a user's image.\nOur models set a new state-of-the-art on multiple tasks: text-to-video\nsynthesis, video personalization, video editing, video-to-audio generation, and\ntext-to-audio generation. Our largest video generation model is a 30B parameter\ntransformer trained with a maximum context length of 73K video tokens,\ncorresponding to a generated video of 16 seconds at 16 frames-per-second. We\nshow multiple technical innovations and simplifications on the architecture,\nlatent spaces, training objectives and recipes, data curation, evaluation\nprotocols, parallelization techniques, and inference optimizations that allow\nus to reap the benefits of scaling pre-training data, model size, and training\ncompute for training large scale media generation models. We hope this paper\nhelps the research community to accelerate progress and innovation in media\ngeneration models. All videos from this paper are available at\nhttps://go.fb.me/MovieGenResearchVideos.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Movie Gen, a cast of foundation models that generates\nhigh-quality, 1080p HD videos with different aspect ratios and synchronized\naudio. We also show additional capabilities such as precise instruction-based\nvideo editing and generation of personalized videos based on a user's image.\nOur models set a new state-of-the-art on multiple tasks: text-to-video\nsynthesis, video personalization, video editing, video-to-audio generation, and\ntext-to-audio generation. Our largest video generation model is a 30B parameter\ntransformer trained with a maximum context length of 73K video tokens,\ncorresponding to a generated video of 16 seconds at 16 frames-per-second. We\nshow multiple technical innovations and simplifications on the architecture,\nlatent spaces, training objectives and recipes, data curation, evaluation\nprotocols, parallelization techniques, and inference optimizations that allow\nus to reap the benefits of scaling pre-training data, model size, and training\ncompute for training large scale media generation models. We hope this paper\nhelps the research community to accelerate progress and innovation in media\ngeneration models. All videos from this paper are available at\nhttps://go.fb.me/MovieGenResearchVideos."
                },
                "authors": [
                    {
                        "name": "Adam Polyak"
                    },
                    {
                        "name": "Amit Zohar"
                    },
                    {
                        "name": "Andrew Brown"
                    },
                    {
                        "name": "Andros Tjandra"
                    },
                    {
                        "name": "Animesh Sinha"
                    },
                    {
                        "name": "Ann Lee"
                    },
                    {
                        "name": "Apoorv Vyas"
                    },
                    {
                        "name": "Bowen Shi"
                    },
                    {
                        "name": "Chih-Yao Ma"
                    },
                    {
                        "name": "Ching-Yao Chuang"
                    },
                    {
                        "name": "David Yan"
                    },
                    {
                        "name": "Dhruv Choudhary"
                    },
                    {
                        "name": "Dingkang Wang"
                    },
                    {
                        "name": "Geet Sethi"
                    },
                    {
                        "name": "Guan Pang"
                    },
                    {
                        "name": "Haoyu Ma"
                    },
                    {
                        "name": "Ishan Misra"
                    },
                    {
                        "name": "Ji Hou"
                    },
                    {
                        "name": "Jialiang Wang"
                    },
                    {
                        "name": "Kiran Jagadeesh"
                    },
                    {
                        "name": "Kunpeng Li"
                    },
                    {
                        "name": "Luxin Zhang"
                    },
                    {
                        "name": "Mannat Singh"
                    },
                    {
                        "name": "Mary Williamson"
                    },
                    {
                        "name": "Matt Le"
                    },
                    {
                        "name": "Matthew Yu"
                    },
                    {
                        "name": "Mitesh Kumar Singh"
                    },
                    {
                        "name": "Peizhao Zhang"
                    },
                    {
                        "name": "Peter Vajda"
                    },
                    {
                        "name": "Quentin Duval"
                    },
                    {
                        "name": "Rohit Girdhar"
                    },
                    {
                        "name": "Roshan Sumbaly"
                    },
                    {
                        "name": "Sai Saketh Rambhatla"
                    },
                    {
                        "name": "Sam Tsai"
                    },
                    {
                        "name": "Samaneh Azadi"
                    },
                    {
                        "name": "Samyak Datta"
                    },
                    {
                        "name": "Sanyuan Chen"
                    },
                    {
                        "name": "Sean Bell"
                    },
                    {
                        "name": "Sharadh Ramaswamy"
                    },
                    {
                        "name": "Shelly Sheynin"
                    },
                    {
                        "name": "Siddharth Bhattacharya"
                    },
                    {
                        "name": "Simran Motwani"
                    },
                    {
                        "name": "Tao Xu"
                    },
                    {
                        "name": "Tianhe Li"
                    },
                    {
                        "name": "Tingbo Hou"
                    },
                    {
                        "name": "Wei-Ning Hsu"
                    },
                    {
                        "name": "Xi Yin"
                    },
                    {
                        "name": "Xiaoliang Dai"
                    },
                    {
                        "name": "Yaniv Taigman"
                    },
                    {
                        "name": "Yaqiao Luo"
                    },
                    {
                        "name": "Yen-Cheng Liu"
                    },
                    {
                        "name": "Yi-Chiao Wu"
                    },
                    {
                        "name": "Yue Zhao"
                    },
                    {
                        "name": "Yuval Kirstain"
                    },
                    {
                        "name": "Zecheng He"
                    },
                    {
                        "name": "Zijian He"
                    },
                    {
                        "name": "Albert Pumarola"
                    },
                    {
                        "name": "Ali Thabet"
                    },
                    {
                        "name": "Artsiom Sanakoyeu"
                    },
                    {
                        "name": "Arun Mallya"
                    },
                    {
                        "name": "Baishan Guo"
                    },
                    {
                        "name": "Boris Araya"
                    },
                    {
                        "name": "Breena Kerr"
                    },
                    {
                        "name": "Carleigh Wood"
                    },
                    {
                        "name": "Ce Liu"
                    },
                    {
                        "name": "Cen Peng"
                    },
                    {
                        "name": "Dimitry Vengertsev"
                    },
                    {
                        "name": "Edgar Schonfeld"
                    },
                    {
                        "name": "Elliot Blanchard"
                    },
                    {
                        "name": "Felix Juefei-Xu"
                    },
                    {
                        "name": "Fraylie Nord"
                    },
                    {
                        "name": "Jeff Liang"
                    },
                    {
                        "name": "John Hoffman"
                    },
                    {
                        "name": "Jonas Kohler"
                    },
                    {
                        "name": "Kaolin Fire"
                    },
                    {
                        "name": "Karthik Sivakumar"
                    },
                    {
                        "name": "Lawrence Chen"
                    },
                    {
                        "name": "Licheng Yu"
                    },
                    {
                        "name": "Luya Gao"
                    },
                    {
                        "name": "Markos Georgopoulos"
                    },
                    {
                        "name": "Rashel Moritz"
                    },
                    {
                        "name": "Sara K. Sampson"
                    },
                    {
                        "name": "Shikai Li"
                    },
                    {
                        "name": "Simone Parmeggiani"
                    },
                    {
                        "name": "Steve Fine"
                    },
                    {
                        "name": "Tara Fowler"
                    },
                    {
                        "name": "Vladan Petrovic"
                    },
                    {
                        "name": "Yuming Du"
                    }
                ],
                "author_detail": {
                    "name": "Yuming Du"
                },
                "author": "Yuming Du",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.13720v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.13720v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19253v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19253v1",
                "updated": "2025-02-26T15:58:35Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    15,
                    58,
                    35,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-26T15:58:35Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    15,
                    58,
                    35,
                    2,
                    57,
                    0
                ],
                "title": "U-Net 3+ for Anomalous Diffusion Analysis enhanced with Mixture\n  Estimates (U-AnD-ME) in particle-tracking data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "U-Net 3+ for Anomalous Diffusion Analysis enhanced with Mixture\n  Estimates (U-AnD-ME) in particle-tracking data"
                },
                "summary": "Biophysical processes within living systems rely on encounters and\ninteractions between molecules in complex environments such as cells. They are\noften described by anomalous diffusion transport. Recent advances in\nsingle-molecule microscopy and particle-tracking techniques have yielded an\nabundance of data in the form of videos and trajectories that contain critical\ninformation about these biologically significant processes. However, standard\napproaches for characterizing anomalous diffusion from these measurements often\nstruggle in cases of practical interest, e.g. due to short, noisy trajectories.\nFully exploiting this data therefore requires the development of advanced\nanalysis methods -- a core goal at the heart of the recent international\nAnomalous Diffusion Challenges. Here, we introduce a novel machine-learning\nframework, U-net 3+ for Anomalous Diffusion analysis enhanced with Mixture\nEstimates (U-AnD-ME), that applies a U-Net 3+ based neural network alongside\nGaussian mixture models to enable highly accurate characterisation of\nsingle-particle tracking data. In the 2024 Anomalous Diffusion Challenge,\nU-AnD-ME outperformed all other participating methods for the analysis of\ntwo-dimensional anomalous diffusion trajectories at both single-trajectory and\nensemble levels. Using a large dataset inspired by the Challenge, we further\ncharacterize the performance of U-AnD-ME in segmenting trajectories and\ninferring anomalous diffusion properties.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Biophysical processes within living systems rely on encounters and\ninteractions between molecules in complex environments such as cells. They are\noften described by anomalous diffusion transport. Recent advances in\nsingle-molecule microscopy and particle-tracking techniques have yielded an\nabundance of data in the form of videos and trajectories that contain critical\ninformation about these biologically significant processes. However, standard\napproaches for characterizing anomalous diffusion from these measurements often\nstruggle in cases of practical interest, e.g. due to short, noisy trajectories.\nFully exploiting this data therefore requires the development of advanced\nanalysis methods -- a core goal at the heart of the recent international\nAnomalous Diffusion Challenges. Here, we introduce a novel machine-learning\nframework, U-net 3+ for Anomalous Diffusion analysis enhanced with Mixture\nEstimates (U-AnD-ME), that applies a U-Net 3+ based neural network alongside\nGaussian mixture models to enable highly accurate characterisation of\nsingle-particle tracking data. In the 2024 Anomalous Diffusion Challenge,\nU-AnD-ME outperformed all other participating methods for the analysis of\ntwo-dimensional anomalous diffusion trajectories at both single-trajectory and\nensemble levels. Using a large dataset inspired by the Challenge, we further\ncharacterize the performance of U-AnD-ME in segmenting trajectories and\ninferring anomalous diffusion properties."
                },
                "authors": [
                    {
                        "name": "Solomon Asghar"
                    },
                    {
                        "name": "Ran Ni"
                    },
                    {
                        "name": "Giorgio Volpe"
                    }
                ],
                "author_detail": {
                    "name": "Giorgio Volpe"
                },
                "author": "Giorgio Volpe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19253v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19253v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.data-an",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.data-an",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.dis-nn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.soft",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.stat-mech",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.bio-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19245v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19245v1",
                "updated": "2025-02-26T15:52:00Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    15,
                    52,
                    0,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-26T15:52:00Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    15,
                    52,
                    0,
                    2,
                    57,
                    0
                ],
                "title": "A strike of luck: could the KM3-230213A event be caused by an\n  evaporating primordial black hole?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A strike of luck: could the KM3-230213A event be caused by an\n  evaporating primordial black hole?"
                },
                "summary": "We investigate whether the ultra high energy neutrino inferred by the recent\nKM3NeT observation could have originated from an evaporating black hole. Given\nthe characteristics of black hole (BH) evaporation mechanism, any object\ncapable of producing particles in the energy range of the detected event\n(around 100-800 PeV) must have a mass below 10^7 g. No known astrophysical\nmechanism can generate black holes of such low mass, leaving primordial black\nholes (PBHs)-potentially formed at the end of cosmic inflation-as the only\nviable candidates. Black holes with masses below 10^7 g have lifetimes shorter\nthan 10^-5 seconds, meaning PBHs in this mass range should have fully\nevaporated by now. However, recent studies suggest that quantum effects,\ncollectively referred to as the \"memory burden\", may slow down black hole\nevaporation, potentially extending the lifetimes of low-mass PBHs to timescales\ncomparable to or exceeding the Hubble time. We systematically explore the\nparameter space of memory-burdened PBHs, assuming that they constitute a\nfraction of the dark matter (f-PBH) within current constraints, and identify\nviable regions that could explain the KM3-230213A event. We further predict the\noccurrence rate of similar events and find that KM3NeT's current configuration\ncould test this scenario within a few years.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate whether the ultra high energy neutrino inferred by the recent\nKM3NeT observation could have originated from an evaporating black hole. Given\nthe characteristics of black hole (BH) evaporation mechanism, any object\ncapable of producing particles in the energy range of the detected event\n(around 100-800 PeV) must have a mass below 10^7 g. No known astrophysical\nmechanism can generate black holes of such low mass, leaving primordial black\nholes (PBHs)-potentially formed at the end of cosmic inflation-as the only\nviable candidates. Black holes with masses below 10^7 g have lifetimes shorter\nthan 10^-5 seconds, meaning PBHs in this mass range should have fully\nevaporated by now. However, recent studies suggest that quantum effects,\ncollectively referred to as the \"memory burden\", may slow down black hole\nevaporation, potentially extending the lifetimes of low-mass PBHs to timescales\ncomparable to or exceeding the Hubble time. We systematically explore the\nparameter space of memory-burdened PBHs, assuming that they constitute a\nfraction of the dark matter (f-PBH) within current constraints, and identify\nviable regions that could explain the KM3-230213A event. We further predict the\noccurrence rate of similar events and find that KM3NeT's current configuration\ncould test this scenario within a few years."
                },
                "authors": [
                    {
                        "name": "Andrea Boccia"
                    },
                    {
                        "name": "Fabio Iocco"
                    }
                ],
                "author_detail": {
                    "name": "Fabio Iocco"
                },
                "author": "Fabio Iocco",
                "arxiv_comment": "Comments welcome",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19245v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19245v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19231v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19231v1",
                "updated": "2025-02-26T15:42:06Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    15,
                    42,
                    6,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-26T15:42:06Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    15,
                    42,
                    6,
                    2,
                    57,
                    0
                ],
                "title": "AI-Powered Bayesian Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI-Powered Bayesian Inference"
                },
                "summary": "The advent of Generative Artificial Intelligence (GAI) has heralded an\ninflection point that changed how society thinks about knowledge acquisition.\nWhile GAI cannot be fully trusted for decision-making, it may still provide\nvaluable information that can be integrated into a decision pipeline. Rather\nthan seeing the lack of certitude and inherent randomness of GAI as a problem,\nwe view it as an opportunity. Indeed, variable answers to given prompts can be\nleveraged to construct a prior distribution which reflects assuredness of AI\npredictions. This prior distribution may be combined with tailored datasets for\na fully Bayesian analysis with an AI-driven prior. In this paper, we explore\nsuch a possibility within a non-parametric Bayesian framework. The basic idea\nconsists of assigning a Dirichlet process prior distribution on the\ndata-generating distribution with AI generative model as its baseline.\nHyper-parameters of the prior can be tuned out-of-sample to assess the\ninformativeness of the AI prior. Posterior simulation is achieved by computing\na suitably randomized functional on an augmented data that consists of observed\n(labeled) data as well as fake data whose labels have been imputed using AI.\nThis strategy can be parallelized and rapidly produces iid samples from the\nposterior by optimization as opposed to sampling from conditionals. Our method\nenables (predictive) inference and uncertainty quantification leveraging AI\npredictions in a coherent probabilistic manner.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advent of Generative Artificial Intelligence (GAI) has heralded an\ninflection point that changed how society thinks about knowledge acquisition.\nWhile GAI cannot be fully trusted for decision-making, it may still provide\nvaluable information that can be integrated into a decision pipeline. Rather\nthan seeing the lack of certitude and inherent randomness of GAI as a problem,\nwe view it as an opportunity. Indeed, variable answers to given prompts can be\nleveraged to construct a prior distribution which reflects assuredness of AI\npredictions. This prior distribution may be combined with tailored datasets for\na fully Bayesian analysis with an AI-driven prior. In this paper, we explore\nsuch a possibility within a non-parametric Bayesian framework. The basic idea\nconsists of assigning a Dirichlet process prior distribution on the\ndata-generating distribution with AI generative model as its baseline.\nHyper-parameters of the prior can be tuned out-of-sample to assess the\ninformativeness of the AI prior. Posterior simulation is achieved by computing\na suitably randomized functional on an augmented data that consists of observed\n(labeled) data as well as fake data whose labels have been imputed using AI.\nThis strategy can be parallelized and rapidly produces iid samples from the\nposterior by optimization as opposed to sampling from conditionals. Our method\nenables (predictive) inference and uncertainty quantification leveraging AI\npredictions in a coherent probabilistic manner."
                },
                "authors": [
                    {
                        "name": "Veronika Ročková"
                    },
                    {
                        "name": "Sean O'Hagan"
                    }
                ],
                "author_detail": {
                    "name": "Sean O'Hagan"
                },
                "author": "Sean O'Hagan",
                "arxiv_comment": "Research note, 27 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19231v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19231v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19230v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19230v1",
                "updated": "2025-02-26T15:41:41Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    15,
                    41,
                    41,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-26T15:41:41Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    15,
                    41,
                    41,
                    2,
                    57,
                    0
                ],
                "title": "Two Heads Are Better Than One: Dual-Model Verbal Reflection at\n  Inference-Time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Two Heads Are Better Than One: Dual-Model Verbal Reflection at\n  Inference-Time"
                },
                "summary": "Large Language Models (LLMs) often struggle with complex reasoning scenarios.\nWhile preference optimization methods enhance reasoning performance through\ntraining, they often lack transparency in why one reasoning outcome is\npreferred over another. Verbal reflection techniques improve explainability but\nare limited in LLMs' critique and refinement capacity. To address these\nchallenges, we introduce a contrastive reflection synthesis pipeline that\nenhances the accuracy and depth of LLM-generated reflections. We further\npropose a dual-model reasoning framework within a verbal reinforcement learning\nparadigm, decoupling inference-time self-reflection into specialized, trained\nmodels for reasoning critique and refinement. Extensive experiments show that\nour framework outperforms traditional preference optimization methods across\nall evaluation metrics. Our findings also show that \"two heads are better than\none\", demonstrating that a collaborative Reasoner-Critic model achieves\nsuperior reasoning performance and transparency, compared to single-model\napproaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) often struggle with complex reasoning scenarios.\nWhile preference optimization methods enhance reasoning performance through\ntraining, they often lack transparency in why one reasoning outcome is\npreferred over another. Verbal reflection techniques improve explainability but\nare limited in LLMs' critique and refinement capacity. To address these\nchallenges, we introduce a contrastive reflection synthesis pipeline that\nenhances the accuracy and depth of LLM-generated reflections. We further\npropose a dual-model reasoning framework within a verbal reinforcement learning\nparadigm, decoupling inference-time self-reflection into specialized, trained\nmodels for reasoning critique and refinement. Extensive experiments show that\nour framework outperforms traditional preference optimization methods across\nall evaluation metrics. Our findings also show that \"two heads are better than\none\", demonstrating that a collaborative Reasoner-Critic model achieves\nsuperior reasoning performance and transparency, compared to single-model\napproaches."
                },
                "authors": [
                    {
                        "name": "Jiazheng Li"
                    },
                    {
                        "name": "Yuxiang Zhou"
                    },
                    {
                        "name": "Junru Lu"
                    },
                    {
                        "name": "Gladys Tyen"
                    },
                    {
                        "name": "Lin Gui"
                    },
                    {
                        "name": "Cesare Aloisi"
                    },
                    {
                        "name": "Yulan He"
                    }
                ],
                "author_detail": {
                    "name": "Yulan He"
                },
                "author": "Yulan He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19230v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19230v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15037v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15037v2",
                "updated": "2025-02-26T15:28:35Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    15,
                    28,
                    35,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-20T20:46:09Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    20,
                    46,
                    9,
                    3,
                    51,
                    0
                ],
                "title": "DEFT: Differentiable Branched Discrete Elastic Rods for Modeling\n  Furcated DLOs in Real-Time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DEFT: Differentiable Branched Discrete Elastic Rods for Modeling\n  Furcated DLOs in Real-Time"
                },
                "summary": "Autonomous wire harness assembly requires robots to manipulate complex\nbranched cables with high precision and reliability. A key challenge in\nautomating this process is predicting how these flexible and branched\nstructures behave under manipulation. Without accurate predictions, it is\ndifficult for robots to reliably plan or execute assembly operations. While\nexisting research has made progress in modeling single-threaded Deformable\nLinear Objects (DLOs), extending these approaches to Branched Deformable Linear\nObjects (BDLOs) presents fundamental challenges. The junction points in BDLOs\ncreate complex force interactions and strain propagation patterns that cannot\nbe adequately captured by simply connecting multiple single-DLO models. To\naddress these challenges, this paper presents Differentiable discrete branched\nElastic rods for modeling Furcated DLOs in real-Time (DEFT), a novel framework\nthat combines a differentiable physics-based model with a learning framework\nto: 1) accurately model BDLO dynamics, including dynamic propagation at\njunction points and grasping in the middle of a BDLO, 2) achieve efficient\ncomputation for real-time inference, and 3) enable planning to demonstrate\ndexterous BDLO manipulation. A comprehensive series of real-world experiments\ndemonstrates DEFT's efficacy in terms of accuracy, computational speed, and\ngeneralizability compared to state-of-the-art alternatives. Project\npage:https://roahmlab.github.io/DEFT/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous wire harness assembly requires robots to manipulate complex\nbranched cables with high precision and reliability. A key challenge in\nautomating this process is predicting how these flexible and branched\nstructures behave under manipulation. Without accurate predictions, it is\ndifficult for robots to reliably plan or execute assembly operations. While\nexisting research has made progress in modeling single-threaded Deformable\nLinear Objects (DLOs), extending these approaches to Branched Deformable Linear\nObjects (BDLOs) presents fundamental challenges. The junction points in BDLOs\ncreate complex force interactions and strain propagation patterns that cannot\nbe adequately captured by simply connecting multiple single-DLO models. To\naddress these challenges, this paper presents Differentiable discrete branched\nElastic rods for modeling Furcated DLOs in real-Time (DEFT), a novel framework\nthat combines a differentiable physics-based model with a learning framework\nto: 1) accurately model BDLO dynamics, including dynamic propagation at\njunction points and grasping in the middle of a BDLO, 2) achieve efficient\ncomputation for real-time inference, and 3) enable planning to demonstrate\ndexterous BDLO manipulation. A comprehensive series of real-world experiments\ndemonstrates DEFT's efficacy in terms of accuracy, computational speed, and\ngeneralizability compared to state-of-the-art alternatives. Project\npage:https://roahmlab.github.io/DEFT/."
                },
                "authors": [
                    {
                        "name": "Yizhou Chen"
                    },
                    {
                        "name": "Xiaoyue Wu"
                    },
                    {
                        "name": "Yeheng Zong"
                    },
                    {
                        "name": "Anran Li"
                    },
                    {
                        "name": "Yuzhen Chen"
                    },
                    {
                        "name": "Julie Wu"
                    },
                    {
                        "name": "Bohao Zhang"
                    },
                    {
                        "name": "Ram Vasudevan"
                    }
                ],
                "author_detail": {
                    "name": "Ram Vasudevan"
                },
                "author": "Ram Vasudevan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15037v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15037v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09248v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09248v2",
                "updated": "2025-02-26T15:27:26Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    15,
                    27,
                    26,
                    2,
                    57,
                    0
                ],
                "published": "2024-11-14T07:28:05Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    7,
                    28,
                    5,
                    3,
                    319,
                    0
                ],
                "title": "Constraint on Lorentz Invariance Violation for spectral lag transition\n  in GRB 160625B using profile likelihood",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Constraint on Lorentz Invariance Violation for spectral lag transition\n  in GRB 160625B using profile likelihood"
                },
                "summary": "We reanalyze the spectral lag data for GRB 160625B using frequentist\ninference in order to constrain the energy scale ($E_{QG}$) of Lorentz\nInvariance Violation (LIV). For this purpose, we use profile likelihood to deal\nwith the astrophysical nuisance parameters. This is in contrast to Bayesian\ninference implemented in previous works, where marginalization was carried out\nover the nuisance parameters. We show that with profile likelihood, we do not\nfind a global minimum for $\\chi^2$ as a function of $E_{QG}$ below the Planck\nscale for both linear and quadratic models of LIV, whereas bounded credible\nintervals were previously obtained using Bayesian inference. Therefore, we can\nset one-sided lower limits in a straightforward manner. We find that $E_{QG}\n\\geq 2.55 \\times 10^{16}$ GeV and $E_{QG} \\geq 1.85 \\times 10^7$ GeV at 95\\%\nc.l., for linear and quadratic LIV, respectively. Therefore, this is the first\nproof-of-principles application of profile likelihood method to the analysis of\nGRB spectral lag data to constrain LIV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We reanalyze the spectral lag data for GRB 160625B using frequentist\ninference in order to constrain the energy scale ($E_{QG}$) of Lorentz\nInvariance Violation (LIV). For this purpose, we use profile likelihood to deal\nwith the astrophysical nuisance parameters. This is in contrast to Bayesian\ninference implemented in previous works, where marginalization was carried out\nover the nuisance parameters. We show that with profile likelihood, we do not\nfind a global minimum for $\\chi^2$ as a function of $E_{QG}$ below the Planck\nscale for both linear and quadratic models of LIV, whereas bounded credible\nintervals were previously obtained using Bayesian inference. Therefore, we can\nset one-sided lower limits in a straightforward manner. We find that $E_{QG}\n\\geq 2.55 \\times 10^{16}$ GeV and $E_{QG} \\geq 1.85 \\times 10^7$ GeV at 95\\%\nc.l., for linear and quadratic LIV, respectively. Therefore, this is the first\nproof-of-principles application of profile likelihood method to the analysis of\nGRB spectral lag data to constrain LIV."
                },
                "authors": [
                    {
                        "name": "Shantanu Desai"
                    },
                    {
                        "name": "Shalini Ganguly"
                    }
                ],
                "author_detail": {
                    "name": "Shalini Ganguly"
                },
                "author": "Shalini Ganguly",
                "arxiv_comment": "4 pages, 2 figures. Accepted for publication in EPJC",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09248v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09248v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19216v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19216v1",
                "updated": "2025-02-26T15:18:17Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    15,
                    18,
                    17,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-26T15:18:17Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    15,
                    18,
                    17,
                    2,
                    57,
                    0
                ],
                "title": "Utility-Based Dose Optimization Approaches for Multiple-Dose Randomized\n  Trial Designs Accounting for Multiple Endpoints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Utility-Based Dose Optimization Approaches for Multiple-Dose Randomized\n  Trial Designs Accounting for Multiple Endpoints"
                },
                "summary": "The initiation of dose optimization has driven a paradigm shift in oncology\nclinical trials to determine the optimal biological dose (OBD). Early-phase\ntrials with randomized doses can facilitate additional investigation of the\nidentified OBD in targeted populations by incorporating safety, efficacy, and\nbiomarker data. To support dose comparison in such settings, we propose to\nextend the utility score-based approach (U-MET) and introduce the clinical\nutility index-based approach (CUI-MET) to account for multiple endpoints and\ndoses. The utility-based dose optimization approach for multiple-dose\nrandomized trial designs accounting for multiple endpoints and doses (U-MET-m)\nextends the U-MET, using a utility score to account for multiple endpoints\njointly (e.g., toxicity-efficacy trade-off), while the CUI-MET uses a utility\nindex to do this marginally. U-MET-m and CUI-MET use Bayesian inference within\na hypothesis framework to compare utility metrics across doses to identify the\nOBD. Here we describe simulation studies and present an example to compare the\nU-MET-m design, CUI-MET, and empirical design. The U-MET-m design and CUI-MET\nwere shown to have satisfactory operating characteristics for selecting the\nOBD. Based on these findings, we recommend using the U-MET-m and CUI-MET\ndesigns as the primary dose comparison approach or as supportive evidence to\nselect the OBD.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The initiation of dose optimization has driven a paradigm shift in oncology\nclinical trials to determine the optimal biological dose (OBD). Early-phase\ntrials with randomized doses can facilitate additional investigation of the\nidentified OBD in targeted populations by incorporating safety, efficacy, and\nbiomarker data. To support dose comparison in such settings, we propose to\nextend the utility score-based approach (U-MET) and introduce the clinical\nutility index-based approach (CUI-MET) to account for multiple endpoints and\ndoses. The utility-based dose optimization approach for multiple-dose\nrandomized trial designs accounting for multiple endpoints and doses (U-MET-m)\nextends the U-MET, using a utility score to account for multiple endpoints\njointly (e.g., toxicity-efficacy trade-off), while the CUI-MET uses a utility\nindex to do this marginally. U-MET-m and CUI-MET use Bayesian inference within\na hypothesis framework to compare utility metrics across doses to identify the\nOBD. Here we describe simulation studies and present an example to compare the\nU-MET-m design, CUI-MET, and empirical design. The U-MET-m design and CUI-MET\nwere shown to have satisfactory operating characteristics for selecting the\nOBD. Based on these findings, we recommend using the U-MET-m and CUI-MET\ndesigns as the primary dose comparison approach or as supportive evidence to\nselect the OBD."
                },
                "authors": [
                    {
                        "name": "Gina DAngelo"
                    },
                    {
                        "name": "Guannan Chen"
                    },
                    {
                        "name": "Di Ran"
                    }
                ],
                "author_detail": {
                    "name": "Di Ran"
                },
                "author": "Di Ran",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19216v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19216v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.07978v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.07978v3",
                "updated": "2025-02-26T15:16:47Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    15,
                    16,
                    47,
                    2,
                    57,
                    0
                ],
                "published": "2023-11-14T08:10:14Z",
                "published_parsed": [
                    2023,
                    11,
                    14,
                    8,
                    10,
                    14,
                    1,
                    318,
                    0
                ],
                "title": "AfroBench: How Good are Large Language Models on African Languages?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AfroBench: How Good are Large Language Models on African Languages?"
                },
                "summary": "Large-scale multilingual evaluations, such as MEGA, often include only a\nhandful of African languages due to the scarcity of high-quality evaluation\ndata and the limited discoverability of existing African datasets. This lack of\nrepresentation hinders comprehensive LLM evaluation across a diverse range of\nlanguages and tasks. To address these challenges, we introduce AfroBench -- a\nmulti-task benchmark for evaluating the performance of LLMs across 64 African\nlanguages, 15 tasks and 22 datasets. AfroBench consists of nine natural\nlanguage understanding datasets, six text generation datasets, six knowledge\nand question answering tasks, and one mathematical reasoning task. We present\nresults comparing the performance of prompting LLMs to fine-tuned baselines\nbased on BERT and T5-style models. Our results suggest large gaps in\nperformance between high-resource languages, such as English, and African\nlanguages across most tasks; but performance also varies based on the\navailability of monolingual data resources. Our findings confirm that\nperformance on African languages continues to remain a hurdle for current LLMs,\nunderscoring the need for additional efforts to close this gap.\n  https://mcgill-nlp.github.io/AfroBench/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-scale multilingual evaluations, such as MEGA, often include only a\nhandful of African languages due to the scarcity of high-quality evaluation\ndata and the limited discoverability of existing African datasets. This lack of\nrepresentation hinders comprehensive LLM evaluation across a diverse range of\nlanguages and tasks. To address these challenges, we introduce AfroBench -- a\nmulti-task benchmark for evaluating the performance of LLMs across 64 African\nlanguages, 15 tasks and 22 datasets. AfroBench consists of nine natural\nlanguage understanding datasets, six text generation datasets, six knowledge\nand question answering tasks, and one mathematical reasoning task. We present\nresults comparing the performance of prompting LLMs to fine-tuned baselines\nbased on BERT and T5-style models. Our results suggest large gaps in\nperformance between high-resource languages, such as English, and African\nlanguages across most tasks; but performance also varies based on the\navailability of monolingual data resources. Our findings confirm that\nperformance on African languages continues to remain a hurdle for current LLMs,\nunderscoring the need for additional efforts to close this gap.\n  https://mcgill-nlp.github.io/AfroBench/"
                },
                "authors": [
                    {
                        "name": "Jessica Ojo"
                    },
                    {
                        "name": "Odunayo Ogundepo"
                    },
                    {
                        "name": "Akintunde Oladipo"
                    },
                    {
                        "name": "Kelechi Ogueji"
                    },
                    {
                        "name": "Jimmy Lin"
                    },
                    {
                        "name": "Pontus Stenetorp"
                    },
                    {
                        "name": "David Ifeoluwa Adelani"
                    }
                ],
                "author_detail": {
                    "name": "David Ifeoluwa Adelani"
                },
                "author": "David Ifeoluwa Adelani",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.07978v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.07978v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19214v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19214v1",
                "updated": "2025-02-26T15:15:01Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    15,
                    15,
                    1,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-26T15:15:01Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    15,
                    15,
                    1,
                    2,
                    57,
                    0
                ],
                "title": "A Hybrid Transformer Architecture with a Quantized Self-Attention\n  Mechanism Applied to Molecular Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Hybrid Transformer Architecture with a Quantized Self-Attention\n  Mechanism Applied to Molecular Generation"
                },
                "summary": "The success of the self-attention mechanism in classical machine learning\nmodels has inspired the development of quantum analogs aimed at reducing\ncomputational overhead. Self-attention integrates learnable query and key\nmatrices to calculate attention scores between all pairs of tokens in a\nsequence. These scores are then multiplied by a learnable value matrix to\nobtain the output self-attention matrix, enabling the model to effectively\ncapture long-range dependencies within the input sequence. Here, we propose a\nhybrid quantum-classical self-attention mechanism as part of a transformer\ndecoder, the architecture underlying large language models (LLMs). To\ndemonstrate its utility in chemistry, we train this model on the QM9 dataset\nfor conditional generation, using SMILES strings as input, each labeled with a\nset of physicochemical properties that serve as conditions during inference.\nOur theoretical analysis shows that the time complexity of the query-key dot\nproduct is reduced from $\\mathcal{O}(n^2 d)$ in a classical model to\n$\\mathcal{O}(n^2\\log d)$ in our quantum model, where $n$ and $d$ represent the\nsequence length and embedding dimension, respectively. We perform simulations\nusing NVIDIA's CUDA-Q platform, which is designed for efficient GPU\nscalability. This work provides a promising avenue for quantum-enhanced natural\nlanguage processing (NLP).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The success of the self-attention mechanism in classical machine learning\nmodels has inspired the development of quantum analogs aimed at reducing\ncomputational overhead. Self-attention integrates learnable query and key\nmatrices to calculate attention scores between all pairs of tokens in a\nsequence. These scores are then multiplied by a learnable value matrix to\nobtain the output self-attention matrix, enabling the model to effectively\ncapture long-range dependencies within the input sequence. Here, we propose a\nhybrid quantum-classical self-attention mechanism as part of a transformer\ndecoder, the architecture underlying large language models (LLMs). To\ndemonstrate its utility in chemistry, we train this model on the QM9 dataset\nfor conditional generation, using SMILES strings as input, each labeled with a\nset of physicochemical properties that serve as conditions during inference.\nOur theoretical analysis shows that the time complexity of the query-key dot\nproduct is reduced from $\\mathcal{O}(n^2 d)$ in a classical model to\n$\\mathcal{O}(n^2\\log d)$ in our quantum model, where $n$ and $d$ represent the\nsequence length and embedding dimension, respectively. We perform simulations\nusing NVIDIA's CUDA-Q platform, which is designed for efficient GPU\nscalability. This work provides a promising avenue for quantum-enhanced natural\nlanguage processing (NLP)."
                },
                "authors": [
                    {
                        "name": "Anthony M. Smaldone"
                    },
                    {
                        "name": "Yu Shee"
                    },
                    {
                        "name": "Gregory W. Kyro"
                    },
                    {
                        "name": "Marwa H. Farag"
                    },
                    {
                        "name": "Zohim Chandani"
                    },
                    {
                        "name": "Elica Kyoseva"
                    },
                    {
                        "name": "Victor S. Batista"
                    }
                ],
                "author_detail": {
                    "name": "Victor S. Batista"
                },
                "author": "Victor S. Batista",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19214v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19214v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19211v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19211v1",
                "updated": "2025-02-26T15:13:20Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    15,
                    13,
                    20,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-26T15:13:20Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    15,
                    13,
                    20,
                    2,
                    57,
                    0
                ],
                "title": "Negation-Induced Forgetting in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Negation-Induced Forgetting in LLMs"
                },
                "summary": "The study explores whether Large Language Models (LLMs) exhibit\nnegation-induced forgetting (NIF), a cognitive phenomenon observed in humans\nwhere negating incorrect attributes of an object or event leads to diminished\nrecall of this object or event compared to affirming correct attributes (Mayo\net al., 2014; Zang et al., 2023). We adapted Zang et al. (2023) experimental\nframework to test this effect in ChatGPT-3.5, GPT-4o mini and\nLlama3-70b-instruct. Our results show that ChatGPT-3.5 exhibits NIF, with\nnegated information being less likely to be recalled than affirmed information.\nGPT-4o-mini showed a marginally significant NIF effect, while LLaMA-3-70B did\nnot exhibit NIF. The findings provide initial evidence of negation-induced\nforgetting in some LLMs, suggesting that similar cognitive biases may emerge in\nthese models. This work is a preliminary step in understanding how\nmemory-related phenomena manifest in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The study explores whether Large Language Models (LLMs) exhibit\nnegation-induced forgetting (NIF), a cognitive phenomenon observed in humans\nwhere negating incorrect attributes of an object or event leads to diminished\nrecall of this object or event compared to affirming correct attributes (Mayo\net al., 2014; Zang et al., 2023). We adapted Zang et al. (2023) experimental\nframework to test this effect in ChatGPT-3.5, GPT-4o mini and\nLlama3-70b-instruct. Our results show that ChatGPT-3.5 exhibits NIF, with\nnegated information being less likely to be recalled than affirmed information.\nGPT-4o-mini showed a marginally significant NIF effect, while LLaMA-3-70B did\nnot exhibit NIF. The findings provide initial evidence of negation-induced\nforgetting in some LLMs, suggesting that similar cognitive biases may emerge in\nthese models. This work is a preliminary step in understanding how\nmemory-related phenomena manifest in LLMs."
                },
                "authors": [
                    {
                        "name": "Francesca Capuano"
                    },
                    {
                        "name": "Ellen Boschert"
                    },
                    {
                        "name": "Barbara Kaup"
                    }
                ],
                "author_detail": {
                    "name": "Barbara Kaup"
                },
                "author": "Barbara Kaup",
                "arxiv_comment": "ISCA/ITG Workshop on Diversity in Large Speech and Language Models",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19211v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19211v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19209v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19209v1",
                "updated": "2025-02-26T15:12:59Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    15,
                    12,
                    59,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-26T15:12:59Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    15,
                    12,
                    59,
                    2,
                    57,
                    0
                ],
                "title": "Bi'an: A Bilingual Benchmark and Model for Hallucination Detection in\n  Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bi'an: A Bilingual Benchmark and Model for Hallucination Detection in\n  Retrieval-Augmented Generation"
                },
                "summary": "Retrieval-Augmented Generation (RAG) effectively reduces hallucinations in\nLarge Language Models (LLMs) but can still produce inconsistent or unsupported\ncontent. Although LLM-as-a-Judge is widely used for RAG hallucination detection\ndue to its implementation simplicity, it faces two main challenges: the absence\nof comprehensive evaluation benchmarks and the lack of domain-optimized judge\nmodels. To bridge these gaps, we introduce \\textbf{Bi'an}, a novel framework\nfeaturing a bilingual benchmark dataset and lightweight judge models. The\ndataset supports rigorous evaluation across multiple RAG scenarios, while the\njudge models are fine-tuned from compact open-source LLMs. Extensive\nexperimental evaluations on Bi'anBench show our 14B model outperforms baseline\nmodels with over five times larger parameter scales and rivals state-of-the-art\nclosed-source LLMs. We will release our data and models soon at\nhttps://github.com/OpenSPG/KAG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) effectively reduces hallucinations in\nLarge Language Models (LLMs) but can still produce inconsistent or unsupported\ncontent. Although LLM-as-a-Judge is widely used for RAG hallucination detection\ndue to its implementation simplicity, it faces two main challenges: the absence\nof comprehensive evaluation benchmarks and the lack of domain-optimized judge\nmodels. To bridge these gaps, we introduce \\textbf{Bi'an}, a novel framework\nfeaturing a bilingual benchmark dataset and lightweight judge models. The\ndataset supports rigorous evaluation across multiple RAG scenarios, while the\njudge models are fine-tuned from compact open-source LLMs. Extensive\nexperimental evaluations on Bi'anBench show our 14B model outperforms baseline\nmodels with over five times larger parameter scales and rivals state-of-the-art\nclosed-source LLMs. We will release our data and models soon at\nhttps://github.com/OpenSPG/KAG."
                },
                "authors": [
                    {
                        "name": "Zhouyu Jiang"
                    },
                    {
                        "name": "Mengshu Sun"
                    },
                    {
                        "name": "Zhiqiang Zhang"
                    },
                    {
                        "name": "Lei Liang"
                    }
                ],
                "author_detail": {
                    "name": "Lei Liang"
                },
                "author": "Lei Liang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19209v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19209v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.16749v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.16749v5",
                "updated": "2025-02-26T15:02:29Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    15,
                    2,
                    29,
                    2,
                    57,
                    0
                ],
                "published": "2024-06-24T15:57:49Z",
                "published_parsed": [
                    2024,
                    6,
                    24,
                    15,
                    57,
                    49,
                    0,
                    176,
                    0
                ],
                "title": "Inferring stochastic low-rank recurrent neural networks from neural data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inferring stochastic low-rank recurrent neural networks from neural data"
                },
                "summary": "A central aim in computational neuroscience is to relate the activity of\nlarge populations of neurons to an underlying dynamical system. Models of these\nneural dynamics should ideally be both interpretable and fit the observed data\nwell. Low-rank recurrent neural networks (RNNs) exhibit such interpretability\nby having tractable dynamics. However, it is unclear how to best fit low-rank\nRNNs to data consisting of noisy observations of an underlying stochastic\nsystem. Here, we propose to fit stochastic low-rank RNNs with variational\nsequential Monte Carlo methods. We validate our method on several datasets\nconsisting of both continuous and spiking neural data, where we obtain lower\ndimensional latent dynamics than current state of the art methods.\nAdditionally, for low-rank models with piecewise linear nonlinearities, we show\nhow to efficiently identify all fixed points in polynomial rather than\nexponential cost in the number of units, making analysis of the inferred\ndynamics tractable for large RNNs. Our method both elucidates the dynamical\nsystems underlying experimental recordings and provides a generative model\nwhose trajectories match observed variability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A central aim in computational neuroscience is to relate the activity of\nlarge populations of neurons to an underlying dynamical system. Models of these\nneural dynamics should ideally be both interpretable and fit the observed data\nwell. Low-rank recurrent neural networks (RNNs) exhibit such interpretability\nby having tractable dynamics. However, it is unclear how to best fit low-rank\nRNNs to data consisting of noisy observations of an underlying stochastic\nsystem. Here, we propose to fit stochastic low-rank RNNs with variational\nsequential Monte Carlo methods. We validate our method on several datasets\nconsisting of both continuous and spiking neural data, where we obtain lower\ndimensional latent dynamics than current state of the art methods.\nAdditionally, for low-rank models with piecewise linear nonlinearities, we show\nhow to efficiently identify all fixed points in polynomial rather than\nexponential cost in the number of units, making analysis of the inferred\ndynamics tractable for large RNNs. Our method both elucidates the dynamical\nsystems underlying experimental recordings and provides a generative model\nwhose trajectories match observed variability."
                },
                "authors": [
                    {
                        "name": "Matthijs Pals"
                    },
                    {
                        "name": "A Erdem Sağtekin"
                    },
                    {
                        "name": "Felix Pei"
                    },
                    {
                        "name": "Manuel Gloeckler"
                    },
                    {
                        "name": "Jakob H Macke"
                    }
                ],
                "author_detail": {
                    "name": "Jakob H Macke"
                },
                "author": "Jakob H Macke",
                "arxiv_journal_ref": "The Thirty-eighth Annual Conference on Neural Information\n  Processing Systems (NeurIPS) 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.16749v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.16749v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19193v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19193v1",
                "updated": "2025-02-26T14:59:27Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    14,
                    59,
                    27,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-26T14:59:27Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    14,
                    59,
                    27,
                    2,
                    57,
                    0
                ],
                "title": "Simulation of Language Evolution under Regulated Social Media Platforms:\n  A Synergistic Approach of Large Language Models and Genetic Algorithms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulation of Language Evolution under Regulated Social Media Platforms:\n  A Synergistic Approach of Large Language Models and Genetic Algorithms"
                },
                "summary": "Social media platforms frequently impose restrictive policies to moderate\nuser content, prompting the emergence of creative evasion language strategies.\nThis paper presents a multi-agent framework based on Large Language Models\n(LLMs) to simulate the iterative evolution of language strategies under\nregulatory constraints. In this framework, participant agents, as social media\nusers, continuously evolve their language expression, while supervisory agents\nemulate platform-level regulation by assessing policy violations. To achieve a\nmore faithful simulation, we employ a dual design of language strategies\n(constraint and expression) to differentiate conflicting goals and utilize an\nLLM-driven GA (Genetic Algorithm) for the selection, mutation, and crossover of\nlanguage strategies. The framework is evaluated using two distinct scenarios:\nan abstract password game and a realistic simulated illegal pet trade scenario.\nExperimental results demonstrate that as the number of dialogue rounds\nincreases, both the number of uninterrupted dialogue turns and the accuracy of\ninformation transmission improve significantly. Furthermore, a user study with\n40 participants validates the real-world relevance of the generated dialogues\nand strategies. Moreover, ablation studies validate the importance of the GA,\nemphasizing its contribution to long-term adaptability and improved overall\nresults.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Social media platforms frequently impose restrictive policies to moderate\nuser content, prompting the emergence of creative evasion language strategies.\nThis paper presents a multi-agent framework based on Large Language Models\n(LLMs) to simulate the iterative evolution of language strategies under\nregulatory constraints. In this framework, participant agents, as social media\nusers, continuously evolve their language expression, while supervisory agents\nemulate platform-level regulation by assessing policy violations. To achieve a\nmore faithful simulation, we employ a dual design of language strategies\n(constraint and expression) to differentiate conflicting goals and utilize an\nLLM-driven GA (Genetic Algorithm) for the selection, mutation, and crossover of\nlanguage strategies. The framework is evaluated using two distinct scenarios:\nan abstract password game and a realistic simulated illegal pet trade scenario.\nExperimental results demonstrate that as the number of dialogue rounds\nincreases, both the number of uninterrupted dialogue turns and the accuracy of\ninformation transmission improve significantly. Furthermore, a user study with\n40 participants validates the real-world relevance of the generated dialogues\nand strategies. Moreover, ablation studies validate the importance of the GA,\nemphasizing its contribution to long-term adaptability and improved overall\nresults."
                },
                "authors": [
                    {
                        "name": "Jinyu Cai"
                    },
                    {
                        "name": "Yusei Ishimizu"
                    },
                    {
                        "name": "Mingyue Zhang"
                    },
                    {
                        "name": "Munan Li"
                    },
                    {
                        "name": "Jialong Li"
                    },
                    {
                        "name": "Kenji Tei"
                    }
                ],
                "author_detail": {
                    "name": "Kenji Tei"
                },
                "author": "Kenji Tei",
                "arxiv_comment": "The manuscript has been submitted to IEEE Transactions on\n  Computational Social Systems",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19193v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19193v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.07475v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.07475v3",
                "updated": "2025-02-26T14:58:39Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    14,
                    58,
                    39,
                    2,
                    57,
                    0
                ],
                "published": "2024-06-11T17:21:15Z",
                "published_parsed": [
                    2024,
                    6,
                    11,
                    17,
                    21,
                    15,
                    1,
                    163,
                    0
                ],
                "title": "Partially Observed Trajectory Inference using Optimal Transport and a\n  Dynamics Prior",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Partially Observed Trajectory Inference using Optimal Transport and a\n  Dynamics Prior"
                },
                "summary": "Trajectory inference seeks to recover the temporal dynamics of a population\nfrom snapshots of its (uncoupled) temporal marginals, i.e. where observed\nparticles are not tracked over time. Prior works addressed this challenging\nproblem under a stochastic differential equation (SDE) model with a\ngradient-driven drift in the observed space, introducing a minimum entropy\nestimator relative to the Wiener measure and a practical grid-free mean-field\nLangevin (MFL) algorithm using Schr\\\"odinger bridges. Motivated by the success\nof observable state space models in the traditional paired trajectory inference\nproblem (e.g. target tracking), we extend the above framework to a class of\nlatent SDEs in the form of observable state space models. In this setting, we\nuse partial observations to infer trajectories in the latent space under a\nspecified dynamics model (e.g. the constant velocity/acceleration models from\ntarget tracking). We introduce the PO-MFL algorithm to solve this latent\ntrajectory inference problem and provide theoretical guarantees to the\npartially observed setting. Experiments validate the robustness of our method\nand the exponential convergence of the MFL dynamics, and demonstrate\nsignificant outperformance over the latent-free baseline in key scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trajectory inference seeks to recover the temporal dynamics of a population\nfrom snapshots of its (uncoupled) temporal marginals, i.e. where observed\nparticles are not tracked over time. Prior works addressed this challenging\nproblem under a stochastic differential equation (SDE) model with a\ngradient-driven drift in the observed space, introducing a minimum entropy\nestimator relative to the Wiener measure and a practical grid-free mean-field\nLangevin (MFL) algorithm using Schr\\\"odinger bridges. Motivated by the success\nof observable state space models in the traditional paired trajectory inference\nproblem (e.g. target tracking), we extend the above framework to a class of\nlatent SDEs in the form of observable state space models. In this setting, we\nuse partial observations to infer trajectories in the latent space under a\nspecified dynamics model (e.g. the constant velocity/acceleration models from\ntarget tracking). We introduce the PO-MFL algorithm to solve this latent\ntrajectory inference problem and provide theoretical guarantees to the\npartially observed setting. Experiments validate the robustness of our method\nand the exponential convergence of the MFL dynamics, and demonstrate\nsignificant outperformance over the latent-free baseline in key scenarios."
                },
                "authors": [
                    {
                        "name": "Anming Gu"
                    },
                    {
                        "name": "Edward Chien"
                    },
                    {
                        "name": "Kristjan Greenewald"
                    }
                ],
                "author_detail": {
                    "name": "Kristjan Greenewald"
                },
                "author": "Kristjan Greenewald",
                "arxiv_comment": "ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.07475v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.07475v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19187v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19187v1",
                "updated": "2025-02-26T14:50:50Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    14,
                    50,
                    50,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-26T14:50:50Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    14,
                    50,
                    50,
                    2,
                    57,
                    0
                ],
                "title": "BIG-Bench Extra Hard",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BIG-Bench Extra Hard"
                },
                "summary": "Large language models (LLMs) are increasingly deployed in everyday\napplications, demanding robust general reasoning capabilities and diverse\nreasoning skillset. However, current LLM reasoning benchmarks predominantly\nfocus on mathematical and coding abilities, leaving a gap in evaluating broader\nreasoning proficiencies. One particular exception is the BIG-Bench dataset,\nwhich has served as a crucial benchmark for evaluating the general reasoning\ncapabilities of LLMs, thanks to its diverse set of challenging tasks that\nallowed for a comprehensive assessment of general reasoning across various\nskills within a unified framework. However, recent advances in LLMs have led to\nsaturation on BIG-Bench, and its harder version BIG-Bench Hard (BBH).\nState-of-the-art models achieve near-perfect scores on many tasks in BBH, thus\ndiminishing its utility. To address this limitation, we introduce BIG-Bench\nExtra Hard (BBEH), a new benchmark designed to push the boundaries of LLM\nreasoning evaluation. BBEH replaces each task in BBH with a novel task that\nprobes a similar reasoning capability but exhibits significantly increased\ndifficulty. We evaluate various models on BBEH and observe a (harmonic) average\naccuracy of 9.8\\% for the best general-purpose model and 44.8\\% for the best\nreasoning-specialized model, indicating substantial room for improvement and\nhighlighting the ongoing challenge of achieving robust general reasoning in\nLLMs. We release BBEH publicly at: https://github.com/google-deepmind/bbeh.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly deployed in everyday\napplications, demanding robust general reasoning capabilities and diverse\nreasoning skillset. However, current LLM reasoning benchmarks predominantly\nfocus on mathematical and coding abilities, leaving a gap in evaluating broader\nreasoning proficiencies. One particular exception is the BIG-Bench dataset,\nwhich has served as a crucial benchmark for evaluating the general reasoning\ncapabilities of LLMs, thanks to its diverse set of challenging tasks that\nallowed for a comprehensive assessment of general reasoning across various\nskills within a unified framework. However, recent advances in LLMs have led to\nsaturation on BIG-Bench, and its harder version BIG-Bench Hard (BBH).\nState-of-the-art models achieve near-perfect scores on many tasks in BBH, thus\ndiminishing its utility. To address this limitation, we introduce BIG-Bench\nExtra Hard (BBEH), a new benchmark designed to push the boundaries of LLM\nreasoning evaluation. BBEH replaces each task in BBH with a novel task that\nprobes a similar reasoning capability but exhibits significantly increased\ndifficulty. We evaluate various models on BBEH and observe a (harmonic) average\naccuracy of 9.8\\% for the best general-purpose model and 44.8\\% for the best\nreasoning-specialized model, indicating substantial room for improvement and\nhighlighting the ongoing challenge of achieving robust general reasoning in\nLLMs. We release BBEH publicly at: https://github.com/google-deepmind/bbeh."
                },
                "authors": [
                    {
                        "name": "Mehran Kazemi"
                    },
                    {
                        "name": "Bahare Fatemi"
                    },
                    {
                        "name": "Hritik Bansal"
                    },
                    {
                        "name": "John Palowitch"
                    },
                    {
                        "name": "Chrysovalantis Anastasiou"
                    },
                    {
                        "name": "Sanket Vaibhav Mehta"
                    },
                    {
                        "name": "Lalit K. Jain"
                    },
                    {
                        "name": "Virginia Aglietti"
                    },
                    {
                        "name": "Disha Jindal"
                    },
                    {
                        "name": "Peter Chen"
                    },
                    {
                        "name": "Nishanth Dikkala"
                    },
                    {
                        "name": "Gladys Tyen"
                    },
                    {
                        "name": "Xin Liu"
                    },
                    {
                        "name": "Uri Shalit"
                    },
                    {
                        "name": "Silvia Chiappa"
                    },
                    {
                        "name": "Kate Olszewska"
                    },
                    {
                        "name": "Yi Tay"
                    },
                    {
                        "name": "Vinh Q. Tran"
                    },
                    {
                        "name": "Quoc V. Le"
                    },
                    {
                        "name": "Orhan Firat"
                    }
                ],
                "author_detail": {
                    "name": "Orhan Firat"
                },
                "author": "Orhan Firat",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19187v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19187v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19178v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19178v1",
                "updated": "2025-02-26T14:34:00Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    14,
                    34,
                    0,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-26T14:34:00Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    14,
                    34,
                    0,
                    2,
                    57,
                    0
                ],
                "title": "UQABench: Evaluating User Embedding for Prompting LLMs in Personalized\n  Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UQABench: Evaluating User Embedding for Prompting LLMs in Personalized\n  Question Answering"
                },
                "summary": "Large language models (LLMs) achieve remarkable success in natural language\nprocessing (NLP). In practical scenarios like recommendations, as users\nincreasingly seek personalized experiences, it becomes crucial to incorporate\nuser interaction history into the context of LLMs to enhance personalization.\nHowever, from a practical utility perspective, user interactions' extensive\nlength and noise present challenges when used directly as text prompts. A\npromising solution is to compress and distill interactions into compact\nembeddings, serving as soft prompts to assist LLMs in generating personalized\nresponses. Although this approach brings efficiency, a critical concern\nemerges: Can user embeddings adequately capture valuable information and prompt\nLLMs? To address this concern, we propose \\name, a benchmark designed to\nevaluate the effectiveness of user embeddings in prompting LLMs for\npersonalization. We establish a fair and standardized evaluation process,\nencompassing pre-training, fine-tuning, and evaluation stages. To thoroughly\nevaluate user embeddings, we design three dimensions of tasks: sequence\nunderstanding, action prediction, and interest perception. These evaluation\ntasks cover the industry's demands in traditional recommendation tasks, such as\nimproving prediction accuracy, and its aspirations for LLM-based methods, such\nas accurately understanding user interests and enhancing the user experience.\nWe conduct extensive experiments on various state-of-the-art methods for\nmodeling user embeddings. Additionally, we reveal the scaling laws of\nleveraging user embeddings to prompt LLMs. The benchmark is available online.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) achieve remarkable success in natural language\nprocessing (NLP). In practical scenarios like recommendations, as users\nincreasingly seek personalized experiences, it becomes crucial to incorporate\nuser interaction history into the context of LLMs to enhance personalization.\nHowever, from a practical utility perspective, user interactions' extensive\nlength and noise present challenges when used directly as text prompts. A\npromising solution is to compress and distill interactions into compact\nembeddings, serving as soft prompts to assist LLMs in generating personalized\nresponses. Although this approach brings efficiency, a critical concern\nemerges: Can user embeddings adequately capture valuable information and prompt\nLLMs? To address this concern, we propose \\name, a benchmark designed to\nevaluate the effectiveness of user embeddings in prompting LLMs for\npersonalization. We establish a fair and standardized evaluation process,\nencompassing pre-training, fine-tuning, and evaluation stages. To thoroughly\nevaluate user embeddings, we design three dimensions of tasks: sequence\nunderstanding, action prediction, and interest perception. These evaluation\ntasks cover the industry's demands in traditional recommendation tasks, such as\nimproving prediction accuracy, and its aspirations for LLM-based methods, such\nas accurately understanding user interests and enhancing the user experience.\nWe conduct extensive experiments on various state-of-the-art methods for\nmodeling user embeddings. Additionally, we reveal the scaling laws of\nleveraging user embeddings to prompt LLMs. The benchmark is available online."
                },
                "authors": [
                    {
                        "name": "Langming Liu"
                    },
                    {
                        "name": "Shilei Liu"
                    },
                    {
                        "name": "Yujin Yuan"
                    },
                    {
                        "name": "Yizhen Zhang"
                    },
                    {
                        "name": "Bencheng Yan"
                    },
                    {
                        "name": "Zhiyuan Zeng"
                    },
                    {
                        "name": "Zihao Wang"
                    },
                    {
                        "name": "Jiaqi Liu"
                    },
                    {
                        "name": "Di Wang"
                    },
                    {
                        "name": "Wenbo Su"
                    },
                    {
                        "name": "Pengjie Wang"
                    },
                    {
                        "name": "Jian Xu"
                    },
                    {
                        "name": "Bo Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Bo Zheng"
                },
                "author": "Bo Zheng",
                "arxiv_comment": "10 pages, 3 figures, 7 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19178v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19178v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19175v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19175v1",
                "updated": "2025-02-26T14:31:43Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    14,
                    31,
                    43,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-26T14:31:43Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    14,
                    31,
                    43,
                    2,
                    57,
                    0
                ],
                "title": "MEDDxAgent: A Unified Modular Agent Framework for Explainable Automatic\n  Differential Diagnosis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MEDDxAgent: A Unified Modular Agent Framework for Explainable Automatic\n  Differential Diagnosis"
                },
                "summary": "Differential Diagnosis (DDx) is a fundamental yet complex aspect of clinical\ndecision-making, in which physicians iteratively refine a ranked list of\npossible diseases based on symptoms, antecedents, and medical knowledge. While\nrecent advances in large language models have shown promise in supporting DDx,\nexisting approaches face key limitations, including single-dataset evaluations,\nisolated optimization of components, unrealistic assumptions about complete\npatient profiles, and single-attempt diagnosis. We introduce a Modular\nExplainable DDx Agent (MEDDxAgent) framework designed for interactive DDx,\nwhere diagnostic reasoning evolves through iterative learning, rather than\nassuming a complete patient profile is accessible. MEDDxAgent integrates three\nmodular components: (1) an orchestrator (DDxDriver), (2) a history taking\nsimulator, and (3) two specialized agents for knowledge retrieval and diagnosis\nstrategy. To ensure robust evaluation, we introduce a comprehensive DDx\nbenchmark covering respiratory, skin, and rare diseases. We analyze single-turn\ndiagnostic approaches and demonstrate the importance of iterative refinement\nwhen patient profiles are not available at the outset. Our broad evaluation\ndemonstrates that MEDDxAgent achieves over 10% accuracy improvements in\ninteractive DDx across both large and small LLMs, while offering critical\nexplainability into its diagnostic reasoning process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Differential Diagnosis (DDx) is a fundamental yet complex aspect of clinical\ndecision-making, in which physicians iteratively refine a ranked list of\npossible diseases based on symptoms, antecedents, and medical knowledge. While\nrecent advances in large language models have shown promise in supporting DDx,\nexisting approaches face key limitations, including single-dataset evaluations,\nisolated optimization of components, unrealistic assumptions about complete\npatient profiles, and single-attempt diagnosis. We introduce a Modular\nExplainable DDx Agent (MEDDxAgent) framework designed for interactive DDx,\nwhere diagnostic reasoning evolves through iterative learning, rather than\nassuming a complete patient profile is accessible. MEDDxAgent integrates three\nmodular components: (1) an orchestrator (DDxDriver), (2) a history taking\nsimulator, and (3) two specialized agents for knowledge retrieval and diagnosis\nstrategy. To ensure robust evaluation, we introduce a comprehensive DDx\nbenchmark covering respiratory, skin, and rare diseases. We analyze single-turn\ndiagnostic approaches and demonstrate the importance of iterative refinement\nwhen patient profiles are not available at the outset. Our broad evaluation\ndemonstrates that MEDDxAgent achieves over 10% accuracy improvements in\ninteractive DDx across both large and small LLMs, while offering critical\nexplainability into its diagnostic reasoning process."
                },
                "authors": [
                    {
                        "name": "Daniel Rose"
                    },
                    {
                        "name": "Chia-Chien Hung"
                    },
                    {
                        "name": "Marco Lepri"
                    },
                    {
                        "name": "Israa Alqassem"
                    },
                    {
                        "name": "Kiril Gashteovski"
                    },
                    {
                        "name": "Carolin Lawrence"
                    }
                ],
                "author_detail": {
                    "name": "Carolin Lawrence"
                },
                "author": "Carolin Lawrence",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19175v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19175v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04677v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04677v4",
                "updated": "2025-02-26T14:29:20Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    14,
                    29,
                    20,
                    2,
                    57,
                    0
                ],
                "published": "2024-11-07T13:03:21Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    13,
                    3,
                    21,
                    3,
                    312,
                    0
                ],
                "title": "Lightning IR: Straightforward Fine-tuning and Inference of\n  Transformer-based Language Models for Information Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lightning IR: Straightforward Fine-tuning and Inference of\n  Transformer-based Language Models for Information Retrieval"
                },
                "summary": "A wide range of transformer-based language models have been proposed for\ninformation retrieval tasks. However, including transformer-based models in\nretrieval pipelines is often complex and requires substantial engineering\neffort. In this paper, we introduce Lightning IR, an easy-to-use PyTorch\nLightning-based framework for applying transformer-based language models in\nretrieval scenarios. Lightning IR provides a modular and extensible\narchitecture that supports all stages of a retrieval pipeline: from fine-tuning\nand indexing to searching and re-ranking. Designed to be scalable and\nreproducible, Lightning IR is available as open-source:\nhttps://github.com/webis-de/lightning-ir.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A wide range of transformer-based language models have been proposed for\ninformation retrieval tasks. However, including transformer-based models in\nretrieval pipelines is often complex and requires substantial engineering\neffort. In this paper, we introduce Lightning IR, an easy-to-use PyTorch\nLightning-based framework for applying transformer-based language models in\nretrieval scenarios. Lightning IR provides a modular and extensible\narchitecture that supports all stages of a retrieval pipeline: from fine-tuning\nand indexing to searching and re-ranking. Designed to be scalable and\nreproducible, Lightning IR is available as open-source:\nhttps://github.com/webis-de/lightning-ir."
                },
                "authors": [
                    {
                        "name": "Ferdinand Schlatt"
                    },
                    {
                        "name": "Maik Fröbe"
                    },
                    {
                        "name": "Matthias Hagen"
                    }
                ],
                "author_detail": {
                    "name": "Matthias Hagen"
                },
                "author": "Matthias Hagen",
                "arxiv_doi": "10.1145/3701551.3704118",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3701551.3704118",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2411.04677v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04677v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted as a demo at WSDM'25",
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02367v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02367v5",
                "updated": "2025-02-26T14:28:20Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    14,
                    28,
                    20,
                    2,
                    57,
                    0
                ],
                "published": "2024-10-03T10:25:23Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    10,
                    25,
                    23,
                    3,
                    277,
                    0
                ],
                "title": "SageAttention: Accurate 8-Bit Attention for Plug-and-play Inference\n  Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SageAttention: Accurate 8-Bit Attention for Plug-and-play Inference\n  Acceleration"
                },
                "summary": "The transformer architecture predominates across various models. As the heart\nof the transformer, attention has a computational complexity of O(N^2),\ncompared to O(N) for linear transformations. When handling large sequence\nlengths, attention becomes the primary time-consuming component. Although\nquantization has proven to be an effective method for accelerating model\ninference, existing quantization methods primarily focus on optimizing the\nlinear layer. In response, we first analyze the feasibility of quantization in\nattention detailedly. Following that, we propose SageAttention, a highly\nefficient and accurate quantization method for attention. The OPS (operations\nper second) of our approach outperforms FlashAttention2 and xformers by about\n2.1 times and 2.7 times, respectively. SageAttention also achieves superior\naccuracy performance over FlashAttention3. Comprehensive experiments confirm\nthat our approach incurs almost no end-to-end metrics loss across diverse\nmodels, including those for large language processing, image generation, and\nvideo generation. The codes are available at\nhttps://github.com/thu-ml/SageAttention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The transformer architecture predominates across various models. As the heart\nof the transformer, attention has a computational complexity of O(N^2),\ncompared to O(N) for linear transformations. When handling large sequence\nlengths, attention becomes the primary time-consuming component. Although\nquantization has proven to be an effective method for accelerating model\ninference, existing quantization methods primarily focus on optimizing the\nlinear layer. In response, we first analyze the feasibility of quantization in\nattention detailedly. Following that, we propose SageAttention, a highly\nefficient and accurate quantization method for attention. The OPS (operations\nper second) of our approach outperforms FlashAttention2 and xformers by about\n2.1 times and 2.7 times, respectively. SageAttention also achieves superior\naccuracy performance over FlashAttention3. Comprehensive experiments confirm\nthat our approach incurs almost no end-to-end metrics loss across diverse\nmodels, including those for large language processing, image generation, and\nvideo generation. The codes are available at\nhttps://github.com/thu-ml/SageAttention."
                },
                "authors": [
                    {
                        "name": "Jintao Zhang"
                    },
                    {
                        "name": "Jia wei"
                    },
                    {
                        "name": "Haofeng Huang"
                    },
                    {
                        "name": "Pengle Zhang"
                    },
                    {
                        "name": "Jun Zhu"
                    },
                    {
                        "name": "Jianfei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Jianfei Chen"
                },
                "author": "Jianfei Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02367v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02367v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.07064v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.07064v2",
                "updated": "2025-02-26T14:28:11Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    14,
                    28,
                    11,
                    2,
                    57,
                    0
                ],
                "published": "2024-07-09T17:38:03Z",
                "published_parsed": [
                    2024,
                    7,
                    9,
                    17,
                    38,
                    3,
                    1,
                    191,
                    0
                ],
                "title": "Prompting Techniques for Secure Code Generation: A Systematic\n  Investigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompting Techniques for Secure Code Generation: A Systematic\n  Investigation"
                },
                "summary": "Large Language Models (LLMs) are gaining momentum in software development\nwith prompt-driven programming enabling developers to create code from natural\nlanguage (NL) instructions. However, studies have questioned their ability to\nproduce secure code and, thereby, the quality of prompt-generated software.\nAlongside, various prompting techniques that carefully tailor prompts have\nemerged to elicit optimal responses from LLMs. Still, the interplay between\nsuch prompting strategies and secure code generation remains under-explored and\ncalls for further investigations. OBJECTIVE: In this study, we investigate the\nimpact of different prompting techniques on the security of code generated from\nNL instructions by LLMs. METHOD: First we perform a systematic literature\nreview to identify the existing prompting techniques that can be used for code\ngeneration tasks. A subset of these techniques are evaluated on GPT-3, GPT-3.5,\nand GPT-4 models for secure code generation. For this, we used an existing\ndataset consisting of 150 NL security-relevant code-generation prompts.\nRESULTS: Our work (i) classifies potential prompting techniques for code\ngeneration (ii) adapts and evaluates a subset of the identified techniques for\nsecure code generation tasks and (iii) observes a reduction in security\nweaknesses across the tested LLMs, especially after using an existing technique\ncalled Recursive Criticism and Improvement (RCI), contributing valuable\ninsights to the ongoing discourse on LLM-generated code security.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are gaining momentum in software development\nwith prompt-driven programming enabling developers to create code from natural\nlanguage (NL) instructions. However, studies have questioned their ability to\nproduce secure code and, thereby, the quality of prompt-generated software.\nAlongside, various prompting techniques that carefully tailor prompts have\nemerged to elicit optimal responses from LLMs. Still, the interplay between\nsuch prompting strategies and secure code generation remains under-explored and\ncalls for further investigations. OBJECTIVE: In this study, we investigate the\nimpact of different prompting techniques on the security of code generated from\nNL instructions by LLMs. METHOD: First we perform a systematic literature\nreview to identify the existing prompting techniques that can be used for code\ngeneration tasks. A subset of these techniques are evaluated on GPT-3, GPT-3.5,\nand GPT-4 models for secure code generation. For this, we used an existing\ndataset consisting of 150 NL security-relevant code-generation prompts.\nRESULTS: Our work (i) classifies potential prompting techniques for code\ngeneration (ii) adapts and evaluates a subset of the identified techniques for\nsecure code generation tasks and (iii) observes a reduction in security\nweaknesses across the tested LLMs, especially after using an existing technique\ncalled Recursive Criticism and Improvement (RCI), contributing valuable\ninsights to the ongoing discourse on LLM-generated code security."
                },
                "authors": [
                    {
                        "name": "Catherine Tony"
                    },
                    {
                        "name": "Nicolás E. Díaz Ferreyra"
                    },
                    {
                        "name": "Markus Mutas"
                    },
                    {
                        "name": "Salem Dhiff"
                    },
                    {
                        "name": "Riccardo Scandariato"
                    }
                ],
                "author_detail": {
                    "name": "Riccardo Scandariato"
                },
                "author": "Riccardo Scandariato",
                "arxiv_comment": "Work partially supported by the EU-funded project Sec4AI4Sec:\n  Cybersecurity for AI-Augmented Systems (grant no. 101120393) - ACCEPTED at\n  ACM Transactions on Software Engineering and Methodology (Feb. 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.07064v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.07064v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18168v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18168v2",
                "updated": "2025-02-26T14:27:33Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    14,
                    27,
                    33,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-25T13:00:05Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    13,
                    0,
                    5,
                    1,
                    56,
                    0
                ],
                "title": "SECURA: Sigmoid-Enhanced CUR Decomposition with Uninterrupted Retention\n  and Low-Rank Adaptation in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SECURA: Sigmoid-Enhanced CUR Decomposition with Uninterrupted Retention\n  and Low-Rank Adaptation in Large Language Models"
                },
                "summary": "With the rapid development of large language models (LLMs), fully fine-tuning\n(FT) these models has become increasingly impractical due to the high\ncomputational demands. Additionally, FT can lead to catastrophic forgetting. As\nan alternative, Low-Rank Adaptation (LoRA) has been proposed, which fine-tunes\nonly a small subset of parameters, achieving similar performance to FT while\nsignificantly reducing resource requirements. However, since LoRA inherits FT's\ndesign, the issue of catastrophic forgetting remains.\n  To address these challenges, we propose SECURA: Sigmoid-Enhanced CUR\nDecomposition LoRA, a novel parameter-efficient fine-tuning (PEFT) variant that\nmitigates catastrophic forgetting while improving fine-tuning performance. Our\nmethod introduces a new normalization technique, SigNorm, to enhance parameter\nretention and overall performance.\n  SECURA has been evaluated on a variety of tasks, including mathematical\nproblem-solving (GSM8K), challenging question-answering (CNNDM), translation\n(NewsDE), and complex multiple-choice reasoning (LogiQA). Experimental results\nshow that SECURA achieves an average fine-tuning improvement of 3.59% across\nfour multiple-choice question (MCQ) tasks and a 2.51% improvement across five\nquestion-answering (QA) tasks on models such as Gemma2 2b, Qwen2 1.5b, Qwen 2\n7b, Llama3 8b, and Llama3.1 8b, compared to DoRA. Moreover, SECURA demonstrates\nsuperior knowledge retention capabilities, maintaining more than 70% accuracy\non basic LLM knowledge across 16 continual learning tests, outperforming\nExperience Replay (ER), Sequential Learning (SEQ), EWC, I-LoRA, and CUR-LoRA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid development of large language models (LLMs), fully fine-tuning\n(FT) these models has become increasingly impractical due to the high\ncomputational demands. Additionally, FT can lead to catastrophic forgetting. As\nan alternative, Low-Rank Adaptation (LoRA) has been proposed, which fine-tunes\nonly a small subset of parameters, achieving similar performance to FT while\nsignificantly reducing resource requirements. However, since LoRA inherits FT's\ndesign, the issue of catastrophic forgetting remains.\n  To address these challenges, we propose SECURA: Sigmoid-Enhanced CUR\nDecomposition LoRA, a novel parameter-efficient fine-tuning (PEFT) variant that\nmitigates catastrophic forgetting while improving fine-tuning performance. Our\nmethod introduces a new normalization technique, SigNorm, to enhance parameter\nretention and overall performance.\n  SECURA has been evaluated on a variety of tasks, including mathematical\nproblem-solving (GSM8K), challenging question-answering (CNNDM), translation\n(NewsDE), and complex multiple-choice reasoning (LogiQA). Experimental results\nshow that SECURA achieves an average fine-tuning improvement of 3.59% across\nfour multiple-choice question (MCQ) tasks and a 2.51% improvement across five\nquestion-answering (QA) tasks on models such as Gemma2 2b, Qwen2 1.5b, Qwen 2\n7b, Llama3 8b, and Llama3.1 8b, compared to DoRA. Moreover, SECURA demonstrates\nsuperior knowledge retention capabilities, maintaining more than 70% accuracy\non basic LLM knowledge across 16 continual learning tests, outperforming\nExperience Replay (ER), Sequential Learning (SEQ), EWC, I-LoRA, and CUR-LoRA."
                },
                "authors": [
                    {
                        "name": "Yuxuan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yuxuan Zhang"
                },
                "author": "Yuxuan Zhang",
                "arxiv_comment": "New work on Parameter-Efficient Fine-Tuning (PEFT) for large language\n  models. Includes new techniques SigNorm and CABR-LoRA for optimizing\n  fine-tune performance and Knowledge retention",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18168v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18168v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.6; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10990v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10990v2",
                "updated": "2025-02-26T14:26:14Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    14,
                    26,
                    14,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-16T04:23:52Z",
                "published_parsed": [
                    2025,
                    2,
                    16,
                    4,
                    23,
                    52,
                    6,
                    47,
                    0
                ],
                "title": "FinMTEB: Finance Massive Text Embedding Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FinMTEB: Finance Massive Text Embedding Benchmark"
                },
                "summary": "Embedding models play a crucial role in representing and retrieving\ninformation across various NLP applications. Recent advances in large language\nmodels (LLMs) have further enhanced the performance of embedding models. While\nthese models are often benchmarked on general-purpose datasets, real-world\napplications demand domain-specific evaluation. In this work, we introduce the\nFinance Massive Text Embedding Benchmark (FinMTEB), a specialized counterpart\nto MTEB designed for the financial domain. FinMTEB comprises 64 financial\ndomain-specific embedding datasets across 7 tasks that cover diverse textual\ntypes in both Chinese and English, such as financial news articles, corporate\nannual reports, ESG reports, regulatory filings, and earnings call transcripts.\nWe also develop a finance-adapted model, Fin-E5, using a persona-based data\nsynthetic method to cover diverse financial embedding tasks for training.\nThrough extensive evaluation of 15 embedding models, including Fin-E5, we show\nthree key findings: (1) performance on general-purpose benchmarks shows limited\ncorrelation with financial domain tasks; (2) domain-adapted models consistently\noutperform their general-purpose counterparts; and (3) surprisingly, a simple\nBag-of-Words (BoW) approach outperforms sophisticated dense embeddings in\nfinancial Semantic Textual Similarity (STS) tasks, underscoring current\nlimitations in dense embedding techniques. Our work establishes a robust\nevaluation framework for financial NLP applications and provides crucial\ninsights for developing domain-specific embedding models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embedding models play a crucial role in representing and retrieving\ninformation across various NLP applications. Recent advances in large language\nmodels (LLMs) have further enhanced the performance of embedding models. While\nthese models are often benchmarked on general-purpose datasets, real-world\napplications demand domain-specific evaluation. In this work, we introduce the\nFinance Massive Text Embedding Benchmark (FinMTEB), a specialized counterpart\nto MTEB designed for the financial domain. FinMTEB comprises 64 financial\ndomain-specific embedding datasets across 7 tasks that cover diverse textual\ntypes in both Chinese and English, such as financial news articles, corporate\nannual reports, ESG reports, regulatory filings, and earnings call transcripts.\nWe also develop a finance-adapted model, Fin-E5, using a persona-based data\nsynthetic method to cover diverse financial embedding tasks for training.\nThrough extensive evaluation of 15 embedding models, including Fin-E5, we show\nthree key findings: (1) performance on general-purpose benchmarks shows limited\ncorrelation with financial domain tasks; (2) domain-adapted models consistently\noutperform their general-purpose counterparts; and (3) surprisingly, a simple\nBag-of-Words (BoW) approach outperforms sophisticated dense embeddings in\nfinancial Semantic Textual Similarity (STS) tasks, underscoring current\nlimitations in dense embedding techniques. Our work establishes a robust\nevaluation framework for financial NLP applications and provides crucial\ninsights for developing domain-specific embedding models."
                },
                "authors": [
                    {
                        "name": "Yixuan Tang"
                    },
                    {
                        "name": "Yi Yang"
                    }
                ],
                "author_detail": {
                    "name": "Yi Yang"
                },
                "author": "Yi Yang",
                "arxiv_comment": "https://github.com/yixuantt/FinMTEB",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10990v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10990v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19167v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19167v1",
                "updated": "2025-02-26T14:22:59Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    14,
                    22,
                    59,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-26T14:22:59Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    14,
                    22,
                    59,
                    2,
                    57,
                    0
                ],
                "title": "Generalizable deep learning for photoplethysmography-based blood\n  pressure estimation -- A Benchmarking Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generalizable deep learning for photoplethysmography-based blood\n  pressure estimation -- A Benchmarking Study"
                },
                "summary": "Photoplethysmography (PPG)-based blood pressure (BP) estimation represents a\npromising alternative to cuff-based BP measurements. Recently, an increasing\nnumber of deep learning models have been proposed to infer BP from the raw PPG\nwaveform. However, these models have been predominantly evaluated on\nin-distribution test sets, which immediately raises the question of the\ngeneralizability of these models to external datasets. To investigate this\nquestion, we trained five deep learning models on the recently released PulseDB\ndataset, provided in-distribution benchmarking results on this dataset, and\nthen assessed out-of-distribution performance on several external datasets. The\nbest model (XResNet1d101) achieved in-distribution MAEs of 9.4 and 6.0 mmHg for\nsystolic and diastolic BP respectively on PulseDB (with subject-specific\ncalibration), and 14.0 and 8.5 mmHg respectively without calibration.\nEquivalent MAEs on external test datasets without calibration ranged from 15.0\nto 25.1 mmHg (SBP) and 7.0 to 10.4 mmHg (DBP). Our results indicate that the\nperformance is strongly influenced by the differences in BP distributions\nbetween datasets. We investigated a simple way of improving performance through\nsample-based domain adaptation and put forward recommendations for training\nmodels with good generalization properties. With this work, we hope to educate\nmore researchers for the importance and challenges of out-of-distribution\ngeneralization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Photoplethysmography (PPG)-based blood pressure (BP) estimation represents a\npromising alternative to cuff-based BP measurements. Recently, an increasing\nnumber of deep learning models have been proposed to infer BP from the raw PPG\nwaveform. However, these models have been predominantly evaluated on\nin-distribution test sets, which immediately raises the question of the\ngeneralizability of these models to external datasets. To investigate this\nquestion, we trained five deep learning models on the recently released PulseDB\ndataset, provided in-distribution benchmarking results on this dataset, and\nthen assessed out-of-distribution performance on several external datasets. The\nbest model (XResNet1d101) achieved in-distribution MAEs of 9.4 and 6.0 mmHg for\nsystolic and diastolic BP respectively on PulseDB (with subject-specific\ncalibration), and 14.0 and 8.5 mmHg respectively without calibration.\nEquivalent MAEs on external test datasets without calibration ranged from 15.0\nto 25.1 mmHg (SBP) and 7.0 to 10.4 mmHg (DBP). Our results indicate that the\nperformance is strongly influenced by the differences in BP distributions\nbetween datasets. We investigated a simple way of improving performance through\nsample-based domain adaptation and put forward recommendations for training\nmodels with good generalization properties. With this work, we hope to educate\nmore researchers for the importance and challenges of out-of-distribution\ngeneralization."
                },
                "authors": [
                    {
                        "name": "Mohammad Moulaeifard"
                    },
                    {
                        "name": "Peter H. Charlton"
                    },
                    {
                        "name": "Nils Strodthoff"
                    }
                ],
                "author_detail": {
                    "name": "Nils Strodthoff"
                },
                "author": "Nils Strodthoff",
                "arxiv_comment": "20 pages, 5 figures, code available at\n  https://github.com/AI4HealthUOL/ppg-ood-generalization",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19167v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19167v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19166v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19166v1",
                "updated": "2025-02-26T14:19:49Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    14,
                    19,
                    49,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-26T14:19:49Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    14,
                    19,
                    49,
                    2,
                    57,
                    0
                ],
                "title": "CodeIF: Benchmarking the Instruction-Following Capabilities of Large\n  Language Models for Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CodeIF: Benchmarking the Instruction-Following Capabilities of Large\n  Language Models for Code Generation"
                },
                "summary": "With the rapid advancement of Large Language Models (LLMs), the demand for\nrobust instruction-following capabilities in code generation tasks has grown\nsignificantly. Code generation not only facilitates faster prototyping and\nautomated testing, but also augments developer efficiency through improved\nmaintainability and reusability of code. In this paper, we introduce CodeIF,\nthe first benchmark specifically designed to assess the abilities of LLMs to\nadhere to task-oriented instructions within diverse code generation scenarios.\nCodeIF encompasses a broad range of tasks, including function synthesis, error\ndebugging, algorithmic refactoring, and code explanation, thereby providing a\ncomprehensive suite to evaluate model performance across varying complexity\nlevels and programming domains. We conduct extensive experiments with LLMs,\nanalyzing their strengths and limitations in meeting the demands of these\ntasks. The experimental results offer valuable insights into how well current\nmodels align with human instructions, as well as the extent to which they can\ngenerate consistent, maintainable, and contextually relevant code. Our findings\nnot only underscore the critical role that instruction-following LLMs can play\nin modern software development, but also illuminate pathways for future\nresearch aimed at enhancing their adaptability, reliability, and overall\neffectiveness in automated code generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid advancement of Large Language Models (LLMs), the demand for\nrobust instruction-following capabilities in code generation tasks has grown\nsignificantly. Code generation not only facilitates faster prototyping and\nautomated testing, but also augments developer efficiency through improved\nmaintainability and reusability of code. In this paper, we introduce CodeIF,\nthe first benchmark specifically designed to assess the abilities of LLMs to\nadhere to task-oriented instructions within diverse code generation scenarios.\nCodeIF encompasses a broad range of tasks, including function synthesis, error\ndebugging, algorithmic refactoring, and code explanation, thereby providing a\ncomprehensive suite to evaluate model performance across varying complexity\nlevels and programming domains. We conduct extensive experiments with LLMs,\nanalyzing their strengths and limitations in meeting the demands of these\ntasks. The experimental results offer valuable insights into how well current\nmodels align with human instructions, as well as the extent to which they can\ngenerate consistent, maintainable, and contextually relevant code. Our findings\nnot only underscore the critical role that instruction-following LLMs can play\nin modern software development, but also illuminate pathways for future\nresearch aimed at enhancing their adaptability, reliability, and overall\neffectiveness in automated code generation."
                },
                "authors": [
                    {
                        "name": "Kaiwen Yan"
                    },
                    {
                        "name": "Hongcheng Guo"
                    },
                    {
                        "name": "Xuanqing Shi"
                    },
                    {
                        "name": "Jingyi Xu"
                    },
                    {
                        "name": "Yaonan Gu"
                    },
                    {
                        "name": "Zhoujun Li"
                    }
                ],
                "author_detail": {
                    "name": "Zhoujun Li"
                },
                "author": "Zhoujun Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19166v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19166v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19163v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19163v1",
                "updated": "2025-02-26T14:17:56Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    14,
                    17,
                    56,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-26T14:17:56Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    14,
                    17,
                    56,
                    2,
                    57,
                    0
                ],
                "title": "TestNUC: Enhancing Test-Time Computing Approaches through Neighboring\n  Unlabeled Data Consistency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TestNUC: Enhancing Test-Time Computing Approaches through Neighboring\n  Unlabeled Data Consistency"
                },
                "summary": "Test-time computing approaches, which leverage additional computational\nresources during inference, have been proven effective in enhancing large\nlanguage model performance. This work introduces a novel, linearly scaling\napproach, TestNUC, that improves test-time predictions by leveraging the local\nconsistency of neighboring unlabeled data-it classifies an input instance by\nconsidering not only the model's prediction on that instance but also on\nneighboring unlabeled instances. We evaluate TestNUC across eight diverse\ndatasets, spanning intent classification, topic mining, domain discovery, and\nemotion detection, demonstrating its consistent superiority over baseline\nmethods such as standard prompting and self-consistency. Furthermore, TestNUC\ncan be seamlessly integrated with existing test-time computing approaches,\nsubstantially boosting their performance. Our analysis reveals that TestNUC\nscales effectively with increasing amounts of unlabeled data and performs\nrobustly across different embedding models, making it practical for real-world\napplications. Our code is available at https://github.com/HenryPengZou/TestNUC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time computing approaches, which leverage additional computational\nresources during inference, have been proven effective in enhancing large\nlanguage model performance. This work introduces a novel, linearly scaling\napproach, TestNUC, that improves test-time predictions by leveraging the local\nconsistency of neighboring unlabeled data-it classifies an input instance by\nconsidering not only the model's prediction on that instance but also on\nneighboring unlabeled instances. We evaluate TestNUC across eight diverse\ndatasets, spanning intent classification, topic mining, domain discovery, and\nemotion detection, demonstrating its consistent superiority over baseline\nmethods such as standard prompting and self-consistency. Furthermore, TestNUC\ncan be seamlessly integrated with existing test-time computing approaches,\nsubstantially boosting their performance. Our analysis reveals that TestNUC\nscales effectively with increasing amounts of unlabeled data and performs\nrobustly across different embedding models, making it practical for real-world\napplications. Our code is available at https://github.com/HenryPengZou/TestNUC."
                },
                "authors": [
                    {
                        "name": "Henry Peng Zou"
                    },
                    {
                        "name": "Zhengyao Gu"
                    },
                    {
                        "name": "Yue Zhou"
                    },
                    {
                        "name": "Yankai Chen"
                    },
                    {
                        "name": "Weizhi Zhang"
                    },
                    {
                        "name": "Liancheng Fang"
                    },
                    {
                        "name": "Yibo Wang"
                    },
                    {
                        "name": "Yangning Li"
                    },
                    {
                        "name": "Kay Liu"
                    },
                    {
                        "name": "Philip S. Yu"
                    }
                ],
                "author_detail": {
                    "name": "Philip S. Yu"
                },
                "author": "Philip S. Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19163v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19163v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19160v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19160v1",
                "updated": "2025-02-26T14:15:28Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    14,
                    15,
                    28,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-26T14:15:28Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    14,
                    15,
                    28,
                    2,
                    57,
                    0
                ],
                "title": "Detecting Linguistic Indicators for Stereotype Assessment with Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting Linguistic Indicators for Stereotype Assessment with Large\n  Language Models"
                },
                "summary": "Social categories and stereotypes are embedded in language and can introduce\ndata bias into Large Language Models (LLMs). Despite safeguards, these biases\noften persist in model behavior, potentially leading to representational harm\nin outputs. While sociolinguistic research provides valuable insights into the\nformation of stereotypes, NLP approaches for stereotype detection rarely draw\non this foundation and often lack objectivity, precision, and interpretability.\nTo fill this gap, in this work we propose a new approach that detects and\nquantifies the linguistic indicators of stereotypes in a sentence. We derive\nlinguistic indicators from the Social Category and Stereotype Communication\n(SCSC) framework which indicate strong social category formulation and\nstereotyping in language, and use them to build a categorization scheme. To\nautomate this approach, we instruct different LLMs using in-context learning to\napply the approach to a sentence, where the LLM examines the linguistic\nproperties and provides a basis for a fine-grained assessment. Based on an\nempirical evaluation of the importance of different linguistic indicators, we\nlearn a scoring function that measures the linguistic indicators of a\nstereotype. Our annotations of stereotyped sentences show that these indicators\nare present in these sentences and explain the strength of a stereotype. In\nterms of model performance, our results show that the models generally perform\nwell in detecting and classifying linguistic indicators of category labels used\nto denote a category, but sometimes struggle to correctly evaluate the\nassociated behaviors and characteristics. Using more few-shot examples within\nthe prompts, significantly improves performance. Model performance increases\nwith size, as Llama-3.3-70B-Instruct and GPT-4 achieve comparable results that\nsurpass those of Mixtral-8x7B-Instruct, GPT-4-mini and Llama-3.1-8B-Instruct.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Social categories and stereotypes are embedded in language and can introduce\ndata bias into Large Language Models (LLMs). Despite safeguards, these biases\noften persist in model behavior, potentially leading to representational harm\nin outputs. While sociolinguistic research provides valuable insights into the\nformation of stereotypes, NLP approaches for stereotype detection rarely draw\non this foundation and often lack objectivity, precision, and interpretability.\nTo fill this gap, in this work we propose a new approach that detects and\nquantifies the linguistic indicators of stereotypes in a sentence. We derive\nlinguistic indicators from the Social Category and Stereotype Communication\n(SCSC) framework which indicate strong social category formulation and\nstereotyping in language, and use them to build a categorization scheme. To\nautomate this approach, we instruct different LLMs using in-context learning to\napply the approach to a sentence, where the LLM examines the linguistic\nproperties and provides a basis for a fine-grained assessment. Based on an\nempirical evaluation of the importance of different linguistic indicators, we\nlearn a scoring function that measures the linguistic indicators of a\nstereotype. Our annotations of stereotyped sentences show that these indicators\nare present in these sentences and explain the strength of a stereotype. In\nterms of model performance, our results show that the models generally perform\nwell in detecting and classifying linguistic indicators of category labels used\nto denote a category, but sometimes struggle to correctly evaluate the\nassociated behaviors and characteristics. Using more few-shot examples within\nthe prompts, significantly improves performance. Model performance increases\nwith size, as Llama-3.3-70B-Instruct and GPT-4 achieve comparable results that\nsurpass those of Mixtral-8x7B-Instruct, GPT-4-mini and Llama-3.1-8B-Instruct."
                },
                "authors": [
                    {
                        "name": "Rebekka Görge"
                    },
                    {
                        "name": "Michael Mock"
                    },
                    {
                        "name": "Héctor Allende-Cid"
                    }
                ],
                "author_detail": {
                    "name": "Héctor Allende-Cid"
                },
                "author": "Héctor Allende-Cid",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19160v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19160v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19159v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19159v1",
                "updated": "2025-02-26T14:15:24Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    14,
                    15,
                    24,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-26T14:15:24Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    14,
                    15,
                    24,
                    2,
                    57,
                    0
                ],
                "title": "A Sliding Layer Merging Method for Efficient Depth-Wise Pruning in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Sliding Layer Merging Method for Efficient Depth-Wise Pruning in LLMs"
                },
                "summary": "Compared to width-wise pruning, depth-wise pruning can significantly\naccelerate inference in resource-constrained scenarios. Howerver, treating the\nentire Transformer layer as the minimum pruning unit may degrade model\nperformance by indiscriminately discarding the entire information of the layer.\nThis paper reveals the \"Patch-like\" feature relationship between layers in\nlarge language models by analyzing the correlation of the outputs of different\nlayers in the reproducing kernel Hilbert space. Building on this observation,\nwe proposes a sliding layer merging method that dynamically selects and fuses\nconsecutive layers from top to bottom according to a pre-defined similarity\nthreshold, thereby simplifying the model structure while maintaining its\nperformance. Extensive experiments on LLMs with various architectures and\ndifferent parameter scales show that our method outperforms existing pruning\ntechniques in both zero-shot inference performance and retraining recovery\nquality after pruning. In particular, in the experiment with 35\\% pruning on\nthe Vicuna-7B model, our method achieved a 1.654\\% improvement in average\nperformance on zero-shot tasks compared to the existing method. Moreover, we\nfurther reveal the potential of combining depth pruning with width pruning to\nenhance the pruning effect. Our codes are available at\nhttps://github.com/920927/SLM-a-sliding-layer-merging-method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compared to width-wise pruning, depth-wise pruning can significantly\naccelerate inference in resource-constrained scenarios. Howerver, treating the\nentire Transformer layer as the minimum pruning unit may degrade model\nperformance by indiscriminately discarding the entire information of the layer.\nThis paper reveals the \"Patch-like\" feature relationship between layers in\nlarge language models by analyzing the correlation of the outputs of different\nlayers in the reproducing kernel Hilbert space. Building on this observation,\nwe proposes a sliding layer merging method that dynamically selects and fuses\nconsecutive layers from top to bottom according to a pre-defined similarity\nthreshold, thereby simplifying the model structure while maintaining its\nperformance. Extensive experiments on LLMs with various architectures and\ndifferent parameter scales show that our method outperforms existing pruning\ntechniques in both zero-shot inference performance and retraining recovery\nquality after pruning. In particular, in the experiment with 35\\% pruning on\nthe Vicuna-7B model, our method achieved a 1.654\\% improvement in average\nperformance on zero-shot tasks compared to the existing method. Moreover, we\nfurther reveal the potential of combining depth pruning with width pruning to\nenhance the pruning effect. Our codes are available at\nhttps://github.com/920927/SLM-a-sliding-layer-merging-method."
                },
                "authors": [
                    {
                        "name": "Xuan Ding"
                    },
                    {
                        "name": "Yao Zhu"
                    },
                    {
                        "name": "Yunjian Zhang"
                    },
                    {
                        "name": "Chuanlong Xie"
                    }
                ],
                "author_detail": {
                    "name": "Chuanlong Xie"
                },
                "author": "Chuanlong Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19159v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19159v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19158v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19158v1",
                "updated": "2025-02-26T14:14:58Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    14,
                    14,
                    58,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-26T14:14:58Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    14,
                    14,
                    58,
                    2,
                    57,
                    0
                ],
                "title": "When Personalization Meets Reality: A Multi-Faceted Analysis of\n  Personalized Preference Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Personalization Meets Reality: A Multi-Faceted Analysis of\n  Personalized Preference Learning"
                },
                "summary": "While Reinforcement Learning from Human Feedback (RLHF) is widely used to\nalign Large Language Models (LLMs) with human preferences, it typically assumes\nhomogeneous preferences across users, overlooking diverse human values and\nminority viewpoints. Although personalized preference learning addresses this\nby tailoring separate preferences for individual users, the field lacks\nstandardized methods to assess its effectiveness. We present a multi-faceted\nevaluation framework that measures not only performance but also fairness,\nunintended effects, and adaptability across varying levels of preference\ndivergence. Through extensive experiments comparing eight personalization\nmethods across three preference datasets, we demonstrate that performance\ndifferences between methods could reach 36% when users strongly disagree, and\npersonalization can introduce up to 20% safety misalignment. These findings\nhighlight the critical need for holistic evaluation approaches to advance the\ndevelopment of more effective and inclusive preference learning systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Reinforcement Learning from Human Feedback (RLHF) is widely used to\nalign Large Language Models (LLMs) with human preferences, it typically assumes\nhomogeneous preferences across users, overlooking diverse human values and\nminority viewpoints. Although personalized preference learning addresses this\nby tailoring separate preferences for individual users, the field lacks\nstandardized methods to assess its effectiveness. We present a multi-faceted\nevaluation framework that measures not only performance but also fairness,\nunintended effects, and adaptability across varying levels of preference\ndivergence. Through extensive experiments comparing eight personalization\nmethods across three preference datasets, we demonstrate that performance\ndifferences between methods could reach 36% when users strongly disagree, and\npersonalization can introduce up to 20% safety misalignment. These findings\nhighlight the critical need for holistic evaluation approaches to advance the\ndevelopment of more effective and inclusive preference learning systems."
                },
                "authors": [
                    {
                        "name": "Yijiang River Dong"
                    },
                    {
                        "name": "Tiancheng Hu"
                    },
                    {
                        "name": "Yinhong Liu"
                    },
                    {
                        "name": "Ahmet Üstün"
                    },
                    {
                        "name": "Nigel Collier"
                    }
                ],
                "author_detail": {
                    "name": "Nigel Collier"
                },
                "author": "Nigel Collier",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19158v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19158v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19149v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19149v1",
                "updated": "2025-02-26T14:08:17Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    14,
                    8,
                    17,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-26T14:08:17Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    14,
                    8,
                    17,
                    2,
                    57,
                    0
                ],
                "title": "Isolating Language-Coding from Problem-Solving: Benchmarking LLMs with\n  PseudoEval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Isolating Language-Coding from Problem-Solving: Benchmarking LLMs with\n  PseudoEval"
                },
                "summary": "Existing code generation benchmarks for Large Language Models (LLMs) such as\nHumanEval and MBPP are designed to study LLMs' end-to-end performance, where\nthe benchmarks feed a problem description in natural language as input and\nexamine the generated code in specific programming languages. However, the\nevaluation scores revealed in this way provide a little hint as to the\nbottleneck of the code generation -- whether LLMs are struggling with their\nproblem-solving capability or language-coding capability. To answer this\nquestion, we construct PseudoEval, a multilingual code generation benchmark\nthat provides a solution written in pseudocode as input. By doing so, the\nbottleneck of code generation in various programming languages could be\nisolated and identified. Our study yields several interesting findings. For\nexample, we identify that the bottleneck of LLMs in Python programming is\nproblem-solving, while Rust is struggling relatively more in language-coding.\nAlso, our study indicates that problem-solving capability may transfer across\nprogramming languages, while language-coding needs more language-specific\neffort, especially for undertrained programming languages. Finally, we release\nthe pipeline of constructing PseudoEval to facilitate the extension to existing\nbenchmarks. PseudoEval is available at:\nhttps://anonymous.4open.science/r/PseudocodeACL25-7B74.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing code generation benchmarks for Large Language Models (LLMs) such as\nHumanEval and MBPP are designed to study LLMs' end-to-end performance, where\nthe benchmarks feed a problem description in natural language as input and\nexamine the generated code in specific programming languages. However, the\nevaluation scores revealed in this way provide a little hint as to the\nbottleneck of the code generation -- whether LLMs are struggling with their\nproblem-solving capability or language-coding capability. To answer this\nquestion, we construct PseudoEval, a multilingual code generation benchmark\nthat provides a solution written in pseudocode as input. By doing so, the\nbottleneck of code generation in various programming languages could be\nisolated and identified. Our study yields several interesting findings. For\nexample, we identify that the bottleneck of LLMs in Python programming is\nproblem-solving, while Rust is struggling relatively more in language-coding.\nAlso, our study indicates that problem-solving capability may transfer across\nprogramming languages, while language-coding needs more language-specific\neffort, especially for undertrained programming languages. Finally, we release\nthe pipeline of constructing PseudoEval to facilitate the extension to existing\nbenchmarks. PseudoEval is available at:\nhttps://anonymous.4open.science/r/PseudocodeACL25-7B74."
                },
                "authors": [
                    {
                        "name": "Jiarong Wu"
                    },
                    {
                        "name": "Songqiang Chen"
                    },
                    {
                        "name": "Jialun Cao"
                    },
                    {
                        "name": "Hau Ching Lo"
                    },
                    {
                        "name": "Shing-Chi Cheung"
                    }
                ],
                "author_detail": {
                    "name": "Shing-Chi Cheung"
                },
                "author": "Shing-Chi Cheung",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19149v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19149v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19148v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19148v1",
                "updated": "2025-02-26T14:07:37Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    14,
                    7,
                    37,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-26T14:07:37Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    14,
                    7,
                    37,
                    2,
                    57,
                    0
                ],
                "title": "Amulet: ReAlignment During Test Time for Personalized Preference\n  Adaptation of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Amulet: ReAlignment During Test Time for Personalized Preference\n  Adaptation of LLMs"
                },
                "summary": "How to align large language models (LLMs) with user preferences from a static\ngeneral dataset has been frequently studied. However, user preferences are\nusually personalized, changing, and diverse regarding culture, values, or time.\nThis leads to the problem that the actual user preferences often do not\ncoincide with those trained by the model developers in the practical use of\nLLMs. Since we cannot collect enough data and retrain for every demand,\nresearching efficient real-time preference adaptation methods based on the\nbackbone LLMs during test time is important. To this end, we introduce Amulet,\na novel, training-free framework that formulates the decoding process of every\ntoken as a separate online learning problem with the guidance of simple\nuser-provided prompts, thus enabling real-time optimization to satisfy users'\npersonalized preferences. To reduce the computational cost brought by this\noptimization process for each token, we additionally provide a closed-form\nsolution for each iteration step of the optimization process, thereby reducing\nthe computational time cost to a negligible level. The detailed experimental\nresults demonstrate that Amulet can achieve significant performance\nimprovements in rich settings with combinations of different LLMs, datasets,\nand user preferences, while maintaining acceptable computational efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How to align large language models (LLMs) with user preferences from a static\ngeneral dataset has been frequently studied. However, user preferences are\nusually personalized, changing, and diverse regarding culture, values, or time.\nThis leads to the problem that the actual user preferences often do not\ncoincide with those trained by the model developers in the practical use of\nLLMs. Since we cannot collect enough data and retrain for every demand,\nresearching efficient real-time preference adaptation methods based on the\nbackbone LLMs during test time is important. To this end, we introduce Amulet,\na novel, training-free framework that formulates the decoding process of every\ntoken as a separate online learning problem with the guidance of simple\nuser-provided prompts, thus enabling real-time optimization to satisfy users'\npersonalized preferences. To reduce the computational cost brought by this\noptimization process for each token, we additionally provide a closed-form\nsolution for each iteration step of the optimization process, thereby reducing\nthe computational time cost to a negligible level. The detailed experimental\nresults demonstrate that Amulet can achieve significant performance\nimprovements in rich settings with combinations of different LLMs, datasets,\nand user preferences, while maintaining acceptable computational efficiency."
                },
                "authors": [
                    {
                        "name": "Zhaowei Zhang"
                    },
                    {
                        "name": "Fengshuo Bai"
                    },
                    {
                        "name": "Qizhi Chen"
                    },
                    {
                        "name": "Chengdong Ma"
                    },
                    {
                        "name": "Mingzhi Wang"
                    },
                    {
                        "name": "Haoran Sun"
                    },
                    {
                        "name": "Zilong Zheng"
                    },
                    {
                        "name": "Yaodong Yang"
                    }
                ],
                "author_detail": {
                    "name": "Yaodong Yang"
                },
                "author": "Yaodong Yang",
                "arxiv_comment": "Accepted by ICLR 2025, Project page:\n  https://zowiezhang.github.io/projects/Amulet",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19148v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19148v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.00034v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.00034v2",
                "updated": "2025-02-26T14:07:05Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    14,
                    7,
                    5,
                    2,
                    57,
                    0
                ],
                "published": "2024-05-26T21:39:53Z",
                "published_parsed": [
                    2024,
                    5,
                    26,
                    21,
                    39,
                    53,
                    6,
                    147,
                    0
                ],
                "title": "Adaptive Activation Steering: A Tuning-Free LLM Truthfulness Improvement\n  Method for Diverse Hallucinations Categories",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Activation Steering: A Tuning-Free LLM Truthfulness Improvement\n  Method for Diverse Hallucinations Categories"
                },
                "summary": "Recent studies have indicated that Large Language Models (LLMs) harbor an\ninherent understanding of truthfulness, yet often fail to consistently express\nit and generate false statements. This gap between \"knowing\" and \"telling\"\nposes a challenge for ensuring the truthfulness of generated content. Inspired\nby recent work on the practice of encoding human-interpretable concepts\nlinearly within large language models, we treat truthfulness as a specially\nlinearly encoded concept within LLMs, and introduce Adaptive Activation\nSteering (ACT), a tuning-free method that adaptively shifts LLM's activations\nin the \"truthful\" direction during inference. ACT addresses diverse categories\nof hallucinations by utilizing diverse truthfulness-related steering vectors\nand adjusting the steering intensity adaptively. Applied as an add-on across\nvarious models, ACT significantly improves truthfulness in LLaMA ($\\uparrow$\n142%), LLaMA2 ($\\uparrow$ 24%), Alpaca ($\\uparrow$ 36%), Vicuna ($\\uparrow$\n28%), LLaMA2-Chat ($\\uparrow$ 19%), and LLaMA3($\\uparrow$ 34%). Furthermore, we\nverify ACT's scalability across larger models (13B, 33B, 65B), underscoring the\nadaptability of ACT to large-scale language models. Our code is available at\nhttps://github.com/tianlwang/ACT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent studies have indicated that Large Language Models (LLMs) harbor an\ninherent understanding of truthfulness, yet often fail to consistently express\nit and generate false statements. This gap between \"knowing\" and \"telling\"\nposes a challenge for ensuring the truthfulness of generated content. Inspired\nby recent work on the practice of encoding human-interpretable concepts\nlinearly within large language models, we treat truthfulness as a specially\nlinearly encoded concept within LLMs, and introduce Adaptive Activation\nSteering (ACT), a tuning-free method that adaptively shifts LLM's activations\nin the \"truthful\" direction during inference. ACT addresses diverse categories\nof hallucinations by utilizing diverse truthfulness-related steering vectors\nand adjusting the steering intensity adaptively. Applied as an add-on across\nvarious models, ACT significantly improves truthfulness in LLaMA ($\\uparrow$\n142%), LLaMA2 ($\\uparrow$ 24%), Alpaca ($\\uparrow$ 36%), Vicuna ($\\uparrow$\n28%), LLaMA2-Chat ($\\uparrow$ 19%), and LLaMA3($\\uparrow$ 34%). Furthermore, we\nverify ACT's scalability across larger models (13B, 33B, 65B), underscoring the\nadaptability of ACT to large-scale language models. Our code is available at\nhttps://github.com/tianlwang/ACT."
                },
                "authors": [
                    {
                        "name": "Tianlong Wang"
                    },
                    {
                        "name": "Xianfeng Jiao"
                    },
                    {
                        "name": "Yinghao Zhu"
                    },
                    {
                        "name": "Zhongzhi Chen"
                    },
                    {
                        "name": "Yifan He"
                    },
                    {
                        "name": "Xu Chu"
                    },
                    {
                        "name": "Junyi Gao"
                    },
                    {
                        "name": "Yasha Wang"
                    },
                    {
                        "name": "Liantao Ma"
                    }
                ],
                "author_detail": {
                    "name": "Liantao Ma"
                },
                "author": "Liantao Ma",
                "arxiv_doi": "10.1145/3696410.3714640",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3696410.3714640",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2406.00034v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.00034v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "ACM TheWebConf 2025 Conference (WWW 2025) Research Track",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15846v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15846v2",
                "updated": "2025-02-26T14:06:11Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    14,
                    6,
                    11,
                    2,
                    57,
                    0
                ],
                "published": "2024-09-24T08:17:50Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    8,
                    17,
                    50,
                    1,
                    268,
                    0
                ],
                "title": "Potential Field as Scene Affordance for Behavior Change-Based Visual\n  Risk Object Identification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Potential Field as Scene Affordance for Behavior Change-Based Visual\n  Risk Object Identification"
                },
                "summary": "We study behavior change-based visual risk object identification\n(Visual-ROI), a critical framework designed to detect potential hazards for\nintelligent driving systems. Existing methods often show significant\nlimitations in spatial accuracy and temporal consistency, stemming from an\nincomplete understanding of scene affordance. For example, these methods\nfrequently misidentify vehicles that do not impact the ego vehicle as risk\nobjects. Furthermore, existing behavior change-based methods are inefficient\nbecause they implement causal inference in the perspective image space. We\npropose a new framework with a Bird's Eye View (BEV) representation to overcome\nthe above challenges. Specifically, we utilize potential fields as scene\naffordance, involving repulsive forces derived from road infrastructure and\ntraffic participants, along with attractive forces sourced from target\ndestinations. In this work, we compute potential fields by assigning different\nenergy levels according to the semantic labels obtained from BEV semantic\nsegmentation. We conduct thorough experiments and ablation studies, comparing\nthe proposed method with various state-of-the-art algorithms on both synthetic\nand real-world datasets. Our results show a notable increase in spatial and\ntemporal consistency, with enhancements of 20.3% and 11.6% on the RiskBench\ndataset, respectively. Additionally, we can improve computational efficiency by\n88%. We achieve improvements of 5.4% in spatial accuracy and 7.2% in temporal\nconsistency on the nuScenes dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study behavior change-based visual risk object identification\n(Visual-ROI), a critical framework designed to detect potential hazards for\nintelligent driving systems. Existing methods often show significant\nlimitations in spatial accuracy and temporal consistency, stemming from an\nincomplete understanding of scene affordance. For example, these methods\nfrequently misidentify vehicles that do not impact the ego vehicle as risk\nobjects. Furthermore, existing behavior change-based methods are inefficient\nbecause they implement causal inference in the perspective image space. We\npropose a new framework with a Bird's Eye View (BEV) representation to overcome\nthe above challenges. Specifically, we utilize potential fields as scene\naffordance, involving repulsive forces derived from road infrastructure and\ntraffic participants, along with attractive forces sourced from target\ndestinations. In this work, we compute potential fields by assigning different\nenergy levels according to the semantic labels obtained from BEV semantic\nsegmentation. We conduct thorough experiments and ablation studies, comparing\nthe proposed method with various state-of-the-art algorithms on both synthetic\nand real-world datasets. Our results show a notable increase in spatial and\ntemporal consistency, with enhancements of 20.3% and 11.6% on the RiskBench\ndataset, respectively. Additionally, we can improve computational efficiency by\n88%. We achieve improvements of 5.4% in spatial accuracy and 7.2% in temporal\nconsistency on the nuScenes dataset."
                },
                "authors": [
                    {
                        "name": "Pang-Yuan Pao"
                    },
                    {
                        "name": "Shu-Wei Lu"
                    },
                    {
                        "name": "Ze-Yan Lu"
                    },
                    {
                        "name": "Yi-Ting Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yi-Ting Chen"
                },
                "author": "Yi-Ting Chen",
                "arxiv_comment": "8 pages, 4 figures. In the proceedings of the IEEE International\n  Conference on Robotics and Automation (ICRA) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15846v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15846v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19143v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19143v1",
                "updated": "2025-02-26T13:58:22Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    13,
                    58,
                    22,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-26T13:58:22Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    13,
                    58,
                    22,
                    2,
                    57,
                    0
                ],
                "title": "Language-Parametric Reference Synthesis (Extended)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language-Parametric Reference Synthesis (Extended)"
                },
                "summary": "Modern Integrated Development Environments (IDEs) offer automated\nrefactorings to aid programmers in developing and maintaining software.\nHowever, implementing sound automated refactorings is challenging, as\nrefactorings may inadvertently introduce name-binding errors or cause\nreferences to resolve to incorrect declarations. To address these issues,\nprevious work by Sch\\\"afer et al. proposed replacing concrete references with\nlocked references to separate binding preservation from transformation. Locked\nreferences vacuously resolve to a specific declaration, and after\ntransformation must be replaced with concrete references that also resolve to\nthat declaration. Synthesizing these references requires a faithful inverse of\nthe name lookup functions of the underlying language.\n  Manually implementing such inverse lookup functions is challenging due to the\ncomplex name-binding features in modern programming languages. Instead, we\npropose to automatically derive this function from type system specifications\nwritten in the Statix meta-DSL. To guide the synthesis of qualified references\nwe use scope graphs, which represent the binding structure of a program, to\ninfer their names and discover their syntactic structure.\n  We evaluate our approach by synthesizing concrete references for locked\nreferences in 2528 Java, 196 ChocoPy, and 49 Featherweight Generic Java test\nprograms. Our approach yields a principled language-parametric method for\nsynthesizing references.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern Integrated Development Environments (IDEs) offer automated\nrefactorings to aid programmers in developing and maintaining software.\nHowever, implementing sound automated refactorings is challenging, as\nrefactorings may inadvertently introduce name-binding errors or cause\nreferences to resolve to incorrect declarations. To address these issues,\nprevious work by Sch\\\"afer et al. proposed replacing concrete references with\nlocked references to separate binding preservation from transformation. Locked\nreferences vacuously resolve to a specific declaration, and after\ntransformation must be replaced with concrete references that also resolve to\nthat declaration. Synthesizing these references requires a faithful inverse of\nthe name lookup functions of the underlying language.\n  Manually implementing such inverse lookup functions is challenging due to the\ncomplex name-binding features in modern programming languages. Instead, we\npropose to automatically derive this function from type system specifications\nwritten in the Statix meta-DSL. To guide the synthesis of qualified references\nwe use scope graphs, which represent the binding structure of a program, to\ninfer their names and discover their syntactic structure.\n  We evaluate our approach by synthesizing concrete references for locked\nreferences in 2528 Java, 196 ChocoPy, and 49 Featherweight Generic Java test\nprograms. Our approach yields a principled language-parametric method for\nsynthesizing references."
                },
                "authors": [
                    {
                        "name": "Daniel A. A. Pelsmaeker"
                    },
                    {
                        "name": "Aron Zwaan"
                    },
                    {
                        "name": "Casper Bach"
                    },
                    {
                        "name": "Arjan J. Mooij"
                    }
                ],
                "author_detail": {
                    "name": "Arjan J. Mooij"
                },
                "author": "Arjan J. Mooij",
                "arxiv_doi": "10.1145/3720481",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3720481",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.19143v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19143v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "31 pages, 13 figures, This is an extended version of the paper\n  published at PACMPL OOPSLA'25: https://doi.org/10.1145/3720481",
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.01881v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.01881v3",
                "updated": "2025-02-26T13:57:13Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    13,
                    57,
                    13,
                    2,
                    57,
                    0
                ],
                "published": "2024-02-02T20:12:05Z",
                "published_parsed": [
                    2024,
                    2,
                    2,
                    20,
                    12,
                    5,
                    4,
                    33,
                    0
                ],
                "title": "Large Language Model Agent for Hyper-Parameter Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model Agent for Hyper-Parameter Optimization"
                },
                "summary": "Hyperparameter optimization is critical in modern machine learning, requiring\nexpert knowledge, numerous trials, and high computational and human resources.\nDespite the advancements in Automated Machine Learning (AutoML), challenges in\nterms of trial efficiency, setup complexity, and interoperability still\npersist. To address these issues, we introduce a novel paradigm leveraging\nLarge Language Models (LLMs) to automate hyperparameter optimization across\ndiverse machine learning tasks, which is named AgentHPO (short for LLM\nAgent-based Hyperparameter Optimization). Specifically, AgentHPO processes the\ntask information autonomously, conducts experiments with specific\nhyperparameters (HPs), and iteratively optimizes them based on historical\ntrials. This human-like optimization process largely reduces the number of\nrequired trials, simplifies the setup process, and enhances interpretability\nand user trust, compared to traditional AutoML methods. Extensive empirical\nexperiments conducted on 12 representative machine-learning tasks indicate that\nAgentHPO not only matches but also often surpasses the best human trials in\nterms of performance while simultaneously providing explainable results.\nFurther analysis sheds light on the strategies employed by the LLM in\noptimizing these tasks, highlighting its effectiveness and adaptability in\nvarious scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hyperparameter optimization is critical in modern machine learning, requiring\nexpert knowledge, numerous trials, and high computational and human resources.\nDespite the advancements in Automated Machine Learning (AutoML), challenges in\nterms of trial efficiency, setup complexity, and interoperability still\npersist. To address these issues, we introduce a novel paradigm leveraging\nLarge Language Models (LLMs) to automate hyperparameter optimization across\ndiverse machine learning tasks, which is named AgentHPO (short for LLM\nAgent-based Hyperparameter Optimization). Specifically, AgentHPO processes the\ntask information autonomously, conducts experiments with specific\nhyperparameters (HPs), and iteratively optimizes them based on historical\ntrials. This human-like optimization process largely reduces the number of\nrequired trials, simplifies the setup process, and enhances interpretability\nand user trust, compared to traditional AutoML methods. Extensive empirical\nexperiments conducted on 12 representative machine-learning tasks indicate that\nAgentHPO not only matches but also often surpasses the best human trials in\nterms of performance while simultaneously providing explainable results.\nFurther analysis sheds light on the strategies employed by the LLM in\noptimizing these tasks, highlighting its effectiveness and adaptability in\nvarious scenarios."
                },
                "authors": [
                    {
                        "name": "Siyi Liu"
                    },
                    {
                        "name": "Chen Gao"
                    },
                    {
                        "name": "Yong Li"
                    }
                ],
                "author_detail": {
                    "name": "Yong Li"
                },
                "author": "Yong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.01881v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.01881v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.05516v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.05516v2",
                "updated": "2025-02-26T13:56:16Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    13,
                    56,
                    16,
                    2,
                    57,
                    0
                ],
                "published": "2024-06-08T16:35:31Z",
                "published_parsed": [
                    2024,
                    6,
                    8,
                    16,
                    35,
                    31,
                    5,
                    160,
                    0
                ],
                "title": "Verbalized Probabilistic Graphical Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Verbalized Probabilistic Graphical Modeling"
                },
                "summary": "Human cognition excels at transcending sensory input and forming latent\nrepresentations that structure our understanding of the world. Although Large\nLanguage Models (LLMs) can produce chain-of-thought reasoning, they lack a\nprincipled framework to capture latent structures and model uncertainty,\nespecially in compositional reasoning tasks. We propose Verbalized\nProbabilistic Graphical Modeling (vPGM), a Bayesian prompting framework that\nguides LLMs to simulate key principles of Probabilistic Graphical Models (PGMs)\nin natural language. Unlike many traditional probabilistic methods requiring\nsubstantial domain expertise or specialized training, vPGM bypasses\nexpert-driven model design, making it well-suited for scenarios with limited\nassumptions or scarce data. We evaluated our model on several compositional\nreasoning tasks, both close-ended and open-ended. Our results indicate that the\nmodel effectively enhances confidence calibration and text generation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human cognition excels at transcending sensory input and forming latent\nrepresentations that structure our understanding of the world. Although Large\nLanguage Models (LLMs) can produce chain-of-thought reasoning, they lack a\nprincipled framework to capture latent structures and model uncertainty,\nespecially in compositional reasoning tasks. We propose Verbalized\nProbabilistic Graphical Modeling (vPGM), a Bayesian prompting framework that\nguides LLMs to simulate key principles of Probabilistic Graphical Models (PGMs)\nin natural language. Unlike many traditional probabilistic methods requiring\nsubstantial domain expertise or specialized training, vPGM bypasses\nexpert-driven model design, making it well-suited for scenarios with limited\nassumptions or scarce data. We evaluated our model on several compositional\nreasoning tasks, both close-ended and open-ended. Our results indicate that the\nmodel effectively enhances confidence calibration and text generation quality."
                },
                "authors": [
                    {
                        "name": "Hengguan Huang"
                    },
                    {
                        "name": "Xing Shen"
                    },
                    {
                        "name": "Songtao Wang"
                    },
                    {
                        "name": "Lingfa Meng"
                    },
                    {
                        "name": "Dianbo Liu"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Samir Bhatt"
                    }
                ],
                "author_detail": {
                    "name": "Samir Bhatt"
                },
                "author": "Samir Bhatt",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.05516v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.05516v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02551v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02551v2",
                "updated": "2025-02-26T13:51:56Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    13,
                    51,
                    56,
                    2,
                    57,
                    0
                ],
                "published": "2024-10-03T14:55:22Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    14,
                    55,
                    22,
                    3,
                    277,
                    0
                ],
                "title": "ColaCare: Enhancing Electronic Health Record Modeling through Large\n  Language Model-Driven Multi-Agent Collaboration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ColaCare: Enhancing Electronic Health Record Modeling through Large\n  Language Model-Driven Multi-Agent Collaboration"
                },
                "summary": "We introduce ColaCare, a framework that enhances Electronic Health Record\n(EHR) modeling through multi-agent collaboration driven by Large Language\nModels (LLMs). Our approach seamlessly integrates domain-specific expert models\nwith LLMs to bridge the gap between structured EHR data and text-based\nreasoning. Inspired by the Multidisciplinary Team (MDT) approach used in\nclinical settings, ColaCare employs two types of agents: DoctorAgents and a\nMetaAgent, which collaboratively analyze patient data. Expert models process\nand generate predictions from numerical EHR data, while LLM agents produce\nreasoning references and decision-making reports within the MDT-driven\ncollaborative consultation framework. The MetaAgent orchestrates the\ndiscussion, facilitating consultations and evidence-based debates among\nDoctorAgents, simulating diverse expertise in clinical decision-making. We\nadditionally incorporate the Merck Manual of Diagnosis and Therapy (MSD)\nmedical guideline within a retrieval-augmented generation (RAG) module for\nmedical evidence support, addressing the challenge of knowledge currency.\nExtensive experiments conducted on three EHR datasets demonstrate ColaCare's\nsuperior performance in clinical mortality outcome and readmission prediction\ntasks, underscoring its potential to revolutionize clinical decision support\nsystems and advance personalized precision medicine. All code, case studies and\na questionnaire are available at the project website:\nhttps://colacare.netlify.app.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce ColaCare, a framework that enhances Electronic Health Record\n(EHR) modeling through multi-agent collaboration driven by Large Language\nModels (LLMs). Our approach seamlessly integrates domain-specific expert models\nwith LLMs to bridge the gap between structured EHR data and text-based\nreasoning. Inspired by the Multidisciplinary Team (MDT) approach used in\nclinical settings, ColaCare employs two types of agents: DoctorAgents and a\nMetaAgent, which collaboratively analyze patient data. Expert models process\nand generate predictions from numerical EHR data, while LLM agents produce\nreasoning references and decision-making reports within the MDT-driven\ncollaborative consultation framework. The MetaAgent orchestrates the\ndiscussion, facilitating consultations and evidence-based debates among\nDoctorAgents, simulating diverse expertise in clinical decision-making. We\nadditionally incorporate the Merck Manual of Diagnosis and Therapy (MSD)\nmedical guideline within a retrieval-augmented generation (RAG) module for\nmedical evidence support, addressing the challenge of knowledge currency.\nExtensive experiments conducted on three EHR datasets demonstrate ColaCare's\nsuperior performance in clinical mortality outcome and readmission prediction\ntasks, underscoring its potential to revolutionize clinical decision support\nsystems and advance personalized precision medicine. All code, case studies and\na questionnaire are available at the project website:\nhttps://colacare.netlify.app."
                },
                "authors": [
                    {
                        "name": "Zixiang Wang"
                    },
                    {
                        "name": "Yinghao Zhu"
                    },
                    {
                        "name": "Huiya Zhao"
                    },
                    {
                        "name": "Xiaochen Zheng"
                    },
                    {
                        "name": "Dehao Sui"
                    },
                    {
                        "name": "Tianlong Wang"
                    },
                    {
                        "name": "Wen Tang"
                    },
                    {
                        "name": "Yasha Wang"
                    },
                    {
                        "name": "Ewen Harrison"
                    },
                    {
                        "name": "Chengwei Pan"
                    },
                    {
                        "name": "Junyi Gao"
                    },
                    {
                        "name": "Liantao Ma"
                    }
                ],
                "author_detail": {
                    "name": "Liantao Ma"
                },
                "author": "Liantao Ma",
                "arxiv_doi": "10.1145/3696410.3714877",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3696410.3714877",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2410.02551v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02551v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "ACM TheWebConf 2025 Conference (WWW 2025) Research Track",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19135v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19135v1",
                "updated": "2025-02-26T13:51:28Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    13,
                    51,
                    28,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-26T13:51:28Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    13,
                    51,
                    28,
                    2,
                    57,
                    0
                ],
                "title": "A Temporal Planning Framework for Multi-Agent Systems via LLM-Aided\n  Knowledge Base Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Temporal Planning Framework for Multi-Agent Systems via LLM-Aided\n  Knowledge Base Management"
                },
                "summary": "This paper presents a novel framework, called PLANTOR (PLanning with Natural\nlanguage for Task-Oriented Robots), that integrates Large Language Models\n(LLMs) with Prolog-based knowledge management and planning for multi-robot\ntasks. The system employs a two-phase generation of a robot-oriented knowledge\nbase, ensuring reusability and compositional reasoning, as well as a three-step\nplanning procedure that handles temporal dependencies, resource constraints,\nand parallel task execution via mixed-integer linear programming. The final\nplan is converted into a Behaviour Tree for direct use in ROS2. We tested the\nframework in multi-robot assembly tasks within a block world and an\narch-building scenario. Results demonstrate that LLMs can produce accurate\nknowledge bases with modest human feedback, while Prolog guarantees formal\ncorrectness and explainability. This approach underscores the potential of LLM\nintegration for advanced robotics tasks requiring flexible, scalable, and\nhuman-understandable planning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a novel framework, called PLANTOR (PLanning with Natural\nlanguage for Task-Oriented Robots), that integrates Large Language Models\n(LLMs) with Prolog-based knowledge management and planning for multi-robot\ntasks. The system employs a two-phase generation of a robot-oriented knowledge\nbase, ensuring reusability and compositional reasoning, as well as a three-step\nplanning procedure that handles temporal dependencies, resource constraints,\nand parallel task execution via mixed-integer linear programming. The final\nplan is converted into a Behaviour Tree for direct use in ROS2. We tested the\nframework in multi-robot assembly tasks within a block world and an\narch-building scenario. Results demonstrate that LLMs can produce accurate\nknowledge bases with modest human feedback, while Prolog guarantees formal\ncorrectness and explainability. This approach underscores the potential of LLM\nintegration for advanced robotics tasks requiring flexible, scalable, and\nhuman-understandable planning."
                },
                "authors": [
                    {
                        "name": "Enrico Saccon"
                    },
                    {
                        "name": "Ahmet Tikna"
                    },
                    {
                        "name": "Davide De Martini"
                    },
                    {
                        "name": "Edoardo Lamon"
                    },
                    {
                        "name": "Luigi Palopoli"
                    },
                    {
                        "name": "Marco Roveri"
                    }
                ],
                "author_detail": {
                    "name": "Marco Roveri"
                },
                "author": "Marco Roveri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19135v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19135v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19133v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19133v1",
                "updated": "2025-02-26T13:44:15Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    13,
                    44,
                    15,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-26T13:44:15Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    13,
                    44,
                    15,
                    2,
                    57,
                    0
                ],
                "title": "DBox: Scaffolding Algorithmic Programming Learning through Learner-LLM\n  Co-Decomposition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DBox: Scaffolding Algorithmic Programming Learning through Learner-LLM\n  Co-Decomposition"
                },
                "summary": "Decomposition is a fundamental skill in algorithmic programming, requiring\nlearners to break down complex problems into smaller, manageable parts.\nHowever, current self-study methods, such as browsing reference solutions or\nusing LLM assistants, often provide excessive or generic assistance that\nmisaligns with learners' decomposition strategies, hindering independent\nproblem-solving and critical thinking. To address this, we introduce\nDecomposition Box (DBox), an interactive LLM-based system that scaffolds and\nadapts to learners' personalized construction of a step tree through a\n\"learner-LLM co-decomposition\" approach, providing tailored support at an\nappropriate level. A within-subjects study (N=24) found that compared to the\nbaseline, DBox significantly improved learning gains, cognitive engagement, and\ncritical thinking. Learners also reported a stronger sense of achievement and\nfound the assistance appropriate and helpful for learning. Additionally, we\nexamined DBox's impact on cognitive load, identified usage patterns, and\nanalyzed learners' strategies for managing system errors. We conclude with\ndesign implications for future AI-powered tools to better support algorithmic\nprogramming education.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decomposition is a fundamental skill in algorithmic programming, requiring\nlearners to break down complex problems into smaller, manageable parts.\nHowever, current self-study methods, such as browsing reference solutions or\nusing LLM assistants, often provide excessive or generic assistance that\nmisaligns with learners' decomposition strategies, hindering independent\nproblem-solving and critical thinking. To address this, we introduce\nDecomposition Box (DBox), an interactive LLM-based system that scaffolds and\nadapts to learners' personalized construction of a step tree through a\n\"learner-LLM co-decomposition\" approach, providing tailored support at an\nappropriate level. A within-subjects study (N=24) found that compared to the\nbaseline, DBox significantly improved learning gains, cognitive engagement, and\ncritical thinking. Learners also reported a stronger sense of achievement and\nfound the assistance appropriate and helpful for learning. Additionally, we\nexamined DBox's impact on cognitive load, identified usage patterns, and\nanalyzed learners' strategies for managing system errors. We conclude with\ndesign implications for future AI-powered tools to better support algorithmic\nprogramming education."
                },
                "authors": [
                    {
                        "name": "Shuai Ma"
                    },
                    {
                        "name": "Junling Wang"
                    },
                    {
                        "name": "Yuanhao Zhang"
                    },
                    {
                        "name": "Xiaojuan Ma"
                    },
                    {
                        "name": "April Yi Wang"
                    }
                ],
                "author_detail": {
                    "name": "April Yi Wang"
                },
                "author": "April Yi Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19133v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19133v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19132v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19132v1",
                "updated": "2025-02-26T13:43:09Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    13,
                    43,
                    9,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-26T13:43:09Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    13,
                    43,
                    9,
                    2,
                    57,
                    0
                ],
                "title": "Bayesian optimization to infer parameters in viscoelasticity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian optimization to infer parameters in viscoelasticity"
                },
                "summary": "Inferring viscoelasticity parameters is a key challenge that often leads to\nnon-unique solutions when fitting rheological data. In this context, we propose\na machine learning approach that utilizes Bayesian optimization for parameter\ninference during curve-fitting processes. To fit a viscoelastic model to\nrheological data, the Bayesian optimization maps the parameter values to a\ngiven error function. It then exploits the mapped space to identify parameter\ncombinations that minimize the error. We compare the Bayesian optimization\nresults to traditional fitting routines and demonstrate that our approach finds\nthe fitting parameters in a less or similar number of iterations. Furthermore,\nit also creates a \"white-box\" and supervised framework for parameter estimation\nin linear viscoelasticity modeling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inferring viscoelasticity parameters is a key challenge that often leads to\nnon-unique solutions when fitting rheological data. In this context, we propose\na machine learning approach that utilizes Bayesian optimization for parameter\ninference during curve-fitting processes. To fit a viscoelastic model to\nrheological data, the Bayesian optimization maps the parameter values to a\ngiven error function. It then exploits the mapped space to identify parameter\ncombinations that minimize the error. We compare the Bayesian optimization\nresults to traditional fitting routines and demonstrate that our approach finds\nthe fitting parameters in a less or similar number of iterations. Furthermore,\nit also creates a \"white-box\" and supervised framework for parameter estimation\nin linear viscoelasticity modeling."
                },
                "authors": [
                    {
                        "name": "Isaac Y. Miranda-Valdez"
                    },
                    {
                        "name": "Tero Mäkinen"
                    },
                    {
                        "name": "Juha Koivisto"
                    },
                    {
                        "name": "Mikko J. Alava"
                    }
                ],
                "author_detail": {
                    "name": "Mikko J. Alava"
                },
                "author": "Mikko J. Alava",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19132v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19132v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.soft",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.soft",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.02228v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.02228v3",
                "updated": "2025-02-26T13:42:46Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    13,
                    42,
                    46,
                    2,
                    57,
                    0
                ],
                "published": "2024-04-02T18:32:51Z",
                "published_parsed": [
                    2024,
                    4,
                    2,
                    18,
                    32,
                    51,
                    1,
                    93,
                    0
                ],
                "title": "Seemingly unrelated Bayesian additive regression trees for\n  cost-effectiveness analyses in healthcare",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Seemingly unrelated Bayesian additive regression trees for\n  cost-effectiveness analyses in healthcare"
                },
                "summary": "In recent years, theoretical results and simulation evidence have shown\nBayesian additive regression trees to be a highly-effective method for\nnonparametric regression. Motivated by cost-effectiveness analyses in health\neconomics, where interest lies in jointly modelling the costs of healthcare\ntreatments and the associated health-related quality of life experienced by a\npatient, we propose a multivariate extension of BART which is applicable in\nregression analyses with several dependent outcome variables. Our framework\nallows for continuous or binary outcomes and overcomes some key limitations of\nexisting multivariate BART models by allowing each individual response to be\nassociated with different ensembles of trees, while still handling dependencies\nbetween the outcomes. In the case of continuous outcomes, our model is\nessentially a nonparametric version of seemingly unrelated regression.\nLikewise, our proposal for binary outcomes is a nonparametric generalisation of\nthe multivariate probit model. We give suggestions for easily interpretable\nprior distributions, which allow specification of both informative and\nuninformative priors. We provide detailed discussions of MCMC sampling methods\nto conduct posterior inference. Our methods are implemented in the R package\n\"subart\". We showcase their performance through extensive simulation\nexperiments and an application to an empirical case study from health\neconomics. By also accommodating propensity scores in a manner befitting a\ncausal analysis, we find substantial evidence for a novel trauma care\nintervention's cost-effectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, theoretical results and simulation evidence have shown\nBayesian additive regression trees to be a highly-effective method for\nnonparametric regression. Motivated by cost-effectiveness analyses in health\neconomics, where interest lies in jointly modelling the costs of healthcare\ntreatments and the associated health-related quality of life experienced by a\npatient, we propose a multivariate extension of BART which is applicable in\nregression analyses with several dependent outcome variables. Our framework\nallows for continuous or binary outcomes and overcomes some key limitations of\nexisting multivariate BART models by allowing each individual response to be\nassociated with different ensembles of trees, while still handling dependencies\nbetween the outcomes. In the case of continuous outcomes, our model is\nessentially a nonparametric version of seemingly unrelated regression.\nLikewise, our proposal for binary outcomes is a nonparametric generalisation of\nthe multivariate probit model. We give suggestions for easily interpretable\nprior distributions, which allow specification of both informative and\nuninformative priors. We provide detailed discussions of MCMC sampling methods\nto conduct posterior inference. Our methods are implemented in the R package\n\"subart\". We showcase their performance through extensive simulation\nexperiments and an application to an empirical case study from health\neconomics. By also accommodating propensity scores in a manner befitting a\ncausal analysis, we find substantial evidence for a novel trauma care\nintervention's cost-effectiveness."
                },
                "authors": [
                    {
                        "name": "Jonas Esser"
                    },
                    {
                        "name": "Mateus Maia"
                    },
                    {
                        "name": "Andrew C. Parnell"
                    },
                    {
                        "name": "Judith Bosmans"
                    },
                    {
                        "name": "Hanneke van Dongen"
                    },
                    {
                        "name": "Thomas Klausch"
                    },
                    {
                        "name": "Keefe Murphy"
                    }
                ],
                "author_detail": {
                    "name": "Keefe Murphy"
                },
                "author": "Keefe Murphy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.02228v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.02228v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.01833v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.01833v3",
                "updated": "2025-02-26T13:41:41Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    13,
                    41,
                    41,
                    2,
                    57,
                    0
                ],
                "published": "2024-04-02T10:45:49Z",
                "published_parsed": [
                    2024,
                    4,
                    2,
                    10,
                    45,
                    49,
                    1,
                    93,
                    0
                ],
                "title": "Great, Now Write an Article About That: The Crescendo Multi-Turn LLM\n  Jailbreak Attack",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Great, Now Write an Article About That: The Crescendo Multi-Turn LLM\n  Jailbreak Attack"
                },
                "summary": "Large Language Models (LLMs) have risen significantly in popularity and are\nincreasingly being adopted across multiple applications. These LLMs are heavily\naligned to resist engaging in illegal or unethical topics as a means to avoid\ncontributing to responsible AI harms. However, a recent line of attacks, known\nas jailbreaks, seek to overcome this alignment. Intuitively, jailbreak attacks\naim to narrow the gap between what the model can do and what it is willing to\ndo. In this paper, we introduce a novel jailbreak attack called Crescendo.\nUnlike existing jailbreak methods, Crescendo is a simple multi-turn jailbreak\nthat interacts with the model in a seemingly benign manner. It begins with a\ngeneral prompt or question about the task at hand and then gradually escalates\nthe dialogue by referencing the model's replies progressively leading to a\nsuccessful jailbreak. We evaluate Crescendo on various public systems,\nincluding ChatGPT, Gemini Pro, Gemini-Ultra, LlaMA-2 70b and LlaMA-3 70b Chat,\nand Anthropic Chat. Our results demonstrate the strong efficacy of Crescendo,\nwith it achieving high attack success rates across all evaluated models and\ntasks. Furthermore, we present Crescendomation, a tool that automates the\nCrescendo attack and demonstrate its efficacy against state-of-the-art models\nthrough our evaluations. Crescendomation surpasses other state-of-the-art\njailbreaking techniques on the AdvBench subset dataset, achieving 29-61% higher\nperformance on GPT-4 and 49-71% on Gemini-Pro. Finally, we also demonstrate\nCrescendo's ability to jailbreak multimodal models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have risen significantly in popularity and are\nincreasingly being adopted across multiple applications. These LLMs are heavily\naligned to resist engaging in illegal or unethical topics as a means to avoid\ncontributing to responsible AI harms. However, a recent line of attacks, known\nas jailbreaks, seek to overcome this alignment. Intuitively, jailbreak attacks\naim to narrow the gap between what the model can do and what it is willing to\ndo. In this paper, we introduce a novel jailbreak attack called Crescendo.\nUnlike existing jailbreak methods, Crescendo is a simple multi-turn jailbreak\nthat interacts with the model in a seemingly benign manner. It begins with a\ngeneral prompt or question about the task at hand and then gradually escalates\nthe dialogue by referencing the model's replies progressively leading to a\nsuccessful jailbreak. We evaluate Crescendo on various public systems,\nincluding ChatGPT, Gemini Pro, Gemini-Ultra, LlaMA-2 70b and LlaMA-3 70b Chat,\nand Anthropic Chat. Our results demonstrate the strong efficacy of Crescendo,\nwith it achieving high attack success rates across all evaluated models and\ntasks. Furthermore, we present Crescendomation, a tool that automates the\nCrescendo attack and demonstrate its efficacy against state-of-the-art models\nthrough our evaluations. Crescendomation surpasses other state-of-the-art\njailbreaking techniques on the AdvBench subset dataset, achieving 29-61% higher\nperformance on GPT-4 and 49-71% on Gemini-Pro. Finally, we also demonstrate\nCrescendo's ability to jailbreak multimodal models."
                },
                "authors": [
                    {
                        "name": "Mark Russinovich"
                    },
                    {
                        "name": "Ahmed Salem"
                    },
                    {
                        "name": "Ronen Eldan"
                    }
                ],
                "author_detail": {
                    "name": "Ronen Eldan"
                },
                "author": "Ronen Eldan",
                "arxiv_comment": "Accepted at USENIX Security 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.01833v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.01833v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19127v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19127v1",
                "updated": "2025-02-26T13:34:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    13,
                    34,
                    52,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-26T13:34:52Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    13,
                    34,
                    52,
                    2,
                    57,
                    0
                ],
                "title": "Self-Memory Alignment: Mitigating Factual Hallucinations with\n  Generalized Improvement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-Memory Alignment: Mitigating Factual Hallucinations with\n  Generalized Improvement"
                },
                "summary": "Large Language Models (LLMs) often struggle to align their responses with\nobjective facts, resulting in the issue of factual hallucinations, which can be\ndifficult to detect and mislead users without relevant knowledge. While\npost-training techniques have been employed to mitigate the issue, existing\nmethods usually suffer from poor generalization and trade-offs in different\ncapabilities. In this paper, we propose to address it by directly augmenting\nLLM's fundamental ability to precisely leverage its existing memory--the\nknowledge acquired from pre-training data. We introduce self-memory alignment\n(SMA), which fine-tunes the model on self-generated responses to precise and\nsimple factual questions through preference optimization. Furthermore, we\nconstruct FactualBench, a comprehensive and precise factual QA dataset\ncontaining 181k Chinese data spanning 21 domains, to facilitate both evaluation\nand training. Extensive experiments show that SMA significantly improves LLMs'\noverall performance, with consistent enhancement across various benchmarks\nconcerning factuality, as well as helpfulness and comprehensive skills.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) often struggle to align their responses with\nobjective facts, resulting in the issue of factual hallucinations, which can be\ndifficult to detect and mislead users without relevant knowledge. While\npost-training techniques have been employed to mitigate the issue, existing\nmethods usually suffer from poor generalization and trade-offs in different\ncapabilities. In this paper, we propose to address it by directly augmenting\nLLM's fundamental ability to precisely leverage its existing memory--the\nknowledge acquired from pre-training data. We introduce self-memory alignment\n(SMA), which fine-tunes the model on self-generated responses to precise and\nsimple factual questions through preference optimization. Furthermore, we\nconstruct FactualBench, a comprehensive and precise factual QA dataset\ncontaining 181k Chinese data spanning 21 domains, to facilitate both evaluation\nand training. Extensive experiments show that SMA significantly improves LLMs'\noverall performance, with consistent enhancement across various benchmarks\nconcerning factuality, as well as helpfulness and comprehensive skills."
                },
                "authors": [
                    {
                        "name": "Siyuan Zhang"
                    },
                    {
                        "name": "Yichi Zhang"
                    },
                    {
                        "name": "Yinpeng Dong"
                    },
                    {
                        "name": "Hang Su"
                    }
                ],
                "author_detail": {
                    "name": "Hang Su"
                },
                "author": "Hang Su",
                "arxiv_comment": "29 pages, 17 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19127v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19127v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.00036v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.00036v2",
                "updated": "2025-02-26T13:18:09Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    13,
                    18,
                    9,
                    2,
                    57,
                    0
                ],
                "published": "2024-05-27T10:53:15Z",
                "published_parsed": [
                    2024,
                    5,
                    27,
                    10,
                    53,
                    15,
                    0,
                    148,
                    0
                ],
                "title": "EMERGE: Enhancing Multimodal Electronic Health Records Predictive\n  Modeling with Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EMERGE: Enhancing Multimodal Electronic Health Records Predictive\n  Modeling with Retrieval-Augmented Generation"
                },
                "summary": "The integration of multimodal Electronic Health Records (EHR) data has\nsignificantly advanced clinical predictive capabilities. Existing models, which\nutilize clinical notes and multivariate time-series EHR data, often fall short\nof incorporating the necessary medical context for accurate clinical tasks,\nwhile previous approaches with knowledge graphs (KGs) primarily focus on\nstructured knowledge extraction. In response, we propose EMERGE, a\nRetrieval-Augmented Generation (RAG) driven framework to enhance multimodal EHR\npredictive modeling. We extract entities from both time-series data and\nclinical notes by prompting Large Language Models (LLMs) and align them with\nprofessional PrimeKG, ensuring consistency. In addition to triplet\nrelationships, we incorporate entities' definitions and descriptions for richer\nsemantics. The extracted knowledge is then used to generate task-relevant\nsummaries of patients' health statuses. Finally, we fuse the summary with other\nmodalities using an adaptive multimodal fusion network with cross-attention.\nExtensive experiments on the MIMIC-III and MIMIC-IV datasets' in-hospital\nmortality and 30-day readmission tasks demonstrate the superior performance of\nthe EMERGE framework over baseline models. Comprehensive ablation studies and\nanalysis highlight the efficacy of each designed module and robustness to data\nsparsity. EMERGE contributes to refining the utilization of multimodal EHR data\nin healthcare, bridging the gap with nuanced medical contexts essential for\ninformed clinical predictions. We have publicly released the code at\nhttps://github.com/yhzhu99/EMERGE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of multimodal Electronic Health Records (EHR) data has\nsignificantly advanced clinical predictive capabilities. Existing models, which\nutilize clinical notes and multivariate time-series EHR data, often fall short\nof incorporating the necessary medical context for accurate clinical tasks,\nwhile previous approaches with knowledge graphs (KGs) primarily focus on\nstructured knowledge extraction. In response, we propose EMERGE, a\nRetrieval-Augmented Generation (RAG) driven framework to enhance multimodal EHR\npredictive modeling. We extract entities from both time-series data and\nclinical notes by prompting Large Language Models (LLMs) and align them with\nprofessional PrimeKG, ensuring consistency. In addition to triplet\nrelationships, we incorporate entities' definitions and descriptions for richer\nsemantics. The extracted knowledge is then used to generate task-relevant\nsummaries of patients' health statuses. Finally, we fuse the summary with other\nmodalities using an adaptive multimodal fusion network with cross-attention.\nExtensive experiments on the MIMIC-III and MIMIC-IV datasets' in-hospital\nmortality and 30-day readmission tasks demonstrate the superior performance of\nthe EMERGE framework over baseline models. Comprehensive ablation studies and\nanalysis highlight the efficacy of each designed module and robustness to data\nsparsity. EMERGE contributes to refining the utilization of multimodal EHR data\nin healthcare, bridging the gap with nuanced medical contexts essential for\ninformed clinical predictions. We have publicly released the code at\nhttps://github.com/yhzhu99/EMERGE."
                },
                "authors": [
                    {
                        "name": "Yinghao Zhu"
                    },
                    {
                        "name": "Changyu Ren"
                    },
                    {
                        "name": "Zixiang Wang"
                    },
                    {
                        "name": "Xiaochen Zheng"
                    },
                    {
                        "name": "Shiyun Xie"
                    },
                    {
                        "name": "Junlan Feng"
                    },
                    {
                        "name": "Xi Zhu"
                    },
                    {
                        "name": "Zhoujun Li"
                    },
                    {
                        "name": "Liantao Ma"
                    },
                    {
                        "name": "Chengwei Pan"
                    }
                ],
                "author_detail": {
                    "name": "Chengwei Pan"
                },
                "author": "Chengwei Pan",
                "arxiv_doi": "10.1145/3627673.3679582",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3627673.3679582",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2406.00036v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.00036v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "CIKM 2024 Full Research Paper; arXiv admin note: text overlap with\n  arXiv:2402.07016",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.09838v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.09838v2",
                "updated": "2025-02-26T13:07:05Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    13,
                    7,
                    5,
                    2,
                    57,
                    0
                ],
                "published": "2023-11-16T12:09:32Z",
                "published_parsed": [
                    2023,
                    11,
                    16,
                    12,
                    9,
                    32,
                    3,
                    320,
                    0
                ],
                "title": "Bayesian Inference of Reproduction Number from Epidemiological and\n  Genetic Data Using Particle MCMC",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian Inference of Reproduction Number from Epidemiological and\n  Genetic Data Using Particle MCMC"
                },
                "summary": "Inference of the reproduction number through time is of vital importance\nduring an epidemic outbreak. Typically, epidemiologists tackle this using\nobserved prevalence or incidence data. However, prevalence and incidence data\nalone is often noisy or partial. Models can also have identifiability issues\nwith determining whether a large amount of a small epidemic or a small amount\nof a large epidemic has been observed. Sequencing data however is becoming more\nabundant, so approaches which can incorporate genetic data are an active area\nof research. We propose using particle MCMC methods to infer the time-varying\nreproduction number from a combination of prevalence data reported at a set of\ndiscrete times and a dated phylogeny reconstructed from sequences. We validate\nour approach on simulated epidemics with a variety of scenarios. We then apply\nthe method to real data sets of HIV-1 in North Carolina, USA and tuberculosis\nin Buenos Aires, Argentina. The models and algorithms are implemented in an\nopen source R package called EpiSky which is available at\nhttps://github.com/alicia-gill/EpiSky.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference of the reproduction number through time is of vital importance\nduring an epidemic outbreak. Typically, epidemiologists tackle this using\nobserved prevalence or incidence data. However, prevalence and incidence data\nalone is often noisy or partial. Models can also have identifiability issues\nwith determining whether a large amount of a small epidemic or a small amount\nof a large epidemic has been observed. Sequencing data however is becoming more\nabundant, so approaches which can incorporate genetic data are an active area\nof research. We propose using particle MCMC methods to infer the time-varying\nreproduction number from a combination of prevalence data reported at a set of\ndiscrete times and a dated phylogeny reconstructed from sequences. We validate\nour approach on simulated epidemics with a variety of scenarios. We then apply\nthe method to real data sets of HIV-1 in North Carolina, USA and tuberculosis\nin Buenos Aires, Argentina. The models and algorithms are implemented in an\nopen source R package called EpiSky which is available at\nhttps://github.com/alicia-gill/EpiSky."
                },
                "authors": [
                    {
                        "name": "Alicia Gill"
                    },
                    {
                        "name": "Jere Koskela"
                    },
                    {
                        "name": "Xavier Didelot"
                    },
                    {
                        "name": "Richard G. Everitt"
                    }
                ],
                "author_detail": {
                    "name": "Richard G. Everitt"
                },
                "author": "Richard G. Everitt",
                "arxiv_comment": "24 pages, 11 figures (30 pages, 19 figures including appendices)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.09838v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.09838v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.PE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62P10, 65C05, 92D10, 92D30",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.16894v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.16894v2",
                "updated": "2025-02-26T13:02:48Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    13,
                    2,
                    48,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-24T06:48:13Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    6,
                    48,
                    13,
                    0,
                    55,
                    0
                ],
                "title": "Make LoRA Great Again: Boosting LoRA with Adaptive Singular Values and\n  Mixture-of-Experts Optimization Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Make LoRA Great Again: Boosting LoRA with Adaptive Singular Values and\n  Mixture-of-Experts Optimization Alignment"
                },
                "summary": "While Low-Rank Adaptation (LoRA) enables parameter-efficient fine-tuning for\nLarge Language Models (LLMs), its performance often falls short of Full\nFine-Tuning (Full FT). Current methods optimize LoRA by initializing with\nstatic singular value decomposition (SVD) subsets, leading to suboptimal\nleveraging of pre-trained knowledge. Another path for improving LoRA is\nincorporating a Mixture-of-Experts (MoE) architecture. However, weight\nmisalignment and complex gradient dynamics make it challenging to adopt SVD\nprior to the LoRA MoE architecture. To mitigate these issues, we propose\n\\underline{G}reat L\\underline{o}R\\underline{A} Mixture-of-Exper\\underline{t}\n(GOAT), a framework that (1) adaptively integrates relevant priors using an\nSVD-structured MoE, and (2) aligns optimization with full fine-tuned MoE by\nderiving a theoretical scaling factor. We demonstrate that proper scaling,\nwithout modifying the architecture or training algorithms, boosts LoRA MoE's\nefficiency and performance. Experiments across 25 datasets, including natural\nlanguage understanding, commonsense reasoning, image classification, and\nnatural language generation, demonstrate GOAT's state-of-the-art performance,\nclosing the gap with Full FT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Low-Rank Adaptation (LoRA) enables parameter-efficient fine-tuning for\nLarge Language Models (LLMs), its performance often falls short of Full\nFine-Tuning (Full FT). Current methods optimize LoRA by initializing with\nstatic singular value decomposition (SVD) subsets, leading to suboptimal\nleveraging of pre-trained knowledge. Another path for improving LoRA is\nincorporating a Mixture-of-Experts (MoE) architecture. However, weight\nmisalignment and complex gradient dynamics make it challenging to adopt SVD\nprior to the LoRA MoE architecture. To mitigate these issues, we propose\n\\underline{G}reat L\\underline{o}R\\underline{A} Mixture-of-Exper\\underline{t}\n(GOAT), a framework that (1) adaptively integrates relevant priors using an\nSVD-structured MoE, and (2) aligns optimization with full fine-tuned MoE by\nderiving a theoretical scaling factor. We demonstrate that proper scaling,\nwithout modifying the architecture or training algorithms, boosts LoRA MoE's\nefficiency and performance. Experiments across 25 datasets, including natural\nlanguage understanding, commonsense reasoning, image classification, and\nnatural language generation, demonstrate GOAT's state-of-the-art performance,\nclosing the gap with Full FT."
                },
                "authors": [
                    {
                        "name": "Chenghao Fan"
                    },
                    {
                        "name": "Zhenyi Lu"
                    },
                    {
                        "name": "Sichen Liu"
                    },
                    {
                        "name": "Xiaoye Qu"
                    },
                    {
                        "name": "Wei Wei"
                    },
                    {
                        "name": "Chengfeng Gu"
                    },
                    {
                        "name": "Yu Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Yu Cheng"
                },
                "author": "Yu Cheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.16894v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.16894v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19106v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19106v1",
                "updated": "2025-02-26T12:49:27Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    12,
                    49,
                    27,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-26T12:49:27Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    12,
                    49,
                    27,
                    2,
                    57,
                    0
                ],
                "title": "A Survey on Foundation-Model-Based Industrial Defect Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Foundation-Model-Based Industrial Defect Detection"
                },
                "summary": "As industrial products become abundant and sophisticated, visual industrial\ndefect detection receives much attention, including two-dimensional and\nthree-dimensional visual feature modeling. Traditional methods use statistical\nanalysis, abnormal data synthesis modeling, and generation-based models to\nseparate product defect features and complete defect detection. Recently, the\nemergence of foundation models has brought visual and textual semantic prior\nknowledge. Many methods are based on foundation models (FM) to improve the\naccuracy of detection, but at the same time, increase model complexity and slow\ndown inference speed. Some FM-based methods have begun to explore lightweight\nmodeling ways, which have gradually attracted attention and deserve to be\nsystematically analyzed. In this paper, we conduct a systematic survey with\ncomparisons and discussions of foundation model methods from different aspects\nand briefly review non-foundation model (NFM) methods recently published.\nFurthermore, we discuss the differences between FM and NFM methods from\ntraining objectives, model structure and scale, model performance, and\npotential directions for future exploration. Through comparison, we find FM\nmethods are more suitable for few-shot and zero-shot learning, which are more\nin line with actual industrial application scenarios and worthy of in-depth\nresearch.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As industrial products become abundant and sophisticated, visual industrial\ndefect detection receives much attention, including two-dimensional and\nthree-dimensional visual feature modeling. Traditional methods use statistical\nanalysis, abnormal data synthesis modeling, and generation-based models to\nseparate product defect features and complete defect detection. Recently, the\nemergence of foundation models has brought visual and textual semantic prior\nknowledge. Many methods are based on foundation models (FM) to improve the\naccuracy of detection, but at the same time, increase model complexity and slow\ndown inference speed. Some FM-based methods have begun to explore lightweight\nmodeling ways, which have gradually attracted attention and deserve to be\nsystematically analyzed. In this paper, we conduct a systematic survey with\ncomparisons and discussions of foundation model methods from different aspects\nand briefly review non-foundation model (NFM) methods recently published.\nFurthermore, we discuss the differences between FM and NFM methods from\ntraining objectives, model structure and scale, model performance, and\npotential directions for future exploration. Through comparison, we find FM\nmethods are more suitable for few-shot and zero-shot learning, which are more\nin line with actual industrial application scenarios and worthy of in-depth\nresearch."
                },
                "authors": [
                    {
                        "name": "Tianle Yang"
                    },
                    {
                        "name": "Luyao Chang"
                    },
                    {
                        "name": "Jiadong Yan"
                    },
                    {
                        "name": "Juntao Li"
                    },
                    {
                        "name": "Zhi Wang"
                    },
                    {
                        "name": "Ke Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ke Zhang"
                },
                "author": "Ke Zhang",
                "arxiv_comment": "14 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19106v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19106v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19104v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19104v1",
                "updated": "2025-02-26T12:46:59Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    12,
                    46,
                    59,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-26T12:46:59Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    12,
                    46,
                    59,
                    2,
                    57,
                    0
                ],
                "title": "Evaluating Gender Bias in German Machine Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Gender Bias in German Machine Translation"
                },
                "summary": "We present WinoMTDE, a new gender bias evaluation test set designed to assess\noccupational stereotyping and underrepresentation in German machine translation\n(MT) systems. Building on the automatic evaluation method introduced by\narXiv:1906.00591v1 [cs.CL], we extend the approach to German, a language with\ngrammatical gender. The WinoMTDE dataset comprises 288 German sentences that\nare balanced in regard to gender, as well as stereotype, which was annotated\nusing German labor statistics. We conduct a large-scale evaluation of five\nwidely used MT systems and a large language model. Our results reveal\npersistent bias in most models, with the LLM outperforming traditional systems.\nThe dataset and evaluation code are publicly available under\nhttps://github.com/michellekappl/mt_gender_german.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present WinoMTDE, a new gender bias evaluation test set designed to assess\noccupational stereotyping and underrepresentation in German machine translation\n(MT) systems. Building on the automatic evaluation method introduced by\narXiv:1906.00591v1 [cs.CL], we extend the approach to German, a language with\ngrammatical gender. The WinoMTDE dataset comprises 288 German sentences that\nare balanced in regard to gender, as well as stereotype, which was annotated\nusing German labor statistics. We conduct a large-scale evaluation of five\nwidely used MT systems and a large language model. Our results reveal\npersistent bias in most models, with the LLM outperforming traditional systems.\nThe dataset and evaluation code are publicly available under\nhttps://github.com/michellekappl/mt_gender_german."
                },
                "authors": [
                    {
                        "name": "Michelle Kappl"
                    }
                ],
                "author_detail": {
                    "name": "Michelle Kappl"
                },
                "author": "Michelle Kappl",
                "arxiv_comment": "ISCA/ITG Workshop on Diversity in Large Speech and Language Models",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19104v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19104v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19103v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19103v1",
                "updated": "2025-02-26T12:46:36Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    12,
                    46,
                    36,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-26T12:46:36Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    12,
                    46,
                    36,
                    2,
                    57,
                    0
                ],
                "title": "LongEval: A Comprehensive Analysis of Long-Text Generation Through a\n  Plan-based Paradigm",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LongEval: A Comprehensive Analysis of Long-Text Generation Through a\n  Plan-based Paradigm"
                },
                "summary": "Large Language Models (LLMs) have achieved remarkable success in various\nnatural language processing tasks, yet their ability to generate long-form\ncontent remains poorly understood and evaluated. Our analysis reveals that\ncurrent LLMs struggle with length requirements and information density in\nlong-text generation, with performance deteriorating as text length increases.\nTo quantitively locate such a performance degradation and provide further\ninsights on model development, we present LongEval, a benchmark that evaluates\nlong-text generation through both direct and plan-based generation paradigms,\ninspired by cognitive and linguistic writing models. The comprehensive\nexperiments in this work reveal interesting findings such as that while model\nsize correlates with generation ability, the small-scale model (e.g.,\nLongWriter), well-trained on long texts, has comparable performance. All code\nand datasets are released in https://github.com/Wusiwei0410/LongEval.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved remarkable success in various\nnatural language processing tasks, yet their ability to generate long-form\ncontent remains poorly understood and evaluated. Our analysis reveals that\ncurrent LLMs struggle with length requirements and information density in\nlong-text generation, with performance deteriorating as text length increases.\nTo quantitively locate such a performance degradation and provide further\ninsights on model development, we present LongEval, a benchmark that evaluates\nlong-text generation through both direct and plan-based generation paradigms,\ninspired by cognitive and linguistic writing models. The comprehensive\nexperiments in this work reveal interesting findings such as that while model\nsize correlates with generation ability, the small-scale model (e.g.,\nLongWriter), well-trained on long texts, has comparable performance. All code\nand datasets are released in https://github.com/Wusiwei0410/LongEval."
                },
                "authors": [
                    {
                        "name": "Siwei Wu"
                    },
                    {
                        "name": "Yizhi Li"
                    },
                    {
                        "name": "Xingwei Qu"
                    },
                    {
                        "name": "Rishi Ravikumar"
                    },
                    {
                        "name": "Yucheng Li"
                    },
                    {
                        "name": "Tyler Loakman Shanghaoran Quan Xiaoyong Wei"
                    },
                    {
                        "name": "Riza Batista-Navarro"
                    },
                    {
                        "name": "Chenghua Lin"
                    }
                ],
                "author_detail": {
                    "name": "Chenghua Lin"
                },
                "author": "Chenghua Lin",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19103v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19103v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19098v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19098v1",
                "updated": "2025-02-26T12:43:22Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    12,
                    43,
                    22,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-26T12:43:22Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    12,
                    43,
                    22,
                    2,
                    57,
                    0
                ],
                "title": "Language-Driven Opinion Dynamics in Agent-Based Simulations with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language-Driven Opinion Dynamics in Agent-Based Simulations with LLMs"
                },
                "summary": "Understanding how opinions evolve is crucial for addressing issues such as\npolarization, radicalization, and consensus in social systems. While much\nresearch has focused on identifying factors influencing opinion change, the\nrole of language and argumentative fallacies remains underexplored. This paper\naims to fill this gap by investigating how language - along with social\ndynamics - influences opinion evolution through LODAS, a Language-Driven\nOpinion Dynamics Model for Agent-Based Simulations. The model simulates debates\naround the \"Ship of Theseus\" paradox, in which agents with discrete opinions\ninteract with each other and evolve their opinions by accepting, rejecting, or\nignoring the arguments presented. We study three different scenarios: balanced,\npolarized, and unbalanced opinion distributions. Agreeableness and sycophancy\nemerge as two main characteristics of LLM agents, and consensus around the\npresented statement emerges almost in any setting. Moreover, such AI agents are\noften producers of fallacious arguments in the attempt of persuading their\npeers and - for their complacency - they are also highly influenced by\narguments built on logical fallacies. These results highlight the potential of\nthis framework not only for simulating social dynamics but also for exploring\nfrom another perspective biases and shortcomings of LLMs, which may impact\ntheir interactions with humans.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding how opinions evolve is crucial for addressing issues such as\npolarization, radicalization, and consensus in social systems. While much\nresearch has focused on identifying factors influencing opinion change, the\nrole of language and argumentative fallacies remains underexplored. This paper\naims to fill this gap by investigating how language - along with social\ndynamics - influences opinion evolution through LODAS, a Language-Driven\nOpinion Dynamics Model for Agent-Based Simulations. The model simulates debates\naround the \"Ship of Theseus\" paradox, in which agents with discrete opinions\ninteract with each other and evolve their opinions by accepting, rejecting, or\nignoring the arguments presented. We study three different scenarios: balanced,\npolarized, and unbalanced opinion distributions. Agreeableness and sycophancy\nemerge as two main characteristics of LLM agents, and consensus around the\npresented statement emerges almost in any setting. Moreover, such AI agents are\noften producers of fallacious arguments in the attempt of persuading their\npeers and - for their complacency - they are also highly influenced by\narguments built on logical fallacies. These results highlight the potential of\nthis framework not only for simulating social dynamics but also for exploring\nfrom another perspective biases and shortcomings of LLMs, which may impact\ntheir interactions with humans."
                },
                "authors": [
                    {
                        "name": "Erica Cau"
                    },
                    {
                        "name": "Valentina Pansanella"
                    },
                    {
                        "name": "Dino Pedreschi"
                    },
                    {
                        "name": "Giulio Rossetti"
                    }
                ],
                "author_detail": {
                    "name": "Giulio Rossetti"
                },
                "author": "Giulio Rossetti",
                "arxiv_comment": "34 pages, journal submission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19098v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19098v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18300v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18300v2",
                "updated": "2025-02-26T12:41:47Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    12,
                    41,
                    47,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-25T15:39:33Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    15,
                    39,
                    33,
                    1,
                    56,
                    0
                ],
                "title": "Bayesian Computation in Deep Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian Computation in Deep Learning"
                },
                "summary": "This review paper is intended for the 2nd edition of the Handbook of Markov\nchain Monte Carlo. We provide an introduction to approximate inference\ntechniques as Bayesian computation methods applied to deep learning models. We\norganize the chapter by presenting popular computational methods for Bayesian\nneural networks and deep generative models, explaining their unique challenges\nin posterior inference as well as the solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This review paper is intended for the 2nd edition of the Handbook of Markov\nchain Monte Carlo. We provide an introduction to approximate inference\ntechniques as Bayesian computation methods applied to deep learning models. We\norganize the chapter by presenting popular computational methods for Bayesian\nneural networks and deep generative models, explaining their unique challenges\nin posterior inference as well as the solutions."
                },
                "authors": [
                    {
                        "name": "Wenlong Chen"
                    },
                    {
                        "name": "Bolian Li"
                    },
                    {
                        "name": "Ruqi Zhang"
                    },
                    {
                        "name": "Yingzhen Li"
                    }
                ],
                "author_detail": {
                    "name": "Yingzhen Li"
                },
                "author": "Yingzhen Li",
                "arxiv_comment": "43 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18300v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18300v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08964v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08964v3",
                "updated": "2025-02-26T12:39:40Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    12,
                    39,
                    40,
                    2,
                    57,
                    0
                ],
                "published": "2024-10-11T16:32:05Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    16,
                    32,
                    5,
                    4,
                    285,
                    0
                ],
                "title": "Language Imbalance Driven Rewarding for Multilingual Self-improving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Imbalance Driven Rewarding for Multilingual Self-improving"
                },
                "summary": "Large Language Models (LLMs) have achieved state-of-the-art performance\nacross numerous tasks. However, these advancements have predominantly benefited\n\"first-class\" languages such as English and Chinese, leaving many other\nlanguages underrepresented. This imbalance, while limiting broader\napplications, generates a natural preference ranking between languages,\noffering an opportunity to bootstrap the multilingual capabilities of LLM in a\nself-improving manner. Thus, we propose $\\textit{Language Imbalance Driven\nRewarding}$, where the inherent imbalance between dominant and non-dominant\nlanguages within LLMs is leveraged as a reward signal. Iterative DPO training\ndemonstrates that this approach not only enhances LLM performance in\nnon-dominant languages but also improves the dominant language's capacity,\nthereby yielding an iterative reward signal. Fine-tuning\nMeta-Llama-3-8B-Instruct over two iterations of this approach results in\ncontinuous improvements in multilingual performance across\ninstruction-following and arithmetic reasoning tasks, evidenced by an average\nimprovement of 7.46% win rate on the X-AlpacaEval leaderboard and 13.9%\naccuracy on the MGSM benchmark. This work serves as an initial exploration,\npaving the way for multilingual self-improvement of LLMs. The code is available\nat https://github.com/ZNLP/Language-Imbalance-Driven-Rewarding",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved state-of-the-art performance\nacross numerous tasks. However, these advancements have predominantly benefited\n\"first-class\" languages such as English and Chinese, leaving many other\nlanguages underrepresented. This imbalance, while limiting broader\napplications, generates a natural preference ranking between languages,\noffering an opportunity to bootstrap the multilingual capabilities of LLM in a\nself-improving manner. Thus, we propose $\\textit{Language Imbalance Driven\nRewarding}$, where the inherent imbalance between dominant and non-dominant\nlanguages within LLMs is leveraged as a reward signal. Iterative DPO training\ndemonstrates that this approach not only enhances LLM performance in\nnon-dominant languages but also improves the dominant language's capacity,\nthereby yielding an iterative reward signal. Fine-tuning\nMeta-Llama-3-8B-Instruct over two iterations of this approach results in\ncontinuous improvements in multilingual performance across\ninstruction-following and arithmetic reasoning tasks, evidenced by an average\nimprovement of 7.46% win rate on the X-AlpacaEval leaderboard and 13.9%\naccuracy on the MGSM benchmark. This work serves as an initial exploration,\npaving the way for multilingual self-improvement of LLMs. The code is available\nat https://github.com/ZNLP/Language-Imbalance-Driven-Rewarding"
                },
                "authors": [
                    {
                        "name": "Wen Yang"
                    },
                    {
                        "name": "Junhong Wu"
                    },
                    {
                        "name": "Chen Wang"
                    },
                    {
                        "name": "Chengqing Zong"
                    },
                    {
                        "name": "Jiajun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jiajun Zhang"
                },
                "author": "Jiajun Zhang",
                "arxiv_comment": "Camera ready version for ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08964v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08964v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19091v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19091v1",
                "updated": "2025-02-26T12:37:47Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    12,
                    37,
                    47,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-26T12:37:47Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    12,
                    37,
                    47,
                    2,
                    57,
                    0
                ],
                "title": "Nexus: A Lightweight and Scalable Multi-Agent Framework for Complex\n  Tasks Automation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nexus: A Lightweight and Scalable Multi-Agent Framework for Complex\n  Tasks Automation"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) have substantially\nevolved Multi-Agent Systems (MASs) capabilities, enabling systems that not only\nautomate tasks but also leverage near-human reasoning capabilities. To achieve\nthis, LLM-based MASs need to be built around two critical principles: (i) a\nrobust architecture that fully exploits LLM potential for specific tasks -- or\nrelated task sets -- and ($ii$) an effective methodology for equipping LLMs\nwith the necessary capabilities to perform tasks and manage information\nefficiently. It goes without saying that a priori architectural designs can\nlimit the scalability and domain adaptability of a given MAS.\n  To address these challenges, in this paper we introduce Nexus: a lightweight\nPython framework designed to easily build and manage LLM-based MASs. Nexus\nintroduces the following innovations: (i) a flexible multi-supervisor\nhierarchy, (ii) a simplified workflow design, and (iii) easy installation and\nopen-source flexibility: Nexus can be installed via pip and is distributed\nunder a permissive open-source license, allowing users to freely modify and\nextend its capabilities.\n  Experimental results demonstrate that architectures built with Nexus exhibit\nstate-of-the-art performance across diverse domains. In coding tasks,\nNexus-driven MASs achieve a 99% pass rate on HumanEval and a flawless 100% on\nVerilogEval-Human, outperforming cutting-edge reasoning language models such as\no3-mini and DeepSeek-R1. Moreover, these architectures display robust\nproficiency in complex reasoning and mathematical problem solving, achieving\ncorrect solutions for all randomly selected problems from the MATH dataset. In\nthe realm of multi-objective optimization, Nexus-based architectures\nsuccessfully address challenging timing closure tasks on designs from the VTR\nbenchmark suite, while guaranteeing, on average, a power saving of nearly 30%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) have substantially\nevolved Multi-Agent Systems (MASs) capabilities, enabling systems that not only\nautomate tasks but also leverage near-human reasoning capabilities. To achieve\nthis, LLM-based MASs need to be built around two critical principles: (i) a\nrobust architecture that fully exploits LLM potential for specific tasks -- or\nrelated task sets -- and ($ii$) an effective methodology for equipping LLMs\nwith the necessary capabilities to perform tasks and manage information\nefficiently. It goes without saying that a priori architectural designs can\nlimit the scalability and domain adaptability of a given MAS.\n  To address these challenges, in this paper we introduce Nexus: a lightweight\nPython framework designed to easily build and manage LLM-based MASs. Nexus\nintroduces the following innovations: (i) a flexible multi-supervisor\nhierarchy, (ii) a simplified workflow design, and (iii) easy installation and\nopen-source flexibility: Nexus can be installed via pip and is distributed\nunder a permissive open-source license, allowing users to freely modify and\nextend its capabilities.\n  Experimental results demonstrate that architectures built with Nexus exhibit\nstate-of-the-art performance across diverse domains. In coding tasks,\nNexus-driven MASs achieve a 99% pass rate on HumanEval and a flawless 100% on\nVerilogEval-Human, outperforming cutting-edge reasoning language models such as\no3-mini and DeepSeek-R1. Moreover, these architectures display robust\nproficiency in complex reasoning and mathematical problem solving, achieving\ncorrect solutions for all randomly selected problems from the MATH dataset. In\nthe realm of multi-objective optimization, Nexus-based architectures\nsuccessfully address challenging timing closure tasks on designs from the VTR\nbenchmark suite, while guaranteeing, on average, a power saving of nearly 30%."
                },
                "authors": [
                    {
                        "name": "Humza Sami"
                    },
                    {
                        "name": "Mubashir ul Islam"
                    },
                    {
                        "name": "Samy Charas"
                    },
                    {
                        "name": "Asav Gandhi"
                    },
                    {
                        "name": "Pierre-Emmanuel Gaillardon"
                    },
                    {
                        "name": "Valerio Tenace"
                    }
                ],
                "author_detail": {
                    "name": "Valerio Tenace"
                },
                "author": "Valerio Tenace",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19091v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19091v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19090v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19090v1",
                "updated": "2025-02-26T12:36:16Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    12,
                    36,
                    16,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-26T12:36:16Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    12,
                    36,
                    16,
                    2,
                    57,
                    0
                ],
                "title": "EndoMamba: An Efficient Foundation Model for Endoscopic Videos",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EndoMamba: An Efficient Foundation Model for Endoscopic Videos"
                },
                "summary": "Endoscopic video-based tasks, such as visual navigation and surgical phase\nrecognition, play a crucial role in minimally invasive surgeries by providing\nreal-time assistance. While recent video foundation models have shown promise,\ntheir applications are hindered by (1) computational inefficiencies and (2)\nsuboptimal performance caused by limited data for pre-training in endoscopy. To\naddress these issues, we present EndoMamba, a foundation model designed for\nreal-time inference while learning generalized spatiotemporal representations.\nFirst, to mitigate computational inefficiencies, we propose the EndoMamba\nbackbone, optimized for real-time inference. Inspired by recent advancements in\nstate space models, EndoMamba integrates Bidirectional Mamba blocks for spatial\nmodeling within individual frames and vanilla Mamba blocks for past-to-present\nreasoning across the temporal domain. This design enables both strong\nspatiotemporal modeling and efficient inference in online video streams.\nSecond, we propose a self-supervised hierarchical pre-training diagram to\nenhance EndoMamba's representation learning using endoscopic videos and\nincorporating general video domain knowledge. Specifically, our approach\ncombines masked reconstruction with auxiliary supervision, leveraging low-level\nreconstruction to capture spatial-temporal structures and high-level alignment\nto transfer broader knowledge from a pretrained general-video domain foundation\nmodel. Extensive experiments on four downstream tasks--classification,\nsegmentation, surgical phase recognition, and localization--demonstrate that\nEndoMamba outperforms existing foundation models and task-specific methods\nwhile maintaining real-time inference speed. The source code will be released\nupon acceptance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Endoscopic video-based tasks, such as visual navigation and surgical phase\nrecognition, play a crucial role in minimally invasive surgeries by providing\nreal-time assistance. While recent video foundation models have shown promise,\ntheir applications are hindered by (1) computational inefficiencies and (2)\nsuboptimal performance caused by limited data for pre-training in endoscopy. To\naddress these issues, we present EndoMamba, a foundation model designed for\nreal-time inference while learning generalized spatiotemporal representations.\nFirst, to mitigate computational inefficiencies, we propose the EndoMamba\nbackbone, optimized for real-time inference. Inspired by recent advancements in\nstate space models, EndoMamba integrates Bidirectional Mamba blocks for spatial\nmodeling within individual frames and vanilla Mamba blocks for past-to-present\nreasoning across the temporal domain. This design enables both strong\nspatiotemporal modeling and efficient inference in online video streams.\nSecond, we propose a self-supervised hierarchical pre-training diagram to\nenhance EndoMamba's representation learning using endoscopic videos and\nincorporating general video domain knowledge. Specifically, our approach\ncombines masked reconstruction with auxiliary supervision, leveraging low-level\nreconstruction to capture spatial-temporal structures and high-level alignment\nto transfer broader knowledge from a pretrained general-video domain foundation\nmodel. Extensive experiments on four downstream tasks--classification,\nsegmentation, surgical phase recognition, and localization--demonstrate that\nEndoMamba outperforms existing foundation models and task-specific methods\nwhile maintaining real-time inference speed. The source code will be released\nupon acceptance."
                },
                "authors": [
                    {
                        "name": "Qingyao Tian"
                    },
                    {
                        "name": "Huai Liao"
                    },
                    {
                        "name": "Xinyan Huang"
                    },
                    {
                        "name": "Bingyu Yang"
                    },
                    {
                        "name": "Dongdong Lei"
                    },
                    {
                        "name": "Sebastien Ourselin"
                    },
                    {
                        "name": "Hongbin Liu"
                    }
                ],
                "author_detail": {
                    "name": "Hongbin Liu"
                },
                "author": "Hongbin Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19090v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19090v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2406.06608v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.06608v6",
                "updated": "2025-02-26T18:59:01Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    18,
                    59,
                    1,
                    2,
                    57,
                    0
                ],
                "published": "2024-06-06T18:10:11Z",
                "published_parsed": [
                    2024,
                    6,
                    6,
                    18,
                    10,
                    11,
                    3,
                    158,
                    0
                ],
                "title": "The Prompt Report: A Systematic Survey of Prompt Engineering Techniques",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Prompt Report: A Systematic Survey of Prompt Engineering Techniques"
                },
                "summary": "Generative Artificial Intelligence (GenAI) systems are increasingly being\ndeployed across diverse industries and research domains. Developers and\nend-users interact with these systems through the use of prompting and prompt\nengineering. Although prompt engineering is a widely adopted and extensively\nresearched area, it suffers from conflicting terminology and a fragmented\nontological understanding of what constitutes an effective prompt due to its\nrelatively recent emergence. We establish a structured understanding of prompt\nengineering by assembling a taxonomy of prompting techniques and analyzing\ntheir applications. We present a detailed vocabulary of 33 vocabulary terms, a\ntaxonomy of 58 LLM prompting techniques, and 40 techniques for other\nmodalities. Additionally, we provide best practices and guidelines for prompt\nengineering, including advice for prompting state-of-the-art (SOTA) LLMs such\nas ChatGPT. We further present a meta-analysis of the entire literature on\nnatural language prefix-prompting. As a culmination of these efforts, this\npaper presents the most comprehensive survey on prompt engineering to date.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative Artificial Intelligence (GenAI) systems are increasingly being\ndeployed across diverse industries and research domains. Developers and\nend-users interact with these systems through the use of prompting and prompt\nengineering. Although prompt engineering is a widely adopted and extensively\nresearched area, it suffers from conflicting terminology and a fragmented\nontological understanding of what constitutes an effective prompt due to its\nrelatively recent emergence. We establish a structured understanding of prompt\nengineering by assembling a taxonomy of prompting techniques and analyzing\ntheir applications. We present a detailed vocabulary of 33 vocabulary terms, a\ntaxonomy of 58 LLM prompting techniques, and 40 techniques for other\nmodalities. Additionally, we provide best practices and guidelines for prompt\nengineering, including advice for prompting state-of-the-art (SOTA) LLMs such\nas ChatGPT. We further present a meta-analysis of the entire literature on\nnatural language prefix-prompting. As a culmination of these efforts, this\npaper presents the most comprehensive survey on prompt engineering to date."
                },
                "authors": [
                    {
                        "name": "Sander Schulhoff"
                    },
                    {
                        "name": "Michael Ilie"
                    },
                    {
                        "name": "Nishant Balepur"
                    },
                    {
                        "name": "Konstantine Kahadze"
                    },
                    {
                        "name": "Amanda Liu"
                    },
                    {
                        "name": "Chenglei Si"
                    },
                    {
                        "name": "Yinheng Li"
                    },
                    {
                        "name": "Aayush Gupta"
                    },
                    {
                        "name": "HyoJung Han"
                    },
                    {
                        "name": "Sevien Schulhoff"
                    },
                    {
                        "name": "Pranav Sandeep Dulepet"
                    },
                    {
                        "name": "Saurav Vidyadhara"
                    },
                    {
                        "name": "Dayeon Ki"
                    },
                    {
                        "name": "Sweta Agrawal"
                    },
                    {
                        "name": "Chau Pham"
                    },
                    {
                        "name": "Gerson Kroiz"
                    },
                    {
                        "name": "Feileen Li"
                    },
                    {
                        "name": "Hudson Tao"
                    },
                    {
                        "name": "Ashay Srivastava"
                    },
                    {
                        "name": "Hevander Da Costa"
                    },
                    {
                        "name": "Saloni Gupta"
                    },
                    {
                        "name": "Megan L. Rogers"
                    },
                    {
                        "name": "Inna Goncearenco"
                    },
                    {
                        "name": "Giuseppe Sarli"
                    },
                    {
                        "name": "Igor Galynker"
                    },
                    {
                        "name": "Denis Peskoff"
                    },
                    {
                        "name": "Marine Carpuat"
                    },
                    {
                        "name": "Jules White"
                    },
                    {
                        "name": "Shyamal Anadkat"
                    },
                    {
                        "name": "Alexander Hoyle"
                    },
                    {
                        "name": "Philip Resnik"
                    }
                ],
                "author_detail": {
                    "name": "Philip Resnik"
                },
                "author": "Philip Resnik",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.06608v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.06608v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19416v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19416v1",
                "updated": "2025-02-26T18:58:30Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    18,
                    58,
                    30,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-26T18:58:30Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    18,
                    58,
                    30,
                    2,
                    57,
                    0
                ],
                "title": "Norm Growth and Stability Challenges in Localized Sequential Knowledge\n  Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Norm Growth and Stability Challenges in Localized Sequential Knowledge\n  Editing"
                },
                "summary": "This study investigates the impact of localized updates to large language\nmodels (LLMs), specifically in the context of knowledge editing - a task aimed\nat incorporating or modifying specific facts without altering broader model\ncapabilities. We first show that across different post-training interventions\nlike continuous pre-training, full fine-tuning and LORA-based fine-tuning, the\nFrobenius norm of the updated matrices always increases. This increasing norm\nis especially detrimental for localized knowledge editing, where only a subset\nof matrices are updated in a model . We reveal a consistent phenomenon across\nvarious editing techniques, including fine-tuning, hypernetwork-based\napproaches, and locate-and-edit methods: the norm of the updated matrix\ninvariably increases with successive updates. Such growth disrupts model\nbalance, particularly when isolated matrices are updated while the rest of the\nmodel remains static, leading to potential instability and degradation of\ndownstream performance. Upon deeper investigations of the intermediate\nactivation vectors, we find that the norm of internal activations decreases and\nis accompanied by shifts in the subspaces occupied by these activations, which\nshows that these activation vectors now occupy completely different regions in\nthe representation space compared to the unedited model. With our paper, we\nhighlight the technical challenges with continuous and localized sequential\nknowledge editing and their implications for maintaining model stability and\nutility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study investigates the impact of localized updates to large language\nmodels (LLMs), specifically in the context of knowledge editing - a task aimed\nat incorporating or modifying specific facts without altering broader model\ncapabilities. We first show that across different post-training interventions\nlike continuous pre-training, full fine-tuning and LORA-based fine-tuning, the\nFrobenius norm of the updated matrices always increases. This increasing norm\nis especially detrimental for localized knowledge editing, where only a subset\nof matrices are updated in a model . We reveal a consistent phenomenon across\nvarious editing techniques, including fine-tuning, hypernetwork-based\napproaches, and locate-and-edit methods: the norm of the updated matrix\ninvariably increases with successive updates. Such growth disrupts model\nbalance, particularly when isolated matrices are updated while the rest of the\nmodel remains static, leading to potential instability and degradation of\ndownstream performance. Upon deeper investigations of the intermediate\nactivation vectors, we find that the norm of internal activations decreases and\nis accompanied by shifts in the subspaces occupied by these activations, which\nshows that these activation vectors now occupy completely different regions in\nthe representation space compared to the unedited model. With our paper, we\nhighlight the technical challenges with continuous and localized sequential\nknowledge editing and their implications for maintaining model stability and\nutility."
                },
                "authors": [
                    {
                        "name": "Akshat Gupta"
                    },
                    {
                        "name": "Christine Fang"
                    },
                    {
                        "name": "Atahan Ozdemir"
                    },
                    {
                        "name": "Maochuan Lu"
                    },
                    {
                        "name": "Ahmed Alaa"
                    },
                    {
                        "name": "Thomas Hartvigsen"
                    },
                    {
                        "name": "Gopala Anumanchipalli"
                    }
                ],
                "author_detail": {
                    "name": "Gopala Anumanchipalli"
                },
                "author": "Gopala Anumanchipalli",
                "arxiv_comment": "Accepted for Oral Presentation at KnowFM @ AAAI 2025. arXiv admin\n  note: text overlap with arXiv:2502.01636",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19416v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19416v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19413v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19413v1",
                "updated": "2025-02-26T18:56:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    18,
                    56,
                    52,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-26T18:56:52Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    18,
                    56,
                    52,
                    2,
                    57,
                    0
                ],
                "title": "Project Alexandria: Towards Freeing Scientific Knowledge from Copyright\n  Burdens via LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Project Alexandria: Towards Freeing Scientific Knowledge from Copyright\n  Burdens via LLMs"
                },
                "summary": "Paywalls, licenses and copyright rules often restrict the broad dissemination\nand reuse of scientific knowledge. We take the position that it is both legally\nand technically feasible to extract the scientific knowledge in scholarly\ntexts. Current methods, like text embeddings, fail to reliably preserve factual\ncontent, and simple paraphrasing may not be legally sound. We urge the\ncommunity to adopt a new idea: convert scholarly documents into Knowledge Units\nusing LLMs. These units use structured data capturing entities, attributes and\nrelationships without stylistic content. We provide evidence that Knowledge\nUnits: (1) form a legally defensible framework for sharing knowledge from\ncopyrighted research texts, based on legal analyses of German copyright law and\nU.S. Fair Use doctrine, and (2) preserve most (~95%) factual knowledge from\noriginal text, measured by MCQ performance on facts from the original\ncopyrighted text across four research domains. Freeing scientific knowledge\nfrom copyright promises transformative benefits for scientific research and\neducation by allowing language models to reuse important facts from copyrighted\ntext. To support this, we share open-source tools for converting research\ndocuments into Knowledge Units. Overall, our work posits the feasibility of\ndemocratizing access to scientific knowledge while respecting copyright.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Paywalls, licenses and copyright rules often restrict the broad dissemination\nand reuse of scientific knowledge. We take the position that it is both legally\nand technically feasible to extract the scientific knowledge in scholarly\ntexts. Current methods, like text embeddings, fail to reliably preserve factual\ncontent, and simple paraphrasing may not be legally sound. We urge the\ncommunity to adopt a new idea: convert scholarly documents into Knowledge Units\nusing LLMs. These units use structured data capturing entities, attributes and\nrelationships without stylistic content. We provide evidence that Knowledge\nUnits: (1) form a legally defensible framework for sharing knowledge from\ncopyrighted research texts, based on legal analyses of German copyright law and\nU.S. Fair Use doctrine, and (2) preserve most (~95%) factual knowledge from\noriginal text, measured by MCQ performance on facts from the original\ncopyrighted text across four research domains. Freeing scientific knowledge\nfrom copyright promises transformative benefits for scientific research and\neducation by allowing language models to reuse important facts from copyrighted\ntext. To support this, we share open-source tools for converting research\ndocuments into Knowledge Units. Overall, our work posits the feasibility of\ndemocratizing access to scientific knowledge while respecting copyright."
                },
                "authors": [
                    {
                        "name": "Christoph Schuhmann"
                    },
                    {
                        "name": "Gollam Rabby"
                    },
                    {
                        "name": "Ameya Prabhu"
                    },
                    {
                        "name": "Tawsif Ahmed"
                    },
                    {
                        "name": "Andreas Hochlehnert"
                    },
                    {
                        "name": "Huu Nguyen"
                    },
                    {
                        "name": "Nick Akinci Heidrich"
                    },
                    {
                        "name": "Ludwig Schmidt"
                    },
                    {
                        "name": "Robert Kaczmarczyk"
                    },
                    {
                        "name": "Sören Auer"
                    },
                    {
                        "name": "Jenia Jitsev"
                    },
                    {
                        "name": "Matthias Bethge"
                    }
                ],
                "author_detail": {
                    "name": "Matthias Bethge"
                },
                "author": "Matthias Bethge",
                "arxiv_comment": "Technical Report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19413v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19413v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04697v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04697v2",
                "updated": "2025-02-26T18:55:54Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    18,
                    55,
                    54,
                    2,
                    57,
                    0
                ],
                "published": "2024-12-06T01:20:16Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    1,
                    20,
                    16,
                    4,
                    341,
                    0
                ],
                "title": "Privacy-Preserving Retrieval-Augmented Generation with Differential\n  Privacy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Privacy-Preserving Retrieval-Augmented Generation with Differential\n  Privacy"
                },
                "summary": "With the recent remarkable advancement of large language models (LLMs), there\nhas been a growing interest in utilizing them in the domains with highly\nsensitive data that lies outside their training data. For this purpose,\nretrieval-augmented generation (RAG) is particularly effective -- it assists\nLLMs by directly providing relevant information from the external knowledge\nsources. However, without extra privacy safeguards, RAG outputs risk leaking\nsensitive information from the external data source. In this work, we explore\nRAG under differential privacy (DP), a formal guarantee of data privacy. The\nmain challenge with differentially private RAG is how to generate long accurate\nanswers within a moderate privacy budget. We address this by proposing an\nalgorithm that smartly spends privacy budget only for the tokens that require\nthe sensitive information and uses the non-private LLM for other tokens. Our\nextensive empirical evaluations reveal that our algorithm outperforms the\nnon-RAG baseline under a reasonable privacy budget of $\\epsilon\\approx 10$\nacross different models and datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the recent remarkable advancement of large language models (LLMs), there\nhas been a growing interest in utilizing them in the domains with highly\nsensitive data that lies outside their training data. For this purpose,\nretrieval-augmented generation (RAG) is particularly effective -- it assists\nLLMs by directly providing relevant information from the external knowledge\nsources. However, without extra privacy safeguards, RAG outputs risk leaking\nsensitive information from the external data source. In this work, we explore\nRAG under differential privacy (DP), a formal guarantee of data privacy. The\nmain challenge with differentially private RAG is how to generate long accurate\nanswers within a moderate privacy budget. We address this by proposing an\nalgorithm that smartly spends privacy budget only for the tokens that require\nthe sensitive information and uses the non-private LLM for other tokens. Our\nextensive empirical evaluations reveal that our algorithm outperforms the\nnon-RAG baseline under a reasonable privacy budget of $\\epsilon\\approx 10$\nacross different models and datasets."
                },
                "authors": [
                    {
                        "name": "Tatsuki Koga"
                    },
                    {
                        "name": "Ruihan Wu"
                    },
                    {
                        "name": "Kamalika Chaudhuri"
                    }
                ],
                "author_detail": {
                    "name": "Kamalika Chaudhuri"
                },
                "author": "Kamalika Chaudhuri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04697v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04697v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19411v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19411v1",
                "updated": "2025-02-26T18:55:42Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    18,
                    55,
                    42,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-26T18:55:42Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    18,
                    55,
                    42,
                    2,
                    57,
                    0
                ],
                "title": "Code to Think, Think to Code: A Survey on Code-Enhanced Reasoning and\n  Reasoning-Driven Code Intelligence in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code to Think, Think to Code: A Survey on Code-Enhanced Reasoning and\n  Reasoning-Driven Code Intelligence in LLMs"
                },
                "summary": "In large language models (LLMs), code and reasoning reinforce each other:\ncode offers an abstract, modular, and logic-driven structure that supports\nreasoning, while reasoning translates high-level goals into smaller, executable\nsteps that drive more advanced code intelligence. In this study, we examine how\ncode serves as a structured medium for enhancing reasoning: it provides\nverifiable execution paths, enforces logical decomposition, and enables runtime\nvalidation. We also explore how improvements in reasoning have transformed code\nintelligence from basic completion to advanced capabilities, enabling models to\naddress complex software engineering tasks through planning and debugging.\nFinally, we identify key challenges and propose future research directions to\nstrengthen this synergy, ultimately improving LLM's performance in both areas.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In large language models (LLMs), code and reasoning reinforce each other:\ncode offers an abstract, modular, and logic-driven structure that supports\nreasoning, while reasoning translates high-level goals into smaller, executable\nsteps that drive more advanced code intelligence. In this study, we examine how\ncode serves as a structured medium for enhancing reasoning: it provides\nverifiable execution paths, enforces logical decomposition, and enables runtime\nvalidation. We also explore how improvements in reasoning have transformed code\nintelligence from basic completion to advanced capabilities, enabling models to\naddress complex software engineering tasks through planning and debugging.\nFinally, we identify key challenges and propose future research directions to\nstrengthen this synergy, ultimately improving LLM's performance in both areas."
                },
                "authors": [
                    {
                        "name": "Dayu Yang"
                    },
                    {
                        "name": "Tianyang Liu"
                    },
                    {
                        "name": "Daoan Zhang"
                    },
                    {
                        "name": "Antoine Simoulin"
                    },
                    {
                        "name": "Xiaoyi Liu"
                    },
                    {
                        "name": "Yuwei Cao"
                    },
                    {
                        "name": "Zhaopu Teng"
                    },
                    {
                        "name": "Xin Qian"
                    },
                    {
                        "name": "Grey Yang"
                    },
                    {
                        "name": "Jiebo Luo"
                    },
                    {
                        "name": "Julian McAuley"
                    }
                ],
                "author_detail": {
                    "name": "Julian McAuley"
                },
                "author": "Julian McAuley",
                "arxiv_comment": "Project Repo: https://github.com/dayuyang1999/Awesome-Code-Reasoning",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19411v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19411v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19410v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19410v1",
                "updated": "2025-02-26T18:55:26Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    18,
                    55,
                    26,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-26T18:55:26Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    18,
                    55,
                    26,
                    2,
                    57,
                    0
                ],
                "title": "Less or More: Towards Glanceable Explanations for LLM Recommendations\n  Using Ultra-Small Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Less or More: Towards Glanceable Explanations for LLM Recommendations\n  Using Ultra-Small Devices"
                },
                "summary": "Large Language Models (LLMs) have shown remarkable potential in recommending\neveryday actions as personal AI assistants, while Explainable AI (XAI)\ntechniques are being increasingly utilized to help users understand why a\nrecommendation is given. Personal AI assistants today are often located on\nultra-small devices such as smartwatches, which have limited screen space. The\nverbosity of LLM-generated explanations, however, makes it challenging to\ndeliver glanceable LLM explanations on such ultra-small devices. To address\nthis, we explored 1) spatially structuring an LLM's explanation text using\ndefined contextual components during prompting and 2) presenting temporally\nadaptive explanations to users based on confidence levels. We conducted a user\nstudy to understand how these approaches impacted user experiences when\ninteracting with LLM recommendations and explanations on ultra-small devices.\nThe results showed that structured explanations reduced users' time to action\nand cognitive load when reading an explanation. Always-on structured\nexplanations increased users' acceptance of AI recommendations. However, users\nwere less satisfied with structured explanations compared to unstructured ones\ndue to their lack of sufficient, readable details. Additionally, adaptively\npresenting structured explanations was less effective at improving user\nperceptions of the AI compared to the always-on structured explanations.\nTogether with users' interview feedback, the results led to design implications\nto be mindful of when personalizing the content and timing of LLM explanations\nthat are displayed on ultra-small devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown remarkable potential in recommending\neveryday actions as personal AI assistants, while Explainable AI (XAI)\ntechniques are being increasingly utilized to help users understand why a\nrecommendation is given. Personal AI assistants today are often located on\nultra-small devices such as smartwatches, which have limited screen space. The\nverbosity of LLM-generated explanations, however, makes it challenging to\ndeliver glanceable LLM explanations on such ultra-small devices. To address\nthis, we explored 1) spatially structuring an LLM's explanation text using\ndefined contextual components during prompting and 2) presenting temporally\nadaptive explanations to users based on confidence levels. We conducted a user\nstudy to understand how these approaches impacted user experiences when\ninteracting with LLM recommendations and explanations on ultra-small devices.\nThe results showed that structured explanations reduced users' time to action\nand cognitive load when reading an explanation. Always-on structured\nexplanations increased users' acceptance of AI recommendations. However, users\nwere less satisfied with structured explanations compared to unstructured ones\ndue to their lack of sufficient, readable details. Additionally, adaptively\npresenting structured explanations was less effective at improving user\nperceptions of the AI compared to the always-on structured explanations.\nTogether with users' interview feedback, the results led to design implications\nto be mindful of when personalizing the content and timing of LLM explanations\nthat are displayed on ultra-small devices."
                },
                "authors": [
                    {
                        "name": "Xinru Wang"
                    },
                    {
                        "name": "Mengjie Yu"
                    },
                    {
                        "name": "Hannah Nguyen"
                    },
                    {
                        "name": "Michael Iuzzolino"
                    },
                    {
                        "name": "Tianyi Wang"
                    },
                    {
                        "name": "Peiqi Tang"
                    },
                    {
                        "name": "Natasha Lynova"
                    },
                    {
                        "name": "Co Tran"
                    },
                    {
                        "name": "Ting Zhang"
                    },
                    {
                        "name": "Naveen Sendhilnathan"
                    },
                    {
                        "name": "Hrvoje Benko"
                    },
                    {
                        "name": "Haijun Xia"
                    },
                    {
                        "name": "Tanya Jonker"
                    }
                ],
                "author_detail": {
                    "name": "Tanya Jonker"
                },
                "author": "Tanya Jonker",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19410v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19410v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19407v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19407v1",
                "updated": "2025-02-26T18:54:39Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    18,
                    54,
                    39,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-26T18:54:39Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    18,
                    54,
                    39,
                    2,
                    57,
                    0
                ],
                "title": "Learning Code-Edit Embedding to Model Student Debugging Behavior",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Code-Edit Embedding to Model Student Debugging Behavior"
                },
                "summary": "Providing effective feedback for programming assignments in computer science\neducation can be challenging: students solve problems by iteratively submitting\ncode, executing it, and using limited feedback from the compiler or the\nauto-grader to debug. Analyzing student debugging behavior in this process may\nreveal important insights into their knowledge and inform better personalized\nsupport tools. In this work, we propose an encoder-decoder-based model that\nlearns meaningful code-edit embeddings between consecutive student code\nsubmissions, to capture their debugging behavior. Our model leverages\ninformation on whether a student code submission passes each test case to\nfine-tune large language models (LLMs) to learn code editing representations.\nIt enables personalized next-step code suggestions that maintain the student's\ncoding style while improving test case correctness. Our model also enables us\nto analyze student code-editing patterns to uncover common student errors and\ndebugging behaviors, using clustering techniques. Experimental results on a\nreal-world student code submission dataset demonstrate that our model excels at\ncode reconstruction and personalized code suggestion while revealing\ninteresting patterns in student debugging behavior.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Providing effective feedback for programming assignments in computer science\neducation can be challenging: students solve problems by iteratively submitting\ncode, executing it, and using limited feedback from the compiler or the\nauto-grader to debug. Analyzing student debugging behavior in this process may\nreveal important insights into their knowledge and inform better personalized\nsupport tools. In this work, we propose an encoder-decoder-based model that\nlearns meaningful code-edit embeddings between consecutive student code\nsubmissions, to capture their debugging behavior. Our model leverages\ninformation on whether a student code submission passes each test case to\nfine-tune large language models (LLMs) to learn code editing representations.\nIt enables personalized next-step code suggestions that maintain the student's\ncoding style while improving test case correctness. Our model also enables us\nto analyze student code-editing patterns to uncover common student errors and\ndebugging behaviors, using clustering techniques. Experimental results on a\nreal-world student code submission dataset demonstrate that our model excels at\ncode reconstruction and personalized code suggestion while revealing\ninteresting patterns in student debugging behavior."
                },
                "authors": [
                    {
                        "name": "Hasnain Heickal"
                    },
                    {
                        "name": "Andrew Lan"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Lan"
                },
                "author": "Andrew Lan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19407v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19407v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19405v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19405v1",
                "updated": "2025-02-26T18:53:31Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    18,
                    53,
                    31,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-26T18:53:31Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    18,
                    53,
                    31,
                    2,
                    57,
                    0
                ],
                "title": "Verde: Verification via Refereed Delegation for Machine Learning\n  Programs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Verde: Verification via Refereed Delegation for Machine Learning\n  Programs"
                },
                "summary": "Machine learning programs, such as those performing inference, fine-tuning,\nand training of LLMs, are commonly delegated to untrusted compute providers. To\nprovide correctness guarantees for the client, we propose adapting the\ncryptographic notion of refereed delegation to the machine learning setting.\nThis approach enables a computationally limited client to delegate a program to\nmultiple untrusted compute providers, with a guarantee of obtaining the correct\nresult if at least one of them is honest. Refereed delegation of ML programs\nposes two technical hurdles: (1) an arbitration protocol to resolve disputes\nwhen compute providers disagree on the output, and (2) the ability to bitwise\nreproduce ML programs across different hardware setups, For (1), we design\nVerde, a dispute arbitration protocol that efficiently handles the large scale\nand graph-based computational model of modern ML programs. For (2), we build\nRepOps (Reproducible Operators), a library that eliminates hardware\n\"non-determinism\" by controlling the order of floating point operations\nperformed on all hardware. Our implementation shows that refereed delegation\nachieves both strong guarantees for clients and practical overheads for compute\nproviders.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning programs, such as those performing inference, fine-tuning,\nand training of LLMs, are commonly delegated to untrusted compute providers. To\nprovide correctness guarantees for the client, we propose adapting the\ncryptographic notion of refereed delegation to the machine learning setting.\nThis approach enables a computationally limited client to delegate a program to\nmultiple untrusted compute providers, with a guarantee of obtaining the correct\nresult if at least one of them is honest. Refereed delegation of ML programs\nposes two technical hurdles: (1) an arbitration protocol to resolve disputes\nwhen compute providers disagree on the output, and (2) the ability to bitwise\nreproduce ML programs across different hardware setups, For (1), we design\nVerde, a dispute arbitration protocol that efficiently handles the large scale\nand graph-based computational model of modern ML programs. For (2), we build\nRepOps (Reproducible Operators), a library that eliminates hardware\n\"non-determinism\" by controlling the order of floating point operations\nperformed on all hardware. Our implementation shows that refereed delegation\nachieves both strong guarantees for clients and practical overheads for compute\nproviders."
                },
                "authors": [
                    {
                        "name": "Arasu Arun"
                    },
                    {
                        "name": "Adam St. Arnaud"
                    },
                    {
                        "name": "Alexey Titov"
                    },
                    {
                        "name": "Brian Wilcox"
                    },
                    {
                        "name": "Viktor Kolobaric"
                    },
                    {
                        "name": "Marc Brinkmann"
                    },
                    {
                        "name": "Oguzhan Ersoy"
                    },
                    {
                        "name": "Ben Fielding"
                    },
                    {
                        "name": "Joseph Bonneau"
                    }
                ],
                "author_detail": {
                    "name": "Joseph Bonneau"
                },
                "author": "Joseph Bonneau",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19405v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19405v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19402v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19402v1",
                "updated": "2025-02-26T18:51:12Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    18,
                    51,
                    12,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-26T18:51:12Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    18,
                    51,
                    12,
                    2,
                    57,
                    0
                ],
                "title": "General Reasoning Requires Learning to Reason from the Get-go",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "General Reasoning Requires Learning to Reason from the Get-go"
                },
                "summary": "Large Language Models (LLMs) have demonstrated impressive real-world utility,\nexemplifying artificial useful intelligence (AUI). However, their ability to\nreason adaptively and robustly -- the hallmarks of artificial general\nintelligence (AGI) -- remains fragile. While LLMs seemingly succeed in\ncommonsense reasoning, programming, and mathematics, they struggle to\ngeneralize algorithmic understanding across novel contexts. Our experiments\nwith algorithmic tasks in esoteric programming languages reveal that LLM's\nreasoning overfits to the training data and is limited in its transferability.\nWe hypothesize that the core issue underlying such limited transferability is\nthe coupling of reasoning and knowledge in LLMs.\n  To transition from AUI to AGI, we propose disentangling knowledge and\nreasoning through three key directions: (1) pretaining to reason using RL from\nscratch as an alternative to the widely used next-token prediction pretraining,\n(2) using a curriculum of synthetic tasks to ease the learning of a\n\\textit{reasoning prior} for RL that can then be transferred to natural\nlanguage tasks, and (3) learning more generalizable reasoning functions using a\nsmall context window to reduce exploiting spurious correlations between tokens.\nSuch a reasoning system coupled with a trained retrieval system and a large\nexternal memory bank as a knowledge store can overcome several limitations of\nexisting architectures at learning to reason in novel scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated impressive real-world utility,\nexemplifying artificial useful intelligence (AUI). However, their ability to\nreason adaptively and robustly -- the hallmarks of artificial general\nintelligence (AGI) -- remains fragile. While LLMs seemingly succeed in\ncommonsense reasoning, programming, and mathematics, they struggle to\ngeneralize algorithmic understanding across novel contexts. Our experiments\nwith algorithmic tasks in esoteric programming languages reveal that LLM's\nreasoning overfits to the training data and is limited in its transferability.\nWe hypothesize that the core issue underlying such limited transferability is\nthe coupling of reasoning and knowledge in LLMs.\n  To transition from AUI to AGI, we propose disentangling knowledge and\nreasoning through three key directions: (1) pretaining to reason using RL from\nscratch as an alternative to the widely used next-token prediction pretraining,\n(2) using a curriculum of synthetic tasks to ease the learning of a\n\\textit{reasoning prior} for RL that can then be transferred to natural\nlanguage tasks, and (3) learning more generalizable reasoning functions using a\nsmall context window to reduce exploiting spurious correlations between tokens.\nSuch a reasoning system coupled with a trained retrieval system and a large\nexternal memory bank as a knowledge store can overcome several limitations of\nexisting architectures at learning to reason in novel scenarios."
                },
                "authors": [
                    {
                        "name": "Seungwook Han"
                    },
                    {
                        "name": "Jyothish Pari"
                    },
                    {
                        "name": "Samuel J. Gershman"
                    },
                    {
                        "name": "Pulkit Agrawal"
                    }
                ],
                "author_detail": {
                    "name": "Pulkit Agrawal"
                },
                "author": "Pulkit Agrawal",
                "arxiv_comment": "11 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19402v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19402v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19400v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19400v1",
                "updated": "2025-02-26T18:50:09Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    18,
                    50,
                    9,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-26T18:50:09Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    18,
                    50,
                    9,
                    2,
                    57,
                    0
                ],
                "title": "TheoremExplainAgent: Towards Multimodal Explanations for LLM Theorem\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TheoremExplainAgent: Towards Multimodal Explanations for LLM Theorem\n  Understanding"
                },
                "summary": "Understanding domain-specific theorems often requires more than just\ntext-based reasoning; effective communication through structured visual\nexplanations is crucial for deeper comprehension. While large language models\n(LLMs) demonstrate strong performance in text-based theorem reasoning, their\nability to generate coherent and pedagogically meaningful visual explanations\nremains an open challenge. In this work, we introduce TheoremExplainAgent, an\nagentic approach for generating long-form theorem explanation videos (over 5\nminutes) using Manim animations. To systematically evaluate multimodal theorem\nexplanations, we propose TheoremExplainBench, a benchmark covering 240 theorems\nacross multiple STEM disciplines, along with 5 automated evaluation metrics.\nOur results reveal that agentic planning is essential for generating detailed\nlong-form videos, and the o3-mini agent achieves a success rate of 93.8% and an\noverall score of 0.77. However, our quantitative and qualitative studies show\nthat most of the videos produced exhibit minor issues with visual element\nlayout. Furthermore, multimodal explanations expose deeper reasoning flaws that\ntext-based explanations fail to reveal, highlighting the importance of\nmultimodal explanations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding domain-specific theorems often requires more than just\ntext-based reasoning; effective communication through structured visual\nexplanations is crucial for deeper comprehension. While large language models\n(LLMs) demonstrate strong performance in text-based theorem reasoning, their\nability to generate coherent and pedagogically meaningful visual explanations\nremains an open challenge. In this work, we introduce TheoremExplainAgent, an\nagentic approach for generating long-form theorem explanation videos (over 5\nminutes) using Manim animations. To systematically evaluate multimodal theorem\nexplanations, we propose TheoremExplainBench, a benchmark covering 240 theorems\nacross multiple STEM disciplines, along with 5 automated evaluation metrics.\nOur results reveal that agentic planning is essential for generating detailed\nlong-form videos, and the o3-mini agent achieves a success rate of 93.8% and an\noverall score of 0.77. However, our quantitative and qualitative studies show\nthat most of the videos produced exhibit minor issues with visual element\nlayout. Furthermore, multimodal explanations expose deeper reasoning flaws that\ntext-based explanations fail to reveal, highlighting the importance of\nmultimodal explanations."
                },
                "authors": [
                    {
                        "name": "Max Ku"
                    },
                    {
                        "name": "Thomas Chong"
                    },
                    {
                        "name": "Jonathan Leung"
                    },
                    {
                        "name": "Krish Shah"
                    },
                    {
                        "name": "Alvin Yu"
                    },
                    {
                        "name": "Wenhu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Wenhu Chen"
                },
                "author": "Wenhu Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19400v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19400v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11261v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11261v2",
                "updated": "2025-02-26T18:44:29Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    18,
                    44,
                    29,
                    2,
                    57,
                    0
                ],
                "published": "2024-10-15T04:35:56Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    4,
                    35,
                    56,
                    1,
                    289,
                    0
                ],
                "title": "Beyond Linear Approximations: A Novel Pruning Approach for Attention\n  Matrix",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Linear Approximations: A Novel Pruning Approach for Attention\n  Matrix"
                },
                "summary": "Large Language Models (LLMs) have shown immense potential in enhancing\nvarious aspects of our daily lives, from conversational AI to search and AI\nassistants. However, their growing capabilities come at the cost of extremely\nlarge model sizes, making deployment on edge devices challenging due to memory\nand computational constraints. This paper introduces a novel approach to LLM\nweight pruning that directly optimizes for approximating the attention matrix,\na core component of transformer architectures. Unlike existing methods that\nfocus on linear approximations, our approach accounts for the non-linear nature\nof the Softmax attention mechanism. We provide theoretical guarantees for the\nconvergence of our Gradient Descent-based optimization method to a near-optimal\npruning mask solution. Our empirical results demonstrate the effectiveness of\nour non-linear pruning approach in maintaining model performance while\nsignificantly reducing computational costs, which is beyond the current\nstate-of-the-art methods, i.e., SparseGPT and Wanda, by a large margin. This\nwork establishes a new theoretical foundation for pruning algorithm design in\nLLMs, potentially paving the way for more efficient LLM inference on\nresource-constrained devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown immense potential in enhancing\nvarious aspects of our daily lives, from conversational AI to search and AI\nassistants. However, their growing capabilities come at the cost of extremely\nlarge model sizes, making deployment on edge devices challenging due to memory\nand computational constraints. This paper introduces a novel approach to LLM\nweight pruning that directly optimizes for approximating the attention matrix,\na core component of transformer architectures. Unlike existing methods that\nfocus on linear approximations, our approach accounts for the non-linear nature\nof the Softmax attention mechanism. We provide theoretical guarantees for the\nconvergence of our Gradient Descent-based optimization method to a near-optimal\npruning mask solution. Our empirical results demonstrate the effectiveness of\nour non-linear pruning approach in maintaining model performance while\nsignificantly reducing computational costs, which is beyond the current\nstate-of-the-art methods, i.e., SparseGPT and Wanda, by a large margin. This\nwork establishes a new theoretical foundation for pruning algorithm design in\nLLMs, potentially paving the way for more efficient LLM inference on\nresource-constrained devices."
                },
                "authors": [
                    {
                        "name": "Yingyu Liang"
                    },
                    {
                        "name": "Jiangxuan Long"
                    },
                    {
                        "name": "Zhenmei Shi"
                    },
                    {
                        "name": "Zhao Song"
                    },
                    {
                        "name": "Yufa Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Yufa Zhou"
                },
                "author": "Yufa Zhou",
                "arxiv_comment": "ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11261v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11261v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19385v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19385v1",
                "updated": "2025-02-26T18:30:49Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    18,
                    30,
                    49,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-26T18:30:49Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    18,
                    30,
                    49,
                    2,
                    57,
                    0
                ],
                "title": "HDEE: Heterogeneous Domain Expert Ensemble",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HDEE: Heterogeneous Domain Expert Ensemble"
                },
                "summary": "Training dense LLMs requires enormous amounts of data and centralized\ncompute, which introduces fundamental bottlenecks and ever-growing costs for\nlarge models. Several studies aim to reduce this dependency on centralization\nby reducing the communication overhead of training dense models. Taking this\nidea of reducing communication overhead to a natural extreme, by training\nembarrassingly parallelizable ensembles of small independent experts, has been\nshown to outperform large dense models trained in traditional centralized\nsettings. However, existing studies do not take into account underlying\ndifferences amongst data domains and treat them as monolithic, regardless of\ntheir underlying complexity, size, or distribution. In this paper, we explore\nthe effects of introducing heterogeneity to these ensembles of domain expert\nmodels. Specifically, by allowing models within the ensemble to vary in\nsize--as well as the number of training steps taken depending on the training\ndata's domain--we study the effect heterogeneity has on these ensembles when\nevaluated against domains included in, and excluded from, the training set. We\nuse the same compute budget to train heterogeneous ensembles and homogeneous\nbaselines for comparison. We show that the heterogeneous ensembles achieve the\nlowest perplexity scores in $20$ out of the $21$ data domains used in the\nevaluation. Our code is available at https://github.com/gensyn-ai/hdee.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training dense LLMs requires enormous amounts of data and centralized\ncompute, which introduces fundamental bottlenecks and ever-growing costs for\nlarge models. Several studies aim to reduce this dependency on centralization\nby reducing the communication overhead of training dense models. Taking this\nidea of reducing communication overhead to a natural extreme, by training\nembarrassingly parallelizable ensembles of small independent experts, has been\nshown to outperform large dense models trained in traditional centralized\nsettings. However, existing studies do not take into account underlying\ndifferences amongst data domains and treat them as monolithic, regardless of\ntheir underlying complexity, size, or distribution. In this paper, we explore\nthe effects of introducing heterogeneity to these ensembles of domain expert\nmodels. Specifically, by allowing models within the ensemble to vary in\nsize--as well as the number of training steps taken depending on the training\ndata's domain--we study the effect heterogeneity has on these ensembles when\nevaluated against domains included in, and excluded from, the training set. We\nuse the same compute budget to train heterogeneous ensembles and homogeneous\nbaselines for comparison. We show that the heterogeneous ensembles achieve the\nlowest perplexity scores in $20$ out of the $21$ data domains used in the\nevaluation. Our code is available at https://github.com/gensyn-ai/hdee."
                },
                "authors": [
                    {
                        "name": "Oğuzhan Ersoy"
                    },
                    {
                        "name": "Jari Kolehmainen"
                    },
                    {
                        "name": "Gabriel Passamani Andrade"
                    }
                ],
                "author_detail": {
                    "name": "Gabriel Passamani Andrade"
                },
                "author": "Gabriel Passamani Andrade",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19385v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19385v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03758v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03758v3",
                "updated": "2025-02-26T18:16:15Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    18,
                    16,
                    15,
                    2,
                    57,
                    0
                ],
                "published": "2024-12-04T22:53:56Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    22,
                    53,
                    56,
                    2,
                    339,
                    0
                ],
                "title": "ARCON: Advancing Auto-Regressive Continuation for Driving Videos",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ARCON: Advancing Auto-Regressive Continuation for Driving Videos"
                },
                "summary": "Recent advancements in auto-regressive large language models (LLMs) have led\nto their application in video generation. This paper explores the use of Large\nVision Models (LVMs) for video continuation, a task essential for building\nworld models and predicting future frames. We introduce ARCON, a scheme that\nalternates between generating semantic and RGB tokens, allowing the LVM to\nexplicitly learn high-level structural video information. We find high\nconsistency in the RGB images and semantic maps generated without special\ndesign. Moreover, we employ an optical flow-based texture stitching method to\nenhance visual quality. Experiments in autonomous driving scenarios show that\nour model can consistently generate long videos.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in auto-regressive large language models (LLMs) have led\nto their application in video generation. This paper explores the use of Large\nVision Models (LVMs) for video continuation, a task essential for building\nworld models and predicting future frames. We introduce ARCON, a scheme that\nalternates between generating semantic and RGB tokens, allowing the LVM to\nexplicitly learn high-level structural video information. We find high\nconsistency in the RGB images and semantic maps generated without special\ndesign. Moreover, we employ an optical flow-based texture stitching method to\nenhance visual quality. Experiments in autonomous driving scenarios show that\nour model can consistently generate long videos."
                },
                "authors": [
                    {
                        "name": "Ruibo Ming"
                    },
                    {
                        "name": "Jingwei Wu"
                    },
                    {
                        "name": "Zhewei Huang"
                    },
                    {
                        "name": "Zhuoxuan Ju"
                    },
                    {
                        "name": "Jianming HU"
                    },
                    {
                        "name": "Lihui Peng"
                    },
                    {
                        "name": "Shuchang Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Shuchang Zhou"
                },
                "author": "Shuchang Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03758v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03758v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15823v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15823v2",
                "updated": "2025-02-26T18:13:06Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    18,
                    13,
                    6,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-20T03:48:00Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    3,
                    48,
                    0,
                    3,
                    51,
                    0
                ],
                "title": "InductionBench: LLMs Fail in the Simplest Complexity Class",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InductionBench: LLMs Fail in the Simplest Complexity Class"
                },
                "summary": "Large language models (LLMs) have shown remarkable improvements in reasoning\nand many existing benchmarks have been addressed by models such as o1 and o3\neither fully or partially. However, a majority of these benchmarks emphasize\ndeductive reasoning, including mathematical and coding tasks in which rules\nsuch as mathematical axioms or programming syntax are clearly defined, based on\nwhich LLMs can plan and apply these rules to arrive at a solution. In contrast,\ninductive reasoning, where one infers the underlying rules from observed data,\nremains less explored. Such inductive processes lie at the heart of scientific\ndiscovery, as they enable researchers to extract general principles from\nempirical observations. To assess whether LLMs possess this capacity, we\nintroduce InductionBench, a new benchmark designed to evaluate the inductive\nreasoning ability of LLMs. Our experimental findings reveal that even the most\nadvanced models available struggle to master the simplest complexity classes\nwithin the subregular hierarchy of functions, highlighting a notable deficiency\nin current LLMs' inductive reasoning capabilities. Coda and data are available\nhttps://github.com/Wenyueh/inductive_reasoning_benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown remarkable improvements in reasoning\nand many existing benchmarks have been addressed by models such as o1 and o3\neither fully or partially. However, a majority of these benchmarks emphasize\ndeductive reasoning, including mathematical and coding tasks in which rules\nsuch as mathematical axioms or programming syntax are clearly defined, based on\nwhich LLMs can plan and apply these rules to arrive at a solution. In contrast,\ninductive reasoning, where one infers the underlying rules from observed data,\nremains less explored. Such inductive processes lie at the heart of scientific\ndiscovery, as they enable researchers to extract general principles from\nempirical observations. To assess whether LLMs possess this capacity, we\nintroduce InductionBench, a new benchmark designed to evaluate the inductive\nreasoning ability of LLMs. Our experimental findings reveal that even the most\nadvanced models available struggle to master the simplest complexity classes\nwithin the subregular hierarchy of functions, highlighting a notable deficiency\nin current LLMs' inductive reasoning capabilities. Coda and data are available\nhttps://github.com/Wenyueh/inductive_reasoning_benchmark."
                },
                "authors": [
                    {
                        "name": "Wenyue Hua"
                    },
                    {
                        "name": "Tyler Wong"
                    },
                    {
                        "name": "Sun Fei"
                    },
                    {
                        "name": "Liangming Pan"
                    },
                    {
                        "name": "Adam Jardine"
                    },
                    {
                        "name": "William Yang Wang"
                    }
                ],
                "author_detail": {
                    "name": "William Yang Wang"
                },
                "author": "William Yang Wang",
                "arxiv_comment": "24 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15823v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15823v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.FL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07133v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07133v3",
                "updated": "2025-02-26T18:10:05Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    18,
                    10,
                    5,
                    2,
                    57,
                    0
                ],
                "published": "2024-11-11T17:06:48Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    17,
                    6,
                    48,
                    0,
                    316,
                    0
                ],
                "title": "Stronger Models are NOT Stronger Teachers for Instruction Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stronger Models are NOT Stronger Teachers for Instruction Tuning"
                },
                "summary": "Instruction tuning has been widely adopted to ensure large language models\n(LLMs) follow user instructions effectively. The resulting\ninstruction-following capabilities of LLMs heavily rely on the instruction\ndatasets used for tuning. Recently, synthetic instruction datasets have emerged\nas an economically viable solution to provide LLMs diverse and high-quality\ninstructions. However, existing approaches typically assume that larger or\nstronger models are stronger teachers for instruction tuning, and hence simply\nadopt these models as response generators to the synthetic instructions. In\nthis paper, we challenge this commonly-adopted assumption. Our extensive\nexperiments across five base models and twenty response generators reveal that\nlarger and stronger models are not necessarily stronger teachers of smaller\nmodels. We refer to this phenomenon as the Larger Models' Paradox. We observe\nthat existing metrics cannot precisely predict the effectiveness of response\ngenerators since they ignore the compatibility between teachers and base models\nbeing fine-tuned. We thus develop a novel metric, named as\nCompatibility-Adjusted Reward (CAR) to measure the effectiveness of response\ngenerators. Our experiments across five base models demonstrate that CAR\noutperforms almost all baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction tuning has been widely adopted to ensure large language models\n(LLMs) follow user instructions effectively. The resulting\ninstruction-following capabilities of LLMs heavily rely on the instruction\ndatasets used for tuning. Recently, synthetic instruction datasets have emerged\nas an economically viable solution to provide LLMs diverse and high-quality\ninstructions. However, existing approaches typically assume that larger or\nstronger models are stronger teachers for instruction tuning, and hence simply\nadopt these models as response generators to the synthetic instructions. In\nthis paper, we challenge this commonly-adopted assumption. Our extensive\nexperiments across five base models and twenty response generators reveal that\nlarger and stronger models are not necessarily stronger teachers of smaller\nmodels. We refer to this phenomenon as the Larger Models' Paradox. We observe\nthat existing metrics cannot precisely predict the effectiveness of response\ngenerators since they ignore the compatibility between teachers and base models\nbeing fine-tuned. We thus develop a novel metric, named as\nCompatibility-Adjusted Reward (CAR) to measure the effectiveness of response\ngenerators. Our experiments across five base models demonstrate that CAR\noutperforms almost all baselines."
                },
                "authors": [
                    {
                        "name": "Zhangchen Xu"
                    },
                    {
                        "name": "Fengqing Jiang"
                    },
                    {
                        "name": "Luyao Niu"
                    },
                    {
                        "name": "Bill Yuchen Lin"
                    },
                    {
                        "name": "Radha Poovendran"
                    }
                ],
                "author_detail": {
                    "name": "Radha Poovendran"
                },
                "author": "Radha Poovendran",
                "arxiv_comment": "This is paper is accepted at NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07133v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07133v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01619v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01619v2",
                "updated": "2025-02-26T18:03:54Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    18,
                    3,
                    54,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-03T18:51:43Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    18,
                    51,
                    43,
                    0,
                    34,
                    0
                ],
                "title": "Learning to Generate Unit Tests for Automated Debugging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to Generate Unit Tests for Automated Debugging"
                },
                "summary": "Unit tests (UTs) play an instrumental role in assessing code correctness as\nwell as providing feedback to large language models (LLMs), motivating\nautomated test generation. However, we uncover a trade-off between generating\nunit test inputs that reveal errors when given a faulty code and correctly\npredicting the unit test output without access to the gold solution. To address\nthis trade-off, we propose UTGen, which teaches LLMs to generate unit test\ninputs that reveal errors along with their correct expected outputs based on\ntask descriptions. Since model-generated tests can provide noisy signals (e.g.,\nfrom incorrectly predicted outputs), we propose UTDebug that (i) scales UTGen\nvia test-time compute to improve UT output prediction, and (ii) validates and\nbacktracks edits based on multiple generated UTs to avoid overfitting, and\nhelps LLMs debug effectively. We show that UTGen outperforms other LLM-based\nbaselines by 7.59% based on a metric measuring the presence of both\nerror-revealing UT inputs and correct UT outputs. When used with UTDebug, we\nfind that feedback from UTGen's unit tests improves pass@1 accuracy of Qwen2.5\n32B on HumanEvalFix and our own harder debugging split of MBPP+ by over 3.17%\nand 12.35% (respectively) over other LLM-based UT generation baselines. Lastly,\nwe demonstrate that UTGen is a better judge for code correctness, outperforming\na state-of-the-art trained 8B reward model by 4.43% on HumanEval+ with\nbest-of-10 sampling using Qwen2.5 7B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unit tests (UTs) play an instrumental role in assessing code correctness as\nwell as providing feedback to large language models (LLMs), motivating\nautomated test generation. However, we uncover a trade-off between generating\nunit test inputs that reveal errors when given a faulty code and correctly\npredicting the unit test output without access to the gold solution. To address\nthis trade-off, we propose UTGen, which teaches LLMs to generate unit test\ninputs that reveal errors along with their correct expected outputs based on\ntask descriptions. Since model-generated tests can provide noisy signals (e.g.,\nfrom incorrectly predicted outputs), we propose UTDebug that (i) scales UTGen\nvia test-time compute to improve UT output prediction, and (ii) validates and\nbacktracks edits based on multiple generated UTs to avoid overfitting, and\nhelps LLMs debug effectively. We show that UTGen outperforms other LLM-based\nbaselines by 7.59% based on a metric measuring the presence of both\nerror-revealing UT inputs and correct UT outputs. When used with UTDebug, we\nfind that feedback from UTGen's unit tests improves pass@1 accuracy of Qwen2.5\n32B on HumanEvalFix and our own harder debugging split of MBPP+ by over 3.17%\nand 12.35% (respectively) over other LLM-based UT generation baselines. Lastly,\nwe demonstrate that UTGen is a better judge for code correctness, outperforming\na state-of-the-art trained 8B reward model by 4.43% on HumanEval+ with\nbest-of-10 sampling using Qwen2.5 7B."
                },
                "authors": [
                    {
                        "name": "Archiki Prasad"
                    },
                    {
                        "name": "Elias Stengel-Eskin"
                    },
                    {
                        "name": "Justin Chih-Yao Chen"
                    },
                    {
                        "name": "Zaid Khan"
                    },
                    {
                        "name": "Mohit Bansal"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Bansal"
                },
                "author": "Mohit Bansal",
                "arxiv_comment": "First two authors contributed equally. Dataset and Code:\n  https://github.com/archiki/UTGenDebug",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01619v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01619v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19363v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19363v1",
                "updated": "2025-02-26T18:01:19Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    18,
                    1,
                    19,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-26T18:01:19Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    18,
                    1,
                    19,
                    2,
                    57,
                    0
                ],
                "title": "DataMan: Data Manager for Pre-training Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DataMan: Data Manager for Pre-training Large Language Models"
                },
                "summary": "The performance emergence of large language models (LLMs) driven by data\nscaling laws makes the selection of pre-training data increasingly important.\nHowever, existing methods rely on limited heuristics and human intuition,\nlacking comprehensive and clear guidelines. To address this, we are inspired by\n``reverse thinking'' -- prompting LLMs to self-identify which criteria benefit\nits performance. As its pre-training capabilities are related to perplexity\n(PPL), we derive 14 quality criteria from the causes of text perplexity\nanomalies and introduce 15 common application domains to support domain mixing.\nIn this paper, we train a Data Manager (DataMan) to learn quality ratings and\ndomain recognition from pointwise rating, and use it to annotate a 447B token\npre-training corpus with 14 quality ratings and domain type. Our experiments\nvalidate our approach, using DataMan to select 30B tokens to train a\n1.3B-parameter language model, demonstrating significant improvements in\nin-context learning (ICL), perplexity, and instruction-following ability over\nthe state-of-the-art baseline. The best-performing model, based on the Overall\nScore l=5 surpasses a model trained with 50% more data using uniform sampling.\nWe continue pre-training with high-rated, domain-specific data annotated by\nDataMan to enhance domain-specific ICL performance and thus verify DataMan's\ndomain mixing ability. Our findings emphasize the importance of quality\nranking, the complementary nature of quality criteria, and their low\ncorrelation with perplexity, analyzing misalignment between PPL and ICL\nperformance. We also thoroughly analyzed our pre-training dataset, examining\nits composition, the distribution of quality ratings, and the original document\nsources.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The performance emergence of large language models (LLMs) driven by data\nscaling laws makes the selection of pre-training data increasingly important.\nHowever, existing methods rely on limited heuristics and human intuition,\nlacking comprehensive and clear guidelines. To address this, we are inspired by\n``reverse thinking'' -- prompting LLMs to self-identify which criteria benefit\nits performance. As its pre-training capabilities are related to perplexity\n(PPL), we derive 14 quality criteria from the causes of text perplexity\nanomalies and introduce 15 common application domains to support domain mixing.\nIn this paper, we train a Data Manager (DataMan) to learn quality ratings and\ndomain recognition from pointwise rating, and use it to annotate a 447B token\npre-training corpus with 14 quality ratings and domain type. Our experiments\nvalidate our approach, using DataMan to select 30B tokens to train a\n1.3B-parameter language model, demonstrating significant improvements in\nin-context learning (ICL), perplexity, and instruction-following ability over\nthe state-of-the-art baseline. The best-performing model, based on the Overall\nScore l=5 surpasses a model trained with 50% more data using uniform sampling.\nWe continue pre-training with high-rated, domain-specific data annotated by\nDataMan to enhance domain-specific ICL performance and thus verify DataMan's\ndomain mixing ability. Our findings emphasize the importance of quality\nranking, the complementary nature of quality criteria, and their low\ncorrelation with perplexity, analyzing misalignment between PPL and ICL\nperformance. We also thoroughly analyzed our pre-training dataset, examining\nits composition, the distribution of quality ratings, and the original document\nsources."
                },
                "authors": [
                    {
                        "name": "Ru Peng"
                    },
                    {
                        "name": "Kexin Yang"
                    },
                    {
                        "name": "Yawen Zeng"
                    },
                    {
                        "name": "Junyang Lin"
                    },
                    {
                        "name": "Dayiheng Liu"
                    },
                    {
                        "name": "Junbo Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Junbo Zhao"
                },
                "author": "Junbo Zhao",
                "arxiv_comment": "ICLR2025 paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19363v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19363v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19361v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19361v1",
                "updated": "2025-02-26T17:59:27Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    17,
                    59,
                    27,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-26T17:59:27Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    17,
                    59,
                    27,
                    2,
                    57,
                    0
                ],
                "title": "Can Large Language Models Detect Errors in Long Chain-of-Thought\n  Reasoning?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Large Language Models Detect Errors in Long Chain-of-Thought\n  Reasoning?"
                },
                "summary": "Recently, o1-like models have drawn significant attention, where these models\nproduce the long Chain-of-Thought (CoT) reasoning steps to improve the\nreasoning abilities of existing Large Language Models (LLMs). In this paper, to\nunderstand the qualities of these long CoTs and measure the critique abilities\nof existing LLMs on these long CoTs, we introduce the DeltaBench, including the\ngenerated long CoTs from different o1-like models (e.g., QwQ, DeepSeek-R1) for\ndifferent reasoning tasks (e.g., Math, Code, General Reasoning), to measure the\nability to detect errors in long CoT reasoning. Based on DeltaBench, we first\nperform fine-grained analysis of the generated long CoTs to discover the\neffectiveness and efficiency of different o1-like models. Then, we conduct\nextensive evaluations of existing process reward models (PRMs) and critic\nmodels to detect the errors of each annotated process, which aims to\ninvestigate the boundaries and limitations of existing PRMs and critic models.\nFinally, we hope that DeltaBench could guide developers to better understand\nthe long CoT reasoning abilities of their models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, o1-like models have drawn significant attention, where these models\nproduce the long Chain-of-Thought (CoT) reasoning steps to improve the\nreasoning abilities of existing Large Language Models (LLMs). In this paper, to\nunderstand the qualities of these long CoTs and measure the critique abilities\nof existing LLMs on these long CoTs, we introduce the DeltaBench, including the\ngenerated long CoTs from different o1-like models (e.g., QwQ, DeepSeek-R1) for\ndifferent reasoning tasks (e.g., Math, Code, General Reasoning), to measure the\nability to detect errors in long CoT reasoning. Based on DeltaBench, we first\nperform fine-grained analysis of the generated long CoTs to discover the\neffectiveness and efficiency of different o1-like models. Then, we conduct\nextensive evaluations of existing process reward models (PRMs) and critic\nmodels to detect the errors of each annotated process, which aims to\ninvestigate the boundaries and limitations of existing PRMs and critic models.\nFinally, we hope that DeltaBench could guide developers to better understand\nthe long CoT reasoning abilities of their models."
                },
                "authors": [
                    {
                        "name": "Yancheng He"
                    },
                    {
                        "name": "Shilong Li"
                    },
                    {
                        "name": "Jiaheng Liu"
                    },
                    {
                        "name": "Weixun Wang"
                    },
                    {
                        "name": "Xingyuan Bu"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Zhongyuan Peng"
                    },
                    {
                        "name": "Zhaoxiang Zhang"
                    },
                    {
                        "name": "Wenbo Su"
                    },
                    {
                        "name": "Bo Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Bo Zheng"
                },
                "author": "Bo Zheng",
                "arxiv_comment": "The first three authors contributed equally, 27 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19361v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19361v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04318v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04318v2",
                "updated": "2025-02-26T17:51:31Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    17,
                    51,
                    31,
                    2,
                    57,
                    0
                ],
                "published": "2024-12-05T16:34:20Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    16,
                    34,
                    20,
                    3,
                    340,
                    0
                ],
                "title": "The Hyperfitting Phenomenon: Sharpening and Stabilizing LLMs for\n  Open-Ended Text Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Hyperfitting Phenomenon: Sharpening and Stabilizing LLMs for\n  Open-Ended Text Generation"
                },
                "summary": "This paper introduces the counter-intuitive generalization results of\noverfitting pre-trained large language models (LLMs) on very small datasets. In\nthe setting of open-ended text generation, it is well-documented that LLMs tend\nto generate repetitive and dull sequences, a phenomenon that is especially\napparent when generating using greedy decoding. This issue persists even with\nstate-of-the-art LLMs containing billions of parameters, trained via next-token\nprediction on large datasets. We find that by further fine-tuning these models\nto achieve a near-zero training loss on a small set of samples -- a process we\nrefer to as hyperfitting -- the long-sequence generative capabilities are\ngreatly enhanced. Greedy decoding with these Hyperfitted models even outperform\nTop-P sampling over long-sequences, both in terms of diversity and human\npreferences. This phenomenon extends to LLMs of various sizes, different\ndomains, and even autoregressive image generation. We further find this\nphenomena to be distinctly different from that of Grokking and double descent.\nSurprisingly, our experiments indicate that hyperfitted models rarely fall into\nrepeating sequences they were trained on, and even explicitly blocking these\nsequences results in high-quality output. All hyperfitted models produce\nextremely low-entropy predictions, often allocating nearly all probability to a\nsingle token.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces the counter-intuitive generalization results of\noverfitting pre-trained large language models (LLMs) on very small datasets. In\nthe setting of open-ended text generation, it is well-documented that LLMs tend\nto generate repetitive and dull sequences, a phenomenon that is especially\napparent when generating using greedy decoding. This issue persists even with\nstate-of-the-art LLMs containing billions of parameters, trained via next-token\nprediction on large datasets. We find that by further fine-tuning these models\nto achieve a near-zero training loss on a small set of samples -- a process we\nrefer to as hyperfitting -- the long-sequence generative capabilities are\ngreatly enhanced. Greedy decoding with these Hyperfitted models even outperform\nTop-P sampling over long-sequences, both in terms of diversity and human\npreferences. This phenomenon extends to LLMs of various sizes, different\ndomains, and even autoregressive image generation. We further find this\nphenomena to be distinctly different from that of Grokking and double descent.\nSurprisingly, our experiments indicate that hyperfitted models rarely fall into\nrepeating sequences they were trained on, and even explicitly blocking these\nsequences results in high-quality output. All hyperfitted models produce\nextremely low-entropy predictions, often allocating nearly all probability to a\nsingle token."
                },
                "authors": [
                    {
                        "name": "Fredrik Carlsson"
                    },
                    {
                        "name": "Fangyu Liu"
                    },
                    {
                        "name": "Daniel Ward"
                    },
                    {
                        "name": "Murathan Kurfali"
                    },
                    {
                        "name": "Joakim Nivre"
                    }
                ],
                "author_detail": {
                    "name": "Joakim Nivre"
                },
                "author": "Joakim Nivre",
                "arxiv_comment": "Under review at ICLR",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04318v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04318v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.16627v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.16627v2",
                "updated": "2025-02-26T17:39:46Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    17,
                    39,
                    46,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-23T16:04:56Z",
                "published_parsed": [
                    2025,
                    2,
                    23,
                    16,
                    4,
                    56,
                    6,
                    54,
                    0
                ],
                "title": "Energy-Efficient Transformer Inference: Optimization Strategies for Time\n  Series Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Energy-Efficient Transformer Inference: Optimization Strategies for Time\n  Series Classification"
                },
                "summary": "The increasing computational demands of transformer models in time series\nclassification necessitate effective optimization strategies for\nenergy-efficient deployment. This paper presents a systematic investigation of\noptimization techniques, focusing on structured pruning and quantization\nmethods for transformer architectures. Through extensive experimentation on\nthree distinct datasets (RefrigerationDevices, ElectricDevices, and PLAID), we\nquantitatively evaluate model performance and energy efficiency across\ndifferent transformer configurations. Our experimental results demonstrate that\nstatic quantization reduces energy consumption by 29.14% while maintaining\nclassification performance, and L1 pruning achieves a 63% improvement in\ninference speed with minimal accuracy degradation. These findings provide\nvaluable insights into the effectiveness of optimization strategies for\ntransformer-based time series classification, establishing a foundation for\nefficient model deployment in resource-constrained environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing computational demands of transformer models in time series\nclassification necessitate effective optimization strategies for\nenergy-efficient deployment. This paper presents a systematic investigation of\noptimization techniques, focusing on structured pruning and quantization\nmethods for transformer architectures. Through extensive experimentation on\nthree distinct datasets (RefrigerationDevices, ElectricDevices, and PLAID), we\nquantitatively evaluate model performance and energy efficiency across\ndifferent transformer configurations. Our experimental results demonstrate that\nstatic quantization reduces energy consumption by 29.14% while maintaining\nclassification performance, and L1 pruning achieves a 63% improvement in\ninference speed with minimal accuracy degradation. These findings provide\nvaluable insights into the effectiveness of optimization strategies for\ntransformer-based time series classification, establishing a foundation for\nefficient model deployment in resource-constrained environments."
                },
                "authors": [
                    {
                        "name": "Arshia Kermani"
                    },
                    {
                        "name": "Ehsan Zeraatkar"
                    },
                    {
                        "name": "Habib Irani"
                    }
                ],
                "author_detail": {
                    "name": "Habib Irani"
                },
                "author": "Habib Irani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.16627v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.16627v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19347v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19347v1",
                "updated": "2025-02-26T17:38:58Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    17,
                    38,
                    58,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-26T17:38:58Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    17,
                    38,
                    58,
                    2,
                    57,
                    0
                ],
                "title": "Controlled Diversity: Length-optimized Natural Language Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Controlled Diversity: Length-optimized Natural Language Generation"
                },
                "summary": "LLMs are not generally able to adjust the length of their outputs based on\nstrict length requirements, a capability that would improve their usefulness in\napplications that require adherence to diverse user and system requirements. We\npresent an approach to train LLMs to acquire this capability by augmenting\nexisting data and applying existing fine-tuning techniques, which we compare\nbased on the trained models' adherence to the length requirement and overall\nresponse quality relative to the baseline model. Our results demonstrate that\nthese techniques can be successfully applied to train LLMs to adhere to length\nrequirements, with the trained models generating texts which better align to\nthe length requirements. Our results indicate that our method may change the\nresponse quality when using training data that was not generated by the\nbaseline model. This allows simultaneous alignment to another training\nobjective in certain scenarios, but is undesirable otherwise. Training on a\ndataset containing the model's own responses eliminates this issue.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs are not generally able to adjust the length of their outputs based on\nstrict length requirements, a capability that would improve their usefulness in\napplications that require adherence to diverse user and system requirements. We\npresent an approach to train LLMs to acquire this capability by augmenting\nexisting data and applying existing fine-tuning techniques, which we compare\nbased on the trained models' adherence to the length requirement and overall\nresponse quality relative to the baseline model. Our results demonstrate that\nthese techniques can be successfully applied to train LLMs to adhere to length\nrequirements, with the trained models generating texts which better align to\nthe length requirements. Our results indicate that our method may change the\nresponse quality when using training data that was not generated by the\nbaseline model. This allows simultaneous alignment to another training\nobjective in certain scenarios, but is undesirable otherwise. Training on a\ndataset containing the model's own responses eliminates this issue."
                },
                "authors": [
                    {
                        "name": "Diana Marie Schenke"
                    },
                    {
                        "name": "Timo Baumann"
                    }
                ],
                "author_detail": {
                    "name": "Timo Baumann"
                },
                "author": "Timo Baumann",
                "arxiv_comment": "ISCA/ITG Workshop on Diversity in Large Speech and Language Models",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19347v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19347v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.15756v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.15756v4",
                "updated": "2025-02-26T17:32:10Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    17,
                    32,
                    10,
                    2,
                    57,
                    0
                ],
                "published": "2024-05-24T17:51:39Z",
                "published_parsed": [
                    2024,
                    5,
                    24,
                    17,
                    51,
                    39,
                    4,
                    145,
                    0
                ],
                "title": "Wasserstein Distances, Neuronal Entanglement, and Sparsity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wasserstein Distances, Neuronal Entanglement, and Sparsity"
                },
                "summary": "Disentangling polysemantic neurons is at the core of many current approaches\nto interpretability of large language models. Here we attempt to study how\ndisentanglement can be used to understand performance, particularly under\nweight sparsity, a leading post-training optimization technique. We suggest a\nnovel measure for estimating neuronal entanglement: the Wasserstein distance of\na neuron's output distribution to a Gaussian. Moreover, we show the existence\nof a small number of highly entangled \"Wasserstein Neurons\" in each linear\nlayer of an LLM, characterized by their highly non-Gaussian output\ndistributions, their role in mapping similar inputs to dissimilar outputs, and\ntheir significant impact on model accuracy. To study these phenomena, we\npropose a new experimental framework for disentangling polysemantic neurons.\nOur framework separates each layer's inputs to create a mixture of experts\nwhere each neuron's output is computed by a mixture of neurons of lower\nWasserstein distance, each better at maintaining accuracy when sparsified\nwithout retraining. We provide strong evidence that this is because the mixture\nof sparse experts is effectively disentangling the input-output relationship of\nindividual neurons, in particular the difficult Wasserstein neurons.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disentangling polysemantic neurons is at the core of many current approaches\nto interpretability of large language models. Here we attempt to study how\ndisentanglement can be used to understand performance, particularly under\nweight sparsity, a leading post-training optimization technique. We suggest a\nnovel measure for estimating neuronal entanglement: the Wasserstein distance of\na neuron's output distribution to a Gaussian. Moreover, we show the existence\nof a small number of highly entangled \"Wasserstein Neurons\" in each linear\nlayer of an LLM, characterized by their highly non-Gaussian output\ndistributions, their role in mapping similar inputs to dissimilar outputs, and\ntheir significant impact on model accuracy. To study these phenomena, we\npropose a new experimental framework for disentangling polysemantic neurons.\nOur framework separates each layer's inputs to create a mixture of experts\nwhere each neuron's output is computed by a mixture of neurons of lower\nWasserstein distance, each better at maintaining accuracy when sparsified\nwithout retraining. We provide strong evidence that this is because the mixture\nof sparse experts is effectively disentangling the input-output relationship of\nindividual neurons, in particular the difficult Wasserstein neurons."
                },
                "authors": [
                    {
                        "name": "Shashata Sawmya"
                    },
                    {
                        "name": "Linghao Kong"
                    },
                    {
                        "name": "Ilia Markov"
                    },
                    {
                        "name": "Dan Alistarh"
                    },
                    {
                        "name": "Nir Shavit"
                    }
                ],
                "author_detail": {
                    "name": "Nir Shavit"
                },
                "author": "Nir Shavit",
                "arxiv_comment": "10 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.15756v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.15756v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19339v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19339v1",
                "updated": "2025-02-26T17:32:07Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    17,
                    32,
                    7,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-26T17:32:07Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    17,
                    32,
                    7,
                    2,
                    57,
                    0
                ],
                "title": "Evaluating LLMs and Pre-trained Models for Text Summarization Across\n  Diverse Datasets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating LLMs and Pre-trained Models for Text Summarization Across\n  Diverse Datasets"
                },
                "summary": "Text summarization plays a crucial role in natural language processing by\ncondensing large volumes of text into concise and coherent summaries. As\ndigital content continues to grow rapidly and the demand for effective\ninformation retrieval increases, text summarization has become a focal point of\nresearch in recent years. This study offers a thorough evaluation of four\nleading pre-trained and open-source large language models: BART, FLAN-T5,\nLLaMA-3-8B, and Gemma-7B, across five diverse datasets CNN/DM, Gigaword, News\nSummary, XSum, and BBC News. The evaluation employs widely recognized automatic\nmetrics, including ROUGE-1, ROUGE-2, ROUGE-L, BERTScore, and METEOR, to assess\nthe models' capabilities in generating coherent and informative summaries. The\nresults reveal the comparative strengths and limitations of these models in\nprocessing various text types.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text summarization plays a crucial role in natural language processing by\ncondensing large volumes of text into concise and coherent summaries. As\ndigital content continues to grow rapidly and the demand for effective\ninformation retrieval increases, text summarization has become a focal point of\nresearch in recent years. This study offers a thorough evaluation of four\nleading pre-trained and open-source large language models: BART, FLAN-T5,\nLLaMA-3-8B, and Gemma-7B, across five diverse datasets CNN/DM, Gigaword, News\nSummary, XSum, and BBC News. The evaluation employs widely recognized automatic\nmetrics, including ROUGE-1, ROUGE-2, ROUGE-L, BERTScore, and METEOR, to assess\nthe models' capabilities in generating coherent and informative summaries. The\nresults reveal the comparative strengths and limitations of these models in\nprocessing various text types."
                },
                "authors": [
                    {
                        "name": "Tohida Rehman"
                    },
                    {
                        "name": "Soumabha Ghosh"
                    },
                    {
                        "name": "Kuntal Das"
                    },
                    {
                        "name": "Souvik Bhattacharjee"
                    },
                    {
                        "name": "Debarshi Kumar Sanyal"
                    },
                    {
                        "name": "Samiran Chattopadhyay"
                    }
                ],
                "author_detail": {
                    "name": "Samiran Chattopadhyay"
                },
                "author": "Samiran Chattopadhyay",
                "arxiv_comment": "5 pages, 2 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19339v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19339v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19328v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19328v1",
                "updated": "2025-02-26T17:19:12Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    17,
                    19,
                    12,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-26T17:19:12Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    17,
                    19,
                    12,
                    2,
                    57,
                    0
                ],
                "title": "Agentic Reward Modeling: Integrating Human Preferences with Verifiable\n  Correctness Signals for Reliable Reward Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentic Reward Modeling: Integrating Human Preferences with Verifiable\n  Correctness Signals for Reliable Reward Systems"
                },
                "summary": "Reward models (RMs) are crucial for the training and inference-time scaling\nup of large language models (LLMs). However, existing reward models primarily\nfocus on human preferences, neglecting verifiable correctness signals which\nhave shown strong potential in training LLMs. In this paper, we propose agentic\nreward modeling, a reward system that combines reward models with verifiable\ncorrectness signals from different aspects to provide reliable rewards. We\nempirically implement a reward agent, named RewardAgent, that combines human\npreference rewards with two verifiable signals: factuality and instruction\nfollowing, to provide more reliable rewards. We conduct comprehensive\nexperiments on existing reward model benchmarks and inference time best-of-n\nsearches on real-world downstream tasks. RewardAgent significantly outperforms\nvanilla reward models, demonstrating its effectiveness. We further construct\ntraining preference pairs using RewardAgent and train an LLM with the DPO\nobjective, achieving superior performance on various NLP benchmarks compared to\nconventional reward models. Our codes are publicly released to facilitate\nfurther research (https://github.com/THU-KEG/Agentic-Reward-Modeling).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reward models (RMs) are crucial for the training and inference-time scaling\nup of large language models (LLMs). However, existing reward models primarily\nfocus on human preferences, neglecting verifiable correctness signals which\nhave shown strong potential in training LLMs. In this paper, we propose agentic\nreward modeling, a reward system that combines reward models with verifiable\ncorrectness signals from different aspects to provide reliable rewards. We\nempirically implement a reward agent, named RewardAgent, that combines human\npreference rewards with two verifiable signals: factuality and instruction\nfollowing, to provide more reliable rewards. We conduct comprehensive\nexperiments on existing reward model benchmarks and inference time best-of-n\nsearches on real-world downstream tasks. RewardAgent significantly outperforms\nvanilla reward models, demonstrating its effectiveness. We further construct\ntraining preference pairs using RewardAgent and train an LLM with the DPO\nobjective, achieving superior performance on various NLP benchmarks compared to\nconventional reward models. Our codes are publicly released to facilitate\nfurther research (https://github.com/THU-KEG/Agentic-Reward-Modeling)."
                },
                "authors": [
                    {
                        "name": "Hao Peng"
                    },
                    {
                        "name": "Yunjia Qi"
                    },
                    {
                        "name": "Xiaozhi Wang"
                    },
                    {
                        "name": "Zijun Yao"
                    },
                    {
                        "name": "Bin Xu"
                    },
                    {
                        "name": "Lei Hou"
                    },
                    {
                        "name": "Juanzi Li"
                    }
                ],
                "author_detail": {
                    "name": "Juanzi Li"
                },
                "author": "Juanzi Li",
                "arxiv_comment": "16 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19328v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19328v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19320v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19320v1",
                "updated": "2025-02-26T17:13:19Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    17,
                    13,
                    19,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-26T17:13:19Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    17,
                    13,
                    19,
                    2,
                    57,
                    0
                ],
                "title": "Shh, don't say that! Domain Certification in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shh, don't say that! Domain Certification in LLMs"
                },
                "summary": "Large language models (LLMs) are often deployed to perform constrained tasks,\nwith narrow domains. For example, customer support bots can be built on top of\nLLMs, relying on their broad language understanding and capabilities to enhance\nperformance. However, these LLMs are adversarially susceptible, potentially\ngenerating outputs outside the intended domain. To formalize, assess, and\nmitigate this risk, we introduce domain certification; a guarantee that\naccurately characterizes the out-of-domain behavior of language models. We then\npropose a simple yet effective approach, which we call VALID that provides\nadversarial bounds as a certificate. Finally, we evaluate our method across a\ndiverse set of datasets, demonstrating that it yields meaningful certificates,\nwhich bound the probability of out-of-domain samples tightly with minimum\npenalty to refusal behavior.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are often deployed to perform constrained tasks,\nwith narrow domains. For example, customer support bots can be built on top of\nLLMs, relying on their broad language understanding and capabilities to enhance\nperformance. However, these LLMs are adversarially susceptible, potentially\ngenerating outputs outside the intended domain. To formalize, assess, and\nmitigate this risk, we introduce domain certification; a guarantee that\naccurately characterizes the out-of-domain behavior of language models. We then\npropose a simple yet effective approach, which we call VALID that provides\nadversarial bounds as a certificate. Finally, we evaluate our method across a\ndiverse set of datasets, demonstrating that it yields meaningful certificates,\nwhich bound the probability of out-of-domain samples tightly with minimum\npenalty to refusal behavior."
                },
                "authors": [
                    {
                        "name": "Cornelius Emde"
                    },
                    {
                        "name": "Alasdair Paren"
                    },
                    {
                        "name": "Preetham Arvind"
                    },
                    {
                        "name": "Maxime Kayser"
                    },
                    {
                        "name": "Tom Rainforth"
                    },
                    {
                        "name": "Thomas Lukasiewicz"
                    },
                    {
                        "name": "Bernard Ghanem"
                    },
                    {
                        "name": "Philip H. S. Torr"
                    },
                    {
                        "name": "Adel Bibi"
                    }
                ],
                "author_detail": {
                    "name": "Adel Bibi"
                },
                "author": "Adel Bibi",
                "arxiv_comment": "10 pages, includes appendix Published in International Conference on\n  Learning Representations (ICLR) 2025",
                "arxiv_journal_ref": "International Conference on Learning Representations (ICLR) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19320v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19320v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19312v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19312v1",
                "updated": "2025-02-26T17:08:46Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    17,
                    8,
                    46,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-26T17:08:46Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    17,
                    8,
                    46,
                    2,
                    57,
                    0
                ],
                "title": "FSPO: Few-Shot Preference Optimization of Synthetic Preference Data in\n  LLMs Elicits Effective Personalization to Real Users",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FSPO: Few-Shot Preference Optimization of Synthetic Preference Data in\n  LLMs Elicits Effective Personalization to Real Users"
                },
                "summary": "Effective personalization of LLMs is critical for a broad range of\nuser-interfacing applications such as virtual assistants and content curation.\nInspired by the strong in-context learning capabilities of LLMs, we propose\nFew-Shot Preference Optimization (FSPO), which reframes reward modeling as a\nmeta-learning problem. Under this framework, an LLM learns to quickly adapt to\na user via a few labeled preferences from that user, constructing a\npersonalized reward function for them. Additionally, since real-world\npreference data is scarce and challenging to collect at scale, we propose\ncareful design choices to construct synthetic preference datasets for\npersonalization, generating over 1M synthetic personalized preferences using\npublicly available LLMs. In particular, to successfully transfer from synthetic\ndata to real users, we find it crucial for the data to exhibit both high\ndiversity and coherent, self-consistent structure. We evaluate FSPO on\npersonalized open-ended generation for up to 1,500 synthetic users across\nacross three domains: movie reviews, pedagogical adaptation based on\neducational background, and general question answering, along with a controlled\nhuman study. Overall, FSPO achieves an 87% Alpaca Eval winrate on average in\ngenerating responses that are personalized to synthetic users and a 72% winrate\nwith real human users in open-ended question answering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective personalization of LLMs is critical for a broad range of\nuser-interfacing applications such as virtual assistants and content curation.\nInspired by the strong in-context learning capabilities of LLMs, we propose\nFew-Shot Preference Optimization (FSPO), which reframes reward modeling as a\nmeta-learning problem. Under this framework, an LLM learns to quickly adapt to\na user via a few labeled preferences from that user, constructing a\npersonalized reward function for them. Additionally, since real-world\npreference data is scarce and challenging to collect at scale, we propose\ncareful design choices to construct synthetic preference datasets for\npersonalization, generating over 1M synthetic personalized preferences using\npublicly available LLMs. In particular, to successfully transfer from synthetic\ndata to real users, we find it crucial for the data to exhibit both high\ndiversity and coherent, self-consistent structure. We evaluate FSPO on\npersonalized open-ended generation for up to 1,500 synthetic users across\nacross three domains: movie reviews, pedagogical adaptation based on\neducational background, and general question answering, along with a controlled\nhuman study. Overall, FSPO achieves an 87% Alpaca Eval winrate on average in\ngenerating responses that are personalized to synthetic users and a 72% winrate\nwith real human users in open-ended question answering."
                },
                "authors": [
                    {
                        "name": "Anikait Singh"
                    },
                    {
                        "name": "Sheryl Hsu"
                    },
                    {
                        "name": "Kyle Hsu"
                    },
                    {
                        "name": "Eric Mitchell"
                    },
                    {
                        "name": "Stefano Ermon"
                    },
                    {
                        "name": "Tatsunori Hashimoto"
                    },
                    {
                        "name": "Archit Sharma"
                    },
                    {
                        "name": "Chelsea Finn"
                    }
                ],
                "author_detail": {
                    "name": "Chelsea Finn"
                },
                "author": "Chelsea Finn",
                "arxiv_comment": "Website: https://fewshot-preference-optimization.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19312v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19312v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19311v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19311v1",
                "updated": "2025-02-26T17:08:07Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    17,
                    8,
                    7,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-26T17:08:07Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    17,
                    8,
                    7,
                    2,
                    57,
                    0
                ],
                "title": "Faithful Logic Embeddings in HOL -- A recipe to have it all: deep and\n  shallow, automated and interactive, heavy and light, proofs and\n  counterexamples, meta and object level",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Faithful Logic Embeddings in HOL -- A recipe to have it all: deep and\n  shallow, automated and interactive, heavy and light, proofs and\n  counterexamples, meta and object level"
                },
                "summary": "Deep and shallow embeddings of non-classical logics in classical higher-order\nlogic have been explored, implemented, and used in various automated reasoning\ntools in recent years. This paper presents a recipe for the simultaneous\ndeployment of different forms of deep and shallow embeddings in classical\nhigher-order logic, enabling not only flexible interactive and automated\ntheorem proving and counterexample finding at meta and object level, but also\nautomated faithfulness proofs between the logic embeddings. The approach, which\nis fruitful for logic education, research and application, is deliberately\nillustrated here using simple propositional modal logic. However, the work\npresented is conceptual in nature and not limited to such a simple logic\ncontext.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep and shallow embeddings of non-classical logics in classical higher-order\nlogic have been explored, implemented, and used in various automated reasoning\ntools in recent years. This paper presents a recipe for the simultaneous\ndeployment of different forms of deep and shallow embeddings in classical\nhigher-order logic, enabling not only flexible interactive and automated\ntheorem proving and counterexample finding at meta and object level, but also\nautomated faithfulness proofs between the logic embeddings. The approach, which\nis fruitful for logic education, research and application, is deliberately\nillustrated here using simple propositional modal logic. However, the work\npresented is conceptual in nature and not limited to such a simple logic\ncontext."
                },
                "authors": [
                    {
                        "name": "Christoph Benzmüller"
                    }
                ],
                "author_detail": {
                    "name": "Christoph Benzmüller"
                },
                "author": "Christoph Benzmüller",
                "arxiv_comment": "22 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19311v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19311v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "03Axx, 03Bxx, 03B15, 68T15",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "F.4; I.2.3; I.2.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19301v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19301v1",
                "updated": "2025-02-26T16:59:21Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    16,
                    59,
                    21,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-26T16:59:21Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    16,
                    59,
                    21,
                    2,
                    57,
                    0
                ],
                "title": "Rethinking LLM Unlearning Objectives: A Gradient Perspective and Go\n  Beyond",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking LLM Unlearning Objectives: A Gradient Perspective and Go\n  Beyond"
                },
                "summary": "Large language models (LLMs) should undergo rigorous audits to identify\npotential risks, such as copyright and privacy infringements. Once these risks\nemerge, timely updates are crucial to remove undesirable responses, ensuring\nlegal and safe model usage. It has spurred recent research into LLM unlearning,\nfocusing on erasing targeted undesirable knowledge without compromising the\nintegrity of other, non-targeted responses. Existing studies have introduced\nvarious unlearning objectives to pursue LLM unlearning without necessitating\ncomplete retraining. However, each of these objectives has unique properties,\nand no unified framework is currently available to comprehend them thoroughly.\nTo fill the gap, we propose a toolkit of the gradient effect (G-effect),\nquantifying the impacts of unlearning objectives on model performance from a\ngradient perspective. A notable advantage is its broad ability to detail the\nunlearning impacts from various aspects across instances, updating steps, and\nLLM layers. Accordingly, the G-effect offers new insights into identifying\ndrawbacks of existing unlearning objectives, further motivating us to explore a\nseries of new solutions for their mitigation and improvements. Finally, we\noutline promising directions that merit further studies, aiming at contributing\nto the community to advance this important field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) should undergo rigorous audits to identify\npotential risks, such as copyright and privacy infringements. Once these risks\nemerge, timely updates are crucial to remove undesirable responses, ensuring\nlegal and safe model usage. It has spurred recent research into LLM unlearning,\nfocusing on erasing targeted undesirable knowledge without compromising the\nintegrity of other, non-targeted responses. Existing studies have introduced\nvarious unlearning objectives to pursue LLM unlearning without necessitating\ncomplete retraining. However, each of these objectives has unique properties,\nand no unified framework is currently available to comprehend them thoroughly.\nTo fill the gap, we propose a toolkit of the gradient effect (G-effect),\nquantifying the impacts of unlearning objectives on model performance from a\ngradient perspective. A notable advantage is its broad ability to detail the\nunlearning impacts from various aspects across instances, updating steps, and\nLLM layers. Accordingly, the G-effect offers new insights into identifying\ndrawbacks of existing unlearning objectives, further motivating us to explore a\nseries of new solutions for their mitigation and improvements. Finally, we\noutline promising directions that merit further studies, aiming at contributing\nto the community to advance this important field."
                },
                "authors": [
                    {
                        "name": "Qizhou Wang"
                    },
                    {
                        "name": "Jin Peng Zhou"
                    },
                    {
                        "name": "Zhanke Zhou"
                    },
                    {
                        "name": "Saebyeol Shin"
                    },
                    {
                        "name": "Bo Han"
                    },
                    {
                        "name": "Kilian Q. Weinberger"
                    }
                ],
                "author_detail": {
                    "name": "Kilian Q. Weinberger"
                },
                "author": "Kilian Q. Weinberger",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19301v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19301v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19298v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19298v1",
                "updated": "2025-02-26T16:56:19Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    16,
                    56,
                    19,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-26T16:56:19Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    16,
                    56,
                    19,
                    2,
                    57,
                    0
                ],
                "title": "Agent-centric Information Access",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agent-centric Information Access"
                },
                "summary": "As large language models (LLMs) become more specialized, we envision a future\nwhere millions of expert LLMs exist, each trained on proprietary data and\nexcelling in specific domains. In such a system, answering a query requires\nselecting a small subset of relevant models, querying them efficiently, and\nsynthesizing their responses. This paper introduces a framework for\nagent-centric information access, where LLMs function as knowledge agents that\nare dynamically ranked and queried based on their demonstrated expertise.\nUnlike traditional document retrieval, this approach requires inferring\nexpertise on the fly, rather than relying on static metadata or predefined\nmodel descriptions. This shift introduces several challenges, including\nefficient expert selection, cost-effective querying, response aggregation\nacross multiple models, and robustness against adversarial manipulation. To\naddress these issues, we propose a scalable evaluation framework that leverages\nretrieval-augmented generation and clustering techniques to construct and\nassess thousands of specialized models, with the potential to scale toward\nmillions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) become more specialized, we envision a future\nwhere millions of expert LLMs exist, each trained on proprietary data and\nexcelling in specific domains. In such a system, answering a query requires\nselecting a small subset of relevant models, querying them efficiently, and\nsynthesizing their responses. This paper introduces a framework for\nagent-centric information access, where LLMs function as knowledge agents that\nare dynamically ranked and queried based on their demonstrated expertise.\nUnlike traditional document retrieval, this approach requires inferring\nexpertise on the fly, rather than relying on static metadata or predefined\nmodel descriptions. This shift introduces several challenges, including\nefficient expert selection, cost-effective querying, response aggregation\nacross multiple models, and robustness against adversarial manipulation. To\naddress these issues, we propose a scalable evaluation framework that leverages\nretrieval-augmented generation and clustering techniques to construct and\nassess thousands of specialized models, with the potential to scale toward\nmillions."
                },
                "authors": [
                    {
                        "name": "Evangelos Kanoulas"
                    },
                    {
                        "name": "Panagiotis Eustratiadis"
                    },
                    {
                        "name": "Yongkang Li"
                    },
                    {
                        "name": "Yougang Lyu"
                    },
                    {
                        "name": "Vaishali Pal"
                    },
                    {
                        "name": "Gabrielle Poerwawinata"
                    },
                    {
                        "name": "Jingfen Qiao"
                    },
                    {
                        "name": "Zihan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zihan Wang"
                },
                "author": "Zihan Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19298v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19298v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19295v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19295v1",
                "updated": "2025-02-26T16:52:31Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    16,
                    52,
                    31,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-26T16:52:31Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    16,
                    52,
                    31,
                    2,
                    57,
                    0
                ],
                "title": "Complex LLM Planning via Automated Heuristics Discovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Complex LLM Planning via Automated Heuristics Discovery"
                },
                "summary": "We consider enhancing large language models (LLMs) for complex planning\ntasks. While existing methods allow LLMs to explore intermediate steps to make\nplans, they either depend on unreliable self-verification or external verifiers\nto evaluate these steps, which demand significant data and computations. Here,\nwe propose automated heuristics discovery (AutoHD), a novel approach that\nenables LLMs to explicitly generate heuristic functions to guide inference-time\nsearch, allowing accurate evaluation of intermediate states. These heuristic\nfunctions are further refined through a heuristic evolution process, improving\ntheir robustness and effectiveness. Our proposed method requires no additional\nmodel training or fine-tuning, and the explicit definition of heuristic\nfunctions generated by the LLMs provides interpretability and insights into the\nreasoning process. Extensive experiments across diverse benchmarks demonstrate\nsignificant gains over multiple baselines, including nearly twice the accuracy\non some datasets, establishing our approach as a reliable and interpretable\nsolution for complex planning tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider enhancing large language models (LLMs) for complex planning\ntasks. While existing methods allow LLMs to explore intermediate steps to make\nplans, they either depend on unreliable self-verification or external verifiers\nto evaluate these steps, which demand significant data and computations. Here,\nwe propose automated heuristics discovery (AutoHD), a novel approach that\nenables LLMs to explicitly generate heuristic functions to guide inference-time\nsearch, allowing accurate evaluation of intermediate states. These heuristic\nfunctions are further refined through a heuristic evolution process, improving\ntheir robustness and effectiveness. Our proposed method requires no additional\nmodel training or fine-tuning, and the explicit definition of heuristic\nfunctions generated by the LLMs provides interpretability and insights into the\nreasoning process. Extensive experiments across diverse benchmarks demonstrate\nsignificant gains over multiple baselines, including nearly twice the accuracy\non some datasets, establishing our approach as a reliable and interpretable\nsolution for complex planning tasks."
                },
                "authors": [
                    {
                        "name": "Hongyi Ling"
                    },
                    {
                        "name": "Shubham Parashar"
                    },
                    {
                        "name": "Sambhav Khurana"
                    },
                    {
                        "name": "Blake Olson"
                    },
                    {
                        "name": "Anwesha Basu"
                    },
                    {
                        "name": "Gaurangi Sinha"
                    },
                    {
                        "name": "Zhengzhong Tu"
                    },
                    {
                        "name": "James Caverlee"
                    },
                    {
                        "name": "Shuiwang Ji"
                    }
                ],
                "author_detail": {
                    "name": "Shuiwang Ji"
                },
                "author": "Shuiwang Ji",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19295v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19295v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02764v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02764v2",
                "updated": "2025-02-26T16:52:21Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    16,
                    52,
                    21,
                    2,
                    57,
                    0
                ],
                "published": "2024-12-03T19:05:37Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    19,
                    5,
                    37,
                    1,
                    338,
                    0
                ],
                "title": "Drawing Pandas: A Benchmark for LLMs in Generating Plotting Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Drawing Pandas: A Benchmark for LLMs in Generating Plotting Code"
                },
                "summary": "This paper introduces the human-curated PandasPlotBench dataset, designed to\nevaluate language models' effectiveness as assistants in visual data\nexploration. Our benchmark focuses on generating code for visualizing tabular\ndata - such as a Pandas DataFrame - based on natural language instructions,\ncomplementing current evaluation tools and expanding their scope. The dataset\nincludes 175 unique tasks. Our experiments assess several leading Large\nLanguage Models (LLMs) across three visualization libraries: Matplotlib,\nSeaborn, and Plotly. We show that the shortening of tasks has a minimal effect\non plotting capabilities, allowing for the user interface that accommodates\nconcise user input without sacrificing functionality or accuracy. Another of\nour findings reveals that while LLMs perform well with popular libraries like\nMatplotlib and Seaborn, challenges persist with Plotly, highlighting areas for\nimprovement. We hope that the modular design of our benchmark will broaden the\ncurrent studies on generating visualizations. Our dataset and benchmark code\nare available online:\nhttps://huggingface.co/datasets/JetBrains-Research/PandasPlotBench;\nhttps://github.com/JetBrains-Research/PandasPlotBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces the human-curated PandasPlotBench dataset, designed to\nevaluate language models' effectiveness as assistants in visual data\nexploration. Our benchmark focuses on generating code for visualizing tabular\ndata - such as a Pandas DataFrame - based on natural language instructions,\ncomplementing current evaluation tools and expanding their scope. The dataset\nincludes 175 unique tasks. Our experiments assess several leading Large\nLanguage Models (LLMs) across three visualization libraries: Matplotlib,\nSeaborn, and Plotly. We show that the shortening of tasks has a minimal effect\non plotting capabilities, allowing for the user interface that accommodates\nconcise user input without sacrificing functionality or accuracy. Another of\nour findings reveals that while LLMs perform well with popular libraries like\nMatplotlib and Seaborn, challenges persist with Plotly, highlighting areas for\nimprovement. We hope that the modular design of our benchmark will broaden the\ncurrent studies on generating visualizations. Our dataset and benchmark code\nare available online:\nhttps://huggingface.co/datasets/JetBrains-Research/PandasPlotBench;\nhttps://github.com/JetBrains-Research/PandasPlotBench."
                },
                "authors": [
                    {
                        "name": "Timur Galimzyanov"
                    },
                    {
                        "name": "Sergey Titov"
                    },
                    {
                        "name": "Yaroslav Golubev"
                    },
                    {
                        "name": "Egor Bogomolov"
                    }
                ],
                "author_detail": {
                    "name": "Egor Bogomolov"
                },
                "author": "Egor Bogomolov",
                "arxiv_comment": "5 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02764v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02764v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16502v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16502v2",
                "updated": "2025-02-26T16:46:25Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    16,
                    46,
                    25,
                    2,
                    57,
                    0
                ],
                "published": "2024-11-25T15:37:27Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    15,
                    37,
                    27,
                    0,
                    330,
                    0
                ],
                "title": "Interpreting Language Reward Models via Contrastive Explanations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interpreting Language Reward Models via Contrastive Explanations"
                },
                "summary": "Reward models (RMs) are a crucial component in the alignment of large\nlanguage models' (LLMs) outputs with human values. RMs approximate human\npreferences over possible LLM responses to the same prompt by predicting and\ncomparing reward scores. However, as they are typically modified versions of\nLLMs with scalar output heads, RMs are large black boxes whose predictions are\nnot explainable. More transparent RMs would enable improved trust in the\nalignment of LLMs. In this work, we propose to use contrastive explanations to\nexplain any binary response comparison made by an RM. Specifically, we generate\na diverse set of new comparisons similar to the original one to characterise\nthe RM's local behaviour. The perturbed responses forming the new comparisons\nare generated to explicitly modify manually specified high-level evaluation\nattributes, on which analyses of RM behaviour are grounded. In quantitative\nexperiments, we validate the effectiveness of our method for finding\nhigh-quality contrastive explanations. We then showcase the qualitative\nusefulness of our method for investigating global sensitivity of RMs to each\nevaluation attribute, and demonstrate how representative examples can be\nautomatically extracted to explain and compare behaviours of different RMs. We\nsee our method as a flexible framework for RM explanation, providing a basis\nfor more interpretable and trustworthy LLM alignment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reward models (RMs) are a crucial component in the alignment of large\nlanguage models' (LLMs) outputs with human values. RMs approximate human\npreferences over possible LLM responses to the same prompt by predicting and\ncomparing reward scores. However, as they are typically modified versions of\nLLMs with scalar output heads, RMs are large black boxes whose predictions are\nnot explainable. More transparent RMs would enable improved trust in the\nalignment of LLMs. In this work, we propose to use contrastive explanations to\nexplain any binary response comparison made by an RM. Specifically, we generate\na diverse set of new comparisons similar to the original one to characterise\nthe RM's local behaviour. The perturbed responses forming the new comparisons\nare generated to explicitly modify manually specified high-level evaluation\nattributes, on which analyses of RM behaviour are grounded. In quantitative\nexperiments, we validate the effectiveness of our method for finding\nhigh-quality contrastive explanations. We then showcase the qualitative\nusefulness of our method for investigating global sensitivity of RMs to each\nevaluation attribute, and demonstrate how representative examples can be\nautomatically extracted to explain and compare behaviours of different RMs. We\nsee our method as a flexible framework for RM explanation, providing a basis\nfor more interpretable and trustworthy LLM alignment."
                },
                "authors": [
                    {
                        "name": "Junqi Jiang"
                    },
                    {
                        "name": "Tom Bewley"
                    },
                    {
                        "name": "Saumitra Mishra"
                    },
                    {
                        "name": "Freddy Lecue"
                    },
                    {
                        "name": "Manuela Veloso"
                    }
                ],
                "author_detail": {
                    "name": "Manuela Veloso"
                },
                "author": "Manuela Veloso",
                "arxiv_comment": "Accepted at ICLR 2025 conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16502v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16502v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06689v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06689v3",
                "updated": "2025-02-26T16:36:55Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    16,
                    36,
                    55,
                    2,
                    57,
                    0
                ],
                "published": "2025-01-12T02:43:59Z",
                "published_parsed": [
                    2025,
                    1,
                    12,
                    2,
                    43,
                    59,
                    6,
                    12,
                    0
                ],
                "title": "TAPO: Task-Referenced Adaptation for Prompt Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TAPO: Task-Referenced Adaptation for Prompt Optimization"
                },
                "summary": "Prompt engineering can significantly improve the performance of large\nlanguage models (LLMs), with automated prompt optimization (APO) gaining\nsignificant attention due to the time-consuming and laborious nature of manual\nprompt design. However, much of the existing work in APO overlooks\ntask-specific characteristics, resulting in prompts that lack domain\nspecificity and are not well-suited for task-specific optimization. In this\npaper, we introduce TAPO, a multitask-aware prompt optimization framework\ncomposed of three key modules. First, a task-aware metric selection module is\nproposed to enhance task-specific prompt generation capabilities. Second, we\npresent a multi-metrics evaluation module to jointly evaluate prompts from\nmultiple perspectives. Third, an evolution-based optimization framework is\nintroduced for automatic prompt refinement, which improves adaptability across\nvarious tasks. Extensive experiments on six datasets demonstrate the\neffectiveness of our approach, and our code is publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt engineering can significantly improve the performance of large\nlanguage models (LLMs), with automated prompt optimization (APO) gaining\nsignificant attention due to the time-consuming and laborious nature of manual\nprompt design. However, much of the existing work in APO overlooks\ntask-specific characteristics, resulting in prompts that lack domain\nspecificity and are not well-suited for task-specific optimization. In this\npaper, we introduce TAPO, a multitask-aware prompt optimization framework\ncomposed of three key modules. First, a task-aware metric selection module is\nproposed to enhance task-specific prompt generation capabilities. Second, we\npresent a multi-metrics evaluation module to jointly evaluate prompts from\nmultiple perspectives. Third, an evolution-based optimization framework is\nintroduced for automatic prompt refinement, which improves adaptability across\nvarious tasks. Extensive experiments on six datasets demonstrate the\neffectiveness of our approach, and our code is publicly available."
                },
                "authors": [
                    {
                        "name": "Wenxin Luo"
                    },
                    {
                        "name": "Weirui Wang"
                    },
                    {
                        "name": "Xiaopeng Li"
                    },
                    {
                        "name": "Weibo Zhou"
                    },
                    {
                        "name": "Pengyue Jia"
                    },
                    {
                        "name": "Xiangyu Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyu Zhao"
                },
                "author": "Xiangyu Zhao",
                "arxiv_comment": "Accepted to ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06689v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06689v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19280v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19280v1",
                "updated": "2025-02-26T16:36:24Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    16,
                    36,
                    24,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-26T16:36:24Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    16,
                    36,
                    24,
                    2,
                    57,
                    0
                ],
                "title": "Efficient Federated Search for Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Federated Search for Retrieval-Augmented Generation"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable capabilities across\nvarious domains but remain susceptible to hallucinations and inconsistencies,\nlimiting their reliability. Retrieval-augmented generation (RAG) mitigates\nthese issues by grounding model responses in external knowledge sources.\nExisting RAG workflows often leverage a single vector database, which is\nimpractical in the common setting where information is distributed across\nmultiple repositories. We introduce RAGRoute, a novel mechanism for federated\nRAG search. RAGRoute dynamically selects relevant data sources at query time\nusing a lightweight neural network classifier. By not querying every data\nsource, this approach significantly reduces query overhead, improves retrieval\nefficiency, and minimizes the retrieval of irrelevant information. We evaluate\nRAGRoute using the MIRAGE and MMLU benchmarks and demonstrate its effectiveness\nin retrieving relevant documents while reducing the number of queries. RAGRoute\nreduces the total number of queries up to 77.5% and communication volume up to\n76.2%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable capabilities across\nvarious domains but remain susceptible to hallucinations and inconsistencies,\nlimiting their reliability. Retrieval-augmented generation (RAG) mitigates\nthese issues by grounding model responses in external knowledge sources.\nExisting RAG workflows often leverage a single vector database, which is\nimpractical in the common setting where information is distributed across\nmultiple repositories. We introduce RAGRoute, a novel mechanism for federated\nRAG search. RAGRoute dynamically selects relevant data sources at query time\nusing a lightweight neural network classifier. By not querying every data\nsource, this approach significantly reduces query overhead, improves retrieval\nefficiency, and minimizes the retrieval of irrelevant information. We evaluate\nRAGRoute using the MIRAGE and MMLU benchmarks and demonstrate its effectiveness\nin retrieving relevant documents while reducing the number of queries. RAGRoute\nreduces the total number of queries up to 77.5% and communication volume up to\n76.2%."
                },
                "authors": [
                    {
                        "name": "Rachid Guerraoui"
                    },
                    {
                        "name": "Anne-Marie Kermarrec"
                    },
                    {
                        "name": "Diana Petrescu"
                    },
                    {
                        "name": "Rafael Pires"
                    },
                    {
                        "name": "Mathis Randl"
                    },
                    {
                        "name": "Martijn de Vos"
                    }
                ],
                "author_detail": {
                    "name": "Martijn de Vos"
                },
                "author": "Martijn de Vos",
                "arxiv_comment": "To appear in the proceedings of EuroMLSys'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19280v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19280v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19263v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19263v1",
                "updated": "2025-02-26T16:17:15Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    16,
                    17,
                    15,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-26T16:17:15Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    16,
                    17,
                    15,
                    2,
                    57,
                    0
                ],
                "title": "ArtInsight: Enabling AI-Powered Artwork Engagement for Mixed\n  Visual-Ability Families",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ArtInsight: Enabling AI-Powered Artwork Engagement for Mixed\n  Visual-Ability Families"
                },
                "summary": "We introduce ArtInsight, a novel AI-powered system to facilitate deeper\nengagement with child-created artwork in mixed visual-ability families.\nArtInsight leverages large language models (LLMs) to craft a respectful and\nthorough initial description of a child's artwork, and provides: creative\nAI-generated descriptions for a vivid overview, audio recording to capture the\nchild's own description of their artwork, and a set of AI-generated questions\nto facilitate discussion between blind or low-vision (BLV) family members and\ntheir children. Alongside ArtInsight, we also contribute a new rubric to score\nAI-generated descriptions of child-created artwork and an assessment of\nstate-of-the-art LLMs. We evaluated ArtInsight with five groups of BLV family\nmembers and their children, and as a case study with one BLV child therapist.\nOur findings highlight a preference for ArtInsight's longer,\nartistically-tailored descriptions over those generated by existing BLV AI\ntools. Participants highlighted the creative description and audio recording\ncomponents as most beneficial, with the former helping ``bring a picture to\nlife'' and the latter centering the child's narrative to generate context-aware\nAI responses. Our findings reveal different ways that AI can be used to support\nart engagement, including before, during, and after interaction with the child\nartist, as well as expectations that BLV adults and their sighted children have\nabout AI-powered tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce ArtInsight, a novel AI-powered system to facilitate deeper\nengagement with child-created artwork in mixed visual-ability families.\nArtInsight leverages large language models (LLMs) to craft a respectful and\nthorough initial description of a child's artwork, and provides: creative\nAI-generated descriptions for a vivid overview, audio recording to capture the\nchild's own description of their artwork, and a set of AI-generated questions\nto facilitate discussion between blind or low-vision (BLV) family members and\ntheir children. Alongside ArtInsight, we also contribute a new rubric to score\nAI-generated descriptions of child-created artwork and an assessment of\nstate-of-the-art LLMs. We evaluated ArtInsight with five groups of BLV family\nmembers and their children, and as a case study with one BLV child therapist.\nOur findings highlight a preference for ArtInsight's longer,\nartistically-tailored descriptions over those generated by existing BLV AI\ntools. Participants highlighted the creative description and audio recording\ncomponents as most beneficial, with the former helping ``bring a picture to\nlife'' and the latter centering the child's narrative to generate context-aware\nAI responses. Our findings reveal different ways that AI can be used to support\nart engagement, including before, during, and after interaction with the child\nartist, as well as expectations that BLV adults and their sighted children have\nabout AI-powered tools."
                },
                "authors": [
                    {
                        "name": "Arnavi Chheda-Kothary"
                    },
                    {
                        "name": "Ritesh Kanchi"
                    },
                    {
                        "name": "Chris Sanders"
                    },
                    {
                        "name": "Kevin Xiao"
                    },
                    {
                        "name": "Aditya Sengupta"
                    },
                    {
                        "name": "Melanie Kneitmix"
                    },
                    {
                        "name": "Jacob O. Wobbrock"
                    },
                    {
                        "name": "Jon E. Froehlich"
                    }
                ],
                "author_detail": {
                    "name": "Jon E. Froehlich"
                },
                "author": "Jon E. Froehlich",
                "arxiv_doi": "10.1145/3708359.3712082",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3708359.3712082",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.19263v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19263v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "21 pages, 30th International Conference on Intelligent User\n  Interfaces (IUI 2025)",
                "arxiv_journal_ref": "30th International Conference on Intelligent User Interfaces 2025",
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19243v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19243v1",
                "updated": "2025-02-26T15:51:38Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    15,
                    51,
                    38,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-26T15:51:38Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    15,
                    51,
                    38,
                    2,
                    57,
                    0
                ],
                "title": "Modelling Regional Solar Photovoltaic Capacity in Great Britain",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modelling Regional Solar Photovoltaic Capacity in Great Britain"
                },
                "summary": "Great Britain aims to meet growing electricity demand and achieve a fully\ndecarbonised grid by 2035, targeting 70 GW of solar photovoltaic (PV) capacity.\nHowever, grid constraints and connection delays hinder solar integration. To\naddress these integration challenges, various connection reform processes and\npolicies are being developed [1]. This study supports the connection reforms\nwith a model that estimates regional PV capacity at the NUTS 3 level,\nexplaining 89% of the variation in capacity, with a mean absolute error of 20\nMW and a national mean absolute percentage error of 5.4%. Artificial surfaces\nand agricultural areas are identified as key factors in deployment. The model\nhas three primary applications: disaggregating national PV capacity into\nregional capacity, benchmarking regional PV deployment between different\nregions, and forecasting future PV capacity distribution. These applications\nsupport grid operators in generation monitoring and strategic grid planning by\nidentifying regions where capacity is likely to be concentrated. This can\naddress grid connection delays, plan network expansions, and resolve land-use\nconflicts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Great Britain aims to meet growing electricity demand and achieve a fully\ndecarbonised grid by 2035, targeting 70 GW of solar photovoltaic (PV) capacity.\nHowever, grid constraints and connection delays hinder solar integration. To\naddress these integration challenges, various connection reform processes and\npolicies are being developed [1]. This study supports the connection reforms\nwith a model that estimates regional PV capacity at the NUTS 3 level,\nexplaining 89% of the variation in capacity, with a mean absolute error of 20\nMW and a national mean absolute percentage error of 5.4%. Artificial surfaces\nand agricultural areas are identified as key factors in deployment. The model\nhas three primary applications: disaggregating national PV capacity into\nregional capacity, benchmarking regional PV deployment between different\nregions, and forecasting future PV capacity distribution. These applications\nsupport grid operators in generation monitoring and strategic grid planning by\nidentifying regions where capacity is likely to be concentrated. This can\naddress grid connection delays, plan network expansions, and resolve land-use\nconflicts."
                },
                "authors": [
                    {
                        "name": "Hussah Alghanem"
                    },
                    {
                        "name": "Alastair Buckley"
                    }
                ],
                "author_detail": {
                    "name": "Alastair Buckley"
                },
                "author": "Alastair Buckley",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19243v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19243v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19230v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19230v1",
                "updated": "2025-02-26T15:41:41Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    15,
                    41,
                    41,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-26T15:41:41Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    15,
                    41,
                    41,
                    2,
                    57,
                    0
                ],
                "title": "Two Heads Are Better Than One: Dual-Model Verbal Reflection at\n  Inference-Time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Two Heads Are Better Than One: Dual-Model Verbal Reflection at\n  Inference-Time"
                },
                "summary": "Large Language Models (LLMs) often struggle with complex reasoning scenarios.\nWhile preference optimization methods enhance reasoning performance through\ntraining, they often lack transparency in why one reasoning outcome is\npreferred over another. Verbal reflection techniques improve explainability but\nare limited in LLMs' critique and refinement capacity. To address these\nchallenges, we introduce a contrastive reflection synthesis pipeline that\nenhances the accuracy and depth of LLM-generated reflections. We further\npropose a dual-model reasoning framework within a verbal reinforcement learning\nparadigm, decoupling inference-time self-reflection into specialized, trained\nmodels for reasoning critique and refinement. Extensive experiments show that\nour framework outperforms traditional preference optimization methods across\nall evaluation metrics. Our findings also show that \"two heads are better than\none\", demonstrating that a collaborative Reasoner-Critic model achieves\nsuperior reasoning performance and transparency, compared to single-model\napproaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) often struggle with complex reasoning scenarios.\nWhile preference optimization methods enhance reasoning performance through\ntraining, they often lack transparency in why one reasoning outcome is\npreferred over another. Verbal reflection techniques improve explainability but\nare limited in LLMs' critique and refinement capacity. To address these\nchallenges, we introduce a contrastive reflection synthesis pipeline that\nenhances the accuracy and depth of LLM-generated reflections. We further\npropose a dual-model reasoning framework within a verbal reinforcement learning\nparadigm, decoupling inference-time self-reflection into specialized, trained\nmodels for reasoning critique and refinement. Extensive experiments show that\nour framework outperforms traditional preference optimization methods across\nall evaluation metrics. Our findings also show that \"two heads are better than\none\", demonstrating that a collaborative Reasoner-Critic model achieves\nsuperior reasoning performance and transparency, compared to single-model\napproaches."
                },
                "authors": [
                    {
                        "name": "Jiazheng Li"
                    },
                    {
                        "name": "Yuxiang Zhou"
                    },
                    {
                        "name": "Junru Lu"
                    },
                    {
                        "name": "Gladys Tyen"
                    },
                    {
                        "name": "Lin Gui"
                    },
                    {
                        "name": "Cesare Aloisi"
                    },
                    {
                        "name": "Yulan He"
                    }
                ],
                "author_detail": {
                    "name": "Yulan He"
                },
                "author": "Yulan He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19230v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19230v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19217v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19217v1",
                "updated": "2025-02-26T15:19:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    15,
                    19,
                    52,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-26T15:19:52Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    15,
                    19,
                    52,
                    2,
                    57,
                    0
                ],
                "title": "A Lightweight and Extensible Cell Segmentation and Classification Model\n  for Whole Slide Images",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Lightweight and Extensible Cell Segmentation and Classification Model\n  for Whole Slide Images"
                },
                "summary": "Developing clinically useful cell-level analysis tools in digital pathology\nremains challenging due to limitations in dataset granularity, inconsistent\nannotations, high computational demands, and difficulties integrating new\ntechnologies into workflows. To address these issues, we propose a solution\nthat enhances data quality, model performance, and usability by creating a\nlightweight, extensible cell segmentation and classification model. First, we\nupdate data labels through cross-relabeling to refine annotations of PanNuke\nand MoNuSAC, producing a unified dataset with seven distinct cell types.\nSecond, we leverage the H-Optimus foundation model as a fixed encoder to\nimprove feature representation for simultaneous segmentation and classification\ntasks. Third, to address foundation models' computational demands, we distill\nknowledge to reduce model size and complexity while maintaining comparable\nperformance. Finally, we integrate the distilled model into QuPath, a widely\nused open-source digital pathology platform. Results demonstrate improved\nsegmentation and classification performance using the H-Optimus-based model\ncompared to a CNN-based model. Specifically, average $R^2$ improved from 0.575\nto 0.871, and average $PQ$ score improved from 0.450 to 0.492, indicating\nbetter alignment with actual cell counts and enhanced segmentation quality. The\ndistilled model maintains comparable performance while reducing parameter count\nby a factor of 48. By reducing computational complexity and integrating into\nworkflows, this approach may significantly impact diagnostics, reduce\npathologist workload, and improve outcomes. Although the method shows promise,\nextensive validation is necessary prior to clinical deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Developing clinically useful cell-level analysis tools in digital pathology\nremains challenging due to limitations in dataset granularity, inconsistent\nannotations, high computational demands, and difficulties integrating new\ntechnologies into workflows. To address these issues, we propose a solution\nthat enhances data quality, model performance, and usability by creating a\nlightweight, extensible cell segmentation and classification model. First, we\nupdate data labels through cross-relabeling to refine annotations of PanNuke\nand MoNuSAC, producing a unified dataset with seven distinct cell types.\nSecond, we leverage the H-Optimus foundation model as a fixed encoder to\nimprove feature representation for simultaneous segmentation and classification\ntasks. Third, to address foundation models' computational demands, we distill\nknowledge to reduce model size and complexity while maintaining comparable\nperformance. Finally, we integrate the distilled model into QuPath, a widely\nused open-source digital pathology platform. Results demonstrate improved\nsegmentation and classification performance using the H-Optimus-based model\ncompared to a CNN-based model. Specifically, average $R^2$ improved from 0.575\nto 0.871, and average $PQ$ score improved from 0.450 to 0.492, indicating\nbetter alignment with actual cell counts and enhanced segmentation quality. The\ndistilled model maintains comparable performance while reducing parameter count\nby a factor of 48. By reducing computational complexity and integrating into\nworkflows, this approach may significantly impact diagnostics, reduce\npathologist workload, and improve outcomes. Although the method shows promise,\nextensive validation is necessary prior to clinical deployment."
                },
                "authors": [
                    {
                        "name": "Nikita Shvetsov"
                    },
                    {
                        "name": "Thomas K. Kilvaer"
                    },
                    {
                        "name": "Masoud Tafavvoghi"
                    },
                    {
                        "name": "Anders Sildnes"
                    },
                    {
                        "name": "Kajsa Møllersen"
                    },
                    {
                        "name": "Lill-Tove Rasmussen Busund"
                    },
                    {
                        "name": "Lars Ailo Bongo"
                    }
                ],
                "author_detail": {
                    "name": "Lars Ailo Bongo"
                },
                "author": "Lars Ailo Bongo",
                "arxiv_comment": "27 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19217v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19217v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.4.6; I.4.9; I.2.10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.07978v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.07978v3",
                "updated": "2025-02-26T15:16:47Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    15,
                    16,
                    47,
                    2,
                    57,
                    0
                ],
                "published": "2023-11-14T08:10:14Z",
                "published_parsed": [
                    2023,
                    11,
                    14,
                    8,
                    10,
                    14,
                    1,
                    318,
                    0
                ],
                "title": "AfroBench: How Good are Large Language Models on African Languages?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AfroBench: How Good are Large Language Models on African Languages?"
                },
                "summary": "Large-scale multilingual evaluations, such as MEGA, often include only a\nhandful of African languages due to the scarcity of high-quality evaluation\ndata and the limited discoverability of existing African datasets. This lack of\nrepresentation hinders comprehensive LLM evaluation across a diverse range of\nlanguages and tasks. To address these challenges, we introduce AfroBench -- a\nmulti-task benchmark for evaluating the performance of LLMs across 64 African\nlanguages, 15 tasks and 22 datasets. AfroBench consists of nine natural\nlanguage understanding datasets, six text generation datasets, six knowledge\nand question answering tasks, and one mathematical reasoning task. We present\nresults comparing the performance of prompting LLMs to fine-tuned baselines\nbased on BERT and T5-style models. Our results suggest large gaps in\nperformance between high-resource languages, such as English, and African\nlanguages across most tasks; but performance also varies based on the\navailability of monolingual data resources. Our findings confirm that\nperformance on African languages continues to remain a hurdle for current LLMs,\nunderscoring the need for additional efforts to close this gap.\n  https://mcgill-nlp.github.io/AfroBench/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-scale multilingual evaluations, such as MEGA, often include only a\nhandful of African languages due to the scarcity of high-quality evaluation\ndata and the limited discoverability of existing African datasets. This lack of\nrepresentation hinders comprehensive LLM evaluation across a diverse range of\nlanguages and tasks. To address these challenges, we introduce AfroBench -- a\nmulti-task benchmark for evaluating the performance of LLMs across 64 African\nlanguages, 15 tasks and 22 datasets. AfroBench consists of nine natural\nlanguage understanding datasets, six text generation datasets, six knowledge\nand question answering tasks, and one mathematical reasoning task. We present\nresults comparing the performance of prompting LLMs to fine-tuned baselines\nbased on BERT and T5-style models. Our results suggest large gaps in\nperformance between high-resource languages, such as English, and African\nlanguages across most tasks; but performance also varies based on the\navailability of monolingual data resources. Our findings confirm that\nperformance on African languages continues to remain a hurdle for current LLMs,\nunderscoring the need for additional efforts to close this gap.\n  https://mcgill-nlp.github.io/AfroBench/"
                },
                "authors": [
                    {
                        "name": "Jessica Ojo"
                    },
                    {
                        "name": "Odunayo Ogundepo"
                    },
                    {
                        "name": "Akintunde Oladipo"
                    },
                    {
                        "name": "Kelechi Ogueji"
                    },
                    {
                        "name": "Jimmy Lin"
                    },
                    {
                        "name": "Pontus Stenetorp"
                    },
                    {
                        "name": "David Ifeoluwa Adelani"
                    }
                ],
                "author_detail": {
                    "name": "David Ifeoluwa Adelani"
                },
                "author": "David Ifeoluwa Adelani",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.07978v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.07978v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19214v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19214v1",
                "updated": "2025-02-26T15:15:01Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    15,
                    15,
                    1,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-26T15:15:01Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    15,
                    15,
                    1,
                    2,
                    57,
                    0
                ],
                "title": "A Hybrid Transformer Architecture with a Quantized Self-Attention\n  Mechanism Applied to Molecular Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Hybrid Transformer Architecture with a Quantized Self-Attention\n  Mechanism Applied to Molecular Generation"
                },
                "summary": "The success of the self-attention mechanism in classical machine learning\nmodels has inspired the development of quantum analogs aimed at reducing\ncomputational overhead. Self-attention integrates learnable query and key\nmatrices to calculate attention scores between all pairs of tokens in a\nsequence. These scores are then multiplied by a learnable value matrix to\nobtain the output self-attention matrix, enabling the model to effectively\ncapture long-range dependencies within the input sequence. Here, we propose a\nhybrid quantum-classical self-attention mechanism as part of a transformer\ndecoder, the architecture underlying large language models (LLMs). To\ndemonstrate its utility in chemistry, we train this model on the QM9 dataset\nfor conditional generation, using SMILES strings as input, each labeled with a\nset of physicochemical properties that serve as conditions during inference.\nOur theoretical analysis shows that the time complexity of the query-key dot\nproduct is reduced from $\\mathcal{O}(n^2 d)$ in a classical model to\n$\\mathcal{O}(n^2\\log d)$ in our quantum model, where $n$ and $d$ represent the\nsequence length and embedding dimension, respectively. We perform simulations\nusing NVIDIA's CUDA-Q platform, which is designed for efficient GPU\nscalability. This work provides a promising avenue for quantum-enhanced natural\nlanguage processing (NLP).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The success of the self-attention mechanism in classical machine learning\nmodels has inspired the development of quantum analogs aimed at reducing\ncomputational overhead. Self-attention integrates learnable query and key\nmatrices to calculate attention scores between all pairs of tokens in a\nsequence. These scores are then multiplied by a learnable value matrix to\nobtain the output self-attention matrix, enabling the model to effectively\ncapture long-range dependencies within the input sequence. Here, we propose a\nhybrid quantum-classical self-attention mechanism as part of a transformer\ndecoder, the architecture underlying large language models (LLMs). To\ndemonstrate its utility in chemistry, we train this model on the QM9 dataset\nfor conditional generation, using SMILES strings as input, each labeled with a\nset of physicochemical properties that serve as conditions during inference.\nOur theoretical analysis shows that the time complexity of the query-key dot\nproduct is reduced from $\\mathcal{O}(n^2 d)$ in a classical model to\n$\\mathcal{O}(n^2\\log d)$ in our quantum model, where $n$ and $d$ represent the\nsequence length and embedding dimension, respectively. We perform simulations\nusing NVIDIA's CUDA-Q platform, which is designed for efficient GPU\nscalability. This work provides a promising avenue for quantum-enhanced natural\nlanguage processing (NLP)."
                },
                "authors": [
                    {
                        "name": "Anthony M. Smaldone"
                    },
                    {
                        "name": "Yu Shee"
                    },
                    {
                        "name": "Gregory W. Kyro"
                    },
                    {
                        "name": "Marwa H. Farag"
                    },
                    {
                        "name": "Zohim Chandani"
                    },
                    {
                        "name": "Elica Kyoseva"
                    },
                    {
                        "name": "Victor S. Batista"
                    }
                ],
                "author_detail": {
                    "name": "Victor S. Batista"
                },
                "author": "Victor S. Batista",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19214v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19214v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19211v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19211v1",
                "updated": "2025-02-26T15:13:20Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    15,
                    13,
                    20,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-26T15:13:20Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    15,
                    13,
                    20,
                    2,
                    57,
                    0
                ],
                "title": "Negation-Induced Forgetting in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Negation-Induced Forgetting in LLMs"
                },
                "summary": "The study explores whether Large Language Models (LLMs) exhibit\nnegation-induced forgetting (NIF), a cognitive phenomenon observed in humans\nwhere negating incorrect attributes of an object or event leads to diminished\nrecall of this object or event compared to affirming correct attributes (Mayo\net al., 2014; Zang et al., 2023). We adapted Zang et al. (2023) experimental\nframework to test this effect in ChatGPT-3.5, GPT-4o mini and\nLlama3-70b-instruct. Our results show that ChatGPT-3.5 exhibits NIF, with\nnegated information being less likely to be recalled than affirmed information.\nGPT-4o-mini showed a marginally significant NIF effect, while LLaMA-3-70B did\nnot exhibit NIF. The findings provide initial evidence of negation-induced\nforgetting in some LLMs, suggesting that similar cognitive biases may emerge in\nthese models. This work is a preliminary step in understanding how\nmemory-related phenomena manifest in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The study explores whether Large Language Models (LLMs) exhibit\nnegation-induced forgetting (NIF), a cognitive phenomenon observed in humans\nwhere negating incorrect attributes of an object or event leads to diminished\nrecall of this object or event compared to affirming correct attributes (Mayo\net al., 2014; Zang et al., 2023). We adapted Zang et al. (2023) experimental\nframework to test this effect in ChatGPT-3.5, GPT-4o mini and\nLlama3-70b-instruct. Our results show that ChatGPT-3.5 exhibits NIF, with\nnegated information being less likely to be recalled than affirmed information.\nGPT-4o-mini showed a marginally significant NIF effect, while LLaMA-3-70B did\nnot exhibit NIF. The findings provide initial evidence of negation-induced\nforgetting in some LLMs, suggesting that similar cognitive biases may emerge in\nthese models. This work is a preliminary step in understanding how\nmemory-related phenomena manifest in LLMs."
                },
                "authors": [
                    {
                        "name": "Francesca Capuano"
                    },
                    {
                        "name": "Ellen Boschert"
                    },
                    {
                        "name": "Barbara Kaup"
                    }
                ],
                "author_detail": {
                    "name": "Barbara Kaup"
                },
                "author": "Barbara Kaup",
                "arxiv_comment": "ISCA/ITG Workshop on Diversity in Large Speech and Language Models",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19211v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19211v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19209v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19209v1",
                "updated": "2025-02-26T15:12:59Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    15,
                    12,
                    59,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-26T15:12:59Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    15,
                    12,
                    59,
                    2,
                    57,
                    0
                ],
                "title": "Bi'an: A Bilingual Benchmark and Model for Hallucination Detection in\n  Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bi'an: A Bilingual Benchmark and Model for Hallucination Detection in\n  Retrieval-Augmented Generation"
                },
                "summary": "Retrieval-Augmented Generation (RAG) effectively reduces hallucinations in\nLarge Language Models (LLMs) but can still produce inconsistent or unsupported\ncontent. Although LLM-as-a-Judge is widely used for RAG hallucination detection\ndue to its implementation simplicity, it faces two main challenges: the absence\nof comprehensive evaluation benchmarks and the lack of domain-optimized judge\nmodels. To bridge these gaps, we introduce \\textbf{Bi'an}, a novel framework\nfeaturing a bilingual benchmark dataset and lightweight judge models. The\ndataset supports rigorous evaluation across multiple RAG scenarios, while the\njudge models are fine-tuned from compact open-source LLMs. Extensive\nexperimental evaluations on Bi'anBench show our 14B model outperforms baseline\nmodels with over five times larger parameter scales and rivals state-of-the-art\nclosed-source LLMs. We will release our data and models soon at\nhttps://github.com/OpenSPG/KAG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) effectively reduces hallucinations in\nLarge Language Models (LLMs) but can still produce inconsistent or unsupported\ncontent. Although LLM-as-a-Judge is widely used for RAG hallucination detection\ndue to its implementation simplicity, it faces two main challenges: the absence\nof comprehensive evaluation benchmarks and the lack of domain-optimized judge\nmodels. To bridge these gaps, we introduce \\textbf{Bi'an}, a novel framework\nfeaturing a bilingual benchmark dataset and lightweight judge models. The\ndataset supports rigorous evaluation across multiple RAG scenarios, while the\njudge models are fine-tuned from compact open-source LLMs. Extensive\nexperimental evaluations on Bi'anBench show our 14B model outperforms baseline\nmodels with over five times larger parameter scales and rivals state-of-the-art\nclosed-source LLMs. We will release our data and models soon at\nhttps://github.com/OpenSPG/KAG."
                },
                "authors": [
                    {
                        "name": "Zhouyu Jiang"
                    },
                    {
                        "name": "Mengshu Sun"
                    },
                    {
                        "name": "Zhiqiang Zhang"
                    },
                    {
                        "name": "Lei Liang"
                    }
                ],
                "author_detail": {
                    "name": "Lei Liang"
                },
                "author": "Lei Liang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19209v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19209v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19193v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19193v1",
                "updated": "2025-02-26T14:59:27Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    14,
                    59,
                    27,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-26T14:59:27Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    14,
                    59,
                    27,
                    2,
                    57,
                    0
                ],
                "title": "Simulation of Language Evolution under Regulated Social Media Platforms:\n  A Synergistic Approach of Large Language Models and Genetic Algorithms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulation of Language Evolution under Regulated Social Media Platforms:\n  A Synergistic Approach of Large Language Models and Genetic Algorithms"
                },
                "summary": "Social media platforms frequently impose restrictive policies to moderate\nuser content, prompting the emergence of creative evasion language strategies.\nThis paper presents a multi-agent framework based on Large Language Models\n(LLMs) to simulate the iterative evolution of language strategies under\nregulatory constraints. In this framework, participant agents, as social media\nusers, continuously evolve their language expression, while supervisory agents\nemulate platform-level regulation by assessing policy violations. To achieve a\nmore faithful simulation, we employ a dual design of language strategies\n(constraint and expression) to differentiate conflicting goals and utilize an\nLLM-driven GA (Genetic Algorithm) for the selection, mutation, and crossover of\nlanguage strategies. The framework is evaluated using two distinct scenarios:\nan abstract password game and a realistic simulated illegal pet trade scenario.\nExperimental results demonstrate that as the number of dialogue rounds\nincreases, both the number of uninterrupted dialogue turns and the accuracy of\ninformation transmission improve significantly. Furthermore, a user study with\n40 participants validates the real-world relevance of the generated dialogues\nand strategies. Moreover, ablation studies validate the importance of the GA,\nemphasizing its contribution to long-term adaptability and improved overall\nresults.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Social media platforms frequently impose restrictive policies to moderate\nuser content, prompting the emergence of creative evasion language strategies.\nThis paper presents a multi-agent framework based on Large Language Models\n(LLMs) to simulate the iterative evolution of language strategies under\nregulatory constraints. In this framework, participant agents, as social media\nusers, continuously evolve their language expression, while supervisory agents\nemulate platform-level regulation by assessing policy violations. To achieve a\nmore faithful simulation, we employ a dual design of language strategies\n(constraint and expression) to differentiate conflicting goals and utilize an\nLLM-driven GA (Genetic Algorithm) for the selection, mutation, and crossover of\nlanguage strategies. The framework is evaluated using two distinct scenarios:\nan abstract password game and a realistic simulated illegal pet trade scenario.\nExperimental results demonstrate that as the number of dialogue rounds\nincreases, both the number of uninterrupted dialogue turns and the accuracy of\ninformation transmission improve significantly. Furthermore, a user study with\n40 participants validates the real-world relevance of the generated dialogues\nand strategies. Moreover, ablation studies validate the importance of the GA,\nemphasizing its contribution to long-term adaptability and improved overall\nresults."
                },
                "authors": [
                    {
                        "name": "Jinyu Cai"
                    },
                    {
                        "name": "Yusei Ishimizu"
                    },
                    {
                        "name": "Mingyue Zhang"
                    },
                    {
                        "name": "Munan Li"
                    },
                    {
                        "name": "Jialong Li"
                    },
                    {
                        "name": "Kenji Tei"
                    }
                ],
                "author_detail": {
                    "name": "Kenji Tei"
                },
                "author": "Kenji Tei",
                "arxiv_comment": "The manuscript has been submitted to IEEE Transactions on\n  Computational Social Systems",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19193v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19193v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19187v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19187v1",
                "updated": "2025-02-26T14:50:50Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    14,
                    50,
                    50,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-26T14:50:50Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    14,
                    50,
                    50,
                    2,
                    57,
                    0
                ],
                "title": "BIG-Bench Extra Hard",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BIG-Bench Extra Hard"
                },
                "summary": "Large language models (LLMs) are increasingly deployed in everyday\napplications, demanding robust general reasoning capabilities and diverse\nreasoning skillset. However, current LLM reasoning benchmarks predominantly\nfocus on mathematical and coding abilities, leaving a gap in evaluating broader\nreasoning proficiencies. One particular exception is the BIG-Bench dataset,\nwhich has served as a crucial benchmark for evaluating the general reasoning\ncapabilities of LLMs, thanks to its diverse set of challenging tasks that\nallowed for a comprehensive assessment of general reasoning across various\nskills within a unified framework. However, recent advances in LLMs have led to\nsaturation on BIG-Bench, and its harder version BIG-Bench Hard (BBH).\nState-of-the-art models achieve near-perfect scores on many tasks in BBH, thus\ndiminishing its utility. To address this limitation, we introduce BIG-Bench\nExtra Hard (BBEH), a new benchmark designed to push the boundaries of LLM\nreasoning evaluation. BBEH replaces each task in BBH with a novel task that\nprobes a similar reasoning capability but exhibits significantly increased\ndifficulty. We evaluate various models on BBEH and observe a (harmonic) average\naccuracy of 9.8\\% for the best general-purpose model and 44.8\\% for the best\nreasoning-specialized model, indicating substantial room for improvement and\nhighlighting the ongoing challenge of achieving robust general reasoning in\nLLMs. We release BBEH publicly at: https://github.com/google-deepmind/bbeh.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly deployed in everyday\napplications, demanding robust general reasoning capabilities and diverse\nreasoning skillset. However, current LLM reasoning benchmarks predominantly\nfocus on mathematical and coding abilities, leaving a gap in evaluating broader\nreasoning proficiencies. One particular exception is the BIG-Bench dataset,\nwhich has served as a crucial benchmark for evaluating the general reasoning\ncapabilities of LLMs, thanks to its diverse set of challenging tasks that\nallowed for a comprehensive assessment of general reasoning across various\nskills within a unified framework. However, recent advances in LLMs have led to\nsaturation on BIG-Bench, and its harder version BIG-Bench Hard (BBH).\nState-of-the-art models achieve near-perfect scores on many tasks in BBH, thus\ndiminishing its utility. To address this limitation, we introduce BIG-Bench\nExtra Hard (BBEH), a new benchmark designed to push the boundaries of LLM\nreasoning evaluation. BBEH replaces each task in BBH with a novel task that\nprobes a similar reasoning capability but exhibits significantly increased\ndifficulty. We evaluate various models on BBEH and observe a (harmonic) average\naccuracy of 9.8\\% for the best general-purpose model and 44.8\\% for the best\nreasoning-specialized model, indicating substantial room for improvement and\nhighlighting the ongoing challenge of achieving robust general reasoning in\nLLMs. We release BBEH publicly at: https://github.com/google-deepmind/bbeh."
                },
                "authors": [
                    {
                        "name": "Mehran Kazemi"
                    },
                    {
                        "name": "Bahare Fatemi"
                    },
                    {
                        "name": "Hritik Bansal"
                    },
                    {
                        "name": "John Palowitch"
                    },
                    {
                        "name": "Chrysovalantis Anastasiou"
                    },
                    {
                        "name": "Sanket Vaibhav Mehta"
                    },
                    {
                        "name": "Lalit K. Jain"
                    },
                    {
                        "name": "Virginia Aglietti"
                    },
                    {
                        "name": "Disha Jindal"
                    },
                    {
                        "name": "Peter Chen"
                    },
                    {
                        "name": "Nishanth Dikkala"
                    },
                    {
                        "name": "Gladys Tyen"
                    },
                    {
                        "name": "Xin Liu"
                    },
                    {
                        "name": "Uri Shalit"
                    },
                    {
                        "name": "Silvia Chiappa"
                    },
                    {
                        "name": "Kate Olszewska"
                    },
                    {
                        "name": "Yi Tay"
                    },
                    {
                        "name": "Vinh Q. Tran"
                    },
                    {
                        "name": "Quoc V. Le"
                    },
                    {
                        "name": "Orhan Firat"
                    }
                ],
                "author_detail": {
                    "name": "Orhan Firat"
                },
                "author": "Orhan Firat",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19187v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19187v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19178v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19178v1",
                "updated": "2025-02-26T14:34:00Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    14,
                    34,
                    0,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-26T14:34:00Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    14,
                    34,
                    0,
                    2,
                    57,
                    0
                ],
                "title": "UQABench: Evaluating User Embedding for Prompting LLMs in Personalized\n  Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UQABench: Evaluating User Embedding for Prompting LLMs in Personalized\n  Question Answering"
                },
                "summary": "Large language models (LLMs) achieve remarkable success in natural language\nprocessing (NLP). In practical scenarios like recommendations, as users\nincreasingly seek personalized experiences, it becomes crucial to incorporate\nuser interaction history into the context of LLMs to enhance personalization.\nHowever, from a practical utility perspective, user interactions' extensive\nlength and noise present challenges when used directly as text prompts. A\npromising solution is to compress and distill interactions into compact\nembeddings, serving as soft prompts to assist LLMs in generating personalized\nresponses. Although this approach brings efficiency, a critical concern\nemerges: Can user embeddings adequately capture valuable information and prompt\nLLMs? To address this concern, we propose \\name, a benchmark designed to\nevaluate the effectiveness of user embeddings in prompting LLMs for\npersonalization. We establish a fair and standardized evaluation process,\nencompassing pre-training, fine-tuning, and evaluation stages. To thoroughly\nevaluate user embeddings, we design three dimensions of tasks: sequence\nunderstanding, action prediction, and interest perception. These evaluation\ntasks cover the industry's demands in traditional recommendation tasks, such as\nimproving prediction accuracy, and its aspirations for LLM-based methods, such\nas accurately understanding user interests and enhancing the user experience.\nWe conduct extensive experiments on various state-of-the-art methods for\nmodeling user embeddings. Additionally, we reveal the scaling laws of\nleveraging user embeddings to prompt LLMs. The benchmark is available online.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) achieve remarkable success in natural language\nprocessing (NLP). In practical scenarios like recommendations, as users\nincreasingly seek personalized experiences, it becomes crucial to incorporate\nuser interaction history into the context of LLMs to enhance personalization.\nHowever, from a practical utility perspective, user interactions' extensive\nlength and noise present challenges when used directly as text prompts. A\npromising solution is to compress and distill interactions into compact\nembeddings, serving as soft prompts to assist LLMs in generating personalized\nresponses. Although this approach brings efficiency, a critical concern\nemerges: Can user embeddings adequately capture valuable information and prompt\nLLMs? To address this concern, we propose \\name, a benchmark designed to\nevaluate the effectiveness of user embeddings in prompting LLMs for\npersonalization. We establish a fair and standardized evaluation process,\nencompassing pre-training, fine-tuning, and evaluation stages. To thoroughly\nevaluate user embeddings, we design three dimensions of tasks: sequence\nunderstanding, action prediction, and interest perception. These evaluation\ntasks cover the industry's demands in traditional recommendation tasks, such as\nimproving prediction accuracy, and its aspirations for LLM-based methods, such\nas accurately understanding user interests and enhancing the user experience.\nWe conduct extensive experiments on various state-of-the-art methods for\nmodeling user embeddings. Additionally, we reveal the scaling laws of\nleveraging user embeddings to prompt LLMs. The benchmark is available online."
                },
                "authors": [
                    {
                        "name": "Langming Liu"
                    },
                    {
                        "name": "Shilei Liu"
                    },
                    {
                        "name": "Yujin Yuan"
                    },
                    {
                        "name": "Yizhen Zhang"
                    },
                    {
                        "name": "Bencheng Yan"
                    },
                    {
                        "name": "Zhiyuan Zeng"
                    },
                    {
                        "name": "Zihao Wang"
                    },
                    {
                        "name": "Jiaqi Liu"
                    },
                    {
                        "name": "Di Wang"
                    },
                    {
                        "name": "Wenbo Su"
                    },
                    {
                        "name": "Pengjie Wang"
                    },
                    {
                        "name": "Jian Xu"
                    },
                    {
                        "name": "Bo Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Bo Zheng"
                },
                "author": "Bo Zheng",
                "arxiv_comment": "10 pages, 3 figures, 7 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19178v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19178v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19175v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19175v1",
                "updated": "2025-02-26T14:31:43Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    14,
                    31,
                    43,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-26T14:31:43Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    14,
                    31,
                    43,
                    2,
                    57,
                    0
                ],
                "title": "MEDDxAgent: A Unified Modular Agent Framework for Explainable Automatic\n  Differential Diagnosis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MEDDxAgent: A Unified Modular Agent Framework for Explainable Automatic\n  Differential Diagnosis"
                },
                "summary": "Differential Diagnosis (DDx) is a fundamental yet complex aspect of clinical\ndecision-making, in which physicians iteratively refine a ranked list of\npossible diseases based on symptoms, antecedents, and medical knowledge. While\nrecent advances in large language models have shown promise in supporting DDx,\nexisting approaches face key limitations, including single-dataset evaluations,\nisolated optimization of components, unrealistic assumptions about complete\npatient profiles, and single-attempt diagnosis. We introduce a Modular\nExplainable DDx Agent (MEDDxAgent) framework designed for interactive DDx,\nwhere diagnostic reasoning evolves through iterative learning, rather than\nassuming a complete patient profile is accessible. MEDDxAgent integrates three\nmodular components: (1) an orchestrator (DDxDriver), (2) a history taking\nsimulator, and (3) two specialized agents for knowledge retrieval and diagnosis\nstrategy. To ensure robust evaluation, we introduce a comprehensive DDx\nbenchmark covering respiratory, skin, and rare diseases. We analyze single-turn\ndiagnostic approaches and demonstrate the importance of iterative refinement\nwhen patient profiles are not available at the outset. Our broad evaluation\ndemonstrates that MEDDxAgent achieves over 10% accuracy improvements in\ninteractive DDx across both large and small LLMs, while offering critical\nexplainability into its diagnostic reasoning process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Differential Diagnosis (DDx) is a fundamental yet complex aspect of clinical\ndecision-making, in which physicians iteratively refine a ranked list of\npossible diseases based on symptoms, antecedents, and medical knowledge. While\nrecent advances in large language models have shown promise in supporting DDx,\nexisting approaches face key limitations, including single-dataset evaluations,\nisolated optimization of components, unrealistic assumptions about complete\npatient profiles, and single-attempt diagnosis. We introduce a Modular\nExplainable DDx Agent (MEDDxAgent) framework designed for interactive DDx,\nwhere diagnostic reasoning evolves through iterative learning, rather than\nassuming a complete patient profile is accessible. MEDDxAgent integrates three\nmodular components: (1) an orchestrator (DDxDriver), (2) a history taking\nsimulator, and (3) two specialized agents for knowledge retrieval and diagnosis\nstrategy. To ensure robust evaluation, we introduce a comprehensive DDx\nbenchmark covering respiratory, skin, and rare diseases. We analyze single-turn\ndiagnostic approaches and demonstrate the importance of iterative refinement\nwhen patient profiles are not available at the outset. Our broad evaluation\ndemonstrates that MEDDxAgent achieves over 10% accuracy improvements in\ninteractive DDx across both large and small LLMs, while offering critical\nexplainability into its diagnostic reasoning process."
                },
                "authors": [
                    {
                        "name": "Daniel Rose"
                    },
                    {
                        "name": "Chia-Chien Hung"
                    },
                    {
                        "name": "Marco Lepri"
                    },
                    {
                        "name": "Israa Alqassem"
                    },
                    {
                        "name": "Kiril Gashteovski"
                    },
                    {
                        "name": "Carolin Lawrence"
                    }
                ],
                "author_detail": {
                    "name": "Carolin Lawrence"
                },
                "author": "Carolin Lawrence",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19175v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19175v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.07064v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.07064v2",
                "updated": "2025-02-26T14:28:11Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    14,
                    28,
                    11,
                    2,
                    57,
                    0
                ],
                "published": "2024-07-09T17:38:03Z",
                "published_parsed": [
                    2024,
                    7,
                    9,
                    17,
                    38,
                    3,
                    1,
                    191,
                    0
                ],
                "title": "Prompting Techniques for Secure Code Generation: A Systematic\n  Investigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompting Techniques for Secure Code Generation: A Systematic\n  Investigation"
                },
                "summary": "Large Language Models (LLMs) are gaining momentum in software development\nwith prompt-driven programming enabling developers to create code from natural\nlanguage (NL) instructions. However, studies have questioned their ability to\nproduce secure code and, thereby, the quality of prompt-generated software.\nAlongside, various prompting techniques that carefully tailor prompts have\nemerged to elicit optimal responses from LLMs. Still, the interplay between\nsuch prompting strategies and secure code generation remains under-explored and\ncalls for further investigations. OBJECTIVE: In this study, we investigate the\nimpact of different prompting techniques on the security of code generated from\nNL instructions by LLMs. METHOD: First we perform a systematic literature\nreview to identify the existing prompting techniques that can be used for code\ngeneration tasks. A subset of these techniques are evaluated on GPT-3, GPT-3.5,\nand GPT-4 models for secure code generation. For this, we used an existing\ndataset consisting of 150 NL security-relevant code-generation prompts.\nRESULTS: Our work (i) classifies potential prompting techniques for code\ngeneration (ii) adapts and evaluates a subset of the identified techniques for\nsecure code generation tasks and (iii) observes a reduction in security\nweaknesses across the tested LLMs, especially after using an existing technique\ncalled Recursive Criticism and Improvement (RCI), contributing valuable\ninsights to the ongoing discourse on LLM-generated code security.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are gaining momentum in software development\nwith prompt-driven programming enabling developers to create code from natural\nlanguage (NL) instructions. However, studies have questioned their ability to\nproduce secure code and, thereby, the quality of prompt-generated software.\nAlongside, various prompting techniques that carefully tailor prompts have\nemerged to elicit optimal responses from LLMs. Still, the interplay between\nsuch prompting strategies and secure code generation remains under-explored and\ncalls for further investigations. OBJECTIVE: In this study, we investigate the\nimpact of different prompting techniques on the security of code generated from\nNL instructions by LLMs. METHOD: First we perform a systematic literature\nreview to identify the existing prompting techniques that can be used for code\ngeneration tasks. A subset of these techniques are evaluated on GPT-3, GPT-3.5,\nand GPT-4 models for secure code generation. For this, we used an existing\ndataset consisting of 150 NL security-relevant code-generation prompts.\nRESULTS: Our work (i) classifies potential prompting techniques for code\ngeneration (ii) adapts and evaluates a subset of the identified techniques for\nsecure code generation tasks and (iii) observes a reduction in security\nweaknesses across the tested LLMs, especially after using an existing technique\ncalled Recursive Criticism and Improvement (RCI), contributing valuable\ninsights to the ongoing discourse on LLM-generated code security."
                },
                "authors": [
                    {
                        "name": "Catherine Tony"
                    },
                    {
                        "name": "Nicolás E. Díaz Ferreyra"
                    },
                    {
                        "name": "Markus Mutas"
                    },
                    {
                        "name": "Salem Dhiff"
                    },
                    {
                        "name": "Riccardo Scandariato"
                    }
                ],
                "author_detail": {
                    "name": "Riccardo Scandariato"
                },
                "author": "Riccardo Scandariato",
                "arxiv_comment": "Work partially supported by the EU-funded project Sec4AI4Sec:\n  Cybersecurity for AI-Augmented Systems (grant no. 101120393) - ACCEPTED at\n  ACM Transactions on Software Engineering and Methodology (Feb. 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.07064v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.07064v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18168v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18168v2",
                "updated": "2025-02-26T14:27:33Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    14,
                    27,
                    33,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-25T13:00:05Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    13,
                    0,
                    5,
                    1,
                    56,
                    0
                ],
                "title": "SECURA: Sigmoid-Enhanced CUR Decomposition with Uninterrupted Retention\n  and Low-Rank Adaptation in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SECURA: Sigmoid-Enhanced CUR Decomposition with Uninterrupted Retention\n  and Low-Rank Adaptation in Large Language Models"
                },
                "summary": "With the rapid development of large language models (LLMs), fully fine-tuning\n(FT) these models has become increasingly impractical due to the high\ncomputational demands. Additionally, FT can lead to catastrophic forgetting. As\nan alternative, Low-Rank Adaptation (LoRA) has been proposed, which fine-tunes\nonly a small subset of parameters, achieving similar performance to FT while\nsignificantly reducing resource requirements. However, since LoRA inherits FT's\ndesign, the issue of catastrophic forgetting remains.\n  To address these challenges, we propose SECURA: Sigmoid-Enhanced CUR\nDecomposition LoRA, a novel parameter-efficient fine-tuning (PEFT) variant that\nmitigates catastrophic forgetting while improving fine-tuning performance. Our\nmethod introduces a new normalization technique, SigNorm, to enhance parameter\nretention and overall performance.\n  SECURA has been evaluated on a variety of tasks, including mathematical\nproblem-solving (GSM8K), challenging question-answering (CNNDM), translation\n(NewsDE), and complex multiple-choice reasoning (LogiQA). Experimental results\nshow that SECURA achieves an average fine-tuning improvement of 3.59% across\nfour multiple-choice question (MCQ) tasks and a 2.51% improvement across five\nquestion-answering (QA) tasks on models such as Gemma2 2b, Qwen2 1.5b, Qwen 2\n7b, Llama3 8b, and Llama3.1 8b, compared to DoRA. Moreover, SECURA demonstrates\nsuperior knowledge retention capabilities, maintaining more than 70% accuracy\non basic LLM knowledge across 16 continual learning tests, outperforming\nExperience Replay (ER), Sequential Learning (SEQ), EWC, I-LoRA, and CUR-LoRA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid development of large language models (LLMs), fully fine-tuning\n(FT) these models has become increasingly impractical due to the high\ncomputational demands. Additionally, FT can lead to catastrophic forgetting. As\nan alternative, Low-Rank Adaptation (LoRA) has been proposed, which fine-tunes\nonly a small subset of parameters, achieving similar performance to FT while\nsignificantly reducing resource requirements. However, since LoRA inherits FT's\ndesign, the issue of catastrophic forgetting remains.\n  To address these challenges, we propose SECURA: Sigmoid-Enhanced CUR\nDecomposition LoRA, a novel parameter-efficient fine-tuning (PEFT) variant that\nmitigates catastrophic forgetting while improving fine-tuning performance. Our\nmethod introduces a new normalization technique, SigNorm, to enhance parameter\nretention and overall performance.\n  SECURA has been evaluated on a variety of tasks, including mathematical\nproblem-solving (GSM8K), challenging question-answering (CNNDM), translation\n(NewsDE), and complex multiple-choice reasoning (LogiQA). Experimental results\nshow that SECURA achieves an average fine-tuning improvement of 3.59% across\nfour multiple-choice question (MCQ) tasks and a 2.51% improvement across five\nquestion-answering (QA) tasks on models such as Gemma2 2b, Qwen2 1.5b, Qwen 2\n7b, Llama3 8b, and Llama3.1 8b, compared to DoRA. Moreover, SECURA demonstrates\nsuperior knowledge retention capabilities, maintaining more than 70% accuracy\non basic LLM knowledge across 16 continual learning tests, outperforming\nExperience Replay (ER), Sequential Learning (SEQ), EWC, I-LoRA, and CUR-LoRA."
                },
                "authors": [
                    {
                        "name": "Yuxuan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yuxuan Zhang"
                },
                "author": "Yuxuan Zhang",
                "arxiv_comment": "New work on Parameter-Efficient Fine-Tuning (PEFT) for large language\n  models. Includes new techniques SigNorm and CABR-LoRA for optimizing\n  fine-tune performance and Knowledge retention",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18168v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18168v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.6; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19171v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19171v1",
                "updated": "2025-02-26T14:27:04Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    14,
                    27,
                    4,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-26T14:27:04Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    14,
                    27,
                    4,
                    2,
                    57,
                    0
                ],
                "title": "PlantPal: Leveraging Precision Agriculture Robots to Facilitate Remote\n  Engagement in Urban Gardening",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PlantPal: Leveraging Precision Agriculture Robots to Facilitate Remote\n  Engagement in Urban Gardening"
                },
                "summary": "Urban gardening is widely recognized for its numerous health and\nenvironmental benefits. However, the lack of suitable garden spaces, demanding\ndaily schedules and limited gardening expertise present major roadblocks for\ncitizens looking to engage in urban gardening. While prior research has\nexplored smart home solutions to support urban gardeners, these approaches\ncurrently do not fully address these practical barriers. In this paper, we\npresent PlantPal, a system that enables the cultivation of garden spaces\nirrespective of one's location, expertise level, or time constraints. PlantPal\nenables the shared operation of a precision agriculture robot (PAR) that is\nequipped with garden tools and a multi-camera system. Insights from a 3-week\ndeployment (N=18) indicate that PlantPal facilitated the integration of\ngardening tasks into daily routines, fostered a sense of connection with one's\nfield, and provided an engaging experience despite the remote setting. We\ncontribute design considerations for future robot-assisted urban gardening\nconcepts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Urban gardening is widely recognized for its numerous health and\nenvironmental benefits. However, the lack of suitable garden spaces, demanding\ndaily schedules and limited gardening expertise present major roadblocks for\ncitizens looking to engage in urban gardening. While prior research has\nexplored smart home solutions to support urban gardeners, these approaches\ncurrently do not fully address these practical barriers. In this paper, we\npresent PlantPal, a system that enables the cultivation of garden spaces\nirrespective of one's location, expertise level, or time constraints. PlantPal\nenables the shared operation of a precision agriculture robot (PAR) that is\nequipped with garden tools and a multi-camera system. Insights from a 3-week\ndeployment (N=18) indicate that PlantPal facilitated the integration of\ngardening tasks into daily routines, fostered a sense of connection with one's\nfield, and provided an engaging experience despite the remote setting. We\ncontribute design considerations for future robot-assisted urban gardening\nconcepts."
                },
                "authors": [
                    {
                        "name": "Albin Zeqiri"
                    },
                    {
                        "name": "Julian Britten"
                    },
                    {
                        "name": "Clara Schramm"
                    },
                    {
                        "name": "Pascal Jansen"
                    },
                    {
                        "name": "Michael Rietzler"
                    },
                    {
                        "name": "Enrico Rukzio"
                    }
                ],
                "author_detail": {
                    "name": "Enrico Rukzio"
                },
                "author": "Enrico Rukzio",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19171v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19171v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10990v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10990v2",
                "updated": "2025-02-26T14:26:14Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    14,
                    26,
                    14,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-16T04:23:52Z",
                "published_parsed": [
                    2025,
                    2,
                    16,
                    4,
                    23,
                    52,
                    6,
                    47,
                    0
                ],
                "title": "FinMTEB: Finance Massive Text Embedding Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FinMTEB: Finance Massive Text Embedding Benchmark"
                },
                "summary": "Embedding models play a crucial role in representing and retrieving\ninformation across various NLP applications. Recent advances in large language\nmodels (LLMs) have further enhanced the performance of embedding models. While\nthese models are often benchmarked on general-purpose datasets, real-world\napplications demand domain-specific evaluation. In this work, we introduce the\nFinance Massive Text Embedding Benchmark (FinMTEB), a specialized counterpart\nto MTEB designed for the financial domain. FinMTEB comprises 64 financial\ndomain-specific embedding datasets across 7 tasks that cover diverse textual\ntypes in both Chinese and English, such as financial news articles, corporate\nannual reports, ESG reports, regulatory filings, and earnings call transcripts.\nWe also develop a finance-adapted model, Fin-E5, using a persona-based data\nsynthetic method to cover diverse financial embedding tasks for training.\nThrough extensive evaluation of 15 embedding models, including Fin-E5, we show\nthree key findings: (1) performance on general-purpose benchmarks shows limited\ncorrelation with financial domain tasks; (2) domain-adapted models consistently\noutperform their general-purpose counterparts; and (3) surprisingly, a simple\nBag-of-Words (BoW) approach outperforms sophisticated dense embeddings in\nfinancial Semantic Textual Similarity (STS) tasks, underscoring current\nlimitations in dense embedding techniques. Our work establishes a robust\nevaluation framework for financial NLP applications and provides crucial\ninsights for developing domain-specific embedding models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embedding models play a crucial role in representing and retrieving\ninformation across various NLP applications. Recent advances in large language\nmodels (LLMs) have further enhanced the performance of embedding models. While\nthese models are often benchmarked on general-purpose datasets, real-world\napplications demand domain-specific evaluation. In this work, we introduce the\nFinance Massive Text Embedding Benchmark (FinMTEB), a specialized counterpart\nto MTEB designed for the financial domain. FinMTEB comprises 64 financial\ndomain-specific embedding datasets across 7 tasks that cover diverse textual\ntypes in both Chinese and English, such as financial news articles, corporate\nannual reports, ESG reports, regulatory filings, and earnings call transcripts.\nWe also develop a finance-adapted model, Fin-E5, using a persona-based data\nsynthetic method to cover diverse financial embedding tasks for training.\nThrough extensive evaluation of 15 embedding models, including Fin-E5, we show\nthree key findings: (1) performance on general-purpose benchmarks shows limited\ncorrelation with financial domain tasks; (2) domain-adapted models consistently\noutperform their general-purpose counterparts; and (3) surprisingly, a simple\nBag-of-Words (BoW) approach outperforms sophisticated dense embeddings in\nfinancial Semantic Textual Similarity (STS) tasks, underscoring current\nlimitations in dense embedding techniques. Our work establishes a robust\nevaluation framework for financial NLP applications and provides crucial\ninsights for developing domain-specific embedding models."
                },
                "authors": [
                    {
                        "name": "Yixuan Tang"
                    },
                    {
                        "name": "Yi Yang"
                    }
                ],
                "author_detail": {
                    "name": "Yi Yang"
                },
                "author": "Yi Yang",
                "arxiv_comment": "https://github.com/yixuantt/FinMTEB",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10990v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10990v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19166v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19166v1",
                "updated": "2025-02-26T14:19:49Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    14,
                    19,
                    49,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-26T14:19:49Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    14,
                    19,
                    49,
                    2,
                    57,
                    0
                ],
                "title": "CodeIF: Benchmarking the Instruction-Following Capabilities of Large\n  Language Models for Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CodeIF: Benchmarking the Instruction-Following Capabilities of Large\n  Language Models for Code Generation"
                },
                "summary": "With the rapid advancement of Large Language Models (LLMs), the demand for\nrobust instruction-following capabilities in code generation tasks has grown\nsignificantly. Code generation not only facilitates faster prototyping and\nautomated testing, but also augments developer efficiency through improved\nmaintainability and reusability of code. In this paper, we introduce CodeIF,\nthe first benchmark specifically designed to assess the abilities of LLMs to\nadhere to task-oriented instructions within diverse code generation scenarios.\nCodeIF encompasses a broad range of tasks, including function synthesis, error\ndebugging, algorithmic refactoring, and code explanation, thereby providing a\ncomprehensive suite to evaluate model performance across varying complexity\nlevels and programming domains. We conduct extensive experiments with LLMs,\nanalyzing their strengths and limitations in meeting the demands of these\ntasks. The experimental results offer valuable insights into how well current\nmodels align with human instructions, as well as the extent to which they can\ngenerate consistent, maintainable, and contextually relevant code. Our findings\nnot only underscore the critical role that instruction-following LLMs can play\nin modern software development, but also illuminate pathways for future\nresearch aimed at enhancing their adaptability, reliability, and overall\neffectiveness in automated code generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid advancement of Large Language Models (LLMs), the demand for\nrobust instruction-following capabilities in code generation tasks has grown\nsignificantly. Code generation not only facilitates faster prototyping and\nautomated testing, but also augments developer efficiency through improved\nmaintainability and reusability of code. In this paper, we introduce CodeIF,\nthe first benchmark specifically designed to assess the abilities of LLMs to\nadhere to task-oriented instructions within diverse code generation scenarios.\nCodeIF encompasses a broad range of tasks, including function synthesis, error\ndebugging, algorithmic refactoring, and code explanation, thereby providing a\ncomprehensive suite to evaluate model performance across varying complexity\nlevels and programming domains. We conduct extensive experiments with LLMs,\nanalyzing their strengths and limitations in meeting the demands of these\ntasks. The experimental results offer valuable insights into how well current\nmodels align with human instructions, as well as the extent to which they can\ngenerate consistent, maintainable, and contextually relevant code. Our findings\nnot only underscore the critical role that instruction-following LLMs can play\nin modern software development, but also illuminate pathways for future\nresearch aimed at enhancing their adaptability, reliability, and overall\neffectiveness in automated code generation."
                },
                "authors": [
                    {
                        "name": "Kaiwen Yan"
                    },
                    {
                        "name": "Hongcheng Guo"
                    },
                    {
                        "name": "Xuanqing Shi"
                    },
                    {
                        "name": "Jingyi Xu"
                    },
                    {
                        "name": "Yaonan Gu"
                    },
                    {
                        "name": "Zhoujun Li"
                    }
                ],
                "author_detail": {
                    "name": "Zhoujun Li"
                },
                "author": "Zhoujun Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19166v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19166v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19160v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19160v1",
                "updated": "2025-02-26T14:15:28Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    14,
                    15,
                    28,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-26T14:15:28Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    14,
                    15,
                    28,
                    2,
                    57,
                    0
                ],
                "title": "Detecting Linguistic Indicators for Stereotype Assessment with Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting Linguistic Indicators for Stereotype Assessment with Large\n  Language Models"
                },
                "summary": "Social categories and stereotypes are embedded in language and can introduce\ndata bias into Large Language Models (LLMs). Despite safeguards, these biases\noften persist in model behavior, potentially leading to representational harm\nin outputs. While sociolinguistic research provides valuable insights into the\nformation of stereotypes, NLP approaches for stereotype detection rarely draw\non this foundation and often lack objectivity, precision, and interpretability.\nTo fill this gap, in this work we propose a new approach that detects and\nquantifies the linguistic indicators of stereotypes in a sentence. We derive\nlinguistic indicators from the Social Category and Stereotype Communication\n(SCSC) framework which indicate strong social category formulation and\nstereotyping in language, and use them to build a categorization scheme. To\nautomate this approach, we instruct different LLMs using in-context learning to\napply the approach to a sentence, where the LLM examines the linguistic\nproperties and provides a basis for a fine-grained assessment. Based on an\nempirical evaluation of the importance of different linguistic indicators, we\nlearn a scoring function that measures the linguistic indicators of a\nstereotype. Our annotations of stereotyped sentences show that these indicators\nare present in these sentences and explain the strength of a stereotype. In\nterms of model performance, our results show that the models generally perform\nwell in detecting and classifying linguistic indicators of category labels used\nto denote a category, but sometimes struggle to correctly evaluate the\nassociated behaviors and characteristics. Using more few-shot examples within\nthe prompts, significantly improves performance. Model performance increases\nwith size, as Llama-3.3-70B-Instruct and GPT-4 achieve comparable results that\nsurpass those of Mixtral-8x7B-Instruct, GPT-4-mini and Llama-3.1-8B-Instruct.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Social categories and stereotypes are embedded in language and can introduce\ndata bias into Large Language Models (LLMs). Despite safeguards, these biases\noften persist in model behavior, potentially leading to representational harm\nin outputs. While sociolinguistic research provides valuable insights into the\nformation of stereotypes, NLP approaches for stereotype detection rarely draw\non this foundation and often lack objectivity, precision, and interpretability.\nTo fill this gap, in this work we propose a new approach that detects and\nquantifies the linguistic indicators of stereotypes in a sentence. We derive\nlinguistic indicators from the Social Category and Stereotype Communication\n(SCSC) framework which indicate strong social category formulation and\nstereotyping in language, and use them to build a categorization scheme. To\nautomate this approach, we instruct different LLMs using in-context learning to\napply the approach to a sentence, where the LLM examines the linguistic\nproperties and provides a basis for a fine-grained assessment. Based on an\nempirical evaluation of the importance of different linguistic indicators, we\nlearn a scoring function that measures the linguistic indicators of a\nstereotype. Our annotations of stereotyped sentences show that these indicators\nare present in these sentences and explain the strength of a stereotype. In\nterms of model performance, our results show that the models generally perform\nwell in detecting and classifying linguistic indicators of category labels used\nto denote a category, but sometimes struggle to correctly evaluate the\nassociated behaviors and characteristics. Using more few-shot examples within\nthe prompts, significantly improves performance. Model performance increases\nwith size, as Llama-3.3-70B-Instruct and GPT-4 achieve comparable results that\nsurpass those of Mixtral-8x7B-Instruct, GPT-4-mini and Llama-3.1-8B-Instruct."
                },
                "authors": [
                    {
                        "name": "Rebekka Görge"
                    },
                    {
                        "name": "Michael Mock"
                    },
                    {
                        "name": "Héctor Allende-Cid"
                    }
                ],
                "author_detail": {
                    "name": "Héctor Allende-Cid"
                },
                "author": "Héctor Allende-Cid",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19160v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19160v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19159v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19159v1",
                "updated": "2025-02-26T14:15:24Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    14,
                    15,
                    24,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-26T14:15:24Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    14,
                    15,
                    24,
                    2,
                    57,
                    0
                ],
                "title": "A Sliding Layer Merging Method for Efficient Depth-Wise Pruning in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Sliding Layer Merging Method for Efficient Depth-Wise Pruning in LLMs"
                },
                "summary": "Compared to width-wise pruning, depth-wise pruning can significantly\naccelerate inference in resource-constrained scenarios. Howerver, treating the\nentire Transformer layer as the minimum pruning unit may degrade model\nperformance by indiscriminately discarding the entire information of the layer.\nThis paper reveals the \"Patch-like\" feature relationship between layers in\nlarge language models by analyzing the correlation of the outputs of different\nlayers in the reproducing kernel Hilbert space. Building on this observation,\nwe proposes a sliding layer merging method that dynamically selects and fuses\nconsecutive layers from top to bottom according to a pre-defined similarity\nthreshold, thereby simplifying the model structure while maintaining its\nperformance. Extensive experiments on LLMs with various architectures and\ndifferent parameter scales show that our method outperforms existing pruning\ntechniques in both zero-shot inference performance and retraining recovery\nquality after pruning. In particular, in the experiment with 35\\% pruning on\nthe Vicuna-7B model, our method achieved a 1.654\\% improvement in average\nperformance on zero-shot tasks compared to the existing method. Moreover, we\nfurther reveal the potential of combining depth pruning with width pruning to\nenhance the pruning effect. Our codes are available at\nhttps://github.com/920927/SLM-a-sliding-layer-merging-method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compared to width-wise pruning, depth-wise pruning can significantly\naccelerate inference in resource-constrained scenarios. Howerver, treating the\nentire Transformer layer as the minimum pruning unit may degrade model\nperformance by indiscriminately discarding the entire information of the layer.\nThis paper reveals the \"Patch-like\" feature relationship between layers in\nlarge language models by analyzing the correlation of the outputs of different\nlayers in the reproducing kernel Hilbert space. Building on this observation,\nwe proposes a sliding layer merging method that dynamically selects and fuses\nconsecutive layers from top to bottom according to a pre-defined similarity\nthreshold, thereby simplifying the model structure while maintaining its\nperformance. Extensive experiments on LLMs with various architectures and\ndifferent parameter scales show that our method outperforms existing pruning\ntechniques in both zero-shot inference performance and retraining recovery\nquality after pruning. In particular, in the experiment with 35\\% pruning on\nthe Vicuna-7B model, our method achieved a 1.654\\% improvement in average\nperformance on zero-shot tasks compared to the existing method. Moreover, we\nfurther reveal the potential of combining depth pruning with width pruning to\nenhance the pruning effect. Our codes are available at\nhttps://github.com/920927/SLM-a-sliding-layer-merging-method."
                },
                "authors": [
                    {
                        "name": "Xuan Ding"
                    },
                    {
                        "name": "Yao Zhu"
                    },
                    {
                        "name": "Yunjian Zhang"
                    },
                    {
                        "name": "Chuanlong Xie"
                    }
                ],
                "author_detail": {
                    "name": "Chuanlong Xie"
                },
                "author": "Chuanlong Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19159v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19159v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19158v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19158v1",
                "updated": "2025-02-26T14:14:58Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    14,
                    14,
                    58,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-26T14:14:58Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    14,
                    14,
                    58,
                    2,
                    57,
                    0
                ],
                "title": "When Personalization Meets Reality: A Multi-Faceted Analysis of\n  Personalized Preference Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Personalization Meets Reality: A Multi-Faceted Analysis of\n  Personalized Preference Learning"
                },
                "summary": "While Reinforcement Learning from Human Feedback (RLHF) is widely used to\nalign Large Language Models (LLMs) with human preferences, it typically assumes\nhomogeneous preferences across users, overlooking diverse human values and\nminority viewpoints. Although personalized preference learning addresses this\nby tailoring separate preferences for individual users, the field lacks\nstandardized methods to assess its effectiveness. We present a multi-faceted\nevaluation framework that measures not only performance but also fairness,\nunintended effects, and adaptability across varying levels of preference\ndivergence. Through extensive experiments comparing eight personalization\nmethods across three preference datasets, we demonstrate that performance\ndifferences between methods could reach 36% when users strongly disagree, and\npersonalization can introduce up to 20% safety misalignment. These findings\nhighlight the critical need for holistic evaluation approaches to advance the\ndevelopment of more effective and inclusive preference learning systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Reinforcement Learning from Human Feedback (RLHF) is widely used to\nalign Large Language Models (LLMs) with human preferences, it typically assumes\nhomogeneous preferences across users, overlooking diverse human values and\nminority viewpoints. Although personalized preference learning addresses this\nby tailoring separate preferences for individual users, the field lacks\nstandardized methods to assess its effectiveness. We present a multi-faceted\nevaluation framework that measures not only performance but also fairness,\nunintended effects, and adaptability across varying levels of preference\ndivergence. Through extensive experiments comparing eight personalization\nmethods across three preference datasets, we demonstrate that performance\ndifferences between methods could reach 36% when users strongly disagree, and\npersonalization can introduce up to 20% safety misalignment. These findings\nhighlight the critical need for holistic evaluation approaches to advance the\ndevelopment of more effective and inclusive preference learning systems."
                },
                "authors": [
                    {
                        "name": "Yijiang River Dong"
                    },
                    {
                        "name": "Tiancheng Hu"
                    },
                    {
                        "name": "Yinhong Liu"
                    },
                    {
                        "name": "Ahmet Üstün"
                    },
                    {
                        "name": "Nigel Collier"
                    }
                ],
                "author_detail": {
                    "name": "Nigel Collier"
                },
                "author": "Nigel Collier",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19158v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19158v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19149v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19149v1",
                "updated": "2025-02-26T14:08:17Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    14,
                    8,
                    17,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-26T14:08:17Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    14,
                    8,
                    17,
                    2,
                    57,
                    0
                ],
                "title": "Isolating Language-Coding from Problem-Solving: Benchmarking LLMs with\n  PseudoEval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Isolating Language-Coding from Problem-Solving: Benchmarking LLMs with\n  PseudoEval"
                },
                "summary": "Existing code generation benchmarks for Large Language Models (LLMs) such as\nHumanEval and MBPP are designed to study LLMs' end-to-end performance, where\nthe benchmarks feed a problem description in natural language as input and\nexamine the generated code in specific programming languages. However, the\nevaluation scores revealed in this way provide a little hint as to the\nbottleneck of the code generation -- whether LLMs are struggling with their\nproblem-solving capability or language-coding capability. To answer this\nquestion, we construct PseudoEval, a multilingual code generation benchmark\nthat provides a solution written in pseudocode as input. By doing so, the\nbottleneck of code generation in various programming languages could be\nisolated and identified. Our study yields several interesting findings. For\nexample, we identify that the bottleneck of LLMs in Python programming is\nproblem-solving, while Rust is struggling relatively more in language-coding.\nAlso, our study indicates that problem-solving capability may transfer across\nprogramming languages, while language-coding needs more language-specific\neffort, especially for undertrained programming languages. Finally, we release\nthe pipeline of constructing PseudoEval to facilitate the extension to existing\nbenchmarks. PseudoEval is available at:\nhttps://anonymous.4open.science/r/PseudocodeACL25-7B74.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing code generation benchmarks for Large Language Models (LLMs) such as\nHumanEval and MBPP are designed to study LLMs' end-to-end performance, where\nthe benchmarks feed a problem description in natural language as input and\nexamine the generated code in specific programming languages. However, the\nevaluation scores revealed in this way provide a little hint as to the\nbottleneck of the code generation -- whether LLMs are struggling with their\nproblem-solving capability or language-coding capability. To answer this\nquestion, we construct PseudoEval, a multilingual code generation benchmark\nthat provides a solution written in pseudocode as input. By doing so, the\nbottleneck of code generation in various programming languages could be\nisolated and identified. Our study yields several interesting findings. For\nexample, we identify that the bottleneck of LLMs in Python programming is\nproblem-solving, while Rust is struggling relatively more in language-coding.\nAlso, our study indicates that problem-solving capability may transfer across\nprogramming languages, while language-coding needs more language-specific\neffort, especially for undertrained programming languages. Finally, we release\nthe pipeline of constructing PseudoEval to facilitate the extension to existing\nbenchmarks. PseudoEval is available at:\nhttps://anonymous.4open.science/r/PseudocodeACL25-7B74."
                },
                "authors": [
                    {
                        "name": "Jiarong Wu"
                    },
                    {
                        "name": "Songqiang Chen"
                    },
                    {
                        "name": "Jialun Cao"
                    },
                    {
                        "name": "Hau Ching Lo"
                    },
                    {
                        "name": "Shing-Chi Cheung"
                    }
                ],
                "author_detail": {
                    "name": "Shing-Chi Cheung"
                },
                "author": "Shing-Chi Cheung",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19149v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19149v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19148v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19148v1",
                "updated": "2025-02-26T14:07:37Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    14,
                    7,
                    37,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-26T14:07:37Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    14,
                    7,
                    37,
                    2,
                    57,
                    0
                ],
                "title": "Amulet: ReAlignment During Test Time for Personalized Preference\n  Adaptation of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Amulet: ReAlignment During Test Time for Personalized Preference\n  Adaptation of LLMs"
                },
                "summary": "How to align large language models (LLMs) with user preferences from a static\ngeneral dataset has been frequently studied. However, user preferences are\nusually personalized, changing, and diverse regarding culture, values, or time.\nThis leads to the problem that the actual user preferences often do not\ncoincide with those trained by the model developers in the practical use of\nLLMs. Since we cannot collect enough data and retrain for every demand,\nresearching efficient real-time preference adaptation methods based on the\nbackbone LLMs during test time is important. To this end, we introduce Amulet,\na novel, training-free framework that formulates the decoding process of every\ntoken as a separate online learning problem with the guidance of simple\nuser-provided prompts, thus enabling real-time optimization to satisfy users'\npersonalized preferences. To reduce the computational cost brought by this\noptimization process for each token, we additionally provide a closed-form\nsolution for each iteration step of the optimization process, thereby reducing\nthe computational time cost to a negligible level. The detailed experimental\nresults demonstrate that Amulet can achieve significant performance\nimprovements in rich settings with combinations of different LLMs, datasets,\nand user preferences, while maintaining acceptable computational efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How to align large language models (LLMs) with user preferences from a static\ngeneral dataset has been frequently studied. However, user preferences are\nusually personalized, changing, and diverse regarding culture, values, or time.\nThis leads to the problem that the actual user preferences often do not\ncoincide with those trained by the model developers in the practical use of\nLLMs. Since we cannot collect enough data and retrain for every demand,\nresearching efficient real-time preference adaptation methods based on the\nbackbone LLMs during test time is important. To this end, we introduce Amulet,\na novel, training-free framework that formulates the decoding process of every\ntoken as a separate online learning problem with the guidance of simple\nuser-provided prompts, thus enabling real-time optimization to satisfy users'\npersonalized preferences. To reduce the computational cost brought by this\noptimization process for each token, we additionally provide a closed-form\nsolution for each iteration step of the optimization process, thereby reducing\nthe computational time cost to a negligible level. The detailed experimental\nresults demonstrate that Amulet can achieve significant performance\nimprovements in rich settings with combinations of different LLMs, datasets,\nand user preferences, while maintaining acceptable computational efficiency."
                },
                "authors": [
                    {
                        "name": "Zhaowei Zhang"
                    },
                    {
                        "name": "Fengshuo Bai"
                    },
                    {
                        "name": "Qizhi Chen"
                    },
                    {
                        "name": "Chengdong Ma"
                    },
                    {
                        "name": "Mingzhi Wang"
                    },
                    {
                        "name": "Haoran Sun"
                    },
                    {
                        "name": "Zilong Zheng"
                    },
                    {
                        "name": "Yaodong Yang"
                    }
                ],
                "author_detail": {
                    "name": "Yaodong Yang"
                },
                "author": "Yaodong Yang",
                "arxiv_comment": "Accepted by ICLR 2025, Project page:\n  https://zowiezhang.github.io/projects/Amulet",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19148v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19148v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.00034v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.00034v2",
                "updated": "2025-02-26T14:07:05Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    14,
                    7,
                    5,
                    2,
                    57,
                    0
                ],
                "published": "2024-05-26T21:39:53Z",
                "published_parsed": [
                    2024,
                    5,
                    26,
                    21,
                    39,
                    53,
                    6,
                    147,
                    0
                ],
                "title": "Adaptive Activation Steering: A Tuning-Free LLM Truthfulness Improvement\n  Method for Diverse Hallucinations Categories",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Activation Steering: A Tuning-Free LLM Truthfulness Improvement\n  Method for Diverse Hallucinations Categories"
                },
                "summary": "Recent studies have indicated that Large Language Models (LLMs) harbor an\ninherent understanding of truthfulness, yet often fail to consistently express\nit and generate false statements. This gap between \"knowing\" and \"telling\"\nposes a challenge for ensuring the truthfulness of generated content. Inspired\nby recent work on the practice of encoding human-interpretable concepts\nlinearly within large language models, we treat truthfulness as a specially\nlinearly encoded concept within LLMs, and introduce Adaptive Activation\nSteering (ACT), a tuning-free method that adaptively shifts LLM's activations\nin the \"truthful\" direction during inference. ACT addresses diverse categories\nof hallucinations by utilizing diverse truthfulness-related steering vectors\nand adjusting the steering intensity adaptively. Applied as an add-on across\nvarious models, ACT significantly improves truthfulness in LLaMA ($\\uparrow$\n142%), LLaMA2 ($\\uparrow$ 24%), Alpaca ($\\uparrow$ 36%), Vicuna ($\\uparrow$\n28%), LLaMA2-Chat ($\\uparrow$ 19%), and LLaMA3($\\uparrow$ 34%). Furthermore, we\nverify ACT's scalability across larger models (13B, 33B, 65B), underscoring the\nadaptability of ACT to large-scale language models. Our code is available at\nhttps://github.com/tianlwang/ACT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent studies have indicated that Large Language Models (LLMs) harbor an\ninherent understanding of truthfulness, yet often fail to consistently express\nit and generate false statements. This gap between \"knowing\" and \"telling\"\nposes a challenge for ensuring the truthfulness of generated content. Inspired\nby recent work on the practice of encoding human-interpretable concepts\nlinearly within large language models, we treat truthfulness as a specially\nlinearly encoded concept within LLMs, and introduce Adaptive Activation\nSteering (ACT), a tuning-free method that adaptively shifts LLM's activations\nin the \"truthful\" direction during inference. ACT addresses diverse categories\nof hallucinations by utilizing diverse truthfulness-related steering vectors\nand adjusting the steering intensity adaptively. Applied as an add-on across\nvarious models, ACT significantly improves truthfulness in LLaMA ($\\uparrow$\n142%), LLaMA2 ($\\uparrow$ 24%), Alpaca ($\\uparrow$ 36%), Vicuna ($\\uparrow$\n28%), LLaMA2-Chat ($\\uparrow$ 19%), and LLaMA3($\\uparrow$ 34%). Furthermore, we\nverify ACT's scalability across larger models (13B, 33B, 65B), underscoring the\nadaptability of ACT to large-scale language models. Our code is available at\nhttps://github.com/tianlwang/ACT."
                },
                "authors": [
                    {
                        "name": "Tianlong Wang"
                    },
                    {
                        "name": "Xianfeng Jiao"
                    },
                    {
                        "name": "Yinghao Zhu"
                    },
                    {
                        "name": "Zhongzhi Chen"
                    },
                    {
                        "name": "Yifan He"
                    },
                    {
                        "name": "Xu Chu"
                    },
                    {
                        "name": "Junyi Gao"
                    },
                    {
                        "name": "Yasha Wang"
                    },
                    {
                        "name": "Liantao Ma"
                    }
                ],
                "author_detail": {
                    "name": "Liantao Ma"
                },
                "author": "Liantao Ma",
                "arxiv_doi": "10.1145/3696410.3714640",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3696410.3714640",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2406.00034v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.00034v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "ACM TheWebConf 2025 Conference (WWW 2025) Research Track",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.01881v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.01881v3",
                "updated": "2025-02-26T13:57:13Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    13,
                    57,
                    13,
                    2,
                    57,
                    0
                ],
                "published": "2024-02-02T20:12:05Z",
                "published_parsed": [
                    2024,
                    2,
                    2,
                    20,
                    12,
                    5,
                    4,
                    33,
                    0
                ],
                "title": "Large Language Model Agent for Hyper-Parameter Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model Agent for Hyper-Parameter Optimization"
                },
                "summary": "Hyperparameter optimization is critical in modern machine learning, requiring\nexpert knowledge, numerous trials, and high computational and human resources.\nDespite the advancements in Automated Machine Learning (AutoML), challenges in\nterms of trial efficiency, setup complexity, and interoperability still\npersist. To address these issues, we introduce a novel paradigm leveraging\nLarge Language Models (LLMs) to automate hyperparameter optimization across\ndiverse machine learning tasks, which is named AgentHPO (short for LLM\nAgent-based Hyperparameter Optimization). Specifically, AgentHPO processes the\ntask information autonomously, conducts experiments with specific\nhyperparameters (HPs), and iteratively optimizes them based on historical\ntrials. This human-like optimization process largely reduces the number of\nrequired trials, simplifies the setup process, and enhances interpretability\nand user trust, compared to traditional AutoML methods. Extensive empirical\nexperiments conducted on 12 representative machine-learning tasks indicate that\nAgentHPO not only matches but also often surpasses the best human trials in\nterms of performance while simultaneously providing explainable results.\nFurther analysis sheds light on the strategies employed by the LLM in\noptimizing these tasks, highlighting its effectiveness and adaptability in\nvarious scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hyperparameter optimization is critical in modern machine learning, requiring\nexpert knowledge, numerous trials, and high computational and human resources.\nDespite the advancements in Automated Machine Learning (AutoML), challenges in\nterms of trial efficiency, setup complexity, and interoperability still\npersist. To address these issues, we introduce a novel paradigm leveraging\nLarge Language Models (LLMs) to automate hyperparameter optimization across\ndiverse machine learning tasks, which is named AgentHPO (short for LLM\nAgent-based Hyperparameter Optimization). Specifically, AgentHPO processes the\ntask information autonomously, conducts experiments with specific\nhyperparameters (HPs), and iteratively optimizes them based on historical\ntrials. This human-like optimization process largely reduces the number of\nrequired trials, simplifies the setup process, and enhances interpretability\nand user trust, compared to traditional AutoML methods. Extensive empirical\nexperiments conducted on 12 representative machine-learning tasks indicate that\nAgentHPO not only matches but also often surpasses the best human trials in\nterms of performance while simultaneously providing explainable results.\nFurther analysis sheds light on the strategies employed by the LLM in\noptimizing these tasks, highlighting its effectiveness and adaptability in\nvarious scenarios."
                },
                "authors": [
                    {
                        "name": "Siyi Liu"
                    },
                    {
                        "name": "Chen Gao"
                    },
                    {
                        "name": "Yong Li"
                    }
                ],
                "author_detail": {
                    "name": "Yong Li"
                },
                "author": "Yong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.01881v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.01881v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.05516v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.05516v2",
                "updated": "2025-02-26T13:56:16Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    13,
                    56,
                    16,
                    2,
                    57,
                    0
                ],
                "published": "2024-06-08T16:35:31Z",
                "published_parsed": [
                    2024,
                    6,
                    8,
                    16,
                    35,
                    31,
                    5,
                    160,
                    0
                ],
                "title": "Verbalized Probabilistic Graphical Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Verbalized Probabilistic Graphical Modeling"
                },
                "summary": "Human cognition excels at transcending sensory input and forming latent\nrepresentations that structure our understanding of the world. Although Large\nLanguage Models (LLMs) can produce chain-of-thought reasoning, they lack a\nprincipled framework to capture latent structures and model uncertainty,\nespecially in compositional reasoning tasks. We propose Verbalized\nProbabilistic Graphical Modeling (vPGM), a Bayesian prompting framework that\nguides LLMs to simulate key principles of Probabilistic Graphical Models (PGMs)\nin natural language. Unlike many traditional probabilistic methods requiring\nsubstantial domain expertise or specialized training, vPGM bypasses\nexpert-driven model design, making it well-suited for scenarios with limited\nassumptions or scarce data. We evaluated our model on several compositional\nreasoning tasks, both close-ended and open-ended. Our results indicate that the\nmodel effectively enhances confidence calibration and text generation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human cognition excels at transcending sensory input and forming latent\nrepresentations that structure our understanding of the world. Although Large\nLanguage Models (LLMs) can produce chain-of-thought reasoning, they lack a\nprincipled framework to capture latent structures and model uncertainty,\nespecially in compositional reasoning tasks. We propose Verbalized\nProbabilistic Graphical Modeling (vPGM), a Bayesian prompting framework that\nguides LLMs to simulate key principles of Probabilistic Graphical Models (PGMs)\nin natural language. Unlike many traditional probabilistic methods requiring\nsubstantial domain expertise or specialized training, vPGM bypasses\nexpert-driven model design, making it well-suited for scenarios with limited\nassumptions or scarce data. We evaluated our model on several compositional\nreasoning tasks, both close-ended and open-ended. Our results indicate that the\nmodel effectively enhances confidence calibration and text generation quality."
                },
                "authors": [
                    {
                        "name": "Hengguan Huang"
                    },
                    {
                        "name": "Xing Shen"
                    },
                    {
                        "name": "Songtao Wang"
                    },
                    {
                        "name": "Lingfa Meng"
                    },
                    {
                        "name": "Dianbo Liu"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Samir Bhatt"
                    }
                ],
                "author_detail": {
                    "name": "Samir Bhatt"
                },
                "author": "Samir Bhatt",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.05516v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.05516v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02551v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02551v2",
                "updated": "2025-02-26T13:51:56Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    13,
                    51,
                    56,
                    2,
                    57,
                    0
                ],
                "published": "2024-10-03T14:55:22Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    14,
                    55,
                    22,
                    3,
                    277,
                    0
                ],
                "title": "ColaCare: Enhancing Electronic Health Record Modeling through Large\n  Language Model-Driven Multi-Agent Collaboration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ColaCare: Enhancing Electronic Health Record Modeling through Large\n  Language Model-Driven Multi-Agent Collaboration"
                },
                "summary": "We introduce ColaCare, a framework that enhances Electronic Health Record\n(EHR) modeling through multi-agent collaboration driven by Large Language\nModels (LLMs). Our approach seamlessly integrates domain-specific expert models\nwith LLMs to bridge the gap between structured EHR data and text-based\nreasoning. Inspired by the Multidisciplinary Team (MDT) approach used in\nclinical settings, ColaCare employs two types of agents: DoctorAgents and a\nMetaAgent, which collaboratively analyze patient data. Expert models process\nand generate predictions from numerical EHR data, while LLM agents produce\nreasoning references and decision-making reports within the MDT-driven\ncollaborative consultation framework. The MetaAgent orchestrates the\ndiscussion, facilitating consultations and evidence-based debates among\nDoctorAgents, simulating diverse expertise in clinical decision-making. We\nadditionally incorporate the Merck Manual of Diagnosis and Therapy (MSD)\nmedical guideline within a retrieval-augmented generation (RAG) module for\nmedical evidence support, addressing the challenge of knowledge currency.\nExtensive experiments conducted on three EHR datasets demonstrate ColaCare's\nsuperior performance in clinical mortality outcome and readmission prediction\ntasks, underscoring its potential to revolutionize clinical decision support\nsystems and advance personalized precision medicine. All code, case studies and\na questionnaire are available at the project website:\nhttps://colacare.netlify.app.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce ColaCare, a framework that enhances Electronic Health Record\n(EHR) modeling through multi-agent collaboration driven by Large Language\nModels (LLMs). Our approach seamlessly integrates domain-specific expert models\nwith LLMs to bridge the gap between structured EHR data and text-based\nreasoning. Inspired by the Multidisciplinary Team (MDT) approach used in\nclinical settings, ColaCare employs two types of agents: DoctorAgents and a\nMetaAgent, which collaboratively analyze patient data. Expert models process\nand generate predictions from numerical EHR data, while LLM agents produce\nreasoning references and decision-making reports within the MDT-driven\ncollaborative consultation framework. The MetaAgent orchestrates the\ndiscussion, facilitating consultations and evidence-based debates among\nDoctorAgents, simulating diverse expertise in clinical decision-making. We\nadditionally incorporate the Merck Manual of Diagnosis and Therapy (MSD)\nmedical guideline within a retrieval-augmented generation (RAG) module for\nmedical evidence support, addressing the challenge of knowledge currency.\nExtensive experiments conducted on three EHR datasets demonstrate ColaCare's\nsuperior performance in clinical mortality outcome and readmission prediction\ntasks, underscoring its potential to revolutionize clinical decision support\nsystems and advance personalized precision medicine. All code, case studies and\na questionnaire are available at the project website:\nhttps://colacare.netlify.app."
                },
                "authors": [
                    {
                        "name": "Zixiang Wang"
                    },
                    {
                        "name": "Yinghao Zhu"
                    },
                    {
                        "name": "Huiya Zhao"
                    },
                    {
                        "name": "Xiaochen Zheng"
                    },
                    {
                        "name": "Dehao Sui"
                    },
                    {
                        "name": "Tianlong Wang"
                    },
                    {
                        "name": "Wen Tang"
                    },
                    {
                        "name": "Yasha Wang"
                    },
                    {
                        "name": "Ewen Harrison"
                    },
                    {
                        "name": "Chengwei Pan"
                    },
                    {
                        "name": "Junyi Gao"
                    },
                    {
                        "name": "Liantao Ma"
                    }
                ],
                "author_detail": {
                    "name": "Liantao Ma"
                },
                "author": "Liantao Ma",
                "arxiv_doi": "10.1145/3696410.3714877",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3696410.3714877",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2410.02551v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02551v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "ACM TheWebConf 2025 Conference (WWW 2025) Research Track",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19135v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19135v1",
                "updated": "2025-02-26T13:51:28Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    13,
                    51,
                    28,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-26T13:51:28Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    13,
                    51,
                    28,
                    2,
                    57,
                    0
                ],
                "title": "A Temporal Planning Framework for Multi-Agent Systems via LLM-Aided\n  Knowledge Base Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Temporal Planning Framework for Multi-Agent Systems via LLM-Aided\n  Knowledge Base Management"
                },
                "summary": "This paper presents a novel framework, called PLANTOR (PLanning with Natural\nlanguage for Task-Oriented Robots), that integrates Large Language Models\n(LLMs) with Prolog-based knowledge management and planning for multi-robot\ntasks. The system employs a two-phase generation of a robot-oriented knowledge\nbase, ensuring reusability and compositional reasoning, as well as a three-step\nplanning procedure that handles temporal dependencies, resource constraints,\nand parallel task execution via mixed-integer linear programming. The final\nplan is converted into a Behaviour Tree for direct use in ROS2. We tested the\nframework in multi-robot assembly tasks within a block world and an\narch-building scenario. Results demonstrate that LLMs can produce accurate\nknowledge bases with modest human feedback, while Prolog guarantees formal\ncorrectness and explainability. This approach underscores the potential of LLM\nintegration for advanced robotics tasks requiring flexible, scalable, and\nhuman-understandable planning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a novel framework, called PLANTOR (PLanning with Natural\nlanguage for Task-Oriented Robots), that integrates Large Language Models\n(LLMs) with Prolog-based knowledge management and planning for multi-robot\ntasks. The system employs a two-phase generation of a robot-oriented knowledge\nbase, ensuring reusability and compositional reasoning, as well as a three-step\nplanning procedure that handles temporal dependencies, resource constraints,\nand parallel task execution via mixed-integer linear programming. The final\nplan is converted into a Behaviour Tree for direct use in ROS2. We tested the\nframework in multi-robot assembly tasks within a block world and an\narch-building scenario. Results demonstrate that LLMs can produce accurate\nknowledge bases with modest human feedback, while Prolog guarantees formal\ncorrectness and explainability. This approach underscores the potential of LLM\nintegration for advanced robotics tasks requiring flexible, scalable, and\nhuman-understandable planning."
                },
                "authors": [
                    {
                        "name": "Enrico Saccon"
                    },
                    {
                        "name": "Ahmet Tikna"
                    },
                    {
                        "name": "Davide De Martini"
                    },
                    {
                        "name": "Edoardo Lamon"
                    },
                    {
                        "name": "Luigi Palopoli"
                    },
                    {
                        "name": "Marco Roveri"
                    }
                ],
                "author_detail": {
                    "name": "Marco Roveri"
                },
                "author": "Marco Roveri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19135v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19135v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19133v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19133v1",
                "updated": "2025-02-26T13:44:15Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    13,
                    44,
                    15,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-26T13:44:15Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    13,
                    44,
                    15,
                    2,
                    57,
                    0
                ],
                "title": "DBox: Scaffolding Algorithmic Programming Learning through Learner-LLM\n  Co-Decomposition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DBox: Scaffolding Algorithmic Programming Learning through Learner-LLM\n  Co-Decomposition"
                },
                "summary": "Decomposition is a fundamental skill in algorithmic programming, requiring\nlearners to break down complex problems into smaller, manageable parts.\nHowever, current self-study methods, such as browsing reference solutions or\nusing LLM assistants, often provide excessive or generic assistance that\nmisaligns with learners' decomposition strategies, hindering independent\nproblem-solving and critical thinking. To address this, we introduce\nDecomposition Box (DBox), an interactive LLM-based system that scaffolds and\nadapts to learners' personalized construction of a step tree through a\n\"learner-LLM co-decomposition\" approach, providing tailored support at an\nappropriate level. A within-subjects study (N=24) found that compared to the\nbaseline, DBox significantly improved learning gains, cognitive engagement, and\ncritical thinking. Learners also reported a stronger sense of achievement and\nfound the assistance appropriate and helpful for learning. Additionally, we\nexamined DBox's impact on cognitive load, identified usage patterns, and\nanalyzed learners' strategies for managing system errors. We conclude with\ndesign implications for future AI-powered tools to better support algorithmic\nprogramming education.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decomposition is a fundamental skill in algorithmic programming, requiring\nlearners to break down complex problems into smaller, manageable parts.\nHowever, current self-study methods, such as browsing reference solutions or\nusing LLM assistants, often provide excessive or generic assistance that\nmisaligns with learners' decomposition strategies, hindering independent\nproblem-solving and critical thinking. To address this, we introduce\nDecomposition Box (DBox), an interactive LLM-based system that scaffolds and\nadapts to learners' personalized construction of a step tree through a\n\"learner-LLM co-decomposition\" approach, providing tailored support at an\nappropriate level. A within-subjects study (N=24) found that compared to the\nbaseline, DBox significantly improved learning gains, cognitive engagement, and\ncritical thinking. Learners also reported a stronger sense of achievement and\nfound the assistance appropriate and helpful for learning. Additionally, we\nexamined DBox's impact on cognitive load, identified usage patterns, and\nanalyzed learners' strategies for managing system errors. We conclude with\ndesign implications for future AI-powered tools to better support algorithmic\nprogramming education."
                },
                "authors": [
                    {
                        "name": "Shuai Ma"
                    },
                    {
                        "name": "Junling Wang"
                    },
                    {
                        "name": "Yuanhao Zhang"
                    },
                    {
                        "name": "Xiaojuan Ma"
                    },
                    {
                        "name": "April Yi Wang"
                    }
                ],
                "author_detail": {
                    "name": "April Yi Wang"
                },
                "author": "April Yi Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19133v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19133v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.01833v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.01833v3",
                "updated": "2025-02-26T13:41:41Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    13,
                    41,
                    41,
                    2,
                    57,
                    0
                ],
                "published": "2024-04-02T10:45:49Z",
                "published_parsed": [
                    2024,
                    4,
                    2,
                    10,
                    45,
                    49,
                    1,
                    93,
                    0
                ],
                "title": "Great, Now Write an Article About That: The Crescendo Multi-Turn LLM\n  Jailbreak Attack",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Great, Now Write an Article About That: The Crescendo Multi-Turn LLM\n  Jailbreak Attack"
                },
                "summary": "Large Language Models (LLMs) have risen significantly in popularity and are\nincreasingly being adopted across multiple applications. These LLMs are heavily\naligned to resist engaging in illegal or unethical topics as a means to avoid\ncontributing to responsible AI harms. However, a recent line of attacks, known\nas jailbreaks, seek to overcome this alignment. Intuitively, jailbreak attacks\naim to narrow the gap between what the model can do and what it is willing to\ndo. In this paper, we introduce a novel jailbreak attack called Crescendo.\nUnlike existing jailbreak methods, Crescendo is a simple multi-turn jailbreak\nthat interacts with the model in a seemingly benign manner. It begins with a\ngeneral prompt or question about the task at hand and then gradually escalates\nthe dialogue by referencing the model's replies progressively leading to a\nsuccessful jailbreak. We evaluate Crescendo on various public systems,\nincluding ChatGPT, Gemini Pro, Gemini-Ultra, LlaMA-2 70b and LlaMA-3 70b Chat,\nand Anthropic Chat. Our results demonstrate the strong efficacy of Crescendo,\nwith it achieving high attack success rates across all evaluated models and\ntasks. Furthermore, we present Crescendomation, a tool that automates the\nCrescendo attack and demonstrate its efficacy against state-of-the-art models\nthrough our evaluations. Crescendomation surpasses other state-of-the-art\njailbreaking techniques on the AdvBench subset dataset, achieving 29-61% higher\nperformance on GPT-4 and 49-71% on Gemini-Pro. Finally, we also demonstrate\nCrescendo's ability to jailbreak multimodal models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have risen significantly in popularity and are\nincreasingly being adopted across multiple applications. These LLMs are heavily\naligned to resist engaging in illegal or unethical topics as a means to avoid\ncontributing to responsible AI harms. However, a recent line of attacks, known\nas jailbreaks, seek to overcome this alignment. Intuitively, jailbreak attacks\naim to narrow the gap between what the model can do and what it is willing to\ndo. In this paper, we introduce a novel jailbreak attack called Crescendo.\nUnlike existing jailbreak methods, Crescendo is a simple multi-turn jailbreak\nthat interacts with the model in a seemingly benign manner. It begins with a\ngeneral prompt or question about the task at hand and then gradually escalates\nthe dialogue by referencing the model's replies progressively leading to a\nsuccessful jailbreak. We evaluate Crescendo on various public systems,\nincluding ChatGPT, Gemini Pro, Gemini-Ultra, LlaMA-2 70b and LlaMA-3 70b Chat,\nand Anthropic Chat. Our results demonstrate the strong efficacy of Crescendo,\nwith it achieving high attack success rates across all evaluated models and\ntasks. Furthermore, we present Crescendomation, a tool that automates the\nCrescendo attack and demonstrate its efficacy against state-of-the-art models\nthrough our evaluations. Crescendomation surpasses other state-of-the-art\njailbreaking techniques on the AdvBench subset dataset, achieving 29-61% higher\nperformance on GPT-4 and 49-71% on Gemini-Pro. Finally, we also demonstrate\nCrescendo's ability to jailbreak multimodal models."
                },
                "authors": [
                    {
                        "name": "Mark Russinovich"
                    },
                    {
                        "name": "Ahmed Salem"
                    },
                    {
                        "name": "Ronen Eldan"
                    }
                ],
                "author_detail": {
                    "name": "Ronen Eldan"
                },
                "author": "Ronen Eldan",
                "arxiv_comment": "Accepted at USENIX Security 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.01833v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.01833v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.08588v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.08588v2",
                "updated": "2025-02-26T13:34:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    13,
                    34,
                    52,
                    2,
                    57,
                    0
                ],
                "published": "2024-04-12T16:34:20Z",
                "published_parsed": [
                    2024,
                    4,
                    12,
                    16,
                    34,
                    20,
                    4,
                    103,
                    0
                ],
                "title": "Efficient Sensors Selection for Traffic Flow Monitoring: An Overview of\n  Model-Based Techniques leveraging Network Observability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Sensors Selection for Traffic Flow Monitoring: An Overview of\n  Model-Based Techniques leveraging Network Observability"
                },
                "summary": "The emergence of 6G-enabled Internet of Vehicles (IoV) promises to\nrevolutionize mobility and connectivity, integrating vehicles into a mobile\nInternet of Things (IoT)-oriented wireless sensor network (WSN). Meanwhile, 5G\ntechnologies and mobile edge computing further support this vision by\nfacilitating real-time connectivity and empowering massive access to the\nInternet. Within this context, IoT-oriented WSNs play a crucial role in\nintelligent transportation systems, offering affordable alternatives for\ntraffic monitoring and management. Efficient sensor selection thus represents a\ncritical concern while deploying WSNs on urban networks. In this paper, we\nprovide an overview of such a notably hard problem. The contribution is\ntwofold: (i) surveying state-of-the-art model-based techniques for efficient\nsensor selection in traffic flow monitoring, emphasizing challenges of sensor\nplacement, and (ii) advocating for {the development of} data-driven\nmethodologies to enhance sensor deployment efficacy and traffic modeling\naccuracy. Further considerations underscore the importance of data-driven\napproaches for adaptive transportation systems aligned with the IoV paradigm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of 6G-enabled Internet of Vehicles (IoV) promises to\nrevolutionize mobility and connectivity, integrating vehicles into a mobile\nInternet of Things (IoT)-oriented wireless sensor network (WSN). Meanwhile, 5G\ntechnologies and mobile edge computing further support this vision by\nfacilitating real-time connectivity and empowering massive access to the\nInternet. Within this context, IoT-oriented WSNs play a crucial role in\nintelligent transportation systems, offering affordable alternatives for\ntraffic monitoring and management. Efficient sensor selection thus represents a\ncritical concern while deploying WSNs on urban networks. In this paper, we\nprovide an overview of such a notably hard problem. The contribution is\ntwofold: (i) surveying state-of-the-art model-based techniques for efficient\nsensor selection in traffic flow monitoring, emphasizing challenges of sensor\nplacement, and (ii) advocating for {the development of} data-driven\nmethodologies to enhance sensor deployment efficacy and traffic modeling\naccuracy. Further considerations underscore the importance of data-driven\napproaches for adaptive transportation systems aligned with the IoV paradigm."
                },
                "authors": [
                    {
                        "name": "Marco Fabris"
                    },
                    {
                        "name": "Riccardo Ceccato"
                    },
                    {
                        "name": "Andrea Zanella"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Zanella"
                },
                "author": "Andrea Zanella",
                "arxiv_doi": "10.3390/s25051416",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.3390/s25051416",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2404.08588v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.08588v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "21 pages, 2 figures",
                "arxiv_journal_ref": "MDPI: Sensors. 2025; 25(5):1416",
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19127v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19127v1",
                "updated": "2025-02-26T13:34:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    13,
                    34,
                    52,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-26T13:34:52Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    13,
                    34,
                    52,
                    2,
                    57,
                    0
                ],
                "title": "Self-Memory Alignment: Mitigating Factual Hallucinations with\n  Generalized Improvement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-Memory Alignment: Mitigating Factual Hallucinations with\n  Generalized Improvement"
                },
                "summary": "Large Language Models (LLMs) often struggle to align their responses with\nobjective facts, resulting in the issue of factual hallucinations, which can be\ndifficult to detect and mislead users without relevant knowledge. While\npost-training techniques have been employed to mitigate the issue, existing\nmethods usually suffer from poor generalization and trade-offs in different\ncapabilities. In this paper, we propose to address it by directly augmenting\nLLM's fundamental ability to precisely leverage its existing memory--the\nknowledge acquired from pre-training data. We introduce self-memory alignment\n(SMA), which fine-tunes the model on self-generated responses to precise and\nsimple factual questions through preference optimization. Furthermore, we\nconstruct FactualBench, a comprehensive and precise factual QA dataset\ncontaining 181k Chinese data spanning 21 domains, to facilitate both evaluation\nand training. Extensive experiments show that SMA significantly improves LLMs'\noverall performance, with consistent enhancement across various benchmarks\nconcerning factuality, as well as helpfulness and comprehensive skills.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) often struggle to align their responses with\nobjective facts, resulting in the issue of factual hallucinations, which can be\ndifficult to detect and mislead users without relevant knowledge. While\npost-training techniques have been employed to mitigate the issue, existing\nmethods usually suffer from poor generalization and trade-offs in different\ncapabilities. In this paper, we propose to address it by directly augmenting\nLLM's fundamental ability to precisely leverage its existing memory--the\nknowledge acquired from pre-training data. We introduce self-memory alignment\n(SMA), which fine-tunes the model on self-generated responses to precise and\nsimple factual questions through preference optimization. Furthermore, we\nconstruct FactualBench, a comprehensive and precise factual QA dataset\ncontaining 181k Chinese data spanning 21 domains, to facilitate both evaluation\nand training. Extensive experiments show that SMA significantly improves LLMs'\noverall performance, with consistent enhancement across various benchmarks\nconcerning factuality, as well as helpfulness and comprehensive skills."
                },
                "authors": [
                    {
                        "name": "Siyuan Zhang"
                    },
                    {
                        "name": "Yichi Zhang"
                    },
                    {
                        "name": "Yinpeng Dong"
                    },
                    {
                        "name": "Hang Su"
                    }
                ],
                "author_detail": {
                    "name": "Hang Su"
                },
                "author": "Hang Su",
                "arxiv_comment": "29 pages, 17 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19127v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19127v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.00036v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.00036v2",
                "updated": "2025-02-26T13:18:09Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    13,
                    18,
                    9,
                    2,
                    57,
                    0
                ],
                "published": "2024-05-27T10:53:15Z",
                "published_parsed": [
                    2024,
                    5,
                    27,
                    10,
                    53,
                    15,
                    0,
                    148,
                    0
                ],
                "title": "EMERGE: Enhancing Multimodal Electronic Health Records Predictive\n  Modeling with Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EMERGE: Enhancing Multimodal Electronic Health Records Predictive\n  Modeling with Retrieval-Augmented Generation"
                },
                "summary": "The integration of multimodal Electronic Health Records (EHR) data has\nsignificantly advanced clinical predictive capabilities. Existing models, which\nutilize clinical notes and multivariate time-series EHR data, often fall short\nof incorporating the necessary medical context for accurate clinical tasks,\nwhile previous approaches with knowledge graphs (KGs) primarily focus on\nstructured knowledge extraction. In response, we propose EMERGE, a\nRetrieval-Augmented Generation (RAG) driven framework to enhance multimodal EHR\npredictive modeling. We extract entities from both time-series data and\nclinical notes by prompting Large Language Models (LLMs) and align them with\nprofessional PrimeKG, ensuring consistency. In addition to triplet\nrelationships, we incorporate entities' definitions and descriptions for richer\nsemantics. The extracted knowledge is then used to generate task-relevant\nsummaries of patients' health statuses. Finally, we fuse the summary with other\nmodalities using an adaptive multimodal fusion network with cross-attention.\nExtensive experiments on the MIMIC-III and MIMIC-IV datasets' in-hospital\nmortality and 30-day readmission tasks demonstrate the superior performance of\nthe EMERGE framework over baseline models. Comprehensive ablation studies and\nanalysis highlight the efficacy of each designed module and robustness to data\nsparsity. EMERGE contributes to refining the utilization of multimodal EHR data\nin healthcare, bridging the gap with nuanced medical contexts essential for\ninformed clinical predictions. We have publicly released the code at\nhttps://github.com/yhzhu99/EMERGE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of multimodal Electronic Health Records (EHR) data has\nsignificantly advanced clinical predictive capabilities. Existing models, which\nutilize clinical notes and multivariate time-series EHR data, often fall short\nof incorporating the necessary medical context for accurate clinical tasks,\nwhile previous approaches with knowledge graphs (KGs) primarily focus on\nstructured knowledge extraction. In response, we propose EMERGE, a\nRetrieval-Augmented Generation (RAG) driven framework to enhance multimodal EHR\npredictive modeling. We extract entities from both time-series data and\nclinical notes by prompting Large Language Models (LLMs) and align them with\nprofessional PrimeKG, ensuring consistency. In addition to triplet\nrelationships, we incorporate entities' definitions and descriptions for richer\nsemantics. The extracted knowledge is then used to generate task-relevant\nsummaries of patients' health statuses. Finally, we fuse the summary with other\nmodalities using an adaptive multimodal fusion network with cross-attention.\nExtensive experiments on the MIMIC-III and MIMIC-IV datasets' in-hospital\nmortality and 30-day readmission tasks demonstrate the superior performance of\nthe EMERGE framework over baseline models. Comprehensive ablation studies and\nanalysis highlight the efficacy of each designed module and robustness to data\nsparsity. EMERGE contributes to refining the utilization of multimodal EHR data\nin healthcare, bridging the gap with nuanced medical contexts essential for\ninformed clinical predictions. We have publicly released the code at\nhttps://github.com/yhzhu99/EMERGE."
                },
                "authors": [
                    {
                        "name": "Yinghao Zhu"
                    },
                    {
                        "name": "Changyu Ren"
                    },
                    {
                        "name": "Zixiang Wang"
                    },
                    {
                        "name": "Xiaochen Zheng"
                    },
                    {
                        "name": "Shiyun Xie"
                    },
                    {
                        "name": "Junlan Feng"
                    },
                    {
                        "name": "Xi Zhu"
                    },
                    {
                        "name": "Zhoujun Li"
                    },
                    {
                        "name": "Liantao Ma"
                    },
                    {
                        "name": "Chengwei Pan"
                    }
                ],
                "author_detail": {
                    "name": "Chengwei Pan"
                },
                "author": "Chengwei Pan",
                "arxiv_doi": "10.1145/3627673.3679582",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3627673.3679582",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2406.00036v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.00036v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "CIKM 2024 Full Research Paper; arXiv admin note: text overlap with\n  arXiv:2402.07016",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.16894v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.16894v2",
                "updated": "2025-02-26T13:02:48Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    13,
                    2,
                    48,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-24T06:48:13Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    6,
                    48,
                    13,
                    0,
                    55,
                    0
                ],
                "title": "Make LoRA Great Again: Boosting LoRA with Adaptive Singular Values and\n  Mixture-of-Experts Optimization Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Make LoRA Great Again: Boosting LoRA with Adaptive Singular Values and\n  Mixture-of-Experts Optimization Alignment"
                },
                "summary": "While Low-Rank Adaptation (LoRA) enables parameter-efficient fine-tuning for\nLarge Language Models (LLMs), its performance often falls short of Full\nFine-Tuning (Full FT). Current methods optimize LoRA by initializing with\nstatic singular value decomposition (SVD) subsets, leading to suboptimal\nleveraging of pre-trained knowledge. Another path for improving LoRA is\nincorporating a Mixture-of-Experts (MoE) architecture. However, weight\nmisalignment and complex gradient dynamics make it challenging to adopt SVD\nprior to the LoRA MoE architecture. To mitigate these issues, we propose\n\\underline{G}reat L\\underline{o}R\\underline{A} Mixture-of-Exper\\underline{t}\n(GOAT), a framework that (1) adaptively integrates relevant priors using an\nSVD-structured MoE, and (2) aligns optimization with full fine-tuned MoE by\nderiving a theoretical scaling factor. We demonstrate that proper scaling,\nwithout modifying the architecture or training algorithms, boosts LoRA MoE's\nefficiency and performance. Experiments across 25 datasets, including natural\nlanguage understanding, commonsense reasoning, image classification, and\nnatural language generation, demonstrate GOAT's state-of-the-art performance,\nclosing the gap with Full FT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Low-Rank Adaptation (LoRA) enables parameter-efficient fine-tuning for\nLarge Language Models (LLMs), its performance often falls short of Full\nFine-Tuning (Full FT). Current methods optimize LoRA by initializing with\nstatic singular value decomposition (SVD) subsets, leading to suboptimal\nleveraging of pre-trained knowledge. Another path for improving LoRA is\nincorporating a Mixture-of-Experts (MoE) architecture. However, weight\nmisalignment and complex gradient dynamics make it challenging to adopt SVD\nprior to the LoRA MoE architecture. To mitigate these issues, we propose\n\\underline{G}reat L\\underline{o}R\\underline{A} Mixture-of-Exper\\underline{t}\n(GOAT), a framework that (1) adaptively integrates relevant priors using an\nSVD-structured MoE, and (2) aligns optimization with full fine-tuned MoE by\nderiving a theoretical scaling factor. We demonstrate that proper scaling,\nwithout modifying the architecture or training algorithms, boosts LoRA MoE's\nefficiency and performance. Experiments across 25 datasets, including natural\nlanguage understanding, commonsense reasoning, image classification, and\nnatural language generation, demonstrate GOAT's state-of-the-art performance,\nclosing the gap with Full FT."
                },
                "authors": [
                    {
                        "name": "Chenghao Fan"
                    },
                    {
                        "name": "Zhenyi Lu"
                    },
                    {
                        "name": "Sichen Liu"
                    },
                    {
                        "name": "Xiaoye Qu"
                    },
                    {
                        "name": "Wei Wei"
                    },
                    {
                        "name": "Chengfeng Gu"
                    },
                    {
                        "name": "Yu Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Yu Cheng"
                },
                "author": "Yu Cheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.16894v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.16894v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19104v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19104v1",
                "updated": "2025-02-26T12:46:59Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    12,
                    46,
                    59,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-26T12:46:59Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    12,
                    46,
                    59,
                    2,
                    57,
                    0
                ],
                "title": "Evaluating Gender Bias in German Machine Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Gender Bias in German Machine Translation"
                },
                "summary": "We present WinoMTDE, a new gender bias evaluation test set designed to assess\noccupational stereotyping and underrepresentation in German machine translation\n(MT) systems. Building on the automatic evaluation method introduced by\narXiv:1906.00591v1 [cs.CL], we extend the approach to German, a language with\ngrammatical gender. The WinoMTDE dataset comprises 288 German sentences that\nare balanced in regard to gender, as well as stereotype, which was annotated\nusing German labor statistics. We conduct a large-scale evaluation of five\nwidely used MT systems and a large language model. Our results reveal\npersistent bias in most models, with the LLM outperforming traditional systems.\nThe dataset and evaluation code are publicly available under\nhttps://github.com/michellekappl/mt_gender_german.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present WinoMTDE, a new gender bias evaluation test set designed to assess\noccupational stereotyping and underrepresentation in German machine translation\n(MT) systems. Building on the automatic evaluation method introduced by\narXiv:1906.00591v1 [cs.CL], we extend the approach to German, a language with\ngrammatical gender. The WinoMTDE dataset comprises 288 German sentences that\nare balanced in regard to gender, as well as stereotype, which was annotated\nusing German labor statistics. We conduct a large-scale evaluation of five\nwidely used MT systems and a large language model. Our results reveal\npersistent bias in most models, with the LLM outperforming traditional systems.\nThe dataset and evaluation code are publicly available under\nhttps://github.com/michellekappl/mt_gender_german."
                },
                "authors": [
                    {
                        "name": "Michelle Kappl"
                    }
                ],
                "author_detail": {
                    "name": "Michelle Kappl"
                },
                "author": "Michelle Kappl",
                "arxiv_comment": "ISCA/ITG Workshop on Diversity in Large Speech and Language Models",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19104v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19104v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19103v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19103v1",
                "updated": "2025-02-26T12:46:36Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    12,
                    46,
                    36,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-26T12:46:36Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    12,
                    46,
                    36,
                    2,
                    57,
                    0
                ],
                "title": "LongEval: A Comprehensive Analysis of Long-Text Generation Through a\n  Plan-based Paradigm",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LongEval: A Comprehensive Analysis of Long-Text Generation Through a\n  Plan-based Paradigm"
                },
                "summary": "Large Language Models (LLMs) have achieved remarkable success in various\nnatural language processing tasks, yet their ability to generate long-form\ncontent remains poorly understood and evaluated. Our analysis reveals that\ncurrent LLMs struggle with length requirements and information density in\nlong-text generation, with performance deteriorating as text length increases.\nTo quantitively locate such a performance degradation and provide further\ninsights on model development, we present LongEval, a benchmark that evaluates\nlong-text generation through both direct and plan-based generation paradigms,\ninspired by cognitive and linguistic writing models. The comprehensive\nexperiments in this work reveal interesting findings such as that while model\nsize correlates with generation ability, the small-scale model (e.g.,\nLongWriter), well-trained on long texts, has comparable performance. All code\nand datasets are released in https://github.com/Wusiwei0410/LongEval.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved remarkable success in various\nnatural language processing tasks, yet their ability to generate long-form\ncontent remains poorly understood and evaluated. Our analysis reveals that\ncurrent LLMs struggle with length requirements and information density in\nlong-text generation, with performance deteriorating as text length increases.\nTo quantitively locate such a performance degradation and provide further\ninsights on model development, we present LongEval, a benchmark that evaluates\nlong-text generation through both direct and plan-based generation paradigms,\ninspired by cognitive and linguistic writing models. The comprehensive\nexperiments in this work reveal interesting findings such as that while model\nsize correlates with generation ability, the small-scale model (e.g.,\nLongWriter), well-trained on long texts, has comparable performance. All code\nand datasets are released in https://github.com/Wusiwei0410/LongEval."
                },
                "authors": [
                    {
                        "name": "Siwei Wu"
                    },
                    {
                        "name": "Yizhi Li"
                    },
                    {
                        "name": "Xingwei Qu"
                    },
                    {
                        "name": "Rishi Ravikumar"
                    },
                    {
                        "name": "Yucheng Li"
                    },
                    {
                        "name": "Tyler Loakman Shanghaoran Quan Xiaoyong Wei"
                    },
                    {
                        "name": "Riza Batista-Navarro"
                    },
                    {
                        "name": "Chenghua Lin"
                    }
                ],
                "author_detail": {
                    "name": "Chenghua Lin"
                },
                "author": "Chenghua Lin",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19103v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19103v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19098v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19098v1",
                "updated": "2025-02-26T12:43:22Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    12,
                    43,
                    22,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-26T12:43:22Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    12,
                    43,
                    22,
                    2,
                    57,
                    0
                ],
                "title": "Language-Driven Opinion Dynamics in Agent-Based Simulations with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language-Driven Opinion Dynamics in Agent-Based Simulations with LLMs"
                },
                "summary": "Understanding how opinions evolve is crucial for addressing issues such as\npolarization, radicalization, and consensus in social systems. While much\nresearch has focused on identifying factors influencing opinion change, the\nrole of language and argumentative fallacies remains underexplored. This paper\naims to fill this gap by investigating how language - along with social\ndynamics - influences opinion evolution through LODAS, a Language-Driven\nOpinion Dynamics Model for Agent-Based Simulations. The model simulates debates\naround the \"Ship of Theseus\" paradox, in which agents with discrete opinions\ninteract with each other and evolve their opinions by accepting, rejecting, or\nignoring the arguments presented. We study three different scenarios: balanced,\npolarized, and unbalanced opinion distributions. Agreeableness and sycophancy\nemerge as two main characteristics of LLM agents, and consensus around the\npresented statement emerges almost in any setting. Moreover, such AI agents are\noften producers of fallacious arguments in the attempt of persuading their\npeers and - for their complacency - they are also highly influenced by\narguments built on logical fallacies. These results highlight the potential of\nthis framework not only for simulating social dynamics but also for exploring\nfrom another perspective biases and shortcomings of LLMs, which may impact\ntheir interactions with humans.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding how opinions evolve is crucial for addressing issues such as\npolarization, radicalization, and consensus in social systems. While much\nresearch has focused on identifying factors influencing opinion change, the\nrole of language and argumentative fallacies remains underexplored. This paper\naims to fill this gap by investigating how language - along with social\ndynamics - influences opinion evolution through LODAS, a Language-Driven\nOpinion Dynamics Model for Agent-Based Simulations. The model simulates debates\naround the \"Ship of Theseus\" paradox, in which agents with discrete opinions\ninteract with each other and evolve their opinions by accepting, rejecting, or\nignoring the arguments presented. We study three different scenarios: balanced,\npolarized, and unbalanced opinion distributions. Agreeableness and sycophancy\nemerge as two main characteristics of LLM agents, and consensus around the\npresented statement emerges almost in any setting. Moreover, such AI agents are\noften producers of fallacious arguments in the attempt of persuading their\npeers and - for their complacency - they are also highly influenced by\narguments built on logical fallacies. These results highlight the potential of\nthis framework not only for simulating social dynamics but also for exploring\nfrom another perspective biases and shortcomings of LLMs, which may impact\ntheir interactions with humans."
                },
                "authors": [
                    {
                        "name": "Erica Cau"
                    },
                    {
                        "name": "Valentina Pansanella"
                    },
                    {
                        "name": "Dino Pedreschi"
                    },
                    {
                        "name": "Giulio Rossetti"
                    }
                ],
                "author_detail": {
                    "name": "Giulio Rossetti"
                },
                "author": "Giulio Rossetti",
                "arxiv_comment": "34 pages, journal submission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19098v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19098v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08964v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08964v3",
                "updated": "2025-02-26T12:39:40Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    12,
                    39,
                    40,
                    2,
                    57,
                    0
                ],
                "published": "2024-10-11T16:32:05Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    16,
                    32,
                    5,
                    4,
                    285,
                    0
                ],
                "title": "Language Imbalance Driven Rewarding for Multilingual Self-improving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Imbalance Driven Rewarding for Multilingual Self-improving"
                },
                "summary": "Large Language Models (LLMs) have achieved state-of-the-art performance\nacross numerous tasks. However, these advancements have predominantly benefited\n\"first-class\" languages such as English and Chinese, leaving many other\nlanguages underrepresented. This imbalance, while limiting broader\napplications, generates a natural preference ranking between languages,\noffering an opportunity to bootstrap the multilingual capabilities of LLM in a\nself-improving manner. Thus, we propose $\\textit{Language Imbalance Driven\nRewarding}$, where the inherent imbalance between dominant and non-dominant\nlanguages within LLMs is leveraged as a reward signal. Iterative DPO training\ndemonstrates that this approach not only enhances LLM performance in\nnon-dominant languages but also improves the dominant language's capacity,\nthereby yielding an iterative reward signal. Fine-tuning\nMeta-Llama-3-8B-Instruct over two iterations of this approach results in\ncontinuous improvements in multilingual performance across\ninstruction-following and arithmetic reasoning tasks, evidenced by an average\nimprovement of 7.46% win rate on the X-AlpacaEval leaderboard and 13.9%\naccuracy on the MGSM benchmark. This work serves as an initial exploration,\npaving the way for multilingual self-improvement of LLMs. The code is available\nat https://github.com/ZNLP/Language-Imbalance-Driven-Rewarding",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved state-of-the-art performance\nacross numerous tasks. However, these advancements have predominantly benefited\n\"first-class\" languages such as English and Chinese, leaving many other\nlanguages underrepresented. This imbalance, while limiting broader\napplications, generates a natural preference ranking between languages,\noffering an opportunity to bootstrap the multilingual capabilities of LLM in a\nself-improving manner. Thus, we propose $\\textit{Language Imbalance Driven\nRewarding}$, where the inherent imbalance between dominant and non-dominant\nlanguages within LLMs is leveraged as a reward signal. Iterative DPO training\ndemonstrates that this approach not only enhances LLM performance in\nnon-dominant languages but also improves the dominant language's capacity,\nthereby yielding an iterative reward signal. Fine-tuning\nMeta-Llama-3-8B-Instruct over two iterations of this approach results in\ncontinuous improvements in multilingual performance across\ninstruction-following and arithmetic reasoning tasks, evidenced by an average\nimprovement of 7.46% win rate on the X-AlpacaEval leaderboard and 13.9%\naccuracy on the MGSM benchmark. This work serves as an initial exploration,\npaving the way for multilingual self-improvement of LLMs. The code is available\nat https://github.com/ZNLP/Language-Imbalance-Driven-Rewarding"
                },
                "authors": [
                    {
                        "name": "Wen Yang"
                    },
                    {
                        "name": "Junhong Wu"
                    },
                    {
                        "name": "Chen Wang"
                    },
                    {
                        "name": "Chengqing Zong"
                    },
                    {
                        "name": "Jiajun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jiajun Zhang"
                },
                "author": "Jiajun Zhang",
                "arxiv_comment": "Camera ready version for ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08964v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08964v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19091v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19091v1",
                "updated": "2025-02-26T12:37:47Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    12,
                    37,
                    47,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-26T12:37:47Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    12,
                    37,
                    47,
                    2,
                    57,
                    0
                ],
                "title": "Nexus: A Lightweight and Scalable Multi-Agent Framework for Complex\n  Tasks Automation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nexus: A Lightweight and Scalable Multi-Agent Framework for Complex\n  Tasks Automation"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) have substantially\nevolved Multi-Agent Systems (MASs) capabilities, enabling systems that not only\nautomate tasks but also leverage near-human reasoning capabilities. To achieve\nthis, LLM-based MASs need to be built around two critical principles: (i) a\nrobust architecture that fully exploits LLM potential for specific tasks -- or\nrelated task sets -- and ($ii$) an effective methodology for equipping LLMs\nwith the necessary capabilities to perform tasks and manage information\nefficiently. It goes without saying that a priori architectural designs can\nlimit the scalability and domain adaptability of a given MAS.\n  To address these challenges, in this paper we introduce Nexus: a lightweight\nPython framework designed to easily build and manage LLM-based MASs. Nexus\nintroduces the following innovations: (i) a flexible multi-supervisor\nhierarchy, (ii) a simplified workflow design, and (iii) easy installation and\nopen-source flexibility: Nexus can be installed via pip and is distributed\nunder a permissive open-source license, allowing users to freely modify and\nextend its capabilities.\n  Experimental results demonstrate that architectures built with Nexus exhibit\nstate-of-the-art performance across diverse domains. In coding tasks,\nNexus-driven MASs achieve a 99% pass rate on HumanEval and a flawless 100% on\nVerilogEval-Human, outperforming cutting-edge reasoning language models such as\no3-mini and DeepSeek-R1. Moreover, these architectures display robust\nproficiency in complex reasoning and mathematical problem solving, achieving\ncorrect solutions for all randomly selected problems from the MATH dataset. In\nthe realm of multi-objective optimization, Nexus-based architectures\nsuccessfully address challenging timing closure tasks on designs from the VTR\nbenchmark suite, while guaranteeing, on average, a power saving of nearly 30%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) have substantially\nevolved Multi-Agent Systems (MASs) capabilities, enabling systems that not only\nautomate tasks but also leverage near-human reasoning capabilities. To achieve\nthis, LLM-based MASs need to be built around two critical principles: (i) a\nrobust architecture that fully exploits LLM potential for specific tasks -- or\nrelated task sets -- and ($ii$) an effective methodology for equipping LLMs\nwith the necessary capabilities to perform tasks and manage information\nefficiently. It goes without saying that a priori architectural designs can\nlimit the scalability and domain adaptability of a given MAS.\n  To address these challenges, in this paper we introduce Nexus: a lightweight\nPython framework designed to easily build and manage LLM-based MASs. Nexus\nintroduces the following innovations: (i) a flexible multi-supervisor\nhierarchy, (ii) a simplified workflow design, and (iii) easy installation and\nopen-source flexibility: Nexus can be installed via pip and is distributed\nunder a permissive open-source license, allowing users to freely modify and\nextend its capabilities.\n  Experimental results demonstrate that architectures built with Nexus exhibit\nstate-of-the-art performance across diverse domains. In coding tasks,\nNexus-driven MASs achieve a 99% pass rate on HumanEval and a flawless 100% on\nVerilogEval-Human, outperforming cutting-edge reasoning language models such as\no3-mini and DeepSeek-R1. Moreover, these architectures display robust\nproficiency in complex reasoning and mathematical problem solving, achieving\ncorrect solutions for all randomly selected problems from the MATH dataset. In\nthe realm of multi-objective optimization, Nexus-based architectures\nsuccessfully address challenging timing closure tasks on designs from the VTR\nbenchmark suite, while guaranteeing, on average, a power saving of nearly 30%."
                },
                "authors": [
                    {
                        "name": "Humza Sami"
                    },
                    {
                        "name": "Mubashir ul Islam"
                    },
                    {
                        "name": "Samy Charas"
                    },
                    {
                        "name": "Asav Gandhi"
                    },
                    {
                        "name": "Pierre-Emmanuel Gaillardon"
                    },
                    {
                        "name": "Valerio Tenace"
                    }
                ],
                "author_detail": {
                    "name": "Valerio Tenace"
                },
                "author": "Valerio Tenace",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19091v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19091v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19078v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19078v1",
                "updated": "2025-02-26T12:11:16Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    12,
                    11,
                    16,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-26T12:11:16Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    12,
                    11,
                    16,
                    2,
                    57,
                    0
                ],
                "title": "Sparse Brains are Also Adaptive Brains: Cognitive-Load-Aware Dynamic\n  Activation for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse Brains are Also Adaptive Brains: Cognitive-Load-Aware Dynamic\n  Activation for LLMs"
                },
                "summary": "Dense large language models(LLMs) face critical efficiency bottlenecks as\nthey rigidly activate all parameters regardless of input complexity. While\nexisting sparsity methods(static pruning or dynamic activation) address this\npartially, they either lack adaptivity to contextual or model structural\ndemands or incur prohibitive computational overhead. Inspired by human brain's\ndual-process mechanisms - predictive coding (N400) for backbone sparsity and\nstructural reanalysis (P600) for complex context - we propose CLADA, a\n\\textit{\\textbf{C}ognitive-\\textbf{L}oad-\\textbf{A}ware \\textbf{D}ynamic\n\\textbf{A}ctivation} framework that synergizes statistical sparsity with\nsemantic adaptability. Our key insight is that LLM activations exhibit two\ncomplementary patterns: 1) \\textit{Global statistical sparsity} driven by\nsequence-level prefix information, and 2) \\textit{Local semantic adaptability}\nmodulated by cognitive load metrics(e.g., surprisal and entropy). CLADA employs\na hierarchical thresholding strategy: a baseline from offline error-controlled\noptimization ensures 40\\%+ sparsity, dynamically adjusted by real-time\ncognitive signals. Evaluations across six mainstream LLMs and nine benchmarks\ndemonstrate that CLADA achieves \\textbf{~20\\% average speedup with <2\\%\naccuracy drop}, outperforming Griffin (5\\%+ degradation) and TT (negligible\nspeedup). Crucially, we establish the first formal connection between\nneurolinguistic event-related potential (ERP) components and LLM efficiency\nmechanisms through multi-level regression analysis ($R^2=0.17$ for\nsparsity-adaptation synergy). Requiring no retraining or architectural changes,\nCLADA offers a deployable solution for resource-aware LLM inference while\nadvancing biologically-inspired AI design. Our code is available at\n\\href{https://github.com/Oldify/CLADA}{CLADA}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dense large language models(LLMs) face critical efficiency bottlenecks as\nthey rigidly activate all parameters regardless of input complexity. While\nexisting sparsity methods(static pruning or dynamic activation) address this\npartially, they either lack adaptivity to contextual or model structural\ndemands or incur prohibitive computational overhead. Inspired by human brain's\ndual-process mechanisms - predictive coding (N400) for backbone sparsity and\nstructural reanalysis (P600) for complex context - we propose CLADA, a\n\\textit{\\textbf{C}ognitive-\\textbf{L}oad-\\textbf{A}ware \\textbf{D}ynamic\n\\textbf{A}ctivation} framework that synergizes statistical sparsity with\nsemantic adaptability. Our key insight is that LLM activations exhibit two\ncomplementary patterns: 1) \\textit{Global statistical sparsity} driven by\nsequence-level prefix information, and 2) \\textit{Local semantic adaptability}\nmodulated by cognitive load metrics(e.g., surprisal and entropy). CLADA employs\na hierarchical thresholding strategy: a baseline from offline error-controlled\noptimization ensures 40\\%+ sparsity, dynamically adjusted by real-time\ncognitive signals. Evaluations across six mainstream LLMs and nine benchmarks\ndemonstrate that CLADA achieves \\textbf{~20\\% average speedup with <2\\%\naccuracy drop}, outperforming Griffin (5\\%+ degradation) and TT (negligible\nspeedup). Crucially, we establish the first formal connection between\nneurolinguistic event-related potential (ERP) components and LLM efficiency\nmechanisms through multi-level regression analysis ($R^2=0.17$ for\nsparsity-adaptation synergy). Requiring no retraining or architectural changes,\nCLADA offers a deployable solution for resource-aware LLM inference while\nadvancing biologically-inspired AI design. Our code is available at\n\\href{https://github.com/Oldify/CLADA}{CLADA}."
                },
                "authors": [
                    {
                        "name": "Yiheng Yang"
                    },
                    {
                        "name": "Yujie Wang"
                    },
                    {
                        "name": "Chi Ma"
                    },
                    {
                        "name": "Lei Yu"
                    },
                    {
                        "name": "Emmanuele Chersoni"
                    },
                    {
                        "name": "Chu-Ren Huang"
                    }
                ],
                "author_detail": {
                    "name": "Chu-Ren Huang"
                },
                "author": "Chu-Ren Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19078v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19078v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19067v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19067v1",
                "updated": "2025-02-26T11:48:42Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    11,
                    48,
                    42,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-26T11:48:42Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    11,
                    48,
                    42,
                    2,
                    57,
                    0
                ],
                "title": "IndicEval-XL: Bridging Linguistic Diversity in Code Generation Across\n  Indic Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IndicEval-XL: Bridging Linguistic Diversity in Code Generation Across\n  Indic Languages"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\ncode generation from natural language prompts, revolutionizing software\ndevelopment workflows. As we advance towards agent-based development paradigms,\nthese models form the cornerstone of next-generation software development\nlifecycles. However, current benchmarks for evaluating multilingual code\ngeneration capabilities are predominantly English-centric, limiting their\napplicability across the global developer community. To address this\nlimitation, we present IndicEval-XL, a comprehensive benchmark for code\ngeneration that incorporates 6 major Indic languages, collectively spoken by\napproximately 14\\% of the world's population. Our benchmark bridges these\nlanguages with 12 programming languages, creating a robust evaluation\nframework. This work is particularly significant given India's representation\nof one-eighth of the global population and the crucial role Indic languages\nplay in Indian society. IndicEval-XL represents a significant step toward\nexpanding the linguistic diversity in code generation systems and evaluation\nframeworks. By developing resources that support multiple languages, we aim to\nmake AI-powered development tools more inclusive and accessible to developers\nof various linguistic backgrounds. To facilitate further research and\ndevelopment in this direction, we make our dataset and evaluation benchmark\npublicly available at https://github.com/telekom/IndicEval-XL",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\ncode generation from natural language prompts, revolutionizing software\ndevelopment workflows. As we advance towards agent-based development paradigms,\nthese models form the cornerstone of next-generation software development\nlifecycles. However, current benchmarks for evaluating multilingual code\ngeneration capabilities are predominantly English-centric, limiting their\napplicability across the global developer community. To address this\nlimitation, we present IndicEval-XL, a comprehensive benchmark for code\ngeneration that incorporates 6 major Indic languages, collectively spoken by\napproximately 14\\% of the world's population. Our benchmark bridges these\nlanguages with 12 programming languages, creating a robust evaluation\nframework. This work is particularly significant given India's representation\nof one-eighth of the global population and the crucial role Indic languages\nplay in Indian society. IndicEval-XL represents a significant step toward\nexpanding the linguistic diversity in code generation systems and evaluation\nframeworks. By developing resources that support multiple languages, we aim to\nmake AI-powered development tools more inclusive and accessible to developers\nof various linguistic backgrounds. To facilitate further research and\ndevelopment in this direction, we make our dataset and evaluation benchmark\npublicly available at https://github.com/telekom/IndicEval-XL"
                },
                "authors": [
                    {
                        "name": "Ujjwal Singh"
                    },
                    {
                        "name": "Aditi Sharma"
                    },
                    {
                        "name": "Nikhil Gupta"
                    },
                    {
                        "name": "Deepakshi"
                    },
                    {
                        "name": "Vivek Kumar Jha"
                    }
                ],
                "author_detail": {
                    "name": "Vivek Kumar Jha"
                },
                "author": "Vivek Kumar Jha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19067v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19067v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15766v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15766v3",
                "updated": "2025-02-26T11:47:58Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    11,
                    47,
                    58,
                    2,
                    57,
                    0
                ],
                "published": "2024-08-28T12:59:12Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    12,
                    59,
                    12,
                    2,
                    241,
                    0
                ],
                "title": "Learning Harmonized Representations for Speculative Sampling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Harmonized Representations for Speculative Sampling"
                },
                "summary": "Speculative sampling is a promising approach to accelerate the decoding stage\nfor Large Language Models (LLMs). Recent advancements that leverage target\nLLM's contextual information, such as hidden states and KV cache, have shown\nsignificant practical improvements. However, these approaches suffer from\ninconsistent context between training and decoding. We also observe another\ndiscrepancy between the training and decoding objectives in existing\nspeculative sampling methods. In this work, we propose a solution named\nHArmonized Speculative Sampling (HASS) that learns harmonized representations\nto address these issues. HASS accelerates the decoding stage without adding\ninference overhead through harmonized objective distillation and harmonized\ncontext alignment. Experiments on four LLaMA models demonstrate that HASS\nachieves 2.81x-4.05x wall-clock time speedup ratio averaging across three\ndatasets, surpassing EAGLE-2 by 8%-20%. The code is available at\nhttps://github.com/HArmonizedSS/HASS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative sampling is a promising approach to accelerate the decoding stage\nfor Large Language Models (LLMs). Recent advancements that leverage target\nLLM's contextual information, such as hidden states and KV cache, have shown\nsignificant practical improvements. However, these approaches suffer from\ninconsistent context between training and decoding. We also observe another\ndiscrepancy between the training and decoding objectives in existing\nspeculative sampling methods. In this work, we propose a solution named\nHArmonized Speculative Sampling (HASS) that learns harmonized representations\nto address these issues. HASS accelerates the decoding stage without adding\ninference overhead through harmonized objective distillation and harmonized\ncontext alignment. Experiments on four LLaMA models demonstrate that HASS\nachieves 2.81x-4.05x wall-clock time speedup ratio averaging across three\ndatasets, surpassing EAGLE-2 by 8%-20%. The code is available at\nhttps://github.com/HArmonizedSS/HASS."
                },
                "authors": [
                    {
                        "name": "Lefan Zhang"
                    },
                    {
                        "name": "Xiaodan Wang"
                    },
                    {
                        "name": "Yanhua Huang"
                    },
                    {
                        "name": "Ruiwen Xu"
                    }
                ],
                "author_detail": {
                    "name": "Ruiwen Xu"
                },
                "author": "Ruiwen Xu",
                "arxiv_comment": "Published as a conference paper at ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15766v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15766v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19064v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19064v1",
                "updated": "2025-02-26T11:43:25Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    11,
                    43,
                    25,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-26T11:43:25Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    11,
                    43,
                    25,
                    2,
                    57,
                    0
                ],
                "title": "Can Large Language Models Outperform Non-Experts in Poetry Evaluation? A\n  Comparative Study Using the Consensual Assessment Technique",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Large Language Models Outperform Non-Experts in Poetry Evaluation? A\n  Comparative Study Using the Consensual Assessment Technique"
                },
                "summary": "The Consensual Assessment Technique (CAT) evaluates creativity through\nholistic expert judgments. We investigate the use of two advanced Large\nLanguage Models (LLMs), Claude-3-Opus and GPT-4o, to evaluate poetry by a\nmethodology inspired by the CAT. Using a dataset of 90 poems, we found that\nthese LLMs can surpass the results achieved by non-expert human judges at\nmatching a ground truth based on publication venue, particularly when assessing\nsmaller subsets of poems. Claude-3-Opus exhibited slightly superior performance\nthan GPT-4o. We show that LLMs are viable tools for accurately assessing\npoetry, paving the way for their broader application into other creative\ndomains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Consensual Assessment Technique (CAT) evaluates creativity through\nholistic expert judgments. We investigate the use of two advanced Large\nLanguage Models (LLMs), Claude-3-Opus and GPT-4o, to evaluate poetry by a\nmethodology inspired by the CAT. Using a dataset of 90 poems, we found that\nthese LLMs can surpass the results achieved by non-expert human judges at\nmatching a ground truth based on publication venue, particularly when assessing\nsmaller subsets of poems. Claude-3-Opus exhibited slightly superior performance\nthan GPT-4o. We show that LLMs are viable tools for accurately assessing\npoetry, paving the way for their broader application into other creative\ndomains."
                },
                "authors": [
                    {
                        "name": "Piotr Sawicki"
                    },
                    {
                        "name": "Marek Grześ"
                    },
                    {
                        "name": "Dan Brown"
                    },
                    {
                        "name": "Fabrício Góes"
                    }
                ],
                "author_detail": {
                    "name": "Fabrício Góes"
                },
                "author": "Fabrício Góes",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19064v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19064v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19058v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19058v1",
                "updated": "2025-02-26T11:17:50Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    11,
                    17,
                    50,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-26T11:17:50Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    11,
                    17,
                    50,
                    2,
                    57,
                    0
                ],
                "title": "MathClean: A Benchmark for Synthetic Mathematical Data Cleaning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MathClean: A Benchmark for Synthetic Mathematical Data Cleaning"
                },
                "summary": "With the rapid development of large language models (LLMs), the quality of\ntraining data has become crucial. Among the various types of training data,\nmathematical data plays a key role in enabling LLMs to acquire strong reasoning\nabilities. While high-quality open-source data is important, it is often\ninsufficient for pre-training, necessitating the addition of synthetic math\nproblems. However, synthetic math questions and answers can introduce\ninaccuracies, which may degrade both the training data and web data. Therefore,\nan effective method for cleaning synthetic math data is essential. In this\npaper, we propose the MathClean benchmark to evaluate the effectiveness of math\ndata cleaning models. The MathClean benchmark consists of 2,000 correct\nquestions and 2,000 erroneous questions with additional 2,000 correct and\nerroneous answers sourced from augmented data based on GSM8K and MATH.\nMoreover, we also annotate error types for each question or answer, since it\ncan assess whether models can correctly identify the error categories for\nfuture improvements. Finally, we present comprehensive evaluations using\nstate-of-the-art (SOTA) models. Our results demonstrate that even strong models\nlike GPT-o1 and DeepSeek-R1 perform poorly on this benchmark, highlighting the\nutility of MathClean. Our code and data is available at\nhttps://github.com/YuYingLi0/MathClean.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid development of large language models (LLMs), the quality of\ntraining data has become crucial. Among the various types of training data,\nmathematical data plays a key role in enabling LLMs to acquire strong reasoning\nabilities. While high-quality open-source data is important, it is often\ninsufficient for pre-training, necessitating the addition of synthetic math\nproblems. However, synthetic math questions and answers can introduce\ninaccuracies, which may degrade both the training data and web data. Therefore,\nan effective method for cleaning synthetic math data is essential. In this\npaper, we propose the MathClean benchmark to evaluate the effectiveness of math\ndata cleaning models. The MathClean benchmark consists of 2,000 correct\nquestions and 2,000 erroneous questions with additional 2,000 correct and\nerroneous answers sourced from augmented data based on GSM8K and MATH.\nMoreover, we also annotate error types for each question or answer, since it\ncan assess whether models can correctly identify the error categories for\nfuture improvements. Finally, we present comprehensive evaluations using\nstate-of-the-art (SOTA) models. Our results demonstrate that even strong models\nlike GPT-o1 and DeepSeek-R1 perform poorly on this benchmark, highlighting the\nutility of MathClean. Our code and data is available at\nhttps://github.com/YuYingLi0/MathClean."
                },
                "authors": [
                    {
                        "name": "Hao Liang"
                    },
                    {
                        "name": "Meiyi Qiang"
                    },
                    {
                        "name": "Yuying Li"
                    },
                    {
                        "name": "Zefeng He"
                    },
                    {
                        "name": "Yongzhen Guo"
                    },
                    {
                        "name": "Zhengzhou Zhu"
                    },
                    {
                        "name": "Wentao Zhang"
                    },
                    {
                        "name": "Bin Cui"
                    }
                ],
                "author_detail": {
                    "name": "Bin Cui"
                },
                "author": "Bin Cui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19058v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19058v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.16965v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.16965v2",
                "updated": "2025-02-26T11:15:13Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    11,
                    15,
                    13,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-24T08:44:01Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    8,
                    44,
                    1,
                    0,
                    55,
                    0
                ],
                "title": "Autoregressive Image Generation Guided by Chains of Thought",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive Image Generation Guided by Chains of Thought"
                },
                "summary": "In the field of autoregressive (AR) image generation, models based on the\n'next-token prediction' paradigm of LLMs have shown comparable performance to\ndiffusion models by reducing inductive biases. However, directly applying LLMs\nto complex image generation can struggle with reconstructing the structure and\ndetails of the image, impacting the accuracy and stability of generation.\nAdditionally, the 'next-token prediction' paradigm in the AR model does not\nalign with the contextual scanning and logical reasoning processes involved in\nhuman visual perception, limiting effective image generation. Chain-of-Thought\n(CoT), as a key reasoning capability of LLMs, utilizes reasoning prompts to\nguide the model, improving reasoning performance on complex natural language\nprocess (NLP) tasks, enhancing accuracy and stability of generation, and\nhelping the model maintain contextual coherence and logical consistency,\nsimilar to human reasoning. Inspired by CoT from the field of NLP, we propose\nautoregressive Image Generation with Thoughtful Reasoning (IGTR) to enhance\nautoregressive image generation. IGTR adds reasoning prompts without modifying\nthe model structure or raster generation order. Specifically, we design\nspecialized image-related reasoning prompts for AR image generation to simulate\nthe human reasoning process, which enhances contextual reasoning by allowing\nthe model to first perceive overall distribution information before generating\nthe image, and improve generation stability by increasing the inference steps.\nCompared to the AR method without prompts, our method shows outstanding\nperformance and achieves an approximate improvement of 20%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the field of autoregressive (AR) image generation, models based on the\n'next-token prediction' paradigm of LLMs have shown comparable performance to\ndiffusion models by reducing inductive biases. However, directly applying LLMs\nto complex image generation can struggle with reconstructing the structure and\ndetails of the image, impacting the accuracy and stability of generation.\nAdditionally, the 'next-token prediction' paradigm in the AR model does not\nalign with the contextual scanning and logical reasoning processes involved in\nhuman visual perception, limiting effective image generation. Chain-of-Thought\n(CoT), as a key reasoning capability of LLMs, utilizes reasoning prompts to\nguide the model, improving reasoning performance on complex natural language\nprocess (NLP) tasks, enhancing accuracy and stability of generation, and\nhelping the model maintain contextual coherence and logical consistency,\nsimilar to human reasoning. Inspired by CoT from the field of NLP, we propose\nautoregressive Image Generation with Thoughtful Reasoning (IGTR) to enhance\nautoregressive image generation. IGTR adds reasoning prompts without modifying\nthe model structure or raster generation order. Specifically, we design\nspecialized image-related reasoning prompts for AR image generation to simulate\nthe human reasoning process, which enhances contextual reasoning by allowing\nthe model to first perceive overall distribution information before generating\nthe image, and improve generation stability by increasing the inference steps.\nCompared to the AR method without prompts, our method shows outstanding\nperformance and achieves an approximate improvement of 20%."
                },
                "authors": [
                    {
                        "name": "Miaomiao Cai"
                    },
                    {
                        "name": "Guanjie Wang"
                    },
                    {
                        "name": "Wei Li"
                    },
                    {
                        "name": "Zhijun Tu"
                    },
                    {
                        "name": "Hanting Chen"
                    },
                    {
                        "name": "Shaohui Lin"
                    },
                    {
                        "name": "Jie Hu"
                    }
                ],
                "author_detail": {
                    "name": "Jie Hu"
                },
                "author": "Jie Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.16965v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.16965v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03187v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03187v2",
                "updated": "2025-02-26T11:10:48Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    11,
                    10,
                    48,
                    2,
                    57,
                    0
                ],
                "published": "2024-12-04T10:15:12Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    10,
                    15,
                    12,
                    2,
                    339,
                    0
                ],
                "title": "Weighted-Reward Preference Optimization for Implicit Model Fusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Weighted-Reward Preference Optimization for Implicit Model Fusion"
                },
                "summary": "While fusing heterogeneous open-source LLMs with varying architectures and\nsizes can potentially integrate the strengths of different models, existing\nfusion methods face significant challenges, such as vocabulary alignment and\nmerging distribution matrices. These procedures are not only complex but also\nprone to introducing noise and errors. In this paper, we propose an implicit\nfusion method, Weighted-Reward Preference Optimization (WRPO), which leverages\npreference optimization between the source LLMs and the target LLM to transfer\ntheir capabilities effectively. WRPO eliminates the need for vocabulary\nalignment and matrix fusion and can be efficiently scaled to accommodate\nvarious LLMs. To address distributional deviations between the source and\ntarget LLMs, WRPO introduces a progressive adaptation strategy that gradually\nshifts reliance on preferred examples from the target LLM to the source LLMs.\nExtensive experiments on the MT-Bench, AlpacaEval-2, and Arena-Hard benchmarks\ndemonstrate that WRPO consistently outperforms existing knowledge fusion\nmethods and various fine-tuning baselines. When applied to LLaMA3-8B-Instruct\nas the target model, WRPO achieves a length-controlled win rate of 55.9%\nagainst GPT-4-Preview-1106 on AlpacaEval-2 and a win rate of 46.2% against\nGPT-4-0314 on Arena-Hard. Our code is available at\nhttps://github.com/SLIT-AI/WRPO.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While fusing heterogeneous open-source LLMs with varying architectures and\nsizes can potentially integrate the strengths of different models, existing\nfusion methods face significant challenges, such as vocabulary alignment and\nmerging distribution matrices. These procedures are not only complex but also\nprone to introducing noise and errors. In this paper, we propose an implicit\nfusion method, Weighted-Reward Preference Optimization (WRPO), which leverages\npreference optimization between the source LLMs and the target LLM to transfer\ntheir capabilities effectively. WRPO eliminates the need for vocabulary\nalignment and matrix fusion and can be efficiently scaled to accommodate\nvarious LLMs. To address distributional deviations between the source and\ntarget LLMs, WRPO introduces a progressive adaptation strategy that gradually\nshifts reliance on preferred examples from the target LLM to the source LLMs.\nExtensive experiments on the MT-Bench, AlpacaEval-2, and Arena-Hard benchmarks\ndemonstrate that WRPO consistently outperforms existing knowledge fusion\nmethods and various fine-tuning baselines. When applied to LLaMA3-8B-Instruct\nas the target model, WRPO achieves a length-controlled win rate of 55.9%\nagainst GPT-4-Preview-1106 on AlpacaEval-2 and a win rate of 46.2% against\nGPT-4-0314 on Arena-Hard. Our code is available at\nhttps://github.com/SLIT-AI/WRPO."
                },
                "authors": [
                    {
                        "name": "Ziyi Yang"
                    },
                    {
                        "name": "Fanqi Wan"
                    },
                    {
                        "name": "Longguang Zhong"
                    },
                    {
                        "name": "Tianyuan Shi"
                    },
                    {
                        "name": "Xiaojun Quan"
                    }
                ],
                "author_detail": {
                    "name": "Xiaojun Quan"
                },
                "author": "Xiaojun Quan",
                "arxiv_comment": "Accepted to ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03187v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03187v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19041v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19041v1",
                "updated": "2025-02-26T10:53:58Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    10,
                    53,
                    58,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-26T10:53:58Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    10,
                    53,
                    58,
                    2,
                    57,
                    0
                ],
                "title": "Beyond Surface-Level Patterns: An Essence-Driven Defense Framework\n  Against Jailbreak Attacks in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Surface-Level Patterns: An Essence-Driven Defense Framework\n  Against Jailbreak Attacks in LLMs"
                },
                "summary": "Although Aligned Large Language Models (LLMs) are trained to refuse harmful\nrequests, they remain vulnerable to jailbreak attacks. Unfortunately, existing\nmethods often focus on surface-level patterns, overlooking the deeper attack\nessences. As a result, defenses fail when attack prompts change, even though\nthe underlying \"attack essence\" remains the same. To address this issue, we\nintroduce EDDF, an \\textbf{E}ssence-\\textbf{D}riven \\textbf{D}efense\n\\textbf{F}ramework Against Jailbreak Attacks in LLMs. EDDF is a plug-and-play\ninput-filtering method and operates in two stages: 1) offline essence database\nconstruction, and 2) online adversarial query detection. The key idea behind\nEDDF is to extract the \"attack essence\" from a diverse set of known attack\ninstances and store it in an offline vector database. Experimental results\ndemonstrate that EDDF significantly outperforms existing methods by reducing\nthe Attack Success Rate by at least 20\\%, underscoring its superior robustness\nagainst jailbreak attacks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although Aligned Large Language Models (LLMs) are trained to refuse harmful\nrequests, they remain vulnerable to jailbreak attacks. Unfortunately, existing\nmethods often focus on surface-level patterns, overlooking the deeper attack\nessences. As a result, defenses fail when attack prompts change, even though\nthe underlying \"attack essence\" remains the same. To address this issue, we\nintroduce EDDF, an \\textbf{E}ssence-\\textbf{D}riven \\textbf{D}efense\n\\textbf{F}ramework Against Jailbreak Attacks in LLMs. EDDF is a plug-and-play\ninput-filtering method and operates in two stages: 1) offline essence database\nconstruction, and 2) online adversarial query detection. The key idea behind\nEDDF is to extract the \"attack essence\" from a diverse set of known attack\ninstances and store it in an offline vector database. Experimental results\ndemonstrate that EDDF significantly outperforms existing methods by reducing\nthe Attack Success Rate by at least 20\\%, underscoring its superior robustness\nagainst jailbreak attacks."
                },
                "authors": [
                    {
                        "name": "Shiyu Xiang"
                    },
                    {
                        "name": "Ansen Zhang"
                    },
                    {
                        "name": "Yanfei Cao"
                    },
                    {
                        "name": "Yang Fan"
                    },
                    {
                        "name": "Ronghao Chen"
                    }
                ],
                "author_detail": {
                    "name": "Ronghao Chen"
                },
                "author": "Ronghao Chen",
                "arxiv_comment": "15 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19041v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19041v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19038v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19038v1",
                "updated": "2025-02-26T10:48:36Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    10,
                    48,
                    36,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-26T10:48:36Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    10,
                    48,
                    36,
                    2,
                    57,
                    0
                ],
                "title": "FungalZSL: Zero-Shot Fungal Classification with Image Captioning Using a\n  Synthetic Data Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FungalZSL: Zero-Shot Fungal Classification with Image Captioning Using a\n  Synthetic Data Approach"
                },
                "summary": "The effectiveness of zero-shot classification in large vision-language models\n(VLMs), such as Contrastive Language-Image Pre-training (CLIP), depends on\naccess to extensive, well-aligned text-image datasets. In this work, we\nintroduce two complementary data sources, one generated by large language\nmodels (LLMs) to describe the stages of fungal growth and another comprising a\ndiverse set of synthetic fungi images. These datasets are designed to enhance\nCLIPs zero-shot classification capabilities for fungi-related tasks. To ensure\neffective alignment between text and image data, we project them into CLIPs\nshared representation space, focusing on different fungal growth stages. We\ngenerate text using LLaMA3.2 to bridge modality gaps and synthetically create\nfungi images. Furthermore, we investigate knowledge transfer by comparing text\noutputs from different LLM techniques to refine classification across growth\nstages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The effectiveness of zero-shot classification in large vision-language models\n(VLMs), such as Contrastive Language-Image Pre-training (CLIP), depends on\naccess to extensive, well-aligned text-image datasets. In this work, we\nintroduce two complementary data sources, one generated by large language\nmodels (LLMs) to describe the stages of fungal growth and another comprising a\ndiverse set of synthetic fungi images. These datasets are designed to enhance\nCLIPs zero-shot classification capabilities for fungi-related tasks. To ensure\neffective alignment between text and image data, we project them into CLIPs\nshared representation space, focusing on different fungal growth stages. We\ngenerate text using LLaMA3.2 to bridge modality gaps and synthetically create\nfungi images. Furthermore, we investigate knowledge transfer by comparing text\noutputs from different LLM techniques to refine classification across growth\nstages."
                },
                "authors": [
                    {
                        "name": "Anju Rani"
                    },
                    {
                        "name": "Daniel O. Arroyo"
                    },
                    {
                        "name": "Petar Durdevic"
                    }
                ],
                "author_detail": {
                    "name": "Petar Durdevic"
                },
                "author": "Petar Durdevic",
                "arxiv_comment": "11 pages, 5 Figures, 1 Table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19038v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19038v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19024v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19024v1",
                "updated": "2025-02-26T10:30:40Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    10,
                    30,
                    40,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-26T10:30:40Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    10,
                    30,
                    40,
                    2,
                    57,
                    0
                ],
                "title": "Ground-level Viewpoint Vision-and-Language Navigation in Continuous\n  Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ground-level Viewpoint Vision-and-Language Navigation in Continuous\n  Environments"
                },
                "summary": "Vision-and-Language Navigation (VLN) empowers agents to associate\ntime-sequenced visual observations with corresponding instructions to make\nsequential decisions. However, generalization remains a persistent challenge,\nparticularly when dealing with visually diverse scenes or transitioning from\nsimulated environments to real-world deployment. In this paper, we address the\nmismatch between human-centric instructions and quadruped robots with a\nlow-height field of view, proposing a Ground-level Viewpoint Navigation (GVNav)\napproach to mitigate this issue. This work represents the first attempt to\nhighlight the generalization gap in VLN across varying heights of visual\nobservation in realistic robot deployments. Our approach leverages weighted\nhistorical observations as enriched spatiotemporal contexts for instruction\nfollowing, effectively managing feature collisions within cells by assigning\nappropriate weights to identical features across different viewpoints. This\nenables low-height robots to overcome challenges such as visual obstructions\nand perceptual mismatches. Additionally, we transfer the connectivity graph\nfrom the HM3D and Gibson datasets as an extra resource to enhance spatial\npriors and a more comprehensive representation of real-world scenarios, leading\nto improved performance and generalizability of the waypoint predictor in\nreal-world environments. Extensive experiments demonstrate that our\nGround-level Viewpoint Navigation (GVnav) approach significantly improves\nperformance in both simulated environments and real-world deployments with\nquadruped robots.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-and-Language Navigation (VLN) empowers agents to associate\ntime-sequenced visual observations with corresponding instructions to make\nsequential decisions. However, generalization remains a persistent challenge,\nparticularly when dealing with visually diverse scenes or transitioning from\nsimulated environments to real-world deployment. In this paper, we address the\nmismatch between human-centric instructions and quadruped robots with a\nlow-height field of view, proposing a Ground-level Viewpoint Navigation (GVNav)\napproach to mitigate this issue. This work represents the first attempt to\nhighlight the generalization gap in VLN across varying heights of visual\nobservation in realistic robot deployments. Our approach leverages weighted\nhistorical observations as enriched spatiotemporal contexts for instruction\nfollowing, effectively managing feature collisions within cells by assigning\nappropriate weights to identical features across different viewpoints. This\nenables low-height robots to overcome challenges such as visual obstructions\nand perceptual mismatches. Additionally, we transfer the connectivity graph\nfrom the HM3D and Gibson datasets as an extra resource to enhance spatial\npriors and a more comprehensive representation of real-world scenarios, leading\nto improved performance and generalizability of the waypoint predictor in\nreal-world environments. Extensive experiments demonstrate that our\nGround-level Viewpoint Navigation (GVnav) approach significantly improves\nperformance in both simulated environments and real-world deployments with\nquadruped robots."
                },
                "authors": [
                    {
                        "name": "Zerui Li"
                    },
                    {
                        "name": "Gengze Zhou"
                    },
                    {
                        "name": "Haodong Hong"
                    },
                    {
                        "name": "Yanyan Shao"
                    },
                    {
                        "name": "Wenqi Lyu"
                    },
                    {
                        "name": "Yanyuan Qiao"
                    },
                    {
                        "name": "Qi Wu"
                    }
                ],
                "author_detail": {
                    "name": "Qi Wu"
                },
                "author": "Qi Wu",
                "arxiv_comment": "Accepted by ICRA 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19024v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19024v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19008v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19008v1",
                "updated": "2025-02-26T10:14:19Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    10,
                    14,
                    19,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-26T10:14:19Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    10,
                    14,
                    19,
                    2,
                    57,
                    0
                ],
                "title": "Binary Neural Networks for Large Language Model: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Binary Neural Networks for Large Language Model: A Survey"
                },
                "summary": "Large language models (LLMs) have wide applications in the field of natural\nlanguage processing(NLP), such as GPT-4 and Llama. However, with the\nexponential growth of model parameter sizes, LLMs bring significant resource\noverheads. Low-bit quantization, as a key technique, reduces memory usage and\ncomputational demands by decreasing the bit-width of model parameters,\nactivations, and gradients. Previous quantization methods for LLMs have largely\nemployed Post-Training Quantization (PTQ) and Quantization-Aware Training\n(QAT). PTQ does not require any retraining of the original model, while QAT\ninvolves optimizing precision during training to achieve the best quantization\nparameters. The BitNet team proposed a radically different approach, where\nquantization is performed from the start of model training, utilizing\nlow-precision binary weights during the training process. This approach has led\nto the emergence of many binary quantization techniques for large language\nmodels. This paper provides a comprehensive review of these binary quantization\ntechniques. Specifically, we will introduce binary quantization techniques in\ndeep neural networks and further explore their application to LLMs, reviewing\ntheir various contributions, implementations, and applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have wide applications in the field of natural\nlanguage processing(NLP), such as GPT-4 and Llama. However, with the\nexponential growth of model parameter sizes, LLMs bring significant resource\noverheads. Low-bit quantization, as a key technique, reduces memory usage and\ncomputational demands by decreasing the bit-width of model parameters,\nactivations, and gradients. Previous quantization methods for LLMs have largely\nemployed Post-Training Quantization (PTQ) and Quantization-Aware Training\n(QAT). PTQ does not require any retraining of the original model, while QAT\ninvolves optimizing precision during training to achieve the best quantization\nparameters. The BitNet team proposed a radically different approach, where\nquantization is performed from the start of model training, utilizing\nlow-precision binary weights during the training process. This approach has led\nto the emergence of many binary quantization techniques for large language\nmodels. This paper provides a comprehensive review of these binary quantization\ntechniques. Specifically, we will introduce binary quantization techniques in\ndeep neural networks and further explore their application to LLMs, reviewing\ntheir various contributions, implementations, and applications."
                },
                "authors": [
                    {
                        "name": "Liangdong Liu"
                    },
                    {
                        "name": "Zhitong Zheng"
                    },
                    {
                        "name": "Cong Wang"
                    },
                    {
                        "name": "Tianhuang Su"
                    },
                    {
                        "name": "Zhenyu Yang"
                    }
                ],
                "author_detail": {
                    "name": "Zhenyu Yang"
                },
                "author": "Zhenyu Yang",
                "arxiv_comment": "23 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19008v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19008v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19002v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19002v1",
                "updated": "2025-02-26T10:06:37Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    10,
                    6,
                    37,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-26T10:06:37Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    10,
                    6,
                    37,
                    2,
                    57,
                    0
                ],
                "title": "The Sharpness Disparity Principle in Transformers for Accelerating\n  Language Model Pre-Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Sharpness Disparity Principle in Transformers for Accelerating\n  Language Model Pre-Training"
                },
                "summary": "Transformers consist of diverse building blocks, such as embedding layers,\nnormalization layers, self-attention mechanisms, and point-wise feedforward\nnetworks. Thus, understanding the differences and interactions among these\nblocks is important. In this paper, we uncover a clear Sharpness Disparity\nacross these blocks, which emerges early in training and intriguingly persists\nthroughout the training process. Motivated by this finding, we propose\nBlockwise Learning Rate (LR), a strategy that tailors the LR to each block's\nsharpness, accelerating large language model (LLM) pre-training. By integrating\nBlockwise LR into AdamW, we consistently achieve lower terminal loss and nearly\n$2\\times$ speedup compared to vanilla AdamW. We demonstrate this acceleration\nacross GPT-2 and LLaMA, with model sizes ranging from 0.12B to 1.1B and\ndatasets of OpenWebText and MiniPile. Finally, we incorporate Blockwise LR into\nAdam-mini (Zhang et al., 2024), a recently proposed memory-efficient variant of\nAdam, achieving a combined $2\\times$ speedup and $2\\times$ memory saving. These\nresults underscore the potential of exploiting the sharpness disparity to\nimprove LLM training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers consist of diverse building blocks, such as embedding layers,\nnormalization layers, self-attention mechanisms, and point-wise feedforward\nnetworks. Thus, understanding the differences and interactions among these\nblocks is important. In this paper, we uncover a clear Sharpness Disparity\nacross these blocks, which emerges early in training and intriguingly persists\nthroughout the training process. Motivated by this finding, we propose\nBlockwise Learning Rate (LR), a strategy that tailors the LR to each block's\nsharpness, accelerating large language model (LLM) pre-training. By integrating\nBlockwise LR into AdamW, we consistently achieve lower terminal loss and nearly\n$2\\times$ speedup compared to vanilla AdamW. We demonstrate this acceleration\nacross GPT-2 and LLaMA, with model sizes ranging from 0.12B to 1.1B and\ndatasets of OpenWebText and MiniPile. Finally, we incorporate Blockwise LR into\nAdam-mini (Zhang et al., 2024), a recently proposed memory-efficient variant of\nAdam, achieving a combined $2\\times$ speedup and $2\\times$ memory saving. These\nresults underscore the potential of exploiting the sharpness disparity to\nimprove LLM training."
                },
                "authors": [
                    {
                        "name": "Jinbo Wang"
                    },
                    {
                        "name": "Mingze Wang"
                    },
                    {
                        "name": "Zhanpeng Zhou"
                    },
                    {
                        "name": "Junchi Yan"
                    },
                    {
                        "name": "Weinan E"
                    },
                    {
                        "name": "Lei Wu"
                    }
                ],
                "author_detail": {
                    "name": "Lei Wu"
                },
                "author": "Lei Wu",
                "arxiv_comment": "23 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19002v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19002v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18996v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18996v1",
                "updated": "2025-02-26T10:01:28Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    10,
                    1,
                    28,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-26T10:01:28Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    10,
                    1,
                    28,
                    2,
                    57,
                    0
                ],
                "title": "Sequential Entanglement-Swapping assisted by Quantum Protocol over\n  Ethernet Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequential Entanglement-Swapping assisted by Quantum Protocol over\n  Ethernet Networks"
                },
                "summary": "The integration of quantum communication protocols over Ethernet networks is\nproposed, showing the potential of combining classical and quantum technologies\nfor efficient, scalable quantum networking. By leveraging the inherent\nstrengths of Ethernet, such as addressing, MAC layer functionality, and\nscalability; we propose a practical framework to support the rigorous\nrequirements of quantum communication. Some novel protocols given in this study\nenable reliable end-to-end quantum entanglement over Ethernet, ensuring the\nadaptability needed for implementing a stable quantum internet. Detailed\ntime-delay analyses confirm that our protocols offer superior performance\ncompared to existing methods, with total time delay kept within the decoherence\nthreshold of qubits. These results suggest that our approach is well-suited for\ndeployment in realistic environments, meeting both the immediate needs of\nquantum networking and laying the groundwork for future advances in data\nexchange and quantum computational capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of quantum communication protocols over Ethernet networks is\nproposed, showing the potential of combining classical and quantum technologies\nfor efficient, scalable quantum networking. By leveraging the inherent\nstrengths of Ethernet, such as addressing, MAC layer functionality, and\nscalability; we propose a practical framework to support the rigorous\nrequirements of quantum communication. Some novel protocols given in this study\nenable reliable end-to-end quantum entanglement over Ethernet, ensuring the\nadaptability needed for implementing a stable quantum internet. Detailed\ntime-delay analyses confirm that our protocols offer superior performance\ncompared to existing methods, with total time delay kept within the decoherence\nthreshold of qubits. These results suggest that our approach is well-suited for\ndeployment in realistic environments, meeting both the immediate needs of\nquantum networking and laying the groundwork for future advances in data\nexchange and quantum computational capabilities."
                },
                "authors": [
                    {
                        "name": "Kun Chen-Hu"
                    },
                    {
                        "name": "Kristian S. Jensen"
                    },
                    {
                        "name": "Petar Popovski"
                    }
                ],
                "author_detail": {
                    "name": "Petar Popovski"
                },
                "author": "Petar Popovski",
                "arxiv_comment": "Accepted in QCNC 2025 Nara, Japan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18996v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18996v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18993v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18993v1",
                "updated": "2025-02-26T09:56:51Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    9,
                    56,
                    51,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-26T09:56:51Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    9,
                    56,
                    51,
                    2,
                    57,
                    0
                ],
                "title": "MEBench: Benchmarking Large Language Models for Cross-Document\n  Multi-Entity Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MEBench: Benchmarking Large Language Models for Cross-Document\n  Multi-Entity Question Answering"
                },
                "summary": "Multi-entity question answering (MEQA) represents significant challenges for\nlarge language models (LLM) and retrieval-augmented generation (RAG) systems,\nwhich frequently struggle to consolidate scattered information across diverse\ndocuments. While existing methods excel at single-document comprehension, they\noften struggle with cross-document aggregation, particularly when resolving\nentity-dense questions like \"What is the distribution of ACM Fellows among\nvarious fields of study?\", which require integrating entity-centric insights\nfrom heterogeneous sources (e.g., Wikipedia pages). To address this gap, we\nintroduce MEBench, a novel multi-document, multi-entity benchmark designed to\nsystematically evaluate LLMs' capacity to retrieve, consolidate, and reason\nover fragmented information. Our benchmark comprises 4,780 questions which are\nsystematically categorized into three primary categories, further divided into\neight distinct types, ensuring broad coverage of real-world multi-entity\nreasoning scenarios. Our experiments on state-of-the-art LLMs (e.g., GPT-4,\nLlama-3) and RAG pipelines reveal critical limitations: even advanced models\nachieve only 59% accuracy on MEBench. Our benchmark emphasizes the importance\nof completeness and factual precision of information extraction in MEQA tasks,\nusing Entity-Attributed F1 (EA-F1) metric for granular evaluation of\nentity-level correctness and attribution validity. MEBench not only highlights\nsystemic weaknesses in current LLM frameworks but also provides a foundation\nfor advancing robust, entity-aware QA architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-entity question answering (MEQA) represents significant challenges for\nlarge language models (LLM) and retrieval-augmented generation (RAG) systems,\nwhich frequently struggle to consolidate scattered information across diverse\ndocuments. While existing methods excel at single-document comprehension, they\noften struggle with cross-document aggregation, particularly when resolving\nentity-dense questions like \"What is the distribution of ACM Fellows among\nvarious fields of study?\", which require integrating entity-centric insights\nfrom heterogeneous sources (e.g., Wikipedia pages). To address this gap, we\nintroduce MEBench, a novel multi-document, multi-entity benchmark designed to\nsystematically evaluate LLMs' capacity to retrieve, consolidate, and reason\nover fragmented information. Our benchmark comprises 4,780 questions which are\nsystematically categorized into three primary categories, further divided into\neight distinct types, ensuring broad coverage of real-world multi-entity\nreasoning scenarios. Our experiments on state-of-the-art LLMs (e.g., GPT-4,\nLlama-3) and RAG pipelines reveal critical limitations: even advanced models\nachieve only 59% accuracy on MEBench. Our benchmark emphasizes the importance\nof completeness and factual precision of information extraction in MEQA tasks,\nusing Entity-Attributed F1 (EA-F1) metric for granular evaluation of\nentity-level correctness and attribution validity. MEBench not only highlights\nsystemic weaknesses in current LLM frameworks but also provides a foundation\nfor advancing robust, entity-aware QA architectures."
                },
                "authors": [
                    {
                        "name": "Teng Lin"
                    }
                ],
                "author_detail": {
                    "name": "Teng Lin"
                },
                "author": "Teng Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18993v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18993v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18992v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18992v1",
                "updated": "2025-02-26T09:56:10Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    9,
                    56,
                    10,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-26T09:56:10Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    9,
                    56,
                    10,
                    2,
                    57,
                    0
                ],
                "title": "OntologyRAG: Better and Faster Biomedical Code Mapping with\n  Retrieval-Augmented Generation (RAG) Leveraging Ontology Knowledge Graphs and\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OntologyRAG: Better and Faster Biomedical Code Mapping with\n  Retrieval-Augmented Generation (RAG) Leveraging Ontology Knowledge Graphs and\n  Large Language Models"
                },
                "summary": "Biomedical ontologies, which comprehensively define concepts and relations\nfor biomedical entities, are crucial for structuring and formalizing\ndomain-specific information representations. Biomedical code mapping identifies\nsimilarity or equivalence between concepts from different ontologies. Obtaining\nhigh-quality mapping usually relies on automatic generation of unrefined\nmapping with ontology domain fine-tuned language models (LMs), followed by\nmanual selections or corrections by coding experts who have extensive domain\nexpertise and familiarity with ontology schemas. The LMs usually provide\nunrefined code mapping suggestions as a list of candidates without reasoning or\nsupporting evidence, hence coding experts still need to verify each suggested\ncandidate against ontology sources to pick the best matches. This is also a\nrecurring task as ontology sources are updated regularly to incorporate new\nresearch findings. Consequently, the need of regular LM retraining and manual\nrefinement make code mapping time-consuming and labour intensive. In this work,\nwe created OntologyRAG, an ontology-enhanced retrieval-augmented generation\n(RAG) method that leverages the inductive biases from ontological knowledge\ngraphs for in-context-learning (ICL) in large language models (LLMs). Our\nsolution grounds LLMs to knowledge graphs with unrefined mappings between\nontologies and processes questions by generating an interpretable set of\nresults that include prediction rational with mapping proximity assessment. Our\nsolution doesn't require re-training LMs, as all ontology updates could be\nreflected by updating the knowledge graphs with a standard process. Evaluation\nresults on a self-curated gold dataset show promises of using our method to\nenable coding experts to achieve better and faster code mapping. The code is\navailable at https://github.com/iqvianlp/ontologyRAG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Biomedical ontologies, which comprehensively define concepts and relations\nfor biomedical entities, are crucial for structuring and formalizing\ndomain-specific information representations. Biomedical code mapping identifies\nsimilarity or equivalence between concepts from different ontologies. Obtaining\nhigh-quality mapping usually relies on automatic generation of unrefined\nmapping with ontology domain fine-tuned language models (LMs), followed by\nmanual selections or corrections by coding experts who have extensive domain\nexpertise and familiarity with ontology schemas. The LMs usually provide\nunrefined code mapping suggestions as a list of candidates without reasoning or\nsupporting evidence, hence coding experts still need to verify each suggested\ncandidate against ontology sources to pick the best matches. This is also a\nrecurring task as ontology sources are updated regularly to incorporate new\nresearch findings. Consequently, the need of regular LM retraining and manual\nrefinement make code mapping time-consuming and labour intensive. In this work,\nwe created OntologyRAG, an ontology-enhanced retrieval-augmented generation\n(RAG) method that leverages the inductive biases from ontological knowledge\ngraphs for in-context-learning (ICL) in large language models (LLMs). Our\nsolution grounds LLMs to knowledge graphs with unrefined mappings between\nontologies and processes questions by generating an interpretable set of\nresults that include prediction rational with mapping proximity assessment. Our\nsolution doesn't require re-training LMs, as all ontology updates could be\nreflected by updating the knowledge graphs with a standard process. Evaluation\nresults on a self-curated gold dataset show promises of using our method to\nenable coding experts to achieve better and faster code mapping. The code is\navailable at https://github.com/iqvianlp/ontologyRAG."
                },
                "authors": [
                    {
                        "name": "Hui Feng"
                    },
                    {
                        "name": "Yuntzu Yin"
                    },
                    {
                        "name": "Emiliano Reynares"
                    },
                    {
                        "name": "Jay Nanavati"
                    }
                ],
                "author_detail": {
                    "name": "Jay Nanavati"
                },
                "author": "Jay Nanavati",
                "arxiv_comment": "This paper has been accepted as a workshop paper for KEIR@ECIR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18992v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18992v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18990v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18990v1",
                "updated": "2025-02-26T09:54:33Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    9,
                    54,
                    33,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-26T09:54:33Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    9,
                    54,
                    33,
                    2,
                    57,
                    0
                ],
                "title": "GenTool: Enhancing Tool Generalization in Language Models through\n  Zero-to-One and Weak-to-Strong Simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GenTool: Enhancing Tool Generalization in Language Models through\n  Zero-to-One and Weak-to-Strong Simulation"
                },
                "summary": "Large Language Models (LLMs) can enhance their capabilities as AI assistants\nby integrating external tools, allowing them to access a wider range of\ninformation. While recent LLMs are typically fine-tuned with tool usage\nexamples during supervised fine-tuning (SFT), questions remain about their\nability to develop robust tool-usage skills and can effectively generalize to\nunseen queries and tools. In this work, we present GenTool, a novel training\nframework that prepares LLMs for diverse generalization challenges in tool\nutilization. Our approach addresses two fundamental dimensions critical for\nreal-world applications: Zero-to-One Generalization, enabling the model to\naddress queries initially lacking a suitable tool by adopting and utilizing one\nwhen it becomes available, and Weak-to-Strong Generalization, allowing models\nto leverage enhanced versions of existing tools to solve queries. To achieve\nthis, we develop synthetic training data simulating these two dimensions of\ntool usage and introduce a two-stage fine-tuning approach: optimizing tool\nranking, then refining tool selection. Through extensive experiments across\nfour generalization scenarios, we demonstrate that our method significantly\nenhances the tool-usage capabilities of LLMs ranging from 1B to 8B parameters,\nachieving performance that surpasses GPT-4o. Furthermore, our analysis also\nprovides valuable insights into the challenges LLMs encounter in tool\ngeneralization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) can enhance their capabilities as AI assistants\nby integrating external tools, allowing them to access a wider range of\ninformation. While recent LLMs are typically fine-tuned with tool usage\nexamples during supervised fine-tuning (SFT), questions remain about their\nability to develop robust tool-usage skills and can effectively generalize to\nunseen queries and tools. In this work, we present GenTool, a novel training\nframework that prepares LLMs for diverse generalization challenges in tool\nutilization. Our approach addresses two fundamental dimensions critical for\nreal-world applications: Zero-to-One Generalization, enabling the model to\naddress queries initially lacking a suitable tool by adopting and utilizing one\nwhen it becomes available, and Weak-to-Strong Generalization, allowing models\nto leverage enhanced versions of existing tools to solve queries. To achieve\nthis, we develop synthetic training data simulating these two dimensions of\ntool usage and introduce a two-stage fine-tuning approach: optimizing tool\nranking, then refining tool selection. Through extensive experiments across\nfour generalization scenarios, we demonstrate that our method significantly\nenhances the tool-usage capabilities of LLMs ranging from 1B to 8B parameters,\nachieving performance that surpasses GPT-4o. Furthermore, our analysis also\nprovides valuable insights into the challenges LLMs encounter in tool\ngeneralization."
                },
                "authors": [
                    {
                        "name": "Jie He"
                    },
                    {
                        "name": "Jennifer Neville"
                    },
                    {
                        "name": "Mengting Wan"
                    },
                    {
                        "name": "Longqi Yang"
                    },
                    {
                        "name": "Hui Liu"
                    },
                    {
                        "name": "Xiaofeng Xu"
                    },
                    {
                        "name": "Xia Song"
                    },
                    {
                        "name": "Jeff Z. Pan"
                    },
                    {
                        "name": "Pei Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Pei Zhou"
                },
                "author": "Pei Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18990v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18990v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12851v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12851v4",
                "updated": "2025-02-26T09:54:28Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    9,
                    54,
                    28,
                    2,
                    57,
                    0
                ],
                "published": "2025-01-22T12:59:08Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    12,
                    59,
                    8,
                    2,
                    22,
                    0
                ],
                "title": "ACEBench: Who Wins the Match Point in Tool Usage?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ACEBench: Who Wins the Match Point in Tool Usage?"
                },
                "summary": "Large Language Models (LLMs) have demonstrated significant potential in\ndecision-making and reasoning, particularly when integrated with various tools\nto effectively solve complex problems. However, existing benchmarks for\nevaluating LLMs' tool usage face several limitations: (1) limited evaluation\nscenarios, often lacking assessments in real multi-turn dialogue contexts; (2)\nnarrow evaluation dimensions, with insufficient detailed assessments of how\nLLMs use tools; and (3) reliance on LLMs or real API executions for evaluation,\nwhich introduces significant overhead. To address these challenges, we\nintroduce ACEBench, a comprehensive benchmark for assessing tool usage in LLMs.\nACEBench categorizes data into three primary types based on evaluation\nmethodology: Normal, Special, and Agent. \"Normal\" evaluates tool usage in basic\nscenarios; \"Special\" evaluates tool usage in situations with ambiguous or\nincomplete instructions; \"Agent\" evaluates tool usage through multi-agent\ninteractions to simulate real-world, multi-turn dialogues. We conducted\nextensive experiments using ACEBench, analyzing various LLMs in-depth and\nproviding a more granular examination of error causes across different data\ntypes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated significant potential in\ndecision-making and reasoning, particularly when integrated with various tools\nto effectively solve complex problems. However, existing benchmarks for\nevaluating LLMs' tool usage face several limitations: (1) limited evaluation\nscenarios, often lacking assessments in real multi-turn dialogue contexts; (2)\nnarrow evaluation dimensions, with insufficient detailed assessments of how\nLLMs use tools; and (3) reliance on LLMs or real API executions for evaluation,\nwhich introduces significant overhead. To address these challenges, we\nintroduce ACEBench, a comprehensive benchmark for assessing tool usage in LLMs.\nACEBench categorizes data into three primary types based on evaluation\nmethodology: Normal, Special, and Agent. \"Normal\" evaluates tool usage in basic\nscenarios; \"Special\" evaluates tool usage in situations with ambiguous or\nincomplete instructions; \"Agent\" evaluates tool usage through multi-agent\ninteractions to simulate real-world, multi-turn dialogues. We conducted\nextensive experiments using ACEBench, analyzing various LLMs in-depth and\nproviding a more granular examination of error causes across different data\ntypes."
                },
                "authors": [
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Xinlong Hao"
                    },
                    {
                        "name": "Weiwen Liu"
                    },
                    {
                        "name": "Xu Huang"
                    },
                    {
                        "name": "Xingshan Zeng"
                    },
                    {
                        "name": "Shuai Yu"
                    },
                    {
                        "name": "Dexun Li"
                    },
                    {
                        "name": "Shuai Wang"
                    },
                    {
                        "name": "Weinan Gan"
                    },
                    {
                        "name": "Yuefeng Huang"
                    },
                    {
                        "name": "Wulong Liu"
                    },
                    {
                        "name": "Xinzhi Wang"
                    },
                    {
                        "name": "Defu Lian"
                    },
                    {
                        "name": "Baoqun Yin"
                    },
                    {
                        "name": "Yasheng Wang"
                    },
                    {
                        "name": "Wu Liu"
                    }
                ],
                "author_detail": {
                    "name": "Wu Liu"
                },
                "author": "Wu Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12851v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12851v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18980v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18980v1",
                "updated": "2025-02-26T09:43:08Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    9,
                    43,
                    8,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-26T09:43:08Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    9,
                    43,
                    8,
                    2,
                    57,
                    0
                ],
                "title": "PEToolLLM: Towards Personalized Tool Learning in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PEToolLLM: Towards Personalized Tool Learning in Large Language Models"
                },
                "summary": "Tool learning has emerged as a promising direction by extending Large\nLanguage Models' (LLMs) capabilities with external tools. Existing tool\nlearning studies primarily focus on the general-purpose tool-use capability,\nwhich addresses explicit user requirements in instructions. However, they\noverlook the importance of personalized tool-use capability, leading to an\ninability to handle implicit user preferences. To address the limitation, we\nfirst formulate the task of personalized tool learning, which integrates user's\ninteraction history towards personalized tool usage. To fill the gap of missing\nbenchmarks, we construct PEToolBench, featuring diverse user preferences\nreflected in interaction history under three distinct personalized settings,\nand encompassing a wide range of tool-use scenarios. Moreover, we propose a\nframework PEToolLLaMA to adapt LLMs to the personalized tool learning task,\nwhich is trained through supervised fine-tuning and direct preference\noptimization. Extensive experiments on PEToolBench demonstrate the superiority\nof PEToolLLaMA over existing LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tool learning has emerged as a promising direction by extending Large\nLanguage Models' (LLMs) capabilities with external tools. Existing tool\nlearning studies primarily focus on the general-purpose tool-use capability,\nwhich addresses explicit user requirements in instructions. However, they\noverlook the importance of personalized tool-use capability, leading to an\ninability to handle implicit user preferences. To address the limitation, we\nfirst formulate the task of personalized tool learning, which integrates user's\ninteraction history towards personalized tool usage. To fill the gap of missing\nbenchmarks, we construct PEToolBench, featuring diverse user preferences\nreflected in interaction history under three distinct personalized settings,\nand encompassing a wide range of tool-use scenarios. Moreover, we propose a\nframework PEToolLLaMA to adapt LLMs to the personalized tool learning task,\nwhich is trained through supervised fine-tuning and direct preference\noptimization. Extensive experiments on PEToolBench demonstrate the superiority\nof PEToolLLaMA over existing LLMs."
                },
                "authors": [
                    {
                        "name": "Qiancheng Xu"
                    },
                    {
                        "name": "Yongqi Li"
                    },
                    {
                        "name": "Heming Xia"
                    },
                    {
                        "name": "Fan Liu"
                    },
                    {
                        "name": "Min Yang"
                    },
                    {
                        "name": "Wenjie Li"
                    }
                ],
                "author_detail": {
                    "name": "Wenjie Li"
                },
                "author": "Wenjie Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18980v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18980v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03382v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03382v2",
                "updated": "2025-02-26T09:31:58Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    9,
                    31,
                    58,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-05T17:18:55Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    17,
                    18,
                    55,
                    2,
                    36,
                    0
                ],
                "title": "High-Fidelity Simultaneous Speech-To-Speech Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-Fidelity Simultaneous Speech-To-Speech Translation"
                },
                "summary": "We introduce Hibiki, a decoder-only model for simultaneous speech\ntranslation. Hibiki leverages a multistream language model to synchronously\nprocess source and target speech, and jointly produces text and audio tokens to\nperform speech-to-text and speech-to-speech translation. We furthermore address\nthe fundamental challenge of simultaneous interpretation, which unlike its\nconsecutive counterpart, where one waits for the end of the source utterance to\nstart translating, adapts its flow to accumulate just enough context to produce\na correct translation in real-time, chunk by chunk. To do so, we introduce a\nweakly-supervised method that leverages the perplexity of an off-the-shelf text\ntranslation system to identify optimal delays on a per-word basis and create\naligned synthetic data. After supervised training, Hibiki performs adaptive,\nsimultaneous speech translation with vanilla temperature sampling. On a\nFrench-English simultaneous speech translation task, Hibiki demonstrates\nstate-of-the-art performance in translation quality, speaker fidelity and\nnaturalness. Moreover, the simplicity of its inference process makes it\ncompatible with batched translation and even real-time on-device deployment. We\nprovide examples as well as models and inference code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Hibiki, a decoder-only model for simultaneous speech\ntranslation. Hibiki leverages a multistream language model to synchronously\nprocess source and target speech, and jointly produces text and audio tokens to\nperform speech-to-text and speech-to-speech translation. We furthermore address\nthe fundamental challenge of simultaneous interpretation, which unlike its\nconsecutive counterpart, where one waits for the end of the source utterance to\nstart translating, adapts its flow to accumulate just enough context to produce\na correct translation in real-time, chunk by chunk. To do so, we introduce a\nweakly-supervised method that leverages the perplexity of an off-the-shelf text\ntranslation system to identify optimal delays on a per-word basis and create\naligned synthetic data. After supervised training, Hibiki performs adaptive,\nsimultaneous speech translation with vanilla temperature sampling. On a\nFrench-English simultaneous speech translation task, Hibiki demonstrates\nstate-of-the-art performance in translation quality, speaker fidelity and\nnaturalness. Moreover, the simplicity of its inference process makes it\ncompatible with batched translation and even real-time on-device deployment. We\nprovide examples as well as models and inference code."
                },
                "authors": [
                    {
                        "name": "Tom Labiausse"
                    },
                    {
                        "name": "Laurent Mazaré"
                    },
                    {
                        "name": "Edouard Grave"
                    },
                    {
                        "name": "Patrick Pérez"
                    },
                    {
                        "name": "Alexandre Défossez"
                    },
                    {
                        "name": "Neil Zeghidour"
                    }
                ],
                "author_detail": {
                    "name": "Neil Zeghidour"
                },
                "author": "Neil Zeghidour",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03382v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03382v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18968v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18968v1",
                "updated": "2025-02-26T09:26:54Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    9,
                    26,
                    54,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-26T09:26:54Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    9,
                    26,
                    54,
                    2,
                    57,
                    0
                ],
                "title": "Know You First and Be You Better: Modeling Human-Like User Simulators\n  via Implicit Profiles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Know You First and Be You Better: Modeling Human-Like User Simulators\n  via Implicit Profiles"
                },
                "summary": "User simulators are crucial for replicating human interactions with dialogue\nsystems, supporting both collaborative training and automatic evaluation,\nespecially for large language models (LLMs). However, existing simulators often\nrely solely on text utterances, missing implicit user traits such as\npersonality, speaking style, and goals. In contrast, persona-based methods lack\ngeneralizability, as they depend on predefined profiles of famous individuals\nor archetypes. To address these challenges, we propose User Simulator with\nimplicit Profiles (USP), a framework that infers implicit user profiles from\nhuman-machine conversations and uses them to generate more personalized and\nrealistic dialogues. We first develop an LLM-driven extractor with a\ncomprehensive profile schema. Then, we refine the simulation through\nconditional supervised fine-tuning and reinforcement learning with cycle\nconsistency, optimizing it at both the utterance and conversation levels.\nFinally, we adopt a diverse profile sampler to capture the distribution of\nreal-world user profiles. Experimental results demonstrate that USP outperforms\nstrong baselines in terms of authenticity and diversity while achieving\ncomparable performance in consistency. Furthermore, dynamic multi-turn\nevaluations based on USP strongly align with mainstream benchmarks,\ndemonstrating its effectiveness in real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "User simulators are crucial for replicating human interactions with dialogue\nsystems, supporting both collaborative training and automatic evaluation,\nespecially for large language models (LLMs). However, existing simulators often\nrely solely on text utterances, missing implicit user traits such as\npersonality, speaking style, and goals. In contrast, persona-based methods lack\ngeneralizability, as they depend on predefined profiles of famous individuals\nor archetypes. To address these challenges, we propose User Simulator with\nimplicit Profiles (USP), a framework that infers implicit user profiles from\nhuman-machine conversations and uses them to generate more personalized and\nrealistic dialogues. We first develop an LLM-driven extractor with a\ncomprehensive profile schema. Then, we refine the simulation through\nconditional supervised fine-tuning and reinforcement learning with cycle\nconsistency, optimizing it at both the utterance and conversation levels.\nFinally, we adopt a diverse profile sampler to capture the distribution of\nreal-world user profiles. Experimental results demonstrate that USP outperforms\nstrong baselines in terms of authenticity and diversity while achieving\ncomparable performance in consistency. Furthermore, dynamic multi-turn\nevaluations based on USP strongly align with mainstream benchmarks,\ndemonstrating its effectiveness in real-world applications."
                },
                "authors": [
                    {
                        "name": "Kuang Wang"
                    },
                    {
                        "name": "Xianfei Li"
                    },
                    {
                        "name": "Shenghao Yang"
                    },
                    {
                        "name": "Li Zhou"
                    },
                    {
                        "name": "Feng Jiang"
                    },
                    {
                        "name": "Haizhou Li"
                    }
                ],
                "author_detail": {
                    "name": "Haizhou Li"
                },
                "author": "Haizhou Li",
                "arxiv_comment": "9 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18968v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18968v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.02936v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.02936v2",
                "updated": "2025-02-26T09:17:32Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    9,
                    17,
                    32,
                    2,
                    57,
                    0
                ],
                "published": "2024-07-03T09:12:38Z",
                "published_parsed": [
                    2024,
                    7,
                    3,
                    9,
                    12,
                    38,
                    2,
                    185,
                    0
                ],
                "title": "GraCoRe: Benchmarking Graph Comprehension and Complex Reasoning in Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GraCoRe: Benchmarking Graph Comprehension and Complex Reasoning in Large\n  Language Models"
                },
                "summary": "Evaluating the graph comprehension and reasoning abilities of Large Language\nModels (LLMs) is challenging and often incomplete. Existing benchmarks focus\nprimarily on pure graph understanding, lacking a comprehensive evaluation\nacross all graph types and detailed capability definitions. This paper presents\nGraCoRe, a benchmark for systematically assessing LLMs' graph comprehension and\nreasoning. GraCoRe uses a three-tier hierarchical taxonomy to categorize and\ntest models on pure graph and heterogeneous graphs, subdividing capabilities\ninto 10 distinct areas tested through 19 tasks. Our benchmark includes 11\ndatasets with 5,140 graphs of varying complexity. We evaluate four\nclosed-source and eight open-source LLMs, conducting thorough analyses from\nboth ability and task perspectives. Key findings reveal that OpenAI o1 model\nhas amazing comprehension and reasoning capabilities, semantic enrichment\nenhances reasoning performance, node ordering impacts task success, and the\nability to process longer texts does not necessarily improve graph\ncomprehension or reasoning.GraCoRe is open-sourced at\nhttps://github.com/ZIKEYUAN/GraCoRe",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating the graph comprehension and reasoning abilities of Large Language\nModels (LLMs) is challenging and often incomplete. Existing benchmarks focus\nprimarily on pure graph understanding, lacking a comprehensive evaluation\nacross all graph types and detailed capability definitions. This paper presents\nGraCoRe, a benchmark for systematically assessing LLMs' graph comprehension and\nreasoning. GraCoRe uses a three-tier hierarchical taxonomy to categorize and\ntest models on pure graph and heterogeneous graphs, subdividing capabilities\ninto 10 distinct areas tested through 19 tasks. Our benchmark includes 11\ndatasets with 5,140 graphs of varying complexity. We evaluate four\nclosed-source and eight open-source LLMs, conducting thorough analyses from\nboth ability and task perspectives. Key findings reveal that OpenAI o1 model\nhas amazing comprehension and reasoning capabilities, semantic enrichment\nenhances reasoning performance, node ordering impacts task success, and the\nability to process longer texts does not necessarily improve graph\ncomprehension or reasoning.GraCoRe is open-sourced at\nhttps://github.com/ZIKEYUAN/GraCoRe"
                },
                "authors": [
                    {
                        "name": "Zike Yuan"
                    },
                    {
                        "name": "Ming Liu"
                    },
                    {
                        "name": "Hui Wang"
                    },
                    {
                        "name": "Bing Qin"
                    }
                ],
                "author_detail": {
                    "name": "Bing Qin"
                },
                "author": "Bing Qin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.02936v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.02936v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.00356v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.00356v2",
                "updated": "2025-02-26T09:17:30Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    9,
                    17,
                    30,
                    2,
                    57,
                    0
                ],
                "published": "2024-10-01T03:04:08Z",
                "published_parsed": [
                    2024,
                    10,
                    1,
                    3,
                    4,
                    8,
                    1,
                    275,
                    0
                ],
                "title": "A Digital Twin Framework for Physical-Virtual Integration in V2X-Enabled\n  Connected Vehicle Corridors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Digital Twin Framework for Physical-Virtual Integration in V2X-Enabled\n  Connected Vehicle Corridors"
                },
                "summary": "Transportation Cyber-Physical Systems (T-CPS) enhance safety and mobility by\nintegrating cyber and physical transportation systems. A key component of T-CPS\nis the Digital Twin (DT), a virtual representation that enables simulation,\nanalysis, and optimization through real-time data exchange and communication.\nAlthough existing studies have explored DTs for vehicles, communications,\npedestrians, and traffic, real-world validations and implementations of DTs\nthat encompass infrastructure, vehicles, signals, communications, and more\nremain limited due to several challenges. These include accessing real-world\nconnected infrastructure, integrating heterogeneous, multi-sourced data,\nensuring real-time data processing, and synchronizing the digital and physical\nsystems. To address these challenges, this study develops a traffic DT based on\na real-world connected vehicle corridor. Leveraging the Cellular\nVehicle-to-Everything (C-V2X) infrastructure in the corridor, along with\ncommunication, computing, and simulation technologies, the proposed DT\naccurately replicates physical vehicle behaviors, signal timing,\ncommunications, and traffic patterns within the virtual environment. Building\nupon the previous data pipeline, the digital system ensures robust\nsynchronization with the physical environment. Moreover, the DT's scalable and\nredundant architecture enhances data integrity, making it capable of supporting\nfuture large-scale C-V2X deployments. Furthermore, its ability to provide\nfeedback to the physical system is demonstrated through applications such as\nsignal timing adjustments, vehicle advisory messages, and incident\nnotifications. The proposed DT is a vital tool in T-CPS, enabling real-time\ntraffic monitoring, prediction, and optimization to enhance the reliability and\nsafety of transportation systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transportation Cyber-Physical Systems (T-CPS) enhance safety and mobility by\nintegrating cyber and physical transportation systems. A key component of T-CPS\nis the Digital Twin (DT), a virtual representation that enables simulation,\nanalysis, and optimization through real-time data exchange and communication.\nAlthough existing studies have explored DTs for vehicles, communications,\npedestrians, and traffic, real-world validations and implementations of DTs\nthat encompass infrastructure, vehicles, signals, communications, and more\nremain limited due to several challenges. These include accessing real-world\nconnected infrastructure, integrating heterogeneous, multi-sourced data,\nensuring real-time data processing, and synchronizing the digital and physical\nsystems. To address these challenges, this study develops a traffic DT based on\na real-world connected vehicle corridor. Leveraging the Cellular\nVehicle-to-Everything (C-V2X) infrastructure in the corridor, along with\ncommunication, computing, and simulation technologies, the proposed DT\naccurately replicates physical vehicle behaviors, signal timing,\ncommunications, and traffic patterns within the virtual environment. Building\nupon the previous data pipeline, the digital system ensures robust\nsynchronization with the physical environment. Moreover, the DT's scalable and\nredundant architecture enhances data integrity, making it capable of supporting\nfuture large-scale C-V2X deployments. Furthermore, its ability to provide\nfeedback to the physical system is demonstrated through applications such as\nsignal timing adjustments, vehicle advisory messages, and incident\nnotifications. The proposed DT is a vital tool in T-CPS, enabling real-time\ntraffic monitoring, prediction, and optimization to enhance the reliability and\nsafety of transportation systems."
                },
                "authors": [
                    {
                        "name": "Keshu Wu"
                    },
                    {
                        "name": "Pei Li"
                    },
                    {
                        "name": "Yang Cheng"
                    },
                    {
                        "name": "Steven T. Parker"
                    },
                    {
                        "name": "Bin Ran"
                    },
                    {
                        "name": "David A. Noyce"
                    },
                    {
                        "name": "Xinyue Ye"
                    }
                ],
                "author_detail": {
                    "name": "Xinyue Ye"
                },
                "author": "Xinyue Ye",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.00356v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.00356v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14182v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14182v2",
                "updated": "2025-02-26T09:17:27Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    9,
                    17,
                    27,
                    2,
                    57,
                    0
                ],
                "published": "2024-10-18T05:21:05Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    5,
                    21,
                    5,
                    4,
                    292,
                    0
                ],
                "title": "LabSafety Bench: Benchmarking LLMs on Safety Issues in Scientific Labs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LabSafety Bench: Benchmarking LLMs on Safety Issues in Scientific Labs"
                },
                "summary": "Artificial Intelligence (AI) is revolutionizing scientific research, yet its\ngrowing integration into laboratory environments presents critical safety\nchallenges. While large language models (LLMs) increasingly assist in tasks\nranging from procedural guidance to autonomous experiment orchestration, an\n\"illusion of understanding\" may lead researchers to overestimate their\nreliability. Such overreliance is especially hazardous in high-stakes\nlaboratory settings, where failures in hazard identification or risk assessment\ncan result in severe accidents. To address these concerns, we propose the\nLaboratory Safety Benchmark (LabSafety Bench), a comprehensive framework that\nevaluates LLMs and vision language models (VLMs) on their ability to identify\npotential hazards, assess risks, and predict the consequences of unsafe actions\nin lab environments. LabSafety Bench comprises 765 multiple-choice questions\naligned with US Occupational Safety and Health Administration (OSHA) protocols,\nalong with 520 realistic laboratory scenarios featuring dual evaluation tasks:\nthe Hazards Identification Test and the Consequence Identification Test, with\n4090 open-ended questions in total. Evaluations across eight proprietary\nmodels, seven open-weight LLMs, and four VLMs reveal that, despite advanced\nperformance on structured assessments, no model achieves the safety threshold\nrequired for reliable operation. None scored above 75% on the Hazards\nIdentification Test. Moreover, while proprietary models tend to excel in\nmultiple-choice evaluations, their performance in open-ended, real-world\nscenario responses is comparable to that of open-source models. These findings\nunderscore the urgent need for specialized evaluation frameworks to ensure the\nsafe and responsible deployment of AI in laboratory settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial Intelligence (AI) is revolutionizing scientific research, yet its\ngrowing integration into laboratory environments presents critical safety\nchallenges. While large language models (LLMs) increasingly assist in tasks\nranging from procedural guidance to autonomous experiment orchestration, an\n\"illusion of understanding\" may lead researchers to overestimate their\nreliability. Such overreliance is especially hazardous in high-stakes\nlaboratory settings, where failures in hazard identification or risk assessment\ncan result in severe accidents. To address these concerns, we propose the\nLaboratory Safety Benchmark (LabSafety Bench), a comprehensive framework that\nevaluates LLMs and vision language models (VLMs) on their ability to identify\npotential hazards, assess risks, and predict the consequences of unsafe actions\nin lab environments. LabSafety Bench comprises 765 multiple-choice questions\naligned with US Occupational Safety and Health Administration (OSHA) protocols,\nalong with 520 realistic laboratory scenarios featuring dual evaluation tasks:\nthe Hazards Identification Test and the Consequence Identification Test, with\n4090 open-ended questions in total. Evaluations across eight proprietary\nmodels, seven open-weight LLMs, and four VLMs reveal that, despite advanced\nperformance on structured assessments, no model achieves the safety threshold\nrequired for reliable operation. None scored above 75% on the Hazards\nIdentification Test. Moreover, while proprietary models tend to excel in\nmultiple-choice evaluations, their performance in open-ended, real-world\nscenario responses is comparable to that of open-source models. These findings\nunderscore the urgent need for specialized evaluation frameworks to ensure the\nsafe and responsible deployment of AI in laboratory settings."
                },
                "authors": [
                    {
                        "name": "Yujun Zhou"
                    },
                    {
                        "name": "Jingdong Yang"
                    },
                    {
                        "name": "Yue Huang"
                    },
                    {
                        "name": "Kehan Guo"
                    },
                    {
                        "name": "Zoe Emory"
                    },
                    {
                        "name": "Bikram Ghosh"
                    },
                    {
                        "name": "Amita Bedar"
                    },
                    {
                        "name": "Sujay Shekar"
                    },
                    {
                        "name": "Pin-Yu Chen"
                    },
                    {
                        "name": "Tian Gao"
                    },
                    {
                        "name": "Werner Geyer"
                    },
                    {
                        "name": "Nuno Moniz"
                    },
                    {
                        "name": "Nitesh V Chawla"
                    },
                    {
                        "name": "Xiangliang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xiangliang Zhang"
                },
                "author": "Xiangliang Zhang",
                "arxiv_comment": "71 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14182v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14182v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07413v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07413v3",
                "updated": "2025-02-26T09:13:06Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    9,
                    13,
                    6,
                    2,
                    57,
                    0
                ],
                "published": "2024-08-14T09:43:32Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    9,
                    43,
                    32,
                    2,
                    227,
                    0
                ],
                "title": "Knowledge in Superposition: Unveiling the Failures of Lifelong Knowledge\n  Editing for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge in Superposition: Unveiling the Failures of Lifelong Knowledge\n  Editing for Large Language Models"
                },
                "summary": "Knowledge editing aims to update outdated or incorrect knowledge in large\nlanguage models (LLMs). However, current knowledge editing methods have limited\nscalability for lifelong editing. This study explores the fundamental reason\nwhy knowledge editing fails in lifelong editing. We begin with the closed-form\nsolution derived from linear associative memory, which underpins\nstate-of-the-art knowledge editing methods. We extend the solution from single\nediting to lifelong editing, and through rigorous mathematical derivation,\nidentify an interference term in the final solution, suggesting that editing\nknowledge may impact irrelevant knowledge. Further analysis of the interference\nterm reveals a close relationship with superposition between knowledge\nrepresentations. When knowledge superposition does not exist in language\nmodels, the interference term vanishes, allowing for lossless knowledge\nediting. Experiments across numerous language models reveal that knowledge\nsuperposition is universal, exhibiting high kurtosis, zero mean, and\nheavy-tailed distributions with clear scaling laws. Ultimately, by combining\ntheory and experiments, we demonstrate that knowledge superposition is the\nfundamental reason for the failure of lifelong editing. Moreover, this is the\nfirst study to investigate knowledge editing from the perspective of\nsuperposition and provides a comprehensive observation of superposition across\nnumerous real-world language models. Code available at\nhttps://github.com/ChenhuiHu/knowledge_in_superposition.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge editing aims to update outdated or incorrect knowledge in large\nlanguage models (LLMs). However, current knowledge editing methods have limited\nscalability for lifelong editing. This study explores the fundamental reason\nwhy knowledge editing fails in lifelong editing. We begin with the closed-form\nsolution derived from linear associative memory, which underpins\nstate-of-the-art knowledge editing methods. We extend the solution from single\nediting to lifelong editing, and through rigorous mathematical derivation,\nidentify an interference term in the final solution, suggesting that editing\nknowledge may impact irrelevant knowledge. Further analysis of the interference\nterm reveals a close relationship with superposition between knowledge\nrepresentations. When knowledge superposition does not exist in language\nmodels, the interference term vanishes, allowing for lossless knowledge\nediting. Experiments across numerous language models reveal that knowledge\nsuperposition is universal, exhibiting high kurtosis, zero mean, and\nheavy-tailed distributions with clear scaling laws. Ultimately, by combining\ntheory and experiments, we demonstrate that knowledge superposition is the\nfundamental reason for the failure of lifelong editing. Moreover, this is the\nfirst study to investigate knowledge editing from the perspective of\nsuperposition and provides a comprehensive observation of superposition across\nnumerous real-world language models. Code available at\nhttps://github.com/ChenhuiHu/knowledge_in_superposition."
                },
                "authors": [
                    {
                        "name": "Chenhui Hu"
                    },
                    {
                        "name": "Pengfei Cao"
                    },
                    {
                        "name": "Yubo Chen"
                    },
                    {
                        "name": "Kang Liu"
                    },
                    {
                        "name": "Jun Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhao"
                },
                "author": "Jun Zhao",
                "arxiv_comment": "To be published in AAAI 2025 (Oral)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07413v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07413v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.08658v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.08658v3",
                "updated": "2025-02-26T08:57:38Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    8,
                    57,
                    38,
                    2,
                    57,
                    0
                ],
                "published": "2024-02-13T18:39:36Z",
                "published_parsed": [
                    2024,
                    2,
                    13,
                    18,
                    39,
                    36,
                    1,
                    44,
                    0
                ],
                "title": "The Last JITAI? Exploring Large Language Models for Issuing Just-in-Time\n  Adaptive Interventions: Fostering Physical Activity in a Conceptual Cardiac\n  Rehabilitation Setting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Last JITAI? Exploring Large Language Models for Issuing Just-in-Time\n  Adaptive Interventions: Fostering Physical Activity in a Conceptual Cardiac\n  Rehabilitation Setting"
                },
                "summary": "We evaluated the viability of using Large Language Models (LLMs) to trigger\nand personalize content in Just-in-Time Adaptive Interventions (JITAIs) in\ndigital health. As an interaction pattern representative of context-aware\ncomputing, JITAIs are being explored for their potential to support sustainable\nbehavior change, adapting interventions to an individual's current context and\nneeds. Challenging traditional JITAI implementation models, which face severe\nscalability and flexibility limitations, we tested GPT-4 for suggesting JITAIs\nin the use case of heart-healthy activity in cardiac rehabilitation. Using\nthree personas representing patients affected by CVD with varying severeness\nand five context sets per persona, we generated 450 JITAI decisions and\nmessages. These were systematically evaluated against those created by 10\nlaypersons (LayPs) and 10 healthcare professionals (HCPs). GPT-4-generated\nJITAIs surpassed human-generated intervention suggestions, outperforming both\nLayPs and HCPs across all metrics (i.e., appropriateness, engagement,\neffectiveness, and professionalism). These results highlight the potential of\nLLMs to enhance JITAI implementations in personalized health interventions,\ndemonstrating how generative AI could revolutionize context-aware computing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We evaluated the viability of using Large Language Models (LLMs) to trigger\nand personalize content in Just-in-Time Adaptive Interventions (JITAIs) in\ndigital health. As an interaction pattern representative of context-aware\ncomputing, JITAIs are being explored for their potential to support sustainable\nbehavior change, adapting interventions to an individual's current context and\nneeds. Challenging traditional JITAI implementation models, which face severe\nscalability and flexibility limitations, we tested GPT-4 for suggesting JITAIs\nin the use case of heart-healthy activity in cardiac rehabilitation. Using\nthree personas representing patients affected by CVD with varying severeness\nand five context sets per persona, we generated 450 JITAI decisions and\nmessages. These were systematically evaluated against those created by 10\nlaypersons (LayPs) and 10 healthcare professionals (HCPs). GPT-4-generated\nJITAIs surpassed human-generated intervention suggestions, outperforming both\nLayPs and HCPs across all metrics (i.e., appropriateness, engagement,\neffectiveness, and professionalism). These results highlight the potential of\nLLMs to enhance JITAI implementations in personalized health interventions,\ndemonstrating how generative AI could revolutionize context-aware computing."
                },
                "authors": [
                    {
                        "name": "David Haag"
                    },
                    {
                        "name": "Devender Kumar"
                    },
                    {
                        "name": "Sebastian Gruber"
                    },
                    {
                        "name": "Dominik Hofer"
                    },
                    {
                        "name": "Mahdi Sareban"
                    },
                    {
                        "name": "Gunnar Treff"
                    },
                    {
                        "name": "Josef Niebauer"
                    },
                    {
                        "name": "Christopher Bull"
                    },
                    {
                        "name": "Albrecht Schmidt"
                    },
                    {
                        "name": "Jan David Smeddinck"
                    }
                ],
                "author_detail": {
                    "name": "Jan David Smeddinck"
                },
                "author": "Jan David Smeddinck",
                "arxiv_doi": "10.1145/3706598.3713307",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3706598.3713307",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2402.08658v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.08658v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "J.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18943v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18943v1",
                "updated": "2025-02-26T08:47:19Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    8,
                    47,
                    19,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-26T08:47:19Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    8,
                    47,
                    19,
                    2,
                    57,
                    0
                ],
                "title": "Towards Label-Only Membership Inference Attack against Pre-trained Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Label-Only Membership Inference Attack against Pre-trained Large\n  Language Models"
                },
                "summary": "Membership Inference Attacks (MIAs) aim to predict whether a data sample\nbelongs to the model's training set or not. Although prior research has\nextensively explored MIAs in Large Language Models (LLMs), they typically\nrequire accessing to complete output logits (\\ie, \\textit{logits-based\nattacks}), which are usually not available in practice. In this paper, we study\nthe vulnerability of pre-trained LLMs to MIAs in the \\textit{label-only\nsetting}, where the adversary can only access generated tokens (text). We first\nreveal that existing label-only MIAs have minor effects in attacking\npre-trained LLMs, although they are highly effective in inferring fine-tuning\ndatasets used for personalized LLMs. We find that their failure stems from two\nmain reasons, including better generalization and overly coarse perturbation.\nSpecifically, due to the extensive pre-training corpora and exposing each\nsample only a few times, LLMs exhibit minimal robustness differences between\nmembers and non-members. This makes token-level perturbations too coarse to\ncapture such differences.\n  To alleviate these problems, we propose \\textbf{PETAL}: a label-only\nmembership inference attack based on \\textbf{PE}r-\\textbf{T}oken\nsem\\textbf{A}ntic simi\\textbf{L}arity. Specifically, PETAL leverages\ntoken-level semantic similarity to approximate output probabilities and\nsubsequently calculate the perplexity. It finally exposes membership based on\nthe common assumption that members are `better' memorized and have smaller\nperplexity. We conduct extensive experiments on the WikiMIA benchmark and the\nmore challenging MIMIR benchmark. Empirically, our PETAL performs better than\nthe extensions of existing label-only attacks against personalized LLMs and\neven on par with other advanced logit-based attacks across all metrics on five\nprevalent open-source LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Membership Inference Attacks (MIAs) aim to predict whether a data sample\nbelongs to the model's training set or not. Although prior research has\nextensively explored MIAs in Large Language Models (LLMs), they typically\nrequire accessing to complete output logits (\\ie, \\textit{logits-based\nattacks}), which are usually not available in practice. In this paper, we study\nthe vulnerability of pre-trained LLMs to MIAs in the \\textit{label-only\nsetting}, where the adversary can only access generated tokens (text). We first\nreveal that existing label-only MIAs have minor effects in attacking\npre-trained LLMs, although they are highly effective in inferring fine-tuning\ndatasets used for personalized LLMs. We find that their failure stems from two\nmain reasons, including better generalization and overly coarse perturbation.\nSpecifically, due to the extensive pre-training corpora and exposing each\nsample only a few times, LLMs exhibit minimal robustness differences between\nmembers and non-members. This makes token-level perturbations too coarse to\ncapture such differences.\n  To alleviate these problems, we propose \\textbf{PETAL}: a label-only\nmembership inference attack based on \\textbf{PE}r-\\textbf{T}oken\nsem\\textbf{A}ntic simi\\textbf{L}arity. Specifically, PETAL leverages\ntoken-level semantic similarity to approximate output probabilities and\nsubsequently calculate the perplexity. It finally exposes membership based on\nthe common assumption that members are `better' memorized and have smaller\nperplexity. We conduct extensive experiments on the WikiMIA benchmark and the\nmore challenging MIMIR benchmark. Empirically, our PETAL performs better than\nthe extensions of existing label-only attacks against personalized LLMs and\neven on par with other advanced logit-based attacks across all metrics on five\nprevalent open-source LLMs."
                },
                "authors": [
                    {
                        "name": "Yu He"
                    },
                    {
                        "name": "Boheng Li"
                    },
                    {
                        "name": "Liu Liu"
                    },
                    {
                        "name": "Zhongjie Ba"
                    },
                    {
                        "name": "Wei Dong"
                    },
                    {
                        "name": "Yiming Li"
                    },
                    {
                        "name": "Zhan Qin"
                    },
                    {
                        "name": "Kui Ren"
                    },
                    {
                        "name": "Chun Chen"
                    }
                ],
                "author_detail": {
                    "name": "Chun Chen"
                },
                "author": "Chun Chen",
                "arxiv_comment": "Accepted by USENIX Security 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18943v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18943v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18940v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18940v1",
                "updated": "2025-02-26T08:43:47Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    8,
                    43,
                    47,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-26T08:43:47Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    8,
                    43,
                    47,
                    2,
                    57,
                    0
                ],
                "title": "MathTutorBench: A Benchmark for Measuring Open-ended Pedagogical\n  Capabilities of LLM Tutors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MathTutorBench: A Benchmark for Measuring Open-ended Pedagogical\n  Capabilities of LLM Tutors"
                },
                "summary": "Evaluating the pedagogical capabilities of AI-based tutoring models is\ncritical for making guided progress in the field. Yet, we lack a reliable,\neasy-to-use, and simple-to-run evaluation that reflects the pedagogical\nabilities of models. To fill this gap, we present MathTutorBench, an\nopen-source benchmark for holistic tutoring model evaluation. MathTutorBench\ncontains a collection of datasets and metrics that broadly cover tutor\nabilities as defined by learning sciences research in dialog-based teaching. To\nscore the pedagogical quality of open-ended teacher responses, we train a\nreward model and show it can discriminate expert from novice teacher responses\nwith high accuracy. We evaluate a wide set of closed- and open-weight models on\nMathTutorBench and find that subject expertise, indicated by solving ability,\ndoes not immediately translate to good teaching. Rather, pedagogy and subject\nexpertise appear to form a trade-off that is navigated by the degree of\ntutoring specialization of the model. Furthermore, tutoring appears to become\nmore challenging in longer dialogs, where simpler questioning strategies begin\nto fail. We release the benchmark, code, and leaderboard openly to enable rapid\nbenchmarking of future models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating the pedagogical capabilities of AI-based tutoring models is\ncritical for making guided progress in the field. Yet, we lack a reliable,\neasy-to-use, and simple-to-run evaluation that reflects the pedagogical\nabilities of models. To fill this gap, we present MathTutorBench, an\nopen-source benchmark for holistic tutoring model evaluation. MathTutorBench\ncontains a collection of datasets and metrics that broadly cover tutor\nabilities as defined by learning sciences research in dialog-based teaching. To\nscore the pedagogical quality of open-ended teacher responses, we train a\nreward model and show it can discriminate expert from novice teacher responses\nwith high accuracy. We evaluate a wide set of closed- and open-weight models on\nMathTutorBench and find that subject expertise, indicated by solving ability,\ndoes not immediately translate to good teaching. Rather, pedagogy and subject\nexpertise appear to form a trade-off that is navigated by the degree of\ntutoring specialization of the model. Furthermore, tutoring appears to become\nmore challenging in longer dialogs, where simpler questioning strategies begin\nto fail. We release the benchmark, code, and leaderboard openly to enable rapid\nbenchmarking of future models."
                },
                "authors": [
                    {
                        "name": "Jakub Macina"
                    },
                    {
                        "name": "Nico Daheim"
                    },
                    {
                        "name": "Ido Hakimi"
                    },
                    {
                        "name": "Manu Kapur"
                    },
                    {
                        "name": "Iryna Gurevych"
                    },
                    {
                        "name": "Mrinmaya Sachan"
                    }
                ],
                "author_detail": {
                    "name": "Mrinmaya Sachan"
                },
                "author": "Mrinmaya Sachan",
                "arxiv_comment": "https://eth-lre.github.io/mathtutorbench",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18940v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18940v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18935v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18935v1",
                "updated": "2025-02-26T08:36:42Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    8,
                    36,
                    42,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-26T08:36:42Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    8,
                    36,
                    42,
                    2,
                    57,
                    0
                ],
                "title": "JailBench: A Comprehensive Chinese Security Assessment Benchmark for\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JailBench: A Comprehensive Chinese Security Assessment Benchmark for\n  Large Language Models"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable capabilities across\nvarious applications, highlighting the urgent need for comprehensive safety\nevaluations. In particular, the enhanced Chinese language proficiency of LLMs,\ncombined with the unique characteristics and complexity of Chinese expressions,\nhas driven the emergence of Chinese-specific benchmarks for safety assessment.\nHowever, these benchmarks generally fall short in effectively exposing LLM\nsafety vulnerabilities. To address the gap, we introduce JailBench, the first\ncomprehensive Chinese benchmark for evaluating deep-seated vulnerabilities in\nLLMs, featuring a refined hierarchical safety taxonomy tailored to the Chinese\ncontext. To improve generation efficiency, we employ a novel Automatic\nJailbreak Prompt Engineer (AJPE) framework for JailBench construction, which\nincorporates jailbreak techniques to enhance assessing effectiveness and\nleverages LLMs to automatically scale up the dataset through context-learning.\nThe proposed JailBench is extensively evaluated over 13 mainstream LLMs and\nachieves the highest attack success rate against ChatGPT compared to existing\nChinese benchmarks, underscoring its efficacy in identifying latent\nvulnerabilities in LLMs, as well as illustrating the substantial room for\nimprovement in the security and trustworthiness of LLMs within the Chinese\ncontext. Our benchmark is publicly available at\nhttps://github.com/STAIR-BUPT/JailBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable capabilities across\nvarious applications, highlighting the urgent need for comprehensive safety\nevaluations. In particular, the enhanced Chinese language proficiency of LLMs,\ncombined with the unique characteristics and complexity of Chinese expressions,\nhas driven the emergence of Chinese-specific benchmarks for safety assessment.\nHowever, these benchmarks generally fall short in effectively exposing LLM\nsafety vulnerabilities. To address the gap, we introduce JailBench, the first\ncomprehensive Chinese benchmark for evaluating deep-seated vulnerabilities in\nLLMs, featuring a refined hierarchical safety taxonomy tailored to the Chinese\ncontext. To improve generation efficiency, we employ a novel Automatic\nJailbreak Prompt Engineer (AJPE) framework for JailBench construction, which\nincorporates jailbreak techniques to enhance assessing effectiveness and\nleverages LLMs to automatically scale up the dataset through context-learning.\nThe proposed JailBench is extensively evaluated over 13 mainstream LLMs and\nachieves the highest attack success rate against ChatGPT compared to existing\nChinese benchmarks, underscoring its efficacy in identifying latent\nvulnerabilities in LLMs, as well as illustrating the substantial room for\nimprovement in the security and trustworthiness of LLMs within the Chinese\ncontext. Our benchmark is publicly available at\nhttps://github.com/STAIR-BUPT/JailBench."
                },
                "authors": [
                    {
                        "name": "Shuyi Liu"
                    },
                    {
                        "name": "Simiao Cui"
                    },
                    {
                        "name": "Haoran Bu"
                    },
                    {
                        "name": "Yuming Shang"
                    },
                    {
                        "name": "Xi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xi Zhang"
                },
                "author": "Xi Zhang",
                "arxiv_comment": "12 pages, 5 figures, accepted at PAKDD 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18935v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18935v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]