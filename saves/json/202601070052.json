[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2601.02333v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.02333v1",
                "title": "High-repetition-rate terahertz and ultraviolet radiation for high-throughput ultrafast electron diffraction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-repetition-rate terahertz and ultraviolet radiation for high-throughput ultrafast electron diffraction"
                },
                "updated": "2026-01-05T18:25:44Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    18,
                    25,
                    44,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.02333v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.02333v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Scaling femtosecond terahertz (THz) and ultraviolet (UV) sources to high repetition rates is essential for high-throughput ultrafast spectroscopy and imaging applications. Yet, their efficient generation at high average power remains limited by thermal effects, phase-matching constraints, and material damage. Here, we demonstrate broadband THz and UV generation driven by a common Yb:KGW laser operating from at 40 - 600 kHz. THz radiation is produced by optical rectification in stoichiometric MgO:LiNbO$_3$ using a line-focus geometry, yielding single-cycle pulses of 55 - 92 nJ energy with peak electric fields of 37 - 90 kV/cm. Electro-optic sampling and beam-quality measurements reveal tunable control between central frequency, bandwidth and field amplitude by translating the generation region transversely within the crystal. Using shorter pump pulses preserves THz conversion efficiency, while longer pulses at 100 kHz reduce THz output by up to a factor of four due to cumulative thermal effects. Femtosecond 257.5 nm UV pulses are generated by cascaded fourth-harmonic generation in $β$-barium borate with conversion efficiencies exceeding 10% at 40 kHz and stable operation up to 600 kHz. These results demonstrate a compact, thermally robust platform for high-average-power nonlinear conversion and are directly relevant to next-generation high-repetition-rate ultrafast electron diffraction and spectroscopy systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling femtosecond terahertz (THz) and ultraviolet (UV) sources to high repetition rates is essential for high-throughput ultrafast spectroscopy and imaging applications. Yet, their efficient generation at high average power remains limited by thermal effects, phase-matching constraints, and material damage. Here, we demonstrate broadband THz and UV generation driven by a common Yb:KGW laser operating from at 40 - 600 kHz. THz radiation is produced by optical rectification in stoichiometric MgO:LiNbO$_3$ using a line-focus geometry, yielding single-cycle pulses of 55 - 92 nJ energy with peak electric fields of 37 - 90 kV/cm. Electro-optic sampling and beam-quality measurements reveal tunable control between central frequency, bandwidth and field amplitude by translating the generation region transversely within the crystal. Using shorter pump pulses preserves THz conversion efficiency, while longer pulses at 100 kHz reduce THz output by up to a factor of four due to cumulative thermal effects. Femtosecond 257.5 nm UV pulses are generated by cascaded fourth-harmonic generation in $β$-barium borate with conversion efficiencies exceeding 10% at 40 kHz and stable operation up to 600 kHz. These results demonstrate a compact, thermally robust platform for high-average-power nonlinear conversion and are directly relevant to next-generation high-repetition-rate ultrafast electron diffraction and spectroscopy systems."
                },
                "tags": [
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-05T18:25:44Z",
                "published_parsed": [
                    2026,
                    1,
                    5,
                    18,
                    25,
                    44,
                    0,
                    5,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "physics.optics"
                },
                "authors": [
                    {
                        "name": "Andrey Ryabov"
                    },
                    {
                        "name": "Kasra Amini"
                    }
                ],
                "author_detail": {
                    "name": "Kasra Amini"
                },
                "author": "Kasra Amini"
            },
            {
                "id": "http://arxiv.org/abs/2503.18773v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2503.18773v3",
                "title": "BitDecoding: Unlocking Tensor Cores for Long-Context LLMs with Low-Bit KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BitDecoding: Unlocking Tensor Cores for Long-Context LLMs with Low-Bit KV Cache"
                },
                "updated": "2026-01-05T18:08:27Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    18,
                    8,
                    27,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2503.18773v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2503.18773v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The growth of long-context Large Language Models (LLMs) significantly increases memory and bandwidth pressure during autoregressive decoding due to the expanding Key-Value (KV) cache. While accuracy-preserving KV-cache quantization (e.g., 4-bit or 2-bit) reduces memory footprint, existing systems decode inefficiently by relying solely on CUDA cores, underutilizing Tensor Cores-the dominant compute resource on GPUs.\n  We present BitDecoding, the first inference system to efficiently decode low-bit KV caches by cooperatively leveraging CUDA cores and Tensor Cores. BitDecoding smartly induces Tensor-Core-friendly layouts, introduces warp-level dequantization parallelism, and provides unified system support through query transformation, high-performance tensor- and channel-wise quantization, and a software-pipelined dequantization kernel enabling mixed-precision execution. Architecture-aware optimizations further leverage Hopper's warpgroup tensor instructions and Blackwell's NVFP4 (MXFP4) tensor formats.\n  Evaluated on Blackwell, Hopper, and Ampere GPUs, BitDecoding achieves an average 7.5x decoding speedup over FP16 FlashDecoding-v2, up to 8.6x on Blackwell with NVFP4, and up to 4.3x over state-of-the-art approaches. On LLaMA-3.1-8B with a 128K context, BitDecoding reduces single-batch decoding latency by 3x. BitDecoding is open-sourced at https://github.com/OpenBitSys/BitDecoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growth of long-context Large Language Models (LLMs) significantly increases memory and bandwidth pressure during autoregressive decoding due to the expanding Key-Value (KV) cache. While accuracy-preserving KV-cache quantization (e.g., 4-bit or 2-bit) reduces memory footprint, existing systems decode inefficiently by relying solely on CUDA cores, underutilizing Tensor Cores-the dominant compute resource on GPUs.\n  We present BitDecoding, the first inference system to efficiently decode low-bit KV caches by cooperatively leveraging CUDA cores and Tensor Cores. BitDecoding smartly induces Tensor-Core-friendly layouts, introduces warp-level dequantization parallelism, and provides unified system support through query transformation, high-performance tensor- and channel-wise quantization, and a software-pipelined dequantization kernel enabling mixed-precision execution. Architecture-aware optimizations further leverage Hopper's warpgroup tensor instructions and Blackwell's NVFP4 (MXFP4) tensor formats.\n  Evaluated on Blackwell, Hopper, and Ampere GPUs, BitDecoding achieves an average 7.5x decoding speedup over FP16 FlashDecoding-v2, up to 8.6x on Blackwell with NVFP4, and up to 4.3x over state-of-the-art approaches. On LLaMA-3.1-8B with a 128K context, BitDecoding reduces single-batch decoding latency by 3x. BitDecoding is open-sourced at https://github.com/OpenBitSys/BitDecoding."
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-03-24T15:22:41Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    15,
                    22,
                    41,
                    0,
                    83,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR"
                },
                "authors": [
                    {
                        "name": "Dayou Du"
                    },
                    {
                        "name": "Shijie Cao"
                    },
                    {
                        "name": "Jianyi Cheng"
                    },
                    {
                        "name": "Luo Mai"
                    },
                    {
                        "name": "Ting Cao"
                    },
                    {
                        "name": "Mao Yang"
                    }
                ],
                "author_detail": {
                    "name": "Mao Yang"
                },
                "author": "Mao Yang"
            },
            {
                "id": "http://arxiv.org/abs/2601.02281v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.02281v1",
                "title": "InfiniteVGGT: Visual Geometry Grounded Transformer for Endless Streams",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InfiniteVGGT: Visual Geometry Grounded Transformer for Endless Streams"
                },
                "updated": "2026-01-05T17:11:00Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    17,
                    11,
                    0,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.02281v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.02281v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The grand vision of enabling persistent, large-scale 3D visual geometry understanding is shackled by the irreconcilable demands of scalability and long-term stability. While offline models like VGGT achieve inspiring geometry capability, their batch-based nature renders them irrelevant for live systems. Streaming architectures, though the intended solution for live operation, have proven inadequate. Existing methods either fail to support truly infinite-horizon inputs or suffer from catastrophic drift over long sequences. We shatter this long-standing dilemma with InfiniteVGGT, a causal visual geometry transformer that operationalizes the concept of a rolling memory through a bounded yet adaptive and perpetually expressive KV cache. Capitalizing on this, we devise a training-free, attention-agnostic pruning strategy that intelligently discards obsolete information, effectively ``rolling'' the memory forward with each new frame. Fully compatible with FlashAttention, InfiniteVGGT finally alleviates the compromise, enabling infinite-horizon streaming while outperforming existing streaming methods in long-term stability. The ultimate test for such a system is its performance over a truly infinite horizon, a capability that has been impossible to rigorously validate due to the lack of extremely long-term, continuous benchmarks. To address this critical gap, we introduce the Long3D benchmark, which, for the first time, enables a rigorous evaluation of continuous 3D geometry estimation on sequences about 10,000 frames. This provides the definitive evaluation platform for future research in long-term 3D geometry understanding. Code is available at: https://github.com/AutoLab-SAI-SJTU/InfiniteVGGT",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The grand vision of enabling persistent, large-scale 3D visual geometry understanding is shackled by the irreconcilable demands of scalability and long-term stability. While offline models like VGGT achieve inspiring geometry capability, their batch-based nature renders them irrelevant for live systems. Streaming architectures, though the intended solution for live operation, have proven inadequate. Existing methods either fail to support truly infinite-horizon inputs or suffer from catastrophic drift over long sequences. We shatter this long-standing dilemma with InfiniteVGGT, a causal visual geometry transformer that operationalizes the concept of a rolling memory through a bounded yet adaptive and perpetually expressive KV cache. Capitalizing on this, we devise a training-free, attention-agnostic pruning strategy that intelligently discards obsolete information, effectively ``rolling'' the memory forward with each new frame. Fully compatible with FlashAttention, InfiniteVGGT finally alleviates the compromise, enabling infinite-horizon streaming while outperforming existing streaming methods in long-term stability. The ultimate test for such a system is its performance over a truly infinite horizon, a capability that has been impossible to rigorously validate due to the lack of extremely long-term, continuous benchmarks. To address this critical gap, we introduce the Long3D benchmark, which, for the first time, enables a rigorous evaluation of continuous 3D geometry estimation on sequences about 10,000 frames. This provides the definitive evaluation platform for future research in long-term 3D geometry understanding. Code is available at: https://github.com/AutoLab-SAI-SJTU/InfiniteVGGT"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-05T17:11:00Z",
                "published_parsed": [
                    2026,
                    1,
                    5,
                    17,
                    11,
                    0,
                    0,
                    5,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Shuai Yuan"
                    },
                    {
                        "name": "Yantai Yang"
                    },
                    {
                        "name": "Xiaotian Yang"
                    },
                    {
                        "name": "Xupeng Zhang"
                    },
                    {
                        "name": "Zhonghao Zhao"
                    },
                    {
                        "name": "Lingming Zhang"
                    },
                    {
                        "name": "Zhipeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Zhipeng Zhang"
                },
                "author": "Zhipeng Zhang"
            },
            {
                "id": "http://arxiv.org/abs/2504.11320v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2504.11320v2",
                "title": "Optimizing LLM Inference: Fluid-Guided Online Scheduling with Memory Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing LLM Inference: Fluid-Guided Online Scheduling with Memory Constraints"
                },
                "updated": "2026-01-05T14:10:45Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    14,
                    10,
                    45,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2504.11320v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2504.11320v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) power many modern applications, but their inference procedure poses unique scheduling challenges: the Key-Value (KV) cache grows dynamically during response generation, and memory overflow triggers eviction that can cascade into system-wide failures. Even when memory capacity exceeds the theoretical requirement, conventional scheduling algorithms fail because they do not account for this dynamic memory growth -- a system that should be stable can become unstable under poor scheduling.\n  This paper formulates LLM inference optimization as a multi-stage online scheduling problem. We develop a fluid dynamics approximation to establish a tractable benchmark and derive the Waiting for Accumulated Inference Threshold (WAIT) algorithm. WAIT uses threshold-based batching to prevent eviction by keeping the system near load balance, achieving near-optimal throughput when output lengths are known.\n  For practical settings where output lengths are unknown at arrival, we introduce Nested WAIT. Rather than predicting output lengths, Nested WAIT classifies prompts on-the-fly: short prompts complete early and exit, while longer prompts naturally advance to later segments. A safety buffer provides high-probability protection against memory overflow with only logarithmic overhead.\n  Theoretical analysis establishes near-optimal performance in the asymptotic regime. Experiments on Llama-7B with an A100 GPU demonstrate that our approach achieves superior throughput and reduced latency compared to vLLM and Sarathi. This work applies operations research principles to establish a theoretical framework for LLM deployment under memory constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) power many modern applications, but their inference procedure poses unique scheduling challenges: the Key-Value (KV) cache grows dynamically during response generation, and memory overflow triggers eviction that can cascade into system-wide failures. Even when memory capacity exceeds the theoretical requirement, conventional scheduling algorithms fail because they do not account for this dynamic memory growth -- a system that should be stable can become unstable under poor scheduling.\n  This paper formulates LLM inference optimization as a multi-stage online scheduling problem. We develop a fluid dynamics approximation to establish a tractable benchmark and derive the Waiting for Accumulated Inference Threshold (WAIT) algorithm. WAIT uses threshold-based batching to prevent eviction by keeping the system near load balance, achieving near-optimal throughput when output lengths are known.\n  For practical settings where output lengths are unknown at arrival, we introduce Nested WAIT. Rather than predicting output lengths, Nested WAIT classifies prompts on-the-fly: short prompts complete early and exit, while longer prompts naturally advance to later segments. A safety buffer provides high-probability protection against memory overflow with only logarithmic overhead.\n  Theoretical analysis establishes near-optimal performance in the asymptotic regime. Experiments on Llama-7B with an A100 GPU demonstrate that our approach achieves superior throughput and reduced latency compared to vLLM and Sarathi. This work applies operations research principles to establish a theoretical framework for LLM deployment under memory constraints."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-04-15T16:00:21Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    16,
                    0,
                    21,
                    1,
                    105,
                    0
                ],
                "arxiv_comment": "49 pages, 18 figures",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Ruicheng Ao"
                    },
                    {
                        "name": "Gan Luo"
                    },
                    {
                        "name": "David Simchi-Levi"
                    },
                    {
                        "name": "Xinshang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xinshang Wang"
                },
                "author": "Xinshang Wang"
            },
            {
                "id": "http://arxiv.org/abs/2512.22673v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.22673v2",
                "title": "TravelBench: A Broader Real-World Benchmark for Multi-Turn and Tool-Using Travel Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TravelBench: A Broader Real-World Benchmark for Multi-Turn and Tool-Using Travel Planning"
                },
                "updated": "2026-01-05T13:19:13Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    13,
                    19,
                    13,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.22673v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.22673v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Travel planning is a natural real-world task to test large language models (LLMs) planning and tool-use abilities. Although prior work has studied LLM performance on travel planning, existing settings still differ from real-world needs, mainly due to limited domain coverage, insufficient modeling of users' implicit preferences in multi-turn conversations, and a lack of clear evaluation of agents' capability boundaries. To mitigate these gaps, we propose \\textbf{TravelBench}, a benchmark for fully real-world travel planning. We collect user queries, user profile and tools from real scenarios, and construct three subtasks-Single-Turn, Multi-Turn, and Unsolvable-to evaluate agent's three core capabilities in real settings: (1) solving problems autonomously, (2) interacting with users over multiple turns to refine requirements, and (3) recognizing the limits of own abilities. To enable stable tool invocation and reproducible evaluation, we cache real tool-call results and build a sandbox environment that integrates ten travel-related tools. Agents can combine these tools to solve most practical travel planning problems, and our systematic verification demonstrates the stability of the proposed benchmark. We further evaluate multiple LLMs on TravelBench and conduct an in-depth analysis of their behaviors and performance. TravelBench provides a practical and reproducible evaluation benchmark to advance research on LLM agents for travel planning.\\footnote{Our code and data will be available after internal review.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Travel planning is a natural real-world task to test large language models (LLMs) planning and tool-use abilities. Although prior work has studied LLM performance on travel planning, existing settings still differ from real-world needs, mainly due to limited domain coverage, insufficient modeling of users' implicit preferences in multi-turn conversations, and a lack of clear evaluation of agents' capability boundaries. To mitigate these gaps, we propose \\textbf{TravelBench}, a benchmark for fully real-world travel planning. We collect user queries, user profile and tools from real scenarios, and construct three subtasks-Single-Turn, Multi-Turn, and Unsolvable-to evaluate agent's three core capabilities in real settings: (1) solving problems autonomously, (2) interacting with users over multiple turns to refine requirements, and (3) recognizing the limits of own abilities. To enable stable tool invocation and reproducible evaluation, we cache real tool-call results and build a sandbox environment that integrates ten travel-related tools. Agents can combine these tools to solve most practical travel planning problems, and our systematic verification demonstrates the stability of the proposed benchmark. We further evaluate multiple LLMs on TravelBench and conduct an in-depth analysis of their behaviors and performance. TravelBench provides a practical and reproducible evaluation benchmark to advance research on LLM agents for travel planning.\\footnote{Our code and data will be available after internal review."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-27T18:25:14Z",
                "published_parsed": [
                    2025,
                    12,
                    27,
                    18,
                    25,
                    14,
                    5,
                    361,
                    0
                ],
                "arxiv_comment": "In progress",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Xiang Cheng"
                    },
                    {
                        "name": "Yulan Hu"
                    },
                    {
                        "name": "Xiangwen Zhang"
                    },
                    {
                        "name": "Lu Xu"
                    },
                    {
                        "name": "Zheng Pan"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Yong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Liu"
                },
                "author": "Yong Liu"
            },
            {
                "id": "http://arxiv.org/abs/2601.02076v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.02076v1",
                "title": "Deferred Commitment Decoding for Diffusion Language Models with Confidence-Aware Sliding Windows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deferred Commitment Decoding for Diffusion Language Models with Confidence-Aware Sliding Windows"
                },
                "updated": "2026-01-05T12:57:33Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    12,
                    57,
                    33,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.02076v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.02076v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Diffusion language models (DLMs) have recently emerged as a strong alternative to autoregressive models by enabling parallel text generation. To improve inference efficiency and KV-cache compatibility, prior work commonly adopts block-based diffusion, decoding tokens block by block. However, this paradigm suffers from a structural limitation that we term Boundary-Induced Context Truncation (BICT): undecoded tokens near block boundaries are forced to commit without access to nearby future context, even when such context could substantially reduce uncertainty. This limitation degrades decoding confidence and generation quality, especially for tasks requiring precise reasoning, such as mathematical problem solving and code generation. We propose Deferred Commitment Decoding (DCD), a novel, training-free decoding strategy that mitigates this issue. DCD maintains a confidence-aware sliding window over masked tokens, resolving low-uncertainty tokens early while deferring high-uncertainty tokens until sufficient contextual evidence becomes available. This design enables effective bidirectional information flow within the decoding window without sacrificing efficiency. Extensive experiments across multiple diffusion language models, benchmarks, and caching configurations show that DCD improves generation accuracy by 1.39% with comparable time on average compared to fixed block-based diffusion methods, with the most significant improvement reaching 9.0%. These results demonstrate that deferring token commitment based on uncertainty is a simple yet effective principle for improving both the quality and efficiency of diffusion language model decoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion language models (DLMs) have recently emerged as a strong alternative to autoregressive models by enabling parallel text generation. To improve inference efficiency and KV-cache compatibility, prior work commonly adopts block-based diffusion, decoding tokens block by block. However, this paradigm suffers from a structural limitation that we term Boundary-Induced Context Truncation (BICT): undecoded tokens near block boundaries are forced to commit without access to nearby future context, even when such context could substantially reduce uncertainty. This limitation degrades decoding confidence and generation quality, especially for tasks requiring precise reasoning, such as mathematical problem solving and code generation. We propose Deferred Commitment Decoding (DCD), a novel, training-free decoding strategy that mitigates this issue. DCD maintains a confidence-aware sliding window over masked tokens, resolving low-uncertainty tokens early while deferring high-uncertainty tokens until sufficient contextual evidence becomes available. This design enables effective bidirectional information flow within the decoding window without sacrificing efficiency. Extensive experiments across multiple diffusion language models, benchmarks, and caching configurations show that DCD improves generation accuracy by 1.39% with comparable time on average compared to fixed block-based diffusion methods, with the most significant improvement reaching 9.0%. These results demonstrate that deferring token commitment based on uncertainty is a simple yet effective principle for improving both the quality and efficiency of diffusion language model decoding."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-05T12:57:33Z",
                "published_parsed": [
                    2026,
                    1,
                    5,
                    12,
                    57,
                    33,
                    0,
                    5,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Yingte Shu"
                    },
                    {
                        "name": "Yuchuan Tian"
                    },
                    {
                        "name": "Chao Xu"
                    },
                    {
                        "name": "Yunhe Wang"
                    },
                    {
                        "name": "Hanting Chen"
                    }
                ],
                "author_detail": {
                    "name": "Hanting Chen"
                },
                "author": "Hanting Chen"
            },
            {
                "id": "http://arxiv.org/abs/2601.02023v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.02023v1",
                "title": "Not All Needles Are Found: How Fact Distribution and Don't Make It Up Prompts Shape Literal Extraction, Logical Inference, and Hallucination Risks in Long-Context LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not All Needles Are Found: How Fact Distribution and Don't Make It Up Prompts Shape Literal Extraction, Logical Inference, and Hallucination Risks in Long-Context LLMs"
                },
                "updated": "2026-01-05T11:30:56Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    11,
                    30,
                    56,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.02023v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.02023v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models (LLMs) increasingly support very long input contexts. Yet it remains unclear how reliably they extract and infer information at scale. Performance varies with context length and strongly interacts with how information is distributed in real-world corpora. Motivated by these observations, we study how fact placement, corpus-level fact distributions, and Don't Make It Up prompts influence model behavior. We introduce an extended needle-in-a-haystack benchmark across four production-scale models: Gemini-2.5-flash, ChatGPT-5-mini, Claude-4.5-haiku, and Deepseek-v3.2-chat. Unlike prior work, we separately evaluate literal extraction, logical inference, and hallucination risk. Our study considers both positional effects and realistic distributions of evidence across long contexts, as well as prompts that explicitly discourage fabrication. We find that longer contexts alone do not guarantee better performance and can be detrimental when relevant evidence is diluted or widely dispersed. Performance varies substantially across models: some show severe degradation under realistic conditions, while others remain more robust at longer context lengths. Anti-hallucination (AH) instructions can make some models overly conservative, sharply reducing accuracy in literal extraction and logical inference. While we do not directly compare retrieval-augmented generation (RAG) and cache-augmented generation (CAG), our results suggest many failures stem from ineffective context utilization. Models often struggle to identify and prioritize relevant information even when it is present. These findings have direct practical implications, as enterprise workflows increasingly involve pasting large volumes of unfiltered documents into LLM prompts. Effective context length and model-specific robustness to long contexts are therefore critical for reliable LLM deployment in research and business.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) increasingly support very long input contexts. Yet it remains unclear how reliably they extract and infer information at scale. Performance varies with context length and strongly interacts with how information is distributed in real-world corpora. Motivated by these observations, we study how fact placement, corpus-level fact distributions, and Don't Make It Up prompts influence model behavior. We introduce an extended needle-in-a-haystack benchmark across four production-scale models: Gemini-2.5-flash, ChatGPT-5-mini, Claude-4.5-haiku, and Deepseek-v3.2-chat. Unlike prior work, we separately evaluate literal extraction, logical inference, and hallucination risk. Our study considers both positional effects and realistic distributions of evidence across long contexts, as well as prompts that explicitly discourage fabrication. We find that longer contexts alone do not guarantee better performance and can be detrimental when relevant evidence is diluted or widely dispersed. Performance varies substantially across models: some show severe degradation under realistic conditions, while others remain more robust at longer context lengths. Anti-hallucination (AH) instructions can make some models overly conservative, sharply reducing accuracy in literal extraction and logical inference. While we do not directly compare retrieval-augmented generation (RAG) and cache-augmented generation (CAG), our results suggest many failures stem from ineffective context utilization. Models often struggle to identify and prioritize relevant information even when it is present. These findings have direct practical implications, as enterprise workflows increasingly involve pasting large volumes of unfiltered documents into LLM prompts. Effective context length and model-specific robustness to long contexts are therefore critical for reliable LLM deployment in research and business."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-05T11:30:56Z",
                "published_parsed": [
                    2026,
                    1,
                    5,
                    11,
                    30,
                    56,
                    0,
                    5,
                    0
                ],
                "arxiv_comment": "25 pages, 8 figures, 3 tables",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Amirali Ebrahimzadeh"
                    },
                    {
                        "name": "Seyyed M. Salili"
                    }
                ],
                "author_detail": {
                    "name": "Seyyed M. Salili"
                },
                "author": "Seyyed M. Salili"
            },
            {
                "id": "http://arxiv.org/abs/2601.01925v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.01925v1",
                "title": "AR-MOT: Autoregressive Multi-object Tracking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AR-MOT: Autoregressive Multi-object Tracking"
                },
                "updated": "2026-01-05T09:17:28Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    9,
                    17,
                    28,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.01925v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.01925v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "As multi-object tracking (MOT) tasks continue to evolve toward more general and multi-modal scenarios, the rigid and task-specific architectures of existing MOT methods increasingly hinder their applicability across diverse tasks and limit flexibility in adapting to new tracking formulations. Most approaches rely on fixed output heads and bespoke tracking pipelines, making them difficult to extend to more complex or instruction-driven tasks. To address these limitations, we propose AR-MOT, a novel autoregressive paradigm that formulates MOT as a sequence generation task within a large language model (LLM) framework. This design enables the model to output structured results through flexible sequence construction, without requiring any task-specific heads. To enhance region-level visual perception, we introduce an Object Tokenizer based on a pretrained detector. To mitigate the misalignment between global and regional features, we propose a Region-Aware Alignment (RAA) module, and to support long-term tracking, we design a Temporal Memory Fusion (TMF) module that caches historical object tokens. AR-MOT offers strong potential for extensibility, as new modalities or instructions can be integrated by simply modifying the output sequence format without altering the model architecture. Extensive experiments on MOT17 and DanceTrack validate the feasibility of our approach, achieving performance comparable to state-of-the-art methods while laying the foundation for more general and flexible MOT systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As multi-object tracking (MOT) tasks continue to evolve toward more general and multi-modal scenarios, the rigid and task-specific architectures of existing MOT methods increasingly hinder their applicability across diverse tasks and limit flexibility in adapting to new tracking formulations. Most approaches rely on fixed output heads and bespoke tracking pipelines, making them difficult to extend to more complex or instruction-driven tasks. To address these limitations, we propose AR-MOT, a novel autoregressive paradigm that formulates MOT as a sequence generation task within a large language model (LLM) framework. This design enables the model to output structured results through flexible sequence construction, without requiring any task-specific heads. To enhance region-level visual perception, we introduce an Object Tokenizer based on a pretrained detector. To mitigate the misalignment between global and regional features, we propose a Region-Aware Alignment (RAA) module, and to support long-term tracking, we design a Temporal Memory Fusion (TMF) module that caches historical object tokens. AR-MOT offers strong potential for extensibility, as new modalities or instructions can be integrated by simply modifying the output sequence format without altering the model architecture. Extensive experiments on MOT17 and DanceTrack validate the feasibility of our approach, achieving performance comparable to state-of-the-art methods while laying the foundation for more general and flexible MOT systems."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-05T09:17:28Z",
                "published_parsed": [
                    2026,
                    1,
                    5,
                    9,
                    17,
                    28,
                    0,
                    5,
                    0
                ],
                "arxiv_comment": "12 pages, 5 figures",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Lianjie Jia"
                    },
                    {
                        "name": "Yuhan Wu"
                    },
                    {
                        "name": "Binghao Ran"
                    },
                    {
                        "name": "Yifan Wang"
                    },
                    {
                        "name": "Lijun Wang"
                    },
                    {
                        "name": "Huchuan Lu"
                    }
                ],
                "author_detail": {
                    "name": "Huchuan Lu"
                },
                "author": "Huchuan Lu"
            },
            {
                "id": "http://arxiv.org/abs/2305.07205v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2305.07205v3",
                "title": "Mem-Rec: Memory Efficient Recommendation System using Alternative Representation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mem-Rec: Memory Efficient Recommendation System using Alternative Representation"
                },
                "updated": "2026-01-05T03:36:57Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    3,
                    36,
                    57,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2305.07205v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2305.07205v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Deep learning-based recommendation systems (e.g., DLRMs) are widely used AI models to provide high-quality personalized recommendations. Training data used for modern recommendation systems commonly includes categorical features taking on tens-of-millions of possible distinct values. These categorical tokens are typically assigned learned vector representations, that are stored in large embedding tables, on the order of 100s of GB. Storing and accessing these tables represent a substantial burden in commercial deployments. Our work proposes MEM-REC, a novel alternative representation approach for embedding tables. MEM-REC leverages bloom filters and hashing methods to encode categorical features using two cache-friendly embedding tables. The first table (token embedding) contains raw embeddings (i.e. learned vector representation), and the second table (weight embedding), which is much smaller, contains weights to scale these raw embeddings to provide better discriminative capability to each data point. We provide a detailed architecture, design and analysis of MEM-REC addressing trade-offs in accuracy and computation requirements, in comparison with state-of-the-art techniques. We show that MEM-REC can not only maintain the recommendation quality and significantly reduce the memory footprint for commercial scale recommendation models but can also improve the embedding latency. In particular, based on our results, MEM-REC compresses the MLPerf CriteoTB benchmark DLRM model size by 2900x and performs up to 3.4x faster embeddings while achieving the same AUC as that of the full uncompressed model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning-based recommendation systems (e.g., DLRMs) are widely used AI models to provide high-quality personalized recommendations. Training data used for modern recommendation systems commonly includes categorical features taking on tens-of-millions of possible distinct values. These categorical tokens are typically assigned learned vector representations, that are stored in large embedding tables, on the order of 100s of GB. Storing and accessing these tables represent a substantial burden in commercial deployments. Our work proposes MEM-REC, a novel alternative representation approach for embedding tables. MEM-REC leverages bloom filters and hashing methods to encode categorical features using two cache-friendly embedding tables. The first table (token embedding) contains raw embeddings (i.e. learned vector representation), and the second table (weight embedding), which is much smaller, contains weights to scale these raw embeddings to provide better discriminative capability to each data point. We provide a detailed architecture, design and analysis of MEM-REC addressing trade-offs in accuracy and computation requirements, in comparison with state-of-the-art techniques. We show that MEM-REC can not only maintain the recommendation quality and significantly reduce the memory footprint for commercial scale recommendation models but can also improve the embedding latency. In particular, based on our results, MEM-REC compresses the MLPerf CriteoTB benchmark DLRM model size by 2900x and performs up to 3.4x faster embeddings while achieving the same AUC as that of the full uncompressed model."
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2023-05-12T02:36:07Z",
                "published_parsed": [
                    2023,
                    5,
                    12,
                    2,
                    36,
                    7,
                    4,
                    132,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR"
                },
                "arxiv_journal_ref": "Proceedings of the 15th Asian Conference on Machine Learning (2023), PMLR 222:518-533",
                "authors": [
                    {
                        "name": "Gopi Krishna Jha"
                    },
                    {
                        "name": "Anthony Thomas"
                    },
                    {
                        "name": "Nilesh Jain"
                    },
                    {
                        "name": "Sameh Gobriel"
                    },
                    {
                        "name": "Tajana Rosing"
                    },
                    {
                        "name": "Ravi Iyer"
                    }
                ],
                "author_detail": {
                    "name": "Ravi Iyer"
                },
                "author": "Ravi Iyer"
            },
            {
                "id": "http://arxiv.org/abs/2503.00022v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2503.00022v2",
                "title": "KVCrush: Key value cache size-reduction using similarity in head-behaviour",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVCrush: Key value cache size-reduction using similarity in head-behaviour"
                },
                "updated": "2026-01-05T03:29:51Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    3,
                    29,
                    51,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2503.00022v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2503.00022v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Key-value (KV) caching has emerged as a crucial optimization technique for accelerating inference in large language models (LLMs). By allowing the attention operation to scale linearly rather than quadratically with the total sequence length, KV caching significantly enhances generation throughput. However, due to large context lengths in the modern LLMs, the memory footprint of the KV is a huge bottleneck for model deployment directly impacting the model's batch size, hindering its ability to deliver high-throughput. Existing research addresses this challenge using several techniques, such as discarding low-attention tokens, quantization, and matrix approximation which typically lead to a negative impact on the model accuracy.\n  In this paper, We propose KVCrush technology which can be combined with many KV compression technologies to improve the model accuracy at a much smaller memory. KVCrush provides an alternate representation scheme for key-value states, along with a low-overhead token pruning algorithm that accounts for the token distribution in the KV cache, which in turn allows for a a smaller footprint while maintaining the accuracy of the model. Based on our results, KVCrush reduces LongBench KV Cache size by 4x with less than 1% accuracy drop and achieves state-of-the-art average accuracy with minimal overhead, incurring less than 0.5% total inference latency. KVCrush not only outperforms the accuracy of state-of-the-art importance-based token retention schemes but is also compatible with typical practical LLM deployments using KV cache paging schemes such as vLLM and mixed precision quantization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-value (KV) caching has emerged as a crucial optimization technique for accelerating inference in large language models (LLMs). By allowing the attention operation to scale linearly rather than quadratically with the total sequence length, KV caching significantly enhances generation throughput. However, due to large context lengths in the modern LLMs, the memory footprint of the KV is a huge bottleneck for model deployment directly impacting the model's batch size, hindering its ability to deliver high-throughput. Existing research addresses this challenge using several techniques, such as discarding low-attention tokens, quantization, and matrix approximation which typically lead to a negative impact on the model accuracy.\n  In this paper, We propose KVCrush technology which can be combined with many KV compression technologies to improve the model accuracy at a much smaller memory. KVCrush provides an alternate representation scheme for key-value states, along with a low-overhead token pruning algorithm that accounts for the token distribution in the KV cache, which in turn allows for a a smaller footprint while maintaining the accuracy of the model. Based on our results, KVCrush reduces LongBench KV Cache size by 4x with less than 1% accuracy drop and achieves state-of-the-art average accuracy with minimal overhead, incurring less than 0.5% total inference latency. KVCrush not only outperforms the accuracy of state-of-the-art importance-based token retention schemes but is also compatible with typical practical LLM deployments using KV cache paging schemes such as vLLM and mixed precision quantization."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-02-24T02:57:51Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    2,
                    57,
                    51,
                    0,
                    55,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "arxiv_journal_ref": "Proceedings of the 17th Asian Conference on Machine Learning (2025), PMLR",
                "authors": [
                    {
                        "name": "Gopi Krishna Jha"
                    },
                    {
                        "name": "Sameh Gobriel"
                    },
                    {
                        "name": "Liubov Talamanova"
                    },
                    {
                        "name": "Nilesh Jain"
                    }
                ],
                "author_detail": {
                    "name": "Nilesh Jain"
                },
                "author": "Nilesh Jain"
            },
            {
                "id": "http://arxiv.org/abs/2601.01712v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.01712v1",
                "title": "RelayGR: Scaling Long-Sequence Generative Recommendation via Cross-Stage Relay-Race Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RelayGR: Scaling Long-Sequence Generative Recommendation via Cross-Stage Relay-Race Inference"
                },
                "updated": "2026-01-05T01:34:06Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    1,
                    34,
                    6,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.01712v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.01712v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Real-time recommender systems execute multi-stage cascades (retrieval, pre-processing, fine-grained ranking) under strict tail-latency SLOs, leaving only tens of milliseconds for ranking. Generative recommendation (GR) models can improve quality by consuming long user-behavior sequences, but in production their online sequence length is tightly capped by the ranking-stage P99 budget. We observe that the majority of GR tokens encode user behaviors that are independent of the item candidates, suggesting an opportunity to pre-infer a user-behavior prefix once and reuse it during ranking rather than recomputing it on the critical path. Realizing this idea at industrial scale is non-trivial: the prefix cache must survive across multiple pipeline stages before the final ranking instance is determined, the user population implies cache footprints far beyond a single device, and indiscriminate pre-inference would overload shared resources under high QPS. We present RelayGR, a production system that enables in-HBM relay-race inference for GR. RelayGR selectively pre-infers long-term user prefixes, keeps their KV caches resident in HBM over the request lifecycle, and ensures the subsequent ranking can consume them without remote fetches. RelayGR combines three techniques: 1) a sequence-aware trigger that admits only at-risk requests under a bounded cache footprint and pre-inference load, 2) an affinity-aware router that co-locates cache production and consumption by routing both the auxiliary pre-infer signal and the ranking request to the same instance, and 3) a memory-aware expander that uses server-local DRAM to capture short-term cross-request reuse while avoiding redundant reloads. We implement RelayGR on Huawei Ascend NPUs and evaluate it with real queries. Under a fixed P99 SLO, RelayGR supports up to 1.5$\\times$ longer sequences and improves SLO-compliant throughput by up to 3.6$\\times$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time recommender systems execute multi-stage cascades (retrieval, pre-processing, fine-grained ranking) under strict tail-latency SLOs, leaving only tens of milliseconds for ranking. Generative recommendation (GR) models can improve quality by consuming long user-behavior sequences, but in production their online sequence length is tightly capped by the ranking-stage P99 budget. We observe that the majority of GR tokens encode user behaviors that are independent of the item candidates, suggesting an opportunity to pre-infer a user-behavior prefix once and reuse it during ranking rather than recomputing it on the critical path. Realizing this idea at industrial scale is non-trivial: the prefix cache must survive across multiple pipeline stages before the final ranking instance is determined, the user population implies cache footprints far beyond a single device, and indiscriminate pre-inference would overload shared resources under high QPS. We present RelayGR, a production system that enables in-HBM relay-race inference for GR. RelayGR selectively pre-infers long-term user prefixes, keeps their KV caches resident in HBM over the request lifecycle, and ensures the subsequent ranking can consume them without remote fetches. RelayGR combines three techniques: 1) a sequence-aware trigger that admits only at-risk requests under a bounded cache footprint and pre-inference load, 2) an affinity-aware router that co-locates cache production and consumption by routing both the auxiliary pre-infer signal and the ranking request to the same instance, and 3) a memory-aware expander that uses server-local DRAM to capture short-term cross-request reuse while avoiding redundant reloads. We implement RelayGR on Huawei Ascend NPUs and evaluate it with real queries. Under a fixed P99 SLO, RelayGR supports up to 1.5$\\times$ longer sequences and improves SLO-compliant throughput by up to 3.6$\\times$."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-05T01:34:06Z",
                "published_parsed": [
                    2026,
                    1,
                    5,
                    1,
                    34,
                    6,
                    0,
                    5,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Jiarui Wang"
                    },
                    {
                        "name": "Huichao Chai"
                    },
                    {
                        "name": "Yuanhang Zhang"
                    },
                    {
                        "name": "Zongjin Zhou"
                    },
                    {
                        "name": "Wei Guo"
                    },
                    {
                        "name": "Xingkun Yang"
                    },
                    {
                        "name": "Qiang Tang"
                    },
                    {
                        "name": "Bo Pan"
                    },
                    {
                        "name": "Jiawei Zhu"
                    },
                    {
                        "name": "Ke Cheng"
                    },
                    {
                        "name": "Yuting Yan"
                    },
                    {
                        "name": "Shulan Wang"
                    },
                    {
                        "name": "Yingjie Zhu"
                    },
                    {
                        "name": "Zhengfan Yuan"
                    },
                    {
                        "name": "Jiaqi Huang"
                    },
                    {
                        "name": "Yuhan Zhang"
                    },
                    {
                        "name": "Xiaosong Sun"
                    },
                    {
                        "name": "Zhinan Zhang"
                    },
                    {
                        "name": "Hong Zhu"
                    },
                    {
                        "name": "Yongsheng Zhang"
                    },
                    {
                        "name": "Tiantian Dong"
                    },
                    {
                        "name": "Zhong Xiao"
                    },
                    {
                        "name": "Deliang Liu"
                    },
                    {
                        "name": "Chengzhou Lu"
                    },
                    {
                        "name": "Yuan Sun"
                    },
                    {
                        "name": "Zhiyuan Chen"
                    },
                    {
                        "name": "Xinming Han"
                    },
                    {
                        "name": "Zaizhu Liu"
                    },
                    {
                        "name": "Yaoyuan Wang"
                    },
                    {
                        "name": "Ziyang Zhang"
                    },
                    {
                        "name": "Yong Liu"
                    },
                    {
                        "name": "Jinxin Xu"
                    },
                    {
                        "name": "Yajing Sun"
                    },
                    {
                        "name": "Zhoujun Yu"
                    },
                    {
                        "name": "Wenting Zhou"
                    },
                    {
                        "name": "Qidong Zhang"
                    },
                    {
                        "name": "Zhengyong Zhang"
                    },
                    {
                        "name": "Zhonghai Gu"
                    },
                    {
                        "name": "Yibo Jin"
                    },
                    {
                        "name": "Yongxiang Feng"
                    },
                    {
                        "name": "Pengfei Zuo"
                    }
                ],
                "author_detail": {
                    "name": "Pengfei Zuo"
                },
                "author": "Pengfei Zuo"
            },
            {
                "id": "http://arxiv.org/abs/2601.01310v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.01310v1",
                "title": "Making MoE based LLM inference resilient with Tarragon",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Making MoE based LLM inference resilient with Tarragon"
                },
                "updated": "2026-01-04T00:13:31Z",
                "updated_parsed": [
                    2026,
                    1,
                    4,
                    0,
                    13,
                    31,
                    6,
                    4,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.01310v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.01310v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Mixture-of-Experts (MoE) models are increasingly used to serve LLMs at scale, but failures become common as deployment scale grows. Existing systems exhibit poor failure resilience: even a single worker failure triggers a coarse-grained, service-wide restart, discarding accumulated progress and halting the entire inference pipeline during recovery--an approach clearly ill-suited for latency-sensitive, LLM services.\n  We present Tarragon, a resilient MoE inference framework that confines the failures impact to individual workers while allowing the rest of the pipeline to continue making forward progress. Tarragon exploits the natural separation between the attention and expert computation in MoE-based transformers, treating attention workers (AWs) and expert workers (EWs) as distinct failure domains. Tarragon introduces a reconfigurable datapath to mask failures by rerouting requests to healthy workers. On top of this datapath, Tarragon implements a self-healing mechanism that relaxes the tightly synchronized execution of existing MoE frameworks. For stateful AWs, Tarragon performs asynchronous, incremental KV cache checkpointing with per-request restoration, and for stateless EWs, it leverages residual GPU memory to deploy shadow experts. These together keep recovery cost and recomputation overhead extremely low. Our evaluation shows that, compared to state-of-the-art MegaScale-Infer, Tarragon reduces failure-induced stalls by 160-213x (from ~64 s down to 0.3-0.4 s) while preserving performance when no failures occur.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) models are increasingly used to serve LLMs at scale, but failures become common as deployment scale grows. Existing systems exhibit poor failure resilience: even a single worker failure triggers a coarse-grained, service-wide restart, discarding accumulated progress and halting the entire inference pipeline during recovery--an approach clearly ill-suited for latency-sensitive, LLM services.\n  We present Tarragon, a resilient MoE inference framework that confines the failures impact to individual workers while allowing the rest of the pipeline to continue making forward progress. Tarragon exploits the natural separation between the attention and expert computation in MoE-based transformers, treating attention workers (AWs) and expert workers (EWs) as distinct failure domains. Tarragon introduces a reconfigurable datapath to mask failures by rerouting requests to healthy workers. On top of this datapath, Tarragon implements a self-healing mechanism that relaxes the tightly synchronized execution of existing MoE frameworks. For stateful AWs, Tarragon performs asynchronous, incremental KV cache checkpointing with per-request restoration, and for stateless EWs, it leverages residual GPU memory to deploy shadow experts. These together keep recovery cost and recomputation overhead extremely low. Our evaluation shows that, compared to state-of-the-art MegaScale-Infer, Tarragon reduces failure-induced stalls by 160-213x (from ~64 s down to 0.3-0.4 s) while preserving performance when no failures occur."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-04T00:13:31Z",
                "published_parsed": [
                    2026,
                    1,
                    4,
                    0,
                    13,
                    31,
                    6,
                    4,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Songyu Zhang"
                    },
                    {
                        "name": "Aaron Tam"
                    },
                    {
                        "name": "Myungjin Lee"
                    },
                    {
                        "name": "Shixiong Qi"
                    },
                    {
                        "name": "K. K. Ramakrishnan"
                    }
                ],
                "author_detail": {
                    "name": "K. K. Ramakrishnan"
                },
                "author": "K. K. Ramakrishnan"
            },
            {
                "id": "http://arxiv.org/abs/2512.23914v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.23914v2",
                "title": "Hardware Acceleration for Neural Networks: A Comprehensive Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hardware Acceleration for Neural Networks: A Comprehensive Survey"
                },
                "updated": "2026-01-04T00:01:40Z",
                "updated_parsed": [
                    2026,
                    1,
                    4,
                    0,
                    1,
                    40,
                    6,
                    4,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.23914v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.23914v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Neural networks have become a dominant computational workload across cloud and edge platforms, but rapid growth in model size and deployment diversity has exposed hardware bottlenecks increasingly dominated by memory movement, communication, and irregular operators rather than peak arithmetic throughput. This survey reviews the technology landscape for hardware acceleration of deep learning, spanning GPUs and tensor-core architectures; domain-specific accelerators (e.g., TPUs/NPUs); FPGA-based designs; ASIC inference engines; and emerging LLM-serving accelerators such as LPUs (language processing units), alongside in-/near-memory computing and neuromorphic/analog approaches. We organize the space using a unified taxonomy across (i) workloads (CNNs, RNNs, GNNs, and Transformers/LLMs), (ii) execution settings (training vs.\\ inference; datacenter vs.\\ edge), and (iii) optimization levers (reduced precision, sparsity and pruning, operator fusion, compilation and scheduling, and memory-system/interconnect design). We synthesize key architectural ideas including systolic arrays, vector and SIMD engines, specialized attention and softmax kernels, quantization-aware datapaths, and high-bandwidth memory, and we discuss how software stacks and compilers bridge model semantics to hardware. Finally, we highlight open challenges -- including efficient long-context LLM inference (KV-cache management), robust support for dynamic and sparse workloads, energy- and security-aware deployment, and fair benchmarking -- and point to promising directions for the next generation of neural acceleration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural networks have become a dominant computational workload across cloud and edge platforms, but rapid growth in model size and deployment diversity has exposed hardware bottlenecks increasingly dominated by memory movement, communication, and irregular operators rather than peak arithmetic throughput. This survey reviews the technology landscape for hardware acceleration of deep learning, spanning GPUs and tensor-core architectures; domain-specific accelerators (e.g., TPUs/NPUs); FPGA-based designs; ASIC inference engines; and emerging LLM-serving accelerators such as LPUs (language processing units), alongside in-/near-memory computing and neuromorphic/analog approaches. We organize the space using a unified taxonomy across (i) workloads (CNNs, RNNs, GNNs, and Transformers/LLMs), (ii) execution settings (training vs.\\ inference; datacenter vs.\\ edge), and (iii) optimization levers (reduced precision, sparsity and pruning, operator fusion, compilation and scheduling, and memory-system/interconnect design). We synthesize key architectural ideas including systolic arrays, vector and SIMD engines, specialized attention and softmax kernels, quantization-aware datapaths, and high-bandwidth memory, and we discuss how software stacks and compilers bridge model semantics to hardware. Finally, we highlight open challenges -- including efficient long-context LLM inference (KV-cache management), robust support for dynamic and sparse workloads, energy- and security-aware deployment, and fair benchmarking -- and point to promising directions for the next generation of neural acceleration."
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-30T00:27:02Z",
                "published_parsed": [
                    2025,
                    12,
                    30,
                    0,
                    27,
                    2,
                    1,
                    364,
                    0
                ],
                "arxiv_comment": "error in section 5.1",
                "arxiv_primary_category": {
                    "term": "eess.SY"
                },
                "authors": [
                    {
                        "name": "Bin Xu"
                    },
                    {
                        "name": "Ayan Banerjee"
                    },
                    {
                        "name": "Sandeep Gupta"
                    }
                ],
                "author_detail": {
                    "name": "Sandeep Gupta"
                },
                "author": "Sandeep Gupta"
            },
            {
                "id": "http://arxiv.org/abs/2601.01298v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.01298v1",
                "title": "Warp-Cortex: An Asynchronous, Memory-Efficient Architecture for Million-Agent Cognitive Scaling on Consumer Hardware",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Warp-Cortex: An Asynchronous, Memory-Efficient Architecture for Million-Agent Cognitive Scaling on Consumer Hardware"
                },
                "updated": "2026-01-03T23:11:21Z",
                "updated_parsed": [
                    2026,
                    1,
                    3,
                    23,
                    11,
                    21,
                    5,
                    3,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.01298v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.01298v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Current multi-agent Large Language Model (LLM) frameworks suffer from linear memory scaling, rendering \"System 2\" parallel reasoning impractical on consumer hardware. We present Warp Cortex, an asynchronous architecture that theoretically enables million-agent cognitive scaling by decoupling agent logic from physical memory. Through Singleton Weight Sharing and a novel Topological Synapse--inspired by hybrid landmarking techniques from Topological Data Analysis (TDA)--we reduce memory complexity from O(N * L) to O(1) for weights and O(N * k) for context, where k << L. By treating the KV-cache as a point cloud in latent space, we apply witness-complex-inspired sparsification to preserve persistent homological features of the context manifold. On a single NVIDIA RTX 4090, we empirically demonstrate 100 concurrent agents at 2.2 GB total VRAM, with theoretical capacity exceeding 1,000 agents before compute latency becomes the bottleneck. We further introduce Referential Injection, a non-intrusive KV-cache update mechanism that allows asynchronous sub-agents to influence primary generation without stream disruption.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current multi-agent Large Language Model (LLM) frameworks suffer from linear memory scaling, rendering \"System 2\" parallel reasoning impractical on consumer hardware. We present Warp Cortex, an asynchronous architecture that theoretically enables million-agent cognitive scaling by decoupling agent logic from physical memory. Through Singleton Weight Sharing and a novel Topological Synapse--inspired by hybrid landmarking techniques from Topological Data Analysis (TDA)--we reduce memory complexity from O(N * L) to O(1) for weights and O(N * k) for context, where k << L. By treating the KV-cache as a point cloud in latent space, we apply witness-complex-inspired sparsification to preserve persistent homological features of the context manifold. On a single NVIDIA RTX 4090, we empirically demonstrate 100 concurrent agents at 2.2 GB total VRAM, with theoretical capacity exceeding 1,000 agents before compute latency becomes the bottleneck. We further introduce Referential Injection, a non-intrusive KV-cache update mechanism that allows asynchronous sub-agents to influence primary generation without stream disruption."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-03T23:11:21Z",
                "published_parsed": [
                    2026,
                    1,
                    3,
                    23,
                    11,
                    21,
                    5,
                    3,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Jorge L. Ruiz Williams"
                    }
                ],
                "author_detail": {
                    "name": "Jorge L. Ruiz Williams"
                },
                "author": "Jorge L. Ruiz Williams"
            },
            {
                "id": "http://arxiv.org/abs/2601.01204v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.01204v1",
                "title": "XStreamVGGT: Extremely Memory-Efficient Streaming Vision Geometry Grounded Transformer with KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XStreamVGGT: Extremely Memory-Efficient Streaming Vision Geometry Grounded Transformer with KV Cache Compression"
                },
                "updated": "2026-01-03T14:59:50Z",
                "updated_parsed": [
                    2026,
                    1,
                    3,
                    14,
                    59,
                    50,
                    5,
                    3,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.01204v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.01204v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Learning-based 3D visual geometry models have benefited substantially from large-scale transformers. Among these, StreamVGGT leverages frame-wise causal attention for strong streaming reconstruction, but suffers from unbounded KV cache growth, leading to escalating memory consumption and inference latency as input frames accumulate. We propose XStreamVGGT, a tuning-free approach that systematically compresses the KV cache through joint pruning and quantization, enabling extremely memory-efficient streaming inference. Specifically, redundant KVs originating from multi-view inputs are pruned through efficient token importance identification, enabling a fixed memory budget. Leveraging the unique distribution of KV tensors, we incorporate KV quantization to further reduce memory consumption. Extensive evaluations show that XStreamVGGT achieves mostly negligible performance degradation while substantially reducing memory usage by 4.42$\\times$ and accelerating inference by 5.48$\\times$, enabling scalable and practical streaming 3D applications. The code is available at https://github.com/ywh187/XStreamVGGT/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning-based 3D visual geometry models have benefited substantially from large-scale transformers. Among these, StreamVGGT leverages frame-wise causal attention for strong streaming reconstruction, but suffers from unbounded KV cache growth, leading to escalating memory consumption and inference latency as input frames accumulate. We propose XStreamVGGT, a tuning-free approach that systematically compresses the KV cache through joint pruning and quantization, enabling extremely memory-efficient streaming inference. Specifically, redundant KVs originating from multi-view inputs are pruned through efficient token importance identification, enabling a fixed memory budget. Leveraging the unique distribution of KV tensors, we incorporate KV quantization to further reduce memory consumption. Extensive evaluations show that XStreamVGGT achieves mostly negligible performance degradation while substantially reducing memory usage by 4.42$\\times$ and accelerating inference by 5.48$\\times$, enabling scalable and practical streaming 3D applications. The code is available at https://github.com/ywh187/XStreamVGGT/."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-03T14:59:50Z",
                "published_parsed": [
                    2026,
                    1,
                    3,
                    14,
                    59,
                    50,
                    5,
                    3,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Zunhai Su"
                    },
                    {
                        "name": "Weihao Ye"
                    },
                    {
                        "name": "Hansen Feng"
                    },
                    {
                        "name": "Keyu Fan"
                    },
                    {
                        "name": "Jing Zhang"
                    },
                    {
                        "name": "Dahai Yu"
                    },
                    {
                        "name": "Zhengwu Liu"
                    },
                    {
                        "name": "Ngai Wong"
                    }
                ],
                "author_detail": {
                    "name": "Ngai Wong"
                },
                "author": "Ngai Wong"
            },
            {
                "id": "http://arxiv.org/abs/2601.01112v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.01112v1",
                "title": "EmoLoom-2B: Fast Base-Model Screening for Emotion Classification and VAD with Lexicon-Weak Supervision and KV-Off Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EmoLoom-2B: Fast Base-Model Screening for Emotion Classification and VAD with Lexicon-Weak Supervision and KV-Off Evaluation"
                },
                "updated": "2026-01-03T08:25:58Z",
                "updated_parsed": [
                    2026,
                    1,
                    3,
                    8,
                    25,
                    58,
                    5,
                    3,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.01112v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.01112v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We introduce EmoLoom-2B, a lightweight and reproducible pipeline that turns small language models under 2B parameters into fast screening candidates for joint emotion classification and Valence-Arousal-Dominance prediction. To ensure protocol-faithful and fair evaluation, we unify data loading, training, and inference under a single JSON input-output contract and remove avoidable variance by adopting KV-off decoding as the default setting. We incorporate two orthogonal semantic regularizers: a VAD-preserving constraint that aligns generated text with target VAD triples, and a lightweight external appraisal classifier that provides training-time guidance on goal attainment, controllability, certainty, and fairness without injecting long rationales. To improve polarity sensitivity, we introduce Valence Flip augmentation based on mirrored emotional pairs. During supervised fine-tuning, we apply A/B mixture sampling with entropy-aware temperature scheduling to balance coverage and convergence. Using Qwen-1.8B-Chat as the base model, EmoLoom-2B achieves strong performance on GoEmotions and EmpatheticDialogues, and demonstrates robust cross-corpus generalization on DailyDialog. The proposed recipe is budget-aware, auditable, and re-entrant, serving as a dependable screening pass before heavier training or multimodal fusion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce EmoLoom-2B, a lightweight and reproducible pipeline that turns small language models under 2B parameters into fast screening candidates for joint emotion classification and Valence-Arousal-Dominance prediction. To ensure protocol-faithful and fair evaluation, we unify data loading, training, and inference under a single JSON input-output contract and remove avoidable variance by adopting KV-off decoding as the default setting. We incorporate two orthogonal semantic regularizers: a VAD-preserving constraint that aligns generated text with target VAD triples, and a lightweight external appraisal classifier that provides training-time guidance on goal attainment, controllability, certainty, and fairness without injecting long rationales. To improve polarity sensitivity, we introduce Valence Flip augmentation based on mirrored emotional pairs. During supervised fine-tuning, we apply A/B mixture sampling with entropy-aware temperature scheduling to balance coverage and convergence. Using Qwen-1.8B-Chat as the base model, EmoLoom-2B achieves strong performance on GoEmotions and EmpatheticDialogues, and demonstrates robust cross-corpus generalization on DailyDialog. The proposed recipe is budget-aware, auditable, and re-entrant, serving as a dependable screening pass before heavier training or multimodal fusion."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-03T08:25:58Z",
                "published_parsed": [
                    2026,
                    1,
                    3,
                    8,
                    25,
                    58,
                    5,
                    3,
                    0
                ],
                "arxiv_comment": "This paper presents an initial and self-contained study of a lightweight screening pipeline for emotion-aware language modeling, intended as a reproducible baseline and system-level design reference",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Zilin Li"
                    },
                    {
                        "name": "Weiwei Xu"
                    },
                    {
                        "name": "Xuanbo Lu"
                    },
                    {
                        "name": "Zheda Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zheda Liu"
                },
                "author": "Zheda Liu"
            },
            {
                "id": "http://arxiv.org/abs/2512.12595v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.12595v2",
                "title": "Vision-Enhanced Large Language Models for High-Resolution Image Synthesis and Multimodal Data Interpretation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Enhanced Large Language Models for High-Resolution Image Synthesis and Multimodal Data Interpretation"
                },
                "updated": "2026-01-03T07:05:35Z",
                "updated_parsed": [
                    2026,
                    1,
                    3,
                    7,
                    5,
                    35,
                    5,
                    3,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.12595v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.12595v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "This research introduces a transformative framework for integrating Vision-Enhanced Large Language Models (LLMs) with advanced transformer-based architectures to tackle challenges in high-resolution image synthesis and multimodal data interpretation. The proposed model incorporates a rectified flow mechanism that connects noise and data with linear paths, enabling efficient and high-quality generation. A bidirectional tokenization strategy is employed to seamlessly merge inputs from text, image, and video modalities, fostering a unified understanding across diverse data types. By embedding spatial-temporal features and leveraging a hybrid text-image sequence modeling approach, the framework achieves unparalleled fidelity in synthesized images and coherent multimodal representations. The architecture is optimized with a noise-aware learning algorithm, addressing discrepancies in noisy data distributions and improving generative performance under varying input conditions. Rigorous evaluations on benchmark datasets demonstrate a 25% increase in image resolution clarity and a 20% reduction in computational requirements compared to diffusion-based methods. Furthermore, the model exhibits robust scalability and adaptability, showcasing its potential in applications like autonomous systems, creative content generation, and advanced video analysis. This work underscores the role of vision-centric LLMs in redefining capabilities in computer vision and multimodal artificial intelligence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This research introduces a transformative framework for integrating Vision-Enhanced Large Language Models (LLMs) with advanced transformer-based architectures to tackle challenges in high-resolution image synthesis and multimodal data interpretation. The proposed model incorporates a rectified flow mechanism that connects noise and data with linear paths, enabling efficient and high-quality generation. A bidirectional tokenization strategy is employed to seamlessly merge inputs from text, image, and video modalities, fostering a unified understanding across diverse data types. By embedding spatial-temporal features and leveraging a hybrid text-image sequence modeling approach, the framework achieves unparalleled fidelity in synthesized images and coherent multimodal representations. The architecture is optimized with a noise-aware learning algorithm, addressing discrepancies in noisy data distributions and improving generative performance under varying input conditions. Rigorous evaluations on benchmark datasets demonstrate a 25% increase in image resolution clarity and a 20% reduction in computational requirements compared to diffusion-based methods. Furthermore, the model exhibits robust scalability and adaptability, showcasing its potential in applications like autonomous systems, creative content generation, and advanced video analysis. This work underscores the role of vision-centric LLMs in redefining capabilities in computer vision and multimodal artificial intelligence."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-14T08:28:50Z",
                "published_parsed": [
                    2025,
                    12,
                    14,
                    8,
                    28,
                    50,
                    6,
                    348,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Karthikeya KV"
                    }
                ],
                "author_detail": {
                    "name": "Karthikeya KV"
                },
                "author": "Karthikeya KV"
            },
            {
                "id": "http://arxiv.org/abs/2601.01086v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.01086v1",
                "title": "Decision-Aware Semantic State Synchronization in Compute-First Networking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decision-Aware Semantic State Synchronization in Compute-First Networking"
                },
                "updated": "2026-01-03T06:22:48Z",
                "updated_parsed": [
                    2026,
                    1,
                    3,
                    6,
                    22,
                    48,
                    5,
                    3,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.01086v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.01086v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "In Compute-First Networking (CFN), an Access Point (AP) makes task offloading decisions based on resource state information reported by a Service Node (SN). A fundamental challenge arises from the trade-off between update overhead and decision accuracy: Frequent state updates consume limited network resources, while infrequent updates lead to stale state views and degraded task performance, especially under high system load. Existing approaches based on periodic updates or Age of Information (AoI) mainly focus on temporal freshness and often overlook whether a state change is actually relevant to offloading decisions. This paper proposes SenseCFN, a decision-aware state synchronization framework for CFN. Instead of synchronizing raw resource states, SenseCFN focuses on identifying state changes that are likely to alter offloading decisions. To this end, we introduce a lightweight semantic state representation that captures decision-relevant system characteristics, along with a Semantic Deviation Index (SDI) to quantify the impact of state shifts on decision outcomes. Based on SDI, the SN triggers updates only when significant decision-impacting changes are detected. Meanwhile, the AP performs offloading decisions using cached semantic states with explicit awareness of potential staleness. The update and offloading policies are jointly optimized using a centralized training with distributed execution (CTDE) approach. Simulation results show that SenseCFN maintains a task success rate of up to 99.6% in saturation-prone scenarios, outperforming baseline methods by more than 25%, while reducing status update frequency by approximately 70% to 96%. These results indicate that decision-aware state synchronization provides an effective and practical alternative to purely time-based update strategies in CFN.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In Compute-First Networking (CFN), an Access Point (AP) makes task offloading decisions based on resource state information reported by a Service Node (SN). A fundamental challenge arises from the trade-off between update overhead and decision accuracy: Frequent state updates consume limited network resources, while infrequent updates lead to stale state views and degraded task performance, especially under high system load. Existing approaches based on periodic updates or Age of Information (AoI) mainly focus on temporal freshness and often overlook whether a state change is actually relevant to offloading decisions. This paper proposes SenseCFN, a decision-aware state synchronization framework for CFN. Instead of synchronizing raw resource states, SenseCFN focuses on identifying state changes that are likely to alter offloading decisions. To this end, we introduce a lightweight semantic state representation that captures decision-relevant system characteristics, along with a Semantic Deviation Index (SDI) to quantify the impact of state shifts on decision outcomes. Based on SDI, the SN triggers updates only when significant decision-impacting changes are detected. Meanwhile, the AP performs offloading decisions using cached semantic states with explicit awareness of potential staleness. The update and offloading policies are jointly optimized using a centralized training with distributed execution (CTDE) approach. Simulation results show that SenseCFN maintains a task success rate of up to 99.6% in saturation-prone scenarios, outperforming baseline methods by more than 25%, while reducing status update frequency by approximately 70% to 96%. These results indicate that decision-aware state synchronization provides an effective and practical alternative to purely time-based update strategies in CFN."
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-03T06:22:48Z",
                "published_parsed": [
                    2026,
                    1,
                    3,
                    6,
                    22,
                    48,
                    5,
                    3,
                    0
                ],
                "arxiv_comment": "12 pages, 9 figures",
                "arxiv_primary_category": {
                    "term": "cs.NI"
                },
                "authors": [
                    {
                        "name": "Jianpeng Qi"
                    },
                    {
                        "name": "Chao Liu"
                    },
                    {
                        "name": "Chengrui Wang"
                    },
                    {
                        "name": "Rui Wang"
                    },
                    {
                        "name": "Junyu Dong"
                    },
                    {
                        "name": "Yanwei Yu"
                    }
                ],
                "author_detail": {
                    "name": "Yanwei Yu"
                },
                "author": "Yanwei Yu"
            },
            {
                "id": "http://arxiv.org/abs/2601.01046v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.01046v1",
                "title": "KV-Embedding: Training-free Text Embedding via Internal KV Re-routing in Decoder-only LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV-Embedding: Training-free Text Embedding via Internal KV Re-routing in Decoder-only LLMs"
                },
                "updated": "2026-01-03T02:55:43Z",
                "updated_parsed": [
                    2026,
                    1,
                    3,
                    2,
                    55,
                    43,
                    5,
                    3,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.01046v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.01046v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "While LLMs are powerful embedding backbones, their application in training-free settings faces two structural challenges: causal attention restricts early tokens from accessing subsequent context, and the next-token prediction objective biases representations toward generation rather than semantic compression. To address these limitations, we propose KV-Embedding, a framework that activates the latent representation power of frozen LLMs. Our method leverages the observation that the key-value (KV) states of the final token at each layer encode a compressed view of the sequence. By re-routing these states as a prepended prefix, we enable all tokens to access sequence-level context within a single forward pass. To ensure model-agnostic applicability, we introduce an automated layer selection strategy based on intrinsic dimensionality. Evaluations on MTEB across Qwen, Mistral, and Llama backbones show that KV-Embedding outperforms existing training-free baselines by up to 10%, while maintaining robust performance on sequences up to 4,096 tokens. These results demonstrate that internal state manipulation offers an efficient alternative to input modification, and we hope this work encourages further exploration of LLM internals for representation learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While LLMs are powerful embedding backbones, their application in training-free settings faces two structural challenges: causal attention restricts early tokens from accessing subsequent context, and the next-token prediction objective biases representations toward generation rather than semantic compression. To address these limitations, we propose KV-Embedding, a framework that activates the latent representation power of frozen LLMs. Our method leverages the observation that the key-value (KV) states of the final token at each layer encode a compressed view of the sequence. By re-routing these states as a prepended prefix, we enable all tokens to access sequence-level context within a single forward pass. To ensure model-agnostic applicability, we introduce an automated layer selection strategy based on intrinsic dimensionality. Evaluations on MTEB across Qwen, Mistral, and Llama backbones show that KV-Embedding outperforms existing training-free baselines by up to 10%, while maintaining robust performance on sequences up to 4,096 tokens. These results demonstrate that internal state manipulation offers an efficient alternative to input modification, and we hope this work encourages further exploration of LLM internals for representation learning."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-03T02:55:43Z",
                "published_parsed": [
                    2026,
                    1,
                    3,
                    2,
                    55,
                    43,
                    5,
                    3,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Yixuan Tang"
                    },
                    {
                        "name": "Yi Yang"
                    }
                ],
                "author_detail": {
                    "name": "Yi Yang"
                },
                "author": "Yi Yang"
            },
            {
                "id": "http://arxiv.org/abs/2512.09238v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.09238v2",
                "title": "Training-free Context-adaptive Attention for Efficient Long Context Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training-free Context-adaptive Attention for Efficient Long Context Modeling"
                },
                "updated": "2026-01-02T12:55:29Z",
                "updated_parsed": [
                    2026,
                    1,
                    2,
                    12,
                    55,
                    29,
                    4,
                    2,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.09238v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.09238v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across a wide range of natural language processing tasks. These capabilities stem primarily from the self-attention mechanism, which enables modeling of long-range dependencies. However, the quadratic complexity of self-attention with respect to sequence length poses significant computational and memory challenges, especially as sequence length extends to extremes. While various sparse attention and KV cache compression methods have been proposed to improve efficiency, they often suffer from limitations such as reliance on fixed patterns, inability to handle both prefilling and decoding stages, or the requirement for additional training. In this paper, we propose Training-free Context-adaptive Attention (TCA-Attention), a training-free sparse attention mechanism that selectively attends to only the informative tokens for efficient long-context inference. Our method consists of two lightweight phases: i) an offline calibration phase that determines head-specific sparsity budgets via a single forward pass, and ii) an online token selection phase that adaptively retains core context tokens using a lightweight redundancy metric. TCA-Attention provides a unified solution that accelerates both prefilling and decoding while reducing KV cache memory footprint, without requiring parameter updates or architectural changes. Theoretical analysis shows that our approach maintains bounded approximation error. Extensive experiments demonstrate that TCA-Attention achieves a 2.8$\\times$ speedup and reduces KV cache by 61% at 128K context length while maintaining performance comparable to full attention across various benchmarks, offering a practical plug-and-play solution for efficient long-context inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities across a wide range of natural language processing tasks. These capabilities stem primarily from the self-attention mechanism, which enables modeling of long-range dependencies. However, the quadratic complexity of self-attention with respect to sequence length poses significant computational and memory challenges, especially as sequence length extends to extremes. While various sparse attention and KV cache compression methods have been proposed to improve efficiency, they often suffer from limitations such as reliance on fixed patterns, inability to handle both prefilling and decoding stages, or the requirement for additional training. In this paper, we propose Training-free Context-adaptive Attention (TCA-Attention), a training-free sparse attention mechanism that selectively attends to only the informative tokens for efficient long-context inference. Our method consists of two lightweight phases: i) an offline calibration phase that determines head-specific sparsity budgets via a single forward pass, and ii) an online token selection phase that adaptively retains core context tokens using a lightweight redundancy metric. TCA-Attention provides a unified solution that accelerates both prefilling and decoding while reducing KV cache memory footprint, without requiring parameter updates or architectural changes. Theoretical analysis shows that our approach maintains bounded approximation error. Extensive experiments demonstrate that TCA-Attention achieves a 2.8$\\times$ speedup and reduces KV cache by 61% at 128K context length while maintaining performance comparable to full attention across various benchmarks, offering a practical plug-and-play solution for efficient long-context inference."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-10T01:54:57Z",
                "published_parsed": [
                    2025,
                    12,
                    10,
                    1,
                    54,
                    57,
                    2,
                    344,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Zeng You"
                    },
                    {
                        "name": "Yaofo Chen"
                    },
                    {
                        "name": "Shuhai Zhang"
                    },
                    {
                        "name": "Zhijie Qiu"
                    },
                    {
                        "name": "Tingyu Wu"
                    },
                    {
                        "name": "Yingjian Li"
                    },
                    {
                        "name": "Yaowei Wang"
                    },
                    {
                        "name": "Mingkui Tan"
                    }
                ],
                "author_detail": {
                    "name": "Mingkui Tan"
                },
                "author": "Mingkui Tan"
            },
            {
                "id": "http://arxiv.org/abs/2601.00456v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.00456v1",
                "title": "ROBIN: Incremental Oblique Interleaved ECC for Reliability Improvement in STT-MRAM Caches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ROBIN: Incremental Oblique Interleaved ECC for Reliability Improvement in STT-MRAM Caches"
                },
                "updated": "2026-01-01T19:45:12Z",
                "updated_parsed": [
                    2026,
                    1,
                    1,
                    19,
                    45,
                    12,
                    3,
                    1,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.00456v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.00456v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Spin-Transfer Torque Magnetic RAM} (STT-MRAM) is a promising alternative for SRAMs in on-chip cache memories. Besides all its advantages, high error rate in STT-MRAM is a major limiting factor for on-chip cache memories. In this paper, we first present a comprehensive analysis that reveals that the conventional Error-Correcting Codes (ECCs) lose their efficiency due to data-dependent error patterns, and then propose an efficient ECC configuration, so-called ROBIN, to improve the correction capability. The evaluations show that the inefficiency of conventional ECC increases the cache error rate by an average of 151.7% while ROBIN reduces this value by more than 28.6x.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spin-Transfer Torque Magnetic RAM} (STT-MRAM) is a promising alternative for SRAMs in on-chip cache memories. Besides all its advantages, high error rate in STT-MRAM is a major limiting factor for on-chip cache memories. In this paper, we first present a comprehensive analysis that reveals that the conventional Error-Correcting Codes (ECCs) lose their efficiency due to data-dependent error patterns, and then propose an efficient ECC configuration, so-called ROBIN, to improve the correction capability. The evaluations show that the inefficiency of conventional ECC increases the cache error rate by an average of 151.7% while ROBIN reduces this value by more than 28.6x."
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-01T19:45:12Z",
                "published_parsed": [
                    2026,
                    1,
                    1,
                    19,
                    45,
                    12,
                    3,
                    1,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR"
                },
                "authors": [
                    {
                        "name": "Elham Cheshmikhani"
                    },
                    {
                        "name": "Hamed Farbeh"
                    },
                    {
                        "name": "Hossein Asadi"
                    }
                ],
                "author_detail": {
                    "name": "Hossein Asadi"
                },
                "author": "Hossein Asadi"
            },
            {
                "id": "http://arxiv.org/abs/2601.00450v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.00450v1",
                "title": "Enhancing Reliability of STT-MRAM Caches by Eliminating Read Disturbance Accumulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Reliability of STT-MRAM Caches by Eliminating Read Disturbance Accumulation"
                },
                "updated": "2026-01-01T19:22:51Z",
                "updated_parsed": [
                    2026,
                    1,
                    1,
                    19,
                    22,
                    51,
                    3,
                    1,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.00450v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.00450v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Spin-Transfer Torque Magnetic RAM (STT-MRAM) as one of the most promising replacements for SRAMs in on-chip cache memories benefits from higher density and scalability, near-zero leakage power, and non-volatility, but its reliability is threatened by high read disturbance error rate. Error-Correcting Codes (ECCs) are conventionally suggested to overcome the read disturbance errors in STT-MRAM caches. By employing aggressive ECCs and checking out a cache block on every read access, a high level of cache reliability is achieved. However, to minimize the cache access time in modern processors, all blocks in the target cache set are simultaneously read in parallel for tags comparison operation and only the requested block is sent out, if any, after checking its ECC. These extra cache block reads without checking their ECCs until requesting the blocks by the processor cause the accumulation of read disturbance error, which significantly degrade the cache reliability. In this paper, we first introduce and formulate the read disturbance accumulation phenomenon and reveal that this accumulation due to conventional parallel accesses of cache blocks significantly increases the cache error rate. Then, we propose a simple yet effective scheme, so-called Read Error Accumulation Preventer cache (REAP-cache), to completely eliminate the accumulation of read disturbances without compromising the cache performance. Our evaluations show that the proposed REAP-cache extends the cache Mean Time To Failure (MTTF) by 171x, while increases the cache area by less than 1% and energy consumption by only 2.7%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spin-Transfer Torque Magnetic RAM (STT-MRAM) as one of the most promising replacements for SRAMs in on-chip cache memories benefits from higher density and scalability, near-zero leakage power, and non-volatility, but its reliability is threatened by high read disturbance error rate. Error-Correcting Codes (ECCs) are conventionally suggested to overcome the read disturbance errors in STT-MRAM caches. By employing aggressive ECCs and checking out a cache block on every read access, a high level of cache reliability is achieved. However, to minimize the cache access time in modern processors, all blocks in the target cache set are simultaneously read in parallel for tags comparison operation and only the requested block is sent out, if any, after checking its ECC. These extra cache block reads without checking their ECCs until requesting the blocks by the processor cause the accumulation of read disturbance error, which significantly degrade the cache reliability. In this paper, we first introduce and formulate the read disturbance accumulation phenomenon and reveal that this accumulation due to conventional parallel accesses of cache blocks significantly increases the cache error rate. Then, we propose a simple yet effective scheme, so-called Read Error Accumulation Preventer cache (REAP-cache), to completely eliminate the accumulation of read disturbances without compromising the cache performance. Our evaluations show that the proposed REAP-cache extends the cache Mean Time To Failure (MTTF) by 171x, while increases the cache area by less than 1% and energy consumption by only 2.7%."
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-01T19:22:51Z",
                "published_parsed": [
                    2026,
                    1,
                    1,
                    19,
                    22,
                    51,
                    3,
                    1,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR"
                },
                "authors": [
                    {
                        "name": "Elham Cheshmikhani"
                    },
                    {
                        "name": "Hamed Farbeh"
                    },
                    {
                        "name": "Hossein Asadi"
                    }
                ],
                "author_detail": {
                    "name": "Hossein Asadi"
                },
                "author": "Hossein Asadi"
            },
            {
                "id": "http://arxiv.org/abs/2507.03445v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2507.03445v2",
                "title": "Quantum Algorithm for the Fixed-Radius Neighbor Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum Algorithm for the Fixed-Radius Neighbor Search"
                },
                "updated": "2026-01-01T15:41:16Z",
                "updated_parsed": [
                    2026,
                    1,
                    1,
                    15,
                    41,
                    16,
                    3,
                    1,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2507.03445v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2507.03445v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Neighbor search is a computationally demanding problem, usually both time- and memory-consuming. The main problem of this kind of algorithms is the long execution time due to cache misses. In this work, we propose a quantum algorithm for the Fixed RAdius Neighbor Search problem (FRANS) based on the fixed-point version of Grover's algorithm. We propose an efficient circuit for solving the FRANS with linear query complexity with the number of particles $N$. The quantum circuit returns the list of all the neighbors' pairs within the fixed radius, together with their distance, avoiding the slow down given by cache miss.\n  We analyzed the gate and the query complexity of the circuit. Our FRANS algorithm presents a query complexity of $\\mathcal{O}(N/\\sqrt{M})$, where $M$ is the number of solutions, reaching the optimal lower bound of the Grover's algorithm. We propose different implementations of the oracle, which must be chosen depending on the precise structure of the database. Among these, we present an implementation using the Chebyshev distance with depth $\\mathcal{O}(q_1)$, where $2^{q_1}$ is the number of grid points used to discretize a spatial dimension. State-of-the-art algorithms for state preparation allow for a trade-off between depth and width of the circuit, with a volume (depth$\\times$ width) of $\\mathcal{O}(N\\log(N))$. This unfavorable scaling can be brought down to $\\mathcal{O}(\\text{poly}(\\log N))$ in case of structured datasets. We proposed a stopping criterion based on Bayes interference and tested its validity on $1D$ simulations. Finally, we accounted for the readout complexity and assessed the resilience of the model to the readout error, suggesting an error correction-free strategy to check the accuracy of the results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neighbor search is a computationally demanding problem, usually both time- and memory-consuming. The main problem of this kind of algorithms is the long execution time due to cache misses. In this work, we propose a quantum algorithm for the Fixed RAdius Neighbor Search problem (FRANS) based on the fixed-point version of Grover's algorithm. We propose an efficient circuit for solving the FRANS with linear query complexity with the number of particles $N$. The quantum circuit returns the list of all the neighbors' pairs within the fixed radius, together with their distance, avoiding the slow down given by cache miss.\n  We analyzed the gate and the query complexity of the circuit. Our FRANS algorithm presents a query complexity of $\\mathcal{O}(N/\\sqrt{M})$, where $M$ is the number of solutions, reaching the optimal lower bound of the Grover's algorithm. We propose different implementations of the oracle, which must be chosen depending on the precise structure of the database. Among these, we present an implementation using the Chebyshev distance with depth $\\mathcal{O}(q_1)$, where $2^{q_1}$ is the number of grid points used to discretize a spatial dimension. State-of-the-art algorithms for state preparation allow for a trade-off between depth and width of the circuit, with a volume (depth$\\times$ width) of $\\mathcal{O}(N\\log(N))$. This unfavorable scaling can be brought down to $\\mathcal{O}(\\text{poly}(\\log N))$ in case of structured datasets. We proposed a stopping criterion based on Bayes interference and tested its validity on $1D$ simulations. Finally, we accounted for the readout complexity and assessed the resilience of the model to the readout error, suggesting an error correction-free strategy to check the accuracy of the results."
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-07-04T10:01:10Z",
                "published_parsed": [
                    2025,
                    7,
                    4,
                    10,
                    1,
                    10,
                    4,
                    185,
                    0
                ],
                "arxiv_comment": "19 pages, 10 figures, 1 table",
                "arxiv_primary_category": {
                    "term": "quant-ph"
                },
                "authors": [
                    {
                        "name": "Luca Cappelli"
                    },
                    {
                        "name": "Claudio Sanavio"
                    },
                    {
                        "name": "Alessandro Andrea Zecchi"
                    },
                    {
                        "name": "Giuseppe Murante"
                    },
                    {
                        "name": "Sauro Succi"
                    }
                ],
                "author_detail": {
                    "name": "Sauro Succi"
                },
                "author": "Sauro Succi"
            },
            {
                "id": "http://arxiv.org/abs/2505.15683v4",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2505.15683v4",
                "title": "FedSEA-LLaMA: A Secure, Efficient and Adaptive Federated Splitting Framework for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FedSEA-LLaMA: A Secure, Efficient and Adaptive Federated Splitting Framework for Large Language Models"
                },
                "updated": "2026-01-01T05:45:17Z",
                "updated_parsed": [
                    2026,
                    1,
                    1,
                    5,
                    45,
                    17,
                    3,
                    1,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2505.15683v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2505.15683v4",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Private data holds promise for improving LLMs due to its high quality, but its scattered distribution across data silos and the high computational demands of LLMs limit their deployment in federated environments. To address this, the transformer-based federated split models are proposed, which offload most model parameters to the server (or distributed clients) while retaining only a small portion on the client to ensure data privacy. Despite this design, they still face three challenges: 1) Peer-to-peer key encryption struggles to secure transmitted vectors effectively; 2) The auto-regressive nature of LLMs means that federated split learning can only train and infer sequentially, causing high communication overhead; 3) Fixed partition points lack adaptability to downstream tasks. In this paper, we introduce FedSEA-LLaMA, a Secure, Efficient, and Adaptive Federated splitting framework based on LLaMA2. First, we inject Gaussian noise into forward-pass hidden states to enable secure end-to-end vector transmission. Second, we employ attention-mask compression and KV cache collaboration to reduce communication costs, accelerating training and inference. Third, we allow users to dynamically adjust the partition points for input/output blocks based on specific task requirements. Experiments on natural language understanding, summarization, and conversational QA tasks show that FedSEA-LLaMA maintains performance comparable to centralized LLaMA2 and achieves up to 8x speedups in training and inference. Further analysis of privacy attacks and different partition points also demonstrates the effectiveness of FedSEA-LLaMA in security and adaptability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Private data holds promise for improving LLMs due to its high quality, but its scattered distribution across data silos and the high computational demands of LLMs limit their deployment in federated environments. To address this, the transformer-based federated split models are proposed, which offload most model parameters to the server (or distributed clients) while retaining only a small portion on the client to ensure data privacy. Despite this design, they still face three challenges: 1) Peer-to-peer key encryption struggles to secure transmitted vectors effectively; 2) The auto-regressive nature of LLMs means that federated split learning can only train and infer sequentially, causing high communication overhead; 3) Fixed partition points lack adaptability to downstream tasks. In this paper, we introduce FedSEA-LLaMA, a Secure, Efficient, and Adaptive Federated splitting framework based on LLaMA2. First, we inject Gaussian noise into forward-pass hidden states to enable secure end-to-end vector transmission. Second, we employ attention-mask compression and KV cache collaboration to reduce communication costs, accelerating training and inference. Third, we allow users to dynamically adjust the partition points for input/output blocks based on specific task requirements. Experiments on natural language understanding, summarization, and conversational QA tasks show that FedSEA-LLaMA maintains performance comparable to centralized LLaMA2 and achieves up to 8x speedups in training and inference. Further analysis of privacy attacks and different partition points also demonstrates the effectiveness of FedSEA-LLaMA in security and adaptability."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-05-21T15:58:08Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    15,
                    58,
                    8,
                    2,
                    141,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Zishuai Zhang"
                    },
                    {
                        "name": "Hainan zhang"
                    },
                    {
                        "name": "Weihua Li"
                    },
                    {
                        "name": "Qinnan zhang"
                    },
                    {
                        "name": "jin Dong"
                    },
                    {
                        "name": "Yongxin Tong"
                    },
                    {
                        "name": "Zhiming Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zhiming Zheng"
                },
                "author": "Zhiming Zheng"
            },
            {
                "id": "http://arxiv.org/abs/2506.01928v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2506.01928v2",
                "title": "Esoteric Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Esoteric Language Models"
                },
                "updated": "2025-12-31T19:08:07Z",
                "updated_parsed": [
                    2025,
                    12,
                    31,
                    19,
                    8,
                    7,
                    2,
                    365,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2506.01928v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2506.01928v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Diffusion-based language models offer a compelling alternative to autoregressive (AR) models by enabling parallel and controllable generation. Within this family, Masked Diffusion Models (MDMs) currently perform best but still underperform AR models in perplexity and lack key inference-time efficiency features, most notably KV caching. We introduce Eso-LMs, a new family of models that fuses AR and MDM paradigms, smoothly interpolating between their perplexities while overcoming their respective limitations. Unlike prior work, which uses transformers with bidirectional attention as MDM denoisers, we exploit the connection between MDMs and Any-Order autoregressive models and adopt causal attention. This design lets us compute the exact likelihood of MDMs for the first time and, crucially, enables us \\to introduce KV caching for MDMs while preserving parallel generation for the first time, significantly improving inference efficiency. Combined with an optimized sampling schedule, Eso-LMs achieves a new state of the art on the speed-quality Pareto frontier for unconditional generation. On long contexts, it yields $\\mathbf{14 - 65{}\\times}$ faster inference than standard MDMs and $\\mathbf{3 - 4{}\\times}$ faster inference than prior semi-autoregressive approaches. We provide code, model checkpoints, and video tutorials on the project page: http://s-sahoo.github.io/Eso-LMs",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based language models offer a compelling alternative to autoregressive (AR) models by enabling parallel and controllable generation. Within this family, Masked Diffusion Models (MDMs) currently perform best but still underperform AR models in perplexity and lack key inference-time efficiency features, most notably KV caching. We introduce Eso-LMs, a new family of models that fuses AR and MDM paradigms, smoothly interpolating between their perplexities while overcoming their respective limitations. Unlike prior work, which uses transformers with bidirectional attention as MDM denoisers, we exploit the connection between MDMs and Any-Order autoregressive models and adopt causal attention. This design lets us compute the exact likelihood of MDMs for the first time and, crucially, enables us \\to introduce KV caching for MDMs while preserving parallel generation for the first time, significantly improving inference efficiency. Combined with an optimized sampling schedule, Eso-LMs achieves a new state of the art on the speed-quality Pareto frontier for unconditional generation. On long contexts, it yields $\\mathbf{14 - 65{}\\times}$ faster inference than standard MDMs and $\\mathbf{3 - 4{}\\times}$ faster inference than prior semi-autoregressive approaches. We provide code, model checkpoints, and video tutorials on the project page: http://s-sahoo.github.io/Eso-LMs"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-06-02T17:47:27Z",
                "published_parsed": [
                    2025,
                    6,
                    2,
                    17,
                    47,
                    27,
                    0,
                    153,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Subham Sekhar Sahoo"
                    },
                    {
                        "name": "Zhihan Yang"
                    },
                    {
                        "name": "Yash Akhauri"
                    },
                    {
                        "name": "Johnna Liu"
                    },
                    {
                        "name": "Deepansha Singh"
                    },
                    {
                        "name": "Zhoujun Cheng"
                    },
                    {
                        "name": "Zhengzhong Liu"
                    },
                    {
                        "name": "Eric Xing"
                    },
                    {
                        "name": "John Thickstun"
                    },
                    {
                        "name": "Arash Vahdat"
                    }
                ],
                "author_detail": {
                    "name": "Arash Vahdat"
                },
                "author": "Arash Vahdat"
            },
            {
                "id": "http://arxiv.org/abs/2512.25065v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.25065v1",
                "title": "Vulcan: Instance-Optimal Systems Heuristics Through LLM-Driven Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vulcan: Instance-Optimal Systems Heuristics Through LLM-Driven Search"
                },
                "updated": "2025-12-31T18:58:19Z",
                "updated_parsed": [
                    2025,
                    12,
                    31,
                    18,
                    58,
                    19,
                    2,
                    365,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.25065v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.25065v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Resource-management tasks in modern operating and distributed systems continue to rely primarily on hand-designed heuristics for tasks such as scheduling, caching, or active queue management. Designing performant heuristics is an expensive, time-consuming process that we are forced to continuously go through due to the constant flux of hardware, workloads and environments.\n  We propose a new alternative: synthesizing instance-optimal heuristics -- specialized for the exact workloads and hardware where they will be deployed -- using code-generating large language models (LLMs). To make this synthesis tractable, Vulcan separates policy and mechanism through LLM-friendly, task-agnostic interfaces. With these interfaces, users specify the inputs and objectives of their desired policy, while Vulcan searches for performant policies via evolutionary search over LLM-generated code. This interface is expressive enough to capture a wide range of system policies, yet sufficiently constrained to allow even small, inexpensive LLMs to generate correct and executable code.\n  We use Vulcan to synthesize performant heuristics for cache eviction and memory tiering, and find that these heuristics outperform all human-designed state-of-the-art algorithms by upto 69% and 7.9% in performance for each of these tasks respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Resource-management tasks in modern operating and distributed systems continue to rely primarily on hand-designed heuristics for tasks such as scheduling, caching, or active queue management. Designing performant heuristics is an expensive, time-consuming process that we are forced to continuously go through due to the constant flux of hardware, workloads and environments.\n  We propose a new alternative: synthesizing instance-optimal heuristics -- specialized for the exact workloads and hardware where they will be deployed -- using code-generating large language models (LLMs). To make this synthesis tractable, Vulcan separates policy and mechanism through LLM-friendly, task-agnostic interfaces. With these interfaces, users specify the inputs and objectives of their desired policy, while Vulcan searches for performant policies via evolutionary search over LLM-generated code. This interface is expressive enough to capture a wide range of system policies, yet sufficiently constrained to allow even small, inexpensive LLMs to generate correct and executable code.\n  We use Vulcan to synthesize performant heuristics for cache eviction and memory tiering, and find that these heuristics outperform all human-designed state-of-the-art algorithms by upto 69% and 7.9% in performance for each of these tasks respectively."
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-31T18:58:19Z",
                "published_parsed": [
                    2025,
                    12,
                    31,
                    18,
                    58,
                    19,
                    2,
                    365,
                    0
                ],
                "arxiv_comment": "27 pages, 11 figures, 7 tables",
                "arxiv_primary_category": {
                    "term": "cs.OS"
                },
                "authors": [
                    {
                        "name": "Rohit Dwivedula"
                    },
                    {
                        "name": "Divyanshu Saxena"
                    },
                    {
                        "name": "Sujay Yadalam"
                    },
                    {
                        "name": "Daehyeok Kim"
                    },
                    {
                        "name": "Aditya Akella"
                    }
                ],
                "author_detail": {
                    "name": "Aditya Akella"
                },
                "author": "Aditya Akella"
            },
            {
                "id": "http://arxiv.org/abs/2410.00414v4",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2410.00414v4",
                "title": "Semantic Parsing with Candidate Expressions for Knowledge Base Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic Parsing with Candidate Expressions for Knowledge Base Question Answering"
                },
                "updated": "2025-12-31T18:45:49Z",
                "updated_parsed": [
                    2025,
                    12,
                    31,
                    18,
                    45,
                    49,
                    2,
                    365,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2410.00414v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2410.00414v4",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.1016/j.eswa.2025.130564",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "Semantic parsers convert natural language to logical forms, which can be evaluated on knowledge bases (KBs) to produce denotations. Recent semantic parsers have been developed with sequence-to-sequence (seq2seq) pre-trained language models (PLMs) or large language models, where the models treat logical forms as sequences of tokens. For syntactic and semantic validity, the semantic parsers use grammars that enable constrained decoding. However, the grammars lack the ability to utilize large information of KBs, although logical forms contain representations of KB elements, such as entities or relations. In this work, we propose a grammar augmented with candidate expressions for semantic parsing on a large KB with a seq2seq PLM. The grammar defines actions as production rules, and our semantic parser predicts actions during inference under the constraints by types and candidate expressions. We apply the grammar to knowledge base question answering, where the constraints by candidate expressions assist a semantic parser to generate valid KB elements. We also introduce two special rules, sub-type inference and union types, and a mask caching algorithm. In particular, sub-type inference and the mask caching algorithm greatly increase the decoding speed of our semantic parser. We experimented on two benchmarks, KQA Pro and Overnight, where the constraints by candidate expressions increased the accuracy of our semantic parser, whether it was trained with strong supervision or weak supervision. In addition, our semantic parser had a fast decoding speed in the experiments. Our source code is publicly available at https://github.com/daehwannam/candexpr-sp.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic parsers convert natural language to logical forms, which can be evaluated on knowledge bases (KBs) to produce denotations. Recent semantic parsers have been developed with sequence-to-sequence (seq2seq) pre-trained language models (PLMs) or large language models, where the models treat logical forms as sequences of tokens. For syntactic and semantic validity, the semantic parsers use grammars that enable constrained decoding. However, the grammars lack the ability to utilize large information of KBs, although logical forms contain representations of KB elements, such as entities or relations. In this work, we propose a grammar augmented with candidate expressions for semantic parsing on a large KB with a seq2seq PLM. The grammar defines actions as production rules, and our semantic parser predicts actions during inference under the constraints by types and candidate expressions. We apply the grammar to knowledge base question answering, where the constraints by candidate expressions assist a semantic parser to generate valid KB elements. We also introduce two special rules, sub-type inference and union types, and a mask caching algorithm. In particular, sub-type inference and the mask caching algorithm greatly increase the decoding speed of our semantic parser. We experimented on two benchmarks, KQA Pro and Overnight, where the constraints by candidate expressions increased the accuracy of our semantic parser, whether it was trained with strong supervision or weak supervision. In addition, our semantic parser had a fast decoding speed in the experiments. Our source code is publicly available at https://github.com/daehwannam/candexpr-sp.git."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2024-10-01T05:46:22Z",
                "published_parsed": [
                    2024,
                    10,
                    1,
                    5,
                    46,
                    22,
                    1,
                    275,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "arxiv_journal_ref": "Expert Syst. Appl. 306 (2026) 130564",
                "authors": [
                    {
                        "name": "Daehwan Nam"
                    },
                    {
                        "name": "Gary Geunbae Lee"
                    }
                ],
                "author_detail": {
                    "name": "Gary Geunbae Lee"
                },
                "author": "Gary Geunbae Lee",
                "arxiv_doi": "10.1016/j.eswa.2025.130564"
            },
            {
                "id": "http://arxiv.org/abs/2512.24902v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.24902v1",
                "title": "Adaptive Resource Orchestration for Distributed Quantum Computing Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Resource Orchestration for Distributed Quantum Computing Systems"
                },
                "updated": "2025-12-31T14:58:05Z",
                "updated_parsed": [
                    2025,
                    12,
                    31,
                    14,
                    58,
                    5,
                    2,
                    365,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.24902v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.24902v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Scaling quantum computing beyond a single device requires networking many quantum processing units (QPUs) into a coherent quantum-HPC system. We propose the Modular Entanglement Hub (ModEn-Hub) architecture: a hub-and-spoke photonic interconnect paired with a real-time quantum network orchestrator. ModEn-Hub centralizes entanglement sources and shared quantum memory to deliver on-demand, high-fidelity Bell pairs across heterogeneous QPUs, while the control plane schedules teleportation-based non-local gates, launches parallel entanglement attempts, and maintains a small ebit cache. To quantify benefits, we implement a lightweight, reproducible Monte Carlo study under realistic loss and tight round budgets, comparing a naive sequential baseline to an orchestrated policy with logarithmically scaled parallelism and opportunistic caching. Across 1-128 QPUs and 2,500 trials per point, ModEn-Hub-style orchestration sustains about 90% teleportation success while the baseline degrades toward about 30%, at the cost of higher average entanglement attempts (about 10-12 versus about 3). These results provide clear, high-level evidence that adaptive resource orchestration in the ModEn-Hub enables scalable and efficient quantum-HPC operation on near-term hardware.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling quantum computing beyond a single device requires networking many quantum processing units (QPUs) into a coherent quantum-HPC system. We propose the Modular Entanglement Hub (ModEn-Hub) architecture: a hub-and-spoke photonic interconnect paired with a real-time quantum network orchestrator. ModEn-Hub centralizes entanglement sources and shared quantum memory to deliver on-demand, high-fidelity Bell pairs across heterogeneous QPUs, while the control plane schedules teleportation-based non-local gates, launches parallel entanglement attempts, and maintains a small ebit cache. To quantify benefits, we implement a lightweight, reproducible Monte Carlo study under realistic loss and tight round budgets, comparing a naive sequential baseline to an orchestrated policy with logarithmically scaled parallelism and opportunistic caching. Across 1-128 QPUs and 2,500 trials per point, ModEn-Hub-style orchestration sustains about 90% teleportation success while the baseline degrades toward about 30%, at the cost of higher average entanglement attempts (about 10-12 versus about 3). These results provide clear, high-level evidence that adaptive resource orchestration in the ModEn-Hub enables scalable and efficient quantum-HPC operation on near-term hardware."
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-31T14:58:05Z",
                "published_parsed": [
                    2025,
                    12,
                    31,
                    14,
                    58,
                    5,
                    2,
                    365,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph"
                },
                "authors": [
                    {
                        "name": "Kuan-Cheng Chen"
                    },
                    {
                        "name": "Felix Burt"
                    },
                    {
                        "name": "Nitish K. Panigrahy"
                    },
                    {
                        "name": "Kin K. Leung"
                    }
                ],
                "author_detail": {
                    "name": "Kin K. Leung"
                },
                "author": "Kin K. Leung"
            },
            {
                "id": "http://arxiv.org/abs/2512.24711v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.24711v1",
                "title": "MEIC-DT: Memory-Efficient Incremental Clustering for Long-Text Coreference Resolution with Dual-Threshold Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MEIC-DT: Memory-Efficient Incremental Clustering for Long-Text Coreference Resolution with Dual-Threshold Constraints"
                },
                "updated": "2025-12-31T08:26:34Z",
                "updated_parsed": [
                    2025,
                    12,
                    31,
                    8,
                    26,
                    34,
                    2,
                    365,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.24711v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.24711v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "In the era of large language models (LLMs), supervised neural methods remain the state-of-the-art (SOTA) for Coreference Resolution. Yet, their full potential is underexplored, particularly in incremental clustering, which faces the critical challenge of balancing efficiency with performance for long texts. To address the limitation, we propose \\textbf{MEIC-DT}, a novel dual-threshold, memory-efficient incremental clustering approach based on a lightweight Transformer. MEIC-DT features a dual-threshold constraint mechanism designed to precisely control the Transformer's input scale within a predefined memory budget. This mechanism incorporates a Statistics-Aware Eviction Strategy (\\textbf{SAES}), which utilizes distinct statistical profiles from the training and inference phases for intelligent cache management. Furthermore, we introduce an Internal Regularization Policy (\\textbf{IRP}) that strategically condenses clusters by selecting the most representative mentions, thereby preserving semantic integrity. Extensive experiments on common benchmarks demonstrate that MEIC-DT achieves highly competitive coreference performance under stringent memory constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the era of large language models (LLMs), supervised neural methods remain the state-of-the-art (SOTA) for Coreference Resolution. Yet, their full potential is underexplored, particularly in incremental clustering, which faces the critical challenge of balancing efficiency with performance for long texts. To address the limitation, we propose \\textbf{MEIC-DT}, a novel dual-threshold, memory-efficient incremental clustering approach based on a lightweight Transformer. MEIC-DT features a dual-threshold constraint mechanism designed to precisely control the Transformer's input scale within a predefined memory budget. This mechanism incorporates a Statistics-Aware Eviction Strategy (\\textbf{SAES}), which utilizes distinct statistical profiles from the training and inference phases for intelligent cache management. Furthermore, we introduce an Internal Regularization Policy (\\textbf{IRP}) that strategically condenses clusters by selecting the most representative mentions, thereby preserving semantic integrity. Extensive experiments on common benchmarks demonstrate that MEIC-DT achieves highly competitive coreference performance under stringent memory constraints."
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-31T08:26:34Z",
                "published_parsed": [
                    2025,
                    12,
                    31,
                    8,
                    26,
                    34,
                    2,
                    365,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR"
                },
                "authors": [
                    {
                        "name": "Kangyang Luo"
                    },
                    {
                        "name": "Shuzheng Si"
                    },
                    {
                        "name": "Yuzhuo Bai"
                    },
                    {
                        "name": "Cheng Gao"
                    },
                    {
                        "name": "Zhitong Wang"
                    },
                    {
                        "name": "Cheng Huang"
                    },
                    {
                        "name": "Yingli Shen"
                    },
                    {
                        "name": "Yufeng Han"
                    },
                    {
                        "name": "Wenhao Li"
                    },
                    {
                        "name": "Cunliang Kong"
                    },
                    {
                        "name": "Maosong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Maosong Sun"
                },
                "author": "Maosong Sun"
            },
            {
                "id": "http://arxiv.org/abs/2510.13940v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.13940v2",
                "title": "Less is More: Improving LLM Reasoning with Minimal Test-Time Intervention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Less is More: Improving LLM Reasoning with Minimal Test-Time Intervention"
                },
                "updated": "2025-12-31T07:36:24Z",
                "updated_parsed": [
                    2025,
                    12,
                    31,
                    7,
                    36,
                    24,
                    2,
                    365,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.13940v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.13940v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Recent progress in large language models (LLMs) has focused on test-time scaling to improve reasoning via increased inference computation, but often at the cost of efficiency. We revisit test-time behavior and uncover a simple yet underexplored phenomenon: reasoning uncertainty is highly localized-only a small subset of high-entropy tokens dominantly affects output correctness. Motivated by this, we propose Minimal Test-Time Intervention (MTI), a training-free framework that enhances reasoning accuracy and stability with minimal overhead. MTI includes: (i) Selective CFG intervention, applying classifier-free guidance only at uncertain positions; and (ii) Lightweight negative-prompt guidance, reusing the main model's KV cache to approximate unconditional decoding efficiently. MTI yields consistent gains across general, coding, and STEM tasks-e.g., +9.28% average improvement on six benchmarks for DeepSeek-R1-7B and +11.25% on AIME2024 using Ling-mini-2.0-while remaining highly efficient.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent progress in large language models (LLMs) has focused on test-time scaling to improve reasoning via increased inference computation, but often at the cost of efficiency. We revisit test-time behavior and uncover a simple yet underexplored phenomenon: reasoning uncertainty is highly localized-only a small subset of high-entropy tokens dominantly affects output correctness. Motivated by this, we propose Minimal Test-Time Intervention (MTI), a training-free framework that enhances reasoning accuracy and stability with minimal overhead. MTI includes: (i) Selective CFG intervention, applying classifier-free guidance only at uncertain positions; and (ii) Lightweight negative-prompt guidance, reusing the main model's KV cache to approximate unconditional decoding efficiently. MTI yields consistent gains across general, coding, and STEM tasks-e.g., +9.28% average improvement on six benchmarks for DeepSeek-R1-7B and +11.25% on AIME2024 using Ling-mini-2.0-while remaining highly efficient."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-15T17:59:45Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    17,
                    59,
                    45,
                    2,
                    288,
                    0
                ],
                "arxiv_comment": "Code: https://github.com/EnVision-Research/MTI",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Zhen Yang"
                    },
                    {
                        "name": "Mingyang Zhang"
                    },
                    {
                        "name": "Feng Chen"
                    },
                    {
                        "name": "Ganggui Ding"
                    },
                    {
                        "name": "Liang Hou"
                    },
                    {
                        "name": "Xin Tao"
                    },
                    {
                        "name": "Pengfei Wan"
                    },
                    {
                        "name": "Ying-Cong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Ying-Cong Chen"
                },
                "author": "Ying-Cong Chen"
            },
            {
                "id": "http://arxiv.org/abs/2512.17298v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.17298v2",
                "title": "ProCache: Constraint-Aware Feature Caching with Selective Computation for Diffusion Transformer Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProCache: Constraint-Aware Feature Caching with Selective Computation for Diffusion Transformer Acceleration"
                },
                "updated": "2025-12-31T06:37:00Z",
                "updated_parsed": [
                    2025,
                    12,
                    31,
                    6,
                    37,
                    0,
                    2,
                    365,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.17298v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.17298v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Diffusion Transformers (DiTs) have achieved state-of-the-art performance in generative modeling, yet their high computational cost hinders real-time deployment. While feature caching offers a promising training-free acceleration solution by exploiting temporal redundancy, existing methods suffer from two key limitations: (1) uniform caching intervals fail to align with the non-uniform temporal dynamics of DiT, and (2) naive feature reuse with excessively large caching intervals can lead to severe error accumulation. In this work, we analyze the evolution of DiT features during denoising and reveal that both feature changes and error propagation are highly time- and depth-varying. Motivated by this, we propose ProCache, a training-free dynamic feature caching framework that addresses these issues via two core components: (i) a constraint-aware caching pattern search module that generates non-uniform activation schedules through offline constrained sampling, tailored to the model's temporal characteristics; and (ii) a selective computation module that selectively computes within deep blocks and high-importance tokens for cached segments to mitigate error accumulation with minimal overhead. Extensive experiments on PixArt-alpha and DiT demonstrate that ProCache achieves up to 1.96x and 2.90x acceleration with negligible quality degradation, significantly outperforming prior caching-based methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiTs) have achieved state-of-the-art performance in generative modeling, yet their high computational cost hinders real-time deployment. While feature caching offers a promising training-free acceleration solution by exploiting temporal redundancy, existing methods suffer from two key limitations: (1) uniform caching intervals fail to align with the non-uniform temporal dynamics of DiT, and (2) naive feature reuse with excessively large caching intervals can lead to severe error accumulation. In this work, we analyze the evolution of DiT features during denoising and reveal that both feature changes and error propagation are highly time- and depth-varying. Motivated by this, we propose ProCache, a training-free dynamic feature caching framework that addresses these issues via two core components: (i) a constraint-aware caching pattern search module that generates non-uniform activation schedules through offline constrained sampling, tailored to the model's temporal characteristics; and (ii) a selective computation module that selectively computes within deep blocks and high-importance tokens for cached segments to mitigate error accumulation with minimal overhead. Extensive experiments on PixArt-alpha and DiT demonstrate that ProCache achieves up to 1.96x and 2.90x acceleration with negligible quality degradation, significantly outperforming prior caching-based methods."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-19T07:27:19Z",
                "published_parsed": [
                    2025,
                    12,
                    19,
                    7,
                    27,
                    19,
                    4,
                    353,
                    0
                ],
                "arxiv_comment": "Accepted for poster presentation at AAAI 2026",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Fanpu Cao"
                    },
                    {
                        "name": "Yaofo Chen"
                    },
                    {
                        "name": "Zeng You"
                    },
                    {
                        "name": "Wei Luo"
                    },
                    {
                        "name": "Cen Chen"
                    }
                ],
                "author_detail": {
                    "name": "Cen Chen"
                },
                "author": "Cen Chen"
            },
            {
                "id": "http://arxiv.org/abs/2512.24449v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.24449v1",
                "title": "PackKV: Reducing KV Cache Memory Footprint through LLM-Aware Lossy Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PackKV: Reducing KV Cache Memory Footprint through LLM-Aware Lossy Compression"
                },
                "updated": "2025-12-30T20:05:32Z",
                "updated_parsed": [
                    2025,
                    12,
                    30,
                    20,
                    5,
                    32,
                    1,
                    364,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.24449v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.24449v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Transformer-based large language models (LLMs) have demonstrated remarkable potential across a wide range of practical applications. However, long-context inference remains a significant challenge due to the substantial memory requirements of the key-value (KV) cache, which can scale to several gigabytes as sequence length and batch size increase. In this paper, we present \\textbf{PackKV}, a generic and efficient KV cache management framework optimized for long-context generation. %, which synergistically supports both latency-critical and throughput-critical inference scenarios. PackKV introduces novel lossy compression techniques specifically tailored to the characteristics of KV cache data, featuring a careful co-design of compression algorithms and system architecture. Our approach is compatible with the dynamically growing nature of the KV cache while preserving high computational efficiency. Experimental results show that, under the same and minimum accuracy drop as state-of-the-art quantization methods, PackKV achieves, on average, \\textbf{153.2}\\% higher memory reduction rate for the K cache and \\textbf{179.6}\\% for the V cache. Furthermore, PackKV delivers extremely high execution throughput, effectively eliminating decompression overhead and accelerating the matrix-vector multiplication operation. Specifically, PackKV achieves an average throughput improvement of \\textbf{75.7}\\% for K and \\textbf{171.7}\\% for V across A100 and RTX Pro 6000 GPUs, compared to cuBLAS matrix-vector multiplication kernels, while demanding less GPU memory bandwidth. Code available on https://github.com/BoJiang03/PackKV",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models (LLMs) have demonstrated remarkable potential across a wide range of practical applications. However, long-context inference remains a significant challenge due to the substantial memory requirements of the key-value (KV) cache, which can scale to several gigabytes as sequence length and batch size increase. In this paper, we present \\textbf{PackKV}, a generic and efficient KV cache management framework optimized for long-context generation. %, which synergistically supports both latency-critical and throughput-critical inference scenarios. PackKV introduces novel lossy compression techniques specifically tailored to the characteristics of KV cache data, featuring a careful co-design of compression algorithms and system architecture. Our approach is compatible with the dynamically growing nature of the KV cache while preserving high computational efficiency. Experimental results show that, under the same and minimum accuracy drop as state-of-the-art quantization methods, PackKV achieves, on average, \\textbf{153.2}\\% higher memory reduction rate for the K cache and \\textbf{179.6}\\% for the V cache. Furthermore, PackKV delivers extremely high execution throughput, effectively eliminating decompression overhead and accelerating the matrix-vector multiplication operation. Specifically, PackKV achieves an average throughput improvement of \\textbf{75.7}\\% for K and \\textbf{171.7}\\% for V across A100 and RTX Pro 6000 GPUs, compared to cuBLAS matrix-vector multiplication kernels, while demanding less GPU memory bandwidth. Code available on https://github.com/BoJiang03/PackKV"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-30T20:05:32Z",
                "published_parsed": [
                    2025,
                    12,
                    30,
                    20,
                    5,
                    32,
                    1,
                    364,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Bo Jiang"
                    },
                    {
                        "name": "Taolue Yang"
                    },
                    {
                        "name": "Youyuan Liu"
                    },
                    {
                        "name": "Xubin He"
                    },
                    {
                        "name": "Sheng Di"
                    },
                    {
                        "name": "Sian Jin"
                    }
                ],
                "author_detail": {
                    "name": "Sian Jin"
                },
                "author": "Sian Jin"
            },
            {
                "id": "http://arxiv.org/abs/2512.24255v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.24255v1",
                "title": "How Would Oblivious Memory Boost Graph Analytics on Trusted Processors?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Would Oblivious Memory Boost Graph Analytics on Trusted Processors?"
                },
                "updated": "2025-12-30T14:28:29Z",
                "updated_parsed": [
                    2025,
                    12,
                    30,
                    14,
                    28,
                    29,
                    1,
                    364,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.24255v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.24255v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Trusted processors provide a way to perform joint computations while preserving data privacy. To overcome the performance degradation caused by data-oblivious algorithms to prevent information leakage, we explore the benefits of oblivious memory (OM) integrated in processors, to which the accesses are unobservable by adversaries. We focus on graph analytics, an important application vulnerable to access-pattern attacks. With a co-design between storage structure and algorithms, our prototype system is 100x faster than baselines given an OM sized around the per-core cache which can be implemented on existing processors with negligible overhead. This gives insights into equipping trusted processors with OM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trusted processors provide a way to perform joint computations while preserving data privacy. To overcome the performance degradation caused by data-oblivious algorithms to prevent information leakage, we explore the benefits of oblivious memory (OM) integrated in processors, to which the accesses are unobservable by adversaries. We focus on graph analytics, an important application vulnerable to access-pattern attacks. With a co-design between storage structure and algorithms, our prototype system is 100x faster than baselines given an OM sized around the per-core cache which can be implemented on existing processors with negligible overhead. This gives insights into equipping trusted processors with OM."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-30T14:28:29Z",
                "published_parsed": [
                    2025,
                    12,
                    30,
                    14,
                    28,
                    29,
                    1,
                    364,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "authors": [
                    {
                        "name": "Jiping Yu"
                    },
                    {
                        "name": "Xiaowei Zhu"
                    },
                    {
                        "name": "Kun Chen"
                    },
                    {
                        "name": "Guanyu Feng"
                    },
                    {
                        "name": "Yunyi Chen"
                    },
                    {
                        "name": "Xiaoyu Fan"
                    },
                    {
                        "name": "Wenguang Chen"
                    }
                ],
                "author_detail": {
                    "name": "Wenguang Chen"
                },
                "author": "Wenguang Chen"
            },
            {
                "id": "http://arxiv.org/abs/2512.24229v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.24229v1",
                "title": "High-Performance KV$_3$Sb$_5$/WSe$_2$ van der Waals Photodetectors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-Performance KV$_3$Sb$_5$/WSe$_2$ van der Waals Photodetectors"
                },
                "updated": "2025-12-30T13:40:49Z",
                "updated_parsed": [
                    2025,
                    12,
                    30,
                    13,
                    40,
                    49,
                    1,
                    364,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.24229v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.24229v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Kagome metals AV$_3$Sb$_5$ (A = K, Rb, Cs) have recently emerged as a promising platform for exploring correlated and topological quantum states, yet their potential for optoelectronic applications remains largely unexplored. Here, we report high-performance photodetectors based on van der Waals KV$_3$Sb$_5$/WSe$_2$ heterojunctions. A high-quality Schottky interface readily forms between KV$_3$Sb$_5$ and WSe$_2$, enabling efficient separation and transport of photoinduced carriers. Under 520 nm illumination, the device achieves an open-circuit voltage up to 0.6 V, a responsivity of 809 mA/W, and a fast response time of 18.3 us. This work demonstrates the promising optoelectronic applications of Kagome metals and highlights the potential of KV$_3$Sb$_5$-based van der Waals heterostructures for high-performance photodetection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Kagome metals AV$_3$Sb$_5$ (A = K, Rb, Cs) have recently emerged as a promising platform for exploring correlated and topological quantum states, yet their potential for optoelectronic applications remains largely unexplored. Here, we report high-performance photodetectors based on van der Waals KV$_3$Sb$_5$/WSe$_2$ heterojunctions. A high-quality Schottky interface readily forms between KV$_3$Sb$_5$ and WSe$_2$, enabling efficient separation and transport of photoinduced carriers. Under 520 nm illumination, the device achieves an open-circuit voltage up to 0.6 V, a responsivity of 809 mA/W, and a fast response time of 18.3 us. This work demonstrates the promising optoelectronic applications of Kagome metals and highlights the potential of KV$_3$Sb$_5$-based van der Waals heterostructures for high-performance photodetection."
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-30T13:40:49Z",
                "published_parsed": [
                    2025,
                    12,
                    30,
                    13,
                    40,
                    49,
                    1,
                    364,
                    0
                ],
                "arxiv_comment": "16 pages including Supporting Information",
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci"
                },
                "authors": [
                    {
                        "name": "Yang Yang"
                    },
                    {
                        "name": "Shaofeng Rao"
                    },
                    {
                        "name": "Yuxuan Hou"
                    },
                    {
                        "name": "Jiabo Liu"
                    },
                    {
                        "name": "Deng Hu"
                    },
                    {
                        "name": "Yunfei Guo"
                    },
                    {
                        "name": "Jianzhou Zhao"
                    },
                    {
                        "name": "Hechen Ren"
                    },
                    {
                        "name": "Zhiwei Wang"
                    },
                    {
                        "name": "Fan Yang"
                    }
                ],
                "author_detail": {
                    "name": "Fan Yang"
                },
                "author": "Fan Yang"
            },
            {
                "id": "http://arxiv.org/abs/2512.24195v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.24195v1",
                "title": "CorGi: Contribution-Guided Block-Wise Interval Caching for Training-Free Acceleration of Diffusion Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CorGi: Contribution-Guided Block-Wise Interval Caching for Training-Free Acceleration of Diffusion Transformers"
                },
                "updated": "2025-12-30T12:55:38Z",
                "updated_parsed": [
                    2025,
                    12,
                    30,
                    12,
                    55,
                    38,
                    1,
                    364,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.24195v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.24195v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Diffusion transformer (DiT) achieves remarkable performance in visual generation, but its iterative denoising process combined with larger capacity leads to a high inference cost. Recent works have demonstrated that the iterative denoising process of DiT models involves substantial redundant computation across steps. To effectively reduce the redundant computation in DiT, we propose CorGi (Contribution-Guided Block-Wise Interval Caching), training-free DiT inference acceleration framework that selectively reuses the outputs of transformer blocks in DiT across denoising steps. CorGi caches low-contribution blocks and reuses them in later steps within each interval to reduce redundant computation while preserving generation quality. For text-to-image tasks, we further propose CorGi+, which leverages per-block cross-attention maps to identify salient tokens and applies partial attention updates to protect important object details. Evaluation on the state-of-the-art DiT models demonstrates that CorGi and CorGi+ achieve up to 2.0x speedup on average, while preserving high generation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion transformer (DiT) achieves remarkable performance in visual generation, but its iterative denoising process combined with larger capacity leads to a high inference cost. Recent works have demonstrated that the iterative denoising process of DiT models involves substantial redundant computation across steps. To effectively reduce the redundant computation in DiT, we propose CorGi (Contribution-Guided Block-Wise Interval Caching), training-free DiT inference acceleration framework that selectively reuses the outputs of transformer blocks in DiT across denoising steps. CorGi caches low-contribution blocks and reuses them in later steps within each interval to reduce redundant computation while preserving generation quality. For text-to-image tasks, we further propose CorGi+, which leverages per-block cross-attention maps to identify salient tokens and applies partial attention updates to protect important object details. Evaluation on the state-of-the-art DiT models demonstrates that CorGi and CorGi+ achieve up to 2.0x speedup on average, while preserving high generation quality."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-30T12:55:38Z",
                "published_parsed": [
                    2025,
                    12,
                    30,
                    12,
                    55,
                    38,
                    1,
                    364,
                    0
                ],
                "arxiv_comment": "16 pages, 20 figures",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Yonglak Son"
                    },
                    {
                        "name": "Suhyeok Kim"
                    },
                    {
                        "name": "Seungryong Kim"
                    },
                    {
                        "name": "Young Geun Kim"
                    }
                ],
                "author_detail": {
                    "name": "Young Geun Kim"
                },
                "author": "Young Geun Kim"
            },
            {
                "id": "http://arxiv.org/abs/2504.05646v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2504.05646v2",
                "title": "Lattice: Learning to Efficiently Compress the Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lattice: Learning to Efficiently Compress the Memory"
                },
                "updated": "2025-12-30T10:25:28Z",
                "updated_parsed": [
                    2025,
                    12,
                    30,
                    10,
                    25,
                    28,
                    1,
                    364,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2504.05646v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2504.05646v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Attention mechanisms have revolutionized sequence learning but suffer from quadratic computational complexity. This paper introduces \\model, a novel recurrent neural network (RNN) mechanism that leverages the inherent low-rank structure of K-V matrices to efficiently compress the cache into a fixed number of memory slots, achieving sub-quadratic complexity. We formulate this compression as an online optimization problem and derive a dynamic memory update rule based on a single gradient descent step. The resulting recurrence features a state- and input-dependent gating mechanism, offering an interpretable memory update process. The core innovation is the orthogonal update: each memory slot is updated exclusively with information orthogonal to its current state, hence incorporating only novel, non-redundant data to minimize interference with previously stored information. We derive an efficient computation for this orthogonal update rule and further approximate it with chunk-wise parallelization to ensure training scalability. Empirically, Lattice outperforms strong baselines on language modeling and associative recall tasks across diverse context lengths and model sizes, achieving superior memory efficiency with significantly reduced memory sizes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention mechanisms have revolutionized sequence learning but suffer from quadratic computational complexity. This paper introduces \\model, a novel recurrent neural network (RNN) mechanism that leverages the inherent low-rank structure of K-V matrices to efficiently compress the cache into a fixed number of memory slots, achieving sub-quadratic complexity. We formulate this compression as an online optimization problem and derive a dynamic memory update rule based on a single gradient descent step. The resulting recurrence features a state- and input-dependent gating mechanism, offering an interpretable memory update process. The core innovation is the orthogonal update: each memory slot is updated exclusively with information orthogonal to its current state, hence incorporating only novel, non-redundant data to minimize interference with previously stored information. We derive an efficient computation for this orthogonal update rule and further approximate it with chunk-wise parallelization to ensure training scalability. Empirically, Lattice outperforms strong baselines on language modeling and associative recall tasks across diverse context lengths and model sizes, achieving superior memory efficiency with significantly reduced memory sizes."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-04-08T03:48:43Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    3,
                    48,
                    43,
                    1,
                    98,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Mahdi Karami"
                    },
                    {
                        "name": "Razvan Pascanu"
                    },
                    {
                        "name": "Vahab Mirrokni"
                    }
                ],
                "author_detail": {
                    "name": "Vahab Mirrokni"
                },
                "author": "Vahab Mirrokni"
            },
            {
                "id": "http://arxiv.org/abs/2511.20649v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.20649v2",
                "title": "Infinity-RoPE: Action-Controllable Infinite Video Generation Emerges From Autoregressive Self-Rollout",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Infinity-RoPE: Action-Controllable Infinite Video Generation Emerges From Autoregressive Self-Rollout"
                },
                "updated": "2025-12-30T10:14:37Z",
                "updated_parsed": [
                    2025,
                    12,
                    30,
                    10,
                    14,
                    37,
                    1,
                    364,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.20649v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.20649v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Current autoregressive video diffusion models are constrained by three core bottlenecks: (i) the finite temporal horizon imposed by the base model's 3D Rotary Positional Embedding (3D-RoPE), (ii) slow prompt responsiveness in maintaining fine-grained action control during long-form rollouts, and (iii) the inability to realize discontinuous cinematic transitions within a single generation stream. We introduce $\\infty$-RoPE, a unified inference-time framework that addresses all three limitations through three interconnected components: Block-Relativistic RoPE, KV Flush, and RoPE Cut. Block-Relativistic RoPE reformulates temporal encoding as a moving local reference frame, where each newly generated latent block is rotated relative to the base model's maximum frame horizon while earlier blocks are rotated backward to preserve relative temporal geometry. This relativistic formulation eliminates fixed temporal positions, enabling continuous video generation far beyond the base positional limits. To obtain fine-grained action control without re-encoding, KV Flush renews the KV cache by retaining only two latent frames, the global sink and the last generated latent frame, thereby ensuring immediate prompt responsiveness. Finally, RoPE Cut introduces controlled discontinuities in temporal RoPE coordinates, enabling multi-cut scene transitions within a single continuous rollout. Together, these components establish $\\infty$-RoPE as a training-free foundation for infinite-horizon, controllable, and cinematic video diffusion. Comprehensive experiments show that $\\infty$-RoPE consistently surpasses previous autoregressive models in overall VBench scores.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current autoregressive video diffusion models are constrained by three core bottlenecks: (i) the finite temporal horizon imposed by the base model's 3D Rotary Positional Embedding (3D-RoPE), (ii) slow prompt responsiveness in maintaining fine-grained action control during long-form rollouts, and (iii) the inability to realize discontinuous cinematic transitions within a single generation stream. We introduce $\\infty$-RoPE, a unified inference-time framework that addresses all three limitations through three interconnected components: Block-Relativistic RoPE, KV Flush, and RoPE Cut. Block-Relativistic RoPE reformulates temporal encoding as a moving local reference frame, where each newly generated latent block is rotated relative to the base model's maximum frame horizon while earlier blocks are rotated backward to preserve relative temporal geometry. This relativistic formulation eliminates fixed temporal positions, enabling continuous video generation far beyond the base positional limits. To obtain fine-grained action control without re-encoding, KV Flush renews the KV cache by retaining only two latent frames, the global sink and the last generated latent frame, thereby ensuring immediate prompt responsiveness. Finally, RoPE Cut introduces controlled discontinuities in temporal RoPE coordinates, enabling multi-cut scene transitions within a single continuous rollout. Together, these components establish $\\infty$-RoPE as a training-free foundation for infinite-horizon, controllable, and cinematic video diffusion. Comprehensive experiments show that $\\infty$-RoPE consistently surpasses previous autoregressive models in overall VBench scores."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-25T18:59:46Z",
                "published_parsed": [
                    2025,
                    11,
                    25,
                    18,
                    59,
                    46,
                    1,
                    329,
                    0
                ],
                "arxiv_comment": "Project Page: https://infinity-rope.github.io/",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Hidir Yesiltepe"
                    },
                    {
                        "name": "Tuna Han Salih Meral"
                    },
                    {
                        "name": "Adil Kaan Akan"
                    },
                    {
                        "name": "Kaan Oktay"
                    },
                    {
                        "name": "Pinar Yanardag"
                    }
                ],
                "author_detail": {
                    "name": "Pinar Yanardag"
                },
                "author": "Pinar Yanardag"
            },
            {
                "id": "http://arxiv.org/abs/2509.01085v4",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2509.01085v4",
                "title": "Bidirectional Sparse Attention for Faster Video Diffusion Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bidirectional Sparse Attention for Faster Video Diffusion Training"
                },
                "updated": "2025-12-30T08:58:30Z",
                "updated_parsed": [
                    2025,
                    12,
                    30,
                    8,
                    58,
                    30,
                    1,
                    364,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2509.01085v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2509.01085v4",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Video diffusion Transformer (DiT) models excel in generative quality but hit major computational bottlenecks when producing high-resolution, long-duration videos. The quadratic complexity of full attention leads to prohibitively high training and inference costs. Full attention inefficiency stems from two key challenges: excessive computation due to the inherent sparsity of Queries and Key-Value pairs, and redundant computation as fixed sparse patterns fail to leverage DiT's dynamic attention. To overcome this limitation, we propose a Bidirectional Sparse Attention (BSA) framework for faster video DiT training, the first to dynamically sparsify both Queries and Key-Value pairs within 3D full attention, thereby substantially improving training and inference efficiency. BSA addresses these issues through two key components. Query sparsity is optimized by selecting the most informative query tokens via semantic similarity and with a dynamic spatial-time training strategy, while KV sparsity is achieved by computing a statistical dynamic threshold to retain only the most salient KV blocks for computation. Extensive experiments demonstrate that BSA significantly accelerates DiT training across long sequences, reducing FLOPs by up to 20x and achieving 17.79x faster attention training, while preserving or even surpassing the generative quality of full attention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video diffusion Transformer (DiT) models excel in generative quality but hit major computational bottlenecks when producing high-resolution, long-duration videos. The quadratic complexity of full attention leads to prohibitively high training and inference costs. Full attention inefficiency stems from two key challenges: excessive computation due to the inherent sparsity of Queries and Key-Value pairs, and redundant computation as fixed sparse patterns fail to leverage DiT's dynamic attention. To overcome this limitation, we propose a Bidirectional Sparse Attention (BSA) framework for faster video DiT training, the first to dynamically sparsify both Queries and Key-Value pairs within 3D full attention, thereby substantially improving training and inference efficiency. BSA addresses these issues through two key components. Query sparsity is optimized by selecting the most informative query tokens via semantic similarity and with a dynamic spatial-time training strategy, while KV sparsity is achieved by computing a statistical dynamic threshold to retain only the most salient KV blocks for computation. Extensive experiments demonstrate that BSA significantly accelerates DiT training across long sequences, reducing FLOPs by up to 20x and achieving 17.79x faster attention training, while preserving or even surpassing the generative quality of full attention."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-09-01T03:16:52Z",
                "published_parsed": [
                    2025,
                    9,
                    1,
                    3,
                    16,
                    52,
                    0,
                    244,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Chenlu Zhan"
                    },
                    {
                        "name": "Wen Li"
                    },
                    {
                        "name": "Chuyu Shen"
                    },
                    {
                        "name": "Jun Zhang"
                    },
                    {
                        "name": "Suhui Wu"
                    },
                    {
                        "name": "Hao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Zhang"
                },
                "author": "Hao Zhang"
            },
            {
                "id": "http://arxiv.org/abs/2512.24073v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.24073v1",
                "title": "CPePC: Cooperative and Predictive Popularity based Caching for Named Data Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CPePC: Cooperative and Predictive Popularity based Caching for Named Data Networks"
                },
                "updated": "2025-12-30T08:35:28Z",
                "updated_parsed": [
                    2025,
                    12,
                    30,
                    8,
                    35,
                    28,
                    1,
                    364,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.24073v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.24073v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Caching content is an inherent feature of Named Data Networks. Limited cache capacity of routers warrants that the choice of content being cached is judiciously done. Existing techniques resort to caching popular content to maximize utilization. However, these methods experience significant overhead for coordinating and estimating the popularity of content. To address this issue, in this paper, we present CPePC, which is a cooperative caching technique designed to improve performance. It accomplishes this through a combination of two factors. First, CPePC enhances efficiency by minimizing the overhead of popularity estimation. Second, it forecasts a parameter that governs caching decisions. Efficiency in popularity estimation is achieved by dividing the network into several non-overlapping communities using a community estimation algorithm and selecting a leader node to coordinate this on behalf of all the nodes in the community. CPePC bases its caching decisions by predicting a parameter whose value is estimated using current cache occupancy and the popularity of the content into account. We present algorithms for community detection, leader selection, content popularity estimation, and caching decisions made by the CPePC method. We evaluate and compare it with six other state-of-the-art caching techniques, with simulations performed using a discrete event simulator to show that it outperforms others.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caching content is an inherent feature of Named Data Networks. Limited cache capacity of routers warrants that the choice of content being cached is judiciously done. Existing techniques resort to caching popular content to maximize utilization. However, these methods experience significant overhead for coordinating and estimating the popularity of content. To address this issue, in this paper, we present CPePC, which is a cooperative caching technique designed to improve performance. It accomplishes this through a combination of two factors. First, CPePC enhances efficiency by minimizing the overhead of popularity estimation. Second, it forecasts a parameter that governs caching decisions. Efficiency in popularity estimation is achieved by dividing the network into several non-overlapping communities using a community estimation algorithm and selecting a leader node to coordinate this on behalf of all the nodes in the community. CPePC bases its caching decisions by predicting a parameter whose value is estimated using current cache occupancy and the popularity of the content into account. We present algorithms for community detection, leader selection, content popularity estimation, and caching decisions made by the CPePC method. We evaluate and compare it with six other state-of-the-art caching techniques, with simulations performed using a discrete event simulator to show that it outperforms others."
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-30T08:35:28Z",
                "published_parsed": [
                    2025,
                    12,
                    30,
                    8,
                    35,
                    28,
                    1,
                    364,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI"
                },
                "authors": [
                    {
                        "name": "Pankaj Chaudhary"
                    },
                    {
                        "name": "Neminath Hubballi"
                    },
                    {
                        "name": "Sameer G. Kulkarni"
                    }
                ],
                "author_detail": {
                    "name": "Sameer G. Kulkarni"
                },
                "author": "Sameer G. Kulkarni"
            },
            {
                "id": "http://arxiv.org/abs/2512.23977v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.23977v1",
                "title": "Causify DataFlow: A Framework For High-performance Machine Learning Stream Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causify DataFlow: A Framework For High-performance Machine Learning Stream Computing"
                },
                "updated": "2025-12-30T04:24:04Z",
                "updated_parsed": [
                    2025,
                    12,
                    30,
                    4,
                    24,
                    4,
                    1,
                    364,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.23977v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.23977v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We present DataFlow, a computational framework for building, testing, and deploying high-performance machine learning systems on unbounded time-series data. Traditional data science workflows assume finite datasets and require substantial reimplementation when moving from batch prototypes to streaming production systems. This gap introduces causality violations, batch boundary artifacts, and poor reproducibility of real-time failures.\n  DataFlow resolves these issues through a unified execution model based on directed acyclic graphs (DAGs) with point-in-time idempotency: outputs at any time t depend only on a fixed-length context window preceding t. This guarantee ensures that models developed in batch mode execute identically in streaming production without code changes. The framework enforces strict causality by automatically tracking knowledge time across all transformations, eliminating future-peeking bugs.\n  DataFlow supports flexible tiling across temporal and feature dimensions, allowing the same model to operate at different frequencies and memory profiles via configuration alone. It integrates natively with the Python data science stack and provides fit/predict semantics for online learning, caching and incremental computation, and automatic parallelization through DAG-based scheduling. We demonstrate its effectiveness across domains including financial trading, IoT, fraud detection, and real-time analytics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present DataFlow, a computational framework for building, testing, and deploying high-performance machine learning systems on unbounded time-series data. Traditional data science workflows assume finite datasets and require substantial reimplementation when moving from batch prototypes to streaming production systems. This gap introduces causality violations, batch boundary artifacts, and poor reproducibility of real-time failures.\n  DataFlow resolves these issues through a unified execution model based on directed acyclic graphs (DAGs) with point-in-time idempotency: outputs at any time t depend only on a fixed-length context window preceding t. This guarantee ensures that models developed in batch mode execute identically in streaming production without code changes. The framework enforces strict causality by automatically tracking knowledge time across all transformations, eliminating future-peeking bugs.\n  DataFlow supports flexible tiling across temporal and feature dimensions, allowing the same model to operate at different frequencies and memory profiles via configuration alone. It integrates natively with the Python data science stack and provides fit/predict semantics for online learning, caching and incremental computation, and automatic parallelization through DAG-based scheduling. We demonstrate its effectiveness across domains including financial trading, IoT, fraud detection, and real-time analytics."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-30T04:24:04Z",
                "published_parsed": [
                    2025,
                    12,
                    30,
                    4,
                    24,
                    4,
                    1,
                    364,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Giacinto Paolo Saggese"
                    },
                    {
                        "name": "Paul Smith"
                    }
                ],
                "author_detail": {
                    "name": "Paul Smith"
                },
                "author": "Paul Smith"
            },
            {
                "id": "http://arxiv.org/abs/2512.23938v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.23938v1",
                "title": "Learnable Query Aggregation with KV Routing for Cross-view Geo-localisation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learnable Query Aggregation with KV Routing for Cross-view Geo-localisation"
                },
                "updated": "2025-12-30T01:51:52Z",
                "updated_parsed": [
                    2025,
                    12,
                    30,
                    1,
                    51,
                    52,
                    1,
                    364,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.23938v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.23938v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Cross-view geo-localisation (CVGL) aims to estimate the geographic location of a query image by matching it with images from a large-scale database. However, the significant view-point discrepancies present considerable challenges for effective feature aggregation and alignment. To address these challenges, we propose a novel CVGL system that incorporates three key improvements. Firstly, we leverage the DINOv2 backbone with a convolution adapter fine-tuning to enhance model adaptability to cross-view variations. Secondly, we propose a multi-scale channel reallocation module to strengthen the diversity and stability of spatial representations. Finally, we propose an improved aggregation module that integrates a Mixture-of-Experts (MoE) routing into the feature aggregation process. Specifically, the module dynamically selects expert subspaces for the keys and values in a cross-attention framework, enabling adaptive processing of heterogeneous input domains. Extensive experiments on the University-1652 and SUES-200 datasets demonstrate that our method achieves competitive performance with fewer trained parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-view geo-localisation (CVGL) aims to estimate the geographic location of a query image by matching it with images from a large-scale database. However, the significant view-point discrepancies present considerable challenges for effective feature aggregation and alignment. To address these challenges, we propose a novel CVGL system that incorporates three key improvements. Firstly, we leverage the DINOv2 backbone with a convolution adapter fine-tuning to enhance model adaptability to cross-view variations. Secondly, we propose a multi-scale channel reallocation module to strengthen the diversity and stability of spatial representations. Finally, we propose an improved aggregation module that integrates a Mixture-of-Experts (MoE) routing into the feature aggregation process. Specifically, the module dynamically selects expert subspaces for the keys and values in a cross-attention framework, enabling adaptive processing of heterogeneous input domains. Extensive experiments on the University-1652 and SUES-200 datasets demonstrate that our method achieves competitive performance with fewer trained parameters."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-30T01:51:52Z",
                "published_parsed": [
                    2025,
                    12,
                    30,
                    1,
                    51,
                    52,
                    1,
                    364,
                    0
                ],
                "arxiv_comment": "7 pages, 4 figures",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Hualin Ye"
                    },
                    {
                        "name": "Bingxi Liu"
                    },
                    {
                        "name": "Jixiang Du"
                    },
                    {
                        "name": "Yu Qin"
                    },
                    {
                        "name": "Ziyi Chen"
                    },
                    {
                        "name": "Hong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Hong Zhang"
                },
                "author": "Hong Zhang"
            },
            {
                "id": "http://arxiv.org/abs/2512.23852v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.23852v1",
                "title": "Trellis: Learning to Compress Key-Value Memory in Attention Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trellis: Learning to Compress Key-Value Memory in Attention Models"
                },
                "updated": "2025-12-29T20:32:10Z",
                "updated_parsed": [
                    2025,
                    12,
                    29,
                    20,
                    32,
                    10,
                    0,
                    363,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.23852v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.23852v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Transformers, while powerful, suffer from quadratic computational complexity and the ever-growing Key-Value (KV) cache of the attention mechanism. This paper introduces Trellis, a novel Transformer architecture with bounded memory that learns how to compress its key-value memory dynamically at test time. Trellis replaces the standard KV cache with a fixed-size memory and train a two-pass recurrent compression mechanism to store new keys and values into memory. To achieve this, it leverages an online gradient descent procedure with a forget gate, enabling the compressed memory to be updated recursively while learning to retain important contextual information from incoming tokens at test time. Extensive experiments on language modeling, common-sense reasoning, recall-intensive tasks, and time series show that the proposed architecture outperforms strong baselines. Notably, its performance gains increase as the sequence length grows, highlighting its potential for long-context applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers, while powerful, suffer from quadratic computational complexity and the ever-growing Key-Value (KV) cache of the attention mechanism. This paper introduces Trellis, a novel Transformer architecture with bounded memory that learns how to compress its key-value memory dynamically at test time. Trellis replaces the standard KV cache with a fixed-size memory and train a two-pass recurrent compression mechanism to store new keys and values into memory. To achieve this, it leverages an online gradient descent procedure with a forget gate, enabling the compressed memory to be updated recursively while learning to retain important contextual information from incoming tokens at test time. Extensive experiments on language modeling, common-sense reasoning, recall-intensive tasks, and time series show that the proposed architecture outperforms strong baselines. Notably, its performance gains increase as the sequence length grows, highlighting its potential for long-context applications."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-29T20:32:10Z",
                "published_parsed": [
                    2025,
                    12,
                    29,
                    20,
                    32,
                    10,
                    0,
                    363,
                    0
                ],
                "arxiv_comment": "In Second Conference on Language Modeling (COLM) (2025)",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Mahdi Karami"
                    },
                    {
                        "name": "Ali Behrouz"
                    },
                    {
                        "name": "Praneeth Kacham"
                    },
                    {
                        "name": "Vahab Mirrokni"
                    }
                ],
                "author_detail": {
                    "name": "Vahab Mirrokni"
                },
                "author": "Vahab Mirrokni"
            },
            {
                "id": "http://arxiv.org/abs/2512.23635v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.23635v1",
                "title": "Rethinking the Spatio-Temporal Alignment of End-to-End 3D Perception",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking the Spatio-Temporal Alignment of End-to-End 3D Perception"
                },
                "updated": "2025-12-29T17:48:56Z",
                "updated_parsed": [
                    2025,
                    12,
                    29,
                    17,
                    48,
                    56,
                    0,
                    363,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.23635v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.23635v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Spatio-temporal alignment is crucial for temporal modeling of end-to-end (E2E) perception in autonomous driving (AD), providing valuable structural and textural prior information. Existing methods typically rely on the attention mechanism to align objects across frames, simplifying the motion model with a unified explicit physical model (constant velocity, etc.). These approaches prefer semantic features for implicit alignment, challenging the importance of explicit motion modeling in the traditional perception paradigm. However, variations in motion states and object features across categories and frames render this alignment suboptimal. To address this, we propose HAT, a spatio-temporal alignment module that allows each object to adaptively decode the optimal alignment proposal from multiple hypotheses without direct supervision. Specifically, HAT first utilizes multiple explicit motion models to generate spatial anchors and motion-aware feature proposals for historical instances. It then performs multi-hypothesis decoding by incorporating semantic and motion cues embedded in cached object queries, ultimately providing the optimal alignment proposal for the target frame. On nuScenes, HAT consistently improves 3D temporal detectors and trackers across diverse baselines. It achieves state-of-the-art tracking results with 46.0% AMOTA on the test set when paired with the DETR3D detector. In an object-centric E2E AD method, HAT enhances perception accuracy (+1.3% mAP, +3.1% AMOTA) and reduces the collision rate by 32%. When semantics are corrupted (nuScenes-C), the enhancement of motion modeling by HAT enables more robust perception and planning in the E2E AD.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spatio-temporal alignment is crucial for temporal modeling of end-to-end (E2E) perception in autonomous driving (AD), providing valuable structural and textural prior information. Existing methods typically rely on the attention mechanism to align objects across frames, simplifying the motion model with a unified explicit physical model (constant velocity, etc.). These approaches prefer semantic features for implicit alignment, challenging the importance of explicit motion modeling in the traditional perception paradigm. However, variations in motion states and object features across categories and frames render this alignment suboptimal. To address this, we propose HAT, a spatio-temporal alignment module that allows each object to adaptively decode the optimal alignment proposal from multiple hypotheses without direct supervision. Specifically, HAT first utilizes multiple explicit motion models to generate spatial anchors and motion-aware feature proposals for historical instances. It then performs multi-hypothesis decoding by incorporating semantic and motion cues embedded in cached object queries, ultimately providing the optimal alignment proposal for the target frame. On nuScenes, HAT consistently improves 3D temporal detectors and trackers across diverse baselines. It achieves state-of-the-art tracking results with 46.0% AMOTA on the test set when paired with the DETR3D detector. In an object-centric E2E AD method, HAT enhances perception accuracy (+1.3% mAP, +3.1% AMOTA) and reduces the collision rate by 32%. When semantics are corrupted (nuScenes-C), the enhancement of motion modeling by HAT enables more robust perception and planning in the E2E AD."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-29T17:48:56Z",
                "published_parsed": [
                    2025,
                    12,
                    29,
                    17,
                    48,
                    56,
                    0,
                    363,
                    0
                ],
                "arxiv_comment": "Accepted to AAAI 2026",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Xiaoyu Li"
                    },
                    {
                        "name": "Peidong Li"
                    },
                    {
                        "name": "Xian Wu"
                    },
                    {
                        "name": "Long Shi"
                    },
                    {
                        "name": "Dedong Liu"
                    },
                    {
                        "name": "Yitao Wu"
                    },
                    {
                        "name": "Jiajia Fu"
                    },
                    {
                        "name": "Dixiao Cui"
                    },
                    {
                        "name": "Lijun Zhao"
                    },
                    {
                        "name": "Lining Sun"
                    }
                ],
                "author_detail": {
                    "name": "Lining Sun"
                },
                "author": "Lining Sun"
            },
            {
                "id": "http://arxiv.org/abs/2412.16060v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2412.16060v2",
                "title": "Adaptable TeaStore",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptable TeaStore"
                },
                "updated": "2025-12-29T14:34:18Z",
                "updated_parsed": [
                    2025,
                    12,
                    29,
                    14,
                    34,
                    18,
                    0,
                    363,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2412.16060v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2412.16060v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.4204/EPTCS.438.1",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "Modern cloud-native systems require adapting dynamically to changing operational conditions, including service outages, traffic surges, and evolving user requirements. While existing benchmarks provide valuable testbeds for performance and scalability evaluation, they lack explicit support for studying adaptation mechanisms, reconfiguration strategies, and graceful degradation. These limitations hinder systematic research on self-adaptive architectures in realistic cloud environments.\n  To cover this gap, we introduce Adaptable TeaStore, an extension of the renowned TeaStore architecture that incorporates adaptability as a first-class design concern. Our extension distinguishes between mandatory and optional services, supports multiple component versions -- with varying resource requirements and functionality levels -- considers the outsourcing of functionalities to external providers, and provides local cache mechanisms for performance and resilience. These features enable the systematic exploration of reconfiguration policies across diverse operational scenarios.\n  We discuss a broad catalogue of reference adaptation scenarios centred around Adaptable TeaStore, useful to evaluate the ability of a given adaptation technology to address conditions such as component unavailability, cyberattacks, provider outages, benign/malicious traffic increases, and user-triggered reconfigurations. Moreover, we present an open-source implementation of the architecture with APIs for metrics collection and adaptation triggers, to enable reproducible experiments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern cloud-native systems require adapting dynamically to changing operational conditions, including service outages, traffic surges, and evolving user requirements. While existing benchmarks provide valuable testbeds for performance and scalability evaluation, they lack explicit support for studying adaptation mechanisms, reconfiguration strategies, and graceful degradation. These limitations hinder systematic research on self-adaptive architectures in realistic cloud environments.\n  To cover this gap, we introduce Adaptable TeaStore, an extension of the renowned TeaStore architecture that incorporates adaptability as a first-class design concern. Our extension distinguishes between mandatory and optional services, supports multiple component versions -- with varying resource requirements and functionality levels -- considers the outsourcing of functionalities to external providers, and provides local cache mechanisms for performance and resilience. These features enable the systematic exploration of reconfiguration policies across diverse operational scenarios.\n  We discuss a broad catalogue of reference adaptation scenarios centred around Adaptable TeaStore, useful to evaluate the ability of a given adaptation technology to address conditions such as component unavailability, cyberattacks, provider outages, benign/malicious traffic increases, and user-triggered reconfigurations. Moreover, we present an open-source implementation of the architecture with APIs for metrics collection and adaptation triggers, to enable reproducible experiments."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2024-12-20T17:06:11Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    17,
                    6,
                    11,
                    4,
                    355,
                    0
                ],
                "arxiv_comment": "In Proceedings WACA 2025, arXiv:2512.22054",
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "arxiv_journal_ref": "EPTCS 438, 2025, pp. 1-14",
                "authors": [
                    {
                        "name": "Simon Bliudze"
                    },
                    {
                        "name": "Giuseppe De Palma"
                    },
                    {
                        "name": "Saverio Giallorenzo"
                    },
                    {
                        "name": "Ivan Lanese"
                    },
                    {
                        "name": "Gianluigi Zavattaro"
                    },
                    {
                        "name": "Brice Arléon Zemtsop Ndadji"
                    }
                ],
                "author_detail": {
                    "name": "Brice Arléon Zemtsop Ndadji"
                },
                "arxiv_affiliation": "Univ. Lille, CNRS, Inria, Centrale Lille, CRIStAL, Lille, France",
                "author": "Brice Arléon Zemtsop Ndadji",
                "arxiv_doi": "10.4204/EPTCS.438.1"
            },
            {
                "id": "http://arxiv.org/abs/2510.13797v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.13797v3",
                "title": "Breadcrumbs Reasoning: Memory-Efficient Reasoning with Compression Beacons",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breadcrumbs Reasoning: Memory-Efficient Reasoning with Compression Beacons"
                },
                "updated": "2025-12-29T13:06:57Z",
                "updated_parsed": [
                    2025,
                    12,
                    29,
                    13,
                    6,
                    57,
                    0,
                    363,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.13797v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.13797v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The scalability of large language models for long-context reasoning is severely constrained by the linear growth of their Transformer key-value cache, which incurs significant memory and computational costs. We posit that as a model generates reasoning tokens, the informational value of past generated tokens diminishes, creating an opportunity for compression. In this work, we propose to periodically compress the generation KV cache with a learned, special-purpose token and evict compressed entries. We train the model to perform this compression via a modified joint distillation and reinforcement learning (RL) framework. Our training method minimizes overhead over the conventional RL process, as it leverages RL outputs for distillation. Empirically, our method achieves a superior memory-accuracy Pareto frontier compared to both the model without cache compression and training-free compression techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The scalability of large language models for long-context reasoning is severely constrained by the linear growth of their Transformer key-value cache, which incurs significant memory and computational costs. We posit that as a model generates reasoning tokens, the informational value of past generated tokens diminishes, creating an opportunity for compression. In this work, we propose to periodically compress the generation KV cache with a learned, special-purpose token and evict compressed entries. We train the model to perform this compression via a modified joint distillation and reinforcement learning (RL) framework. Our training method minimizes overhead over the conventional RL process, as it leverages RL outputs for distillation. Empirically, our method achieves a superior memory-accuracy Pareto frontier compared to both the model without cache compression and training-free compression techniques."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-15T17:57:21Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    17,
                    57,
                    21,
                    2,
                    288,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Giovanni Monea"
                    },
                    {
                        "name": "Yair Feldman"
                    },
                    {
                        "name": "Shankar Padmanabhan"
                    },
                    {
                        "name": "Kianté Brantley"
                    },
                    {
                        "name": "Yoav Artzi"
                    }
                ],
                "author_detail": {
                    "name": "Yoav Artzi"
                },
                "author": "Yoav Artzi"
            },
            {
                "id": "http://arxiv.org/abs/2512.23434v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.23434v1",
                "title": "Local Rendezvous Hashing: Bounded Loads and Minimal Churn via Cache-Local Candidates",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Local Rendezvous Hashing: Bounded Loads and Minimal Churn via Cache-Local Candidates"
                },
                "updated": "2025-12-29T12:52:57Z",
                "updated_parsed": [
                    2025,
                    12,
                    29,
                    12,
                    52,
                    57,
                    0,
                    363,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.23434v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.23434v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Consistent hashing is fundamental to distributed systems, but ring-based schemes can exhibit high peak-to-average load ratios unless they use many virtual nodes, while multi-probe methods improve balance at the cost of scattered memory accesses. This paper introduces Local Rendezvous Hashing (LRH), which preserves a token ring but restricts Highest Random Weight (HRW) selection to a cache-local window of C distinct neighboring physical nodes. LRH locates a key by one binary search, enumerates exactly C distinct candidates using precomputed next-distinct offsets, and chooses the HRW winner (optionally weighted). Lookup cost is O(log|R| + C). Under fixed-topology liveness changes, fixed-candidate filtering remaps only keys whose original winner is down, yielding zero excess churn. In a benchmark with N=5000, V=256 (|R|=1.28M), K=50M and C=8, LRH reduces Max/Avg load from 1.2785 to 1.0947 and achieves 60.05 Mkeys/s, about 6.8x faster than multi-probe consistent hashing with 8 probes (8.80 Mkeys/s) while approaching its balance (Max/Avg 1.0697). A microbenchmark indicates multi-probe assignment is dominated by repeated ring searches and memory traffic rather than probe-generation arithmetic.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Consistent hashing is fundamental to distributed systems, but ring-based schemes can exhibit high peak-to-average load ratios unless they use many virtual nodes, while multi-probe methods improve balance at the cost of scattered memory accesses. This paper introduces Local Rendezvous Hashing (LRH), which preserves a token ring but restricts Highest Random Weight (HRW) selection to a cache-local window of C distinct neighboring physical nodes. LRH locates a key by one binary search, enumerates exactly C distinct candidates using precomputed next-distinct offsets, and chooses the HRW winner (optionally weighted). Lookup cost is O(log|R| + C). Under fixed-topology liveness changes, fixed-candidate filtering remaps only keys whose original winner is down, yielding zero excess churn. In a benchmark with N=5000, V=256 (|R|=1.28M), K=50M and C=8, LRH reduces Max/Avg load from 1.2785 to 1.0947 and achieves 60.05 Mkeys/s, about 6.8x faster than multi-probe consistent hashing with 8 probes (8.80 Mkeys/s) while approaching its balance (Max/Avg 1.0697). A microbenchmark indicates multi-probe assignment is dominated by repeated ring searches and memory traffic rather than probe-generation arithmetic."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-29T12:52:57Z",
                "published_parsed": [
                    2025,
                    12,
                    29,
                    12,
                    52,
                    57,
                    0,
                    363,
                    0
                ],
                "arxiv_comment": "14 pages, 10 figures. Includes appendices",
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Yongjie Guan"
                    }
                ],
                "author_detail": {
                    "name": "Yongjie Guan"
                },
                "arxiv_affiliation": "Zhejiang University of Technology",
                "author": "Yongjie Guan"
            },
            {
                "id": "http://arxiv.org/abs/2512.23298v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.23298v1",
                "title": "BRkNN-light: Batch Processing of Reverse k-Nearest Neighbor Queries for Moving Objects on Road Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BRkNN-light: Batch Processing of Reverse k-Nearest Neighbor Queries for Moving Objects on Road Networks"
                },
                "updated": "2025-12-29T08:36:32Z",
                "updated_parsed": [
                    2025,
                    12,
                    29,
                    8,
                    36,
                    32,
                    0,
                    363,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.23298v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.23298v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.1145/3748777.3748791",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "The Reverse $k$-Nearest Neighbor (R$k$NN) query over moving objects on road networks seeks to find all moving objects that consider the specified query point as one of their $k$ nearest neighbors. In location based services, many users probably submit R$k$NN queries simultaneously. However, existing methods largely overlook how to efficiently process multiple such queries together, missing opportunities to share redundant computations and thus reduce overall processing costs. To address this, this work is the first to explore batch processing of multiple R$k$NN queries, aiming to minimize total computation by sharing duplicate calculations across queries. To tackle this issue, we propose the BR$k$NN-Light algorithm, which uses rapid verification and pruning strategies based on geometric constraints, along with an optimized range search technique, to speed up the process of identifying the R$k$NNs for each query. Furthermore, it proposes a dynamic distance caching mechanism to enable computation reuse when handling multiple queries, thereby significantly reducing unnecessary computations. Experiments on multiple real-world road networks demonstrate the superiority of the BR$k$NN-Light algorithm on the processing of batch queries.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Reverse $k$-Nearest Neighbor (R$k$NN) query over moving objects on road networks seeks to find all moving objects that consider the specified query point as one of their $k$ nearest neighbors. In location based services, many users probably submit R$k$NN queries simultaneously. However, existing methods largely overlook how to efficiently process multiple such queries together, missing opportunities to share redundant computations and thus reduce overall processing costs. To address this, this work is the first to explore batch processing of multiple R$k$NN queries, aiming to minimize total computation by sharing duplicate calculations across queries. To tackle this issue, we propose the BR$k$NN-Light algorithm, which uses rapid verification and pruning strategies based on geometric constraints, along with an optimized range search technique, to speed up the process of identifying the R$k$NNs for each query. Furthermore, it proposes a dynamic distance caching mechanism to enable computation reuse when handling multiple queries, thereby significantly reducing unnecessary computations. Experiments on multiple real-world road networks demonstrate the superiority of the BR$k$NN-Light algorithm on the processing of batch queries."
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-29T08:36:32Z",
                "published_parsed": [
                    2025,
                    12,
                    29,
                    8,
                    36,
                    32,
                    0,
                    363,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB"
                },
                "authors": [
                    {
                        "name": "Anbang Song"
                    },
                    {
                        "name": "Ziqiang Yu"
                    },
                    {
                        "name": "Wei Liu"
                    },
                    {
                        "name": "Yating Xu"
                    },
                    {
                        "name": "Mingjin Tao"
                    }
                ],
                "author_detail": {
                    "name": "Mingjin Tao"
                },
                "author": "Mingjin Tao",
                "arxiv_doi": "10.1145/3748777.3748791"
            },
            {
                "id": "http://arxiv.org/abs/2512.23290v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.23290v1",
                "title": "Atomic-scale spin sensing of a 2D $d$-wave altermagnet via helical tunneling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Atomic-scale spin sensing of a 2D $d$-wave altermagnet via helical tunneling"
                },
                "updated": "2025-12-29T08:22:06Z",
                "updated_parsed": [
                    2025,
                    12,
                    29,
                    8,
                    22,
                    6,
                    0,
                    363,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.23290v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.23290v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Altermagnetism simultaneously possesses nonrelativistic spin responses and zero net magnetization, thus combining advantages of ferromagnetism and antiferromagnetism. This superiority originates from its unique dual feature, i.e., opposite-magnetic sublattices in real space and alternating spin polarization in momentum space enforced by the same crystal symmetry. Therefore, the determination of an altermagnetic order and its unique spin response inherently necessitates atomic-scale spin-resolved measurements in real and momentum spaces, an experimental milestone yet to be achieved. Here, via utilizing the helical edge (hinge) modes of a higher order topological insulator as the spin sensor, we realize spin-resolved scanning tunneling microscopy which enables us to pin down the dual-space feature of a layered $d$-wave altermagnet, KV$_2$Se$_2$O. In real space, atomic-registered mapping demonstrates the checkerboard antiferromagnetic order together with density-wave lattice modulation, and in momentum space, spin-resolved spectroscopic imaging provides a direct visualization of d-wave spin splitting of the band structure. Critically, using this new topology-guaranteed spin filter we directly reveal the unidirectional, spin-polarized quasiparticle excitations originating from the crystal symmetry-paired X and Y valleys around opposite magnetic sublattices simultaneously --the unique spin response for $d$-wave altermagnetism. Our experiments establish a solid basis for the exploration and utilization of altermagnetism in layered materials and further facilitate access to atomic-scale spin sensing and manipulating of 2D quantum materials.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Altermagnetism simultaneously possesses nonrelativistic spin responses and zero net magnetization, thus combining advantages of ferromagnetism and antiferromagnetism. This superiority originates from its unique dual feature, i.e., opposite-magnetic sublattices in real space and alternating spin polarization in momentum space enforced by the same crystal symmetry. Therefore, the determination of an altermagnetic order and its unique spin response inherently necessitates atomic-scale spin-resolved measurements in real and momentum spaces, an experimental milestone yet to be achieved. Here, via utilizing the helical edge (hinge) modes of a higher order topological insulator as the spin sensor, we realize spin-resolved scanning tunneling microscopy which enables us to pin down the dual-space feature of a layered $d$-wave altermagnet, KV$_2$Se$_2$O. In real space, atomic-registered mapping demonstrates the checkerboard antiferromagnetic order together with density-wave lattice modulation, and in momentum space, spin-resolved spectroscopic imaging provides a direct visualization of d-wave spin splitting of the band structure. Critically, using this new topology-guaranteed spin filter we directly reveal the unidirectional, spin-polarized quasiparticle excitations originating from the crystal symmetry-paired X and Y valleys around opposite magnetic sublattices simultaneously --the unique spin response for $d$-wave altermagnetism. Our experiments establish a solid basis for the exploration and utilization of altermagnetism in layered materials and further facilitate access to atomic-scale spin sensing and manipulating of 2D quantum materials."
                },
                "tags": [
                    {
                        "term": "cond-mat.mes-hall",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.str-el",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-29T08:22:06Z",
                "published_parsed": [
                    2025,
                    12,
                    29,
                    8,
                    22,
                    6,
                    0,
                    363,
                    0
                ],
                "arxiv_comment": "21 pages and 5 figures. Extended data figures and Supplementary notes are available from the corresponding author upon request. Comments are welcome",
                "arxiv_primary_category": {
                    "term": "cond-mat.mes-hall"
                },
                "authors": [
                    {
                        "name": "Zhuying Wang"
                    },
                    {
                        "name": "Shuikang Yu"
                    },
                    {
                        "name": "Xingkai Cheng"
                    },
                    {
                        "name": "Xiaoyu Xiao"
                    },
                    {
                        "name": "Wanru Ma"
                    },
                    {
                        "name": "Feixiong Quan"
                    },
                    {
                        "name": "Hongxi Song"
                    },
                    {
                        "name": "Kunming Zhang"
                    },
                    {
                        "name": "Yunmei Zhang"
                    },
                    {
                        "name": "Yitian Ma"
                    },
                    {
                        "name": "Wenhao Liu"
                    },
                    {
                        "name": "Priti Yadav"
                    },
                    {
                        "name": "Xiangbiao Shi"
                    },
                    {
                        "name": "Zhijun Wang"
                    },
                    {
                        "name": "Qian Niu"
                    },
                    {
                        "name": "Yang Gao"
                    },
                    {
                        "name": "Bin Xiang"
                    },
                    {
                        "name": "Junwei Liu"
                    },
                    {
                        "name": "Zhenyu Wang"
                    },
                    {
                        "name": "Xianhui Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xianhui Chen"
                },
                "author": "Xianhui Chen"
            },
            {
                "id": "http://arxiv.org/abs/2512.09427v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.09427v2",
                "title": "ODMA: On-Demand Memory Allocation Framework for LLM Serving on LPDDR-Class Accelerators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ODMA: On-Demand Memory Allocation Framework for LLM Serving on LPDDR-Class Accelerators"
                },
                "updated": "2025-12-29T07:47:50Z",
                "updated_parsed": [
                    2025,
                    12,
                    29,
                    7,
                    47,
                    50,
                    0,
                    363,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.09427v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.09427v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Device-memory management is a key bottleneck for serving large language models (LLMs) on accelerators whose memory has poor small-granularity random-access bandwidth (e.g., LPDDR5-class). Existing approaches either statically pre-allocate worst-case KV-cache per request, wasting substantial device memory, or rely on fine-grained paging that assumes high random-access tolerance and is therefore ill-suited to LPDDR-style systems. We present ODMA, an on-demand memory allocation framework for LLM serving on random-access-constrained device memory (RACM) platforms such as LPDDR5-based Cambricon MLUs. ODMA builds on generation-length prediction while addressing distribution drift and heavy-tailed request lengths via dynamic bucket partitioning and a large-bucket safeguard: bucket boundaries are periodically re-learned from online histograms, and high-uncertainty or overflowed requests fall back to a reserved large bucket for robustness. On Alpaca and Google-NQ, ODMA improves S3's predictor accuracy from 98.60% to 99.55% and from 82.68% to 93.36%, respectively. Serving DeepSeek-R1-Distill-Qwen-7B on four Cambricon MLU370-X4 accelerators, ODMA increases device-memory utilization from 55.05% to 72.45% on Alpaca and from 42.54% to 61.79% on Google-NQ, and boosts throughput by 23% and 27% over a static pre-allocation baseline. These results show that predictor-driven, hardware-aware allocation can unlock efficient LLM serving on RACM accelerators without hardware changes, complementing paging-centric designs tailored to HBM systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Device-memory management is a key bottleneck for serving large language models (LLMs) on accelerators whose memory has poor small-granularity random-access bandwidth (e.g., LPDDR5-class). Existing approaches either statically pre-allocate worst-case KV-cache per request, wasting substantial device memory, or rely on fine-grained paging that assumes high random-access tolerance and is therefore ill-suited to LPDDR-style systems. We present ODMA, an on-demand memory allocation framework for LLM serving on random-access-constrained device memory (RACM) platforms such as LPDDR5-based Cambricon MLUs. ODMA builds on generation-length prediction while addressing distribution drift and heavy-tailed request lengths via dynamic bucket partitioning and a large-bucket safeguard: bucket boundaries are periodically re-learned from online histograms, and high-uncertainty or overflowed requests fall back to a reserved large bucket for robustness. On Alpaca and Google-NQ, ODMA improves S3's predictor accuracy from 98.60% to 99.55% and from 82.68% to 93.36%, respectively. Serving DeepSeek-R1-Distill-Qwen-7B on four Cambricon MLU370-X4 accelerators, ODMA increases device-memory utilization from 55.05% to 72.45% on Alpaca and from 42.54% to 61.79% on Google-NQ, and boosts throughput by 23% and 27% over a static pre-allocation baseline. These results show that predictor-driven, hardware-aware allocation can unlock efficient LLM serving on RACM accelerators without hardware changes, complementing paging-centric designs tailored to HBM systems."
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-10T08:52:20Z",
                "published_parsed": [
                    2025,
                    12,
                    10,
                    8,
                    52,
                    20,
                    2,
                    344,
                    0
                ],
                "arxiv_comment": "10 pages, 5 figures",
                "arxiv_primary_category": {
                    "term": "cs.AR"
                },
                "authors": [
                    {
                        "name": "Guoqiang Zou"
                    },
                    {
                        "name": "Wanyu Wang"
                    },
                    {
                        "name": "Hao Zheng"
                    },
                    {
                        "name": "Longxiang Yin"
                    },
                    {
                        "name": "Yinhe Han"
                    }
                ],
                "author_detail": {
                    "name": "Yinhe Han"
                },
                "author": "Yinhe Han"
            },
            {
                "id": "http://arxiv.org/abs/2512.23258v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.23258v1",
                "title": "Plug-and-Play Fidelity Optimization for Diffusion Transformer Acceleration via Cumulative Error Minimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Plug-and-Play Fidelity Optimization for Diffusion Transformer Acceleration via Cumulative Error Minimization"
                },
                "updated": "2025-12-29T07:36:36Z",
                "updated_parsed": [
                    2025,
                    12,
                    29,
                    7,
                    36,
                    36,
                    0,
                    363,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.23258v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.23258v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Although Diffusion Transformer (DiT) has emerged as a predominant architecture for image and video generation, its iterative denoising process results in slow inference, which hinders broader applicability and development. Caching-based methods achieve training-free acceleration, while suffering from considerable computational error. Existing methods typically incorporate error correction strategies such as pruning or prediction to mitigate it. However, their fixed caching strategy fails to adapt to the complex error variations during denoising, which limits the full potential of error correction. To tackle this challenge, we propose a novel fidelity-optimization plugin for existing error correction methods via cumulative error minimization, named CEM. CEM predefines the error to characterize the sensitivity of model to acceleration jointly influenced by timesteps and cache intervals. Guided by this prior, we formulate a dynamic programming algorithm with cumulative error approximation for strategy optimization, which achieves the caching error minimization, resulting in a substantial improvement in generation fidelity. CEM is model-agnostic and exhibits strong generalization, which is adaptable to arbitrary acceleration budgets. It can be seamlessly integrated into existing error correction frameworks and quantized models without introducing any additional computational overhead. Extensive experiments conducted on nine generation models and quantized methods across three tasks demonstrate that CEM significantly improves generation fidelity of existing acceleration models, and outperforms the original generation performance on FLUX.1-dev, PixArt-$α$, StableDiffusion1.5 and Hunyuan. The code will be made publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although Diffusion Transformer (DiT) has emerged as a predominant architecture for image and video generation, its iterative denoising process results in slow inference, which hinders broader applicability and development. Caching-based methods achieve training-free acceleration, while suffering from considerable computational error. Existing methods typically incorporate error correction strategies such as pruning or prediction to mitigate it. However, their fixed caching strategy fails to adapt to the complex error variations during denoising, which limits the full potential of error correction. To tackle this challenge, we propose a novel fidelity-optimization plugin for existing error correction methods via cumulative error minimization, named CEM. CEM predefines the error to characterize the sensitivity of model to acceleration jointly influenced by timesteps and cache intervals. Guided by this prior, we formulate a dynamic programming algorithm with cumulative error approximation for strategy optimization, which achieves the caching error minimization, resulting in a substantial improvement in generation fidelity. CEM is model-agnostic and exhibits strong generalization, which is adaptable to arbitrary acceleration budgets. It can be seamlessly integrated into existing error correction frameworks and quantized models without introducing any additional computational overhead. Extensive experiments conducted on nine generation models and quantized methods across three tasks demonstrate that CEM significantly improves generation fidelity of existing acceleration models, and outperforms the original generation performance on FLUX.1-dev, PixArt-$α$, StableDiffusion1.5 and Hunyuan. The code will be made publicly available."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-29T07:36:36Z",
                "published_parsed": [
                    2025,
                    12,
                    29,
                    7,
                    36,
                    36,
                    0,
                    363,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Tong Shao"
                    },
                    {
                        "name": "Yusen Fu"
                    },
                    {
                        "name": "Guoying Sun"
                    },
                    {
                        "name": "Jingde Kong"
                    },
                    {
                        "name": "Zhuotao Tian"
                    },
                    {
                        "name": "Jingyong Su"
                    }
                ],
                "author_detail": {
                    "name": "Jingyong Su"
                },
                "author": "Jingyong Su"
            },
            {
                "id": "http://arxiv.org/abs/2512.21734v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.21734v2",
                "title": "Knot Forcing: Taming Autoregressive Video Diffusion Models for Real-time Infinite Interactive Portrait Animation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knot Forcing: Taming Autoregressive Video Diffusion Models for Real-time Infinite Interactive Portrait Animation"
                },
                "updated": "2025-12-29T03:22:01Z",
                "updated_parsed": [
                    2025,
                    12,
                    29,
                    3,
                    22,
                    1,
                    0,
                    363,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.21734v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.21734v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Real-time portrait animation is essential for interactive applications such as virtual assistants and live avatars, requiring high visual fidelity, temporal coherence, ultra-low latency, and responsive control from dynamic inputs like reference images and driving signals. While diffusion-based models achieve strong quality, their non-causal nature hinders streaming deployment. Causal autoregressive video generation approaches enable efficient frame-by-frame generation but suffer from error accumulation, motion discontinuities at chunk boundaries, and degraded long-term consistency. In this work, we present a novel streaming framework named Knot Forcing for real-time portrait animation that addresses these challenges through three key designs: (1) a chunk-wise generation strategy with global identity preservation via cached KV states of the reference image and local temporal modeling using sliding window attention; (2) a temporal knot module that overlaps adjacent chunks and propagates spatio-temporal cues via image-to-video conditioning to smooth inter-chunk motion transitions; and (3) A \"running ahead\" mechanism that dynamically updates the reference frame's temporal coordinate during inference, keeping its semantic context ahead of the current rollout frame to support long-term coherence. Knot Forcing enables high-fidelity, temporally consistent, and interactive portrait animation over infinite sequences, achieving real-time performance with strong visual stability on consumer-grade GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time portrait animation is essential for interactive applications such as virtual assistants and live avatars, requiring high visual fidelity, temporal coherence, ultra-low latency, and responsive control from dynamic inputs like reference images and driving signals. While diffusion-based models achieve strong quality, their non-causal nature hinders streaming deployment. Causal autoregressive video generation approaches enable efficient frame-by-frame generation but suffer from error accumulation, motion discontinuities at chunk boundaries, and degraded long-term consistency. In this work, we present a novel streaming framework named Knot Forcing for real-time portrait animation that addresses these challenges through three key designs: (1) a chunk-wise generation strategy with global identity preservation via cached KV states of the reference image and local temporal modeling using sliding window attention; (2) a temporal knot module that overlaps adjacent chunks and propagates spatio-temporal cues via image-to-video conditioning to smooth inter-chunk motion transitions; and (3) A \"running ahead\" mechanism that dynamically updates the reference frame's temporal coordinate during inference, keeping its semantic context ahead of the current rollout frame to support long-term coherence. Knot Forcing enables high-fidelity, temporally consistent, and interactive portrait animation over infinite sequences, achieving real-time performance with strong visual stability on consumer-grade GPUs."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-25T16:34:56Z",
                "published_parsed": [
                    2025,
                    12,
                    25,
                    16,
                    34,
                    56,
                    3,
                    359,
                    0
                ],
                "arxiv_comment": "Project Page: https://humanaigc.github.io/knot_forcing_demo_page/",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Steven Xiao"
                    },
                    {
                        "name": "Xindi Zhang"
                    },
                    {
                        "name": "Dechao Meng"
                    },
                    {
                        "name": "Qi Wang"
                    },
                    {
                        "name": "Peng Zhang"
                    },
                    {
                        "name": "Bang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Bang Zhang"
                },
                "author": "Bang Zhang"
            },
            {
                "id": "http://arxiv.org/abs/2512.23049v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.23049v1",
                "title": "Accelerating Language Model Workflows with Prompt Choreography",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Language Model Workflows with Prompt Choreography"
                },
                "updated": "2025-12-28T19:21:11Z",
                "updated_parsed": [
                    2025,
                    12,
                    28,
                    19,
                    21,
                    11,
                    6,
                    362,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.23049v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.23049v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models are increasingly deployed in multi-agent workflows. We introduce Prompt Choreography, a framework that efficiently executes LLM workflows by maintaining a dynamic, global KV cache. Each LLM call can attend to an arbitrary, reordered subset of previously encoded messages. Parallel calls are supported. Though caching messages' encodings sometimes gives different results from re-encoding them in a new context, we show in diverse settings that fine-tuning the LLM to work with the cache can help it mimic the original results. Prompt Choreography significantly reduces per-message latency (2.0--6.2$\\times$ faster time-to-first-token) and achieves substantial end-to-end speedups ($>$2.2$\\times$) in some workflows dominated by redundant computation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models are increasingly deployed in multi-agent workflows. We introduce Prompt Choreography, a framework that efficiently executes LLM workflows by maintaining a dynamic, global KV cache. Each LLM call can attend to an arbitrary, reordered subset of previously encoded messages. Parallel calls are supported. Though caching messages' encodings sometimes gives different results from re-encoding them in a new context, we show in diverse settings that fine-tuning the LLM to work with the cache can help it mimic the original results. Prompt Choreography significantly reduces per-message latency (2.0--6.2$\\times$ faster time-to-first-token) and achieves substantial end-to-end speedups ($>$2.2$\\times$) in some workflows dominated by redundant computation."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-28T19:21:11Z",
                "published_parsed": [
                    2025,
                    12,
                    28,
                    19,
                    21,
                    11,
                    6,
                    362,
                    0
                ],
                "arxiv_comment": "to appear in TACL (final preprint of 2025-10-12); 10 pages + appendices",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "TJ Bai"
                    },
                    {
                        "name": "Jason Eisner"
                    }
                ],
                "author_detail": {
                    "name": "Jason Eisner"
                },
                "author": "Jason Eisner"
            },
            {
                "id": "http://arxiv.org/abs/2510.14973v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.14973v2",
                "title": "Attention Is All You Need for KV Cache in Diffusion LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention Is All You Need for KV Cache in Diffusion LLMs"
                },
                "updated": "2025-12-28T17:27:09Z",
                "updated_parsed": [
                    2025,
                    12,
                    28,
                    17,
                    27,
                    9,
                    6,
                    362,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.14973v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.14973v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "This work studies how to adaptively recompute key-value (KV) caches for diffusion large language models (DLMs) to maximize prediction accuracy while minimizing decoding latency. Prior methods' decoders recompute QKV for all tokens at every denoising step and layer, despite KV states changing little across most steps, especially in shallow layers, leading to substantial redundancy. We make three observations: (1) distant ${\\bf MASK}$ tokens primarily act as a length-bias and can be cached block-wise beyond the active prediction window; (2) KV dynamics increase with depth, suggesting that selective refresh starting from deeper layers is sufficient; and (3) the most-attended token exhibits the smallest KV drift, providing a conservative lower bound on cache change for other tokens. Building on these, we propose ${\\bf Elastic-Cache}$, a training-free, architecture-agnostic strategy that jointly decides ${when}$ to refresh (via an attention-aware drift test on the most-attended token) and ${where}$ to refresh (via a depth-aware schedule that recomputes from a chosen layer onward while reusing shallow-layer caches and off-window MASK caches). Unlike fixed-period schemes, Elastic-Cache performs adaptive, layer-aware cache updates for diffusion LLMs, reducing redundant computation and accelerating decoding with negligible loss in generation quality. Experiments on LLaDA-Instruct, LLaDA-1.5, and LLaDA-V across mathematical reasoning and code generation tasks demonstrate consistent speedups: $8.7\\times$ on GSM8K (256 tokens), and $45.1\\times$ on longer sequences, while consistently maintaining higher accuracy than the baseline. Our method achieves significantly higher throughput ($6.8\\times$ on GSM8K) than existing confidence-based approaches while preserving generation quality, enabling practical deployment of diffusion LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work studies how to adaptively recompute key-value (KV) caches for diffusion large language models (DLMs) to maximize prediction accuracy while minimizing decoding latency. Prior methods' decoders recompute QKV for all tokens at every denoising step and layer, despite KV states changing little across most steps, especially in shallow layers, leading to substantial redundancy. We make three observations: (1) distant ${\\bf MASK}$ tokens primarily act as a length-bias and can be cached block-wise beyond the active prediction window; (2) KV dynamics increase with depth, suggesting that selective refresh starting from deeper layers is sufficient; and (3) the most-attended token exhibits the smallest KV drift, providing a conservative lower bound on cache change for other tokens. Building on these, we propose ${\\bf Elastic-Cache}$, a training-free, architecture-agnostic strategy that jointly decides ${when}$ to refresh (via an attention-aware drift test on the most-attended token) and ${where}$ to refresh (via a depth-aware schedule that recomputes from a chosen layer onward while reusing shallow-layer caches and off-window MASK caches). Unlike fixed-period schemes, Elastic-Cache performs adaptive, layer-aware cache updates for diffusion LLMs, reducing redundant computation and accelerating decoding with negligible loss in generation quality. Experiments on LLaDA-Instruct, LLaDA-1.5, and LLaDA-V across mathematical reasoning and code generation tasks demonstrate consistent speedups: $8.7\\times$ on GSM8K (256 tokens), and $45.1\\times$ on longer sequences, while consistently maintaining higher accuracy than the baseline. Our method achieves significantly higher throughput ($6.8\\times$ on GSM8K) than existing confidence-based approaches while preserving generation quality, enabling practical deployment of diffusion LLMs."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-16T17:59:48Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    59,
                    48,
                    3,
                    289,
                    0
                ],
                "arxiv_comment": "Code at: https://github.com/VILA-Lab/Elastic-Cache",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Quan Nguyen-Tri"
                    },
                    {
                        "name": "Mukul Ranjan"
                    },
                    {
                        "name": "Zhiqiang Shen"
                    }
                ],
                "author_detail": {
                    "name": "Zhiqiang Shen"
                },
                "author": "Zhiqiang Shen"
            },
            {
                "id": "http://arxiv.org/abs/2512.22854v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.22854v1",
                "title": "ByteLoom: Weaving Geometry-Consistent Human-Object Interactions through Progressive Curriculum Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ByteLoom: Weaving Geometry-Consistent Human-Object Interactions through Progressive Curriculum Learning"
                },
                "updated": "2025-12-28T09:38:36Z",
                "updated_parsed": [
                    2025,
                    12,
                    28,
                    9,
                    38,
                    36,
                    6,
                    362,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.22854v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.22854v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Human-object interaction (HOI) video generation has garnered increasing attention due to its promising applications in digital humans, e-commerce, advertising, and robotics imitation learning. However, existing methods face two critical limitations: (1) a lack of effective mechanisms to inject multi-view information of the object into the model, leading to poor cross-view consistency, and (2) heavy reliance on fine-grained hand mesh annotations for modeling interaction occlusions. To address these challenges, we introduce ByteLoom, a Diffusion Transformer (DiT)-based framework that generates realistic HOI videos with geometrically consistent object illustration, using simplified human conditioning and 3D object inputs. We first propose an RCM-cache mechanism that leverages Relative Coordinate Maps (RCM) as a universal representation to maintain object's geometry consistency and precisely control 6-DoF object transformations in the meantime. To compensate HOI dataset scarcity and leverage existing datasets, we further design a training curriculum that enhances model capabilities in a progressive style and relaxes the demand of hand mesh. Extensive experiments demonstrate that our method faithfully preserves human identity and the object's multi-view geometry, while maintaining smooth motion and object manipulation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human-object interaction (HOI) video generation has garnered increasing attention due to its promising applications in digital humans, e-commerce, advertising, and robotics imitation learning. However, existing methods face two critical limitations: (1) a lack of effective mechanisms to inject multi-view information of the object into the model, leading to poor cross-view consistency, and (2) heavy reliance on fine-grained hand mesh annotations for modeling interaction occlusions. To address these challenges, we introduce ByteLoom, a Diffusion Transformer (DiT)-based framework that generates realistic HOI videos with geometrically consistent object illustration, using simplified human conditioning and 3D object inputs. We first propose an RCM-cache mechanism that leverages Relative Coordinate Maps (RCM) as a universal representation to maintain object's geometry consistency and precisely control 6-DoF object transformations in the meantime. To compensate HOI dataset scarcity and leverage existing datasets, we further design a training curriculum that enhances model capabilities in a progressive style and relaxes the demand of hand mesh. Extensive experiments demonstrate that our method faithfully preserves human identity and the object's multi-view geometry, while maintaining smooth motion and object manipulation."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-28T09:38:36Z",
                "published_parsed": [
                    2025,
                    12,
                    28,
                    9,
                    38,
                    36,
                    6,
                    362,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Bangya Liu"
                    },
                    {
                        "name": "Xinyu Gong"
                    },
                    {
                        "name": "Zelin Zhao"
                    },
                    {
                        "name": "Ziyang Song"
                    },
                    {
                        "name": "Yulei Lu"
                    },
                    {
                        "name": "Suhui Wu"
                    },
                    {
                        "name": "Jun Zhang"
                    },
                    {
                        "name": "Suman Banerjee"
                    },
                    {
                        "name": "Hao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Zhang"
                },
                "author": "Hao Zhang"
            },
            {
                "id": "http://arxiv.org/abs/2512.22737v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.22737v1",
                "title": "WeDLM: Reconciling Diffusion Language Models with Standard Causal Attention for Fast Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WeDLM: Reconciling Diffusion Language Models with Standard Causal Attention for Fast Inference"
                },
                "updated": "2025-12-28T01:25:48Z",
                "updated_parsed": [
                    2025,
                    12,
                    28,
                    1,
                    25,
                    48,
                    6,
                    362,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.22737v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.22737v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Autoregressive (AR) generation is the standard decoding paradigm for Large Language Models (LLMs), but its token-by-token nature limits parallelism at inference time. Diffusion Language Models (DLLMs) offer parallel decoding by recovering multiple masked tokens per step; however, in practice they often fail to translate this parallelism into deployment speed gains over optimized AR engines (e.g., vLLM). A key reason is that many DLLMs rely on bidirectional attention, which breaks standard prefix KV caching and forces repeated contextualization, undermining efficiency. We propose WeDLM, a diffusion decoding framework built entirely on standard causal attention to make parallel generation prefix-cache friendly. The core idea is to let each masked position condition on all currently observed tokens while keeping a strict causal mask, achieved by Topological Reordering that moves observed tokens to the physical prefix while preserving their logical positions. Building on this property, we introduce a streaming decoding procedure that continuously commits confident tokens into a growing left-to-right prefix and maintains a fixed parallel workload, avoiding the stop-and-wait behavior common in block diffusion methods. Experiments show that WeDLM preserves the quality of strong AR backbones while delivering substantial speedups, approaching 3x on challenging reasoning benchmarks and up to 10x in low-entropy generation regimes; critically, our comparisons are against AR baselines served by vLLM under matched deployment settings, demonstrating that diffusion-style decoding can outperform an optimized AR engine in practice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive (AR) generation is the standard decoding paradigm for Large Language Models (LLMs), but its token-by-token nature limits parallelism at inference time. Diffusion Language Models (DLLMs) offer parallel decoding by recovering multiple masked tokens per step; however, in practice they often fail to translate this parallelism into deployment speed gains over optimized AR engines (e.g., vLLM). A key reason is that many DLLMs rely on bidirectional attention, which breaks standard prefix KV caching and forces repeated contextualization, undermining efficiency. We propose WeDLM, a diffusion decoding framework built entirely on standard causal attention to make parallel generation prefix-cache friendly. The core idea is to let each masked position condition on all currently observed tokens while keeping a strict causal mask, achieved by Topological Reordering that moves observed tokens to the physical prefix while preserving their logical positions. Building on this property, we introduce a streaming decoding procedure that continuously commits confident tokens into a growing left-to-right prefix and maintains a fixed parallel workload, avoiding the stop-and-wait behavior common in block diffusion methods. Experiments show that WeDLM preserves the quality of strong AR backbones while delivering substantial speedups, approaching 3x on challenging reasoning benchmarks and up to 10x in low-entropy generation regimes; critically, our comparisons are against AR baselines served by vLLM under matched deployment settings, demonstrating that diffusion-style decoding can outperform an optimized AR engine in practice."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-28T01:25:48Z",
                "published_parsed": [
                    2025,
                    12,
                    28,
                    1,
                    25,
                    48,
                    6,
                    362,
                    0
                ],
                "arxiv_comment": "23 pages, 8 figures, project page: https://wedlm.github.io/",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Aiwei Liu"
                    },
                    {
                        "name": "Minghua He"
                    },
                    {
                        "name": "Shaoxun Zeng"
                    },
                    {
                        "name": "Sijun Zhang"
                    },
                    {
                        "name": "Linhao Zhang"
                    },
                    {
                        "name": "Chuhan Wu"
                    },
                    {
                        "name": "Wei Jia"
                    },
                    {
                        "name": "Yuan Liu"
                    },
                    {
                        "name": "Xiao Zhou"
                    },
                    {
                        "name": "Jie Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jie Zhou"
                },
                "author": "Jie Zhou"
            },
            {
                "id": "http://arxiv.org/abs/2510.15904v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.15904v2",
                "title": "NVM-in-Cache: Repurposing Commodity 6T SRAM Cache into NVM Analog Processing-in-Memory Engine using a Novel Compute-on-Powerline Scheme",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NVM-in-Cache: Repurposing Commodity 6T SRAM Cache into NVM Analog Processing-in-Memory Engine using a Novel Compute-on-Powerline Scheme"
                },
                "updated": "2025-12-27T20:21:03Z",
                "updated_parsed": [
                    2025,
                    12,
                    27,
                    20,
                    21,
                    3,
                    5,
                    361,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.15904v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.15904v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The rapid growth of deep neural network (DNN) workloads has significantly increased the demand for large-capacity on-chip SRAM in machine learning (ML) applications, with SRAM arrays now occupying a substantial fraction of the total die area. To address the dual challenges of storage density and computation efficiency, this paper proposes an NVM-in-Cache architecture that integrates resistive RAM (RRAM) devices into a conventional 6T-SRAM cell, forming a compact 6T-2R bit-cell. This hybrid cell enables Processing-in-Memory (PIM) mode, which performs massively parallel multiply-and-accumulate (MAC) operations directly on cache power lines while preserving stored cache data. By exploiting the intrinsic properties of the 6T-2R structure, the architecture achieves additional storage capability, high computational throughput without any bit-cell area overhead. Circuit- and array-level simulations in GlobalFoundries 22nm FDSOI technology demonstrate that the proposed design achieves a throughput of 0.4 TOPS and 452.34 TOPS/W. For 128 row-parallel operations, the CIFAR-10 classification is demonstrated by mapping a Resnet-18 neural network, achieving an accuracy of 91.76%. These results highlight the potential of the NVM-in-Cache approach to serve as a scalable, energy-efficient computing method by re-purposing existing 6T SRAM cache architecture for next-generation AI accelerators and general purpose processors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid growth of deep neural network (DNN) workloads has significantly increased the demand for large-capacity on-chip SRAM in machine learning (ML) applications, with SRAM arrays now occupying a substantial fraction of the total die area. To address the dual challenges of storage density and computation efficiency, this paper proposes an NVM-in-Cache architecture that integrates resistive RAM (RRAM) devices into a conventional 6T-SRAM cell, forming a compact 6T-2R bit-cell. This hybrid cell enables Processing-in-Memory (PIM) mode, which performs massively parallel multiply-and-accumulate (MAC) operations directly on cache power lines while preserving stored cache data. By exploiting the intrinsic properties of the 6T-2R structure, the architecture achieves additional storage capability, high computational throughput without any bit-cell area overhead. Circuit- and array-level simulations in GlobalFoundries 22nm FDSOI technology demonstrate that the proposed design achieves a throughput of 0.4 TOPS and 452.34 TOPS/W. For 128 row-parallel operations, the CIFAR-10 classification is demonstrated by mapping a Resnet-18 neural network, achieving an accuracy of 91.76%. These results highlight the potential of the NVM-in-Cache approach to serve as a scalable, energy-efficient computing method by re-purposing existing 6T SRAM cache architecture for next-generation AI accelerators and general purpose processors."
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-09-15T01:09:18Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    1,
                    9,
                    18,
                    0,
                    258,
                    0
                ],
                "arxiv_comment": "11 pages",
                "arxiv_primary_category": {
                    "term": "cs.AR"
                },
                "authors": [
                    {
                        "name": "Subhradip Chakraborty"
                    },
                    {
                        "name": "Ankur Singh"
                    },
                    {
                        "name": "Xuming Chen"
                    },
                    {
                        "name": "Gourav Datta"
                    },
                    {
                        "name": "Akhilesh R. Jaiswal"
                    }
                ],
                "author_detail": {
                    "name": "Akhilesh R. Jaiswal"
                },
                "author": "Akhilesh R. Jaiswal"
            },
            {
                "id": "http://arxiv.org/abs/2512.22581v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.22581v1",
                "title": "KV-Tracker: Real-Time Pose Tracking with Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV-Tracker: Real-Time Pose Tracking with Transformers"
                },
                "updated": "2025-12-27T13:02:30Z",
                "updated_parsed": [
                    2025,
                    12,
                    27,
                    13,
                    2,
                    30,
                    5,
                    361,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.22581v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.22581v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Multi-view 3D geometry networks offer a powerful prior but are prohibitively slow for real-time applications. We propose a novel way to adapt them for online use, enabling real-time 6-DoF pose tracking and online reconstruction of objects and scenes from monocular RGB videos. Our method rapidly selects and manages a set of images as keyframes to map a scene or object via $π^3$ with full bidirectional attention. We then cache the global self-attention block's key-value (KV) pairs and use them as the sole scene representation for online tracking. This allows for up to $15\\times$ speedup during inference without the fear of drift or catastrophic forgetting. Our caching strategy is model-agnostic and can be applied to other off-the-shelf multi-view networks without retraining. We demonstrate KV-Tracker on both scene-level tracking and the more challenging task of on-the-fly object tracking and reconstruction without depth measurements or object priors. Experiments on the TUM RGB-D, 7-Scenes, Arctic and OnePose datasets show the strong performance of our system while maintaining high frame-rates up to ${\\sim}27$ FPS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-view 3D geometry networks offer a powerful prior but are prohibitively slow for real-time applications. We propose a novel way to adapt them for online use, enabling real-time 6-DoF pose tracking and online reconstruction of objects and scenes from monocular RGB videos. Our method rapidly selects and manages a set of images as keyframes to map a scene or object via $π^3$ with full bidirectional attention. We then cache the global self-attention block's key-value (KV) pairs and use them as the sole scene representation for online tracking. This allows for up to $15\\times$ speedup during inference without the fear of drift or catastrophic forgetting. Our caching strategy is model-agnostic and can be applied to other off-the-shelf multi-view networks without retraining. We demonstrate KV-Tracker on both scene-level tracking and the more challenging task of on-the-fly object tracking and reconstruction without depth measurements or object priors. Experiments on the TUM RGB-D, 7-Scenes, Arctic and OnePose datasets show the strong performance of our system while maintaining high frame-rates up to ${\\sim}27$ FPS."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-27T13:02:30Z",
                "published_parsed": [
                    2025,
                    12,
                    27,
                    13,
                    2,
                    30,
                    5,
                    361,
                    0
                ],
                "arxiv_comment": "Project Page: https://marwan99.github.io/kv_tracker",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Marwan Taher"
                    },
                    {
                        "name": "Ignacio Alzugaray"
                    },
                    {
                        "name": "Kirill Mazur"
                    },
                    {
                        "name": "Xin Kong"
                    },
                    {
                        "name": "Andrew J. Davison"
                    }
                ],
                "author_detail": {
                    "name": "Andrew J. Davison"
                },
                "author": "Andrew J. Davison"
            },
            {
                "id": "http://arxiv.org/abs/2601.00024v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.00024v1",
                "title": "Quantitative Rule-Based Strategy modeling in Classic Indian Rummy: A Metric Optimization Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantitative Rule-Based Strategy modeling in Classic Indian Rummy: A Metric Optimization Approach"
                },
                "updated": "2025-12-26T21:03:47Z",
                "updated_parsed": [
                    2025,
                    12,
                    26,
                    21,
                    3,
                    47,
                    4,
                    360,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.00024v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.00024v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The 13-card variant of Classic Indian Rummy is a sequential game of incomplete information that requires probabilistic reasoning and combinatorial decision-making. This paper proposes a rule-based framework for strategic play, driven by a new hand-evaluation metric termed MinDist. The metric modifies the MinScore metric by quantifying the edit distance between a hand and the nearest valid configuration, thereby capturing structural proximity to completion. We design a computationally efficient algorithm derived from the MinScore algorithm, leveraging dynamic pruning and pattern caching to exactly calculate this metric during play. Opponent hand-modeling is also incorporated within a two-player zero-sum simulation framework, and the resulting strategies are evaluated using statistical hypothesis testing. Empirical results show significant improvement in win rates for MinDist-based agents over traditional heuristics, providing a formal and interpretable step toward algorithmic Rummy strategy design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The 13-card variant of Classic Indian Rummy is a sequential game of incomplete information that requires probabilistic reasoning and combinatorial decision-making. This paper proposes a rule-based framework for strategic play, driven by a new hand-evaluation metric termed MinDist. The metric modifies the MinScore metric by quantifying the edit distance between a hand and the nearest valid configuration, thereby capturing structural proximity to completion. We design a computationally efficient algorithm derived from the MinScore algorithm, leveraging dynamic pruning and pattern caching to exactly calculate this metric during play. Opponent hand-modeling is also incorporated within a two-player zero-sum simulation framework, and the resulting strategies are evaluated using statistical hypothesis testing. Empirical results show significant improvement in win rates for MinDist-based agents over traditional heuristics, providing a formal and interpretable step toward algorithmic Rummy strategy design."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-26T21:03:47Z",
                "published_parsed": [
                    2025,
                    12,
                    26,
                    21,
                    3,
                    47,
                    4,
                    360,
                    0
                ],
                "arxiv_comment": "9 pages, 6 figures, 2 algorithms",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Purushottam Saha"
                    },
                    {
                        "name": "Avirup Chakraborty"
                    },
                    {
                        "name": "Sourish Sarkar"
                    },
                    {
                        "name": "Subhamoy Maitra"
                    },
                    {
                        "name": "Diganta Mukherjee"
                    },
                    {
                        "name": "Tridib Mukherjee"
                    }
                ],
                "author_detail": {
                    "name": "Tridib Mukherjee"
                },
                "author": "Tridib Mukherjee"
            },
            {
                "id": "http://arxiv.org/abs/2512.22118v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.22118v1",
                "title": "ProEdit: Inversion-based Editing From Prompts Done Right",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProEdit: Inversion-based Editing From Prompts Done Right"
                },
                "updated": "2025-12-26T18:59:14Z",
                "updated_parsed": [
                    2025,
                    12,
                    26,
                    18,
                    59,
                    14,
                    4,
                    360,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.22118v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.22118v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Inversion-based visual editing provides an effective and training-free way to edit an image or a video based on user instructions. Existing methods typically inject source image information during the sampling process to maintain editing consistency. However, this sampling strategy overly relies on source information, which negatively affects the edits in the target image (e.g., failing to change the subject's atributes like pose, number, or color as instructed). In this work, we propose ProEdit to address this issue both in the attention and the latent aspects. In the attention aspect, we introduce KV-mix, which mixes KV features of the source and the target in the edited region, mitigating the influence of the source image on the editing region while maintaining background consistency. In the latent aspect, we propose Latents-Shift, which perturbs the edited region of the source latent, eliminating the influence of the inverted latent on the sampling. Extensive experiments on several image and video editing benchmarks demonstrate that our method achieves SOTA performance. In addition, our design is plug-and-play, which can be seamlessly integrated into existing inversion and editing methods, such as RF-Solver, FireFlow and UniEdit.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inversion-based visual editing provides an effective and training-free way to edit an image or a video based on user instructions. Existing methods typically inject source image information during the sampling process to maintain editing consistency. However, this sampling strategy overly relies on source information, which negatively affects the edits in the target image (e.g., failing to change the subject's atributes like pose, number, or color as instructed). In this work, we propose ProEdit to address this issue both in the attention and the latent aspects. In the attention aspect, we introduce KV-mix, which mixes KV features of the source and the target in the edited region, mitigating the influence of the source image on the editing region while maintaining background consistency. In the latent aspect, we propose Latents-Shift, which perturbs the edited region of the source latent, eliminating the influence of the inverted latent on the sampling. Extensive experiments on several image and video editing benchmarks demonstrate that our method achieves SOTA performance. In addition, our design is plug-and-play, which can be seamlessly integrated into existing inversion and editing methods, such as RF-Solver, FireFlow and UniEdit."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-26T18:59:14Z",
                "published_parsed": [
                    2025,
                    12,
                    26,
                    18,
                    59,
                    14,
                    4,
                    360,
                    0
                ],
                "arxiv_comment": "Equal contributions from first two authors. Project page: https://isee-laboratory.github.io/ProEdit/ Code: https://github.com/iSEE-Laboratory/ProEdit",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Zhi Ouyang"
                    },
                    {
                        "name": "Dian Zheng"
                    },
                    {
                        "name": "Xiao-Ming Wu"
                    },
                    {
                        "name": "Jian-Jian Jiang"
                    },
                    {
                        "name": "Kun-Yu Lin"
                    },
                    {
                        "name": "Jingke Meng"
                    },
                    {
                        "name": "Wei-Shi Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Wei-Shi Zheng"
                },
                "author": "Wei-Shi Zheng"
            },
            {
                "id": "http://arxiv.org/abs/2512.21967v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.21967v1",
                "title": "BLEST: Blazingly Efficient BFS using Tensor Cores",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BLEST: Blazingly Efficient BFS using Tensor Cores"
                },
                "updated": "2025-12-26T10:30:33Z",
                "updated_parsed": [
                    2025,
                    12,
                    26,
                    10,
                    30,
                    33,
                    4,
                    360,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.21967v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.21967v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Breadth-First Search (BFS) is a fundamental graph kernel that underpins a wide range of applications. While modern GPUs provide specialised Matrix-Multiply-Accumulate (MMA) units, e.g., Tensor Cores (TC), with extremely high throughput, they target dense operations, making it non-trivial to exploit them for irregular, unstructured graph computations. In particular, fully utilising them for a BFS requires an efficient mapping of the edge operations onto TCs while avoiding redundancy, load imbalance, and synchronisation. We present BLEST, a TC-accelerated framework that reformulates the pull-based BFS pipeline around a bitmap-oriented structure and a carefully engineered execution layout. BLEST introduces Binarised Virtual Slice Sets (BVSS) to enforce warp-level load balancing and to eliminate frontier-oblivious work assignment. To improve both memory efficiency and update locality across diverse graphs, we apply two complementary graph reordering strategies: a compression-oriented ordering for social-like graphs and a bandwidth-reducing ordering for non-social graphs. At the compute level, we develop a batched SpMSpV multiplication pattern that uses the bitwise TC tiles to handle dot products without wasting output entries, thereby reducing the number of required MMA calls. Finally, BLEST combines kernel fusion with a lazy vertex update scheme to reduce host-side synchronisation, mitigate atomic overheads, and improve cache locality. Experiments show that BLEST delivers, on average, $3.58\\times$, $4.64\\times$ and $4.9\\times$ speedup over BerryBees, Gunrock, and GSWITCH, respectively, across a broad set of real-world graphs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breadth-First Search (BFS) is a fundamental graph kernel that underpins a wide range of applications. While modern GPUs provide specialised Matrix-Multiply-Accumulate (MMA) units, e.g., Tensor Cores (TC), with extremely high throughput, they target dense operations, making it non-trivial to exploit them for irregular, unstructured graph computations. In particular, fully utilising them for a BFS requires an efficient mapping of the edge operations onto TCs while avoiding redundancy, load imbalance, and synchronisation. We present BLEST, a TC-accelerated framework that reformulates the pull-based BFS pipeline around a bitmap-oriented structure and a carefully engineered execution layout. BLEST introduces Binarised Virtual Slice Sets (BVSS) to enforce warp-level load balancing and to eliminate frontier-oblivious work assignment. To improve both memory efficiency and update locality across diverse graphs, we apply two complementary graph reordering strategies: a compression-oriented ordering for social-like graphs and a bandwidth-reducing ordering for non-social graphs. At the compute level, we develop a batched SpMSpV multiplication pattern that uses the bitwise TC tiles to handle dot products without wasting output entries, thereby reducing the number of required MMA calls. Finally, BLEST combines kernel fusion with a lazy vertex update scheme to reduce host-side synchronisation, mitigate atomic overheads, and improve cache locality. Experiments show that BLEST delivers, on average, $3.58\\times$, $4.64\\times$ and $4.9\\times$ speedup over BerryBees, Gunrock, and GSWITCH, respectively, across a broad set of real-world graphs."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-26T10:30:33Z",
                "published_parsed": [
                    2025,
                    12,
                    26,
                    10,
                    30,
                    33,
                    4,
                    360,
                    0
                ],
                "arxiv_comment": "13 pages, 3 figures, 4 tables, 3 algorithms, 46 references",
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Deniz Elbek"
                    },
                    {
                        "name": "Kamer Kaya"
                    }
                ],
                "author_detail": {
                    "name": "Kamer Kaya"
                },
                "author": "Kamer Kaya"
            },
            {
                "id": "http://arxiv.org/abs/2512.21954v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.21954v1",
                "title": "Latency-Optimal Cache-aided Multicast Streaming via Forward-Backward Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latency-Optimal Cache-aided Multicast Streaming via Forward-Backward Reinforcement Learning"
                },
                "updated": "2025-12-26T10:00:39Z",
                "updated_parsed": [
                    2025,
                    12,
                    26,
                    10,
                    0,
                    39,
                    4,
                    360,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.21954v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.21954v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We consider a cellular network equipped with cache-enabled base-stations (BSs) leveraging an orthogonal multipoint multicast (OMPMC) streaming scheme. The network operates in a time-slotted fashion to serve content-requesting users by streaming cached files. The users being unsatisfied by the multicat streaming face a delivery outage, implying that they will remain interested in their preference at the next time-slot, which leads to a forward dynamics on the user preference. To design a latency-optimal streaming policy, the dynamics of latency is properly modeled and included in the learning procedure. We show that this dynamics surprisingly represents a backward dynamics. The combination of problem's forward and backward dynamics then develops a forward-backward Markov decision process (FB-MDP) that fully captures the network evolution across time. This FB-MDP necessitates usage of a forward-backward multi-objective reinforcement learning (FB-MORL) algorithm to optimize the expected latency as well as other performance metrics of interest including the overall outage probability and total resource consumption. Simulation results show the merit of proposed FB-MORL algorithm in finding a promising dynamic cache policy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider a cellular network equipped with cache-enabled base-stations (BSs) leveraging an orthogonal multipoint multicast (OMPMC) streaming scheme. The network operates in a time-slotted fashion to serve content-requesting users by streaming cached files. The users being unsatisfied by the multicat streaming face a delivery outage, implying that they will remain interested in their preference at the next time-slot, which leads to a forward dynamics on the user preference. To design a latency-optimal streaming policy, the dynamics of latency is properly modeled and included in the learning procedure. We show that this dynamics surprisingly represents a backward dynamics. The combination of problem's forward and backward dynamics then develops a forward-backward Markov decision process (FB-MDP) that fully captures the network evolution across time. This FB-MDP necessitates usage of a forward-backward multi-objective reinforcement learning (FB-MORL) algorithm to optimize the expected latency as well as other performance metrics of interest including the overall outage probability and total resource consumption. Simulation results show the merit of proposed FB-MORL algorithm in finding a promising dynamic cache policy."
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-26T10:00:39Z",
                "published_parsed": [
                    2025,
                    12,
                    26,
                    10,
                    0,
                    39,
                    4,
                    360,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT"
                },
                "authors": [
                    {
                        "name": "Mohsen Amidzadeh"
                    }
                ],
                "author_detail": {
                    "name": "Mohsen Amidzadeh"
                },
                "author": "Mohsen Amidzadeh"
            },
            {
                "id": "http://arxiv.org/abs/2512.21859v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.21859v1",
                "title": "TimeBill: Time-Budgeted Inference for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TimeBill: Time-Budgeted Inference for Large Language Models"
                },
                "updated": "2025-12-26T04:49:35Z",
                "updated_parsed": [
                    2025,
                    12,
                    26,
                    4,
                    49,
                    35,
                    4,
                    360,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.21859v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.21859v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) are increasingly deployed in time-critical systems, such as robotics, autonomous driving, embodied intelligence, and industrial automation, where generating accurate responses within a given time budget is crucial for decision-making, control, or safety-critical tasks. However, the auto-regressive generation process of LLMs makes it challenging to model and estimate the end-to-end execution time. Furthermore, existing efficient inference methods based on a fixed key-value (KV) cache eviction ratio struggle to adapt to varying tasks with diverse time budgets, where an improper eviction ratio may lead to incomplete inference or a drop in response performance. In this paper, we propose TimeBill, a novel time-budgeted inference framework for LLMs that balances the inference efficiency and response performance. To be more specific, we propose a fine-grained response length predictor (RLP) and an execution time estimator (ETE) to accurately predict the end-to-end execution time of LLMs. Following this, we develop a time-budgeted efficient inference approach that adaptively adjusts the KV cache eviction ratio based on execution time prediction and the given time budget. Finally, through extensive experiments, we demonstrate the advantages of TimeBill in improving task completion rate and maintaining response performance under various overrun strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly deployed in time-critical systems, such as robotics, autonomous driving, embodied intelligence, and industrial automation, where generating accurate responses within a given time budget is crucial for decision-making, control, or safety-critical tasks. However, the auto-regressive generation process of LLMs makes it challenging to model and estimate the end-to-end execution time. Furthermore, existing efficient inference methods based on a fixed key-value (KV) cache eviction ratio struggle to adapt to varying tasks with diverse time budgets, where an improper eviction ratio may lead to incomplete inference or a drop in response performance. In this paper, we propose TimeBill, a novel time-budgeted inference framework for LLMs that balances the inference efficiency and response performance. To be more specific, we propose a fine-grained response length predictor (RLP) and an execution time estimator (ETE) to accurately predict the end-to-end execution time of LLMs. Following this, we develop a time-budgeted efficient inference approach that adaptively adjusts the KV cache eviction ratio based on execution time prediction and the given time budget. Finally, through extensive experiments, we demonstrate the advantages of TimeBill in improving task completion rate and maintaining response performance under various overrun strategies."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-26T04:49:35Z",
                "published_parsed": [
                    2025,
                    12,
                    26,
                    4,
                    49,
                    35,
                    4,
                    360,
                    0
                ],
                "arxiv_comment": "Accepted to AAAI 2026",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Qi Fan"
                    },
                    {
                        "name": "An Zou"
                    },
                    {
                        "name": "Yehan Ma"
                    }
                ],
                "author_detail": {
                    "name": "Yehan Ma"
                },
                "author": "Yehan Ma"
            },
            {
                "id": "http://arxiv.org/abs/2512.22301v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.22301v1",
                "title": "A Statistical Side-Channel Risk Model for Timing Variability in Lattice-Based Post-Quantum Cryptography",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Statistical Side-Channel Risk Model for Timing Variability in Lattice-Based Post-Quantum Cryptography"
                },
                "updated": "2025-12-26T03:12:33Z",
                "updated_parsed": [
                    2025,
                    12,
                    26,
                    3,
                    12,
                    33,
                    4,
                    360,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.22301v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.22301v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Timing side-channels are an important threat to cryptography that still needs to be addressed in implementations, and the advent of post-quantum cryptography raises this issue because the lattice-based schemes may produce secret-dependent timing variability with the help of complex arithmetic and control flow. Since also real timing measurements are affected by environmental noise (e.g. scheduling effects, contention, heavy tailed delays), in this work a scenario-based statistical risk model is proposed for timing leakage as a problem of distributional distinguishability under controlled execution conditions. We synthesize traces for two secret classes in idle, jitter and loaded scenarios and for multiple leakage models and quantify leakage with Welch's t-test, KS distance, Cliff's delta, mutual information, and distribution overlap to combine in a TLRI like manner to obtain a consistent score for ranking scenarios. Across representative lattice-based KEM families (Kyber, Saber, Frodo), idle conditions generally have the best distinguishability, jitter and loaded conditions erode distinguishability through an increase in variance and increase in overlap; cache-index and branch-style leakage tends to give the highest risk signals, and faster schemes can have a higher peak risk given similar leakage assumptions, allowing reproducible comparisons at an early design stage, prior to platform-specific validation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Timing side-channels are an important threat to cryptography that still needs to be addressed in implementations, and the advent of post-quantum cryptography raises this issue because the lattice-based schemes may produce secret-dependent timing variability with the help of complex arithmetic and control flow. Since also real timing measurements are affected by environmental noise (e.g. scheduling effects, contention, heavy tailed delays), in this work a scenario-based statistical risk model is proposed for timing leakage as a problem of distributional distinguishability under controlled execution conditions. We synthesize traces for two secret classes in idle, jitter and loaded scenarios and for multiple leakage models and quantify leakage with Welch's t-test, KS distance, Cliff's delta, mutual information, and distribution overlap to combine in a TLRI like manner to obtain a consistent score for ranking scenarios. Across representative lattice-based KEM families (Kyber, Saber, Frodo), idle conditions generally have the best distinguishability, jitter and loaded conditions erode distinguishability through an increase in variance and increase in overlap; cache-index and branch-style leakage tends to give the highest risk signals, and faster schemes can have a higher peak risk given similar leakage assumptions, allowing reproducible comparisons at an early design stage, prior to platform-specific validation."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-26T03:12:33Z",
                "published_parsed": [
                    2025,
                    12,
                    26,
                    3,
                    12,
                    33,
                    4,
                    360,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "authors": [
                    {
                        "name": "Aayush Mainali"
                    },
                    {
                        "name": "Sirjan Ghimire"
                    }
                ],
                "author_detail": {
                    "name": "Sirjan Ghimire"
                },
                "author": "Sirjan Ghimire"
            },
            {
                "id": "http://arxiv.org/abs/2412.20166v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2412.20166v3",
                "title": "PIMphony: Overcoming Bandwidth and Capacity Inefficiency in PIM-based Long-Context LLM Inference System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PIMphony: Overcoming Bandwidth and Capacity Inefficiency in PIM-based Long-Context LLM Inference System"
                },
                "updated": "2025-12-25T14:44:28Z",
                "updated_parsed": [
                    2025,
                    12,
                    25,
                    14,
                    44,
                    28,
                    3,
                    359,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2412.20166v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2412.20166v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The expansion of long-context Large Language Models (LLMs) creates significant memory system challenges. While Processing-in-Memory (PIM) is a promising accelerator, we identify that it suffers from critical inefficiencies when scaled to long contexts: severe channel underutilization, performance-limiting I/O bottlenecks, and massive memory waste from static KV cache management. In this work, we propose PIMphony, a PIM orchestrator that systematically resolves these issues with three co-designed techniques. First, Token-Centric PIM Partitioning (TCP) ensures high channel utilization regardless of batch size. Second, Dynamic PIM Command Scheduling (DCS) mitigates the I/O bottleneck by overlapping data movement and computation. Finally, a Dynamic PIM Access (DPA) controller enables dynamic memory management to eliminate static memory waste. Implemented via an MLIR-based compiler and evaluated on a cycle-accurate simulator, PIMphony significantly improves throughput for long-context LLM inference (up to 72B parameters and 1M context length). Our evaluations show performance boosts of up to 11.3x on PIM-only systems and 8.4x on xPU+PIM systems, enabling more efficient deployment of LLMs in real-world long-context applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The expansion of long-context Large Language Models (LLMs) creates significant memory system challenges. While Processing-in-Memory (PIM) is a promising accelerator, we identify that it suffers from critical inefficiencies when scaled to long contexts: severe channel underutilization, performance-limiting I/O bottlenecks, and massive memory waste from static KV cache management. In this work, we propose PIMphony, a PIM orchestrator that systematically resolves these issues with three co-designed techniques. First, Token-Centric PIM Partitioning (TCP) ensures high channel utilization regardless of batch size. Second, Dynamic PIM Command Scheduling (DCS) mitigates the I/O bottleneck by overlapping data movement and computation. Finally, a Dynamic PIM Access (DPA) controller enables dynamic memory management to eliminate static memory waste. Implemented via an MLIR-based compiler and evaluated on a cycle-accurate simulator, PIMphony significantly improves throughput for long-context LLM inference (up to 72B parameters and 1M context length). Our evaluations show performance boosts of up to 11.3x on PIM-only systems and 8.4x on xPU+PIM systems, enabling more efficient deployment of LLMs in real-world long-context applications."
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2024-12-28T14:38:16Z",
                "published_parsed": [
                    2024,
                    12,
                    28,
                    14,
                    38,
                    16,
                    5,
                    363,
                    0
                ],
                "arxiv_comment": "21 pages, 20 figures, Accepted to 2026 IEEE International Symposium on High-Performance Computer Architecture",
                "arxiv_primary_category": {
                    "term": "cs.AR"
                },
                "authors": [
                    {
                        "name": "Hyucksung Kwon"
                    },
                    {
                        "name": "Kyungmo Koo"
                    },
                    {
                        "name": "Janghyeon Kim"
                    },
                    {
                        "name": "Woongkyu Lee"
                    },
                    {
                        "name": "Minjae Lee"
                    },
                    {
                        "name": "Gyeonggeun Jung"
                    },
                    {
                        "name": "Hyungdeok Lee"
                    },
                    {
                        "name": "Yousub Jung"
                    },
                    {
                        "name": "Jaehan Park"
                    },
                    {
                        "name": "Yosub Song"
                    },
                    {
                        "name": "Byeongsu Yang"
                    },
                    {
                        "name": "Haerang Choi"
                    },
                    {
                        "name": "Guhyun Kim"
                    },
                    {
                        "name": "Jongsoon Won"
                    },
                    {
                        "name": "Woojae Shin"
                    },
                    {
                        "name": "Changhyun Kim"
                    },
                    {
                        "name": "Gyeongcheol Shin"
                    },
                    {
                        "name": "Yongkee Kwon"
                    },
                    {
                        "name": "Ilkon Kim"
                    },
                    {
                        "name": "Euicheol Lim"
                    },
                    {
                        "name": "John Kim"
                    },
                    {
                        "name": "Jungwook Choi"
                    }
                ],
                "author_detail": {
                    "name": "Jungwook Choi"
                },
                "author": "Jungwook Choi"
            },
            {
                "id": "http://arxiv.org/abs/2512.21338v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.21338v2",
                "title": "HiStream: Efficient High-Resolution Video Generation via Redundancy-Eliminated Streaming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HiStream: Efficient High-Resolution Video Generation via Redundancy-Eliminated Streaming"
                },
                "updated": "2025-12-25T14:18:18Z",
                "updated_parsed": [
                    2025,
                    12,
                    25,
                    14,
                    18,
                    18,
                    3,
                    359,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.21338v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.21338v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "High-resolution video generation, while crucial for digital media and film, is computationally bottlenecked by the quadratic complexity of diffusion models, making practical inference infeasible. To address this, we introduce HiStream, an efficient autoregressive framework that systematically reduces redundancy across three axes: i) Spatial Compression: denoising at low resolution before refining at high resolution with cached features; ii) Temporal Compression: a chunk-by-chunk strategy with a fixed-size anchor cache, ensuring stable inference speed; and iii) Timestep Compression: applying fewer denoising steps to subsequent, cache-conditioned chunks. On 1080p benchmarks, our primary HiStream model (i+ii) achieves state-of-the-art visual quality while demonstrating up to 76.2x faster denoising compared to the Wan2.1 baseline and negligible quality loss. Our faster variant, HiStream+, applies all three optimizations (i+ii+iii), achieving a 107.5x acceleration over the baseline, offering a compelling trade-off between speed and quality, thereby making high-resolution video generation both practical and scalable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-resolution video generation, while crucial for digital media and film, is computationally bottlenecked by the quadratic complexity of diffusion models, making practical inference infeasible. To address this, we introduce HiStream, an efficient autoregressive framework that systematically reduces redundancy across three axes: i) Spatial Compression: denoising at low resolution before refining at high resolution with cached features; ii) Temporal Compression: a chunk-by-chunk strategy with a fixed-size anchor cache, ensuring stable inference speed; and iii) Timestep Compression: applying fewer denoising steps to subsequent, cache-conditioned chunks. On 1080p benchmarks, our primary HiStream model (i+ii) achieves state-of-the-art visual quality while demonstrating up to 76.2x faster denoising compared to the Wan2.1 baseline and negligible quality loss. Our faster variant, HiStream+, applies all three optimizations (i+ii+iii), achieving a 107.5x acceleration over the baseline, offering a compelling trade-off between speed and quality, thereby making high-resolution video generation both practical and scalable."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-24T18:59:58Z",
                "published_parsed": [
                    2025,
                    12,
                    24,
                    18,
                    59,
                    58,
                    2,
                    358,
                    0
                ],
                "arxiv_comment": "Project Page: http://haonanqiu.com/projects/HiStream.html",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Haonan Qiu"
                    },
                    {
                        "name": "Shikun Liu"
                    },
                    {
                        "name": "Zijian Zhou"
                    },
                    {
                        "name": "Zhaochong An"
                    },
                    {
                        "name": "Weiming Ren"
                    },
                    {
                        "name": "Zhiheng Liu"
                    },
                    {
                        "name": "Jonas Schult"
                    },
                    {
                        "name": "Sen He"
                    },
                    {
                        "name": "Shoufa Chen"
                    },
                    {
                        "name": "Yuren Cong"
                    },
                    {
                        "name": "Tao Xiang"
                    },
                    {
                        "name": "Ziwei Liu"
                    },
                    {
                        "name": "Juan-Manuel Perez-Rua"
                    }
                ],
                "author_detail": {
                    "name": "Juan-Manuel Perez-Rua"
                },
                "author": "Juan-Manuel Perez-Rua"
            },
            {
                "id": "http://arxiv.org/abs/2512.21615v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.21615v1",
                "title": "Embedding Samples Dispatching for Recommendation Model Training in Edge Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embedding Samples Dispatching for Recommendation Model Training in Edge Environments"
                },
                "updated": "2025-12-25T10:23:14Z",
                "updated_parsed": [
                    2025,
                    12,
                    25,
                    10,
                    23,
                    14,
                    3,
                    359,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.21615v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.21615v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Training deep learning recommendation models (DLRMs) on edge workers brings several benefits, particularly in terms of data privacy protection, low latency and personalization. However, due to the huge size of embedding tables, typical DLRM training frameworks adopt one or more parameter servers to maintain global embedding tables, while leveraging the edge workers cache part of them. This incurs significant transmission cost for embedding transmissions between workers and parameter servers, which can dominate the training cycle. In this paper, we investigate how to dispatch input embedding samples to appropriate edge workers to minimize the total embedding transmission cost when facing edge-specific challenges such as heterogeneous networks and limited resources. We develop ESD, a novel mechanism that optimizes the dispatch of input embedding samples to edge workers based on expected embedding transmission cost. We propose HybridDis as the dispatch decision method within ESD, which combines a resource-intensive optimal algorithm and a heuristic algorithm to balance decision quality and resource consumption. We implement a prototype of ESD and compare it with state-of-the-art mechanisms on real-world workloads. Extensive experimental results show that ESD reduces the embedding transmission cost by up to 36.76% and achieves up to 1.74 times speedup in end-to-end DLRM training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training deep learning recommendation models (DLRMs) on edge workers brings several benefits, particularly in terms of data privacy protection, low latency and personalization. However, due to the huge size of embedding tables, typical DLRM training frameworks adopt one or more parameter servers to maintain global embedding tables, while leveraging the edge workers cache part of them. This incurs significant transmission cost for embedding transmissions between workers and parameter servers, which can dominate the training cycle. In this paper, we investigate how to dispatch input embedding samples to appropriate edge workers to minimize the total embedding transmission cost when facing edge-specific challenges such as heterogeneous networks and limited resources. We develop ESD, a novel mechanism that optimizes the dispatch of input embedding samples to edge workers based on expected embedding transmission cost. We propose HybridDis as the dispatch decision method within ESD, which combines a resource-intensive optimal algorithm and a heuristic algorithm to balance decision quality and resource consumption. We implement a prototype of ESD and compare it with state-of-the-art mechanisms on real-world workloads. Extensive experimental results show that ESD reduces the embedding transmission cost by up to 36.76% and achieves up to 1.74 times speedup in end-to-end DLRM training."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-25T10:23:14Z",
                "published_parsed": [
                    2025,
                    12,
                    25,
                    10,
                    23,
                    14,
                    3,
                    359,
                    0
                ],
                "arxiv_comment": "This paper is an English version of Samples Dispatching Mechanism for Accelerating Recommendation Model Training in Edge Intelligent Computing System published in 2025 in the Journal of Computer Research and Development",
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Guopeng Li"
                    },
                    {
                        "name": "Haisheng Tan"
                    },
                    {
                        "name": "Chi Zhang"
                    },
                    {
                        "name": "Hongqiu Ni"
                    },
                    {
                        "name": "Zilong Wang"
                    },
                    {
                        "name": "Xinyue Zhang"
                    },
                    {
                        "name": "Yang Xu"
                    },
                    {
                        "name": "Han Tian"
                    }
                ],
                "author_detail": {
                    "name": "Han Tian"
                },
                "author": "Han Tian"
            },
            {
                "id": "http://arxiv.org/abs/2512.21571v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.21571v1",
                "title": "nncase: An End-to-End Compiler for Efficient LLM Deployment on Heterogeneous Storage Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "nncase: An End-to-End Compiler for Efficient LLM Deployment on Heterogeneous Storage Architectures"
                },
                "updated": "2025-12-25T08:27:53Z",
                "updated_parsed": [
                    2025,
                    12,
                    25,
                    8,
                    27,
                    53,
                    3,
                    359,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.21571v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.21571v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The efficient deployment of large language models (LLMs) is hindered by memory architecture heterogeneity, where traditional compilers suffer from fragmented workflows and high adaptation costs. We present nncase, an open-source, end-to-end compilation framework designed to unify optimization across diverse targets. Central to nncase is an e-graph-based term rewriting engine that mitigates the phase ordering problem, enabling global exploration of computation and data movement strategies. The framework integrates three key modules: Auto Vectorize for adapting to heterogeneous computing units, Auto Distribution for searching parallel strategies with cost-aware communication optimization, and Auto Schedule for maximizing on-chip cache locality. Furthermore, a buffer-aware Codegen phase ensures efficient kernel instantiation. Evaluations show that nncase outperforms mainstream frameworks like MLC LLM and Intel IPEX on Qwen3 series models and achieves performance comparable to the hand-optimized llama.cpp on CPUs, demonstrating the viability of automated compilation for high-performance LLM deployment. The source code is available at https://github.com/kendryte/nncase.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The efficient deployment of large language models (LLMs) is hindered by memory architecture heterogeneity, where traditional compilers suffer from fragmented workflows and high adaptation costs. We present nncase, an open-source, end-to-end compilation framework designed to unify optimization across diverse targets. Central to nncase is an e-graph-based term rewriting engine that mitigates the phase ordering problem, enabling global exploration of computation and data movement strategies. The framework integrates three key modules: Auto Vectorize for adapting to heterogeneous computing units, Auto Distribution for searching parallel strategies with cost-aware communication optimization, and Auto Schedule for maximizing on-chip cache locality. Furthermore, a buffer-aware Codegen phase ensures efficient kernel instantiation. Evaluations show that nncase outperforms mainstream frameworks like MLC LLM and Intel IPEX on Qwen3 series models and achieves performance comparable to the hand-optimized llama.cpp on CPUs, demonstrating the viability of automated compilation for high-performance LLM deployment. The source code is available at https://github.com/kendryte/nncase."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-25T08:27:53Z",
                "published_parsed": [
                    2025,
                    12,
                    25,
                    8,
                    27,
                    53,
                    3,
                    359,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Hui Guo"
                    },
                    {
                        "name": "Qihang Zheng"
                    },
                    {
                        "name": "Chenghai Huo"
                    },
                    {
                        "name": "Dongliang Guo"
                    },
                    {
                        "name": "Haoqi Yang"
                    },
                    {
                        "name": "Yang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yang Zhang"
                },
                "author": "Yang Zhang"
            },
            {
                "id": "http://arxiv.org/abs/2507.02227v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2507.02227v2",
                "title": "PhysicsCorrect: A Training-Free Approach for Stable Neural PDE Simulations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PhysicsCorrect: A Training-Free Approach for Stable Neural PDE Simulations"
                },
                "updated": "2025-12-25T05:02:40Z",
                "updated_parsed": [
                    2025,
                    12,
                    25,
                    5,
                    2,
                    40,
                    3,
                    359,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2507.02227v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2507.02227v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Neural networks have emerged as powerful surrogates for solving partial differential equations (PDEs), offering significant computational speedups over traditional methods. However, these models suffer from a critical limitation: error accumulation during long-term rollouts, where small inaccuracies compound exponentially, eventually causing complete divergence from physically valid solutions. We present PhysicsCorrect, a training-free correction framework that enforces PDE consistency at each prediction step by formulating correction as a linearized inverse problem based on PDE residuals. Our key innovation is an efficient caching strategy that precomputes the Jacobian and its pseudoinverse during an offline warm-up phase, reducing computational overhead by two orders of magnitude compared to standard correction approaches. Across three representative PDE systems, including Navier-Stokes fluid dynamics, wave equations, and the chaotic Kuramoto-Sivashinsky equation, PhysicsCorrect reduces prediction errors by up to 100x while adding negligible inference time (under 5%). The framework integrates seamlessly with diverse architectures, including Fourier Neural Operators, UNets, and Vision Transformers, effectively transforming unstable neural surrogates into reliable simulation tools that bridge the gap between deep learning's computational efficiency and the physical fidelity demanded by practical scientific applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural networks have emerged as powerful surrogates for solving partial differential equations (PDEs), offering significant computational speedups over traditional methods. However, these models suffer from a critical limitation: error accumulation during long-term rollouts, where small inaccuracies compound exponentially, eventually causing complete divergence from physically valid solutions. We present PhysicsCorrect, a training-free correction framework that enforces PDE consistency at each prediction step by formulating correction as a linearized inverse problem based on PDE residuals. Our key innovation is an efficient caching strategy that precomputes the Jacobian and its pseudoinverse during an offline warm-up phase, reducing computational overhead by two orders of magnitude compared to standard correction approaches. Across three representative PDE systems, including Navier-Stokes fluid dynamics, wave equations, and the chaotic Kuramoto-Sivashinsky equation, PhysicsCorrect reduces prediction errors by up to 100x while adding negligible inference time (under 5%). The framework integrates seamlessly with diverse architectures, including Fourier Neural Operators, UNets, and Vision Transformers, effectively transforming unstable neural surrogates into reliable simulation tools that bridge the gap between deep learning's computational efficiency and the physical fidelity demanded by practical scientific applications."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-07-03T01:22:57Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    1,
                    22,
                    57,
                    3,
                    184,
                    0
                ],
                "arxiv_comment": "AAAI 2026 Oral",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "arxiv_journal_ref": "Proceedings of the AAAI Conference on Artificial Intelligence, 2026",
                "authors": [
                    {
                        "name": "Xinquan Huang"
                    },
                    {
                        "name": "Paris Perdikaris"
                    }
                ],
                "author_detail": {
                    "name": "Paris Perdikaris"
                },
                "author": "Paris Perdikaris"
            },
            {
                "id": "http://arxiv.org/abs/2512.21487v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.21487v1",
                "title": "Efficient MoE Inference with Fine-Grained Scheduling of Disaggregated Expert Parallelism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient MoE Inference with Fine-Grained Scheduling of Disaggregated Expert Parallelism"
                },
                "updated": "2025-12-25T03:22:03Z",
                "updated_parsed": [
                    2025,
                    12,
                    25,
                    3,
                    22,
                    3,
                    3,
                    359,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.21487v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.21487v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The mixture-of-experts (MoE) architecture scales model size with sublinear computational increase but suffers from memory-intensive inference due to KV caches and sparse expert activation. Recent disaggregated expert parallelism (DEP) distributes attention and experts to dedicated GPU groups but lacks support for shared experts and efficient task scheduling, limiting performance.\n  We propose FinDEP, a fine-grained task scheduling algorithm for DEP that maximizes task overlap to improve MoE inference throughput. FinDEP introduces three innovations: 1) partitioning computation/communication into smaller tasks for fine-grained pipelining, 2) formulating a scheduling optimization supporting variable granularity and ordering, and 3) developing an efficient solver for this large search space.\n  Experiments on four GPU systems with DeepSeek-V2 and Qwen3-MoE show FinDEP improves throughput by up to 1.61x over prior methods, achieving up to 1.24x speedup on a 32-GPU system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The mixture-of-experts (MoE) architecture scales model size with sublinear computational increase but suffers from memory-intensive inference due to KV caches and sparse expert activation. Recent disaggregated expert parallelism (DEP) distributes attention and experts to dedicated GPU groups but lacks support for shared experts and efficient task scheduling, limiting performance.\n  We propose FinDEP, a fine-grained task scheduling algorithm for DEP that maximizes task overlap to improve MoE inference throughput. FinDEP introduces three innovations: 1) partitioning computation/communication into smaller tasks for fine-grained pipelining, 2) formulating a scheduling optimization supporting variable granularity and ordering, and 3) developing an efficient solver for this large search space.\n  Experiments on four GPU systems with DeepSeek-V2 and Qwen3-MoE show FinDEP improves throughput by up to 1.61x over prior methods, achieving up to 1.24x speedup on a 32-GPU system."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-25T03:22:03Z",
                "published_parsed": [
                    2025,
                    12,
                    25,
                    3,
                    22,
                    3,
                    3,
                    359,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Xinglin Pan"
                    },
                    {
                        "name": "Shaohuai Shi"
                    },
                    {
                        "name": "Wenxiang Lin"
                    },
                    {
                        "name": "Yuxin Wang"
                    },
                    {
                        "name": "Zhenheng Tang"
                    },
                    {
                        "name": "Wei Wang"
                    },
                    {
                        "name": "Xiaowen Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Chu"
                },
                "author": "Xiaowen Chu"
            },
            {
                "id": "http://arxiv.org/abs/2512.21473v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.21473v1",
                "title": "Demystifying ARM SME to Optimize General Matrix Multiplications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demystifying ARM SME to Optimize General Matrix Multiplications"
                },
                "updated": "2025-12-25T02:25:59Z",
                "updated_parsed": [
                    2025,
                    12,
                    25,
                    2,
                    25,
                    59,
                    3,
                    359,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.21473v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.21473v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "General Matrix Multiplication (GEMM) is a critical kernel in high-performance computing and deep learning. While modern architectures like ARM's Scalable Matrix Extension (SME) introduce dedicated hardware for matrix operations, existing linear algebra libraries fail to fully exploit its potential, particularly for large matrices. This paper presents MpGEMM, an open-source library that leverages key architectural features of SME to optimize GEMM across multiple precisions. Through a systematic characterization of SME, we derive optimization guidelines that inform our design. MpGEMM employs cache-aware partitioning, efficient data packing with on-the-fly transposition, and specialized micro-kernels that utilize multi-vector loads and all available tile registers. Evaluated on an Apple M4 Pro with real-world workloads from DeepSeek and LLaMA, MpGEMM achieves an average speedup of 1.23x over the vendor-optimized Apple Accelerate library and significantly outperforms other open-source alternatives.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "General Matrix Multiplication (GEMM) is a critical kernel in high-performance computing and deep learning. While modern architectures like ARM's Scalable Matrix Extension (SME) introduce dedicated hardware for matrix operations, existing linear algebra libraries fail to fully exploit its potential, particularly for large matrices. This paper presents MpGEMM, an open-source library that leverages key architectural features of SME to optimize GEMM across multiple precisions. Through a systematic characterization of SME, we derive optimization guidelines that inform our design. MpGEMM employs cache-aware partitioning, efficient data packing with on-the-fly transposition, and specialized micro-kernels that utilize multi-vector loads and all available tile registers. Evaluated on an Apple M4 Pro with real-world workloads from DeepSeek and LLaMA, MpGEMM achieves an average speedup of 1.23x over the vendor-optimized Apple Accelerate library and significantly outperforms other open-source alternatives."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-25T02:25:59Z",
                "published_parsed": [
                    2025,
                    12,
                    25,
                    2,
                    25,
                    59,
                    3,
                    359,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Chencheng Deng"
                    },
                    {
                        "name": "Weiling Yang"
                    },
                    {
                        "name": "Jianbin Fang"
                    },
                    {
                        "name": "Dezun Dong"
                    }
                ],
                "author_detail": {
                    "name": "Dezun Dong"
                },
                "author": "Dezun Dong"
            },
            {
                "id": "http://arxiv.org/abs/2512.21295v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.21295v1",
                "title": "Enhancing Grid Resilience for Giga-Watt Scale Data Centers Using High Voltage Circuit Breaker Operated Braking Resistors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Grid Resilience for Giga-Watt Scale Data Centers Using High Voltage Circuit Breaker Operated Braking Resistors"
                },
                "updated": "2025-12-24T17:23:48Z",
                "updated_parsed": [
                    2025,
                    12,
                    24,
                    17,
                    23,
                    48,
                    2,
                    358,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.21295v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.21295v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "As hyperscale and co-located data centers scale, the electric grid sees an increase in large, voltage-sensitive IT loads with these data center plant size ranging between 500 MW to 2 GW. A sudden loss of these loads as they switch to onsite UPS during grid voltage excursion events causes a grid frequency rise from generation and load imbalance, and a voltage rise because less power is flowing through the network. This paper proposes and theoretically demonstrates the use of high voltage circuit breaker operated braking resistors at data center transmission substations as an effective strategy in enhancing grid resilience under such large load loss scenarios. We developed a test bed to illustrate the dynamic behavior of the system with resistive braking on a gigawatt scale data center load cluster connected to a 345 kV network. The braking resistor(s), which in the case of inverter rich system comes in a multi-stage configuration, are connected or disconnected via high-speed circuit breaker(s). Results show that insertion for 0.25 to 0.85 seconds sufficiently reduce rate of change of frequency and provides time for primary governor response and capacitor switching to restore steady state. Sensitivity across different synchronous machines and inverter-based resource mix are tested and confirms robustness. We conclude circuit breaker controlled resistive braking is a practical means to enhance Bulk Electric System (BES) resilience for gigawatt scale data centers. The approach integrates with protection, needs no generator changes, and can be scaled with cluster size or growth of the data center facility load.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As hyperscale and co-located data centers scale, the electric grid sees an increase in large, voltage-sensitive IT loads with these data center plant size ranging between 500 MW to 2 GW. A sudden loss of these loads as they switch to onsite UPS during grid voltage excursion events causes a grid frequency rise from generation and load imbalance, and a voltage rise because less power is flowing through the network. This paper proposes and theoretically demonstrates the use of high voltage circuit breaker operated braking resistors at data center transmission substations as an effective strategy in enhancing grid resilience under such large load loss scenarios. We developed a test bed to illustrate the dynamic behavior of the system with resistive braking on a gigawatt scale data center load cluster connected to a 345 kV network. The braking resistor(s), which in the case of inverter rich system comes in a multi-stage configuration, are connected or disconnected via high-speed circuit breaker(s). Results show that insertion for 0.25 to 0.85 seconds sufficiently reduce rate of change of frequency and provides time for primary governor response and capacitor switching to restore steady state. Sensitivity across different synchronous machines and inverter-based resource mix are tested and confirms robustness. We conclude circuit breaker controlled resistive braking is a practical means to enhance Bulk Electric System (BES) resilience for gigawatt scale data centers. The approach integrates with protection, needs no generator changes, and can be scaled with cluster size or growth of the data center facility load."
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-24T17:23:48Z",
                "published_parsed": [
                    2025,
                    12,
                    24,
                    17,
                    23,
                    48,
                    2,
                    358,
                    0
                ],
                "arxiv_comment": "Provincially accepted for publication in 2025 IEEE International Conference on Energy Technologies for Future Grids (ETFG) conference proceedings",
                "arxiv_primary_category": {
                    "term": "eess.SY"
                },
                "authors": [
                    {
                        "name": "Soham Ghosh"
                    },
                    {
                        "name": "Mohammad Ashraf Hossain Sadi"
                    }
                ],
                "author_detail": {
                    "name": "Mohammad Ashraf Hossain Sadi"
                },
                "author": "Mohammad Ashraf Hossain Sadi"
            },
            {
                "id": "http://arxiv.org/abs/2503.12491v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2503.12491v2",
                "title": "CAKE: Cascading and Adaptive KV Cache Eviction with Layer Preferences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAKE: Cascading and Adaptive KV Cache Eviction with Layer Preferences"
                },
                "updated": "2025-12-24T13:21:40Z",
                "updated_parsed": [
                    2025,
                    12,
                    24,
                    13,
                    21,
                    40,
                    2,
                    358,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2503.12491v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2503.12491v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models (LLMs) excel at processing long sequences, boosting demand for key-value (KV) caching. While recent efforts to evict KV cache have alleviated the inference burden, they often fail to allocate resources rationally across layers with different attention patterns. In this paper, we introduce Cascading and Adaptive KV cache Eviction (CAKE), a novel approach that frames KV cache eviction as a \"cake-slicing problem.\" CAKE assesses layer-specific preferences by considering attention dynamics in both spatial and temporal dimensions, allocates rational cache size for layers accordingly, and manages memory constraints in a cascading manner. This approach enables a global view of cache allocation, adaptively distributing resources across diverse attention mechanisms while maintaining memory budgets. CAKE also employs a new eviction indicator that considers the shifting importance of tokens over time, addressing limitations in existing methods that overlook temporal dynamics. Comprehensive experiments on LongBench and NeedleBench show that CAKE maintains model performance with only 3.2% of the KV cache and consistently outperforms current baselines across various models and memory constraints, particularly in low-memory settings. Additionally, CAKE achieves over 10x speedup in decoding latency compared to full cache when processing contexts of 128K tokens with FlashAttention-2. Our code is available at https://github.com/antgroup/cakekv.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) excel at processing long sequences, boosting demand for key-value (KV) caching. While recent efforts to evict KV cache have alleviated the inference burden, they often fail to allocate resources rationally across layers with different attention patterns. In this paper, we introduce Cascading and Adaptive KV cache Eviction (CAKE), a novel approach that frames KV cache eviction as a \"cake-slicing problem.\" CAKE assesses layer-specific preferences by considering attention dynamics in both spatial and temporal dimensions, allocates rational cache size for layers accordingly, and manages memory constraints in a cascading manner. This approach enables a global view of cache allocation, adaptively distributing resources across diverse attention mechanisms while maintaining memory budgets. CAKE also employs a new eviction indicator that considers the shifting importance of tokens over time, addressing limitations in existing methods that overlook temporal dynamics. Comprehensive experiments on LongBench and NeedleBench show that CAKE maintains model performance with only 3.2% of the KV cache and consistently outperforms current baselines across various models and memory constraints, particularly in low-memory settings. Additionally, CAKE achieves over 10x speedup in decoding latency compared to full cache when processing contexts of 128K tokens with FlashAttention-2. Our code is available at https://github.com/antgroup/cakekv."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-03-16T12:49:44Z",
                "published_parsed": [
                    2025,
                    3,
                    16,
                    12,
                    49,
                    44,
                    6,
                    75,
                    0
                ],
                "arxiv_comment": "Accepted by ICLR 2025",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Ziran Qin"
                    },
                    {
                        "name": "Yuchen Cao"
                    },
                    {
                        "name": "Mingbao Lin"
                    },
                    {
                        "name": "Wen Hu"
                    },
                    {
                        "name": "Shixuan Fan"
                    },
                    {
                        "name": "Ke Cheng"
                    },
                    {
                        "name": "Weiyao Lin"
                    },
                    {
                        "name": "Jianguo Li"
                    }
                ],
                "author_detail": {
                    "name": "Jianguo Li"
                },
                "author": "Jianguo Li"
            },
            {
                "id": "http://arxiv.org/abs/2512.12284v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.12284v3",
                "title": "V-Rex: Real-Time Streaming Video LLM Acceleration via Dynamic KV Cache Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "V-Rex: Real-Time Streaming Video LLM Acceleration via Dynamic KV Cache Retrieval"
                },
                "updated": "2025-12-24T07:46:59Z",
                "updated_parsed": [
                    2025,
                    12,
                    24,
                    7,
                    46,
                    59,
                    2,
                    358,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.12284v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.12284v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Streaming video large language models (LLMs) are increasingly used for real-time multimodal tasks such as video captioning, question answering, conversational agents, and augmented reality. However, these models face fundamental memory and computational challenges because their key-value (KV) caches grow substantially with continuous streaming video input. This process requires an iterative prefill stage, which is a unique feature of streaming video LLMs. Due to its iterative prefill stage, it suffers from significant limitations, including extensive computation, substantial data transfer, and degradation in accuracy. Crucially, this issue is exacerbated for edge deployment, which is the primary target for these models.\n  In this work, we propose V-Rex, the first software-hardware co-designed accelerator that comprehensively addresses both algorithmic and hardware bottlenecks in streaming video LLM inference. At its core, V-Rex introduces ReSV, a training-free dynamic KV cache retrieval algorithm. ReSV exploits temporal and spatial similarity-based token clustering to reduce excessive KV cache memory across video frames. To fully realize these algorithmic benefits, V-Rex offers a compact, low-latency hardware accelerator with a dynamic KV cache retrieval engine (DRE), featuring bit-level and early-exit based computing units. V-Rex achieves unprecedented real-time of 3.9-8.3 FPS and energy-efficient streaming video LLM inference on edge deployment with negligible accuracy loss. While DRE only accounts for 2.2% power and 2.0% area, the system delivers 1.9-19.7x speedup and 3.1-18.5x energy efficiency improvements over AGX Orin GPU. This work is the first to comprehensively tackle KV cache retrieval across algorithms and hardware, enabling real-time streaming video LLM inference on resource-constrained edge devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Streaming video large language models (LLMs) are increasingly used for real-time multimodal tasks such as video captioning, question answering, conversational agents, and augmented reality. However, these models face fundamental memory and computational challenges because their key-value (KV) caches grow substantially with continuous streaming video input. This process requires an iterative prefill stage, which is a unique feature of streaming video LLMs. Due to its iterative prefill stage, it suffers from significant limitations, including extensive computation, substantial data transfer, and degradation in accuracy. Crucially, this issue is exacerbated for edge deployment, which is the primary target for these models.\n  In this work, we propose V-Rex, the first software-hardware co-designed accelerator that comprehensively addresses both algorithmic and hardware bottlenecks in streaming video LLM inference. At its core, V-Rex introduces ReSV, a training-free dynamic KV cache retrieval algorithm. ReSV exploits temporal and spatial similarity-based token clustering to reduce excessive KV cache memory across video frames. To fully realize these algorithmic benefits, V-Rex offers a compact, low-latency hardware accelerator with a dynamic KV cache retrieval engine (DRE), featuring bit-level and early-exit based computing units. V-Rex achieves unprecedented real-time of 3.9-8.3 FPS and energy-efficient streaming video LLM inference on edge deployment with negligible accuracy loss. While DRE only accounts for 2.2% power and 2.0% area, the system delivers 1.9-19.7x speedup and 3.1-18.5x energy efficiency improvements over AGX Orin GPU. This work is the first to comprehensively tackle KV cache retrieval across algorithms and hardware, enabling real-time streaming video LLM inference on resource-constrained edge devices."
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-13T11:02:04Z",
                "published_parsed": [
                    2025,
                    12,
                    13,
                    11,
                    2,
                    4,
                    5,
                    347,
                    0
                ],
                "arxiv_comment": "14 pages, 20 figures, conference, accepted by HPCA 2026",
                "arxiv_primary_category": {
                    "term": "eess.IV"
                },
                "authors": [
                    {
                        "name": "Donghyuk Kim"
                    },
                    {
                        "name": "Sejeong Yang"
                    },
                    {
                        "name": "Wonjin Shin"
                    },
                    {
                        "name": "Joo-Young Kim"
                    }
                ],
                "author_detail": {
                    "name": "Joo-Young Kim"
                },
                "author": "Joo-Young Kim"
            },
            {
                "id": "http://arxiv.org/abs/2512.20920v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20920v1",
                "title": "RevFFN: Memory-Efficient Full-Parameter Fine-Tuning of Mixture-of-Experts LLMs with Reversible Blocks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RevFFN: Memory-Efficient Full-Parameter Fine-Tuning of Mixture-of-Experts LLMs with Reversible Blocks"
                },
                "updated": "2025-12-24T03:56:58Z",
                "updated_parsed": [
                    2025,
                    12,
                    24,
                    3,
                    56,
                    58,
                    2,
                    358,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20920v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20920v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Full parameter fine tuning is a key technique for adapting large language models (LLMs) to downstream tasks, but it incurs substantial memory overhead due to the need to cache extensive intermediate activations for backpropagation. This bottleneck makes full fine tuning of contemporary large scale LLMs challenging in practice. Existing distributed training frameworks such as DeepSpeed alleviate this issue using techniques like ZeRO and FSDP, which rely on multi GPU memory or CPU offloading, but often require additional hardware resources and reduce training speed. We introduce RevFFN, a memory efficient fine tuning paradigm for mixture of experts (MoE) LLMs. RevFFN employs carefully designed reversible Transformer blocks that allow reconstruction of layer input activations from outputs during backpropagation, eliminating the need to store most intermediate activations in memory. While preserving the expressive capacity of MoE architectures, this approach significantly reduces peak memory consumption for full parameter fine tuning. As a result, RevFFN enables efficient full fine tuning on a single consumer grade or server grade GPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Full parameter fine tuning is a key technique for adapting large language models (LLMs) to downstream tasks, but it incurs substantial memory overhead due to the need to cache extensive intermediate activations for backpropagation. This bottleneck makes full fine tuning of contemporary large scale LLMs challenging in practice. Existing distributed training frameworks such as DeepSpeed alleviate this issue using techniques like ZeRO and FSDP, which rely on multi GPU memory or CPU offloading, but often require additional hardware resources and reduce training speed. We introduce RevFFN, a memory efficient fine tuning paradigm for mixture of experts (MoE) LLMs. RevFFN employs carefully designed reversible Transformer blocks that allow reconstruction of layer input activations from outputs during backpropagation, eliminating the need to store most intermediate activations in memory. While preserving the expressive capacity of MoE architectures, this approach significantly reduces peak memory consumption for full parameter fine tuning. As a result, RevFFN enables efficient full fine tuning on a single consumer grade or server grade GPU."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-24T03:56:58Z",
                "published_parsed": [
                    2025,
                    12,
                    24,
                    3,
                    56,
                    58,
                    2,
                    358,
                    0
                ],
                "arxiv_comment": "Under submission",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Ningyuan Liu"
                    },
                    {
                        "name": "Jing Yang"
                    },
                    {
                        "name": "Kaitong Cai"
                    },
                    {
                        "name": "Keze Wang"
                    }
                ],
                "author_detail": {
                    "name": "Keze Wang"
                },
                "author": "Keze Wang"
            },
            {
                "id": "http://arxiv.org/abs/2512.15713v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.15713v2",
                "title": "DiffusionVL: Translating Any Autoregressive Models into Diffusion Vision Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiffusionVL: Translating Any Autoregressive Models into Diffusion Vision Language Models"
                },
                "updated": "2025-12-24T03:37:34Z",
                "updated_parsed": [
                    2025,
                    12,
                    24,
                    3,
                    37,
                    34,
                    2,
                    358,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.15713v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.15713v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "In recent multimodal research, the diffusion paradigm has emerged as a promising alternative to the autoregressive paradigm (AR), owing to its unique decoding advantages. However, due to the capability limitations of the base diffusion language model, the performance of the diffusion vision language model (dVLM) still lags significantly behind that of mainstream models. This leads to a simple yet fundamental question: Is it possible to construct dVLMs based on existing powerful AR models? In response, we propose DiffusionVL, a dVLM family that could be translated from any powerful AR models. Through simple fine-tuning, we successfully adapt AR pre-trained models into the diffusion paradigm. This approach yields two key observations: (1) The paradigm shift from AR-based multimodal models to diffusion is remarkably effective. (2) Direct conversion of an AR language model to a dVLM is also feasible, achieving performance competitive with LLaVA-style visual-instruction-tuning. Further, we introduce a block-decoding design into dVLMs that supports arbitrary-length generation and KV cache reuse, achieving a significant inference speedup. We conduct a large number of experiments. Despite training with less than 5% of the data required by prior methods, DiffusionVL achieves a comprehensive performance improvement-a 34.4% gain on the MMMU-Pro (vision) bench and 37.5% gain on the MME (Cog.) bench-alongside a 2x inference speedup. The model and code are released at https://github.com/hustvl/DiffusionVL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent multimodal research, the diffusion paradigm has emerged as a promising alternative to the autoregressive paradigm (AR), owing to its unique decoding advantages. However, due to the capability limitations of the base diffusion language model, the performance of the diffusion vision language model (dVLM) still lags significantly behind that of mainstream models. This leads to a simple yet fundamental question: Is it possible to construct dVLMs based on existing powerful AR models? In response, we propose DiffusionVL, a dVLM family that could be translated from any powerful AR models. Through simple fine-tuning, we successfully adapt AR pre-trained models into the diffusion paradigm. This approach yields two key observations: (1) The paradigm shift from AR-based multimodal models to diffusion is remarkably effective. (2) Direct conversion of an AR language model to a dVLM is also feasible, achieving performance competitive with LLaVA-style visual-instruction-tuning. Further, we introduce a block-decoding design into dVLMs that supports arbitrary-length generation and KV cache reuse, achieving a significant inference speedup. We conduct a large number of experiments. Despite training with less than 5% of the data required by prior methods, DiffusionVL achieves a comprehensive performance improvement-a 34.4% gain on the MMMU-Pro (vision) bench and 37.5% gain on the MME (Cog.) bench-alongside a 2x inference speedup. The model and code are released at https://github.com/hustvl/DiffusionVL."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-17T18:59:55Z",
                "published_parsed": [
                    2025,
                    12,
                    17,
                    18,
                    59,
                    55,
                    2,
                    351,
                    0
                ],
                "arxiv_comment": "11 pages, 5 figures, conference or other essential info",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Lunbin Zeng"
                    },
                    {
                        "name": "Jingfeng Yao"
                    },
                    {
                        "name": "Bencheng Liao"
                    },
                    {
                        "name": "Hongyuan Tao"
                    },
                    {
                        "name": "Wenyu Liu"
                    },
                    {
                        "name": "Xinggang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xinggang Wang"
                },
                "author": "Xinggang Wang"
            },
            {
                "id": "http://arxiv.org/abs/2512.20884v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20884v1",
                "title": "The Silent Scholar Problem: A Probabilistic Framework for Breaking Epistemic Asymmetry in LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Silent Scholar Problem: A Probabilistic Framework for Breaking Epistemic Asymmetry in LLM Agents"
                },
                "updated": "2025-12-24T02:02:25Z",
                "updated_parsed": [
                    2025,
                    12,
                    24,
                    2,
                    2,
                    25,
                    2,
                    358,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20884v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20884v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Autonomous agents powered by LLMs and Retrieval-Augmented Generation (RAG) are proficient consumers of digital content but remain unidirectional, a limitation we term epistemic asymmetry. This isolation leads to redundant reasoning and stagnates collective intelligence. Current self-reflection frameworks remain largely heuristic and private, lacking a probabilistic foundation to quantify certainty or justify external interaction.To bridge this gap, we propose a formal probabilistic framework that provides agents with a non-altruistic motive for bidirectional knowledge exchange. We model an agent's belief in a proposition using a Beta-Bernoulli distribution with a forgetting factor ($γ$). This allows us to isolate epistemic uncertainty as the variance of belief, establishing a dual drive for interaction: A homeostatic motive: The need to maintain certainty against the temporal decay introduced by $γ$. An optimal learning strategy: Targeting points of maximum ambiguity ($\\mathbb{E}[θ]=0.5$) to maximize information gain. Under this framework, public contribution is reframed as optimal active learning: sharing solutions to elicit feedback is the most efficient method for an agent to reduce its own uncertainty. To ensure scalability, we introduce epistemic caching, which leverages the forgetting factor to dynamically prioritize resources for the active head of non-stationary knowledge distributions. Finally, we demonstrate how these accumulated belief states serve as verifiable reward signals for Reinforcement Learning from Human Feedback (RLHF) and high-quality data filters for Supervised Fine-Tuning (SFT). Simulation results validate that this uncertainty-driven strategy significantly outperforms random baselines in heterogeneous (Zipfian) environments, maintaining high adaptability to concept drift.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous agents powered by LLMs and Retrieval-Augmented Generation (RAG) are proficient consumers of digital content but remain unidirectional, a limitation we term epistemic asymmetry. This isolation leads to redundant reasoning and stagnates collective intelligence. Current self-reflection frameworks remain largely heuristic and private, lacking a probabilistic foundation to quantify certainty or justify external interaction.To bridge this gap, we propose a formal probabilistic framework that provides agents with a non-altruistic motive for bidirectional knowledge exchange. We model an agent's belief in a proposition using a Beta-Bernoulli distribution with a forgetting factor ($γ$). This allows us to isolate epistemic uncertainty as the variance of belief, establishing a dual drive for interaction: A homeostatic motive: The need to maintain certainty against the temporal decay introduced by $γ$. An optimal learning strategy: Targeting points of maximum ambiguity ($\\mathbb{E}[θ]=0.5$) to maximize information gain. Under this framework, public contribution is reframed as optimal active learning: sharing solutions to elicit feedback is the most efficient method for an agent to reduce its own uncertainty. To ensure scalability, we introduce epistemic caching, which leverages the forgetting factor to dynamically prioritize resources for the active head of non-stationary knowledge distributions. Finally, we demonstrate how these accumulated belief states serve as verifiable reward signals for Reinforcement Learning from Human Feedback (RLHF) and high-quality data filters for Supervised Fine-Tuning (SFT). Simulation results validate that this uncertainty-driven strategy significantly outperforms random baselines in heterogeneous (Zipfian) environments, maintaining high adaptability to concept drift."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-24T02:02:25Z",
                "published_parsed": [
                    2025,
                    12,
                    24,
                    2,
                    2,
                    25,
                    2,
                    358,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Zan-Kai Chong"
                    },
                    {
                        "name": "Hiroyuki Ohsaki"
                    },
                    {
                        "name": "Bryan Ng"
                    }
                ],
                "author_detail": {
                    "name": "Bryan Ng"
                },
                "author": "Bryan Ng"
            },
            {
                "id": "http://arxiv.org/abs/2512.18741v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.18741v2",
                "title": "Memorize-and-Generate: Towards Long-Term Consistency in Real-Time Video Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memorize-and-Generate: Towards Long-Term Consistency in Real-Time Video Generation"
                },
                "updated": "2025-12-23T16:47:46Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    16,
                    47,
                    46,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.18741v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.18741v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Frame-level autoregressive (frame-AR) models have achieved significant progress, enabling real-time video generation comparable to bidirectional diffusion models and serving as a foundation for interactive world models and game engines. However, current approaches in long video generation typically rely on window attention, which naively discards historical context outside the window, leading to catastrophic forgetting and scene inconsistency; conversely, retaining full history incurs prohibitive memory costs. To address this trade-off, we propose Memorize-and-Generate (MAG), a framework that decouples memory compression and frame generation into distinct tasks. Specifically, we train a memory model to compress historical information into a compact KV cache, and a separate generator model to synthesize subsequent frames utilizing this compressed representation. Furthermore, we introduce MAG-Bench to strictly evaluate historical memory retention. Extensive experiments demonstrate that MAG achieves superior historical scene consistency while maintaining competitive performance on standard video generation benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Frame-level autoregressive (frame-AR) models have achieved significant progress, enabling real-time video generation comparable to bidirectional diffusion models and serving as a foundation for interactive world models and game engines. However, current approaches in long video generation typically rely on window attention, which naively discards historical context outside the window, leading to catastrophic forgetting and scene inconsistency; conversely, retaining full history incurs prohibitive memory costs. To address this trade-off, we propose Memorize-and-Generate (MAG), a framework that decouples memory compression and frame generation into distinct tasks. Specifically, we train a memory model to compress historical information into a compact KV cache, and a separate generator model to synthesize subsequent frames utilizing this compressed representation. Furthermore, we introduce MAG-Bench to strictly evaluate historical memory retention. Extensive experiments demonstrate that MAG achieves superior historical scene consistency while maintaining competitive performance on standard video generation benchmarks."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-21T14:02:53Z",
                "published_parsed": [
                    2025,
                    12,
                    21,
                    14,
                    2,
                    53,
                    6,
                    355,
                    0
                ],
                "arxiv_comment": "Code will be released at https://github.com/Xilluill/MAG",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Tianrui Zhu"
                    },
                    {
                        "name": "Shiyi Zhang"
                    },
                    {
                        "name": "Zhirui Sun"
                    },
                    {
                        "name": "Jingqi Tian"
                    },
                    {
                        "name": "Yansong Tang"
                    }
                ],
                "author_detail": {
                    "name": "Yansong Tang"
                },
                "author": "Yansong Tang"
            },
            {
                "id": "http://arxiv.org/abs/2512.20276v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20276v1",
                "title": "ActionFlow: A Pipelined Action Acceleration for Vision Language Models on Edge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ActionFlow: A Pipelined Action Acceleration for Vision Language Models on Edge"
                },
                "updated": "2025-12-23T11:29:03Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    11,
                    29,
                    3,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20276v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20276v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Vision-Language-Action (VLA) models have emerged as a unified paradigm for robotic perception and control, enabling emergent generalization and long-horizon task execution. However, their deployment in dynamic, real-world environments is severely hin dered by high inference latency. While smooth robotic interaction requires control frequencies of 20 to 30 Hz, current VLA models typi cally operate at only 3-5 Hz on edge devices due to the memory bound nature of autoregressive decoding. Existing optimizations often require extensive retraining or compromise model accuracy. To bridge this gap, we introduce ActionFlow, a system-level inference framework tailored for resource-constrained edge plat forms. At the core of ActionFlow is a Cross-Request Pipelin ing strategy, a novel scheduler that redefines VLA inference as a macro-pipeline of micro-requests. The strategy intelligently batches memory-bound Decode phases with compute-bound Prefill phases across continuous time steps to maximize hardware utilization. Furthermore, to support this scheduling, we propose a Cross Request State Packed Forward operator and a Unified KV Ring Buffer, which fuse fragmented memory operations into efficient dense computations. Experimental results demonstrate that ActionFlow achieves a 2.55x improvement in FPS on the OpenVLA-7B model without retraining, enabling real-time dy namic manipulation on edge hardware. Our work is available at https://anonymous.4open.science/r/ActionFlow-1D47.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language-Action (VLA) models have emerged as a unified paradigm for robotic perception and control, enabling emergent generalization and long-horizon task execution. However, their deployment in dynamic, real-world environments is severely hin dered by high inference latency. While smooth robotic interaction requires control frequencies of 20 to 30 Hz, current VLA models typi cally operate at only 3-5 Hz on edge devices due to the memory bound nature of autoregressive decoding. Existing optimizations often require extensive retraining or compromise model accuracy. To bridge this gap, we introduce ActionFlow, a system-level inference framework tailored for resource-constrained edge plat forms. At the core of ActionFlow is a Cross-Request Pipelin ing strategy, a novel scheduler that redefines VLA inference as a macro-pipeline of micro-requests. The strategy intelligently batches memory-bound Decode phases with compute-bound Prefill phases across continuous time steps to maximize hardware utilization. Furthermore, to support this scheduling, we propose a Cross Request State Packed Forward operator and a Unified KV Ring Buffer, which fuse fragmented memory operations into efficient dense computations. Experimental results demonstrate that ActionFlow achieves a 2.55x improvement in FPS on the OpenVLA-7B model without retraining, enabling real-time dy namic manipulation on edge hardware. Our work is available at https://anonymous.4open.science/r/ActionFlow-1D47."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T11:29:03Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    11,
                    29,
                    3,
                    1,
                    357,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Yuntao Dai"
                    },
                    {
                        "name": "Hang Gu"
                    },
                    {
                        "name": "Teng Wang"
                    },
                    {
                        "name": "Qianyu Cheng"
                    },
                    {
                        "name": "Yifei Zheng"
                    },
                    {
                        "name": "Zhiyong Qiu"
                    },
                    {
                        "name": "Lei Gong"
                    },
                    {
                        "name": "Wenqi Lou"
                    },
                    {
                        "name": "Xuehai Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Xuehai Zhou"
                },
                "author": "Xuehai Zhou"
            },
            {
                "id": "http://arxiv.org/abs/2512.20245v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20245v1",
                "title": "Memory as Resonance: A Biomimetic Architecture for Infinite Context Memory on Ergodic Phonetic Manifolds",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory as Resonance: A Biomimetic Architecture for Infinite Context Memory on Ergodic Phonetic Manifolds"
                },
                "updated": "2025-12-23T10:55:32Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    10,
                    55,
                    32,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20245v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20245v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The memory of contemporary Large Language Models is bound by a physical paradox: as they learn, they fill up. The linear accumulation (O(N)) of Key-Value states treats context as a warehouse of static artifacts, eventually forcing a destructive choice between amnesia and latency. We challenge this discrete orthodoxy, proposing that long-term memory is not the storage of items, but the persistence of a trajectory. We introduce Phonetic Trajectory Memory (PTM), a neuro-symbolic architecture that encodes language not as a sequence of tensors, but as a continuous path on an ergodic manifold governed by irrational rotation matrices. By decoupling the navigation (an invariant O(1) geometric signal) from the reconstruction (a probabilistic generative act), PTM achieves a compression magnitude of greater than 3,000x relative to dense caches. We demonstrate that retrieval becomes a process of resonance: the phonetic trace stabilizes the model against hallucination via \"Signal Consensus\" mechanism, securing up to approximately 92% factual accuracy. While this aggressive abstraction alters generative texture, it unlocks immediate access latency (approximately 34ms) independent of depth. Our results suggest that infinite context does not require infinite silicon; it requires treating memory not as data to be stored, but as a reconstructive process acting on a conserved, undying physical signal.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The memory of contemporary Large Language Models is bound by a physical paradox: as they learn, they fill up. The linear accumulation (O(N)) of Key-Value states treats context as a warehouse of static artifacts, eventually forcing a destructive choice between amnesia and latency. We challenge this discrete orthodoxy, proposing that long-term memory is not the storage of items, but the persistence of a trajectory. We introduce Phonetic Trajectory Memory (PTM), a neuro-symbolic architecture that encodes language not as a sequence of tensors, but as a continuous path on an ergodic manifold governed by irrational rotation matrices. By decoupling the navigation (an invariant O(1) geometric signal) from the reconstruction (a probabilistic generative act), PTM achieves a compression magnitude of greater than 3,000x relative to dense caches. We demonstrate that retrieval becomes a process of resonance: the phonetic trace stabilizes the model against hallucination via \"Signal Consensus\" mechanism, securing up to approximately 92% factual accuracy. While this aggressive abstraction alters generative texture, it unlocks immediate access latency (approximately 34ms) independent of depth. Our results suggest that infinite context does not require infinite silicon; it requires treating memory not as data to be stored, but as a reconstructive process acting on a conserved, undying physical signal."
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T10:55:32Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    10,
                    55,
                    32,
                    1,
                    357,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE"
                },
                "authors": [
                    {
                        "name": "Tarik Houichime"
                    },
                    {
                        "name": "Abdelghani Souhar"
                    },
                    {
                        "name": "Younes El Amrani"
                    }
                ],
                "author_detail": {
                    "name": "Younes El Amrani"
                },
                "author": "Younes El Amrani"
            },
            {
                "id": "http://arxiv.org/abs/2512.20201v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20201v1",
                "title": "Joint Design of Embedded Index Coding and Beamforming for MIMO-based Distributed Computing via Multi-Agent Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Joint Design of Embedded Index Coding and Beamforming for MIMO-based Distributed Computing via Multi-Agent Reinforcement Learning"
                },
                "updated": "2025-12-23T09:49:25Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    9,
                    49,
                    25,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20201v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20201v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "In distributed computing systems, reducing the communication load during the data shuffling phase is a critical challenge, as excessive inter-node transmissions are a major performance bottleneck. One promising approach to alleviate this burden is Embedded Index Coding (EIC), which exploits cached data at user nodes to encode transmissions more efficiently. However, most prior work on EIC has focused on minimizing code length in wired, error-free environments-an objective often suboptimal for wireless multiple-input multiple-output (MIMO) systems, where channel conditions and spatial multiplexing gains must be considered. This paper investigates the joint design of EIC and transmit beamforming in MIMO systems to minimize total transmission time, an NP-hard problem. We first present a conventional optimization method that determines the optimal EIC via exhaustive search. To address its prohibitive complexity and adapt to dynamic wireless environments, we propose a novel, low-complexity multi-agent reinforcement learning (MARL) framework. The proposed framework enables decentralized agents to act on local observations while effectively managing the hybrid action space of discrete EIC selection and continuous beamforming design. Simulation results demonstrate that the proposed MARL-based approach achieves near-optimal performance with significantly reduced complexity, underscoring its effectiveness and practicality for real-world wireless systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In distributed computing systems, reducing the communication load during the data shuffling phase is a critical challenge, as excessive inter-node transmissions are a major performance bottleneck. One promising approach to alleviate this burden is Embedded Index Coding (EIC), which exploits cached data at user nodes to encode transmissions more efficiently. However, most prior work on EIC has focused on minimizing code length in wired, error-free environments-an objective often suboptimal for wireless multiple-input multiple-output (MIMO) systems, where channel conditions and spatial multiplexing gains must be considered. This paper investigates the joint design of EIC and transmit beamforming in MIMO systems to minimize total transmission time, an NP-hard problem. We first present a conventional optimization method that determines the optimal EIC via exhaustive search. To address its prohibitive complexity and adapt to dynamic wireless environments, we propose a novel, low-complexity multi-agent reinforcement learning (MARL) framework. The proposed framework enables decentralized agents to act on local observations while effectively managing the hybrid action space of discrete EIC selection and continuous beamforming design. Simulation results demonstrate that the proposed MARL-based approach achieves near-optimal performance with significantly reduced complexity, underscoring its effectiveness and practicality for real-world wireless systems."
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T09:49:25Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    9,
                    49,
                    25,
                    1,
                    357,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY"
                },
                "authors": [
                    {
                        "name": "Heekang Song"
                    },
                    {
                        "name": "Wan Choi"
                    }
                ],
                "author_detail": {
                    "name": "Wan Choi"
                },
                "author": "Wan Choi"
            },
            {
                "id": "http://arxiv.org/abs/2510.23649v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.23649v3",
                "title": "Efficient Low Rank Attention for Long-Context Inference in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Low Rank Attention for Long-Context Inference in Large Language Models"
                },
                "updated": "2025-12-23T08:47:31Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    8,
                    47,
                    31,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.23649v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.23649v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "As the length of input text increases, the key-value (KV) cache in LLMs imposes prohibitive GPU memory costs and limits long-context inference on resource constrained devices. Existing approaches, such as KV quantization and pruning, reduce memory usage but suffer from numerical precision loss or suboptimal retention of key-value pairs. In this work, Low Rank Query and Key attention (LRQK) is introduced, a two-stage framework that jointly decomposes full-precision query and key matrices into compact rank-\\(r\\) factors during the prefill stage, and then employs these low-dimensional projections to compute proxy attention scores in \\(\\mathcal{O}(lr)\\) time at each decode step. By selecting only the top-\\(k\\) tokens and a small fixed set of recent tokens, LRQK employs a mixed GPU-CPU cache with a hit-and-miss mechanism where only missing full-precision KV pairs are transferred, thereby preserving exact attention outputs while reducing CPU-GPU data movement. Extensive experiments on the RULER and LongBench benchmarks with LLaMA-3-8B and Qwen2.5-7B demonstrate that LRQK matches or surpasses leading sparse-attention methods in long context settings, while delivering significant memory savings with minimal accuracy loss. Our code is available at https://github.com/tenghuilee/LRQK.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the length of input text increases, the key-value (KV) cache in LLMs imposes prohibitive GPU memory costs and limits long-context inference on resource constrained devices. Existing approaches, such as KV quantization and pruning, reduce memory usage but suffer from numerical precision loss or suboptimal retention of key-value pairs. In this work, Low Rank Query and Key attention (LRQK) is introduced, a two-stage framework that jointly decomposes full-precision query and key matrices into compact rank-\\(r\\) factors during the prefill stage, and then employs these low-dimensional projections to compute proxy attention scores in \\(\\mathcal{O}(lr)\\) time at each decode step. By selecting only the top-\\(k\\) tokens and a small fixed set of recent tokens, LRQK employs a mixed GPU-CPU cache with a hit-and-miss mechanism where only missing full-precision KV pairs are transferred, thereby preserving exact attention outputs while reducing CPU-GPU data movement. Extensive experiments on the RULER and LongBench benchmarks with LLaMA-3-8B and Qwen2.5-7B demonstrate that LRQK matches or surpasses leading sparse-attention methods in long context settings, while delivering significant memory savings with minimal accuracy loss. Our code is available at https://github.com/tenghuilee/LRQK."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-25T11:43:27Z",
                "published_parsed": [
                    2025,
                    10,
                    25,
                    11,
                    43,
                    27,
                    5,
                    298,
                    0
                ],
                "arxiv_comment": "https://neurips.cc/virtual/2025/loc/san-diego/poster/118451",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "arxiv_journal_ref": "39th Conference on Neural Information Processing Systems (NeurIPS 2025)",
                "authors": [
                    {
                        "name": "Tenghui Li"
                    },
                    {
                        "name": "Guoxu Zhou"
                    },
                    {
                        "name": "Xuyang Zhao"
                    },
                    {
                        "name": "Yuning Qiu"
                    },
                    {
                        "name": "Qibin Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Qibin Zhao"
                },
                "author": "Qibin Zhao"
            },
            {
                "id": "http://arxiv.org/abs/2512.20072v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20072v1",
                "title": "Ultrahigh Charge-to-Spin Conversion and Tunneling Magnetoresistance in Quasi-Two-Dimensional d-wave Altermagnet",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ultrahigh Charge-to-Spin Conversion and Tunneling Magnetoresistance in Quasi-Two-Dimensional d-wave Altermagnet"
                },
                "updated": "2025-12-23T05:52:45Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    5,
                    52,
                    45,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20072v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20072v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The emergence of altermagnets has driven groundbreaking advances in spintronics. Notably, d-wave altermagnets support non-relativistic spin transport, efficient charge-to-spin conversion, and T-odd spin currents. In addition, their integration as electrodes in antiferromagnetic tunnel junctions (AFMTJs) enables a tunneling magnetoresistance (TMR) effect, allowing electrical detection of Néel vectors for next-generation memory devices. In this work, we investigate the non-relativistic spin transport properties of the quasi-two-dimensional (quasi-2D) d-wave altermagnet KV\\textsubscript{2}Se\\textsubscript{2}O and the TMR effect in KV\\textsubscript{2}Se\\textsubscript{2}O-based AFMTJs via first-principles calculations. Our results reveal that KV\\textsubscript{2}Se\\textsubscript{2}O exhibits a non-relativistic longitudinal spin polarization and a spin Hall angle both exceeding 60\\% at room temperature, while KV\\textsubscript{2}Se\\textsubscript{2}O-based AFMTJs achieve a giant TMR reaching approximately $10^{12}$\\%, which remains robust against Fermi level shifts. These findings highlight the anisotropic spin polarization inherent to d-wave staggered magnetism and underscore the critical role of Fermi surface topology in enhancing T-odd spin transport and the TMR effect in AFMTJs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of altermagnets has driven groundbreaking advances in spintronics. Notably, d-wave altermagnets support non-relativistic spin transport, efficient charge-to-spin conversion, and T-odd spin currents. In addition, their integration as electrodes in antiferromagnetic tunnel junctions (AFMTJs) enables a tunneling magnetoresistance (TMR) effect, allowing electrical detection of Néel vectors for next-generation memory devices. In this work, we investigate the non-relativistic spin transport properties of the quasi-two-dimensional (quasi-2D) d-wave altermagnet KV\\textsubscript{2}Se\\textsubscript{2}O and the TMR effect in KV\\textsubscript{2}Se\\textsubscript{2}O-based AFMTJs via first-principles calculations. Our results reveal that KV\\textsubscript{2}Se\\textsubscript{2}O exhibits a non-relativistic longitudinal spin polarization and a spin Hall angle both exceeding 60\\% at room temperature, while KV\\textsubscript{2}Se\\textsubscript{2}O-based AFMTJs achieve a giant TMR reaching approximately $10^{12}$\\%, which remains robust against Fermi level shifts. These findings highlight the anisotropic spin polarization inherent to d-wave staggered magnetism and underscore the critical role of Fermi surface topology in enhancing T-odd spin transport and the TMR effect in AFMTJs."
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.str-el",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T05:52:45Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    5,
                    52,
                    45,
                    1,
                    357,
                    0
                ],
                "arxiv_comment": "15 pages,4 figures,Supplementary Material included",
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci"
                },
                "authors": [
                    {
                        "name": "Qing Zhang"
                    },
                    {
                        "name": "Siyun Wang"
                    },
                    {
                        "name": "Jianting Dong"
                    },
                    {
                        "name": "Jia Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jia Zhang"
                },
                "author": "Jia Zhang"
            },
            {
                "id": "http://arxiv.org/abs/2512.19964v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.19964v1",
                "title": "VNF-Cache: An In-Network Key-Value Store Cache Based on Network Function Virtualization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VNF-Cache: An In-Network Key-Value Store Cache Based on Network Function Virtualization"
                },
                "updated": "2025-12-23T01:25:21Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    1,
                    25,
                    21,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.19964v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.19964v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "With the exponential growth of the amount of data available on the Internet, optimizing the response time and resource usage for data access becomes essential. Caches are an effective solution that brings data closer to clients, eliminating repetitive requests to servers. This paper presents VNF-Cache, a caching service for geographically remote key-value databases. VNF-Cache is an NFV-COIN (Network Function Virtualization-Computing In The Network) service, a technology undergoing standardization by the IETF that enables the implementation of arbitrary services directly in the network. VNF-Cache intercepts network packets, processes, stores, and sends values directly to clients when possible. Through a proof-of-concept implementation and experiments conducted with geographically dispersed servers in Brazil, the United States, and Japan, significant reductions in response time and increases in the number of requests processed per second were observed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the exponential growth of the amount of data available on the Internet, optimizing the response time and resource usage for data access becomes essential. Caches are an effective solution that brings data closer to clients, eliminating repetitive requests to servers. This paper presents VNF-Cache, a caching service for geographically remote key-value databases. VNF-Cache is an NFV-COIN (Network Function Virtualization-Computing In The Network) service, a technology undergoing standardization by the IETF that enables the implementation of arbitrary services directly in the network. VNF-Cache intercepts network packets, processes, stores, and sends values directly to clients when possible. Through a proof-of-concept implementation and experiments conducted with geographically dispersed servers in Brazil, the United States, and Japan, significant reductions in response time and increases in the number of requests processed per second were observed."
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T01:25:21Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    1,
                    25,
                    21,
                    1,
                    357,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI"
                },
                "authors": [
                    {
                        "name": "Bruno E. Farias"
                    },
                    {
                        "name": "José Flauzino"
                    },
                    {
                        "name": "Elias P. Duarte"
                    }
                ],
                "author_detail": {
                    "name": "Elias P. Duarte"
                },
                "author": "Elias P. Duarte"
            },
            {
                "id": "http://arxiv.org/abs/2512.20687v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20687v1",
                "title": "PHOTON: Hierarchical Autoregressive Modeling for Lightspeed and Memory-Efficient Language Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PHOTON: Hierarchical Autoregressive Modeling for Lightspeed and Memory-Efficient Language Generation"
                },
                "updated": "2025-12-22T19:26:59Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    19,
                    26,
                    59,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20687v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20687v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Transformers operate as horizontal token-by-token scanners; at each generation step, the model attends to an ever-growing sequence of token-level states. This access pattern increases prefill latency and makes long-context decoding increasingly memory-bound, as KV-cache reads and writes dominate inference throughput rather than arithmetic computation. We propose Parallel Hierarchical Operation for Top-down Networks (PHOTON), a hierarchical autoregressive model that replaces flat scanning with vertical, multi-resolution context access. PHOTON maintains a hierarchy of latent streams: a bottom-up encoder progressively compresses tokens into low-rate contextual states, while lightweight top-down decoders reconstruct fine-grained token representations. Experimental results show that PHOTON is superior to competitive Transformer-based language models regarding the throughput-quality trade-off, offering significant advantages in long-context and multi-query tasks. This reduces decode-time KV-cache traffic, yielding up to $10^{3}\\times$ higher throughput per unit memory.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers operate as horizontal token-by-token scanners; at each generation step, the model attends to an ever-growing sequence of token-level states. This access pattern increases prefill latency and makes long-context decoding increasingly memory-bound, as KV-cache reads and writes dominate inference throughput rather than arithmetic computation. We propose Parallel Hierarchical Operation for Top-down Networks (PHOTON), a hierarchical autoregressive model that replaces flat scanning with vertical, multi-resolution context access. PHOTON maintains a hierarchy of latent streams: a bottom-up encoder progressively compresses tokens into low-rate contextual states, while lightweight top-down decoders reconstruct fine-grained token representations. Experimental results show that PHOTON is superior to competitive Transformer-based language models regarding the throughput-quality trade-off, offering significant advantages in long-context and multi-query tasks. This reduces decode-time KV-cache traffic, yielding up to $10^{3}\\times$ higher throughput per unit memory."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-22T19:26:59Z",
                "published_parsed": [
                    2025,
                    12,
                    22,
                    19,
                    26,
                    59,
                    0,
                    356,
                    0
                ],
                "arxiv_comment": "12 pages, 5 figures",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Yuma Ichikawa"
                    },
                    {
                        "name": "Naoya Takagi"
                    },
                    {
                        "name": "Takumi Nakagawa"
                    },
                    {
                        "name": "Yuzi Kanazawa"
                    },
                    {
                        "name": "Akira Sakai"
                    }
                ],
                "author_detail": {
                    "name": "Akira Sakai"
                },
                "author": "Akira Sakai"
            },
            {
                "id": "http://arxiv.org/abs/2512.19678v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.19678v1",
                "title": "WorldWarp: Propagating 3D Geometry with Asynchronous Video Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WorldWarp: Propagating 3D Geometry with Asynchronous Video Diffusion"
                },
                "updated": "2025-12-22T18:53:50Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    18,
                    53,
                    50,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.19678v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.19678v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Generating long-range, geometrically consistent video presents a fundamental dilemma: while consistency demands strict adherence to 3D geometry in pixel space, state-of-the-art generative models operate most effectively in a camera-conditioned latent space. This disconnect causes current methods to struggle with occluded areas and complex camera trajectories. To bridge this gap, we propose WorldWarp, a framework that couples a 3D structural anchor with a 2D generative refiner. To establish geometric grounding, WorldWarp maintains an online 3D geometric cache built via Gaussian Splatting (3DGS). By explicitly warping historical content into novel views, this cache acts as a structural scaffold, ensuring each new frame respects prior geometry. However, static warping inevitably leaves holes and artifacts due to occlusions. We address this using a Spatio-Temporal Diffusion (ST-Diff) model designed for a \"fill-and-revise\" objective. Our key innovation is a spatio-temporal varying noise schedule: blank regions receive full noise to trigger generation, while warped regions receive partial noise to enable refinement. By dynamically updating the 3D cache at every step, WorldWarp maintains consistency across video chunks. Consequently, it achieves state-of-the-art fidelity by ensuring that 3D logic guides structure while diffusion logic perfects texture. Project page: \\href{https://hyokong.github.io/worldwarp-page/}{https://hyokong.github.io/worldwarp-page/}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating long-range, geometrically consistent video presents a fundamental dilemma: while consistency demands strict adherence to 3D geometry in pixel space, state-of-the-art generative models operate most effectively in a camera-conditioned latent space. This disconnect causes current methods to struggle with occluded areas and complex camera trajectories. To bridge this gap, we propose WorldWarp, a framework that couples a 3D structural anchor with a 2D generative refiner. To establish geometric grounding, WorldWarp maintains an online 3D geometric cache built via Gaussian Splatting (3DGS). By explicitly warping historical content into novel views, this cache acts as a structural scaffold, ensuring each new frame respects prior geometry. However, static warping inevitably leaves holes and artifacts due to occlusions. We address this using a Spatio-Temporal Diffusion (ST-Diff) model designed for a \"fill-and-revise\" objective. Our key innovation is a spatio-temporal varying noise schedule: blank regions receive full noise to trigger generation, while warped regions receive partial noise to enable refinement. By dynamically updating the 3D cache at every step, WorldWarp maintains consistency across video chunks. Consequently, it achieves state-of-the-art fidelity by ensuring that 3D logic guides structure while diffusion logic perfects texture. Project page: \\href{https://hyokong.github.io/worldwarp-page/}{https://hyokong.github.io/worldwarp-page/}."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-22T18:53:50Z",
                "published_parsed": [
                    2025,
                    12,
                    22,
                    18,
                    53,
                    50,
                    0,
                    356,
                    0
                ],
                "arxiv_comment": "Project page: https://hyokong.github.io/worldwarp-page/",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Hanyang Kong"
                    },
                    {
                        "name": "Xingyi Yang"
                    },
                    {
                        "name": "Xiaoxu Zheng"
                    },
                    {
                        "name": "Xinchao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xinchao Wang"
                },
                "author": "Xinchao Wang"
            },
            {
                "id": "http://arxiv.org/abs/2510.06415v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.06415v2",
                "title": "Neutron-Assisted Breakdown Enhancement in $β$-Ga$_2$O$_3$ Schottky Diodes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neutron-Assisted Breakdown Enhancement in $β$-Ga$_2$O$_3$ Schottky Diodes"
                },
                "updated": "2025-12-22T16:32:07Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    16,
                    32,
                    7,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.06415v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.06415v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "This study demonstrates a substantial enhancement of breakdown voltage in $β$-Ga$_2$O$_3$ Schottky diodes through an approach that combines fast neutron irradiation with controlled post-irradiation electro-thermal annealing. Devices irradiated with 1 MeV neutrons at a high fluence of 1E15 n/cm^2 exhibited substantial degradation, including a drastic reduction in on-current and an increase in on-resistance. Electrothermal testing, conducted through simultaneous current-voltage (J-V) measurements and thermal annealing, resulted in significant recovery. After four cycles of electro-thermal testing, the devices demonstrated significant improvements in performance, with a substantial recovery of on-current and a reduction in on-resistance compared to the post-radiation condition, approaching pre-radiation levels. Most recovery occurred during the first two cycles, with diminishing improvements in later cycles, indicating that most thermally recoverable traps were mitigated early. Capacitance-voltage (C-V) measurements revealed a substantial reduction in carrier concentration, decreasing from 3.2E16 cm^-3 pre-radiation to 5.5E15 cm^-3 after the first electro-thermal testing cycle, indicating an over 82% reduction. Following the third cycle, the carrier concentration partially recovered to 9.9E15 cm^-3, reflecting a carrier removal rate of ~22 cm^-1. The breakdown voltage exhibited a remarkable enhancement, increasing from approximately 300 V to 1.28 kV (a ~325% improvement) after the first electro-thermal testing, attributed to the reduction in carrier concentration by compensating radiation-induced traps. Subsequent testing reduced breakdown voltage slightly to 940 V due to partial recovery of carrier concentration, but it remained significantly higher than pre-radiation levels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study demonstrates a substantial enhancement of breakdown voltage in $β$-Ga$_2$O$_3$ Schottky diodes through an approach that combines fast neutron irradiation with controlled post-irradiation electro-thermal annealing. Devices irradiated with 1 MeV neutrons at a high fluence of 1E15 n/cm^2 exhibited substantial degradation, including a drastic reduction in on-current and an increase in on-resistance. Electrothermal testing, conducted through simultaneous current-voltage (J-V) measurements and thermal annealing, resulted in significant recovery. After four cycles of electro-thermal testing, the devices demonstrated significant improvements in performance, with a substantial recovery of on-current and a reduction in on-resistance compared to the post-radiation condition, approaching pre-radiation levels. Most recovery occurred during the first two cycles, with diminishing improvements in later cycles, indicating that most thermally recoverable traps were mitigated early. Capacitance-voltage (C-V) measurements revealed a substantial reduction in carrier concentration, decreasing from 3.2E16 cm^-3 pre-radiation to 5.5E15 cm^-3 after the first electro-thermal testing cycle, indicating an over 82% reduction. Following the third cycle, the carrier concentration partially recovered to 9.9E15 cm^-3, reflecting a carrier removal rate of ~22 cm^-1. The breakdown voltage exhibited a remarkable enhancement, increasing from approximately 300 V to 1.28 kV (a ~325% improvement) after the first electro-thermal testing, attributed to the reduction in carrier concentration by compensating radiation-induced traps. Subsequent testing reduced breakdown voltage slightly to 940 V due to partial recovery of carrier concentration, but it remained significantly higher than pre-radiation levels."
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-07T19:50:52Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    19,
                    50,
                    52,
                    1,
                    280,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph"
                },
                "authors": [
                    {
                        "name": "Saleh Ahmed Khan"
                    },
                    {
                        "name": "Sudipto Saha"
                    },
                    {
                        "name": "Ahmed Ibreljic"
                    },
                    {
                        "name": "Stephen Margiotta"
                    },
                    {
                        "name": "Jiawei Liu"
                    },
                    {
                        "name": "Walid Amir"
                    },
                    {
                        "name": "Surajit Chakraborty"
                    },
                    {
                        "name": "Uttam Singisetti"
                    },
                    {
                        "name": "A F M Anhar Uddin Bhuiyan"
                    }
                ],
                "author_detail": {
                    "name": "A F M Anhar Uddin Bhuiyan"
                },
                "author": "A F M Anhar Uddin Bhuiyan"
            },
            {
                "id": "http://arxiv.org/abs/2512.19437v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.19437v1",
                "title": "Ultra-high precision high voltage system for PTOLEMY",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ultra-high precision high voltage system for PTOLEMY"
                },
                "updated": "2025-12-22T14:34:51Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    14,
                    34,
                    51,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.19437v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.19437v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The PTOLEMY project is prototyping a novel electromagnetic filter for high-precision $β$ spectroscopy, with the ultimate and ambitious long-term goal of detecting the cosmic neutrino background through electron capture on tritium bound to graphene. Intermediate small-scale prototypes can achieve competitive sensitivity to the effective neutrino mass, even with reduced energy resolution. To reach an energy resolution better than \\SI{500}{meV} at the tritium $β$-spectrum endpoint of \\SI{18.6}{keV}, and accounting for all uncertainties in the filtering chain, the electrode voltage must be controlled at the level of a few parts per million and monitored in real time. In this work, we present the first results obtained in this effort, using a chain of commercial ultra-high-precision voltage references, read out by precision multimeters and a \\emph{field mill} device. The currently available precision on high voltage is, in the conservative case, as low as \\SI{0.2}{ppm} per \\SI{1}{kV} single board and $\\lesssim$ \\SI{50}{mV} over the \\SI{10}{kV} series, presently limited by field mill read-out noise. However, assuming uncorrelated Gaussian noise extrapolation, the real precision could in principle be as low as \\SI{0.05}{ppm} over \\SI{20}{kV}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The PTOLEMY project is prototyping a novel electromagnetic filter for high-precision $β$ spectroscopy, with the ultimate and ambitious long-term goal of detecting the cosmic neutrino background through electron capture on tritium bound to graphene. Intermediate small-scale prototypes can achieve competitive sensitivity to the effective neutrino mass, even with reduced energy resolution. To reach an energy resolution better than \\SI{500}{meV} at the tritium $β$-spectrum endpoint of \\SI{18.6}{keV}, and accounting for all uncertainties in the filtering chain, the electrode voltage must be controlled at the level of a few parts per million and monitored in real time. In this work, we present the first results obtained in this effort, using a chain of commercial ultra-high-precision voltage references, read out by precision multimeters and a \\emph{field mill} device. The currently available precision on high voltage is, in the conservative case, as low as \\SI{0.2}{ppm} per \\SI{1}{kV} single board and $\\lesssim$ \\SI{50}{mV} over the \\SI{10}{kV} series, presently limited by field mill read-out noise. However, assuming uncorrelated Gaussian noise extrapolation, the real precision could in principle be as low as \\SI{0.05}{ppm} over \\SI{20}{kV}."
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-22T14:34:51Z",
                "published_parsed": [
                    2025,
                    12,
                    22,
                    14,
                    34,
                    51,
                    0,
                    356,
                    0
                ],
                "arxiv_comment": "17 pages, 13 figures",
                "arxiv_primary_category": {
                    "term": "physics.ins-det"
                },
                "authors": [
                    {
                        "name": "R. Ammendola"
                    },
                    {
                        "name": "A. Apponi"
                    },
                    {
                        "name": "G. Benato"
                    },
                    {
                        "name": "M. G. Betti"
                    },
                    {
                        "name": "R. Biondim"
                    },
                    {
                        "name": "P. Bos"
                    },
                    {
                        "name": "G. Cavoto"
                    },
                    {
                        "name": "M. Cadeddu"
                    },
                    {
                        "name": "A. Casale"
                    },
                    {
                        "name": "O. Castellano"
                    },
                    {
                        "name": "E. Celasco"
                    },
                    {
                        "name": "L. Cecchini"
                    },
                    {
                        "name": "M. Chirico"
                    },
                    {
                        "name": "W. Chung"
                    },
                    {
                        "name": "A. G. Cocco"
                    },
                    {
                        "name": "A. P. Colijn"
                    },
                    {
                        "name": "B. Corcione"
                    },
                    {
                        "name": "N. D'Ambrosio"
                    },
                    {
                        "name": "M. D'Incecco"
                    },
                    {
                        "name": "G. De Bellis"
                    },
                    {
                        "name": "M. De Deo"
                    },
                    {
                        "name": "N. de Groot"
                    },
                    {
                        "name": "A. Esposito"
                    },
                    {
                        "name": "M. Farino"
                    },
                    {
                        "name": "S. Farinon"
                    },
                    {
                        "name": "A. D. Ferella"
                    },
                    {
                        "name": "L. Ferro"
                    },
                    {
                        "name": "L. Ficcadenti"
                    },
                    {
                        "name": "G. Galbato Muscio"
                    },
                    {
                        "name": "S. Gariazzo"
                    },
                    {
                        "name": "H. Garrone"
                    },
                    {
                        "name": "F. Gatti"
                    },
                    {
                        "name": "G. Korga"
                    },
                    {
                        "name": "F. Malnati"
                    },
                    {
                        "name": "G. Mangano"
                    },
                    {
                        "name": "L. E. Marcucci"
                    },
                    {
                        "name": "C. Mariani"
                    },
                    {
                        "name": "J. Mead"
                    },
                    {
                        "name": "G. Menichetti"
                    },
                    {
                        "name": "M. Messina"
                    },
                    {
                        "name": "E. Monticone"
                    },
                    {
                        "name": "M. Naafs"
                    },
                    {
                        "name": "V. Narcisi"
                    },
                    {
                        "name": "S. Nagorny"
                    },
                    {
                        "name": "G. Neri"
                    },
                    {
                        "name": "F. Pandolfi"
                    },
                    {
                        "name": "R. Pavarani"
                    },
                    {
                        "name": "C. Pèrez de los Heros"
                    },
                    {
                        "name": "O. Pisanti"
                    },
                    {
                        "name": "C. Pepe"
                    },
                    {
                        "name": "F. M. Pofi"
                    },
                    {
                        "name": "A. D. Polosa"
                    },
                    {
                        "name": "I. Rago"
                    },
                    {
                        "name": "M. Rajteri N. Rossi"
                    },
                    {
                        "name": "S. Ritarossi"
                    },
                    {
                        "name": "A. Ruocco"
                    },
                    {
                        "name": "G. Salina"
                    },
                    {
                        "name": "A. Santucci"
                    },
                    {
                        "name": "M. Sestu"
                    },
                    {
                        "name": "A. Tan"
                    },
                    {
                        "name": "V. Tozzini"
                    },
                    {
                        "name": "C. G. Tully"
                    },
                    {
                        "name": "I. van Rens"
                    },
                    {
                        "name": "F. Virzi"
                    },
                    {
                        "name": "G. Visser"
                    },
                    {
                        "name": "M. Vivian"
                    }
                ],
                "author_detail": {
                    "name": "M. Vivian"
                },
                "author": "M. Vivian"
            },
            {
                "id": "http://arxiv.org/abs/2512.19434v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.19434v1",
                "title": "Hybrid Analytical-Machine Learning Framework for Ripple Factor Estimation in Cockcroft-Walton Voltage Multipliers with Residual Correction for Non-Ideal Effects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid Analytical-Machine Learning Framework for Ripple Factor Estimation in Cockcroft-Walton Voltage Multipliers with Residual Correction for Non-Ideal Effects"
                },
                "updated": "2025-12-22T14:32:19Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    14,
                    32,
                    19,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.19434v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.19434v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Cockcroft-Walton (CW) voltage multipliers suffer from output ripple that classical analytical models underestimate due to neglected non-idealities like diode drops and capacitor ESR, particularly in high-stage, low-frequency and heavy-load regimes. This paper proposes a hybrid framework that generates a comprehensive 324-case MATLAB/Simulink dataset varying stages (2-8), input voltage (5-25 kV), capacitance (1-10 μF), frequency (50-500 Hz) and load (6-60 MΩ), then trains a Random Forest model to predict residuals between simulated and theoretical peak-to-peak ripple. The approach achieves 70.6% RMSE reduction (131 V vs. 448 V) globally and 66.7% in critical regimes, with near-zero bias, enabling physically interpretable design optimization while outperforming pure ML in extrapolation reliability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cockcroft-Walton (CW) voltage multipliers suffer from output ripple that classical analytical models underestimate due to neglected non-idealities like diode drops and capacitor ESR, particularly in high-stage, low-frequency and heavy-load regimes. This paper proposes a hybrid framework that generates a comprehensive 324-case MATLAB/Simulink dataset varying stages (2-8), input voltage (5-25 kV), capacitance (1-10 μF), frequency (50-500 Hz) and load (6-60 MΩ), then trains a Random Forest model to predict residuals between simulated and theoretical peak-to-peak ripple. The approach achieves 70.6% RMSE reduction (131 V vs. 448 V) globally and 66.7% in critical regimes, with near-zero bias, enabling physically interpretable design optimization while outperforming pure ML in extrapolation reliability."
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-22T14:32:19Z",
                "published_parsed": [
                    2025,
                    12,
                    22,
                    14,
                    32,
                    19,
                    0,
                    356,
                    0
                ],
                "arxiv_comment": "6 Pages, 2 figures, IEEE Conference Template used",
                "arxiv_primary_category": {
                    "term": "eess.SY"
                },
                "authors": [
                    {
                        "name": "Md. Tanvirul Islam"
                    }
                ],
                "author_detail": {
                    "name": "Md. Tanvirul Islam"
                },
                "author": "Md. Tanvirul Islam"
            },
            {
                "id": "http://arxiv.org/abs/2512.17452v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.17452v2",
                "title": "Learning What to Write: Write-Gated KV for Efficient Long-Context Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning What to Write: Write-Gated KV for Efficient Long-Context Inference"
                },
                "updated": "2025-12-22T10:23:36Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    10,
                    23,
                    36,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.17452v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.17452v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Long-context LLM inference is bottlenecked by the quadratic attention complexity and linear KV cache growth. Prior approaches mitigate this via post-hoc selection or eviction but overlook the root inefficiency: indiscriminate writing to persistent memory. In this paper, we formalize KV cache management as a causal system of three primitives: KV Admission, Selection, and Eviction. We instantiate KV Admission via Write-Gated KV, a lightweight mechanism that learns to predict token utility before it enters the cache. By filtering out low-utility states early to maintain a compact global cache alongside a sliding local cache, Write-Gated KV reduces memory usage by 46-57% and delivers 3.03-3.45$\\times$ prefill and 1.89-2.56$\\times$ decode speedups on Llama model with negligible accuracy loss, all while remaining compatible with FlashAttention and paged-KV systems. These results demonstrate that learning what to write, is a principled and practical recipe for efficient long-context inference. Code is available at https://github.com/EMCLab-Sinica/WG-KV .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context LLM inference is bottlenecked by the quadratic attention complexity and linear KV cache growth. Prior approaches mitigate this via post-hoc selection or eviction but overlook the root inefficiency: indiscriminate writing to persistent memory. In this paper, we formalize KV cache management as a causal system of three primitives: KV Admission, Selection, and Eviction. We instantiate KV Admission via Write-Gated KV, a lightweight mechanism that learns to predict token utility before it enters the cache. By filtering out low-utility states early to maintain a compact global cache alongside a sliding local cache, Write-Gated KV reduces memory usage by 46-57% and delivers 3.03-3.45$\\times$ prefill and 1.89-2.56$\\times$ decode speedups on Llama model with negligible accuracy loss, all while remaining compatible with FlashAttention and paged-KV systems. These results demonstrate that learning what to write, is a principled and practical recipe for efficient long-context inference. Code is available at https://github.com/EMCLab-Sinica/WG-KV ."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-19T11:08:58Z",
                "published_parsed": [
                    2025,
                    12,
                    19,
                    11,
                    8,
                    58,
                    4,
                    353,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Yen-Chieh Huang"
                    },
                    {
                        "name": "Pi-Cheng Hsiu"
                    },
                    {
                        "name": "Rui Fang"
                    },
                    {
                        "name": "Ming-Syan Chen"
                    }
                ],
                "author_detail": {
                    "name": "Ming-Syan Chen"
                },
                "author": "Ming-Syan Chen"
            },
            {
                "id": "http://arxiv.org/abs/2512.19206v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.19206v1",
                "title": "MixKVQ: Query-Aware Mixed-Precision KV Cache Quantization for Long-Context Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MixKVQ: Query-Aware Mixed-Precision KV Cache Quantization for Long-Context Reasoning"
                },
                "updated": "2025-12-22T09:44:26Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    9,
                    44,
                    26,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.19206v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.19206v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Long Chain-of-Thought (CoT) reasoning has significantly advanced the capabilities of Large Language Models (LLMs), but this progress is accompanied by substantial memory and latency overhead from the extensive Key-Value (KV) cache. Although KV cache quantization is a promising compression technique, existing low-bit quantization methods often exhibit severe performance degradation on complex reasoning tasks. Fixed-precision quantization struggles to handle outlier channels in the key cache, while current mixed-precision strategies fail to accurately identify components requiring high-precision representation. We find that an effective low-bit KV cache quantization strategy must consider two factors: a key channel's intrinsic quantization difficulty and its relevance to the query. Based on this insight, we propose MixKVQ, a novel plug-and-play method that introduces a lightweight, query-aware algorithm to identify and preserve critical key channels that need higher precision, while applying per-token quantization for value cache. Experiments on complex reasoning datasets demonstrate that our approach significantly outperforms existing low-bit methods, achieving performance comparable to a full-precision baseline at a substantially reduced memory footprint.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long Chain-of-Thought (CoT) reasoning has significantly advanced the capabilities of Large Language Models (LLMs), but this progress is accompanied by substantial memory and latency overhead from the extensive Key-Value (KV) cache. Although KV cache quantization is a promising compression technique, existing low-bit quantization methods often exhibit severe performance degradation on complex reasoning tasks. Fixed-precision quantization struggles to handle outlier channels in the key cache, while current mixed-precision strategies fail to accurately identify components requiring high-precision representation. We find that an effective low-bit KV cache quantization strategy must consider two factors: a key channel's intrinsic quantization difficulty and its relevance to the query. Based on this insight, we propose MixKVQ, a novel plug-and-play method that introduces a lightweight, query-aware algorithm to identify and preserve critical key channels that need higher precision, while applying per-token quantization for value cache. Experiments on complex reasoning datasets demonstrate that our approach significantly outperforms existing low-bit methods, achieving performance comparable to a full-precision baseline at a substantially reduced memory footprint."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-22T09:44:26Z",
                "published_parsed": [
                    2025,
                    12,
                    22,
                    9,
                    44,
                    26,
                    0,
                    356,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Tao Zhang"
                    },
                    {
                        "name": "Ziqian Zeng"
                    },
                    {
                        "name": "Hao Peng"
                    },
                    {
                        "name": "Huiping Zhuang"
                    },
                    {
                        "name": "Cen Chen"
                    }
                ],
                "author_detail": {
                    "name": "Cen Chen"
                },
                "author": "Cen Chen"
            },
            {
                "id": "http://arxiv.org/abs/2510.11496v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.11496v3",
                "title": "AndesVL Technical Report: An Efficient Mobile-side Multimodal Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AndesVL Technical Report: An Efficient Mobile-side Multimodal Large Language Model"
                },
                "updated": "2025-12-22T04:40:04Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    4,
                    40,
                    4,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.11496v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.11496v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "In recent years, while cloud-based MLLMs such as QwenVL, InternVL, GPT-4o, Gemini, and Claude Sonnet have demonstrated outstanding performance with enormous model sizes reaching hundreds of billions of parameters, they significantly surpass the limitations in memory, power consumption, and computing capacity of edge devices such as mobile phones. This paper introduces AndesVL, a suite of mobile-side MLLMs with 0.6B to 4B parameters based on Qwen3's LLM and various visual encoders. We comprehensively outline the model architectures, training pipeline, and training data of AndesVL, which achieves first-tier performance across a wide range of open-source benchmarks, including fields such as text-rich image understanding, reasoning and math, multi-image comprehension, general VQA, hallucination mitigation, multilingual understanding, and GUI-related tasks when compared with state-of-the-art models of a similar scale. Furthermore, we introduce a 1+N LoRA architecture alongside a Quantization-Aware LoRA Fine-Tuning (QALFT) framework to facilitate efficient task adaptation and model compression during mobile-side deployment of AndesVL. Moreover, utilizing our cache eviction algorithm -- OKV -- along with customized speculative decoding and compression strategies, we achieve a 6.7x peak decoding speedup ratio, up to 30.9% memory reduction, and 1.8 bits-per-weight when deploying AndesVL-4B on MediaTek Dimensity 9500 chips. We release all models on https://huggingface.co/OPPOer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, while cloud-based MLLMs such as QwenVL, InternVL, GPT-4o, Gemini, and Claude Sonnet have demonstrated outstanding performance with enormous model sizes reaching hundreds of billions of parameters, they significantly surpass the limitations in memory, power consumption, and computing capacity of edge devices such as mobile phones. This paper introduces AndesVL, a suite of mobile-side MLLMs with 0.6B to 4B parameters based on Qwen3's LLM and various visual encoders. We comprehensively outline the model architectures, training pipeline, and training data of AndesVL, which achieves first-tier performance across a wide range of open-source benchmarks, including fields such as text-rich image understanding, reasoning and math, multi-image comprehension, general VQA, hallucination mitigation, multilingual understanding, and GUI-related tasks when compared with state-of-the-art models of a similar scale. Furthermore, we introduce a 1+N LoRA architecture alongside a Quantization-Aware LoRA Fine-Tuning (QALFT) framework to facilitate efficient task adaptation and model compression during mobile-side deployment of AndesVL. Moreover, utilizing our cache eviction algorithm -- OKV -- along with customized speculative decoding and compression strategies, we achieve a 6.7x peak decoding speedup ratio, up to 30.9% memory reduction, and 1.8 bits-per-weight when deploying AndesVL-4B on MediaTek Dimensity 9500 chips. We release all models on https://huggingface.co/OPPOer."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-13T15:04:38Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    15,
                    4,
                    38,
                    0,
                    286,
                    0
                ],
                "arxiv_comment": "Tech report of OPPO AndesVL Team",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Zhiwei Jin"
                    },
                    {
                        "name": "Xiaohui Song"
                    },
                    {
                        "name": "Nan Wang"
                    },
                    {
                        "name": "Yafei Liu"
                    },
                    {
                        "name": "Chao Li"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Ruichen Wang"
                    },
                    {
                        "name": "Zhihao Li"
                    },
                    {
                        "name": "Qi Qi"
                    },
                    {
                        "name": "Long Cheng"
                    },
                    {
                        "name": "Dongze Hao"
                    },
                    {
                        "name": "Quanlong Zheng"
                    },
                    {
                        "name": "Yanhao Zhang"
                    },
                    {
                        "name": "Haobo Ji"
                    },
                    {
                        "name": "Jian Ma"
                    },
                    {
                        "name": "Zhitong Zheng"
                    },
                    {
                        "name": "Zhenyi Lin"
                    },
                    {
                        "name": "Haolin Deng"
                    },
                    {
                        "name": "Xin Zou"
                    },
                    {
                        "name": "Xiaojie Yin"
                    },
                    {
                        "name": "Ruilin Wang"
                    },
                    {
                        "name": "Liankai Cai"
                    },
                    {
                        "name": "Haijing Liu"
                    },
                    {
                        "name": "Yuqing Qiu"
                    },
                    {
                        "name": "Ke Chen"
                    },
                    {
                        "name": "Zixian Li"
                    },
                    {
                        "name": "Chi Xie"
                    },
                    {
                        "name": "Huafei Li"
                    },
                    {
                        "name": "Chenxing Li"
                    },
                    {
                        "name": "Chuangchuang Wang"
                    },
                    {
                        "name": "Kai Tang"
                    },
                    {
                        "name": "Zhiguang Zhu"
                    },
                    {
                        "name": "Kai Tang"
                    },
                    {
                        "name": "Wenmei Gao"
                    },
                    {
                        "name": "Rui Wang"
                    },
                    {
                        "name": "Jun Wu"
                    },
                    {
                        "name": "Chao Liu"
                    },
                    {
                        "name": "Qin Xie"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Haonan Lu"
                    }
                ],
                "author_detail": {
                    "name": "Haonan Lu"
                },
                "author": "Haonan Lu"
            },
            {
                "id": "http://arxiv.org/abs/2511.01385v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.01385v2",
                "title": "Memory-Efficient Training with In-Place FFT Implementation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory-Efficient Training with In-Place FFT Implementation"
                },
                "updated": "2025-12-22T02:25:29Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    2,
                    25,
                    29,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.01385v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.01385v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Fast Fourier Transforms (FFT) are widely used to reduce memory and computational costs in deep learning. However, existing implementations, including standard FFT and real FFT (rFFT), cannot achieve true in-place computation. In particular, rFFT maps an input of size n to a complex output of size n/2+1, causing dimensional mismatch and requiring additional memory allocation. We propose the first real-domain, fully in-place FFT framework (rdFFT) that preserves input-output memory space consistency. By leveraging butterfly operation symmetry and conjugate properties in the frequency domain, we design an implicit complex encoding scheme that eliminates intermediate cache usage entirely. Experiments on multiple natural language understanding tasks demonstrate the method effectiveness in reducing training memory cost, offering a promising direction for frequency-domain lightweight adaptation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast Fourier Transforms (FFT) are widely used to reduce memory and computational costs in deep learning. However, existing implementations, including standard FFT and real FFT (rFFT), cannot achieve true in-place computation. In particular, rFFT maps an input of size n to a complex output of size n/2+1, causing dimensional mismatch and requiring additional memory allocation. We propose the first real-domain, fully in-place FFT framework (rdFFT) that preserves input-output memory space consistency. By leveraging butterfly operation symmetry and conjugate properties in the frequency domain, we design an implicit complex encoding scheme that eliminates intermediate cache usage entirely. Experiments on multiple natural language understanding tasks demonstrate the method effectiveness in reducing training memory cost, offering a promising direction for frequency-domain lightweight adaptation."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-03T09:36:11Z",
                "published_parsed": [
                    2025,
                    11,
                    3,
                    9,
                    36,
                    11,
                    0,
                    307,
                    0
                ],
                "arxiv_comment": "Accepted at NeurIPS 2025. Version 2 adds links to the ongoing PyTorch upstreaming discussion",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Xinyu Ding"
                    },
                    {
                        "name": "Bangtian Liu"
                    },
                    {
                        "name": "Siyu Liao"
                    },
                    {
                        "name": "Zhongfeng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhongfeng Wang"
                },
                "author": "Zhongfeng Wang"
            },
            {
                "id": "http://arxiv.org/abs/2512.18674v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.18674v1",
                "title": "Remoe: Towards Efficient and Low-Cost MoE Inference in Serverless Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Remoe: Towards Efficient and Low-Cost MoE Inference in Serverless Computing"
                },
                "updated": "2025-12-21T10:27:50Z",
                "updated_parsed": [
                    2025,
                    12,
                    21,
                    10,
                    27,
                    50,
                    6,
                    355,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.18674v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.18674v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Mixture-of-Experts (MoE) has become a dominant architecture in large language models (LLMs) due to its ability to scale model capacity via sparse expert activation. Meanwhile, serverless computing, with its elasticity and pay-per-use billing, is well-suited for deploying MoEs with bursty workloads. However, the large number of experts in MoE models incurs high inference costs due to memory-intensive parameter caching. These costs are difficult to mitigate via simple model partitioning due to input-dependent expert activation. To address these issues, we propose Remoe, a heterogeneous MoE inference system tailored for serverless computing. Remoe assigns non-expert modules to GPUs and expert modules to CPUs, and further offloads infrequently activated experts to separate serverless functions to reduce memory overhead and enable parallel execution. We incorporate three key techniques: (1) a Similar Prompts Searching (SPS) algorithm to predict expert activation patterns based on semantic similarity of inputs; (2) a Main Model Pre-allocation (MMP) algorithm to ensure service-level objectives (SLOs) via worst-case memory estimation; and (3) a joint memory and replica optimization framework leveraging Lagrangian duality and the Longest Processing Time (LPT) algorithm. We implement Remoe on Kubernetes and evaluate it across multiple LLM benchmarks. Experimental results show that Remoe reduces inference cost by up to 57% and cold start latency by 47% compared to state-of-the-art baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) has become a dominant architecture in large language models (LLMs) due to its ability to scale model capacity via sparse expert activation. Meanwhile, serverless computing, with its elasticity and pay-per-use billing, is well-suited for deploying MoEs with bursty workloads. However, the large number of experts in MoE models incurs high inference costs due to memory-intensive parameter caching. These costs are difficult to mitigate via simple model partitioning due to input-dependent expert activation. To address these issues, we propose Remoe, a heterogeneous MoE inference system tailored for serverless computing. Remoe assigns non-expert modules to GPUs and expert modules to CPUs, and further offloads infrequently activated experts to separate serverless functions to reduce memory overhead and enable parallel execution. We incorporate three key techniques: (1) a Similar Prompts Searching (SPS) algorithm to predict expert activation patterns based on semantic similarity of inputs; (2) a Main Model Pre-allocation (MMP) algorithm to ensure service-level objectives (SLOs) via worst-case memory estimation; and (3) a joint memory and replica optimization framework leveraging Lagrangian duality and the Longest Processing Time (LPT) algorithm. We implement Remoe on Kubernetes and evaluate it across multiple LLM benchmarks. Experimental results show that Remoe reduces inference cost by up to 57% and cold start latency by 47% compared to state-of-the-art baselines."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-21T10:27:50Z",
                "published_parsed": [
                    2025,
                    12,
                    21,
                    10,
                    27,
                    50,
                    6,
                    355,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Wentao Liu"
                    },
                    {
                        "name": "Yuhao Hu"
                    },
                    {
                        "name": "Ruiting Zhou"
                    },
                    {
                        "name": "Baochun Li"
                    },
                    {
                        "name": "Ne Wang"
                    }
                ],
                "author_detail": {
                    "name": "Ne Wang"
                },
                "author": "Ne Wang"
            },
            {
                "id": "http://arxiv.org/abs/2512.22195v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.22195v1",
                "title": "MatKV: Trading Compute for Flash Storage in LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MatKV: Trading Compute for Flash Storage in LLM Inference"
                },
                "updated": "2025-12-20T14:17:00Z",
                "updated_parsed": [
                    2025,
                    12,
                    20,
                    14,
                    17,
                    0,
                    5,
                    354,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.22195v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.22195v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We observe two major trends in LLM-based generative AI: (1) inference is becoming the dominant factor in terms of cost and power consumption, surpassing training, and (2) retrieval augmented generation (RAG) is becoming prevalent. When processing long inputs in RAG, the prefill phase of computing the key-value vectors of input text is energy-intensive and time-consuming even with high-end GPUs. Thus, it is crucial to make the prefill phase in RAG inference efficient. To address this issue, we propose MatKV, a scheme that precomputes the key-value vectors (KVs) of RAG objects (e.g., documents), materializes them in inexpensive but fast and power-efficient flash storage, and reuses them at inference time instead of recomputing the KVs using costly and power-inefficient GPU. Experimental results using Hugging Face's Transformers library across state-of-the-art GPUs and flash memory SSDs confirm that, compared to full KV computation on GPUs, MatKV reduces both inference time and power consumption by half for RAG workloads, without severely impacting accuracy in the question-answering task. Furthermore, we demonstrate that MatKV enables additional optimizations in two ways. First, a GPU can decode text while simultaneously loading the materialized KVs for the next instance, reducing load latency. Second, since decoding speed is less sensitive to GPU performance than KV computation, low-end GPUs can be leveraged for decoding without significantly compromising speed once the materialized KVs are loaded into GPU memory. These findings underscore MatKV's potential to make large-scale generative AI applications more cost-effective, power-efficient, and accessible across a wider range of tasks and hardware environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We observe two major trends in LLM-based generative AI: (1) inference is becoming the dominant factor in terms of cost and power consumption, surpassing training, and (2) retrieval augmented generation (RAG) is becoming prevalent. When processing long inputs in RAG, the prefill phase of computing the key-value vectors of input text is energy-intensive and time-consuming even with high-end GPUs. Thus, it is crucial to make the prefill phase in RAG inference efficient. To address this issue, we propose MatKV, a scheme that precomputes the key-value vectors (KVs) of RAG objects (e.g., documents), materializes them in inexpensive but fast and power-efficient flash storage, and reuses them at inference time instead of recomputing the KVs using costly and power-inefficient GPU. Experimental results using Hugging Face's Transformers library across state-of-the-art GPUs and flash memory SSDs confirm that, compared to full KV computation on GPUs, MatKV reduces both inference time and power consumption by half for RAG workloads, without severely impacting accuracy in the question-answering task. Furthermore, we demonstrate that MatKV enables additional optimizations in two ways. First, a GPU can decode text while simultaneously loading the materialized KVs for the next instance, reducing load latency. Second, since decoding speed is less sensitive to GPU performance than KV computation, low-end GPUs can be leveraged for decoding without significantly compromising speed once the materialized KVs are loaded into GPU memory. These findings underscore MatKV's potential to make large-scale generative AI applications more cost-effective, power-efficient, and accessible across a wider range of tasks and hardware environments."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-20T14:17:00Z",
                "published_parsed": [
                    2025,
                    12,
                    20,
                    14,
                    17,
                    0,
                    5,
                    354,
                    0
                ],
                "arxiv_comment": "Accepted for publication in ICDE 2026",
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Kun-Woo Shin"
                    },
                    {
                        "name": "Jay H. Park"
                    },
                    {
                        "name": "Moonwook Oh"
                    },
                    {
                        "name": "Yohan Jo"
                    },
                    {
                        "name": "Jaeyoung Do"
                    },
                    {
                        "name": "Sang-Won Lee"
                    }
                ],
                "author_detail": {
                    "name": "Sang-Won Lee"
                },
                "arxiv_affiliation": "Seoul National University, Korea",
                "author": "Sang-Won Lee"
            },
            {
                "id": "http://arxiv.org/abs/2512.01802v4",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.01802v4",
                "title": "JFR: An Efficient Jump Frontier Relaxation Strategy for Bellman-Ford",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JFR: An Efficient Jump Frontier Relaxation Strategy for Bellman-Ford"
                },
                "updated": "2025-12-20T14:13:44Z",
                "updated_parsed": [
                    2025,
                    12,
                    20,
                    14,
                    13,
                    44,
                    5,
                    354,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.01802v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.01802v4",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We propose JFR, a Bellman-Ford-based optimization framework leveraging frontier contraction and abstract multi-hop jump propagation to accelerate shortest-path computation while strictly preserving correctness. JFR achieves substantial reductions in relaxation operations, ranging from -31 to 99 percent, across sparse, dense, and negative-edge graphs, ensuring robust performance even under adversarial or highly connected topologies. On ultra-large graphs with up to N=10,000 nodes and 55,000,000 edges, JFR maintains strong operational reductions and comparable or improved runtime relative to SPFA-SLF, demonstrating consistent robustness across graph size and density. Lower relaxation counts imply reduced memory-access overheads and computational effort; this normalized work reduction highlights JFR's suitability for scenarios requiring high throughput or energy-conscious operation. Future work focuses on integrating high-performance queue structures, adaptive frontier strategies, and cache-aware techniques to further reduce constant-factor overheads and fully realize JFR's practical runtime potential.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose JFR, a Bellman-Ford-based optimization framework leveraging frontier contraction and abstract multi-hop jump propagation to accelerate shortest-path computation while strictly preserving correctness. JFR achieves substantial reductions in relaxation operations, ranging from -31 to 99 percent, across sparse, dense, and negative-edge graphs, ensuring robust performance even under adversarial or highly connected topologies. On ultra-large graphs with up to N=10,000 nodes and 55,000,000 edges, JFR maintains strong operational reductions and comparable or improved runtime relative to SPFA-SLF, demonstrating consistent robustness across graph size and density. Lower relaxation counts imply reduced memory-access overheads and computational effort; this normalized work reduction highlights JFR's suitability for scenarios requiring high throughput or energy-conscious operation. Future work focuses on integrating high-performance queue structures, adaptive frontier strategies, and cache-aware techniques to further reduce constant-factor overheads and fully realize JFR's practical runtime potential."
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-01T15:35:53Z",
                "published_parsed": [
                    2025,
                    12,
                    1,
                    15,
                    35,
                    53,
                    0,
                    335,
                    0
                ],
                "arxiv_comment": "with editor,25 pages",
                "arxiv_primary_category": {
                    "term": "cs.DS"
                },
                "authors": [
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Xi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xi Chen"
                },
                "author": "Xi Chen"
            },
            {
                "id": "http://arxiv.org/abs/2512.18345v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.18345v1",
                "title": "Theodosian: A Deep Dive into Memory-Hierarchy-Centric FHE Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Theodosian: A Deep Dive into Memory-Hierarchy-Centric FHE Acceleration"
                },
                "updated": "2025-12-20T12:18:29Z",
                "updated_parsed": [
                    2025,
                    12,
                    20,
                    12,
                    18,
                    29,
                    5,
                    354,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.18345v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.18345v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Fully homomorphic encryption (FHE) enables secure computation on encrypted data, mitigating privacy concerns in cloud and edge environments. However, due to its high compute and memory demands, extensive acceleration research has been pursued across diverse hardware platforms, especially GPUs. In this paper, we perform a microarchitectural analysis of CKKS, a popular FHE scheme, on modern GPUs. We focus on on-chip cache behavior, and show that the dominant kernels remain bound by memory bandwidth despite a high-bandwidth L2 cache, exposing a persistent memory wall. We further discover that the overall CKKS pipeline throughput is constrained by low per-kernel hardware utilization, caused by insufficient intra-kernel parallelism. Motivated by these findings, we introduce Theodosian, a set of complementary, memory-aware optimizations that improve cache efficiency and reduce runtime overheads. Our approach delivers consistent speedups across various CKKS workloads. On an RTX 5090, we reduce the bootstrapping latency for 32,768 complex numbers to 15.2ms with Theodosian, and further to 12.8ms with additional algorithmic optimizations, establishing new state-of-the-art GPU performance to the best of our knowledge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fully homomorphic encryption (FHE) enables secure computation on encrypted data, mitigating privacy concerns in cloud and edge environments. However, due to its high compute and memory demands, extensive acceleration research has been pursued across diverse hardware platforms, especially GPUs. In this paper, we perform a microarchitectural analysis of CKKS, a popular FHE scheme, on modern GPUs. We focus on on-chip cache behavior, and show that the dominant kernels remain bound by memory bandwidth despite a high-bandwidth L2 cache, exposing a persistent memory wall. We further discover that the overall CKKS pipeline throughput is constrained by low per-kernel hardware utilization, caused by insufficient intra-kernel parallelism. Motivated by these findings, we introduce Theodosian, a set of complementary, memory-aware optimizations that improve cache efficiency and reduce runtime overheads. Our approach delivers consistent speedups across various CKKS workloads. On an RTX 5090, we reduce the bootstrapping latency for 32,768 complex numbers to 15.2ms with Theodosian, and further to 12.8ms with additional algorithmic optimizations, establishing new state-of-the-art GPU performance to the best of our knowledge."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-20T12:18:29Z",
                "published_parsed": [
                    2025,
                    12,
                    20,
                    12,
                    18,
                    29,
                    5,
                    354,
                    0
                ],
                "arxiv_comment": "11 pages, 8 figures",
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "authors": [
                    {
                        "name": "Wonseok Choi"
                    },
                    {
                        "name": "Hyunah Yu"
                    },
                    {
                        "name": "Jongmin Kim"
                    },
                    {
                        "name": "Hyesung Ji"
                    },
                    {
                        "name": "Jaiyoung Park"
                    },
                    {
                        "name": "Jung Ho Ahn"
                    }
                ],
                "author_detail": {
                    "name": "Jung Ho Ahn"
                },
                "author": "Jung Ho Ahn"
            },
            {
                "id": "http://arxiv.org/abs/2512.18337v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.18337v1",
                "title": "Towards Efficient Agents: A Co-Design of Inference Architecture and System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Efficient Agents: A Co-Design of Inference Architecture and System"
                },
                "updated": "2025-12-20T12:06:13Z",
                "updated_parsed": [
                    2025,
                    12,
                    20,
                    12,
                    6,
                    13,
                    5,
                    354,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.18337v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.18337v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The rapid development of large language model (LLM)-based agents has unlocked new possibilities for autonomous multi-turn reasoning and tool-augmented decision-making. However, their real-world deployment is hindered by severe inefficiencies that arise not from isolated model inference, but from the systemic latency accumulated across reasoning loops, context growth, and heterogeneous tool interactions. This paper presents AgentInfer, a unified framework for end-to-end agent acceleration that bridges inference optimization and architectural design. We decompose the problem into four synergistic components: AgentCollab, a hierarchical dual-model reasoning framework that balances large- and small-model usage through dynamic role assignment; AgentSched, a cache-aware hybrid scheduler that minimizes latency under heterogeneous request patterns; AgentSAM, a suffix-automaton-based speculative decoding method that reuses multi-session semantic memory to achieve low-overhead inference acceleration; and AgentCompress, a semantic compression mechanism that asynchronously distills and reorganizes agent memory without disrupting ongoing reasoning. Together, these modules form a Self-Evolution Engine capable of sustaining efficiency and cognitive stability throughout long-horizon reasoning tasks. Experiments on the BrowseComp-zh and DeepDiver benchmarks demonstrate that through the synergistic collaboration of these methods, AgentInfer reduces ineffective token consumption by over 50%, achieving an overall 1.8-2.5 times speedup with preserved accuracy. These results underscore that optimizing for agentic task completion-rather than merely per-token throughput-is the key to building scalable, efficient, and self-improving intelligent systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development of large language model (LLM)-based agents has unlocked new possibilities for autonomous multi-turn reasoning and tool-augmented decision-making. However, their real-world deployment is hindered by severe inefficiencies that arise not from isolated model inference, but from the systemic latency accumulated across reasoning loops, context growth, and heterogeneous tool interactions. This paper presents AgentInfer, a unified framework for end-to-end agent acceleration that bridges inference optimization and architectural design. We decompose the problem into four synergistic components: AgentCollab, a hierarchical dual-model reasoning framework that balances large- and small-model usage through dynamic role assignment; AgentSched, a cache-aware hybrid scheduler that minimizes latency under heterogeneous request patterns; AgentSAM, a suffix-automaton-based speculative decoding method that reuses multi-session semantic memory to achieve low-overhead inference acceleration; and AgentCompress, a semantic compression mechanism that asynchronously distills and reorganizes agent memory without disrupting ongoing reasoning. Together, these modules form a Self-Evolution Engine capable of sustaining efficiency and cognitive stability throughout long-horizon reasoning tasks. Experiments on the BrowseComp-zh and DeepDiver benchmarks demonstrate that through the synergistic collaboration of these methods, AgentInfer reduces ineffective token consumption by over 50%, achieving an overall 1.8-2.5 times speedup with preserved accuracy. These results underscore that optimizing for agentic task completion-rather than merely per-token throughput-is the key to building scalable, efficient, and self-improving intelligent systems."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-20T12:06:13Z",
                "published_parsed": [
                    2025,
                    12,
                    20,
                    12,
                    6,
                    13,
                    5,
                    354,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Weizhe Lin"
                    },
                    {
                        "name": "Hui-Ling Zhen"
                    },
                    {
                        "name": "Shuai Yang"
                    },
                    {
                        "name": "Xian Wang"
                    },
                    {
                        "name": "Renxi Liu"
                    },
                    {
                        "name": "Hanting Chen"
                    },
                    {
                        "name": "Wangze Zhang"
                    },
                    {
                        "name": "Chuansai Zhou"
                    },
                    {
                        "name": "Yiming Li"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Xing Li"
                    },
                    {
                        "name": "Zhiyuan Yang"
                    },
                    {
                        "name": "Xiaosong Li"
                    },
                    {
                        "name": "Xianzhi Yu"
                    },
                    {
                        "name": "Zhenhua Dong"
                    },
                    {
                        "name": "Mingxuan Yuan"
                    },
                    {
                        "name": "Yunhe Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yunhe Wang"
                },
                "author": "Yunhe Wang"
            },
            {
                "id": "http://arxiv.org/abs/2512.18300v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.18300v1",
                "title": "BARD: Reducing Write Latency of DDR5 Memory by Exploiting Bank-Parallelism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BARD: Reducing Write Latency of DDR5 Memory by Exploiting Bank-Parallelism"
                },
                "updated": "2025-12-20T10:11:47Z",
                "updated_parsed": [
                    2025,
                    12,
                    20,
                    10,
                    11,
                    47,
                    5,
                    354,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.18300v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.18300v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "This paper studies the impact of DRAM writes on DDR5-based system. To efficiently perform DRAM writes, modern systems buffer write requests and try to complete multiple write operations whenever the DRAM mode is switched from read to write. When the DRAM system is performing writes, it is not available to service read requests, thus increasing read latency and reducing performance. We observe that, given the presence of on-die ECC in DDR5 devices, the time to perform a write operation varies significantly: from 1x (for writes to banks of different bankgroups) to 6x (for writes to banks within the same bankgroup) to 24x (for conflicting requests to the same bank). If we can orchestrate the write stream to favor write requests that incur lower latency, then we can reduce the stall time from DRAM writes and improve performance. However, for current systems, the write stream is dictated by the cache replacement policy, which makes eviction decisions without being aware of the variable latency of DRAM writes. The key insight of our work is to improve performance by modifying the cache replacement policy to increase bank-parallelism of DRAM writes.\n  Our paper proposes {\\em BARD (Bank-Aware Replacement Decisions)}, which modifies the cache replacement policy to favor dirty lines that belong to banks without pending writes. We analyze two variants of BARD: BARD-E (Eviction-based), which changes the eviction policy to evict low-cost dirty lines, and BARD-C (Cleansing-Based), which proactively cleans low-cost dirty lines without modifying the eviction decisions. We develop a hybrid policy (BARD-H), which uses a selective combination of both eviction and writeback. Our evaluations across workloads from SPEC2017, LIGRA, STREAM, and Google server traces show that BARD-H improves performance by 4.3\\% on average and up-to 8.5\\%. BARD requires only 8 bytes of SRAM per LLC slice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper studies the impact of DRAM writes on DDR5-based system. To efficiently perform DRAM writes, modern systems buffer write requests and try to complete multiple write operations whenever the DRAM mode is switched from read to write. When the DRAM system is performing writes, it is not available to service read requests, thus increasing read latency and reducing performance. We observe that, given the presence of on-die ECC in DDR5 devices, the time to perform a write operation varies significantly: from 1x (for writes to banks of different bankgroups) to 6x (for writes to banks within the same bankgroup) to 24x (for conflicting requests to the same bank). If we can orchestrate the write stream to favor write requests that incur lower latency, then we can reduce the stall time from DRAM writes and improve performance. However, for current systems, the write stream is dictated by the cache replacement policy, which makes eviction decisions without being aware of the variable latency of DRAM writes. The key insight of our work is to improve performance by modifying the cache replacement policy to increase bank-parallelism of DRAM writes.\n  Our paper proposes {\\em BARD (Bank-Aware Replacement Decisions)}, which modifies the cache replacement policy to favor dirty lines that belong to banks without pending writes. We analyze two variants of BARD: BARD-E (Eviction-based), which changes the eviction policy to evict low-cost dirty lines, and BARD-C (Cleansing-Based), which proactively cleans low-cost dirty lines without modifying the eviction decisions. We develop a hybrid policy (BARD-H), which uses a selective combination of both eviction and writeback. Our evaluations across workloads from SPEC2017, LIGRA, STREAM, and Google server traces show that BARD-H improves performance by 4.3\\% on average and up-to 8.5\\%. BARD requires only 8 bytes of SRAM per LLC slice."
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-20T10:11:47Z",
                "published_parsed": [
                    2025,
                    12,
                    20,
                    10,
                    11,
                    47,
                    5,
                    354,
                    0
                ],
                "arxiv_comment": "Accepted to HPCA 2026",
                "arxiv_primary_category": {
                    "term": "cs.AR"
                },
                "authors": [
                    {
                        "name": "Suhas Vittal"
                    },
                    {
                        "name": "Moinuddin Qureshi"
                    }
                ],
                "author_detail": {
                    "name": "Moinuddin Qureshi"
                },
                "author": "Moinuddin Qureshi"
            },
            {
                "id": "http://arxiv.org/abs/2512.18194v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.18194v1",
                "title": "TraCT: Disaggregated LLM Serving with CXL Shared Memory KV Cache at Rack-Scale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TraCT: Disaggregated LLM Serving with CXL Shared Memory KV Cache at Rack-Scale"
                },
                "updated": "2025-12-20T03:42:19Z",
                "updated_parsed": [
                    2025,
                    12,
                    20,
                    3,
                    42,
                    19,
                    5,
                    354,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.18194v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.18194v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Disaggregated LLM serving improves resource efficiency by separating the compute-intensive prefill phase from the latency-critical decode phase. However, this architecture introduces a fundamental bottleneck: key/value (KV) tensors generated during prefill must be transferred to decode workers, and existing systems rely on RDMA-based network paths for this exchange. As model sizes and context lengths increase, KV transfer dominates both time-to-first-token (TTFT) and peak throughput, and remains highly sensitive to network contention even when prefix reuse is high. This paper presents TraCT, a rack-scale LLM serving system that uses CXL shared memory as both a KV-transfer substrate and a rack-wide prefix-aware KV cache. TraCT enables GPUs to write and read KV blocks directly through CXL load/store and DMA operations, eliminating the NIC hop that constrains existing disaggregated pipelines. However, to realize this design, multiple new challenges such as synchronization, consistency, and data management on non-coherent CXL memory need to be addressed. TraCT proposes various software solutions such as the two-tier inter-node synchronization mechanism to address these challenges. We implement TraCT on the Dynamo LLM inference framework and show that, across static and synthetic workloads, TraCT reduces average TTFT by up to 9.8x, lowers P99 latency by up to 6.2x, and improves peak throughput by up to 1.6x compared to RDMA and DRAM-based caching baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disaggregated LLM serving improves resource efficiency by separating the compute-intensive prefill phase from the latency-critical decode phase. However, this architecture introduces a fundamental bottleneck: key/value (KV) tensors generated during prefill must be transferred to decode workers, and existing systems rely on RDMA-based network paths for this exchange. As model sizes and context lengths increase, KV transfer dominates both time-to-first-token (TTFT) and peak throughput, and remains highly sensitive to network contention even when prefix reuse is high. This paper presents TraCT, a rack-scale LLM serving system that uses CXL shared memory as both a KV-transfer substrate and a rack-wide prefix-aware KV cache. TraCT enables GPUs to write and read KV blocks directly through CXL load/store and DMA operations, eliminating the NIC hop that constrains existing disaggregated pipelines. However, to realize this design, multiple new challenges such as synchronization, consistency, and data management on non-coherent CXL memory need to be addressed. TraCT proposes various software solutions such as the two-tier inter-node synchronization mechanism to address these challenges. We implement TraCT on the Dynamo LLM inference framework and show that, across static and synthetic workloads, TraCT reduces average TTFT by up to 9.8x, lowers P99 latency by up to 6.2x, and improves peak throughput by up to 1.6x compared to RDMA and DRAM-based caching baselines."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-20T03:42:19Z",
                "published_parsed": [
                    2025,
                    12,
                    20,
                    3,
                    42,
                    19,
                    5,
                    354,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Dongha Yoon"
                    },
                    {
                        "name": "Younghoon Min"
                    },
                    {
                        "name": "Hoshik Kim"
                    },
                    {
                        "name": "Sam H. Noh"
                    },
                    {
                        "name": "Jongryool Kim"
                    }
                ],
                "author_detail": {
                    "name": "Jongryool Kim"
                },
                "author": "Jongryool Kim"
            },
            {
                "id": "http://arxiv.org/abs/2511.02230v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.02230v2",
                "title": "Continuum: Efficient and Robust Multi-Turn LLM Agent Scheduling with KV Cache Time-to-Live",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continuum: Efficient and Robust Multi-Turn LLM Agent Scheduling with KV Cache Time-to-Live"
                },
                "updated": "2025-12-20T01:17:03Z",
                "updated_parsed": [
                    2025,
                    12,
                    20,
                    1,
                    17,
                    3,
                    5,
                    354,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.02230v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.02230v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "KV cache management is essential for efficient LLM inference. To maximize utilization, existing inference engines evict finished requests' KV cache if new requests are waiting. This policy breaks for agentic workloads, which interleave LLM calls with tools, introducing pauses that prevent effective KV reuse across turns. Since some tool calls have much shorter durations than human response multi-turn chatbot, it would be promising to retain the KV cache in during these tools. However, there are many challenges. First, we need to consider both the potential cost of recomputation or reloading (if CPU offloading enabled) and the increasing queueing delays after eviction from GPU. Second, due to the internal variance of tool call durations, we need the method to remain robust under limited predictability of tool call durations.\n  We present Continuum, a serving system to optimize job completion time for multi-turn agent workloads by introducing time-to-live mechanism for KV cache retaining. For LLM request that generates a tool call, Continuum selectively pins the KV cache in GPU memory with a time-to-live value determined by considering both the reload cost and ordering preserve benefit of retaining KV cache. Moreover, when the TTL expires, the KV cache can be automatically evicted to free up GPU memory, providing robust performance under edge cases. When combined with program-level first-come-first-serve, Continuum preserves multi-turn continuity, and reduces delay for complex agentic workflows. Our evaluation on real-world agentic workloads (SWE-Bench and BFCL) with Llama-3.1 8B/70B shows that Continuum significantly improves the average job completion times and its improvement scales with turn number increase. We release a preview version at: https://github.com/Hanchenli/vllm-continuum",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV cache management is essential for efficient LLM inference. To maximize utilization, existing inference engines evict finished requests' KV cache if new requests are waiting. This policy breaks for agentic workloads, which interleave LLM calls with tools, introducing pauses that prevent effective KV reuse across turns. Since some tool calls have much shorter durations than human response multi-turn chatbot, it would be promising to retain the KV cache in during these tools. However, there are many challenges. First, we need to consider both the potential cost of recomputation or reloading (if CPU offloading enabled) and the increasing queueing delays after eviction from GPU. Second, due to the internal variance of tool call durations, we need the method to remain robust under limited predictability of tool call durations.\n  We present Continuum, a serving system to optimize job completion time for multi-turn agent workloads by introducing time-to-live mechanism for KV cache retaining. For LLM request that generates a tool call, Continuum selectively pins the KV cache in GPU memory with a time-to-live value determined by considering both the reload cost and ordering preserve benefit of retaining KV cache. Moreover, when the TTL expires, the KV cache can be automatically evicted to free up GPU memory, providing robust performance under edge cases. When combined with program-level first-come-first-serve, Continuum preserves multi-turn continuity, and reduces delay for complex agentic workflows. Our evaluation on real-world agentic workloads (SWE-Bench and BFCL) with Llama-3.1 8B/70B shows that Continuum significantly improves the average job completion times and its improvement scales with turn number increase. We release a preview version at: https://github.com/Hanchenli/vllm-continuum"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-04T03:43:05Z",
                "published_parsed": [
                    2025,
                    11,
                    4,
                    3,
                    43,
                    5,
                    1,
                    308,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS"
                },
                "authors": [
                    {
                        "name": "Hanchen Li"
                    },
                    {
                        "name": "Qiuyang Mang"
                    },
                    {
                        "name": "Runyuan He"
                    },
                    {
                        "name": "Qizheng Zhang"
                    },
                    {
                        "name": "Huanzhi Mao"
                    },
                    {
                        "name": "Xiaokun Chen"
                    },
                    {
                        "name": "Hangrui Zhou"
                    },
                    {
                        "name": "Alvin Cheung"
                    },
                    {
                        "name": "Joseph Gonzalez"
                    },
                    {
                        "name": "Ion Stoica"
                    }
                ],
                "author_detail": {
                    "name": "Ion Stoica"
                },
                "author": "Ion Stoica"
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2601.02360v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.02360v1",
                "title": "Heterogeneous Low-Bandwidth Pre-Training of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Heterogeneous Low-Bandwidth Pre-Training of LLMs"
                },
                "updated": "2026-01-05T18:59:57Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    18,
                    59,
                    57,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.02360v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.02360v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Pre-training large language models (LLMs) increasingly requires distributed compute, yet bandwidth constraints make it difficult to scale beyond well-provisioned datacenters-especially when model parallelism forces frequent, large inter-device communications. We study whether SparseLoCo, a low-communication data parallel method based on infrequent synchronization and sparse pseudo-gradient exchange, can be combined with low-bandwidth pipeline model parallelism via activation and activation-gradient compression. We introduce a heterogeneous distributed training framework where some participants host full replicas on high-bandwidth interconnects, while resource-limited participants are grouped to jointly instantiate a replica using pipeline parallelism with subspace-projected inter-stage communication. To make the recently introduced subspace pipeline compression compatible with SparseLoCo, we study a number of adaptations. Across large-scale language modeling experiments (178M-1B parameters) on standard pretraining corpora, we find that activation compression composes with SparseLoCo at modest cost, while selective (heterogeneous) compression consistently improves the loss-communication tradeoff relative to compressing all replicas-especially at aggressive compression ratios. These results suggest a practical path to incorporating low-bandwidth model parallelism and heterogeneous participants into LLM pre-training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pre-training large language models (LLMs) increasingly requires distributed compute, yet bandwidth constraints make it difficult to scale beyond well-provisioned datacenters-especially when model parallelism forces frequent, large inter-device communications. We study whether SparseLoCo, a low-communication data parallel method based on infrequent synchronization and sparse pseudo-gradient exchange, can be combined with low-bandwidth pipeline model parallelism via activation and activation-gradient compression. We introduce a heterogeneous distributed training framework where some participants host full replicas on high-bandwidth interconnects, while resource-limited participants are grouped to jointly instantiate a replica using pipeline parallelism with subspace-projected inter-stage communication. To make the recently introduced subspace pipeline compression compatible with SparseLoCo, we study a number of adaptations. Across large-scale language modeling experiments (178M-1B parameters) on standard pretraining corpora, we find that activation compression composes with SparseLoCo at modest cost, while selective (heterogeneous) compression consistently improves the loss-communication tradeoff relative to compressing all replicas-especially at aggressive compression ratios. These results suggest a practical path to incorporating low-bandwidth model parallelism and heterogeneous participants into LLM pre-training."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-05T18:59:57Z",
                "published_parsed": [
                    2026,
                    1,
                    5,
                    18,
                    59,
                    57,
                    0,
                    5,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Yazan Obeidi"
                    },
                    {
                        "name": "Amir Sarfi"
                    },
                    {
                        "name": "Joel Lidin"
                    },
                    {
                        "name": "Paul Janson"
                    },
                    {
                        "name": "Eugene Belilovsky"
                    }
                ],
                "author_detail": {
                    "name": "Eugene Belilovsky"
                },
                "author": "Eugene Belilovsky"
            },
            {
                "id": "http://arxiv.org/abs/2601.02346v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.02346v1",
                "title": "Falcon-H1R: Pushing the Reasoning Frontiers with a Hybrid Model for Efficient Test-Time Scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Falcon-H1R: Pushing the Reasoning Frontiers with a Hybrid Model for Efficient Test-Time Scaling"
                },
                "updated": "2026-01-05T18:44:27Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    18,
                    44,
                    27,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.02346v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.02346v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "This work introduces Falcon-H1R, a 7B-parameter reasoning-optimized model that establishes the feasibility of achieving competitive reasoning performance with small language models (SLMs). Falcon-H1R stands out for its parameter efficiency, consistently matching or outperforming SOTA reasoning models that are $2\\times$ to $7\\times$ larger across a variety of reasoning-intensive benchmarks. These results underscore the importance of careful data curation and targeted training strategies (via both efficient SFT and RL scaling) in delivering significant performance gains without increasing model size. Furthermore, Falcon-H1R advances the 3D limits of reasoning efficiency by combining faster inference (through its hybrid-parallel architecture design), token efficiency, and higher accuracy. This unique blend makes Falcon-H1R-7B a practical backbone for scaling advanced reasoning systems, particularly in scenarios requiring extensive chain-of-thoughts generation and parallel test-time scaling. Leveraging the recently introduced DeepConf approach, Falcon-H1R achieves state-of-the-art test-time scaling efficiency, offering substantial improvements in both accuracy and computational cost. As a result, Falcon-H1R demonstrates that compact models, through targeted model training and architectural choices, can deliver robust and scalable reasoning performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work introduces Falcon-H1R, a 7B-parameter reasoning-optimized model that establishes the feasibility of achieving competitive reasoning performance with small language models (SLMs). Falcon-H1R stands out for its parameter efficiency, consistently matching or outperforming SOTA reasoning models that are $2\\times$ to $7\\times$ larger across a variety of reasoning-intensive benchmarks. These results underscore the importance of careful data curation and targeted training strategies (via both efficient SFT and RL scaling) in delivering significant performance gains without increasing model size. Furthermore, Falcon-H1R advances the 3D limits of reasoning efficiency by combining faster inference (through its hybrid-parallel architecture design), token efficiency, and higher accuracy. This unique blend makes Falcon-H1R-7B a practical backbone for scaling advanced reasoning systems, particularly in scenarios requiring extensive chain-of-thoughts generation and parallel test-time scaling. Leveraging the recently introduced DeepConf approach, Falcon-H1R achieves state-of-the-art test-time scaling efficiency, offering substantial improvements in both accuracy and computational cost. As a result, Falcon-H1R demonstrates that compact models, through targeted model training and architectural choices, can deliver robust and scalable reasoning performance."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-05T18:44:27Z",
                "published_parsed": [
                    2026,
                    1,
                    5,
                    18,
                    44,
                    27,
                    0,
                    5,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Falcon LLM Team"
                    },
                    {
                        "name": "Iheb Chaabane"
                    },
                    {
                        "name": "Puneesh Khanna"
                    },
                    {
                        "name": "Suhail Mohmad"
                    },
                    {
                        "name": "Slim Frikha"
                    },
                    {
                        "name": "Shi Hu"
                    },
                    {
                        "name": "Abdalgader Abubaker"
                    },
                    {
                        "name": "Reda Alami"
                    },
                    {
                        "name": "Mikhail Lubinets"
                    },
                    {
                        "name": "Mohamed El Amine Seddik"
                    },
                    {
                        "name": "Hakim Hacid"
                    }
                ],
                "author_detail": {
                    "name": "Hakim Hacid"
                },
                "author": "Hakim Hacid"
            },
            {
                "id": "http://arxiv.org/abs/2507.12171v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2507.12171v3",
                "title": "Cosmic Cartography II: completing galaxy catalogs for gravitational-wave cosmology",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cosmic Cartography II: completing galaxy catalogs for gravitational-wave cosmology"
                },
                "updated": "2026-01-05T18:38:32Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    18,
                    38,
                    32,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2507.12171v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2507.12171v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.1088/1475-7516/2026/01/013",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "The dark siren method exploits the complementarity between gravitational-wave binary coalescence signals and galaxy catalogs originating from the same regions of space. However, all galaxy catalogs are incomplete, i.e. they only include a subset of all galaxies, typically being biased towards the bright end of the luminosity distribution. This sub-selection systematically affects the dark siren inference of the Hubble constant $H_0$, so a completeness relation has to be introduced that accounts for the missing objects. In the literature it is standard to assume that the missing galaxies are uniformly distributed across the sky and that the galaxy magnitude distribution is known. In this work we develop a novel method which improves upon these assumptions and reconstructs the underlying true galaxy field, respecting the spatial correlation of galaxies on large scales. In our method the true magnitude distribution of galaxies is inferred alongside the spatial galaxy distribution. Our method results in an improved three-dimensional prior in redshift and sky position for the host galaxy of a GW event, which is expected to make the resulting $H_0$ posterior more robust. Building on our previous work, we make a number of improvements, and validate our method on simulated data based on the Millennium simulation. The inference results can be reproduced through our publicly available code base light.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The dark siren method exploits the complementarity between gravitational-wave binary coalescence signals and galaxy catalogs originating from the same regions of space. However, all galaxy catalogs are incomplete, i.e. they only include a subset of all galaxies, typically being biased towards the bright end of the luminosity distribution. This sub-selection systematically affects the dark siren inference of the Hubble constant $H_0$, so a completeness relation has to be introduced that accounts for the missing objects. In the literature it is standard to assume that the missing galaxies are uniformly distributed across the sky and that the galaxy magnitude distribution is known. In this work we develop a novel method which improves upon these assumptions and reconstructs the underlying true galaxy field, respecting the spatial correlation of galaxies on large scales. In our method the true magnitude distribution of galaxies is inferred alongside the spatial galaxy distribution. Our method results in an improved three-dimensional prior in redshift and sky position for the host galaxy of a GW event, which is expected to make the resulting $H_0$ posterior more robust. Building on our previous work, we make a number of improvements, and validate our method on simulated data based on the Millennium simulation. The inference results can be reproduced through our publicly available code base light."
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-07-16T12:05:23Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    12,
                    5,
                    23,
                    2,
                    197,
                    0
                ],
                "arxiv_comment": "We refer the busy reader to Fig. 2 for an overview of the method, and to Sec. 4.1 with Figures 7, 8 and 10 for the main results. 45 pages, 20 figures",
                "arxiv_primary_category": {
                    "term": "astro-ph.CO"
                },
                "arxiv_journal_ref": "JCAP 01 (2026) 013",
                "authors": [
                    {
                        "name": "Konstantin Leyde"
                    },
                    {
                        "name": "Tessa Baker"
                    },
                    {
                        "name": "Wolfgang Enzi"
                    }
                ],
                "author_detail": {
                    "name": "Wolfgang Enzi"
                },
                "author": "Wolfgang Enzi",
                "arxiv_doi": "10.1088/1475-7516/2026/01/013"
            },
            {
                "id": "http://arxiv.org/abs/2511.14301v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.14301v3",
                "title": "SteganoBackdoor: Stealthy and Data-Efficient Backdoor Attacks on Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SteganoBackdoor: Stealthy and Data-Efficient Backdoor Attacks on Language Models"
                },
                "updated": "2026-01-05T18:33:56Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    18,
                    33,
                    56,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.14301v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.14301v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Modern language models remain vulnerable to backdoor attacks via poisoned data, where training inputs containing a trigger are paired with a target output, causing the model to reproduce that behavior whenever the trigger appears at inference time. Recent work has emphasized stealthy attacks that stress-test data-curation defenses using stylized artifacts or token-level perturbations as triggers, but this focus leaves a more practically relevant threat model underexplored: backdoors tied to naturally occurring semantic concepts. We introduce SteganoBackdoor, an optimization-based framework that constructs SteganoPoisons, steganographic poisoned training examples in which a backdoor payload is distributed across a fluent sentence while exhibiting no representational overlap with the inference-time semantic trigger. Across diverse model architectures, SteganoBackdoor achieves high attack success under constrained poisoning budgets and remains effective under conservative data-level filtering, highlighting a blind spot in existing data-curation defenses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern language models remain vulnerable to backdoor attacks via poisoned data, where training inputs containing a trigger are paired with a target output, causing the model to reproduce that behavior whenever the trigger appears at inference time. Recent work has emphasized stealthy attacks that stress-test data-curation defenses using stylized artifacts or token-level perturbations as triggers, but this focus leaves a more practically relevant threat model underexplored: backdoors tied to naturally occurring semantic concepts. We introduce SteganoBackdoor, an optimization-based framework that constructs SteganoPoisons, steganographic poisoned training examples in which a backdoor payload is distributed across a fluent sentence while exhibiting no representational overlap with the inference-time semantic trigger. Across diverse model architectures, SteganoBackdoor achieves high attack success under constrained poisoning budgets and remains effective under conservative data-level filtering, highlighting a blind spot in existing data-curation defenses."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-18T09:56:16Z",
                "published_parsed": [
                    2025,
                    11,
                    18,
                    9,
                    56,
                    16,
                    1,
                    322,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "authors": [
                    {
                        "name": "Eric Xue"
                    },
                    {
                        "name": "Ruiyi Zhang"
                    },
                    {
                        "name": "Pengtao Xie"
                    }
                ],
                "author_detail": {
                    "name": "Pengtao Xie"
                },
                "author": "Pengtao Xie"
            },
            {
                "id": "http://arxiv.org/abs/2601.02337v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.02337v1",
                "title": "Robust Persona-Aware Toxicity Detection with Prompt Optimization and Learned Ensembling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust Persona-Aware Toxicity Detection with Prompt Optimization and Learned Ensembling"
                },
                "updated": "2026-01-05T18:32:45Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    18,
                    32,
                    45,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.02337v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.02337v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Toxicity detection is inherently subjective, shaped by the diverse perspectives and social priors of different demographic groups. While ``pluralistic'' modeling as used in economics and the social sciences aims to capture perspective differences across contexts, current Large Language Model (LLM) prompting techniques have different results across different personas and base models. In this work, we conduct a systematic evaluation of persona-aware toxicity detection, showing that no single prompting method, including our proposed automated prompt optimization strategy, uniformly dominates across all model-persona pairs. To exploit complementary errors, we explore ensembling four prompting variants and propose a lightweight meta-ensemble: an SVM over the 4-bit vector of prompt predictions. Our results demonstrate that the proposed SVM ensemble consistently outperforms individual prompting methods and traditional majority-voting techniques, achieving the strongest overall performance across diverse personas. This work provides one of the first systematic comparisons of persona-conditioned prompting for toxicity detection and offers a robust method for pluralistic evaluation in subjective NLP tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toxicity detection is inherently subjective, shaped by the diverse perspectives and social priors of different demographic groups. While ``pluralistic'' modeling as used in economics and the social sciences aims to capture perspective differences across contexts, current Large Language Model (LLM) prompting techniques have different results across different personas and base models. In this work, we conduct a systematic evaluation of persona-aware toxicity detection, showing that no single prompting method, including our proposed automated prompt optimization strategy, uniformly dominates across all model-persona pairs. To exploit complementary errors, we explore ensembling four prompting variants and propose a lightweight meta-ensemble: an SVM over the 4-bit vector of prompt predictions. Our results demonstrate that the proposed SVM ensemble consistently outperforms individual prompting methods and traditional majority-voting techniques, achieving the strongest overall performance across diverse personas. This work provides one of the first systematic comparisons of persona-conditioned prompting for toxicity detection and offers a robust method for pluralistic evaluation in subjective NLP tasks."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-05T18:32:45Z",
                "published_parsed": [
                    2026,
                    1,
                    5,
                    18,
                    32,
                    45,
                    0,
                    5,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Berk Atil"
                    },
                    {
                        "name": "Rebecca J. Passonneau"
                    },
                    {
                        "name": "Ninareh Mehrabi"
                    }
                ],
                "author_detail": {
                    "name": "Ninareh Mehrabi"
                },
                "author": "Ninareh Mehrabi"
            },
            {
                "id": "http://arxiv.org/abs/2601.02336v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.02336v1",
                "title": "The Sequential Monte Carlo goes NUTS: Boosting Gravitational-Wave Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Sequential Monte Carlo goes NUTS: Boosting Gravitational-Wave Inference"
                },
                "updated": "2026-01-05T18:31:26Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    18,
                    31,
                    26,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.02336v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.02336v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Sequential Monte Carlo (SMC) methods have recently been applied to gravitational-wave inference as a powerful alternative to standard sampling techniques, such as Nested Sampling. At the same time, gradient-based Markov Chain Monte Carlo algorithms, most notably the No-U-Turn Sampler (NUTS), provide an efficient way to explore high-dimensional parameter spaces. In this work we present SHARPy, a Bayesian inference framework that combines the parallelism and evidence-estimation capabilities of SMC with the state-of-the-art sampling performance of NUTS. Moreover, SHARPy exploits the local geometric structure of the posterior to further improve efficiency. Built on JAX and accelerated on GPUs, SHARPy performs gravitational-wave inference on binary black-hole events in around ten minutes, yielding posterior samples and Bayesian evidence estimates that are consistent with those obtained through Nested Sampling. This work sets a new milestone in GW inference with likelihood-based methods and paves the way for model comparison tasks to be accomplished in minutes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequential Monte Carlo (SMC) methods have recently been applied to gravitational-wave inference as a powerful alternative to standard sampling techniques, such as Nested Sampling. At the same time, gradient-based Markov Chain Monte Carlo algorithms, most notably the No-U-Turn Sampler (NUTS), provide an efficient way to explore high-dimensional parameter spaces. In this work we present SHARPy, a Bayesian inference framework that combines the parallelism and evidence-estimation capabilities of SMC with the state-of-the-art sampling performance of NUTS. Moreover, SHARPy exploits the local geometric structure of the posterior to further improve efficiency. Built on JAX and accelerated on GPUs, SHARPy performs gravitational-wave inference on binary black-hole events in around ten minutes, yielding posterior samples and Bayesian evidence estimates that are consistent with those obtained through Nested Sampling. This work sets a new milestone in GW inference with likelihood-based methods and paves the way for model comparison tasks to be accomplished in minutes."
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-05T18:31:26Z",
                "published_parsed": [
                    2026,
                    1,
                    5,
                    18,
                    31,
                    26,
                    0,
                    5,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "gr-qc"
                },
                "authors": [
                    {
                        "name": "Gabriele Demasi"
                    },
                    {
                        "name": "Giulia Capurri"
                    },
                    {
                        "name": "Massimo Lenti"
                    },
                    {
                        "name": "Angelo Ricciardone"
                    },
                    {
                        "name": "Barbara Patricelli"
                    },
                    {
                        "name": "Adriano Frattale Mascioli"
                    },
                    {
                        "name": "Lorenzo Piccari"
                    },
                    {
                        "name": "Saulo Albuquerque"
                    },
                    {
                        "name": "Gianluca M. Guidi"
                    },
                    {
                        "name": "Francesco Pannarale"
                    },
                    {
                        "name": "Giulia Stratta"
                    },
                    {
                        "name": "Walter Del Pozzo"
                    }
                ],
                "author_detail": {
                    "name": "Walter Del Pozzo"
                },
                "author": "Walter Del Pozzo"
            },
            {
                "id": "http://arxiv.org/abs/2601.00388v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.00388v2",
                "title": "Vision-Language Reasoning for Geolocalization: A Reinforcement Learning Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language Reasoning for Geolocalization: A Reinforcement Learning Approach"
                },
                "updated": "2026-01-05T18:27:19Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    18,
                    27,
                    19,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.00388v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.00388v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Recent advances in vision-language models have opened up new possibilities for reasoning-driven image geolocalization. However, existing approaches often rely on synthetic reasoning annotations or external image retrieval, which can limit interpretability and generalizability. In this paper, we present Geo-R, a retrieval-free framework that uncovers structured reasoning paths from existing ground-truth coordinates and optimizes geolocation accuracy via reinforcement learning. We propose the Chain of Region, a rule-based hierarchical reasoning paradigm that generates precise, interpretable supervision by mapping GPS coordinates to geographic entities (e.g., country, province, city) without relying on model-generated or synthetic labels. Building on this, we introduce a lightweight reinforcement learning strategy with coordinate-aligned rewards based on Haversine distance, enabling the model to refine predictions through spatially meaningful feedback. Our approach bridges structured geographic reasoning with direct spatial supervision, yielding improved localization accuracy, stronger generalization, and more transparent inference. Experimental results across multiple benchmarks confirm the effectiveness of Geo-R, establishing a new retrieval-free paradigm for scalable and interpretable image geolocalization. To facilitate further research and ensure reproducibility, both the model and code will be made publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in vision-language models have opened up new possibilities for reasoning-driven image geolocalization. However, existing approaches often rely on synthetic reasoning annotations or external image retrieval, which can limit interpretability and generalizability. In this paper, we present Geo-R, a retrieval-free framework that uncovers structured reasoning paths from existing ground-truth coordinates and optimizes geolocation accuracy via reinforcement learning. We propose the Chain of Region, a rule-based hierarchical reasoning paradigm that generates precise, interpretable supervision by mapping GPS coordinates to geographic entities (e.g., country, province, city) without relying on model-generated or synthetic labels. Building on this, we introduce a lightweight reinforcement learning strategy with coordinate-aligned rewards based on Haversine distance, enabling the model to refine predictions through spatially meaningful feedback. Our approach bridges structured geographic reasoning with direct spatial supervision, yielding improved localization accuracy, stronger generalization, and more transparent inference. Experimental results across multiple benchmarks confirm the effectiveness of Geo-R, establishing a new retrieval-free paradigm for scalable and interpretable image geolocalization. To facilitate further research and ensure reproducibility, both the model and code will be made publicly available."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-01T16:51:41Z",
                "published_parsed": [
                    2026,
                    1,
                    1,
                    16,
                    51,
                    41,
                    3,
                    1,
                    0
                ],
                "arxiv_comment": "Accepted to AAAI 2026. Project Page: https://github.com/aialt/geo-r",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Biao Wu"
                    },
                    {
                        "name": "Meng Fang"
                    },
                    {
                        "name": "Ling Chen"
                    },
                    {
                        "name": "Ke Xu"
                    },
                    {
                        "name": "Tao Cheng"
                    },
                    {
                        "name": "Jun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Wang"
                },
                "author": "Jun Wang"
            },
            {
                "id": "http://arxiv.org/abs/2506.21284v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2506.21284v2",
                "title": "Wide-field Polarization Imaging and Numerical Modeling of the Coma and Tail of Comet C/2023 A3 (Tsuchinshan-ATLAS)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wide-field Polarization Imaging and Numerical Modeling of the Coma and Tail of Comet C/2023 A3 (Tsuchinshan-ATLAS)"
                },
                "updated": "2026-01-05T18:25:58Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    18,
                    25,
                    58,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2506.21284v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2506.21284v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Imaging polarimetry enables the spatially resolved investigation of cometary dust properties across different morphological structures. While cometary comae have been studied thoroughly in the pertinent literature, cometary tails have remained less explored. Comparing these regions can reveal differences in the size, structure, and composition of their dust. The goal of this study is to examine the size, structure and composition of the dust particles in the coma and in particular in the tail of the bright comet C/2023 A3 (Tsuchinshan-ATLAS) and to infer possible differences. For this purpose, we rely on the method of telescopic wide-field polarimetric imaging of the comet in the visible to near-infrared domain in order to obtain the dependence of the degree of linear polarization (DoLP) of the coma and tail on the phase angle across a broad range. An off-the-shelf industrial grade polarization camera was used in combination with a telescope of short aperture ratio. These observations are complemented by T-matrix and Discrete Dipole Approximation modeling using the MSTM5 and DDSCAT software framework, respectively, for simulation of light scattering by dust particles of fractal agglomerate and agglomerate debris morphology. Our observations indicate that the coma exhibits a high maximum DoLP of 0.34, which is further exceeded by a factor of about two by the DoLP of the comet's tail. Our modeling results suggest a 50:50 olivine-carbon composition. The fraction of agglomerate debris was found to be 50% in the coma and possibly higher in the tail. The differences between coma and tail in the observed maximum DoLP and the phase angle at which it occurs can be explained by a predominance of particles with radii larger than 0.6 micrometer in the coma vs. smaller sub-micrometer particles close to the Rayleigh limit in the tail [...]",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Imaging polarimetry enables the spatially resolved investigation of cometary dust properties across different morphological structures. While cometary comae have been studied thoroughly in the pertinent literature, cometary tails have remained less explored. Comparing these regions can reveal differences in the size, structure, and composition of their dust. The goal of this study is to examine the size, structure and composition of the dust particles in the coma and in particular in the tail of the bright comet C/2023 A3 (Tsuchinshan-ATLAS) and to infer possible differences. For this purpose, we rely on the method of telescopic wide-field polarimetric imaging of the comet in the visible to near-infrared domain in order to obtain the dependence of the degree of linear polarization (DoLP) of the coma and tail on the phase angle across a broad range. An off-the-shelf industrial grade polarization camera was used in combination with a telescope of short aperture ratio. These observations are complemented by T-matrix and Discrete Dipole Approximation modeling using the MSTM5 and DDSCAT software framework, respectively, for simulation of light scattering by dust particles of fractal agglomerate and agglomerate debris morphology. Our observations indicate that the coma exhibits a high maximum DoLP of 0.34, which is further exceeded by a factor of about two by the DoLP of the comet's tail. Our modeling results suggest a 50:50 olivine-carbon composition. The fraction of agglomerate debris was found to be 50% in the coma and possibly higher in the tail. The differences between coma and tail in the observed maximum DoLP and the phase angle at which it occurs can be explained by a predominance of particles with radii larger than 0.6 micrometer in the coma vs. smaller sub-micrometer particles close to the Rayleigh limit in the tail [...]"
                },
                "tags": [
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-06-26T14:05:27Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    14,
                    5,
                    27,
                    3,
                    177,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.EP"
                },
                "authors": [
                    {
                        "name": "Mirza Arnaut"
                    },
                    {
                        "name": "Christian Wöhler"
                    },
                    {
                        "name": "Pritish Halder"
                    },
                    {
                        "name": "Goldy Ahuja"
                    },
                    {
                        "name": "Shashikiran Ganesh"
                    },
                    {
                        "name": "Megha Bhatt"
                    }
                ],
                "author_detail": {
                    "name": "Megha Bhatt"
                },
                "author": "Megha Bhatt"
            },
            {
                "id": "http://arxiv.org/abs/2601.02329v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.02329v1",
                "title": "BEDS: Bayesian Emergent Dissipative Structures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BEDS: Bayesian Emergent Dissipative Structures"
                },
                "updated": "2026-01-05T18:21:02Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    18,
                    21,
                    2,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.02329v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.02329v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We present BEDS (Bayesian Emergent Dissipative Structures), a theoretical framework that unifies concepts from non-equilibrium thermodynamics, Bayesian inference, information geometry, and machine learning. The central thesis proposes that learning, across physical, biological, and computational systems, fundamentally constitutes the conversion of flux into structure through entropy export. Building on Prigogine's theory of dissipative structures, we establish a formal isomorphism between thermodynamic processes and Bayesian updating, demonstrating that sustainable learning systems must follow dissipative patterns where crystallized posteriors become priors for subsequent levels of emergence.\n  We derive fundamental mathematical constants (e, π, φ) as fixed points of Bayesian inference under minimal axioms, suggesting these constants emerge necessarily from any system capable of representing and updating uncertainty. Furthermore, we propose a conjecture linking Gödel's incompleteness theorems to thermodynamic constraints, hypothesizing that pathologies of formal systems (incompleteness, undecidability) are structurally analogous to dissipation deficits in physical systems.\n  As practical validation, we present a peer-to-peer network architecture implementing BEDS principles, achieving six orders of magnitude improvement in energy efficiency compared to existing distributed consensus systems while enabling continuous learning. This work bridges fundamental physics, mathematical logic, and practical system design, offering both theoretical insights into the nature of learning and computation, and a concrete pathway toward sustainable artificial intelligence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present BEDS (Bayesian Emergent Dissipative Structures), a theoretical framework that unifies concepts from non-equilibrium thermodynamics, Bayesian inference, information geometry, and machine learning. The central thesis proposes that learning, across physical, biological, and computational systems, fundamentally constitutes the conversion of flux into structure through entropy export. Building on Prigogine's theory of dissipative structures, we establish a formal isomorphism between thermodynamic processes and Bayesian updating, demonstrating that sustainable learning systems must follow dissipative patterns where crystallized posteriors become priors for subsequent levels of emergence.\n  We derive fundamental mathematical constants (e, π, φ) as fixed points of Bayesian inference under minimal axioms, suggesting these constants emerge necessarily from any system capable of representing and updating uncertainty. Furthermore, we propose a conjecture linking Gödel's incompleteness theorems to thermodynamic constraints, hypothesizing that pathologies of formal systems (incompleteness, undecidability) are structurally analogous to dissipation deficits in physical systems.\n  As practical validation, we present a peer-to-peer network architecture implementing BEDS principles, achieving six orders of magnitude improvement in energy efficiency compared to existing distributed consensus systems while enabling continuous learning. This work bridges fundamental physics, mathematical logic, and practical system design, offering both theoretical insights into the nature of learning and computation, and a concrete pathway toward sustainable artificial intelligence."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-05T18:21:02Z",
                "published_parsed": [
                    2026,
                    1,
                    5,
                    18,
                    21,
                    2,
                    0,
                    5,
                    0
                ],
                "arxiv_comment": "19 pages",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Laurent Caraffa"
                    }
                ],
                "author_detail": {
                    "name": "Laurent Caraffa"
                },
                "author": "Laurent Caraffa"
            },
            {
                "id": "http://arxiv.org/abs/2601.02320v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.02320v1",
                "title": "Estimating Text Temperature",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Estimating Text Temperature"
                },
                "updated": "2026-01-05T18:09:41Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    18,
                    9,
                    41,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.02320v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.02320v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Autoregressive language models typically use temperature parameter at inference to shape the probability distribution and control the randomness of the text generated. After the text was generated, this parameter can be estimated using maximum likelihood approach. Following it, we propose a procedure to estimate the temperature of any text, including ones written by humans, with respect to a given language model. We evaluate the temperature estimation capability of a wide selection of small-to-medium LLMs. We then use the best-performing Qwen3 14B to estimate temperatures of popular corpora.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive language models typically use temperature parameter at inference to shape the probability distribution and control the randomness of the text generated. After the text was generated, this parameter can be estimated using maximum likelihood approach. Following it, we propose a procedure to estimate the temperature of any text, including ones written by humans, with respect to a given language model. We evaluate the temperature estimation capability of a wide selection of small-to-medium LLMs. We then use the best-performing Qwen3 14B to estimate temperatures of popular corpora."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-05T18:09:41Z",
                "published_parsed": [
                    2026,
                    1,
                    5,
                    18,
                    9,
                    41,
                    0,
                    5,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Nikolay Mikhaylovskiy"
                    }
                ],
                "author_detail": {
                    "name": "Nikolay Mikhaylovskiy"
                },
                "author": "Nikolay Mikhaylovskiy"
            },
            {
                "id": "http://arxiv.org/abs/2503.18773v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2503.18773v3",
                "title": "BitDecoding: Unlocking Tensor Cores for Long-Context LLMs with Low-Bit KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BitDecoding: Unlocking Tensor Cores for Long-Context LLMs with Low-Bit KV Cache"
                },
                "updated": "2026-01-05T18:08:27Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    18,
                    8,
                    27,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2503.18773v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2503.18773v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The growth of long-context Large Language Models (LLMs) significantly increases memory and bandwidth pressure during autoregressive decoding due to the expanding Key-Value (KV) cache. While accuracy-preserving KV-cache quantization (e.g., 4-bit or 2-bit) reduces memory footprint, existing systems decode inefficiently by relying solely on CUDA cores, underutilizing Tensor Cores-the dominant compute resource on GPUs.\n  We present BitDecoding, the first inference system to efficiently decode low-bit KV caches by cooperatively leveraging CUDA cores and Tensor Cores. BitDecoding smartly induces Tensor-Core-friendly layouts, introduces warp-level dequantization parallelism, and provides unified system support through query transformation, high-performance tensor- and channel-wise quantization, and a software-pipelined dequantization kernel enabling mixed-precision execution. Architecture-aware optimizations further leverage Hopper's warpgroup tensor instructions and Blackwell's NVFP4 (MXFP4) tensor formats.\n  Evaluated on Blackwell, Hopper, and Ampere GPUs, BitDecoding achieves an average 7.5x decoding speedup over FP16 FlashDecoding-v2, up to 8.6x on Blackwell with NVFP4, and up to 4.3x over state-of-the-art approaches. On LLaMA-3.1-8B with a 128K context, BitDecoding reduces single-batch decoding latency by 3x. BitDecoding is open-sourced at https://github.com/OpenBitSys/BitDecoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growth of long-context Large Language Models (LLMs) significantly increases memory and bandwidth pressure during autoregressive decoding due to the expanding Key-Value (KV) cache. While accuracy-preserving KV-cache quantization (e.g., 4-bit or 2-bit) reduces memory footprint, existing systems decode inefficiently by relying solely on CUDA cores, underutilizing Tensor Cores-the dominant compute resource on GPUs.\n  We present BitDecoding, the first inference system to efficiently decode low-bit KV caches by cooperatively leveraging CUDA cores and Tensor Cores. BitDecoding smartly induces Tensor-Core-friendly layouts, introduces warp-level dequantization parallelism, and provides unified system support through query transformation, high-performance tensor- and channel-wise quantization, and a software-pipelined dequantization kernel enabling mixed-precision execution. Architecture-aware optimizations further leverage Hopper's warpgroup tensor instructions and Blackwell's NVFP4 (MXFP4) tensor formats.\n  Evaluated on Blackwell, Hopper, and Ampere GPUs, BitDecoding achieves an average 7.5x decoding speedup over FP16 FlashDecoding-v2, up to 8.6x on Blackwell with NVFP4, and up to 4.3x over state-of-the-art approaches. On LLaMA-3.1-8B with a 128K context, BitDecoding reduces single-batch decoding latency by 3x. BitDecoding is open-sourced at https://github.com/OpenBitSys/BitDecoding."
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-03-24T15:22:41Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    15,
                    22,
                    41,
                    0,
                    83,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR"
                },
                "authors": [
                    {
                        "name": "Dayou Du"
                    },
                    {
                        "name": "Shijie Cao"
                    },
                    {
                        "name": "Jianyi Cheng"
                    },
                    {
                        "name": "Luo Mai"
                    },
                    {
                        "name": "Ting Cao"
                    },
                    {
                        "name": "Mao Yang"
                    }
                ],
                "author_detail": {
                    "name": "Mao Yang"
                },
                "author": "Mao Yang"
            },
            {
                "id": "http://arxiv.org/abs/2601.02314v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.02314v1",
                "title": "Project Ariadne: A Structural Causal Framework for Auditing Faithfulness in LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Project Ariadne: A Structural Causal Framework for Auditing Faithfulness in LLM Agents"
                },
                "updated": "2026-01-05T18:05:29Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    18,
                    5,
                    29,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.02314v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.02314v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "As Large Language Model (LLM) agents are increasingly tasked with high-stakes autonomous decision-making, the transparency of their reasoning processes has become a critical safety concern. While \\textit{Chain-of-Thought} (CoT) prompting allows agents to generate human-readable reasoning traces, it remains unclear whether these traces are \\textbf{faithful} generative drivers of the model's output or merely \\textbf{post-hoc rationalizations}. We introduce \\textbf{Project Ariadne}, a novel XAI framework that utilizes Structural Causal Models (SCMs) and counterfactual logic to audit the causal integrity of agentic reasoning. Unlike existing interpretability methods that rely on surface-level textual similarity, Project Ariadne performs \\textbf{hard interventions} ($do$-calculus) on intermediate reasoning nodes -- systematically inverting logic, negating premises, and reversing factual claims -- to measure the \\textbf{Causal Sensitivity} ($φ$) of the terminal answer. Our empirical evaluation of state-of-the-art models reveals a persistent \\textit{Faithfulness Gap}. We define and detect a widespread failure mode termed \\textbf{Causal Decoupling}, where agents exhibit a violation density ($ρ$) of up to $0.77$ in factual and scientific domains. In these instances, agents arrive at identical conclusions despite contradictory internal logic, proving that their reasoning traces function as \"Reasoning Theater\" while decision-making is governed by latent parametric priors. Our findings suggest that current agentic architectures are inherently prone to unfaithful explanation, and we propose the Ariadne Score as a new benchmark for aligning stated logic with model action.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Model (LLM) agents are increasingly tasked with high-stakes autonomous decision-making, the transparency of their reasoning processes has become a critical safety concern. While \\textit{Chain-of-Thought} (CoT) prompting allows agents to generate human-readable reasoning traces, it remains unclear whether these traces are \\textbf{faithful} generative drivers of the model's output or merely \\textbf{post-hoc rationalizations}. We introduce \\textbf{Project Ariadne}, a novel XAI framework that utilizes Structural Causal Models (SCMs) and counterfactual logic to audit the causal integrity of agentic reasoning. Unlike existing interpretability methods that rely on surface-level textual similarity, Project Ariadne performs \\textbf{hard interventions} ($do$-calculus) on intermediate reasoning nodes -- systematically inverting logic, negating premises, and reversing factual claims -- to measure the \\textbf{Causal Sensitivity} ($φ$) of the terminal answer. Our empirical evaluation of state-of-the-art models reveals a persistent \\textit{Faithfulness Gap}. We define and detect a widespread failure mode termed \\textbf{Causal Decoupling}, where agents exhibit a violation density ($ρ$) of up to $0.77$ in factual and scientific domains. In these instances, agents arrive at identical conclusions despite contradictory internal logic, proving that their reasoning traces function as \"Reasoning Theater\" while decision-making is governed by latent parametric priors. Our findings suggest that current agentic architectures are inherently prone to unfaithful explanation, and we propose the Ariadne Score as a new benchmark for aligning stated logic with model action."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-05T18:05:29Z",
                "published_parsed": [
                    2026,
                    1,
                    5,
                    18,
                    5,
                    29,
                    0,
                    5,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Sourena Khanzadeh"
                    }
                ],
                "author_detail": {
                    "name": "Sourena Khanzadeh"
                },
                "author": "Sourena Khanzadeh"
            },
            {
                "id": "http://arxiv.org/abs/2601.00497v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.00497v2",
                "title": "STELLAR: A Search-Based Testing Framework for Large Language Model Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "STELLAR: A Search-Based Testing Framework for Large Language Model Applications"
                },
                "updated": "2026-01-05T18:03:57Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    18,
                    3,
                    57,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.00497v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.00497v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Model (LLM)-based applications are increasingly deployed across various domains, including customer service, education, and mobility. However, these systems are prone to inaccurate, fictitious, or harmful responses, and their vast, high-dimensional input space makes systematic testing particularly challenging. To address this, we present STELLAR, an automated search-based testing framework for LLM-based applications that systematically uncovers text inputs leading to inappropriate system responses. Our framework models test generation as an optimization problem and discretizes the input space into stylistic, content-related, and perturbation features. Unlike prior work that focuses on prompt optimization or coverage heuristics, our work employs evolutionary optimization to dynamically explore feature combinations that are more likely to expose failures. We evaluate STELLAR on three LLM-based conversational question-answering systems. The first focuses on safety, benchmarking both public and proprietary LLMs against malicious or unsafe prompts. The second and third target navigation, using an open-source and an industrial retrieval-augmented system for in-vehicle venue recommendations. Overall, STELLAR exposes up to 4.3 times (average 2.5 times) more failures than the existing baseline approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM)-based applications are increasingly deployed across various domains, including customer service, education, and mobility. However, these systems are prone to inaccurate, fictitious, or harmful responses, and their vast, high-dimensional input space makes systematic testing particularly challenging. To address this, we present STELLAR, an automated search-based testing framework for LLM-based applications that systematically uncovers text inputs leading to inappropriate system responses. Our framework models test generation as an optimization problem and discretizes the input space into stylistic, content-related, and perturbation features. Unlike prior work that focuses on prompt optimization or coverage heuristics, our work employs evolutionary optimization to dynamically explore feature combinations that are more likely to expose failures. We evaluate STELLAR on three LLM-based conversational question-answering systems. The first focuses on safety, benchmarking both public and proprietary LLMs against malicious or unsafe prompts. The second and third target navigation, using an open-source and an industrial retrieval-augmented system for in-vehicle venue recommendations. Overall, STELLAR exposes up to 4.3 times (average 2.5 times) more failures than the existing baseline approaches."
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-01T22:30:15Z",
                "published_parsed": [
                    2026,
                    1,
                    1,
                    22,
                    30,
                    15,
                    3,
                    1,
                    0
                ],
                "arxiv_comment": "Accepted for publication at the 33th International Conference on Software Analysis, Evolution and Reengineering (SANER 2026)",
                "arxiv_primary_category": {
                    "term": "cs.SE"
                },
                "authors": [
                    {
                        "name": "Lev Sorokin"
                    },
                    {
                        "name": "Ivan Vasilev"
                    },
                    {
                        "name": "Ken E. Friedl"
                    },
                    {
                        "name": "Andrea Stocco"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Stocco"
                },
                "author": "Andrea Stocco"
            },
            {
                "id": "http://arxiv.org/abs/2505.09665v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2505.09665v3",
                "title": "Tales of the 2025 Los Angeles Fire: Hotwash for Public Health Concerns in Reddit via LLM-Enhanced Topic Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tales of the 2025 Los Angeles Fire: Hotwash for Public Health Concerns in Reddit via LLM-Enhanced Topic Modeling"
                },
                "updated": "2026-01-05T18:01:24Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    18,
                    1,
                    24,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2505.09665v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2505.09665v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Wildfires have become increasingly frequent, irregular, and severe in recent years. Understanding how affected populations perceive and respond during wildfire crises is critical for timely and empathetic disaster response. Social media platforms offer a crowd-sourced channel to capture evolving public discourse, providing hyperlocal information and insight into public sentiment. This study analyzes Reddit discourse during the 2025 Los Angeles wildfires, spanning from the onset of the disaster to full containment. We collect 385 posts and 114,879 comments related to the Palisades and Eaton fires. We adopt topic modeling methods to identify the latent topics, enhanced by large language models (LLMs) and human-in-the-loop (HITL) refinement. Furthermore, we develop a hierarchical framework to categorize latent topics, consisting of two main categories, Situational Awareness (SA) and Crisis Narratives (CN). The volume of SA category closely aligns with real-world fire progressions, peaking within the first 2-5 days as the fires reach the maximum extent. The most frequent co-occurring category set of public health and safety, loss and damage, and emergency resources expands on a wide range of health-related latent topics, including environmental health, occupational health, and one health. Grief signals and mental health risks consistently accounted for 60 percentage and 40 percentage of CN instances, respectively, with the highest total volume occurring at night. This study contributes the first annotated social media dataset on the 2025 LA fires, and introduces a scalable multi-layer framework that leverages topic modeling for crisis discourse analysis. By identifying persistent public health concerns, our results can inform more empathetic and adaptive strategies for disaster response, public health communication, and future research in comparable climate-related disaster events.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wildfires have become increasingly frequent, irregular, and severe in recent years. Understanding how affected populations perceive and respond during wildfire crises is critical for timely and empathetic disaster response. Social media platforms offer a crowd-sourced channel to capture evolving public discourse, providing hyperlocal information and insight into public sentiment. This study analyzes Reddit discourse during the 2025 Los Angeles wildfires, spanning from the onset of the disaster to full containment. We collect 385 posts and 114,879 comments related to the Palisades and Eaton fires. We adopt topic modeling methods to identify the latent topics, enhanced by large language models (LLMs) and human-in-the-loop (HITL) refinement. Furthermore, we develop a hierarchical framework to categorize latent topics, consisting of two main categories, Situational Awareness (SA) and Crisis Narratives (CN). The volume of SA category closely aligns with real-world fire progressions, peaking within the first 2-5 days as the fires reach the maximum extent. The most frequent co-occurring category set of public health and safety, loss and damage, and emergency resources expands on a wide range of health-related latent topics, including environmental health, occupational health, and one health. Grief signals and mental health risks consistently accounted for 60 percentage and 40 percentage of CN instances, respectively, with the highest total volume occurring at night. This study contributes the first annotated social media dataset on the 2025 LA fires, and introduces a scalable multi-layer framework that leverages topic modeling for crisis discourse analysis. By identifying persistent public health concerns, our results can inform more empathetic and adaptive strategies for disaster response, public health communication, and future research in comparable climate-related disaster events."
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-05-14T16:31:08Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    16,
                    31,
                    8,
                    2,
                    134,
                    0
                ],
                "arxiv_comment": "Fix typos in Method Section. Add data/code availability",
                "arxiv_primary_category": {
                    "term": "cs.SI"
                },
                "authors": [
                    {
                        "name": "Sulong Zhou"
                    },
                    {
                        "name": "Qunying Huang"
                    },
                    {
                        "name": "Shaoheng Zhou"
                    },
                    {
                        "name": "Yun Hang"
                    },
                    {
                        "name": "Xinyue Ye"
                    },
                    {
                        "name": "Aodong Mei"
                    },
                    {
                        "name": "Kathryn Phung"
                    },
                    {
                        "name": "Yuning Ye"
                    },
                    {
                        "name": "Uma Govindswamy"
                    },
                    {
                        "name": "Zehan Li"
                    }
                ],
                "author_detail": {
                    "name": "Zehan Li"
                },
                "author": "Zehan Li"
            },
            {
                "id": "http://arxiv.org/abs/2411.06254v4",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2411.06254v4",
                "title": "Adaptive Evidence Budgeting for Scalable Long-Document Reranking with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Evidence Budgeting for Scalable Long-Document Reranking with LLMs"
                },
                "updated": "2026-01-05T17:50:15Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    17,
                    50,
                    15,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2411.06254v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2411.06254v4",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Decoder-only LLM rerankers are powerful but often struggle with long documents: inference is costly and relevance signals can be diluted as irrelevant text accumulates in the context window. Motivated by an attention analysis showing that relevance-aligned heads degrade when non-relevant text is appended, we propose EviRerank, a scalable framework that (i) scores document blocks with a lightweight selector (BM25, bi-encoder, or cross-encoder), (ii) constructs a compact evidence context under a strict token budget, and (iii) reranks with a decoder-only LLM. Our key contribution is Adaptive Evidence Budgeting (AEB), an information-density-aware dynamic stopping strategy that avoids low-utility tail blocks, and we further study Summary Augmentation (SA) within the same budget. Across TREC DL'19, DL'23, and MLDR-zh, EviRerank consistently improves over full-document LLM reranking and strong block-selection baselines while substantially reducing the required input length. On TREC DL'19, EviRerank achieves 0.743 nDCG@10 and 0.307 MAP, improving over RankLLaMA (0.701/0.288) by +0.042 nDCG@10 (+6.0%) and +0.019 MAP (+6.6%).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decoder-only LLM rerankers are powerful but often struggle with long documents: inference is costly and relevance signals can be diluted as irrelevant text accumulates in the context window. Motivated by an attention analysis showing that relevance-aligned heads degrade when non-relevant text is appended, we propose EviRerank, a scalable framework that (i) scores document blocks with a lightweight selector (BM25, bi-encoder, or cross-encoder), (ii) constructs a compact evidence context under a strict token budget, and (iii) reranks with a decoder-only LLM. Our key contribution is Adaptive Evidence Budgeting (AEB), an information-density-aware dynamic stopping strategy that avoids low-utility tail blocks, and we further study Summary Augmentation (SA) within the same budget. Across TREC DL'19, DL'23, and MLDR-zh, EviRerank consistently improves over full-document LLM reranking and strong block-selection baselines while substantially reducing the required input length. On TREC DL'19, EviRerank achieves 0.743 nDCG@10 and 0.307 MAP, improving over RankLLaMA (0.701/0.288) by +0.042 nDCG@10 (+6.0%) and +0.019 MAP (+6.6%)."
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2024-11-09T19:03:56Z",
                "published_parsed": [
                    2024,
                    11,
                    9,
                    19,
                    3,
                    56,
                    5,
                    314,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR"
                },
                "authors": [
                    {
                        "name": "Minghan Li"
                    },
                    {
                        "name": "Eric Gaussier"
                    },
                    {
                        "name": "Juntao Li"
                    },
                    {
                        "name": "Guodong Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Guodong Zhou"
                },
                "author": "Guodong Zhou"
            },
            {
                "id": "http://arxiv.org/abs/2601.02305v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.02305v1",
                "title": "On Statistical Inference for Rates of Change in Spatial Processes over Riemannian Manifolds",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Statistical Inference for Rates of Change in Spatial Processes over Riemannian Manifolds"
                },
                "updated": "2026-01-05T17:47:24Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    17,
                    47,
                    24,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.02305v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.02305v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Statistical inference for spatial processes from partially realized or scattered data has seen voluminous developments in diverse areas ranging from environmental sciences to business and economics. Inference on the associated rates of change has seen some recent developments. The literature has been restricted to Euclidean domains, where inference is sought on directional derivatives, rates along a chosen direction of interest, at arbitrary locations. Inference for higher order rates, particularly directional curvature has also proved useful in these settings. Modern spatial data often arise from non-Euclidean domains. This manuscript particularly considers spatial processes defined over compact Riemannian manifolds. We develop a comprehensive inferential framework for spatial rates of change for such processes over vector fields. In doing so, we formalize smoothness of process realizations and construct differential processes -- the derivative and curvature processes. We derive conditions for kernels that ensure the existence of these processes and establish validity of the joint multivariate process consisting of the ``parent'' Gaussian process (GP) over the manifold and the associated differential processes. Predictive inference on these rates is devised conditioned on the realized process over the manifold. Manifolds arise as polyhedral meshes in practice. The success of our simulation experiments for assessing derivatives for processes observed over such meshes validate our theoretical findings. By enhancing our understanding of GPs on manifolds, this manuscript unlocks a variety of potential applications in machine learning and statistics where GPs have seen wide usage. We propose a fully model-based approach to inference on the differential processes arising from a spatial process from partially observed or realized data across scattered location on a manifold.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Statistical inference for spatial processes from partially realized or scattered data has seen voluminous developments in diverse areas ranging from environmental sciences to business and economics. Inference on the associated rates of change has seen some recent developments. The literature has been restricted to Euclidean domains, where inference is sought on directional derivatives, rates along a chosen direction of interest, at arbitrary locations. Inference for higher order rates, particularly directional curvature has also proved useful in these settings. Modern spatial data often arise from non-Euclidean domains. This manuscript particularly considers spatial processes defined over compact Riemannian manifolds. We develop a comprehensive inferential framework for spatial rates of change for such processes over vector fields. In doing so, we formalize smoothness of process realizations and construct differential processes -- the derivative and curvature processes. We derive conditions for kernels that ensure the existence of these processes and establish validity of the joint multivariate process consisting of the ``parent'' Gaussian process (GP) over the manifold and the associated differential processes. Predictive inference on these rates is devised conditioned on the realized process over the manifold. Manifolds arise as polyhedral meshes in practice. The success of our simulation experiments for assessing derivatives for processes observed over such meshes validate our theoretical findings. By enhancing our understanding of GPs on manifolds, this manuscript unlocks a variety of potential applications in machine learning and statistics where GPs have seen wide usage. We propose a fully model-based approach to inference on the differential processes arising from a spatial process from partially observed or realized data across scattered location on a manifold."
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.DG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-05T17:47:24Z",
                "published_parsed": [
                    2026,
                    1,
                    5,
                    17,
                    47,
                    24,
                    0,
                    5,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "math.ST"
                },
                "authors": [
                    {
                        "name": "Didong Li"
                    },
                    {
                        "name": "Aritra Halder"
                    },
                    {
                        "name": "Sudipto Banerjee"
                    }
                ],
                "author_detail": {
                    "name": "Sudipto Banerjee"
                },
                "author": "Sudipto Banerjee"
            },
            {
                "id": "http://arxiv.org/abs/2601.02304v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.02304v1",
                "title": "Octopus: A Lightweight Entity-Aware System for Multi-Table Data Discovery and Cell-Level Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Octopus: A Lightweight Entity-Aware System for Multi-Table Data Discovery and Cell-Level Retrieval"
                },
                "updated": "2026-01-05T17:43:49Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    17,
                    43,
                    49,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.02304v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.02304v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Tabular data constitute a dominant form of information in modern data lakes and repositories, yet discovering the relevant tables to answer user questions remains challenging. Existing data discovery systems assume that each question can be answered by a single table and often rely on resource-intensive offline preprocessing, such as model training or large-scale content indexing. In practice, however, many questions require information spread across multiple tables -- either independently or through joins -- and users often seek specific cell values rather than entire tables. In this paper, we present Octopus, a lightweight, entity-aware, and training-free system for multi-table data discovery and cell-level value retrieval. Instead of embedding entire questions, Octopus identifies fine-grained entities (column mentions and value mentions) from natural-language queries using an LLM parser. It then matches these entities to table headers through a compact embedding index and scans table contents directly for value occurrences, eliminating the need for heavy content indexing or costly offline stages. The resulting fine-grained alignment not only improves table retrieval accuracy but also facilitates efficient downstream NL2SQL execution by reducing token usage and redundant LLM calls. To evaluate Octopus, we introduce a new benchmark covering both table- and cell-level discovery under multi-table settings, including five datasets for independent discovery and two for join-based discovery. Experimental results show that Octopus consistently outperforms existing systems while achieving substantially lower computational and token costs. Code is available at https://github.com/wenzhilics/octopus.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tabular data constitute a dominant form of information in modern data lakes and repositories, yet discovering the relevant tables to answer user questions remains challenging. Existing data discovery systems assume that each question can be answered by a single table and often rely on resource-intensive offline preprocessing, such as model training or large-scale content indexing. In practice, however, many questions require information spread across multiple tables -- either independently or through joins -- and users often seek specific cell values rather than entire tables. In this paper, we present Octopus, a lightweight, entity-aware, and training-free system for multi-table data discovery and cell-level value retrieval. Instead of embedding entire questions, Octopus identifies fine-grained entities (column mentions and value mentions) from natural-language queries using an LLM parser. It then matches these entities to table headers through a compact embedding index and scans table contents directly for value occurrences, eliminating the need for heavy content indexing or costly offline stages. The resulting fine-grained alignment not only improves table retrieval accuracy but also facilitates efficient downstream NL2SQL execution by reducing token usage and redundant LLM calls. To evaluate Octopus, we introduce a new benchmark covering both table- and cell-level discovery under multi-table settings, including five datasets for independent discovery and two for join-based discovery. Experimental results show that Octopus consistently outperforms existing systems while achieving substantially lower computational and token costs. Code is available at https://github.com/wenzhilics/octopus."
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-05T17:43:49Z",
                "published_parsed": [
                    2026,
                    1,
                    5,
                    17,
                    43,
                    49,
                    0,
                    5,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB"
                },
                "authors": [
                    {
                        "name": "Wen-Zhi Li"
                    },
                    {
                        "name": "Sainyam Galhotra"
                    }
                ],
                "author_detail": {
                    "name": "Sainyam Galhotra"
                },
                "author": "Sainyam Galhotra"
            },
            {
                "id": "http://arxiv.org/abs/2511.04847v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.04847v3",
                "title": "Grounded Test-Time Adaptation for LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Grounded Test-Time Adaptation for LLM Agents"
                },
                "updated": "2026-01-05T17:43:48Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    17,
                    43,
                    48,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.04847v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.04847v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language model (LLM)-based agents struggle to generalize to novel and complex environments, such as unseen websites or new sets of functions, due to a fundamental mismatch between their pre-training and test-time conditions. This challenge stems from two distinct failure modes: a syntactic misunderstanding of environment-specific components like observation formats, and a semantic misunderstanding of state-transition dynamics, which are only revealed at test time. To address these issues, we propose two distinct and complementary strategies for adapting LLM agents by leveraging environment-specific information available during deployment. First, an online distributional adaptation method parameterizes environmental nuances by learning a lightweight adaptation vector that biases the model's output distribution, enabling rapid alignment with an environment response format. Second, a deployment-time dynamics grounding method employs a persona-driven exploration phase to systematically probe and learn the environment's causal dynamics before task execution, equipping the agent with a nonparametric world model. We evaluate these strategies across diverse agentic benchmarks, including function calling and web navigation. Our empirical results show the effectiveness of both strategies across all benchmarks with minimal computational cost. We find that dynamics grounding is particularly effective in complex environments where unpredictable dynamics pose a major obstacle, demonstrating a robust path toward more generalizable and capable LLM-based agents. For example, on the WebArena multi-site split, this method increases the agent's success rate from 2% to 23%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM)-based agents struggle to generalize to novel and complex environments, such as unseen websites or new sets of functions, due to a fundamental mismatch between their pre-training and test-time conditions. This challenge stems from two distinct failure modes: a syntactic misunderstanding of environment-specific components like observation formats, and a semantic misunderstanding of state-transition dynamics, which are only revealed at test time. To address these issues, we propose two distinct and complementary strategies for adapting LLM agents by leveraging environment-specific information available during deployment. First, an online distributional adaptation method parameterizes environmental nuances by learning a lightweight adaptation vector that biases the model's output distribution, enabling rapid alignment with an environment response format. Second, a deployment-time dynamics grounding method employs a persona-driven exploration phase to systematically probe and learn the environment's causal dynamics before task execution, equipping the agent with a nonparametric world model. We evaluate these strategies across diverse agentic benchmarks, including function calling and web navigation. Our empirical results show the effectiveness of both strategies across all benchmarks with minimal computational cost. We find that dynamics grounding is particularly effective in complex environments where unpredictable dynamics pose a major obstacle, demonstrating a robust path toward more generalizable and capable LLM-based agents. For example, on the WebArena multi-site split, this method increases the agent's success rate from 2% to 23%."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-06T22:24:35Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    22,
                    24,
                    35,
                    3,
                    310,
                    0
                ],
                "arxiv_comment": "Our code is available here: https://github.com/r2llab/GTTA",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Arthur Chen"
                    },
                    {
                        "name": "Zuxin Liu"
                    },
                    {
                        "name": "Jianguo Zhang"
                    },
                    {
                        "name": "Akshara Prabhakar"
                    },
                    {
                        "name": "Zhiwei Liu"
                    },
                    {
                        "name": "Shelby Heinecke"
                    },
                    {
                        "name": "Silvio Savarese"
                    },
                    {
                        "name": "Victor Zhong"
                    },
                    {
                        "name": "Caiming Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Caiming Xiong"
                },
                "author": "Caiming Xiong"
            },
            {
                "id": "http://arxiv.org/abs/2511.06057v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.06057v2",
                "title": "MIND Your Reasoning: A Meta-Cognitive Intuitive-Reflective Network for Dual-Reasoning in Multimodal Stance Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MIND Your Reasoning: A Meta-Cognitive Intuitive-Reflective Network for Dual-Reasoning in Multimodal Stance Detection"
                },
                "updated": "2026-01-05T17:33:44Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    17,
                    33,
                    44,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.06057v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.06057v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Multimodal Stance Detection (MSD) is a crucial task for understanding public opinion on social media. Existing methods predominantly operate by learning to fuse modalities. They lack an explicit reasoning process to discern how inter-modal dynamics, such as irony or conflict, collectively shape the user's final stance, leading to frequent misjudgments. To address this, we advocate for a paradigm shift from *learning to fuse* to *learning to reason*. We introduce **MIND**, a **M**eta-cognitive **I**ntuitive-reflective **N**etwork for **D**ual-reasoning. Inspired by the dual-process theory of human cognition, MIND operationalizes a self-improving loop. It first generates a rapid, intuitive hypothesis by querying evolving Modality and Semantic Experience Pools. Subsequently, a meta-cognitive reflective stage uses Modality-CoT and Semantic-CoT to scrutinize this initial judgment, distill superior adaptive strategies, and evolve the experience pools themselves. These dual experience structures are continuously refined during training and recalled at inference to guide robust and context-aware stance decisions. Extensive experiments on the MMSD benchmark demonstrate that our MIND significantly outperforms most baseline models and exhibits strong generalization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Stance Detection (MSD) is a crucial task for understanding public opinion on social media. Existing methods predominantly operate by learning to fuse modalities. They lack an explicit reasoning process to discern how inter-modal dynamics, such as irony or conflict, collectively shape the user's final stance, leading to frequent misjudgments. To address this, we advocate for a paradigm shift from *learning to fuse* to *learning to reason*. We introduce **MIND**, a **M**eta-cognitive **I**ntuitive-reflective **N**etwork for **D**ual-reasoning. Inspired by the dual-process theory of human cognition, MIND operationalizes a self-improving loop. It first generates a rapid, intuitive hypothesis by querying evolving Modality and Semantic Experience Pools. Subsequently, a meta-cognitive reflective stage uses Modality-CoT and Semantic-CoT to scrutinize this initial judgment, distill superior adaptive strategies, and evolve the experience pools themselves. These dual experience structures are continuously refined during training and recalled at inference to guide robust and context-aware stance decisions. Extensive experiments on the MMSD benchmark demonstrate that our MIND significantly outperforms most baseline models and exhibits strong generalization."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-08T15:56:24Z",
                "published_parsed": [
                    2025,
                    11,
                    8,
                    15,
                    56,
                    24,
                    5,
                    312,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Bingbing Wang"
                    },
                    {
                        "name": "Zhengda Jin"
                    },
                    {
                        "name": "Bin Liang"
                    },
                    {
                        "name": "Wenjie Li"
                    },
                    {
                        "name": "Jing Li"
                    },
                    {
                        "name": "Ruifeng Xu"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang"
            },
            {
                "id": "http://arxiv.org/abs/2510.06478v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.06478v2",
                "title": "Anytime-Valid Answer Sufficiency Certificates for LLM Generation via Sequential Information Lift",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Anytime-Valid Answer Sufficiency Certificates for LLM Generation via Sequential Information Lift"
                },
                "updated": "2026-01-05T17:33:34Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    17,
                    33,
                    34,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.06478v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.06478v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We introduce Sequential-EDFL (Empirical Dynamic Formal Lift), which applies anytime-valid sequential testing to language model generation stopping. Our approach tracks information lift, defined as the log-likelihood ratio between the full model and deliberately weakened \"skeleton\" baselines, using self-normalized empirical-Bernstein e-processes that provide formal delta-level error control regardless of stopping time. This delta guarantee controls premature stopping when information lift is insufficient relative to the skeleton, and it does not imply delta control of factual incorrectness or hallucinations. We handle unknown centering through online mean estimation, combine multiple parameters via mixture e-processes, and support adaptive resets under distributional drift. On six benchmarks, Sequential-EDFL reduces generation length by 22 to 28 percent relative to sequential baselines while maintaining delta-level control with 12 percent computational overhead. We introduce automated skeletons (distilled submodels and randomized logits) and show robustness across skeleton families. Composing EDFL with a lightweight correctness gate (sentence boundaries plus a verifier) improves end-task correctness while preserving anytime-valid guarantees by only delaying stopping. Our certificates control information sufficiency, not factual correctness. Specifically, 10.9 percent of stopped sequences remain incorrect even with the gate (13.2 to 22.7 percent without it). EDFL serves as a first-stage filter that can reduce verification burden: when applied to stopped sequences, the gate validates 83 percent of stops, requiring full verification only for the remaining 17 percent, plus all non-stopped sequences. EDFL is not a standalone solution for safety-critical domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Sequential-EDFL (Empirical Dynamic Formal Lift), which applies anytime-valid sequential testing to language model generation stopping. Our approach tracks information lift, defined as the log-likelihood ratio between the full model and deliberately weakened \"skeleton\" baselines, using self-normalized empirical-Bernstein e-processes that provide formal delta-level error control regardless of stopping time. This delta guarantee controls premature stopping when information lift is insufficient relative to the skeleton, and it does not imply delta control of factual incorrectness or hallucinations. We handle unknown centering through online mean estimation, combine multiple parameters via mixture e-processes, and support adaptive resets under distributional drift. On six benchmarks, Sequential-EDFL reduces generation length by 22 to 28 percent relative to sequential baselines while maintaining delta-level control with 12 percent computational overhead. We introduce automated skeletons (distilled submodels and randomized logits) and show robustness across skeleton families. Composing EDFL with a lightweight correctness gate (sentence boundaries plus a verifier) improves end-task correctness while preserving anytime-valid guarantees by only delaying stopping. Our certificates control information sufficiency, not factual correctness. Specifically, 10.9 percent of stopped sequences remain incorrect even with the gate (13.2 to 22.7 percent without it). EDFL serves as a first-stage filter that can reduce verification burden: when applied to stopped sequences, the gate validates 83 percent of stops, requiring full verification only for the remaining 17 percent, plus all non-stopped sequences. EDFL is not a standalone solution for safety-critical domains."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-07T21:28:53Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    21,
                    28,
                    53,
                    1,
                    280,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Sanjeda Akter"
                    },
                    {
                        "name": "Ibne Farabi Shihab"
                    },
                    {
                        "name": "Anuj Sharma"
                    }
                ],
                "author_detail": {
                    "name": "Anuj Sharma"
                },
                "author": "Anuj Sharma"
            },
            {
                "id": "http://arxiv.org/abs/2601.02298v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.02298v1",
                "title": "Power-of-Two Quantization-Aware-Training (PoT-QAT) in Large Language Models (LLMs)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Power-of-Two Quantization-Aware-Training (PoT-QAT) in Large Language Models (LLMs)"
                },
                "updated": "2026-01-05T17:33:16Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    17,
                    33,
                    16,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.02298v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.02298v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "In Large Language Models (LLMs), the number of parameters has grown exponentially in the past few years, e.g., from 1.5 billion parameters in GPT-2 to 175 billion in GPT-3 to possibly more than trillion in higher versions. This raises a significant challenge for implementation, especially for Edge devices. Unlike cloud computing, memory and processing power for Edge devices are very limited, which necessitates developing novel ideas to make such applications feasible. In this work, we investigate compressing weights with a special quantization that limits numbers to only power-of-two (PoT). This helps save a huge amount of memory as only exponents need to be stored, more importantly, it significantly reduces processing power by replacing costly multiplication with low cost bit shifting. To overcome performance loss due to this strict quantization, we investigate Quantization Aware Training (QAT) to enhance performance through additional training. Results on GPT-2 124M show a major enhancement for quantized PoT model after additional training, with a perplexity enhancement of 66% and BERT-Score loss to baseline GPT-2 of 1%. The memory saving is estimated to be 87.5% while the inference speed is expected to be 3-10x faster with PoT quantization versus full-precision.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In Large Language Models (LLMs), the number of parameters has grown exponentially in the past few years, e.g., from 1.5 billion parameters in GPT-2 to 175 billion in GPT-3 to possibly more than trillion in higher versions. This raises a significant challenge for implementation, especially for Edge devices. Unlike cloud computing, memory and processing power for Edge devices are very limited, which necessitates developing novel ideas to make such applications feasible. In this work, we investigate compressing weights with a special quantization that limits numbers to only power-of-two (PoT). This helps save a huge amount of memory as only exponents need to be stored, more importantly, it significantly reduces processing power by replacing costly multiplication with low cost bit shifting. To overcome performance loss due to this strict quantization, we investigate Quantization Aware Training (QAT) to enhance performance through additional training. Results on GPT-2 124M show a major enhancement for quantized PoT model after additional training, with a perplexity enhancement of 66% and BERT-Score loss to baseline GPT-2 of 1%. The memory saving is estimated to be 87.5% while the inference speed is expected to be 3-10x faster with PoT quantization versus full-precision."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-05T17:33:16Z",
                "published_parsed": [
                    2026,
                    1,
                    5,
                    17,
                    33,
                    16,
                    0,
                    5,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Mahmoud Elgenedy"
                    }
                ],
                "author_detail": {
                    "name": "Mahmoud Elgenedy"
                },
                "author": "Mahmoud Elgenedy"
            },
            {
                "id": "http://arxiv.org/abs/2601.02297v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.02297v1",
                "title": "The Polarization and Magnetic Field of the Radio Arc as Observed by ALMA at 100 GHz",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Polarization and Magnetic Field of the Radio Arc as Observed by ALMA at 100 GHz"
                },
                "updated": "2026-01-05T17:32:18Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    17,
                    32,
                    18,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.02297v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.02297v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The unique Galactic Center non-thermal filaments (NTFs) have been a focus of investigations for over 40 years. The most prominent manifestation of the NTFs is a bundle of parallel filaments known as the Radio Arc. Radio polarimetric observations made with the Very Large Array (VLA) at 10 GHz have revealed an alternating magnetic field pattern in the Radio Arc that could either be a result of multiple field systems being encountered along the line of sight or an intrinsic feature of the Radio Arc. These VLA observations were not able to distinguish between these possibilities due to the large rotation measures encountered towards the source. We present ALMA 100 GHz observations of the Radio Arc that are not impacted by significant Faraday effects. The observations reported here represent both the first time that ALMA has been used to study the NTFs and the first time 100 GHz polarimetric observations have been conducted on the Radio Arc. We find a uniformly rotated magnetic field with respect to the NTF filament orientation, with the angle of rotation being constant along the length of each filament. However, we find a systematically different magnetic field orientation in different Radio Arc filaments. We use this field pattern to update our understanding of the line-of-sight structures local to the Radio Arc. We find that the magnetic field inferred from our ALMA observations is likely a result either of confusion from multiple magnetic field systems or because the polarization is centrally concentrated within the NTF filaments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The unique Galactic Center non-thermal filaments (NTFs) have been a focus of investigations for over 40 years. The most prominent manifestation of the NTFs is a bundle of parallel filaments known as the Radio Arc. Radio polarimetric observations made with the Very Large Array (VLA) at 10 GHz have revealed an alternating magnetic field pattern in the Radio Arc that could either be a result of multiple field systems being encountered along the line of sight or an intrinsic feature of the Radio Arc. These VLA observations were not able to distinguish between these possibilities due to the large rotation measures encountered towards the source. We present ALMA 100 GHz observations of the Radio Arc that are not impacted by significant Faraday effects. The observations reported here represent both the first time that ALMA has been used to study the NTFs and the first time 100 GHz polarimetric observations have been conducted on the Radio Arc. We find a uniformly rotated magnetic field with respect to the NTF filament orientation, with the angle of rotation being constant along the length of each filament. However, we find a systematically different magnetic field orientation in different Radio Arc filaments. We use this field pattern to update our understanding of the line-of-sight structures local to the Radio Arc. We find that the magnetic field inferred from our ALMA observations is likely a result either of confusion from multiple magnetic field systems or because the polarization is centrally concentrated within the NTF filaments."
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-05T17:32:18Z",
                "published_parsed": [
                    2026,
                    1,
                    5,
                    17,
                    32,
                    18,
                    0,
                    5,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA"
                },
                "authors": [
                    {
                        "name": "Nora Salem"
                    },
                    {
                        "name": "Dylan M. Paré"
                    },
                    {
                        "name": "Paulo Cortes"
                    },
                    {
                        "name": "Mark R. Morris"
                    },
                    {
                        "name": "Valentin J. M. Le Gouellec"
                    }
                ],
                "author_detail": {
                    "name": "Valentin J. M. Le Gouellec"
                },
                "author": "Valentin J. M. Le Gouellec"
            },
            {
                "id": "http://arxiv.org/abs/2601.02292v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.02292v1",
                "title": "A neighbour selection approach for identifying differential networks in conditional functional graphical models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A neighbour selection approach for identifying differential networks in conditional functional graphical models"
                },
                "updated": "2026-01-05T17:28:56Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    17,
                    28,
                    56,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.02292v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.02292v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Estimation of brain functional connectivity from EEG data is of great importance both for medical research and diagnosis. It involves quantifying the conditional dependencies among the activity of different brain areas from the time-varying electric field recorded by sensors placed outside the scalp. These dependencies may vary within and across individuals and be influenced by covariates such as age, mental status, or disease severity. Motivated by this problem, we propose a novel neighbour selection approach based on functional-on-functional regression for the characterization of conditional Gaussian functional graphical models. We provide a fully automated, data-driven procedure for inferring conditional dependence structures among observed functional variables. In particular, pairwise interactions are directly identified and allowed to vary as a function of covariates, enabling covariate-specific modulation of connectivity patterns. Our proposed method accommodates an arbitrary number of continuous and discrete covariates. Moreover, unlike existing methods for direct estimation of differential graphical models, the proposed approach yields directly interpretable coefficients, allowing discrimination between covariate-induced increases and decreases in interaction strength. The methodology is evaluated through extensive simulation studies and an application to experimental EEG data. The results demonstrate clear advantages over existing approaches, including higher estimation accuracy and substantially reduced computational cost, especially in high-dimensional settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Estimation of brain functional connectivity from EEG data is of great importance both for medical research and diagnosis. It involves quantifying the conditional dependencies among the activity of different brain areas from the time-varying electric field recorded by sensors placed outside the scalp. These dependencies may vary within and across individuals and be influenced by covariates such as age, mental status, or disease severity. Motivated by this problem, we propose a novel neighbour selection approach based on functional-on-functional regression for the characterization of conditional Gaussian functional graphical models. We provide a fully automated, data-driven procedure for inferring conditional dependence structures among observed functional variables. In particular, pairwise interactions are directly identified and allowed to vary as a function of covariates, enabling covariate-specific modulation of connectivity patterns. Our proposed method accommodates an arbitrary number of continuous and discrete covariates. Moreover, unlike existing methods for direct estimation of differential graphical models, the proposed approach yields directly interpretable coefficients, allowing discrimination between covariate-induced increases and decreases in interaction strength. The methodology is evaluated through extensive simulation studies and an application to experimental EEG data. The results demonstrate clear advantages over existing approaches, including higher estimation accuracy and substantially reduced computational cost, especially in high-dimensional settings."
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-05T17:28:56Z",
                "published_parsed": [
                    2026,
                    1,
                    5,
                    17,
                    28,
                    56,
                    0,
                    5,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME"
                },
                "authors": [
                    {
                        "name": "Alessia Mapelli"
                    },
                    {
                        "name": "Laura Carini"
                    },
                    {
                        "name": "Francesca Ieva"
                    },
                    {
                        "name": "Sara Sommariva"
                    }
                ],
                "author_detail": {
                    "name": "Sara Sommariva"
                },
                "author": "Sara Sommariva"
            },
            {
                "id": "http://arxiv.org/abs/2509.01544v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2509.01544v3",
                "title": "Causal Consistency Regularization: Training Verifiably Sensitive Reasoning in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal Consistency Regularization: Training Verifiably Sensitive Reasoning in Large Language Models"
                },
                "updated": "2026-01-05T17:24:02Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    17,
                    24,
                    2,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2509.01544v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2509.01544v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models can produce correct answers while relying on flawed reasoning traces, partly because common training objectives reward final-answer correctness rather than faithful intermediate reasoning. This undermines trustworthiness in high-stakes settings. We propose Counterfactual Sensitivity Regularization (CSR), a training paradigm that improves reasoning faithfulness by enforcing causal consistency between reasoning steps and outcomes. CSR automatically applies operator-level interventions to reasoning traces, such as swapping \"+\" with \"-\", to generate minimally perturbed counterfactual rationales, and penalizes the model when these logically invalid traces still lead to the original answer. Our implementation is efficient, adding about 9 percent training overhead via a warm-start curriculum and token-subset optimization.\n  We evaluate faithfulness using Counterfactual Outcome Sensitivity (COS), which measures how appropriately answers change under logical perturbations. Across arithmetic (GSM8K), logical deduction (ProofWriter), multi-hop question answering (HotpotQA), and code generation (MBPP), CSR yields improved accuracy versus faithfulness trade-offs, establishing a new Pareto frontier. CSR improves faithfulness over standard fine-tuning and process supervision by up to 70 percentage points, and transfers across model families with 94.2 to 96.7 percent success in structured domains. CSR also complements inference-time methods such as self-consistency. Overall, CSR offers a practical route to more reliable reasoning in structured domains, including mathematics, formal logic, and code, where operators are well-defined and verifiable, covering an estimated 40 to 60 percent of high-stakes reasoning deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models can produce correct answers while relying on flawed reasoning traces, partly because common training objectives reward final-answer correctness rather than faithful intermediate reasoning. This undermines trustworthiness in high-stakes settings. We propose Counterfactual Sensitivity Regularization (CSR), a training paradigm that improves reasoning faithfulness by enforcing causal consistency between reasoning steps and outcomes. CSR automatically applies operator-level interventions to reasoning traces, such as swapping \"+\" with \"-\", to generate minimally perturbed counterfactual rationales, and penalizes the model when these logically invalid traces still lead to the original answer. Our implementation is efficient, adding about 9 percent training overhead via a warm-start curriculum and token-subset optimization.\n  We evaluate faithfulness using Counterfactual Outcome Sensitivity (COS), which measures how appropriately answers change under logical perturbations. Across arithmetic (GSM8K), logical deduction (ProofWriter), multi-hop question answering (HotpotQA), and code generation (MBPP), CSR yields improved accuracy versus faithfulness trade-offs, establishing a new Pareto frontier. CSR improves faithfulness over standard fine-tuning and process supervision by up to 70 percentage points, and transfers across model families with 94.2 to 96.7 percent success in structured domains. CSR also complements inference-time methods such as self-consistency. Overall, CSR offers a practical route to more reliable reasoning in structured domains, including mathematics, formal logic, and code, where operators are well-defined and verifiable, covering an estimated 40 to 60 percent of high-stakes reasoning deployments."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-09-01T15:18:46Z",
                "published_parsed": [
                    2025,
                    9,
                    1,
                    15,
                    18,
                    46,
                    0,
                    244,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Sanjeda Akter"
                    },
                    {
                        "name": "Ibne Farabi Shihab"
                    },
                    {
                        "name": "Anuj Sharma"
                    }
                ],
                "author_detail": {
                    "name": "Anuj Sharma"
                },
                "author": "Anuj Sharma"
            },
            {
                "id": "http://arxiv.org/abs/2601.02285v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.02285v1",
                "title": "pdfQA: Diverse, Challenging, and Realistic Question Answering over PDFs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "pdfQA: Diverse, Challenging, and Realistic Question Answering over PDFs"
                },
                "updated": "2026-01-05T17:15:26Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    17,
                    15,
                    26,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.02285v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.02285v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "PDFs are the second-most used document type on the internet (after HTML). Yet, existing QA datasets commonly start from text sources or only address specific domains. In this paper, we present pdfQA, a multi-domain 2K human-annotated (real-pdfQA) and 2K synthetic dataset (syn-pdfQA) differentiating QA pairs in ten complexity dimensions (e.g., file type, source modality, source position, answer type). We apply and evaluate quality and difficulty filters on both datasets, obtaining valid and challenging QA pairs. We answer the questions with open-source LLMs, revealing existing challenges that correlate with our complexity dimensions. pdfQA presents a basis for end-to-end QA pipeline evaluation, testing diverse skill sets and local optimizations (e.g., in information retrieval or parsing).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PDFs are the second-most used document type on the internet (after HTML). Yet, existing QA datasets commonly start from text sources or only address specific domains. In this paper, we present pdfQA, a multi-domain 2K human-annotated (real-pdfQA) and 2K synthetic dataset (syn-pdfQA) differentiating QA pairs in ten complexity dimensions (e.g., file type, source modality, source position, answer type). We apply and evaluate quality and difficulty filters on both datasets, obtaining valid and challenging QA pairs. We answer the questions with open-source LLMs, revealing existing challenges that correlate with our complexity dimensions. pdfQA presents a basis for end-to-end QA pipeline evaluation, testing diverse skill sets and local optimizations (e.g., in information retrieval or parsing)."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-05T17:15:26Z",
                "published_parsed": [
                    2026,
                    1,
                    5,
                    17,
                    15,
                    26,
                    0,
                    5,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Tobias Schimanski"
                    },
                    {
                        "name": "Imene Kolli"
                    },
                    {
                        "name": "Jingwei Ni"
                    },
                    {
                        "name": "Yu Fan"
                    },
                    {
                        "name": "Ario Saeid Vaghefi"
                    },
                    {
                        "name": "Elliott Ash"
                    },
                    {
                        "name": "Markus Leippold"
                    }
                ],
                "author_detail": {
                    "name": "Markus Leippold"
                },
                "author": "Markus Leippold"
            },
            {
                "id": "http://arxiv.org/abs/2601.00469v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.00469v2",
                "title": "DSL or Code? Evaluating the Quality of LLM-Generated Algebraic Specifications: A Case Study in Optimization at Kinaxis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DSL or Code? Evaluating the Quality of LLM-Generated Algebraic Specifications: A Case Study in Optimization at Kinaxis"
                },
                "updated": "2026-01-05T17:09:37Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    17,
                    9,
                    37,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.00469v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.00469v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.1145/3786583.3786879",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "Model-driven engineering (MDE) provides abstraction and analytical rigour, but industrial adoption in many domains has been limited by the cost of developing and maintaining models. Large language models (LLMs) can help shift this cost balance by supporting direct generation of models from natural-language (NL) descriptions. For domain-specific languages (DSLs), however, LLM-generated models may be less accurate than LLM-generated code in mainstream languages such as Python, due to the latter's dominance in LLM training corpora. We investigate this issue in mathematical optimization, with AMPL, a DSL with established industrial use. We introduce EXEOS, an LLM-based approach that derives AMPL models and Python code from NL problem descriptions and iteratively refines them with solver feedback. Using a public optimization dataset and real-world supply-chain cases from our industrial partner Kinaxis, we evaluate generated AMPL models against Python code in terms of executability and correctness. An ablation study with two LLM families shows that AMPL is competitive with, and sometimes better than, Python, and that our design choices in EXEOS improve the quality of generated specifications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model-driven engineering (MDE) provides abstraction and analytical rigour, but industrial adoption in many domains has been limited by the cost of developing and maintaining models. Large language models (LLMs) can help shift this cost balance by supporting direct generation of models from natural-language (NL) descriptions. For domain-specific languages (DSLs), however, LLM-generated models may be less accurate than LLM-generated code in mainstream languages such as Python, due to the latter's dominance in LLM training corpora. We investigate this issue in mathematical optimization, with AMPL, a DSL with established industrial use. We introduce EXEOS, an LLM-based approach that derives AMPL models and Python code from NL problem descriptions and iteratively refines them with solver feedback. Using a public optimization dataset and real-world supply-chain cases from our industrial partner Kinaxis, we evaluate generated AMPL models against Python code in terms of executability and correctness. An ablation study with two LLM families shows that AMPL is competitive with, and sometimes better than, Python, and that our design choices in EXEOS improve the quality of generated specifications."
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-01T20:48:15Z",
                "published_parsed": [
                    2026,
                    1,
                    1,
                    20,
                    48,
                    15,
                    3,
                    1,
                    0
                ],
                "arxiv_comment": "Accepted for publication in ICSE-SEIP 2026",
                "arxiv_primary_category": {
                    "term": "cs.SE"
                },
                "authors": [
                    {
                        "name": "Negin Ayoughi"
                    },
                    {
                        "name": "David Dewar"
                    },
                    {
                        "name": "Shiva Nejati"
                    },
                    {
                        "name": "Mehrdad Sabetzadeh"
                    }
                ],
                "author_detail": {
                    "name": "Mehrdad Sabetzadeh"
                },
                "author": "Mehrdad Sabetzadeh",
                "arxiv_doi": "10.1145/3786583.3786879"
            },
            {
                "id": "http://arxiv.org/abs/2601.02264v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.02264v1",
                "title": "POSEIDON: Physics-Optimized Seismic Energy Inference and Detection Operating Network",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "POSEIDON: Physics-Optimized Seismic Energy Inference and Detection Operating Network"
                },
                "updated": "2026-01-05T16:46:34Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    16,
                    46,
                    34,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.02264v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.02264v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Earthquake prediction and seismic hazard assessment remain fundamental challenges in geophysics, with existing machine learning approaches often operating as black boxes that ignore established physical laws. We introduce POSEIDON (Physics-Optimized Seismic Energy Inference and Detection Operating Network), a physics-informed energy-based model for unified multi-task seismic event prediction, alongside the Poseidon dataset -- the largest open-source global earthquake catalog comprising 2.8 million events spanning 30 years. POSEIDON embeds fundamental seismological principles, including the Gutenberg-Richter magnitude-frequency relationship and Omori-Utsu aftershock decay law, as learnable constraints within an energy-based modeling framework. The architecture simultaneously addresses three interconnected prediction tasks: aftershock sequence identification, tsunami generation potential, and foreshock detection. Extensive experiments demonstrate that POSEIDON achieves state-of-the-art performance across all tasks, outperforming gradient boosting, random forest, and CNN baselines with the highest average F1 score among all compared methods. Crucially, the learned physics parameters converge to scientifically interpretable values -- Gutenberg-Richter b-value of 0.752 and Omori-Utsu parameters p=0.835, c=0.1948 days -- falling within established seismological ranges while enhancing rather than compromising predictive accuracy. The Poseidon dataset is publicly available at https://huggingface.co/datasets/BorisKriuk/Poseidon, providing pre-computed energy features, spatial grid indices, and standardized quality metrics to advance physics-informed seismic research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Earthquake prediction and seismic hazard assessment remain fundamental challenges in geophysics, with existing machine learning approaches often operating as black boxes that ignore established physical laws. We introduce POSEIDON (Physics-Optimized Seismic Energy Inference and Detection Operating Network), a physics-informed energy-based model for unified multi-task seismic event prediction, alongside the Poseidon dataset -- the largest open-source global earthquake catalog comprising 2.8 million events spanning 30 years. POSEIDON embeds fundamental seismological principles, including the Gutenberg-Richter magnitude-frequency relationship and Omori-Utsu aftershock decay law, as learnable constraints within an energy-based modeling framework. The architecture simultaneously addresses three interconnected prediction tasks: aftershock sequence identification, tsunami generation potential, and foreshock detection. Extensive experiments demonstrate that POSEIDON achieves state-of-the-art performance across all tasks, outperforming gradient boosting, random forest, and CNN baselines with the highest average F1 score among all compared methods. Crucially, the learned physics parameters converge to scientifically interpretable values -- Gutenberg-Richter b-value of 0.752 and Omori-Utsu parameters p=0.835, c=0.1948 days -- falling within established seismological ranges while enhancing rather than compromising predictive accuracy. The Poseidon dataset is publicly available at https://huggingface.co/datasets/BorisKriuk/Poseidon, providing pre-computed energy features, spatial grid indices, and standardized quality metrics to advance physics-informed seismic research."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-05T16:46:34Z",
                "published_parsed": [
                    2026,
                    1,
                    5,
                    16,
                    46,
                    34,
                    0,
                    5,
                    0
                ],
                "arxiv_comment": "8 pages, 14 figures",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Boris Kriuk"
                    },
                    {
                        "name": "Fedor Kriuk"
                    }
                ],
                "author_detail": {
                    "name": "Fedor Kriuk"
                },
                "author": "Fedor Kriuk"
            },
            {
                "id": "http://arxiv.org/abs/2601.02260v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.02260v1",
                "title": "Misinterpreting Spin Precession as Orbital Eccentricity in Gravitational-Wave Signals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Misinterpreting Spin Precession as Orbital Eccentricity in Gravitational-Wave Signals"
                },
                "updated": "2026-01-05T16:42:07Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    16,
                    42,
                    7,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.02260v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.02260v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The increasing scope and breadth of gravitational wave detectors is providing the opportunity to explore new parameters in gravitational-wave astronomy. Eccentricity and spin-precession are two key observables to infer the origin of a gravitational wave (GW) source. The interpretation of GW source parameters can be plagued by degeneracy, such as the well-known degeneracy between mass and spin. As the field has explored new parameters, questions have been raised about possible degeneracies between eccentricity and spin-precession. Although some state-of-the-art models now include these effects individually, models that incorporate spin-precession and eccentricity are only in their infancy. Until models faithfully cover the complete parameter space of compact binary coalescence, our ability to correctly measure the source parameters and infer the formation of the binary is compromised. Here, we present a study of the distinguishability of these two key parameters. Our work finds that there is indeed a degeneracy between eccentricity and spin-precession; however, it is a highly localized effect. We find that the misidentified eccentricity estimates get worse as the signal gets shorter. Additionally, this misidentification is highly sensitive to the inclination angle of the source system. We provide quantifiable estimates of the potency of this degeneracy in addition to identifying some of the regions of parameter space where this degeneracy exists.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing scope and breadth of gravitational wave detectors is providing the opportunity to explore new parameters in gravitational-wave astronomy. Eccentricity and spin-precession are two key observables to infer the origin of a gravitational wave (GW) source. The interpretation of GW source parameters can be plagued by degeneracy, such as the well-known degeneracy between mass and spin. As the field has explored new parameters, questions have been raised about possible degeneracies between eccentricity and spin-precession. Although some state-of-the-art models now include these effects individually, models that incorporate spin-precession and eccentricity are only in their infancy. Until models faithfully cover the complete parameter space of compact binary coalescence, our ability to correctly measure the source parameters and infer the formation of the binary is compromised. Here, we present a study of the distinguishability of these two key parameters. Our work finds that there is indeed a degeneracy between eccentricity and spin-precession; however, it is a highly localized effect. We find that the misidentified eccentricity estimates get worse as the signal gets shorter. Additionally, this misidentification is highly sensitive to the inclination angle of the source system. We provide quantifiable estimates of the potency of this degeneracy in addition to identifying some of the regions of parameter space where this degeneracy exists."
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-05T16:42:07Z",
                "published_parsed": [
                    2026,
                    1,
                    5,
                    16,
                    42,
                    7,
                    0,
                    5,
                    0
                ],
                "arxiv_comment": "15 pages, 12 figures",
                "arxiv_primary_category": {
                    "term": "gr-qc"
                },
                "authors": [
                    {
                        "name": "Snehal Tibrewal"
                    },
                    {
                        "name": "Aaron Zimmerman"
                    },
                    {
                        "name": "Jacob Lange"
                    },
                    {
                        "name": "Deirdre Shoemaker"
                    }
                ],
                "author_detail": {
                    "name": "Deirdre Shoemaker"
                },
                "author": "Deirdre Shoemaker"
            },
            {
                "id": "http://arxiv.org/abs/2411.02570v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2411.02570v3",
                "title": "TI-PREGO: Chain of Thought and In-Context Learning for Online Mistake Detection in PRocedural EGOcentric Videos",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TI-PREGO: Chain of Thought and In-Context Learning for Online Mistake Detection in PRocedural EGOcentric Videos"
                },
                "updated": "2026-01-05T16:20:54Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    16,
                    20,
                    54,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2411.02570v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2411.02570v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.1016/j.cviu.2025.104613",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "Identifying procedural errors online from egocentric videos is a critical yet challenging task across various domains, including manufacturing, healthcare, and skill-based training. The nature of such mistakes is inherently open-set, as unforeseen or novel errors may occur, necessitating robust detection systems that do not rely on prior examples of failure. Currently, however, no technique effectively detects open-set procedural mistakes online.\n  We propose a dual branch architecture to address this problem in an online fashion: one branch continuously performs step recognition from the input egocentric video, while the other anticipates future steps based on the recognition module's output. Mistakes are detected as mismatches between the currently recognized action and the action predicted by the anticipation module. The recognition branch takes input frames, predicts the current action, and aggregates frame-level results into action tokens. The anticipation branch, specifically, leverages the solid pattern-matching capabilities of Large Language Models (LLMs) to predict action tokens based on previously predicted ones.\n  Given the online nature of the task, we also thoroughly benchmark the difficulties associated with per-frame evaluations, particularly the need for accurate and timely predictions in dynamic online scenarios.\n  Extensive experiments on two procedural datasets demonstrate the challenges and opportunities of leveraging a dual-branch architecture for mistake detection, showcasing the effectiveness of our proposed approach. In a thorough evaluation including recognition and anticipation variants and state-of-the-art models, our method reveals its robustness and effectiveness in online applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Identifying procedural errors online from egocentric videos is a critical yet challenging task across various domains, including manufacturing, healthcare, and skill-based training. The nature of such mistakes is inherently open-set, as unforeseen or novel errors may occur, necessitating robust detection systems that do not rely on prior examples of failure. Currently, however, no technique effectively detects open-set procedural mistakes online.\n  We propose a dual branch architecture to address this problem in an online fashion: one branch continuously performs step recognition from the input egocentric video, while the other anticipates future steps based on the recognition module's output. Mistakes are detected as mismatches between the currently recognized action and the action predicted by the anticipation module. The recognition branch takes input frames, predicts the current action, and aggregates frame-level results into action tokens. The anticipation branch, specifically, leverages the solid pattern-matching capabilities of Large Language Models (LLMs) to predict action tokens based on previously predicted ones.\n  Given the online nature of the task, we also thoroughly benchmark the difficulties associated with per-frame evaluations, particularly the need for accurate and timely predictions in dynamic online scenarios.\n  Extensive experiments on two procedural datasets demonstrate the challenges and opportunities of leveraging a dual-branch architecture for mistake detection, showcasing the effectiveness of our proposed approach. In a thorough evaluation including recognition and anticipation variants and state-of-the-art models, our method reveals its robustness and effectiveness in online applications."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2024-11-04T20:03:06Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    20,
                    3,
                    6,
                    0,
                    309,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Leonardo Plini"
                    },
                    {
                        "name": "Luca Scofano"
                    },
                    {
                        "name": "Edoardo De Matteis"
                    },
                    {
                        "name": "Guido Maria D'Amely di Melendugno"
                    },
                    {
                        "name": "Alessandro Flaborea"
                    },
                    {
                        "name": "Andrea Sanchietti"
                    },
                    {
                        "name": "Giovanni Maria Farinella"
                    },
                    {
                        "name": "Fabio Galasso"
                    },
                    {
                        "name": "Antonino Furnari"
                    }
                ],
                "author_detail": {
                    "name": "Antonino Furnari"
                },
                "author": "Antonino Furnari",
                "arxiv_doi": "10.1016/j.cviu.2025.104613"
            },
            {
                "id": "http://arxiv.org/abs/2601.02242v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.02242v1",
                "title": "VIBE: Visual Instruction Based Editor",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VIBE: Visual Instruction Based Editor"
                },
                "updated": "2026-01-05T16:17:20Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    16,
                    17,
                    20,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.02242v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.02242v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Instruction-based image editing is among the fastest developing areas in generative AI. Over the past year, the field has reached a new level, with dozens of open-source models released alongside highly capable commercial systems. However, only a limited number of open-source approaches currently achieve real-world quality. In addition, diffusion backbones, the dominant choice for these pipelines, are often large and computationally expensive for many deployments and research settings, with widely used variants typically containing 6B to 20B parameters. This paper presents a compact, high-throughput instruction-based image editing pipeline that uses a modern 2B-parameter Qwen3-VL model to guide the editing process and the 1.6B-parameter diffusion model Sana1.5 for image generation. Our design decisions across architecture, data processing, training configuration, and evaluation target low-cost inference and strict source consistency while maintaining high quality across the major edit categories feasible at this scale. Evaluated on the ImgEdit and GEdit benchmarks, the proposed method matches or exceeds the performance of substantially heavier baselines, including models with several times as many parameters and higher inference cost, and is particularly strong on edits that require preserving the input image, such as an attribute adjustment, object removal, background edits, and targeted replacement. The model fits within 24 GB of GPU memory and generates edited images at up to 2K resolution in approximately 4 seconds on an NVIDIA H100 in BF16, without additional inference optimizations or distillation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction-based image editing is among the fastest developing areas in generative AI. Over the past year, the field has reached a new level, with dozens of open-source models released alongside highly capable commercial systems. However, only a limited number of open-source approaches currently achieve real-world quality. In addition, diffusion backbones, the dominant choice for these pipelines, are often large and computationally expensive for many deployments and research settings, with widely used variants typically containing 6B to 20B parameters. This paper presents a compact, high-throughput instruction-based image editing pipeline that uses a modern 2B-parameter Qwen3-VL model to guide the editing process and the 1.6B-parameter diffusion model Sana1.5 for image generation. Our design decisions across architecture, data processing, training configuration, and evaluation target low-cost inference and strict source consistency while maintaining high quality across the major edit categories feasible at this scale. Evaluated on the ImgEdit and GEdit benchmarks, the proposed method matches or exceeds the performance of substantially heavier baselines, including models with several times as many parameters and higher inference cost, and is particularly strong on edits that require preserving the input image, such as an attribute adjustment, object removal, background edits, and targeted replacement. The model fits within 24 GB of GPU memory and generates edited images at up to 2K resolution in approximately 4 seconds on an NVIDIA H100 in BF16, without additional inference optimizations or distillation."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-05T16:17:20Z",
                "published_parsed": [
                    2026,
                    1,
                    5,
                    16,
                    17,
                    20,
                    0,
                    5,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Grigorii Alekseenko"
                    },
                    {
                        "name": "Aleksandr Gordeev"
                    },
                    {
                        "name": "Irina Tolstykh"
                    },
                    {
                        "name": "Bulat Suleimanov"
                    },
                    {
                        "name": "Vladimir Dokholyan"
                    },
                    {
                        "name": "Georgii Fedorov"
                    },
                    {
                        "name": "Sergey Yakubson"
                    },
                    {
                        "name": "Aleksandra Tsybina"
                    },
                    {
                        "name": "Mikhail Chernyshov"
                    },
                    {
                        "name": "Maksim Kuprashevich"
                    }
                ],
                "author_detail": {
                    "name": "Maksim Kuprashevich"
                },
                "author": "Maksim Kuprashevich"
            },
            {
                "id": "http://arxiv.org/abs/2601.02241v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.02241v1",
                "title": "From Mice to Trains: Amortized Bayesian Inference on Graph Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Mice to Trains: Amortized Bayesian Inference on Graph Data"
                },
                "updated": "2026-01-05T16:16:28Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    16,
                    16,
                    28,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.02241v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.02241v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Graphs arise across diverse domains, from biology and chemistry to social and information networks, as well as in transportation and logistics. Inference on graph-structured data requires methods that are permutation-invariant, scalable across varying sizes and sparsities, and capable of capturing complex long-range dependencies, making posterior estimation on graph parameters particularly challenging. Amortized Bayesian Inference (ABI) is a simulation-based framework that employs generative neural networks to enable fast, likelihood-free posterior inference. We adapt ABI to graph data to address these challenges to perform inference on node-, edge-, and graph-level parameters. Our approach couples permutation-invariant graph encoders with flexible neural posterior estimators in a two-module pipeline: a summary network maps attributed graphs to fixed-length representations, and an inference network approximates the posterior over parameters. In this setting, several neural architectures can serve as the summary network. In this work we evaluate multiple architectures and assess their performance on controlled synthetic settings and two real-world domains - biology and logistics - in terms of recovery and calibration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graphs arise across diverse domains, from biology and chemistry to social and information networks, as well as in transportation and logistics. Inference on graph-structured data requires methods that are permutation-invariant, scalable across varying sizes and sparsities, and capable of capturing complex long-range dependencies, making posterior estimation on graph parameters particularly challenging. Amortized Bayesian Inference (ABI) is a simulation-based framework that employs generative neural networks to enable fast, likelihood-free posterior inference. We adapt ABI to graph data to address these challenges to perform inference on node-, edge-, and graph-level parameters. Our approach couples permutation-invariant graph encoders with flexible neural posterior estimators in a two-module pipeline: a summary network maps attributed graphs to fixed-length representations, and an inference network approximates the posterior over parameters. In this setting, several neural architectures can serve as the summary network. In this work we evaluate multiple architectures and assess their performance on controlled synthetic settings and two real-world domains - biology and logistics - in terms of recovery and calibration."
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-05T16:16:28Z",
                "published_parsed": [
                    2026,
                    1,
                    5,
                    16,
                    16,
                    28,
                    0,
                    5,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML"
                },
                "authors": [
                    {
                        "name": "Svenja Jedhoff"
                    },
                    {
                        "name": "Elizaveta Semenova"
                    },
                    {
                        "name": "Aura Raulo"
                    },
                    {
                        "name": "Anne Meyer"
                    },
                    {
                        "name": "Paul-Christian Bürkner"
                    }
                ],
                "author_detail": {
                    "name": "Paul-Christian Bürkner"
                },
                "author": "Paul-Christian Bürkner"
            },
            {
                "id": "http://arxiv.org/abs/2512.20629v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20629v3",
                "title": "Learning Evolving Latent Strategies for Multi-Agent Language Systems without Model Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Evolving Latent Strategies for Multi-Agent Language Systems without Model Fine-Tuning"
                },
                "updated": "2026-01-05T16:14:39Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    16,
                    14,
                    39,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20629v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20629v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "This study proposes a multi-agent language framework that enables continual strategy evolution without fine-tuning the language model's parameters. The core idea is to liberate the latent vectors of abstract concepts from traditional static semantic representations, allowing them to be continuously updated through environmental interaction and reinforcement feedback. We construct a dual-loop architecture: the behavior loop adjusts action preferences based on environmental rewards, while the language loop updates the external latent vectors by reflecting on the semantic embeddings of generated text.\n  Together, these mechanisms allow agents to develop stable and disentangled strategic styles over long-horizon multi-round interactions. Experiments show that agents' latent spaces exhibit clear convergence trajectories under reflection-driven updates, along with structured shifts at critical moments. Moreover, the system demonstrates an emergent ability to implicitly infer and continually adapt to emotional agents, even without shared rewards. These results indicate that, without modifying model parameters, an external latent space can provide language agents with a low-cost, scalable, and interpretable form of abstract strategic representation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study proposes a multi-agent language framework that enables continual strategy evolution without fine-tuning the language model's parameters. The core idea is to liberate the latent vectors of abstract concepts from traditional static semantic representations, allowing them to be continuously updated through environmental interaction and reinforcement feedback. We construct a dual-loop architecture: the behavior loop adjusts action preferences based on environmental rewards, while the language loop updates the external latent vectors by reflecting on the semantic embeddings of generated text.\n  Together, these mechanisms allow agents to develop stable and disentangled strategic styles over long-horizon multi-round interactions. Experiments show that agents' latent spaces exhibit clear convergence trajectories under reflection-driven updates, along with structured shifts at critical moments. Moreover, the system demonstrates an emergent ability to implicitly infer and continually adapt to emotional agents, even without shared rewards. These results indicate that, without modifying model parameters, an external latent space can provide language agents with a low-cost, scalable, and interpretable form of abstract strategic representation."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-28T23:36:45Z",
                "published_parsed": [
                    2025,
                    11,
                    28,
                    23,
                    36,
                    45,
                    4,
                    332,
                    0
                ],
                "arxiv_comment": "17 pages, 5 figures. Code available at https://github.com/wltang-dev/Latent-Strategy-RL-Agent",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Wenlong Tang"
                    }
                ],
                "author_detail": {
                    "name": "Wenlong Tang"
                },
                "author": "Wenlong Tang"
            },
            {
                "id": "http://arxiv.org/abs/2508.05685v6",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2508.05685v6",
                "title": "DogFit: Domain-guided Fine-tuning for Efficient Transfer Learning of Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DogFit: Domain-guided Fine-tuning for Efficient Transfer Learning of Diffusion Models"
                },
                "updated": "2026-01-05T16:11:10Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    16,
                    11,
                    10,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2508.05685v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2508.05685v6",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Transfer learning of diffusion models to smaller target domains is challenging, as naively fine-tuning the model often results in poor generalization. Test-time guidance methods help mitigate this by offering controllable improvements in image fidelity through a trade-off with sample diversity. However, this benefit comes at a high computational cost, typically requiring dual forward passes during sampling. We propose the Domain-guided Fine-tuning (DogFit) method, an effective guidance mechanism for diffusion transfer learning that maintains controllability without incurring additional computational overhead. DogFit injects a domain-aware guidance offset into the training loss, effectively internalizing the guided behavior during the fine-tuning process. The domain-aware design is motivated by our observation that during fine-tuning, the unconditional source model offers a stronger marginal estimate than the target model. To support efficient controllable fidelity-diversity trade-offs at inference, we encode the guidance strength value as an additional model input through a lightweight conditioning mechanism. We further investigate the optimal placement and timing of the guidance offset during training and propose two simple scheduling strategies, i.e., late-start and cut-off, which improve generation quality and training stability. Experiments on DiT and SiT backbones across six diverse target domains show that DogFit can outperform prior guidance methods in transfer learning in terms of FID and FDDINOV2 while requiring up to 2x fewer sampling TFLOPS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transfer learning of diffusion models to smaller target domains is challenging, as naively fine-tuning the model often results in poor generalization. Test-time guidance methods help mitigate this by offering controllable improvements in image fidelity through a trade-off with sample diversity. However, this benefit comes at a high computational cost, typically requiring dual forward passes during sampling. We propose the Domain-guided Fine-tuning (DogFit) method, an effective guidance mechanism for diffusion transfer learning that maintains controllability without incurring additional computational overhead. DogFit injects a domain-aware guidance offset into the training loss, effectively internalizing the guided behavior during the fine-tuning process. The domain-aware design is motivated by our observation that during fine-tuning, the unconditional source model offers a stronger marginal estimate than the target model. To support efficient controllable fidelity-diversity trade-offs at inference, we encode the guidance strength value as an additional model input through a lightweight conditioning mechanism. We further investigate the optimal placement and timing of the guidance offset during training and propose two simple scheduling strategies, i.e., late-start and cut-off, which improve generation quality and training stability. Experiments on DiT and SiT backbones across six diverse target domains show that DogFit can outperform prior guidance methods in transfer learning in terms of FID and FDDINOV2 while requiring up to 2x fewer sampling TFLOPS."
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-08-05T21:33:05Z",
                "published_parsed": [
                    2025,
                    8,
                    5,
                    21,
                    33,
                    5,
                    1,
                    217,
                    0
                ],
                "arxiv_comment": "Accepted for poster presentation at AAAI 2026",
                "arxiv_primary_category": {
                    "term": "cs.GR"
                },
                "authors": [
                    {
                        "name": "Yara Bahram"
                    },
                    {
                        "name": "Mohammadhadi Shateri"
                    },
                    {
                        "name": "Eric Granger"
                    }
                ],
                "author_detail": {
                    "name": "Eric Granger"
                },
                "author": "Eric Granger"
            },
            {
                "id": "http://arxiv.org/abs/2507.01752v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2507.01752v3",
                "title": "Tuning without Peeking: Provable Generalization Bounds and Robust LLM Post-Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tuning without Peeking: Provable Generalization Bounds and Robust LLM Post-Training"
                },
                "updated": "2026-01-05T16:10:09Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    16,
                    10,
                    9,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2507.01752v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2507.01752v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Gradient-based optimization is the workhorse of deep learning, offering efficient and scalable training via backpropagation. However, exposing gradients during training can leak sensitive information about the underlying data, raising privacy and security concerns such as susceptibility to data poisoning attacks. In contrast, black box optimization methods, which treat the model as an opaque function, relying solely on function evaluations to guide optimization, offer a promising alternative in scenarios where data access is restricted, adversarial risks are high, or overfitting is a concern. This paper introduces BBoxER, an evolutionary black-box method for LLM post-training that induces an information bottleneck via implicit compression of the training data. Leveraging the tractability of information flow, we provide non-vacuous generalization bounds and strong theoretical guarantees for privacy, robustness to data poisoning attacks, and extraction attacks. In experiments with LLMs, we demonstrate empirically that black-box optimization methods, despite the scalability and computational challenges inherent to black-box approaches, are able to learn, showing how a few iterations of BBoxER improve performance, generalize well on a benchmark of reasoning datasets, and are robust to membership inference attacks. This positions BBoxER as an attractive add-on on top of gradient-based optimization, offering suitability for deployment in restricted or privacy-sensitive environments while also providing non-vacuous generalization guarantees.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gradient-based optimization is the workhorse of deep learning, offering efficient and scalable training via backpropagation. However, exposing gradients during training can leak sensitive information about the underlying data, raising privacy and security concerns such as susceptibility to data poisoning attacks. In contrast, black box optimization methods, which treat the model as an opaque function, relying solely on function evaluations to guide optimization, offer a promising alternative in scenarios where data access is restricted, adversarial risks are high, or overfitting is a concern. This paper introduces BBoxER, an evolutionary black-box method for LLM post-training that induces an information bottleneck via implicit compression of the training data. Leveraging the tractability of information flow, we provide non-vacuous generalization bounds and strong theoretical guarantees for privacy, robustness to data poisoning attacks, and extraction attacks. In experiments with LLMs, we demonstrate empirically that black-box optimization methods, despite the scalability and computational challenges inherent to black-box approaches, are able to learn, showing how a few iterations of BBoxER improve performance, generalize well on a benchmark of reasoning datasets, and are robust to membership inference attacks. This positions BBoxER as an attractive add-on on top of gradient-based optimization, offering suitability for deployment in restricted or privacy-sensitive environments while also providing non-vacuous generalization guarantees."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-07-02T14:29:30Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    14,
                    29,
                    30,
                    2,
                    183,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Ismail Labiad"
                    },
                    {
                        "name": "Mathurin Videau"
                    },
                    {
                        "name": "Matthieu Kowalski"
                    },
                    {
                        "name": "Marc Schoenauer"
                    },
                    {
                        "name": "Alessandro Leite"
                    },
                    {
                        "name": "Julia Kempe"
                    },
                    {
                        "name": "Olivier Teytaud"
                    }
                ],
                "author_detail": {
                    "name": "Olivier Teytaud"
                },
                "author": "Olivier Teytaud"
            },
            {
                "id": "http://arxiv.org/abs/2601.02236v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.02236v1",
                "title": "CD4LM: Consistency Distillation and aDaptive Decoding for Diffusion Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CD4LM: Consistency Distillation and aDaptive Decoding for Diffusion Language Models"
                },
                "updated": "2026-01-05T16:09:22Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    16,
                    9,
                    22,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.02236v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.02236v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Autoregressive large language models achieve strong results on many benchmarks, but decoding remains fundamentally latency-limited by sequential dependence on previously generated tokens. Diffusion language models (DLMs) promise parallel generation but suffer from a fundamental static-to-dynamic misalignment: Training optimizes local transitions under fixed schedules, whereas efficient inference requires adaptive \"long-jump\" refinements through unseen states. Our goal is to enable highly parallel decoding for DLMs with low number of function evaluations while preserving generation quality. To achieve this, we propose CD4LM, a framework that decouples training from inference via Discrete-Space Consistency Distillation (DSCD) and Confidence-Adaptive Decoding (CAD). Unlike standard objectives, DSCD trains a student to be trajectory-invariant, mapping diverse noisy states directly to the clean distribution. This intrinsic robustness enables CAD to dynamically allocate compute resources based on token confidence, aggressively skipping steps without the quality collapse typical of heuristic acceleration. On GSM8K, CD4LM matches the LLaDA baseline with a 5.18x wall-clock speedup; across code and math benchmarks, it strictly dominates the accuracy-efficiency Pareto frontier, achieving a 3.62x mean speedup while improving average accuracy. Code is available at https://github.com/yihao-liang/CDLM",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive large language models achieve strong results on many benchmarks, but decoding remains fundamentally latency-limited by sequential dependence on previously generated tokens. Diffusion language models (DLMs) promise parallel generation but suffer from a fundamental static-to-dynamic misalignment: Training optimizes local transitions under fixed schedules, whereas efficient inference requires adaptive \"long-jump\" refinements through unseen states. Our goal is to enable highly parallel decoding for DLMs with low number of function evaluations while preserving generation quality. To achieve this, we propose CD4LM, a framework that decouples training from inference via Discrete-Space Consistency Distillation (DSCD) and Confidence-Adaptive Decoding (CAD). Unlike standard objectives, DSCD trains a student to be trajectory-invariant, mapping diverse noisy states directly to the clean distribution. This intrinsic robustness enables CAD to dynamically allocate compute resources based on token confidence, aggressively skipping steps without the quality collapse typical of heuristic acceleration. On GSM8K, CD4LM matches the LLaDA baseline with a 5.18x wall-clock speedup; across code and math benchmarks, it strictly dominates the accuracy-efficiency Pareto frontier, achieving a 3.62x mean speedup while improving average accuracy. Code is available at https://github.com/yihao-liang/CDLM"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-05T16:09:22Z",
                "published_parsed": [
                    2026,
                    1,
                    5,
                    16,
                    9,
                    22,
                    0,
                    5,
                    0
                ],
                "arxiv_comment": "33 pages, 7 figures",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Yihao Liang"
                    },
                    {
                        "name": "Ze Wang"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Ximeng Sun"
                    },
                    {
                        "name": "Jialian Wu"
                    },
                    {
                        "name": "Xiaodong Yu"
                    },
                    {
                        "name": "Jiang Liu"
                    },
                    {
                        "name": "Emad Barsoum"
                    },
                    {
                        "name": "Zicheng Liu"
                    },
                    {
                        "name": "Niraj K. Jha"
                    }
                ],
                "author_detail": {
                    "name": "Niraj K. Jha"
                },
                "author": "Niraj K. Jha"
            },
            {
                "id": "http://arxiv.org/abs/2601.02232v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.02232v1",
                "title": "ELLA: Efficient Lifelong Learning for Adapters in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ELLA: Efficient Lifelong Learning for Adapters in Large Language Models"
                },
                "updated": "2026-01-05T15:58:08Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    15,
                    58,
                    8,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.02232v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.02232v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) suffer severe catastrophic forgetting when adapted sequentially to new tasks in a continual learning (CL) setting. Existing approaches are fundamentally limited: replay-based methods are impractical and privacy-violating, while strict orthogonality-based methods collapse under scale: each new task is projected onto an orthogonal complement, progressively reducing the residual degrees of freedom and eliminating forward transfer by forbidding overlap in shared representations. In this work, we introduce ELLA, a training framework built on the principle of selective subspace de-correlation. Rather than forbidding all overlap, ELLA explicitly characterizes the structure of past updates and penalizes alignments along their high-energy, task-specific directions, while preserving freedom in the low-energy residual subspaces to enable transfer. Formally, this is realized via a lightweight regularizer on a single aggregated update matrix. We prove this mechanism corresponds to an anisotropic shrinkage operator that bounds interference, yielding a penalty that is both memory- and compute-constant regardless of task sequence length. ELLA requires no data replay, no architectural expansion, and negligible storage. Empirically, it achieves state-of-the-art CL performance on three popular benchmarks, with relative accuracy gains of up to $9.6\\%$ and a $35\\times$ smaller memory footprint. Further, ELLA scales robustly across architectures and actively enhances the model's zero-shot generalization performance on unseen tasks, establishing a principled and scalable solution for constructive lifelong LLM adaptation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) suffer severe catastrophic forgetting when adapted sequentially to new tasks in a continual learning (CL) setting. Existing approaches are fundamentally limited: replay-based methods are impractical and privacy-violating, while strict orthogonality-based methods collapse under scale: each new task is projected onto an orthogonal complement, progressively reducing the residual degrees of freedom and eliminating forward transfer by forbidding overlap in shared representations. In this work, we introduce ELLA, a training framework built on the principle of selective subspace de-correlation. Rather than forbidding all overlap, ELLA explicitly characterizes the structure of past updates and penalizes alignments along their high-energy, task-specific directions, while preserving freedom in the low-energy residual subspaces to enable transfer. Formally, this is realized via a lightweight regularizer on a single aggregated update matrix. We prove this mechanism corresponds to an anisotropic shrinkage operator that bounds interference, yielding a penalty that is both memory- and compute-constant regardless of task sequence length. ELLA requires no data replay, no architectural expansion, and negligible storage. Empirically, it achieves state-of-the-art CL performance on three popular benchmarks, with relative accuracy gains of up to $9.6\\%$ and a $35\\times$ smaller memory footprint. Further, ELLA scales robustly across architectures and actively enhances the model's zero-shot generalization performance on unseen tasks, establishing a principled and scalable solution for constructive lifelong LLM adaptation."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-05T15:58:08Z",
                "published_parsed": [
                    2026,
                    1,
                    5,
                    15,
                    58,
                    8,
                    0,
                    5,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Shristi Das Biswas"
                    },
                    {
                        "name": "Yue Zhang"
                    },
                    {
                        "name": "Anwesan Pal"
                    },
                    {
                        "name": "Radhika Bhargava"
                    },
                    {
                        "name": "Kaushik Roy"
                    }
                ],
                "author_detail": {
                    "name": "Kaushik Roy"
                },
                "author": "Kaushik Roy"
            },
            {
                "id": "http://arxiv.org/abs/2601.02224v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.02224v1",
                "title": "From XAI to Stories: A Factorial Study of LLM-Generated Explanation Quality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From XAI to Stories: A Factorial Study of LLM-Generated Explanation Quality"
                },
                "updated": "2026-01-05T15:52:20Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    15,
                    52,
                    20,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.02224v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.02224v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Explainable AI (XAI) methods like SHAP and LIME produce numerical feature attributions that remain inaccessible to non expert users. Prior work has shown that Large Language Models (LLMs) can transform these outputs into natural language explanations (NLEs), but it remains unclear which factors contribute to high-quality explanations. We present a systematic factorial study investigating how Forecasting model choice, XAI method, LLM selection, and prompting strategy affect NLE quality. Our design spans four models (XGBoost (XGB), Random Forest (RF), Multilayer Perceptron (MLP), and SARIMAX - comparing black-box Machine-Learning (ML) against classical time-series approaches), three XAI conditions (SHAP, LIME, and a no-XAI baseline), three LLMs (GPT-4o, Llama-3-8B, DeepSeek-R1), and eight prompting strategies. Using G-Eval, an LLM-as-a-judge evaluation method, with dual LLM judges and four evaluation criteria, we evaluate 660 explanations for time-series forecasting. Our results suggest that: (1) XAI provides only small improvements over no-XAI baselines, and only for expert audiences; (2) LLM choice dominates all other factors, with DeepSeek-R1 outperforming GPT-4o and Llama-3; (3) we observe an interpretability paradox: in our setting, SARIMAX yielded lower NLE quality than ML models despite higher prediction accuracy; (4) zero-shot prompting is competitive with self-consistency at 7-times lower cost; and (5) chain-of-thought hurts rather than helps.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explainable AI (XAI) methods like SHAP and LIME produce numerical feature attributions that remain inaccessible to non expert users. Prior work has shown that Large Language Models (LLMs) can transform these outputs into natural language explanations (NLEs), but it remains unclear which factors contribute to high-quality explanations. We present a systematic factorial study investigating how Forecasting model choice, XAI method, LLM selection, and prompting strategy affect NLE quality. Our design spans four models (XGBoost (XGB), Random Forest (RF), Multilayer Perceptron (MLP), and SARIMAX - comparing black-box Machine-Learning (ML) against classical time-series approaches), three XAI conditions (SHAP, LIME, and a no-XAI baseline), three LLMs (GPT-4o, Llama-3-8B, DeepSeek-R1), and eight prompting strategies. Using G-Eval, an LLM-as-a-judge evaluation method, with dual LLM judges and four evaluation criteria, we evaluate 660 explanations for time-series forecasting. Our results suggest that: (1) XAI provides only small improvements over no-XAI baselines, and only for expert audiences; (2) LLM choice dominates all other factors, with DeepSeek-R1 outperforming GPT-4o and Llama-3; (3) we observe an interpretability paradox: in our setting, SARIMAX yielded lower NLE quality than ML models despite higher prediction accuracy; (4) zero-shot prompting is competitive with self-consistency at 7-times lower cost; and (5) chain-of-thought hurts rather than helps."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-05T15:52:20Z",
                "published_parsed": [
                    2026,
                    1,
                    5,
                    15,
                    52,
                    20,
                    0,
                    5,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Fabian Lukassen"
                    },
                    {
                        "name": "Jan Herrmann"
                    },
                    {
                        "name": "Christoph Weisser"
                    },
                    {
                        "name": "Benjamin Saefken"
                    },
                    {
                        "name": "Thomas Kneib"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Kneib"
                },
                "author": "Thomas Kneib"
            },
            {
                "id": "http://arxiv.org/abs/2509.25664v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2509.25664v2",
                "title": "QFrBLiMP: a Quebec-French Benchmark of Linguistic Minimal Pairs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QFrBLiMP: a Quebec-French Benchmark of Linguistic Minimal Pairs"
                },
                "updated": "2026-01-05T15:50:05Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    15,
                    50,
                    5,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2509.25664v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2509.25664v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "In this paper, we introduce the Quebec-French Benchmark of Linguistic Minimal Pairs (QFrBLiMP), a corpus designed to evaluate LLMs' linguistic knowledge of prominent grammatical phenomena in Quebec-French. QFrBLiMP comprises 1,761 minimal pairs annotated with 20 LPs. Specifically, these minimal pairs have been created by manually modifying sentences extracted from an official online resource maintained by a Québec government institution. Each pair is annotated by 12 Quebec-French native speakers, who select the sentence they consider grammatical from the two. These annotations are used to compare the competency of LLMs with that of humans. We evaluate different LLMs on QFrBLiMP and MultiBLiMP-Fr by observing the rate of higher probabilities assigned to the sentences of each minimal pair for each category. We find that while grammatical competence scales with model size, a clear hierarchy of difficulty emerges. All benchmarked models consistently fail on phenomena requiring deep semantic understanding, revealing a critical limitation. Finally, our statistical analysis comparing QFrBLiMP and MultiBLiMP reveals a significant performance degradation for most models on Quebec-French; however, the most capable models remain within the statistical significance interval, demonstrating cross-dialectal robustness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce the Quebec-French Benchmark of Linguistic Minimal Pairs (QFrBLiMP), a corpus designed to evaluate LLMs' linguistic knowledge of prominent grammatical phenomena in Quebec-French. QFrBLiMP comprises 1,761 minimal pairs annotated with 20 LPs. Specifically, these minimal pairs have been created by manually modifying sentences extracted from an official online resource maintained by a Québec government institution. Each pair is annotated by 12 Quebec-French native speakers, who select the sentence they consider grammatical from the two. These annotations are used to compare the competency of LLMs with that of humans. We evaluate different LLMs on QFrBLiMP and MultiBLiMP-Fr by observing the rate of higher probabilities assigned to the sentences of each minimal pair for each category. We find that while grammatical competence scales with model size, a clear hierarchy of difficulty emerges. All benchmarked models consistently fail on phenomena requiring deep semantic understanding, revealing a critical limitation. Finally, our statistical analysis comparing QFrBLiMP and MultiBLiMP reveals a significant performance degradation for most models on Quebec-French; however, the most capable models remain within the statistical significance interval, demonstrating cross-dialectal robustness."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-09-30T02:00:26Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    2,
                    0,
                    26,
                    1,
                    273,
                    0
                ],
                "arxiv_comment": "Acceptged to EACL 2026",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "David Beauchemin"
                    },
                    {
                        "name": "Pier-Luc Veilleux"
                    },
                    {
                        "name": "Johanna-Pascale Roy"
                    },
                    {
                        "name": "Richard Khoury"
                    }
                ],
                "author_detail": {
                    "name": "Richard Khoury"
                },
                "author": "Richard Khoury"
            },
            {
                "id": "http://arxiv.org/abs/2512.15231v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.15231v2",
                "title": "CangLing-KnowFlow: A Unified Knowledge-and-Flow-fused Agent for Comprehensive Remote Sensing Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CangLing-KnowFlow: A Unified Knowledge-and-Flow-fused Agent for Comprehensive Remote Sensing Applications"
                },
                "updated": "2026-01-05T15:48:10Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    15,
                    48,
                    10,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.15231v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.15231v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The automated and intelligent processing of massive remote sensing (RS) datasets is critical in Earth observation (EO). Existing automated systems are normally task-specific, lacking a unified framework to manage diverse, end-to-end workflows--from data preprocessing to advanced interpretation--across diverse RS applications. To address this gap, this paper introduces CangLing-KnowFlow, a unified intelligent agent framework that integrates a Procedural Knowledge Base (PKB), Dynamic Workflow Adjustment, and an Evolutionary Memory Module. The PKB, comprising 1,008 expert-validated workflow cases across 162 practical RS tasks, guides planning and substantially reduces hallucinations common in general-purpose agents. During runtime failures, the Dynamic Workflow Adjustment autonomously diagnoses and replans recovery strategies, while the Evolutionary Memory Module continuously learns from these events, iteratively enhancing the agent's knowledge and performance. This synergy enables CangLing-KnowFlow to adapt, learn, and operate reliably across diverse, complex tasks. We evaluated CangLing-KnowFlow on the KnowFlow-Bench, a novel benchmark of 324 workflows inspired by real-world applications, testing its performance across 13 top Large Language Model (LLM) backbones, from open-source to commercial. Across all complex tasks, CangLing-KnowFlow surpassed the Reflexion baseline by at least 4% in Task Success Rate. As the first most comprehensive validation along this emerging field, this research demonstrates the great potential of CangLing-KnowFlow as a robust, efficient, and scalable automated solution for complex EO challenges by leveraging expert knowledge (Knowledge) into adaptive and verifiable procedures (Flow).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The automated and intelligent processing of massive remote sensing (RS) datasets is critical in Earth observation (EO). Existing automated systems are normally task-specific, lacking a unified framework to manage diverse, end-to-end workflows--from data preprocessing to advanced interpretation--across diverse RS applications. To address this gap, this paper introduces CangLing-KnowFlow, a unified intelligent agent framework that integrates a Procedural Knowledge Base (PKB), Dynamic Workflow Adjustment, and an Evolutionary Memory Module. The PKB, comprising 1,008 expert-validated workflow cases across 162 practical RS tasks, guides planning and substantially reduces hallucinations common in general-purpose agents. During runtime failures, the Dynamic Workflow Adjustment autonomously diagnoses and replans recovery strategies, while the Evolutionary Memory Module continuously learns from these events, iteratively enhancing the agent's knowledge and performance. This synergy enables CangLing-KnowFlow to adapt, learn, and operate reliably across diverse, complex tasks. We evaluated CangLing-KnowFlow on the KnowFlow-Bench, a novel benchmark of 324 workflows inspired by real-world applications, testing its performance across 13 top Large Language Model (LLM) backbones, from open-source to commercial. Across all complex tasks, CangLing-KnowFlow surpassed the Reflexion baseline by at least 4% in Task Success Rate. As the first most comprehensive validation along this emerging field, this research demonstrates the great potential of CangLing-KnowFlow as a robust, efficient, and scalable automated solution for complex EO challenges by leveraging expert knowledge (Knowledge) into adaptive and verifiable procedures (Flow)."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-17T09:31:57Z",
                "published_parsed": [
                    2025,
                    12,
                    17,
                    9,
                    31,
                    57,
                    2,
                    351,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Zhengchao Chen"
                    },
                    {
                        "name": "Haoran Wang"
                    },
                    {
                        "name": "Jing Yao"
                    },
                    {
                        "name": "Pedram Ghamisi"
                    },
                    {
                        "name": "Jun Zhou"
                    },
                    {
                        "name": "Peter M. Atkinson"
                    },
                    {
                        "name": "Bing Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Bing Zhang"
                },
                "author": "Bing Zhang"
            },
            {
                "id": "http://arxiv.org/abs/2510.06310v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.06310v2",
                "title": "MUSEQuBES: Physical conditions, origins, and multi-element abundances of the circumgalactic medium of an isolated, star-forming dwarf galaxy at z=0.57",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MUSEQuBES: Physical conditions, origins, and multi-element abundances of the circumgalactic medium of an isolated, star-forming dwarf galaxy at z=0.57"
                },
                "updated": "2026-01-05T15:43:52Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    15,
                    43,
                    52,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.06310v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.06310v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.3847/2041-8213/ae2f5f",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "In dwarf galaxy models, outflows expel metal-enriched interstellar medium (ISM) into the circumgalactic medium (CGM) to reproduce their observed low metallicities, but measurements of dwarf CGM properties are scarce. We present a study of the CGM of an isolated dwarf at $z=0.5723$ with a stellar mass of $\\approx5\\times10^7\\rm\\,M_{\\odot}$ and star-formation rate ($\\approx0.05\\,\\rm M_\\odot\\,yr^{-1}$) and ISM metallicity ($\\rm [O/H]\\approx-0.9$) consistent with the star-forming main sequence and mass-metallicity relation. A background quasar sightline with archival UV spectra probes the dwarf's CGM at a projected distance of 28 kpc, corresponding to approximately half of the estimated virial radius. The dwarf's CGM is detected in \\ion{H}{1}, intermediate metal ions of \\ion{C}{3}, \\ion{O}{3}, \\ion{O}{4}, and \\textcolor{black}{\\ion{S}{5}}, and kinematically broader, highly-ionized \\ion{O}{6}, but is undetected in \\ion{N}{4} and \\ion{Ne}{8}. Photoionization modeling of the intermediate ions indicates a modest volume-filling factor ($\\sim 6\\%$ along the sightline or $\\sim 2\\%$ globally), and a mass of $\\sim2\\times10^8 {\\rm\\,M_\\odot}$, $\\sim4\\times$ higher than the dwarf's stellar mass, but $\\sim10\\times$ less than the highly ionized CGM. The \\ion{O}{6} kinematics are comparable to the dwarf's estimated virial velocity, suggesting it is likely associated with cool, photoionized, and volume-filling CGM, with bulk motion or turbulence dominating over thermal pressure. The metallicity inferred for the intermediate ions is $\\rm [O/H]=-0.6$, but with low relative abundances of $\\rm [C/O]=-0.6$ and \\textcolor{black}{$\\rm [N/O]<-1.0$}. The [N/O] is below levels expected of the dwarf's ISM, but consistent with core-collapse supernova ejecta, suggesting that supernova-enriched gas escaped the dwarf without mixing significantly with ISM enriched in nitrogen from evolved, low-mass stars.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In dwarf galaxy models, outflows expel metal-enriched interstellar medium (ISM) into the circumgalactic medium (CGM) to reproduce their observed low metallicities, but measurements of dwarf CGM properties are scarce. We present a study of the CGM of an isolated dwarf at $z=0.5723$ with a stellar mass of $\\approx5\\times10^7\\rm\\,M_{\\odot}$ and star-formation rate ($\\approx0.05\\,\\rm M_\\odot\\,yr^{-1}$) and ISM metallicity ($\\rm [O/H]\\approx-0.9$) consistent with the star-forming main sequence and mass-metallicity relation. A background quasar sightline with archival UV spectra probes the dwarf's CGM at a projected distance of 28 kpc, corresponding to approximately half of the estimated virial radius. The dwarf's CGM is detected in \\ion{H}{1}, intermediate metal ions of \\ion{C}{3}, \\ion{O}{3}, \\ion{O}{4}, and \\textcolor{black}{\\ion{S}{5}}, and kinematically broader, highly-ionized \\ion{O}{6}, but is undetected in \\ion{N}{4} and \\ion{Ne}{8}. Photoionization modeling of the intermediate ions indicates a modest volume-filling factor ($\\sim 6\\%$ along the sightline or $\\sim 2\\%$ globally), and a mass of $\\sim2\\times10^8 {\\rm\\,M_\\odot}$, $\\sim4\\times$ higher than the dwarf's stellar mass, but $\\sim10\\times$ less than the highly ionized CGM. The \\ion{O}{6} kinematics are comparable to the dwarf's estimated virial velocity, suggesting it is likely associated with cool, photoionized, and volume-filling CGM, with bulk motion or turbulence dominating over thermal pressure. The metallicity inferred for the intermediate ions is $\\rm [O/H]=-0.6$, but with low relative abundances of $\\rm [C/O]=-0.6$ and \\textcolor{black}{$\\rm [N/O]<-1.0$}. The [N/O] is below levels expected of the dwarf's ISM, but consistent with core-collapse supernova ejecta, suggesting that supernova-enriched gas escaped the dwarf without mixing significantly with ISM enriched in nitrogen from evolved, low-mass stars."
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-07T18:00:00Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    18,
                    0,
                    0,
                    1,
                    280,
                    0
                ],
                "arxiv_comment": "Accepted to ApJ Letters",
                "arxiv_primary_category": {
                    "term": "astro-ph.GA"
                },
                "arxiv_journal_ref": "The Astrophysical Journal Letters, 2026, Volume 996, Number 2",
                "authors": [
                    {
                        "name": "Sean D. Johnson"
                    },
                    {
                        "name": "Nishant Mishra"
                    },
                    {
                        "name": "Sowgat Muzahid"
                    },
                    {
                        "name": "Gwen C. Rudie"
                    },
                    {
                        "name": "Fakhri S. Zahedy"
                    },
                    {
                        "name": "Zhijie Qu"
                    },
                    {
                        "name": "Claude-André Faucher-Giguère"
                    },
                    {
                        "name": "Jonathan Stern"
                    },
                    {
                        "name": "Jennifer I-Hsiu Li"
                    },
                    {
                        "name": "Elise Fuller"
                    },
                    {
                        "name": "Sebastiano Cantalupo"
                    },
                    {
                        "name": "Hsiao-Wen Chen"
                    },
                    {
                        "name": "Ahmad Kadri"
                    },
                    {
                        "name": "Suyash Kumar"
                    },
                    {
                        "name": "Zhuoqi Will Liu"
                    },
                    {
                        "name": "Gregory Walth"
                    }
                ],
                "author_detail": {
                    "name": "Gregory Walth"
                },
                "author": "Gregory Walth",
                "arxiv_doi": "10.3847/2041-8213/ae2f5f"
            },
            {
                "id": "http://arxiv.org/abs/2512.20182v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20182v2",
                "title": "FaithLens: Detecting and Explaining Faithfulness Hallucination",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FaithLens: Detecting and Explaining Faithfulness Hallucination"
                },
                "updated": "2026-01-05T15:43:00Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    15,
                    43,
                    0,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20182v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20182v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Recognizing whether outputs from large language models (LLMs) contain faithfulness hallucination is crucial for real-world applications, e.g., retrieval-augmented generation and summarization. In this paper, we introduce FaithLens, a cost-efficient and effective faithfulness hallucination detection model that can jointly provide binary predictions and corresponding explanations to improve trustworthiness. To achieve this, we first synthesize training data with explanations via advanced LLMs and apply a well-defined data filtering strategy to ensure label correctness, explanation quality, and data diversity. Subsequently, we fine-tune the model on these well-curated training data as a cold start and further optimize it with rule-based reinforcement learning, using rewards for both prediction correctness and explanation quality. Results on 12 diverse tasks show that the 8B-parameter FaithLens outperforms advanced models such as GPT-4.1 and o3. Also, FaithLens can produce high-quality explanations, delivering a distinctive balance of trustworthiness, efficiency, and effectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recognizing whether outputs from large language models (LLMs) contain faithfulness hallucination is crucial for real-world applications, e.g., retrieval-augmented generation and summarization. In this paper, we introduce FaithLens, a cost-efficient and effective faithfulness hallucination detection model that can jointly provide binary predictions and corresponding explanations to improve trustworthiness. To achieve this, we first synthesize training data with explanations via advanced LLMs and apply a well-defined data filtering strategy to ensure label correctness, explanation quality, and data diversity. Subsequently, we fine-tune the model on these well-curated training data as a cold start and further optimize it with rule-based reinforcement learning, using rewards for both prediction correctness and explanation quality. Results on 12 diverse tasks show that the 8B-parameter FaithLens outperforms advanced models such as GPT-4.1 and o3. Also, FaithLens can produce high-quality explanations, delivering a distinctive balance of trustworthiness, efficiency, and effectiveness."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T09:20:32Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    9,
                    20,
                    32,
                    1,
                    357,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Shuzheng Si"
                    },
                    {
                        "name": "Qingyi Wang"
                    },
                    {
                        "name": "Haozhe Zhao"
                    },
                    {
                        "name": "Yuzhuo Bai"
                    },
                    {
                        "name": "Guanqiao Chen"
                    },
                    {
                        "name": "Kangyang Luo"
                    },
                    {
                        "name": "Gang Chen"
                    },
                    {
                        "name": "Fanchao Qi"
                    },
                    {
                        "name": "Minjia Zhang"
                    },
                    {
                        "name": "Baobao Chang"
                    },
                    {
                        "name": "Maosong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Maosong Sun"
                },
                "author": "Maosong Sun"
            },
            {
                "id": "http://arxiv.org/abs/2601.02215v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.02215v1",
                "title": "LLM-Empowered Functional Safety and Security by Design in Automotive Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Empowered Functional Safety and Security by Design in Automotive Systems"
                },
                "updated": "2026-01-05T15:37:08Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    15,
                    37,
                    8,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.02215v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.02215v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "This paper presents LLM-empowered workflow to support Software Defined Vehicle (SDV) software development, covering the aspects of security-aware system topology design, as well as event-driven decision-making code analysis. For code analysis we adopt event chains model which provides formal foundations to systematic validation of functional safety, taking into account the semantic validity of messages exchanged between key components, including both CAN and Vehicle Signal Specification (VSS). Analysis of security aspects for topology relies on synergy with Model-Driven Engineering (MDE) approach and Object Constraint Language (OCL) rules. Both locally deployable and proprietary solution are taken into account for evaluation within Advanced Driver-Assistance Systems (ADAS)-related scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents LLM-empowered workflow to support Software Defined Vehicle (SDV) software development, covering the aspects of security-aware system topology design, as well as event-driven decision-making code analysis. For code analysis we adopt event chains model which provides formal foundations to systematic validation of functional safety, taking into account the semantic validity of messages exchanged between key components, including both CAN and Vehicle Signal Specification (VSS). Analysis of security aspects for topology relies on synergy with Model-Driven Engineering (MDE) approach and Object Constraint Language (OCL) rules. Both locally deployable and proprietary solution are taken into account for evaluation within Advanced Driver-Assistance Systems (ADAS)-related scenarios."
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-05T15:37:08Z",
                "published_parsed": [
                    2026,
                    1,
                    5,
                    15,
                    37,
                    8,
                    0,
                    5,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE"
                },
                "authors": [
                    {
                        "name": "Nenad Petrovic"
                    },
                    {
                        "name": "Vahid Zolfaghari"
                    },
                    {
                        "name": "Fengjunjie Pan"
                    },
                    {
                        "name": "Alois Knoll"
                    }
                ],
                "author_detail": {
                    "name": "Alois Knoll"
                },
                "author": "Alois Knoll"
            },
            {
                "id": "http://arxiv.org/abs/2601.02213v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.02213v1",
                "title": "Quantized SO(3)-Equivariant Graph Neural Networks for Efficient Molecular Property Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantized SO(3)-Equivariant Graph Neural Networks for Efficient Molecular Property Prediction"
                },
                "updated": "2026-01-05T15:36:04Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    15,
                    36,
                    4,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.02213v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.02213v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Deploying 3D graph neural networks (GNNs) that are equivariant to 3D rotations (the group SO(3)) on edge devices is challenging due to their high computational cost. This paper addresses the problem by compressing and accelerating an SO(3)-equivariant GNN using low-bit quantization techniques. Specifically, we introduce three innovations for quantized equivariant transformers: (1) a magnitude-direction decoupled quantization scheme that separately quantizes the norm and orientation of equivariant (vector) features, (2) a branch-separated quantization-aware training strategy that treats invariant and equivariant feature channels differently in an attention-based $SO(3)$-GNN, and (3) a robustness-enhancing attention normalization mechanism that stabilizes low-precision attention computations. Experiments on the QM9 and rMD17 molecular benchmarks demonstrate that our 8-bit models achieve accuracy on energy and force predictions comparable to full-precision baselines with markedly improved efficiency. We also conduct ablation studies to quantify the contribution of each component to maintain accuracy and equivariance under quantization, using the Local error of equivariance (LEE) metric. The proposed techniques enable the deployment of symmetry-aware GNNs in practical chemistry applications with 2.37--2.73x faster inference and 4x smaller model size, without sacrificing accuracy or physical symmetry.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying 3D graph neural networks (GNNs) that are equivariant to 3D rotations (the group SO(3)) on edge devices is challenging due to their high computational cost. This paper addresses the problem by compressing and accelerating an SO(3)-equivariant GNN using low-bit quantization techniques. Specifically, we introduce three innovations for quantized equivariant transformers: (1) a magnitude-direction decoupled quantization scheme that separately quantizes the norm and orientation of equivariant (vector) features, (2) a branch-separated quantization-aware training strategy that treats invariant and equivariant feature channels differently in an attention-based $SO(3)$-GNN, and (3) a robustness-enhancing attention normalization mechanism that stabilizes low-precision attention computations. Experiments on the QM9 and rMD17 molecular benchmarks demonstrate that our 8-bit models achieve accuracy on energy and force predictions comparable to full-precision baselines with markedly improved efficiency. We also conduct ablation studies to quantify the contribution of each component to maintain accuracy and equivariance under quantization, using the Local error of equivariance (LEE) metric. The proposed techniques enable the deployment of symmetry-aware GNNs in practical chemistry applications with 2.37--2.73x faster inference and 4x smaller model size, without sacrificing accuracy or physical symmetry."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-05T15:36:04Z",
                "published_parsed": [
                    2026,
                    1,
                    5,
                    15,
                    36,
                    4,
                    0,
                    5,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Haoyu Zhou"
                    },
                    {
                        "name": "Ping Xue"
                    },
                    {
                        "name": "Tianfan Fu"
                    },
                    {
                        "name": "Hao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Zhang"
                },
                "author": "Hao Zhang"
            },
            {
                "id": "http://arxiv.org/abs/2601.02211v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.02211v1",
                "title": "Unraveling MMDiT Blocks: Training-free Analysis and Enhancement of Text-conditioned Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unraveling MMDiT Blocks: Training-free Analysis and Enhancement of Text-conditioned Diffusion"
                },
                "updated": "2026-01-05T15:32:53Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    15,
                    32,
                    53,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.02211v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.02211v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Recent breakthroughs of transformer-based diffusion models, particularly with Multimodal Diffusion Transformers (MMDiT) driven models like FLUX and Qwen Image, have facilitated thrilling experiences in text-to-image generation and editing. To understand the internal mechanism of MMDiT-based models, existing methods tried to analyze the effect of specific components like positional encoding and attention layers. Yet, a comprehensive understanding of how different blocks and their interactions with textual conditions contribute to the synthesis process remains elusive. In this paper, we first develop a systematic pipeline to comprehensively investigate each block's functionality by removing, disabling and enhancing textual hidden-states at corresponding blocks. Our analysis reveals that 1) semantic information appears in earlier blocks and finer details are rendered in later blocks, 2) removing specific blocks is usually less disruptive than disabling text conditions, and 3) enhancing textual conditions in selective blocks improves semantic attributes. Building on these observations, we further propose novel training-free strategies for improved text alignment, precise editing, and acceleration. Extensive experiments demonstrated that our method outperforms various baselines and remains flexible across text-to-image generation, image editing, and inference acceleration. Our method improves T2I-Combench++ from 56.92% to 63.00% and GenEval from 66.42% to 71.63% on SD3.5, without sacrificing synthesis quality. These results advance understanding of MMDiT models and provide valuable insights to unlock new possibilities for further improvements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent breakthroughs of transformer-based diffusion models, particularly with Multimodal Diffusion Transformers (MMDiT) driven models like FLUX and Qwen Image, have facilitated thrilling experiences in text-to-image generation and editing. To understand the internal mechanism of MMDiT-based models, existing methods tried to analyze the effect of specific components like positional encoding and attention layers. Yet, a comprehensive understanding of how different blocks and their interactions with textual conditions contribute to the synthesis process remains elusive. In this paper, we first develop a systematic pipeline to comprehensively investigate each block's functionality by removing, disabling and enhancing textual hidden-states at corresponding blocks. Our analysis reveals that 1) semantic information appears in earlier blocks and finer details are rendered in later blocks, 2) removing specific blocks is usually less disruptive than disabling text conditions, and 3) enhancing textual conditions in selective blocks improves semantic attributes. Building on these observations, we further propose novel training-free strategies for improved text alignment, precise editing, and acceleration. Extensive experiments demonstrated that our method outperforms various baselines and remains flexible across text-to-image generation, image editing, and inference acceleration. Our method improves T2I-Combench++ from 56.92% to 63.00% and GenEval from 66.42% to 71.63% on SD3.5, without sacrificing synthesis quality. These results advance understanding of MMDiT models and provide valuable insights to unlock new possibilities for further improvements."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-05T15:32:53Z",
                "published_parsed": [
                    2026,
                    1,
                    5,
                    15,
                    32,
                    53,
                    0,
                    5,
                    0
                ],
                "arxiv_comment": "11 pages",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Binglei Li"
                    },
                    {
                        "name": "Mengping Yang"
                    },
                    {
                        "name": "Zhiyu Tan"
                    },
                    {
                        "name": "Junping Zhang"
                    },
                    {
                        "name": "Hao Li"
                    }
                ],
                "author_detail": {
                    "name": "Hao Li"
                },
                "author": "Hao Li"
            },
            {
                "id": "http://arxiv.org/abs/2511.08873v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.08873v2",
                "title": "UCO: A Multi-Turn Interactive Reinforcement Learning Method for Adaptive Teaching with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UCO: A Multi-Turn Interactive Reinforcement Learning Method for Adaptive Teaching with Large Language Models"
                },
                "updated": "2026-01-05T15:26:03Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    15,
                    26,
                    3,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.08873v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.08873v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models (LLMs) are shifting from answer providers to intelligent tutors in educational settings, yet current supervised fine-tuning methods only learn surface teaching patterns without dynamic adaptation capabilities. Recent reinforcement learning approaches address this limitation but face two critical challenges. First, they evaluate teaching effectiveness solely based on whether students produce correct outputs, unable to distinguish whether students genuinely understand or echo teacher-provided answers during interaction. Second, they cannot perceive students' evolving cognitive states in real time through interactive dialogue, thus failing to adapt teaching strategies to match students' cognitive levels dynamically. We propose the Unidirectional Cognitive Optimization (UCO) method to address these challenges. UCO uses a multi-turn interactive reinforcement learning paradigm where the innovation lies in two synergistic reward functions: the Progress Reward captures students' cognitive advancement, evaluating whether students truly transition from confusion to comprehension, while the Scaffold Reward dynamically identifies each student's Zone of Proximal Development (ZPD), encouraging teachers to maintain productive teaching within this zone. We evaluate UCO by comparing it against 11 baseline models on BigMath and MathTutorBench benchmarks. Experimental results demonstrate that our UCO model outperforms all models of equivalent scale and achieves performance comparable to advanced closed-source models. The code and data are available at https://github.com/Mind-Lab-ECNU/UCO.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are shifting from answer providers to intelligent tutors in educational settings, yet current supervised fine-tuning methods only learn surface teaching patterns without dynamic adaptation capabilities. Recent reinforcement learning approaches address this limitation but face two critical challenges. First, they evaluate teaching effectiveness solely based on whether students produce correct outputs, unable to distinguish whether students genuinely understand or echo teacher-provided answers during interaction. Second, they cannot perceive students' evolving cognitive states in real time through interactive dialogue, thus failing to adapt teaching strategies to match students' cognitive levels dynamically. We propose the Unidirectional Cognitive Optimization (UCO) method to address these challenges. UCO uses a multi-turn interactive reinforcement learning paradigm where the innovation lies in two synergistic reward functions: the Progress Reward captures students' cognitive advancement, evaluating whether students truly transition from confusion to comprehension, while the Scaffold Reward dynamically identifies each student's Zone of Proximal Development (ZPD), encouraging teachers to maintain productive teaching within this zone. We evaluate UCO by comparing it against 11 baseline models on BigMath and MathTutorBench benchmarks. Experimental results demonstrate that our UCO model outperforms all models of equivalent scale and achieves performance comparable to advanced closed-source models. The code and data are available at https://github.com/Mind-Lab-ECNU/UCO."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-12T01:27:02Z",
                "published_parsed": [
                    2025,
                    11,
                    12,
                    1,
                    27,
                    2,
                    2,
                    316,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Shouang Wei"
                    },
                    {
                        "name": "Min Zhang"
                    },
                    {
                        "name": "Xin Lin"
                    },
                    {
                        "name": "Bo Jiang"
                    },
                    {
                        "name": "Kun Kuang"
                    },
                    {
                        "name": "Zhongxiang Dai"
                    }
                ],
                "author_detail": {
                    "name": "Zhongxiang Dai"
                },
                "author": "Zhongxiang Dai"
            },
            {
                "id": "http://arxiv.org/abs/2601.02201v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.02201v1",
                "title": "CORE: Code-based Inverse Self-Training Framework with Graph Expansion for Virtual Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CORE: Code-based Inverse Self-Training Framework with Graph Expansion for Virtual Agents"
                },
                "updated": "2026-01-05T15:24:05Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    15,
                    24,
                    5,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.02201v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.02201v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The development of Multimodal Virtual Agents has made significant progress through the integration of Multimodal Large Language Models. However, mainstream training paradigms face key challenges: Behavior Cloning is simple and effective through imitation but suffers from low behavioral diversity, while Reinforcement Learning is capable of discovering novel strategies through exploration but heavily relies on manually designed reward functions. To address the conflict between these two methods, we present CORE, a Code-based Inverse Self-Training Framework with Graph Expansion that bridges imitation and exploration, offering a novel training framework that promotes behavioral diversity while eliminating the reliance on manually reward design. Specifically, we introduce Semantic Code Abstraction to automatically infers reward functions from expert demonstrations without manual design. The inferred reward function, referred to as the Label Function, is executable code that verifies one key step within a task. Building on this, we propose Strategy Graph Expansion to enhance in-domain behavioral diversity, which constructs a multi-path graph called Strategy Graph that captures diverse valid solutions beyond expert demonstrations. Furthermore, we introduce Trajectory-Guided Extrapolation, which enriches out-of-domain behavioral diversity by utilizing both successful and failed trajectories to expand the task space. Experiments on Web and Android platforms demonstrate that CORE significantly improves both overall performance and generalization, highlighting its potential as a robust and generalizable training paradigm for building powerful virtual agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The development of Multimodal Virtual Agents has made significant progress through the integration of Multimodal Large Language Models. However, mainstream training paradigms face key challenges: Behavior Cloning is simple and effective through imitation but suffers from low behavioral diversity, while Reinforcement Learning is capable of discovering novel strategies through exploration but heavily relies on manually designed reward functions. To address the conflict between these two methods, we present CORE, a Code-based Inverse Self-Training Framework with Graph Expansion that bridges imitation and exploration, offering a novel training framework that promotes behavioral diversity while eliminating the reliance on manually reward design. Specifically, we introduce Semantic Code Abstraction to automatically infers reward functions from expert demonstrations without manual design. The inferred reward function, referred to as the Label Function, is executable code that verifies one key step within a task. Building on this, we propose Strategy Graph Expansion to enhance in-domain behavioral diversity, which constructs a multi-path graph called Strategy Graph that captures diverse valid solutions beyond expert demonstrations. Furthermore, we introduce Trajectory-Guided Extrapolation, which enriches out-of-domain behavioral diversity by utilizing both successful and failed trajectories to expand the task space. Experiments on Web and Android platforms demonstrate that CORE significantly improves both overall performance and generalization, highlighting its potential as a robust and generalizable training paradigm for building powerful virtual agents."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-05T15:24:05Z",
                "published_parsed": [
                    2026,
                    1,
                    5,
                    15,
                    24,
                    5,
                    0,
                    5,
                    0
                ],
                "arxiv_comment": "19 pages, 12 figures",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Keyu Wang"
                    },
                    {
                        "name": "Bingchen Miao"
                    },
                    {
                        "name": "Wendong Bu"
                    },
                    {
                        "name": "Yu Wu"
                    },
                    {
                        "name": "Juncheng Li"
                    },
                    {
                        "name": "Shengyu Zhang"
                    },
                    {
                        "name": "Wenqiao Zhang"
                    },
                    {
                        "name": "Siliang Tang"
                    },
                    {
                        "name": "Jun Xiao"
                    },
                    {
                        "name": "Yueting Zhuang"
                    }
                ],
                "author_detail": {
                    "name": "Yueting Zhuang"
                },
                "author": "Yueting Zhuang"
            },
            {
                "id": "http://arxiv.org/abs/2601.02200v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.02200v1",
                "title": "Code for Machines, Not Just Humans: Quantifying AI-Friendliness with Code Health Metrics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code for Machines, Not Just Humans: Quantifying AI-Friendliness with Code Health Metrics"
                },
                "updated": "2026-01-05T15:23:55Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    15,
                    23,
                    55,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.02200v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.02200v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We are entering a hybrid era in which human developers and AI coding agents work in the same codebases. While industry practice has long optimized code for human comprehension, it is increasingly important to ensure that LLMs with different capabilities can edit code reliably. In this study, we investigate the concept of ``AI-friendly code'' via LLM-based refactoring on a dataset of 5,000 Python files from competitive programming. We find a meaningful association between CodeHealth, a quality metric calibrated for human comprehension, and semantic preservation after AI refactoring. Our findings confirm that human-friendly code is also more compatible with AI tooling. These results suggest that organizations can use CodeHealth to guide where AI interventions are lower risk and where additional human oversight is warranted. Investing in maintainability not only helps humans; it also prepares for large-scale AI adoption.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We are entering a hybrid era in which human developers and AI coding agents work in the same codebases. While industry practice has long optimized code for human comprehension, it is increasingly important to ensure that LLMs with different capabilities can edit code reliably. In this study, we investigate the concept of ``AI-friendly code'' via LLM-based refactoring on a dataset of 5,000 Python files from competitive programming. We find a meaningful association between CodeHealth, a quality metric calibrated for human comprehension, and semantic preservation after AI refactoring. Our findings confirm that human-friendly code is also more compatible with AI tooling. These results suggest that organizations can use CodeHealth to guide where AI interventions are lower risk and where additional human oversight is warranted. Investing in maintainability not only helps humans; it also prepares for large-scale AI adoption."
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-05T15:23:55Z",
                "published_parsed": [
                    2026,
                    1,
                    5,
                    15,
                    23,
                    55,
                    0,
                    5,
                    0
                ],
                "arxiv_comment": "Accepted for the 3rd ACM International Conference on AI Foundation Models and Software Engineering (FORGE 2026)",
                "arxiv_primary_category": {
                    "term": "cs.SE"
                },
                "authors": [
                    {
                        "name": "Markus Borg"
                    },
                    {
                        "name": "Nadim Hagatulah"
                    },
                    {
                        "name": "Adam Tornhill"
                    },
                    {
                        "name": "Emma Söderberg"
                    }
                ],
                "author_detail": {
                    "name": "Emma Söderberg"
                },
                "author": "Emma Söderberg"
            },
            {
                "id": "http://arxiv.org/abs/2503.22458v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2503.22458v2",
                "title": "Evaluating LLM-based Agents for Multi-Turn Conversations: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating LLM-based Agents for Multi-Turn Conversations: A Survey"
                },
                "updated": "2026-01-05T15:14:04Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    15,
                    14,
                    4,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2503.22458v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2503.22458v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "This survey examines evaluation methods for large language model (LLM)-based agents in multi-turn conversational settings. Using a PRISMA-inspired framework, we systematically reviewed nearly 250 scholarly sources, capturing the state of the art from various venues of publication, and establishing a solid foundation for our analysis. Our study offers a structured approach by developing two interrelated taxonomy systems: one that defines \\emph{what to evaluate} and another that explains \\emph{how to evaluate}. The first taxonomy identifies key components of LLM-based agents for multi-turn conversations and their evaluation dimensions, including task completion, response quality, user experience, memory and context retention, as well as planning and tool integration. These components ensure that the performance of conversational agents is assessed in a holistic and meaningful manner. The second taxonomy system focuses on the evaluation methodologies. It categorizes approaches into annotation-based evaluations, automated metrics, hybrid strategies that combine human assessments with quantitative measures, and self-judging methods utilizing LLMs. This framework not only captures traditional metrics derived from language understanding, such as BLEU and ROUGE scores, but also incorporates advanced techniques that reflect the dynamic, interactive nature of multi-turn dialogues.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This survey examines evaluation methods for large language model (LLM)-based agents in multi-turn conversational settings. Using a PRISMA-inspired framework, we systematically reviewed nearly 250 scholarly sources, capturing the state of the art from various venues of publication, and establishing a solid foundation for our analysis. Our study offers a structured approach by developing two interrelated taxonomy systems: one that defines \\emph{what to evaluate} and another that explains \\emph{how to evaluate}. The first taxonomy identifies key components of LLM-based agents for multi-turn conversations and their evaluation dimensions, including task completion, response quality, user experience, memory and context retention, as well as planning and tool integration. These components ensure that the performance of conversational agents is assessed in a holistic and meaningful manner. The second taxonomy system focuses on the evaluation methodologies. It categorizes approaches into annotation-based evaluations, automated metrics, hybrid strategies that combine human assessments with quantitative measures, and self-judging methods utilizing LLMs. This framework not only captures traditional metrics derived from language understanding, such as BLEU and ROUGE scores, but also incorporates advanced techniques that reflect the dynamic, interactive nature of multi-turn dialogues."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-03-28T14:08:40Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    14,
                    8,
                    40,
                    4,
                    87,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Shengyue Guan"
                    },
                    {
                        "name": "Jindong Wang"
                    },
                    {
                        "name": "Jiang Bian"
                    },
                    {
                        "name": "Bin Zhu"
                    },
                    {
                        "name": "Jian-guang Lou"
                    },
                    {
                        "name": "Haoyi Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Haoyi Xiong"
                },
                "author": "Haoyi Xiong"
            },
            {
                "id": "http://arxiv.org/abs/2601.02186v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.02186v1",
                "title": "Toward Global Large Language Models in Medicine",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toward Global Large Language Models in Medicine"
                },
                "updated": "2026-01-05T15:05:49Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    15,
                    5,
                    49,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.02186v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.02186v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Despite continuous advances in medical technology, the global distribution of health care resources remains uneven. The development of large language models (LLMs) has transformed the landscape of medicine and holds promise for improving health care quality and expanding access to medical information globally. However, existing LLMs are primarily trained on high-resource languages, limiting their applicability in global medical scenarios. To address this gap, we constructed GlobMed, a large multilingual medical dataset, containing over 500,000 entries spanning 12 languages, including four low-resource languages. Building on this, we established GlobMed-Bench, which systematically assesses 56 state-of-the-art proprietary and open-weight LLMs across multiple multilingual medical tasks, revealing significant performance disparities across languages, particularly for low-resource languages. Additionally, we introduced GlobMed-LLMs, a suite of multilingual medical LLMs trained on GlobMed, with parameters ranging from 1.7B to 8B. GlobMed-LLMs achieved an average performance improvement of over 40% relative to baseline models, with a more than threefold increase in performance on low-resource languages. Together, these resources provide an important foundation for advancing the equitable development and application of LLMs globally, enabling broader language communities to benefit from technological advances.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite continuous advances in medical technology, the global distribution of health care resources remains uneven. The development of large language models (LLMs) has transformed the landscape of medicine and holds promise for improving health care quality and expanding access to medical information globally. However, existing LLMs are primarily trained on high-resource languages, limiting their applicability in global medical scenarios. To address this gap, we constructed GlobMed, a large multilingual medical dataset, containing over 500,000 entries spanning 12 languages, including four low-resource languages. Building on this, we established GlobMed-Bench, which systematically assesses 56 state-of-the-art proprietary and open-weight LLMs across multiple multilingual medical tasks, revealing significant performance disparities across languages, particularly for low-resource languages. Additionally, we introduced GlobMed-LLMs, a suite of multilingual medical LLMs trained on GlobMed, with parameters ranging from 1.7B to 8B. GlobMed-LLMs achieved an average performance improvement of over 40% relative to baseline models, with a more than threefold increase in performance on low-resource languages. Together, these resources provide an important foundation for advancing the equitable development and application of LLMs globally, enabling broader language communities to benefit from technological advances."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-05T15:05:49Z",
                "published_parsed": [
                    2026,
                    1,
                    5,
                    15,
                    5,
                    49,
                    0,
                    5,
                    0
                ],
                "arxiv_comment": "182 pages, 65 figures",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Rui Yang"
                    },
                    {
                        "name": "Huitao Li"
                    },
                    {
                        "name": "Weihao Xuan"
                    },
                    {
                        "name": "Heli Qi"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Kunyu Yu"
                    },
                    {
                        "name": "Yingjian Chen"
                    },
                    {
                        "name": "Rongrong Wang"
                    },
                    {
                        "name": "Jacques Behmoaras"
                    },
                    {
                        "name": "Tianxi Cai"
                    },
                    {
                        "name": "Bibhas Chakraborty"
                    },
                    {
                        "name": "Qingyu Chen"
                    },
                    {
                        "name": "Lionel Tim-Ee Cheng"
                    },
                    {
                        "name": "Marie-Louise Damwanza"
                    },
                    {
                        "name": "Chido Dzinotyiwei"
                    },
                    {
                        "name": "Aosong Feng"
                    },
                    {
                        "name": "Chuan Hong"
                    },
                    {
                        "name": "Yusuke Iwasawa"
                    },
                    {
                        "name": "Yuhe Ke"
                    },
                    {
                        "name": "Linah Kitala"
                    },
                    {
                        "name": "Taehoon Ko"
                    },
                    {
                        "name": "Jisan Lee"
                    },
                    {
                        "name": "Irene Li"
                    },
                    {
                        "name": "Jonathan Chong Kai Liew"
                    },
                    {
                        "name": "Hongfang Liu"
                    },
                    {
                        "name": "Lian Leng Low"
                    },
                    {
                        "name": "Edison Marrese-Taylor"
                    },
                    {
                        "name": "Yutaka Matsuo"
                    },
                    {
                        "name": "Isheanesu Misi"
                    },
                    {
                        "name": "Yilin Ning"
                    },
                    {
                        "name": "Jasmine Chiat Ling Ong"
                    },
                    {
                        "name": "Marcus Eng Hock Ong"
                    },
                    {
                        "name": "Enrico Petretto"
                    },
                    {
                        "name": "Hossein Rouhizadeh"
                    },
                    {
                        "name": "Abiram Sandralegar"
                    },
                    {
                        "name": "Oren Schreier"
                    },
                    {
                        "name": "Iain Bee Huat Tan"
                    },
                    {
                        "name": "Patrick Tan"
                    },
                    {
                        "name": "Daniel Shu Wei Ting"
                    },
                    {
                        "name": "Junjue Wang"
                    },
                    {
                        "name": "Chunhua Weng"
                    },
                    {
                        "name": "Matthew Yu Heng Wong"
                    },
                    {
                        "name": "Fang Wu"
                    },
                    {
                        "name": "Yunze Xiao"
                    },
                    {
                        "name": "Xuhai Xu"
                    },
                    {
                        "name": "Qingcheng Zeng"
                    },
                    {
                        "name": "Zhuo Zheng"
                    },
                    {
                        "name": "Yifan Peng"
                    },
                    {
                        "name": "Douglas Teodoro"
                    },
                    {
                        "name": "Nan Liu"
                    }
                ],
                "author_detail": {
                    "name": "Nan Liu"
                },
                "author": "Nan Liu"
            },
            {
                "id": "http://arxiv.org/abs/2601.02179v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.02179v1",
                "title": "Confidence Estimation for LLMs in Multi-turn Interactions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Confidence Estimation for LLMs in Multi-turn Interactions"
                },
                "updated": "2026-01-05T14:58:04Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    14,
                    58,
                    4,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.02179v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.02179v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "While confidence estimation is a promising direction for mitigating hallucinations in Large Language Models (LLMs), current research dominantly focuses on single-turn settings. The dynamics of model confidence in multi-turn conversations, where context accumulates and ambiguity is progressively resolved, remain largely unexplored. Reliable confidence estimation in multi-turn settings is critical for many downstream applications, such as autonomous agents and human-in-the-loop systems. This work presents the first systematic study of confidence estimation in multi-turn interactions, establishing a formal evaluation framework grounded in two key desiderata: per-turn calibration and monotonicity of confidence as more information becomes available. To facilitate this, we introduce novel metrics, including a length-normalized Expected Calibration Error (InfoECE), and a new \"Hinter-Guesser\" paradigm for generating controlled evaluation datasets. Our experiments reveal that widely-used confidence techniques struggle with calibration and monotonicity in multi-turn dialogues. We propose P(Sufficient), a logit-based probe that achieves comparatively better performance, although the task remains far from solved. Our work provides a foundational methodology for developing more reliable and trustworthy conversational agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While confidence estimation is a promising direction for mitigating hallucinations in Large Language Models (LLMs), current research dominantly focuses on single-turn settings. The dynamics of model confidence in multi-turn conversations, where context accumulates and ambiguity is progressively resolved, remain largely unexplored. Reliable confidence estimation in multi-turn settings is critical for many downstream applications, such as autonomous agents and human-in-the-loop systems. This work presents the first systematic study of confidence estimation in multi-turn interactions, establishing a formal evaluation framework grounded in two key desiderata: per-turn calibration and monotonicity of confidence as more information becomes available. To facilitate this, we introduce novel metrics, including a length-normalized Expected Calibration Error (InfoECE), and a new \"Hinter-Guesser\" paradigm for generating controlled evaluation datasets. Our experiments reveal that widely-used confidence techniques struggle with calibration and monotonicity in multi-turn dialogues. We propose P(Sufficient), a logit-based probe that achieves comparatively better performance, although the task remains far from solved. Our work provides a foundational methodology for developing more reliable and trustworthy conversational agents."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-05T14:58:04Z",
                "published_parsed": [
                    2026,
                    1,
                    5,
                    14,
                    58,
                    4,
                    0,
                    5,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Caiqi Zhang"
                    },
                    {
                        "name": "Ruihan Yang"
                    },
                    {
                        "name": "Xiaochen Zhu"
                    },
                    {
                        "name": "Chengzu Li"
                    },
                    {
                        "name": "Tiancheng Hu"
                    },
                    {
                        "name": "Yijiang River Dong"
                    },
                    {
                        "name": "Deqing Yang"
                    },
                    {
                        "name": "Nigel Collier"
                    }
                ],
                "author_detail": {
                    "name": "Nigel Collier"
                },
                "author": "Nigel Collier"
            },
            {
                "id": "http://arxiv.org/abs/2510.04953v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.04953v2",
                "title": "Euclid preparation. LXXXV. Toward a DR1 application of higher-order weak lensing statistics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Euclid preparation. LXXXV. Toward a DR1 application of higher-order weak lensing statistics"
                },
                "updated": "2026-01-05T14:57:47Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    14,
                    57,
                    47,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.04953v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.04953v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.1051/0004-6361/202557573",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "This is the second paper in the HOWLS (higher-order weak lensing statistics) series exploring the usage of non-Gaussian statistics for cosmology inference within Euclid. With respect to our first paper, we develop a full tomographic analysis based on realistic photometric redshifts that allows us to derive Fisher forecasts in the ($σ_8$, $w_0$) plane for a Euclid-like data release 1 (DR1) setup. We find that the five higher-order statistics (HOS) that satisfy the Gaussian likelihood assumption of the Fisher formalism (one-point probability distribution function, $\\ell$1-norm, peak counts, Minkowski functionals, and Betti numbers) each outperform the shear two-point correlation functions by a factor of $2.5$ on the $w_0$ forecasts, with only marginal improvement when used in combination with two-point estimators, suggesting that every HOS is able to retrieve both the non-Gaussian and Gaussian information of the matter density field. The similar performance of the different estimators is explained by a homogeneous use of multi-scale and tomographic information, optimized to lower computational costs. These results hold for the three mass mapping techniques of the Euclid pipeline, aperture mass, Kaiser--Squires, and Kaiser--Squires plus, and they are unaffected by the application of realistic star masks. Finally, we explored the use of HOS with the Bernardeau--Nishimichi--Taruya (BNT) nulling scheme approach, finding promising results toward applying physical scale cuts to HOS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This is the second paper in the HOWLS (higher-order weak lensing statistics) series exploring the usage of non-Gaussian statistics for cosmology inference within Euclid. With respect to our first paper, we develop a full tomographic analysis based on realistic photometric redshifts that allows us to derive Fisher forecasts in the ($σ_8$, $w_0$) plane for a Euclid-like data release 1 (DR1) setup. We find that the five higher-order statistics (HOS) that satisfy the Gaussian likelihood assumption of the Fisher formalism (one-point probability distribution function, $\\ell$1-norm, peak counts, Minkowski functionals, and Betti numbers) each outperform the shear two-point correlation functions by a factor of $2.5$ on the $w_0$ forecasts, with only marginal improvement when used in combination with two-point estimators, suggesting that every HOS is able to retrieve both the non-Gaussian and Gaussian information of the matter density field. The similar performance of the different estimators is explained by a homogeneous use of multi-scale and tomographic information, optimized to lower computational costs. These results hold for the three mass mapping techniques of the Euclid pipeline, aperture mass, Kaiser--Squires, and Kaiser--Squires plus, and they are unaffected by the application of realistic star masks. Finally, we explored the use of HOS with the Bernardeau--Nishimichi--Taruya (BNT) nulling scheme approach, finding promising results toward applying physical scale cuts to HOS."
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-06T15:52:32Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    15,
                    52,
                    32,
                    0,
                    279,
                    0
                ],
                "arxiv_comment": "Accepted by A&A on 15/12/2025",
                "arxiv_primary_category": {
                    "term": "astro-ph.CO"
                },
                "authors": [
                    {
                        "name": "Euclid Collaboration"
                    },
                    {
                        "name": "S. Vinciguerra"
                    },
                    {
                        "name": "F. Bouchè"
                    },
                    {
                        "name": "N. Martinet"
                    },
                    {
                        "name": "L. Castiblanco"
                    },
                    {
                        "name": "C. Uhlemann"
                    },
                    {
                        "name": "S. Pires"
                    },
                    {
                        "name": "J. Harnois-Déraps"
                    },
                    {
                        "name": "C. Giocoli"
                    },
                    {
                        "name": "M. Baldi"
                    },
                    {
                        "name": "V. F. Cardone"
                    },
                    {
                        "name": "A. Vadalà"
                    },
                    {
                        "name": "N. Dagoneau"
                    },
                    {
                        "name": "L. Linke"
                    },
                    {
                        "name": "E. Sellentin"
                    },
                    {
                        "name": "P. L. Taylor"
                    },
                    {
                        "name": "J. C. Broxterman"
                    },
                    {
                        "name": "S. Heydenreich"
                    },
                    {
                        "name": "V. Tinnaneri Sreekanth"
                    },
                    {
                        "name": "N. Porqueres"
                    },
                    {
                        "name": "L. Porth"
                    },
                    {
                        "name": "M. Gatti"
                    },
                    {
                        "name": "D. Grandón"
                    },
                    {
                        "name": "A. Barthelemy"
                    },
                    {
                        "name": "F. Bernardeau"
                    },
                    {
                        "name": "A. Tersenov"
                    },
                    {
                        "name": "H. Hoekstra"
                    },
                    {
                        "name": "J. -L. Starck"
                    },
                    {
                        "name": "S. Cheng"
                    },
                    {
                        "name": "P. A. Burger"
                    },
                    {
                        "name": "I. Tereno"
                    },
                    {
                        "name": "R. Scaramella"
                    },
                    {
                        "name": "B. Altieri"
                    },
                    {
                        "name": "S. Andreon"
                    },
                    {
                        "name": "N. Auricchio"
                    },
                    {
                        "name": "C. Baccigalupi"
                    },
                    {
                        "name": "S. Bardelli"
                    },
                    {
                        "name": "A. Biviano"
                    },
                    {
                        "name": "E. Branchini"
                    },
                    {
                        "name": "M. Brescia"
                    },
                    {
                        "name": "S. Camera"
                    },
                    {
                        "name": "G. Cañas-Herrera"
                    },
                    {
                        "name": "V. Capobianco"
                    },
                    {
                        "name": "C. Carbone"
                    },
                    {
                        "name": "J. Carretero"
                    },
                    {
                        "name": "M. Castellano"
                    },
                    {
                        "name": "G. Castignani"
                    },
                    {
                        "name": "S. Cavuoti"
                    },
                    {
                        "name": "K. C. Chambers"
                    },
                    {
                        "name": "A. Cimatti"
                    },
                    {
                        "name": "C. Colodro-Conde"
                    },
                    {
                        "name": "G. Congedo"
                    },
                    {
                        "name": "L. Conversi"
                    },
                    {
                        "name": "Y. Copin"
                    },
                    {
                        "name": "F. Courbin"
                    },
                    {
                        "name": "H. M. Courtois"
                    },
                    {
                        "name": "M. Cropper"
                    },
                    {
                        "name": "A. Da Silva"
                    },
                    {
                        "name": "H. Degaudenzi"
                    },
                    {
                        "name": "S. de la Torre"
                    },
                    {
                        "name": "G. De Lucia"
                    },
                    {
                        "name": "H. Dole"
                    },
                    {
                        "name": "F. Dubath"
                    },
                    {
                        "name": "X. Dupac"
                    },
                    {
                        "name": "S. Dusini"
                    },
                    {
                        "name": "S. Escoffier"
                    },
                    {
                        "name": "M. Farina"
                    },
                    {
                        "name": "R. Farinelli"
                    },
                    {
                        "name": "S. Farrens"
                    },
                    {
                        "name": "F. Faustini"
                    },
                    {
                        "name": "S. Ferriol"
                    },
                    {
                        "name": "F. Finelli"
                    },
                    {
                        "name": "M. Frailis"
                    },
                    {
                        "name": "E. Franceschi"
                    },
                    {
                        "name": "M. Fumana"
                    },
                    {
                        "name": "S. Galeotta"
                    },
                    {
                        "name": "K. George"
                    },
                    {
                        "name": "B. Gillis"
                    },
                    {
                        "name": "J. Gracia-Carpio"
                    },
                    {
                        "name": "A. Grazian"
                    },
                    {
                        "name": "F. Grupp"
                    },
                    {
                        "name": "S. V. H. Haugan"
                    },
                    {
                        "name": "W. Holmes"
                    },
                    {
                        "name": "F. Hormuth"
                    },
                    {
                        "name": "A. Hornstrup"
                    },
                    {
                        "name": "P. Hudelot"
                    },
                    {
                        "name": "K. Jahnke"
                    },
                    {
                        "name": "M. Jhabvala"
                    },
                    {
                        "name": "B. Joachimi"
                    },
                    {
                        "name": "E. Keihänen"
                    },
                    {
                        "name": "S. Kermiche"
                    },
                    {
                        "name": "A. Kiessling"
                    },
                    {
                        "name": "M. Kilbinger"
                    },
                    {
                        "name": "B. Kubik"
                    },
                    {
                        "name": "M. Kunz"
                    },
                    {
                        "name": "H. Kurki-Suonio"
                    },
                    {
                        "name": "A. M. C. Le Brun"
                    },
                    {
                        "name": "S. Ligori"
                    },
                    {
                        "name": "P. B. Lilje"
                    },
                    {
                        "name": "V. Lindholm"
                    },
                    {
                        "name": "I. Lloro"
                    },
                    {
                        "name": "G. Mainetti"
                    },
                    {
                        "name": "D. Maino"
                    },
                    {
                        "name": "O. Mansutti"
                    },
                    {
                        "name": "O. Marggraf"
                    },
                    {
                        "name": "M. Martinelli"
                    },
                    {
                        "name": "F. Marulli"
                    },
                    {
                        "name": "R. J. Massey"
                    },
                    {
                        "name": "E. Medinaceli"
                    },
                    {
                        "name": "S. Mei"
                    },
                    {
                        "name": "M. Melchior"
                    },
                    {
                        "name": "Y. Mellier"
                    },
                    {
                        "name": "M. Meneghetti"
                    },
                    {
                        "name": "G. Meylan"
                    },
                    {
                        "name": "A. Mora"
                    },
                    {
                        "name": "M. Moresco"
                    },
                    {
                        "name": "L. Moscardini"
                    },
                    {
                        "name": "C. Neissner"
                    },
                    {
                        "name": "S. -M. Niemi"
                    },
                    {
                        "name": "C. Padilla"
                    },
                    {
                        "name": "S. Paltani"
                    },
                    {
                        "name": "F. Pasian"
                    },
                    {
                        "name": "K. Pedersen"
                    },
                    {
                        "name": "V. Pettorino"
                    },
                    {
                        "name": "G. Polenta"
                    },
                    {
                        "name": "M. Poncet"
                    },
                    {
                        "name": "L. A. Popa"
                    },
                    {
                        "name": "F. Raison"
                    },
                    {
                        "name": "A. Renzi"
                    },
                    {
                        "name": "J. Rhodes"
                    },
                    {
                        "name": "G. Riccio"
                    },
                    {
                        "name": "E. Romelli"
                    },
                    {
                        "name": "M. Roncarelli"
                    },
                    {
                        "name": "R. Saglia"
                    },
                    {
                        "name": "Z. Sakr"
                    },
                    {
                        "name": "A. G. Sánchez"
                    },
                    {
                        "name": "D. Sapone"
                    },
                    {
                        "name": "B. Sartoris"
                    },
                    {
                        "name": "P. Schneider"
                    },
                    {
                        "name": "T. Schrabback"
                    },
                    {
                        "name": "A. Secroun"
                    },
                    {
                        "name": "G. Seidel"
                    },
                    {
                        "name": "S. Serrano"
                    },
                    {
                        "name": "C. Sirignano"
                    },
                    {
                        "name": "G. Sirri"
                    },
                    {
                        "name": "A. Spurio Mancini"
                    },
                    {
                        "name": "L. Stanco"
                    },
                    {
                        "name": "J. Steinwagner"
                    },
                    {
                        "name": "P. Tallada-Crespí"
                    },
                    {
                        "name": "A. N. Taylor"
                    },
                    {
                        "name": "N. Tessore"
                    },
                    {
                        "name": "S. Toft"
                    },
                    {
                        "name": "R. Toledo-Moreo"
                    },
                    {
                        "name": "F. Torradeflot"
                    },
                    {
                        "name": "I. Tutusaus"
                    },
                    {
                        "name": "J. Valiviita"
                    },
                    {
                        "name": "T. Vassallo"
                    },
                    {
                        "name": "Y. Wang"
                    },
                    {
                        "name": "J. Weller"
                    },
                    {
                        "name": "A. Zacchei"
                    },
                    {
                        "name": "G. Zamorani"
                    },
                    {
                        "name": "F. M. Zerbi"
                    },
                    {
                        "name": "E. Zucca"
                    },
                    {
                        "name": "M. Ballardini"
                    },
                    {
                        "name": "M. Bolzonella"
                    },
                    {
                        "name": "A. Boucaud"
                    },
                    {
                        "name": "E. Bozzo"
                    },
                    {
                        "name": "C. Burigana"
                    },
                    {
                        "name": "R. Cabanac"
                    },
                    {
                        "name": "M. Calabrese"
                    },
                    {
                        "name": "A. Cappi"
                    },
                    {
                        "name": "J. A. Escartin Vigo"
                    },
                    {
                        "name": "L. Gabarra"
                    },
                    {
                        "name": "W. G. Hartley"
                    },
                    {
                        "name": "R. Maoli"
                    },
                    {
                        "name": "J. Martín-Fleitas"
                    },
                    {
                        "name": "S. Matthew"
                    },
                    {
                        "name": "N. Mauri"
                    },
                    {
                        "name": "R. B. Metcalf"
                    },
                    {
                        "name": "A. Pezzotta"
                    },
                    {
                        "name": "M. Pöntinen"
                    },
                    {
                        "name": "I. Risso"
                    },
                    {
                        "name": "V. Scottez"
                    },
                    {
                        "name": "M. Sereno"
                    },
                    {
                        "name": "M. Tenti"
                    },
                    {
                        "name": "M. Viel"
                    },
                    {
                        "name": "M. Wiesmann"
                    },
                    {
                        "name": "Y. Akrami"
                    },
                    {
                        "name": "I. T. Andika"
                    },
                    {
                        "name": "R. E. Angulo"
                    },
                    {
                        "name": "S. Anselmi"
                    },
                    {
                        "name": "M. Archidiacono"
                    },
                    {
                        "name": "F. Atrio-Barandela"
                    },
                    {
                        "name": "E. Aubourg"
                    },
                    {
                        "name": "D. Bertacca"
                    },
                    {
                        "name": "M. Bethermin"
                    },
                    {
                        "name": "A. Blanchard"
                    },
                    {
                        "name": "L. Blot"
                    },
                    {
                        "name": "M. Bonici"
                    },
                    {
                        "name": "S. Borgani"
                    },
                    {
                        "name": "M. L. Brown"
                    },
                    {
                        "name": "S. Bruton"
                    },
                    {
                        "name": "A. Calabro"
                    },
                    {
                        "name": "B. Camacho Quevedo"
                    },
                    {
                        "name": "F. Caro"
                    },
                    {
                        "name": "C. S. Carvalho"
                    },
                    {
                        "name": "T. Castro"
                    },
                    {
                        "name": "F. Cogato"
                    },
                    {
                        "name": "S. Conseil"
                    },
                    {
                        "name": "A. R. Cooray"
                    },
                    {
                        "name": "G. Desprez"
                    },
                    {
                        "name": "A. Díaz-Sánchez"
                    },
                    {
                        "name": "J. J. Diaz"
                    },
                    {
                        "name": "S. Di Domizio"
                    },
                    {
                        "name": "J. M. Diego"
                    },
                    {
                        "name": "M. Y. Elkhashab"
                    },
                    {
                        "name": "Y. Fang"
                    },
                    {
                        "name": "P. G. Ferreira"
                    },
                    {
                        "name": "A. Finoguenov"
                    },
                    {
                        "name": "A. Franco"
                    },
                    {
                        "name": "K. Ganga"
                    },
                    {
                        "name": "J. García-Bellido"
                    },
                    {
                        "name": "T. Gasparetto"
                    },
                    {
                        "name": "V. Gautard"
                    },
                    {
                        "name": "R. Gavazzi"
                    },
                    {
                        "name": "E. Gaztanaga"
                    },
                    {
                        "name": "F. Giacomini"
                    },
                    {
                        "name": "F. Gianotti"
                    },
                    {
                        "name": "G. Gozaliasl"
                    },
                    {
                        "name": "M. Guidi"
                    },
                    {
                        "name": "C. M. Gutierrez"
                    },
                    {
                        "name": "A. Hall"
                    },
                    {
                        "name": "S. Hemmati"
                    },
                    {
                        "name": "H. Hildebrandt"
                    },
                    {
                        "name": "J. Hjorth"
                    },
                    {
                        "name": "J. J. E. Kajava"
                    },
                    {
                        "name": "Y. Kang"
                    },
                    {
                        "name": "D. Karagiannis"
                    },
                    {
                        "name": "K. Kiiveri"
                    },
                    {
                        "name": "J. Kim"
                    },
                    {
                        "name": "C. C. Kirkpatrick"
                    },
                    {
                        "name": "S. Kruk"
                    },
                    {
                        "name": "L. Legrand"
                    },
                    {
                        "name": "M. Lembo"
                    },
                    {
                        "name": "F. Lepori"
                    },
                    {
                        "name": "G. Leroy"
                    },
                    {
                        "name": "G. F. Lesci"
                    },
                    {
                        "name": "J. Lesgourgues"
                    },
                    {
                        "name": "T. I. Liaudat"
                    },
                    {
                        "name": "J. Macias-Perez"
                    },
                    {
                        "name": "M. Magliocchetti"
                    },
                    {
                        "name": "F. Mannucci"
                    },
                    {
                        "name": "C. J. A. P. Martins"
                    },
                    {
                        "name": "L. Maurin"
                    },
                    {
                        "name": "M. Miluzio"
                    },
                    {
                        "name": "P. Monaco"
                    },
                    {
                        "name": "C. Moretti"
                    },
                    {
                        "name": "G. Morgante"
                    },
                    {
                        "name": "S. Nadathur"
                    },
                    {
                        "name": "K. Naidoo"
                    },
                    {
                        "name": "A. Navarro-Alsina"
                    },
                    {
                        "name": "S. Nesseris"
                    },
                    {
                        "name": "D. Paoletti"
                    },
                    {
                        "name": "F. Passalacqua"
                    },
                    {
                        "name": "K. Paterson"
                    },
                    {
                        "name": "L. Patrizii"
                    },
                    {
                        "name": "A. Pisani"
                    },
                    {
                        "name": "D. Potter"
                    },
                    {
                        "name": "S. Quai"
                    },
                    {
                        "name": "M. Radovich"
                    },
                    {
                        "name": "S. Sacquegna"
                    },
                    {
                        "name": "M. Sahlén"
                    },
                    {
                        "name": "D. B. Sanders"
                    },
                    {
                        "name": "E. Sarpa"
                    },
                    {
                        "name": "A. Schneider"
                    },
                    {
                        "name": "D. Sciotti"
                    },
                    {
                        "name": "L. C. Smith"
                    },
                    {
                        "name": "K. Tanidis"
                    },
                    {
                        "name": "C. Tao"
                    },
                    {
                        "name": "G. Testera"
                    },
                    {
                        "name": "R. Teyssier"
                    },
                    {
                        "name": "S. Tosi"
                    },
                    {
                        "name": "A. Troja"
                    },
                    {
                        "name": "M. Tucci"
                    },
                    {
                        "name": "D. Vergani"
                    },
                    {
                        "name": "G. Verza"
                    },
                    {
                        "name": "N. A. Walton"
                    }
                ],
                "author_detail": {
                    "name": "N. A. Walton"
                },
                "author": "N. A. Walton",
                "arxiv_affiliation": "Institute of Astronomy, University of Cambridge, Madingley Road, Cambridge CB3 0HA, UK",
                "arxiv_doi": "10.1051/0004-6361/202557573"
            },
            {
                "id": "http://arxiv.org/abs/2601.02163v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.02163v1",
                "title": "EverMemOS: A Self-Organizing Memory Operating System for Structured Long-Horizon Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EverMemOS: A Self-Organizing Memory Operating System for Structured Long-Horizon Reasoning"
                },
                "updated": "2026-01-05T14:39:43Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    14,
                    39,
                    43,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.02163v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.02163v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) are increasingly deployed as long-term interactive agents, yet their limited context windows make it difficult to sustain coherent behavior over extended interactions. Existing memory systems often store isolated records and retrieve fragments, limiting their ability to consolidate evolving user states and resolve conflicts. We introduce EverMemOS, a self-organizing memory operating system that implements an engram-inspired lifecycle for computational memory. Episodic Trace Formation converts dialogue streams into MemCells that capture episodic traces, atomic facts, and time-bounded Foresight signals. Semantic Consolidation organizes MemCells into thematic MemScenes, distilling stable semantic structures and updating user profiles. Reconstructive Recollection performs MemScene-guided agentic retrieval to compose the necessary and sufficient context for downstream reasoning. Experiments on LoCoMo and LongMemEval show that EverMemOS achieves state-of-the-art performance on memory-augmented reasoning tasks. We further report a profile study on PersonaMem v2 and qualitative case studies illustrating chat-oriented capabilities such as user profiling and Foresight. Code is available at https://github.com/EverMind-AI/EverMemOS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly deployed as long-term interactive agents, yet their limited context windows make it difficult to sustain coherent behavior over extended interactions. Existing memory systems often store isolated records and retrieve fragments, limiting their ability to consolidate evolving user states and resolve conflicts. We introduce EverMemOS, a self-organizing memory operating system that implements an engram-inspired lifecycle for computational memory. Episodic Trace Formation converts dialogue streams into MemCells that capture episodic traces, atomic facts, and time-bounded Foresight signals. Semantic Consolidation organizes MemCells into thematic MemScenes, distilling stable semantic structures and updating user profiles. Reconstructive Recollection performs MemScene-guided agentic retrieval to compose the necessary and sufficient context for downstream reasoning. Experiments on LoCoMo and LongMemEval show that EverMemOS achieves state-of-the-art performance on memory-augmented reasoning tasks. We further report a profile study on PersonaMem v2 and qualitative case studies illustrating chat-oriented capabilities such as user profiling and Foresight. Code is available at https://github.com/EverMind-AI/EverMemOS."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-05T14:39:43Z",
                "published_parsed": [
                    2026,
                    1,
                    5,
                    14,
                    39,
                    43,
                    0,
                    5,
                    0
                ],
                "arxiv_comment": "16 pages, 6 figures, 12 tables. Code available at https://github.com/EverMind-AI/EverMemOS",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Chuanrui Hu"
                    },
                    {
                        "name": "Xingze Gao"
                    },
                    {
                        "name": "Zuyi Zhou"
                    },
                    {
                        "name": "Dannong Xu"
                    },
                    {
                        "name": "Yi Bai"
                    },
                    {
                        "name": "Xintong Li"
                    },
                    {
                        "name": "Hui Zhang"
                    },
                    {
                        "name": "Tong Li"
                    },
                    {
                        "name": "Chong Zhang"
                    },
                    {
                        "name": "Lidong Bing"
                    },
                    {
                        "name": "Yafeng Deng"
                    }
                ],
                "author_detail": {
                    "name": "Yafeng Deng"
                },
                "author": "Yafeng Deng"
            },
            {
                "id": "http://arxiv.org/abs/2601.02141v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.02141v1",
                "title": "Efficient Unrolled Networks for Large-Scale 3D Inverse Problems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Unrolled Networks for Large-Scale 3D Inverse Problems"
                },
                "updated": "2026-01-05T14:12:43Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    14,
                    12,
                    43,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.02141v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.02141v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Deep learning-based methods have revolutionized the field of imaging inverse problems, yielding state-of-the-art performance across various imaging domains. The best performing networks incorporate the imaging operator within the network architecture, typically in the form of deep unrolling. However, in large-scale problems, such as 3D imaging, most existing methods fail to incorporate the operator in the architecture due to the prohibitive amount of memory required by global forward operators, which hinder typical patching strategies. In this work, we present a domain partitioning strategy and normal operator approximations that enable the training of end-to-end reconstruction models incorporating forward operators of arbitrarily large problems into their architecture. The proposed method achieves state-of-the-art performance on 3D X-ray cone-beam tomography and 3D multi-coil accelerated MRI, while requiring only a single GPU for both training and inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning-based methods have revolutionized the field of imaging inverse problems, yielding state-of-the-art performance across various imaging domains. The best performing networks incorporate the imaging operator within the network architecture, typically in the form of deep unrolling. However, in large-scale problems, such as 3D imaging, most existing methods fail to incorporate the operator in the architecture due to the prohibitive amount of memory required by global forward operators, which hinder typical patching strategies. In this work, we present a domain partitioning strategy and normal operator approximations that enable the training of end-to-end reconstruction models incorporating forward operators of arbitrarily large problems into their architecture. The proposed method achieves state-of-the-art performance on 3D X-ray cone-beam tomography and 3D multi-coil accelerated MRI, while requiring only a single GPU for both training and inference."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-05T14:12:43Z",
                "published_parsed": [
                    2026,
                    1,
                    5,
                    14,
                    12,
                    43,
                    0,
                    5,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Romain Vo"
                    },
                    {
                        "name": "Julián Tachella"
                    }
                ],
                "author_detail": {
                    "name": "Julián Tachella"
                },
                "author": "Julián Tachella"
            },
            {
                "id": "http://arxiv.org/abs/2504.11320v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2504.11320v2",
                "title": "Optimizing LLM Inference: Fluid-Guided Online Scheduling with Memory Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing LLM Inference: Fluid-Guided Online Scheduling with Memory Constraints"
                },
                "updated": "2026-01-05T14:10:45Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    14,
                    10,
                    45,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2504.11320v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2504.11320v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) power many modern applications, but their inference procedure poses unique scheduling challenges: the Key-Value (KV) cache grows dynamically during response generation, and memory overflow triggers eviction that can cascade into system-wide failures. Even when memory capacity exceeds the theoretical requirement, conventional scheduling algorithms fail because they do not account for this dynamic memory growth -- a system that should be stable can become unstable under poor scheduling.\n  This paper formulates LLM inference optimization as a multi-stage online scheduling problem. We develop a fluid dynamics approximation to establish a tractable benchmark and derive the Waiting for Accumulated Inference Threshold (WAIT) algorithm. WAIT uses threshold-based batching to prevent eviction by keeping the system near load balance, achieving near-optimal throughput when output lengths are known.\n  For practical settings where output lengths are unknown at arrival, we introduce Nested WAIT. Rather than predicting output lengths, Nested WAIT classifies prompts on-the-fly: short prompts complete early and exit, while longer prompts naturally advance to later segments. A safety buffer provides high-probability protection against memory overflow with only logarithmic overhead.\n  Theoretical analysis establishes near-optimal performance in the asymptotic regime. Experiments on Llama-7B with an A100 GPU demonstrate that our approach achieves superior throughput and reduced latency compared to vLLM and Sarathi. This work applies operations research principles to establish a theoretical framework for LLM deployment under memory constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) power many modern applications, but their inference procedure poses unique scheduling challenges: the Key-Value (KV) cache grows dynamically during response generation, and memory overflow triggers eviction that can cascade into system-wide failures. Even when memory capacity exceeds the theoretical requirement, conventional scheduling algorithms fail because they do not account for this dynamic memory growth -- a system that should be stable can become unstable under poor scheduling.\n  This paper formulates LLM inference optimization as a multi-stage online scheduling problem. We develop a fluid dynamics approximation to establish a tractable benchmark and derive the Waiting for Accumulated Inference Threshold (WAIT) algorithm. WAIT uses threshold-based batching to prevent eviction by keeping the system near load balance, achieving near-optimal throughput when output lengths are known.\n  For practical settings where output lengths are unknown at arrival, we introduce Nested WAIT. Rather than predicting output lengths, Nested WAIT classifies prompts on-the-fly: short prompts complete early and exit, while longer prompts naturally advance to later segments. A safety buffer provides high-probability protection against memory overflow with only logarithmic overhead.\n  Theoretical analysis establishes near-optimal performance in the asymptotic regime. Experiments on Llama-7B with an A100 GPU demonstrate that our approach achieves superior throughput and reduced latency compared to vLLM and Sarathi. This work applies operations research principles to establish a theoretical framework for LLM deployment under memory constraints."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-04-15T16:00:21Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    16,
                    0,
                    21,
                    1,
                    105,
                    0
                ],
                "arxiv_comment": "49 pages, 18 figures",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Ruicheng Ao"
                    },
                    {
                        "name": "Gan Luo"
                    },
                    {
                        "name": "David Simchi-Levi"
                    },
                    {
                        "name": "Xinshang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xinshang Wang"
                },
                "author": "Xinshang Wang"
            },
            {
                "id": "http://arxiv.org/abs/2510.18039v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.18039v2",
                "title": "Presenting Large Language Models as Companions Affects What Mental Capacities People Attribute to Them",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Presenting Large Language Models as Companions Affects What Mental Capacities People Attribute to Them"
                },
                "updated": "2026-01-05T14:03:10Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    14,
                    3,
                    10,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.18039v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.18039v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "How might messages about large language models (LLMs) found in public discourse influence the way people think about and interact with these models? To explore this question, we randomly assigned participants (N = 470) to watch short informational videos presenting LLMs as either machines, tools, or companions -- or to watch no video. We then assessed how strongly they believed LLMs to possess various mental capacities, such as the ability have intentions or remember things. We found that participants who watched video messages presenting LLMs as companions reported believing that LLMs more fully possessed these capacities than did participants in other groups. In a follow-up study (N = 604), we replicated these findings and found nuanced effects on how these videos also impact people's reliance on LLM-generated responses when seeking out factual information. Together, these studies suggest that messages about LLMs -- beyond technical advances -- may shape what people believe about these systems and how they rely on LLM-generated responses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How might messages about large language models (LLMs) found in public discourse influence the way people think about and interact with these models? To explore this question, we randomly assigned participants (N = 470) to watch short informational videos presenting LLMs as either machines, tools, or companions -- or to watch no video. We then assessed how strongly they believed LLMs to possess various mental capacities, such as the ability have intentions or remember things. We found that participants who watched video messages presenting LLMs as companions reported believing that LLMs more fully possessed these capacities than did participants in other groups. In a follow-up study (N = 604), we replicated these findings and found nuanced effects on how these videos also impact people's reliance on LLM-generated responses when seeking out factual information. Together, these studies suggest that messages about LLMs -- beyond technical advances -- may shape what people believe about these systems and how they rely on LLM-generated responses."
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-20T19:25:24Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    19,
                    25,
                    24,
                    0,
                    293,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC"
                },
                "authors": [
                    {
                        "name": "Allison Chen"
                    },
                    {
                        "name": "Sunnie S. Y. Kim"
                    },
                    {
                        "name": "Angel Franyutti"
                    },
                    {
                        "name": "Amaya Dharmasiri"
                    },
                    {
                        "name": "Kushin Mukherjee"
                    },
                    {
                        "name": "Olga Russakovsky"
                    },
                    {
                        "name": "Judith E. Fan"
                    }
                ],
                "author_detail": {
                    "name": "Judith E. Fan"
                },
                "author": "Judith E. Fan"
            },
            {
                "id": "http://arxiv.org/abs/2509.14704v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2509.14704v2",
                "title": "Japanese Children's Riddles as a Benchmark for Machine Insight and Metacognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Japanese Children's Riddles as a Benchmark for Machine Insight and Metacognition"
                },
                "updated": "2026-01-05T13:57:38Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    13,
                    57,
                    38,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2509.14704v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2509.14704v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Benchmark saturation and contamination have obscured genuine advances in reasoning for large language models (LLMs). We introduce NazoNazo Benchmark, a low-cost, renewable test built from Japanese children's riddles that demand insight-based reasoning, or representational shifts rather than knowledge recall. We evaluate 38 frontier LLMs (2023-2025) on 201 riddles and a 120-item human-comparison subset, finding that non-reasoning models average 7.6%, reasoning models 17.6%, and humans ~53% accuracy. Importantly, thought-log analysis reveals that reasoning in Japanese did not necessarily improve accuracy, indicating that language understanding alone is insufficient for insight reasoning. Notably, models sometimes generated correct candidates but failed to endorse them, suggesting weak metacognitive control rather than a lack of knowledge. This \"verification failure\" indicates that CoT outputs can reflect genuine intermediate reasoning states rather than post-hoc rationalizations. By exposing this metacognitive bottleneck - models' inability to recognize when they are right - the benchmark provides a scalable, cross-linguistic testbed for studying machine insight, confidence calibration, and self-evaluation. NazoNazo Benchmark thus offers not only a fresh challenge to current LLMs but also a concrete target for developing AI metacognitive psychology and enhancing machine Aha! capability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmark saturation and contamination have obscured genuine advances in reasoning for large language models (LLMs). We introduce NazoNazo Benchmark, a low-cost, renewable test built from Japanese children's riddles that demand insight-based reasoning, or representational shifts rather than knowledge recall. We evaluate 38 frontier LLMs (2023-2025) on 201 riddles and a 120-item human-comparison subset, finding that non-reasoning models average 7.6%, reasoning models 17.6%, and humans ~53% accuracy. Importantly, thought-log analysis reveals that reasoning in Japanese did not necessarily improve accuracy, indicating that language understanding alone is insufficient for insight reasoning. Notably, models sometimes generated correct candidates but failed to endorse them, suggesting weak metacognitive control rather than a lack of knowledge. This \"verification failure\" indicates that CoT outputs can reflect genuine intermediate reasoning states rather than post-hoc rationalizations. By exposing this metacognitive bottleneck - models' inability to recognize when they are right - the benchmark provides a scalable, cross-linguistic testbed for studying machine insight, confidence calibration, and self-evaluation. NazoNazo Benchmark thus offers not only a fresh challenge to current LLMs but also a concrete target for developing AI metacognitive psychology and enhancing machine Aha! capability."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-09-18T07:50:04Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    7,
                    50,
                    4,
                    3,
                    261,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Masaharu Mizumoto"
                    },
                    {
                        "name": "Dat Nguyen"
                    },
                    {
                        "name": "Zhiheng Han"
                    },
                    {
                        "name": "Jiyuan Fang"
                    },
                    {
                        "name": "Heyuan Guan"
                    },
                    {
                        "name": "Xingfu Li"
                    },
                    {
                        "name": "Naoya Shiraishi"
                    },
                    {
                        "name": "Yo Nakawake"
                    },
                    {
                        "name": "Le Minh Nguyen"
                    }
                ],
                "author_detail": {
                    "name": "Le Minh Nguyen"
                },
                "author": "Le Minh Nguyen"
            },
            {
                "id": "http://arxiv.org/abs/2601.02123v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.02123v1",
                "title": "DeCode: Decoupling Content and Delivery for Medical QA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeCode: Decoupling Content and Delivery for Medical QA"
                },
                "updated": "2026-01-05T13:54:38Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    13,
                    54,
                    38,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.02123v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.02123v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models (LLMs) exhibit strong medical knowledge and can generate factually accurate responses. However, existing models often fail to account for individual patient contexts, producing answers that are clinically correct yet poorly aligned with patients' needs. In this work, we introduce DeCode, a training-free, model-agnostic framework that adapts existing LLMs to produce contextualized answers in clinical settings. We evaluate DeCode on OpenAI HealthBench, a comprehensive and challenging benchmark designed to assess clinical relevance and validity of LLM responses. DeCode improves the previous state of the art from $28.4\\%$ to $49.8\\%$, corresponding to a $75\\%$ relative improvement. Experimental results suggest the effectiveness of DeCode in improving clinical question answering of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) exhibit strong medical knowledge and can generate factually accurate responses. However, existing models often fail to account for individual patient contexts, producing answers that are clinically correct yet poorly aligned with patients' needs. In this work, we introduce DeCode, a training-free, model-agnostic framework that adapts existing LLMs to produce contextualized answers in clinical settings. We evaluate DeCode on OpenAI HealthBench, a comprehensive and challenging benchmark designed to assess clinical relevance and validity of LLM responses. DeCode improves the previous state of the art from $28.4\\%$ to $49.8\\%$, corresponding to a $75\\%$ relative improvement. Experimental results suggest the effectiveness of DeCode in improving clinical question answering of LLMs."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-05T13:54:38Z",
                "published_parsed": [
                    2026,
                    1,
                    5,
                    13,
                    54,
                    38,
                    0,
                    5,
                    0
                ],
                "arxiv_comment": "Preprint",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Po-Jen Ko"
                    },
                    {
                        "name": "Chen-Han Tsai"
                    },
                    {
                        "name": "Yu-Shao Peng"
                    }
                ],
                "author_detail": {
                    "name": "Yu-Shao Peng"
                },
                "author": "Yu-Shao Peng"
            },
            {
                "id": "http://arxiv.org/abs/2601.02121v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.02121v1",
                "title": "Inferring Network Evolutionary History via Structure-State Coupled Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inferring Network Evolutionary History via Structure-State Coupled Learning"
                },
                "updated": "2026-01-05T13:53:44Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    13,
                    53,
                    44,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.02121v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.02121v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Inferring a network's evolutionary history from a single final snapshot with limited temporal annotations is fundamental yet challenging. Existing approaches predominantly rely on topology alone, which often provides insufficient and noisy cues. This paper leverages network steady-state dynamics -- converged node states under a given dynamical process -- as an additional and widely accessible observation for network evolution history inference. We propose CS$^2$, which explicitly models structure-state coupling to capture how topology modulates steady states and how the two signals jointly improve edge discrimination for formation-order recovery. Experiments on six real temporal networks, evaluated under multiple dynamical processes, show that CS$^2$ consistently outperforms strong baselines, improving pairwise edge precedence accuracy by 4.0% on average and global ordering consistency (Spearman-$ρ$) by 7.7% on average. CS$^2$ also more faithfully recovers macroscopic evolution trajectories such as clustering formation, degree heterogeneity, and hub growth. Moreover, a steady-state-only variant remains competitive when reliable topology is limited, highlighting steady states as an independent signal for evolution inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inferring a network's evolutionary history from a single final snapshot with limited temporal annotations is fundamental yet challenging. Existing approaches predominantly rely on topology alone, which often provides insufficient and noisy cues. This paper leverages network steady-state dynamics -- converged node states under a given dynamical process -- as an additional and widely accessible observation for network evolution history inference. We propose CS$^2$, which explicitly models structure-state coupling to capture how topology modulates steady states and how the two signals jointly improve edge discrimination for formation-order recovery. Experiments on six real temporal networks, evaluated under multiple dynamical processes, show that CS$^2$ consistently outperforms strong baselines, improving pairwise edge precedence accuracy by 4.0% on average and global ordering consistency (Spearman-$ρ$) by 7.7% on average. CS$^2$ also more faithfully recovers macroscopic evolution trajectories such as clustering formation, degree heterogeneity, and hub growth. Moreover, a steady-state-only variant remains competitive when reliable topology is limited, highlighting steady states as an independent signal for evolution inference."
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-05T13:53:44Z",
                "published_parsed": [
                    2026,
                    1,
                    5,
                    13,
                    53,
                    44,
                    0,
                    5,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI"
                },
                "authors": [
                    {
                        "name": "En Xu"
                    },
                    {
                        "name": "Shihe Zhou"
                    },
                    {
                        "name": "Huandong Wang"
                    },
                    {
                        "name": "Jingtao Ding"
                    },
                    {
                        "name": "Yong Li"
                    }
                ],
                "author_detail": {
                    "name": "Yong Li"
                },
                "author": "Yong Li"
            },
            {
                "id": "http://arxiv.org/abs/2601.02112v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.02112v1",
                "title": "Car Drag Coefficient Prediction from 3D Point Clouds Using a Slice-Based Surrogate Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Car Drag Coefficient Prediction from 3D Point Clouds Using a Slice-Based Surrogate Model"
                },
                "updated": "2026-01-05T13:41:20Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    13,
                    41,
                    20,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.02112v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.02112v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.1007/978-3-032-11442-6_5",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "The automotive industry's pursuit of enhanced fuel economy and performance necessitates efficient aerodynamic design. However, traditional evaluation methods such as computational fluid dynamics (CFD) and wind tunnel testing are resource intensive, hindering rapid iteration in the early design stages. Machine learning-based surrogate models offer a promising alternative, yet many existing approaches suffer from high computational complexity, limited interpretability, or insufficient accuracy for detailed geometric inputs. This paper introduces a novel lightweight surrogate model for the prediction of the aerodynamic drag coefficient (Cd) based on a sequential slice-wise processing of the geometry of the 3D vehicle. Inspired by medical imaging, 3D point clouds of vehicles are decomposed into an ordered sequence of 2D cross-sectional slices along the stream-wise axis. Each slice is encoded by a lightweight PointNet2D module, and the sequence of slice embeddings is processed by a bidirectional LSTM to capture longitudinal geometric evolution. The model, trained and evaluated on the DrivAerNet++ dataset, achieves a high coefficient of determination (R^2 > 0.9528) and a low mean absolute error (MAE approx 6.046 x 10^{-3}) in Cd prediction. With an inference time of approximately 0.025 seconds per sample on a consumer-grade GPU, our approach provides fast, accurate, and interpretable aerodynamic feedback, facilitating more agile and informed automotive design exploration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The automotive industry's pursuit of enhanced fuel economy and performance necessitates efficient aerodynamic design. However, traditional evaluation methods such as computational fluid dynamics (CFD) and wind tunnel testing are resource intensive, hindering rapid iteration in the early design stages. Machine learning-based surrogate models offer a promising alternative, yet many existing approaches suffer from high computational complexity, limited interpretability, or insufficient accuracy for detailed geometric inputs. This paper introduces a novel lightweight surrogate model for the prediction of the aerodynamic drag coefficient (Cd) based on a sequential slice-wise processing of the geometry of the 3D vehicle. Inspired by medical imaging, 3D point clouds of vehicles are decomposed into an ordered sequence of 2D cross-sectional slices along the stream-wise axis. Each slice is encoded by a lightweight PointNet2D module, and the sequence of slice embeddings is processed by a bidirectional LSTM to capture longitudinal geometric evolution. The model, trained and evaluated on the DrivAerNet++ dataset, achieves a high coefficient of determination (R^2 > 0.9528) and a low mean absolute error (MAE approx 6.046 x 10^{-3}) in Cd prediction. With an inference time of approximately 0.025 seconds per sample on a consumer-grade GPU, our approach provides fast, accurate, and interpretable aerodynamic feedback, facilitating more agile and informed automotive design exploration."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-05T13:41:20Z",
                "published_parsed": [
                    2026,
                    1,
                    5,
                    13,
                    41,
                    20,
                    0,
                    5,
                    0
                ],
                "arxiv_comment": "14 pages, 5 figures. Published in: Bramer M., Stahl F. (eds) Artificial Intelligence XLII. SGAI 2025. Lecture Notes in Computer Science, vol 16302. Springer, Cham",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "arxiv_journal_ref": "In: Bramer M., Stahl F. (eds) Artificial Intelligence XLII. SGAI 2025. Lecture Notes in Computer Science, vol 16302, pp 66-79. Springer, Cham (2025)",
                "authors": [
                    {
                        "name": "Utkarsh Singh"
                    },
                    {
                        "name": "Absaar Ali"
                    },
                    {
                        "name": "Adarsh Roy"
                    }
                ],
                "author_detail": {
                    "name": "Adarsh Roy"
                },
                "author": "Adarsh Roy",
                "arxiv_doi": "10.1007/978-3-032-11442-6_5"
            },
            {
                "id": "http://arxiv.org/abs/2512.23260v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.23260v2",
                "title": "Interpretable Safety Alignment via SAE-Constructed Low-Rank Subspace Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interpretable Safety Alignment via SAE-Constructed Low-Rank Subspace Adaptation"
                },
                "updated": "2026-01-05T13:39:39Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    13,
                    39,
                    39,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.23260v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.23260v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Safety alignment -- training large language models (LLMs) to refuse harmful requests while remaining helpful -- is critical for responsible deployment. Prior work established that safety behaviors are governed by low-rank structures, suggesting parameter-efficient fine-tuning (PEFT) should be well-suited for alignment. However, Low-Rank Adaptation (LoRA) consistently underperforms full fine-tuning and reinforcement learning on safety benchmarks. We attribute this gap to semantic entanglement: safety-relevant directions are intertwined with unrelated concepts due to polysemanticity, impeding implicit subspace identification. To address this, we propose SAILS (Safety Alignment via Interpretable Low-rank Subspace), which leverages Sparse Autoencoders (SAEs) to disentangle representations into monosemantic features, constructs an interpretable safety subspace from SAE decoder directions, and uses it to initialize LoRA adapters. Theoretically, we prove that SAE-based identification achieves arbitrarily small recovery error under monosemanticity assumptions, while direct identification suffers an irreducible error floor. Empirically, SAILS achieves up to 99.6% safety rate on Gemma-2-9B -- exceeding full fine-tuning by 7.4 points and matching RLHF-based models -- while updating only 0.19% of parameters and providing interpretability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Safety alignment -- training large language models (LLMs) to refuse harmful requests while remaining helpful -- is critical for responsible deployment. Prior work established that safety behaviors are governed by low-rank structures, suggesting parameter-efficient fine-tuning (PEFT) should be well-suited for alignment. However, Low-Rank Adaptation (LoRA) consistently underperforms full fine-tuning and reinforcement learning on safety benchmarks. We attribute this gap to semantic entanglement: safety-relevant directions are intertwined with unrelated concepts due to polysemanticity, impeding implicit subspace identification. To address this, we propose SAILS (Safety Alignment via Interpretable Low-rank Subspace), which leverages Sparse Autoencoders (SAEs) to disentangle representations into monosemantic features, constructs an interpretable safety subspace from SAE decoder directions, and uses it to initialize LoRA adapters. Theoretically, we prove that SAE-based identification achieves arbitrarily small recovery error under monosemanticity assumptions, while direct identification suffers an irreducible error floor. Empirically, SAILS achieves up to 99.6% safety rate on Gemma-2-9B -- exceeding full fine-tuning by 7.4 points and matching RLHF-based models -- while updating only 0.19% of parameters and providing interpretability."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-29T07:39:49Z",
                "published_parsed": [
                    2025,
                    12,
                    29,
                    7,
                    39,
                    49,
                    0,
                    363,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Dianyun Wang"
                    },
                    {
                        "name": "Qingsen Ma"
                    },
                    {
                        "name": "Yuhu Shang"
                    },
                    {
                        "name": "Zhifeng Lu"
                    },
                    {
                        "name": "Zhenbo Xu"
                    },
                    {
                        "name": "Lechen Ning"
                    },
                    {
                        "name": "Huijia Wu"
                    },
                    {
                        "name": "Zhaofeng He"
                    }
                ],
                "author_detail": {
                    "name": "Zhaofeng He"
                },
                "author": "Zhaofeng He"
            },
            {
                "id": "http://arxiv.org/abs/2506.05623v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2506.05623v2",
                "title": "Deployability-Centric Infrastructure-as-Code Generation: Fail, Learn, Refine, and Succeed through LLM-Empowered DevOps Simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deployability-Centric Infrastructure-as-Code Generation: Fail, Learn, Refine, and Succeed through LLM-Empowered DevOps Simulation"
                },
                "updated": "2026-01-05T13:38:13Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    13,
                    38,
                    13,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2506.05623v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2506.05623v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Infrastructure-as-Code (IaC) generation holds significant promise for automating cloud infrastructure provisioning. Recent advances in Large Language Models (LLMs) present a promising opportunity to democratize IaC development by generating deployable infrastructure templates from natural language descriptions. However, current evaluation focuses on syntactic correctness while ignoring deployability, the critical measure of the utility of IaC configuration files. Six state-of-the-art LLMs performed poorly on deployability, achieving only 20.8$\\sim$30.2% deployment success rate on the first attempt. In this paper, we construct DPIaC-Eval, the first deployability-centric IaC template benchmark consisting of 153 real-world scenarios cross 58 unique services. Also, we propose an LLM-based deployability-centric framework, dubbed IaCGen, that uses iterative feedback mechanism encompassing format verification, syntax checking, and live deployment stages, thereby closely mirroring the real DevOps workflows. Results show that IaCGen can make 54.6$\\sim$91.6% generated IaC templates from all evaluated models deployable in the first 10 iterations. Additionally, human-in-the-loop feedback that provide direct guidance for the deployability errors, can further boost the performance to over 90% passItr@25 on all evaluated LLMs. Furthermore, we explore the trustworthiness of the generated IaC templates on user intent alignment and security compliance. The poor performance (25.2% user requirement coverage and 8.4% security compliance rate) indicates a critical need for continued research in this domain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Infrastructure-as-Code (IaC) generation holds significant promise for automating cloud infrastructure provisioning. Recent advances in Large Language Models (LLMs) present a promising opportunity to democratize IaC development by generating deployable infrastructure templates from natural language descriptions. However, current evaluation focuses on syntactic correctness while ignoring deployability, the critical measure of the utility of IaC configuration files. Six state-of-the-art LLMs performed poorly on deployability, achieving only 20.8$\\sim$30.2% deployment success rate on the first attempt. In this paper, we construct DPIaC-Eval, the first deployability-centric IaC template benchmark consisting of 153 real-world scenarios cross 58 unique services. Also, we propose an LLM-based deployability-centric framework, dubbed IaCGen, that uses iterative feedback mechanism encompassing format verification, syntax checking, and live deployment stages, thereby closely mirroring the real DevOps workflows. Results show that IaCGen can make 54.6$\\sim$91.6% generated IaC templates from all evaluated models deployable in the first 10 iterations. Additionally, human-in-the-loop feedback that provide direct guidance for the deployability errors, can further boost the performance to over 90% passItr@25 on all evaluated LLMs. Furthermore, we explore the trustworthiness of the generated IaC templates on user intent alignment and security compliance. The poor performance (25.2% user requirement coverage and 8.4% security compliance rate) indicates a critical need for continued research in this domain."
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-06-05T22:53:12Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    22,
                    53,
                    12,
                    3,
                    156,
                    0
                ],
                "arxiv_comment": "Accepted by FSE 2026",
                "arxiv_primary_category": {
                    "term": "cs.SE"
                },
                "authors": [
                    {
                        "name": "Tianyi Zhang"
                    },
                    {
                        "name": "Shidong Pan"
                    },
                    {
                        "name": "Zejun Zhang"
                    },
                    {
                        "name": "Zhenchang Xing"
                    },
                    {
                        "name": "Xiaoyu Sun"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoyu Sun"
                },
                "author": "Xiaoyu Sun"
            },
            {
                "id": "http://arxiv.org/abs/2511.06779v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.06779v2",
                "title": "Pedagogical Reflections on the Holistic Cognitive Development (HCD) Framework and AI-Augmented Learning in Creative Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pedagogical Reflections on the Holistic Cognitive Development (HCD) Framework and AI-Augmented Learning in Creative Computing"
                },
                "updated": "2026-01-05T13:31:19Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    13,
                    31,
                    19,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.06779v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.06779v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "This paper presents an expanded account of the Holistic Cognitive Development (HCD) framework for reflective and creative learning in computing education. The HCD framework integrates design thinking, experiential learning, and reflective practice into a unified constructivist pedagogy emphasizing autonomy, ownership, and scaffolding. It is applied across courses in game design (CS3247, CS4350), virtual reality (CS4240), and extended reality systems, where students engage in iterative cycles of thinking, creating, criticizing, and reflecting. The paper also examines how AI-augmented systems such as iReflect, ReflexAI, and Knowledge Graph-enhanced LLM feedback tools operationalize the HCD framework through scalable, personalized feedback. Empirical findings demonstrate improved reflective depth, feedback quality, and learner autonomy. The work advocates a balance of supportive autonomy in supervision, where students practice self-directed inquiry while guided through structured reflection and feedback.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents an expanded account of the Holistic Cognitive Development (HCD) framework for reflective and creative learning in computing education. The HCD framework integrates design thinking, experiential learning, and reflective practice into a unified constructivist pedagogy emphasizing autonomy, ownership, and scaffolding. It is applied across courses in game design (CS3247, CS4350), virtual reality (CS4240), and extended reality systems, where students engage in iterative cycles of thinking, creating, criticizing, and reflecting. The paper also examines how AI-augmented systems such as iReflect, ReflexAI, and Knowledge Graph-enhanced LLM feedback tools operationalize the HCD framework through scalable, personalized feedback. Empirical findings demonstrate improved reflective depth, feedback quality, and learner autonomy. The work advocates a balance of supportive autonomy in supervision, where students practice self-directed inquiry while guided through structured reflection and feedback."
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-10T07:07:37Z",
                "published_parsed": [
                    2025,
                    11,
                    10,
                    7,
                    7,
                    37,
                    0,
                    314,
                    0
                ],
                "arxiv_comment": "Short Abstract",
                "arxiv_primary_category": {
                    "term": "cs.MM"
                },
                "authors": [
                    {
                        "name": "Anand Bhojan"
                    }
                ],
                "author_detail": {
                    "name": "Anand Bhojan"
                },
                "author": "Anand Bhojan"
            },
            {
                "id": "http://arxiv.org/abs/2512.12953v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.12953v2",
                "title": "Asymptotic Inference for Constrained Regression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Asymptotic Inference for Constrained Regression"
                },
                "updated": "2026-01-05T13:29:27Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    13,
                    29,
                    27,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.12953v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.12953v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We consider statistical inference in high-dimensional regression problems under affine constraints on the parameter space. The theoretical study of this is motivated by the study of genetic determinants of diseases, such as diabetes, using external information from mediating protein expression levels. Specifically, we develop rigorous methods for estimating genetic effects on diabetes-related continuous outcomes when these associations are constrained based on external information about genetic determinants of proteins, and genetic relationships between proteins and the outcome of interest. In this regard, we discuss multiple candidate estimators and study their theoretical properties, sharp large sample optimality, and numerical qualities under a high-dimensional proportional asymptotic framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider statistical inference in high-dimensional regression problems under affine constraints on the parameter space. The theoretical study of this is motivated by the study of genetic determinants of diseases, such as diabetes, using external information from mediating protein expression levels. Specifically, we develop rigorous methods for estimating genetic effects on diabetes-related continuous outcomes when these associations are constrained based on external information about genetic determinants of proteins, and genetic relationships between proteins and the outcome of interest. In this regard, we discuss multiple candidate estimators and study their theoretical properties, sharp large sample optimality, and numerical qualities under a high-dimensional proportional asymptotic framework."
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-15T03:39:53Z",
                "published_parsed": [
                    2025,
                    12,
                    15,
                    3,
                    39,
                    53,
                    0,
                    349,
                    0
                ],
                "arxiv_comment": "Fixed citations",
                "arxiv_primary_category": {
                    "term": "stat.ME"
                },
                "authors": [
                    {
                        "name": "Madhav Sankaranarayanan"
                    },
                    {
                        "name": "Yana Hrytsenko"
                    },
                    {
                        "name": "Jerome I. Rotter"
                    },
                    {
                        "name": "Tamar Sofer"
                    },
                    {
                        "name": "Rajarshi Mukherjee"
                    }
                ],
                "author_detail": {
                    "name": "Rajarshi Mukherjee"
                },
                "author": "Rajarshi Mukherjee"
            },
            {
                "id": "http://arxiv.org/abs/2510.23506v4",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.23506v4",
                "title": "Emotion-Coherent Reasoning for Multimodal LLMs via Emotional Rationale Verifier",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emotion-Coherent Reasoning for Multimodal LLMs via Emotional Rationale Verifier"
                },
                "updated": "2026-01-05T13:24:37Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    13,
                    24,
                    37,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.23506v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.23506v4",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The recent advancement of Multimodal Large Language Models (MLLMs) is transforming human-computer interaction (HCI) from surface-level exchanges into more nuanced and emotionally intelligent communication. To realize this shift, emotion understanding becomes essential allowing systems to capture subtle cues underlying user intent. Furthermore, providing faithful explanations for predicted emotions is crucial to ensure interpretability and build user trust. However, current MLLM-based methods often generate emotion explanations that diverge from the target labels and sometimes even contradict their own predicted emotions. This inconsistency poses a critical risk for misunderstanding and erodes reliability in interactive settings. To address this, we propose a novel approach: the Emotional Rationale Verifier (ERV) and an Explanation Reward. Our method guides the model to produce reasoning that is explicitly consistent with the target emotion during multimodal emotion recognition without modifying the model architecture or requiring additional paired video-description annotations. Our method significantly improves faithful explanation-prediction consistency and explanation emotion accuracy on the MAFW and DFEW datasets. Through extensive experiments and human evaluations, we show that our approach not only enhances alignment between explanation and prediction but also empowers MLLMs to deliver emotionally coherent, trustworthy interactions, marking a key step toward truly human-like HCI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent advancement of Multimodal Large Language Models (MLLMs) is transforming human-computer interaction (HCI) from surface-level exchanges into more nuanced and emotionally intelligent communication. To realize this shift, emotion understanding becomes essential allowing systems to capture subtle cues underlying user intent. Furthermore, providing faithful explanations for predicted emotions is crucial to ensure interpretability and build user trust. However, current MLLM-based methods often generate emotion explanations that diverge from the target labels and sometimes even contradict their own predicted emotions. This inconsistency poses a critical risk for misunderstanding and erodes reliability in interactive settings. To address this, we propose a novel approach: the Emotional Rationale Verifier (ERV) and an Explanation Reward. Our method guides the model to produce reasoning that is explicitly consistent with the target emotion during multimodal emotion recognition without modifying the model architecture or requiring additional paired video-description annotations. Our method significantly improves faithful explanation-prediction consistency and explanation emotion accuracy on the MAFW and DFEW datasets. Through extensive experiments and human evaluations, we show that our approach not only enhances alignment between explanation and prediction but also empowers MLLMs to deliver emotionally coherent, trustworthy interactions, marking a key step toward truly human-like HCI systems."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-27T16:40:17Z",
                "published_parsed": [
                    2025,
                    10,
                    27,
                    16,
                    40,
                    17,
                    0,
                    300,
                    0
                ],
                "arxiv_comment": "15 pages, 11 figures",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Hyeongseop Rha"
                    },
                    {
                        "name": "Jeong Hun Yeo"
                    },
                    {
                        "name": "Yeonju Kim"
                    },
                    {
                        "name": "Yong Man Ro"
                    }
                ],
                "author_detail": {
                    "name": "Yong Man Ro"
                },
                "author": "Yong Man Ro"
            },
            {
                "id": "http://arxiv.org/abs/2512.20957v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20957v3",
                "title": "One Tool Is Enough: Reinforcement Learning for Repository-Level LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One Tool Is Enough: Reinforcement Learning for Repository-Level LLM Agents"
                },
                "updated": "2026-01-05T13:23:35Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    13,
                    23,
                    35,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20957v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20957v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Locating the files and functions requiring modification in large open-source software (OSS) repositories is challenging due to their scale and structural complexity. Existing large language model (LLM)-based methods typically treat this as a repository-level retrieval task and rely on multiple auxiliary tools, which overlook code execution logic and complicate model control. We propose RepoNavigator, an LLM agent equipped with a single execution-aware tool-jumping to the definition of an invoked symbol. This unified design reflects the actual flow of code execution while simplifying tool manipulation. RepoNavigator is trained end-to-end via Reinforcement Learning (RL) directly from a pretrained model, without any closed-source distillation. Experiments demonstrate that RL-trained RepoNavigator achieves state-of-the-art performance, with the 7B model outperforming 14B baselines, the 14B model surpassing 32B competitors, and even the 32B model exceeding closed-source models such as Claude-3.7. These results confirm that integrating a single, structurally grounded tool with RL training provides an efficient and scalable solution for repository-level issue localization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Locating the files and functions requiring modification in large open-source software (OSS) repositories is challenging due to their scale and structural complexity. Existing large language model (LLM)-based methods typically treat this as a repository-level retrieval task and rely on multiple auxiliary tools, which overlook code execution logic and complicate model control. We propose RepoNavigator, an LLM agent equipped with a single execution-aware tool-jumping to the definition of an invoked symbol. This unified design reflects the actual flow of code execution while simplifying tool manipulation. RepoNavigator is trained end-to-end via Reinforcement Learning (RL) directly from a pretrained model, without any closed-source distillation. Experiments demonstrate that RL-trained RepoNavigator achieves state-of-the-art performance, with the 7B model outperforming 14B baselines, the 14B model surpassing 32B competitors, and even the 32B model exceeding closed-source models such as Claude-3.7. These results confirm that integrating a single, structurally grounded tool with RL training provides an efficient and scalable solution for repository-level issue localization."
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-24T05:27:53Z",
                "published_parsed": [
                    2025,
                    12,
                    24,
                    5,
                    27,
                    53,
                    2,
                    358,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE"
                },
                "authors": [
                    {
                        "name": "Zhaoxi Zhang"
                    },
                    {
                        "name": "Yitong Duan"
                    },
                    {
                        "name": "Yanzhi Zhang"
                    },
                    {
                        "name": "Yiming Xu"
                    },
                    {
                        "name": "Jiyan He"
                    },
                    {
                        "name": "Yunfang Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yunfang Wu"
                },
                "author": "Yunfang Wu"
            },
            {
                "id": "http://arxiv.org/abs/2512.22673v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.22673v2",
                "title": "TravelBench: A Broader Real-World Benchmark for Multi-Turn and Tool-Using Travel Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TravelBench: A Broader Real-World Benchmark for Multi-Turn and Tool-Using Travel Planning"
                },
                "updated": "2026-01-05T13:19:13Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    13,
                    19,
                    13,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.22673v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.22673v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Travel planning is a natural real-world task to test large language models (LLMs) planning and tool-use abilities. Although prior work has studied LLM performance on travel planning, existing settings still differ from real-world needs, mainly due to limited domain coverage, insufficient modeling of users' implicit preferences in multi-turn conversations, and a lack of clear evaluation of agents' capability boundaries. To mitigate these gaps, we propose \\textbf{TravelBench}, a benchmark for fully real-world travel planning. We collect user queries, user profile and tools from real scenarios, and construct three subtasks-Single-Turn, Multi-Turn, and Unsolvable-to evaluate agent's three core capabilities in real settings: (1) solving problems autonomously, (2) interacting with users over multiple turns to refine requirements, and (3) recognizing the limits of own abilities. To enable stable tool invocation and reproducible evaluation, we cache real tool-call results and build a sandbox environment that integrates ten travel-related tools. Agents can combine these tools to solve most practical travel planning problems, and our systematic verification demonstrates the stability of the proposed benchmark. We further evaluate multiple LLMs on TravelBench and conduct an in-depth analysis of their behaviors and performance. TravelBench provides a practical and reproducible evaluation benchmark to advance research on LLM agents for travel planning.\\footnote{Our code and data will be available after internal review.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Travel planning is a natural real-world task to test large language models (LLMs) planning and tool-use abilities. Although prior work has studied LLM performance on travel planning, existing settings still differ from real-world needs, mainly due to limited domain coverage, insufficient modeling of users' implicit preferences in multi-turn conversations, and a lack of clear evaluation of agents' capability boundaries. To mitigate these gaps, we propose \\textbf{TravelBench}, a benchmark for fully real-world travel planning. We collect user queries, user profile and tools from real scenarios, and construct three subtasks-Single-Turn, Multi-Turn, and Unsolvable-to evaluate agent's three core capabilities in real settings: (1) solving problems autonomously, (2) interacting with users over multiple turns to refine requirements, and (3) recognizing the limits of own abilities. To enable stable tool invocation and reproducible evaluation, we cache real tool-call results and build a sandbox environment that integrates ten travel-related tools. Agents can combine these tools to solve most practical travel planning problems, and our systematic verification demonstrates the stability of the proposed benchmark. We further evaluate multiple LLMs on TravelBench and conduct an in-depth analysis of their behaviors and performance. TravelBench provides a practical and reproducible evaluation benchmark to advance research on LLM agents for travel planning.\\footnote{Our code and data will be available after internal review."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-27T18:25:14Z",
                "published_parsed": [
                    2025,
                    12,
                    27,
                    18,
                    25,
                    14,
                    5,
                    361,
                    0
                ],
                "arxiv_comment": "In progress",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Xiang Cheng"
                    },
                    {
                        "name": "Yulan Hu"
                    },
                    {
                        "name": "Xiangwen Zhang"
                    },
                    {
                        "name": "Lu Xu"
                    },
                    {
                        "name": "Zheng Pan"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Yong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Liu"
                },
                "author": "Yong Liu"
            },
            {
                "id": "http://arxiv.org/abs/2601.02085v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.02085v1",
                "title": "Vision-Based Early Fault Diagnosis and Self-Recovery for Strawberry Harvesting Robots",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Based Early Fault Diagnosis and Self-Recovery for Strawberry Harvesting Robots"
                },
                "updated": "2026-01-05T13:12:42Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    13,
                    12,
                    42,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.02085v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.02085v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Strawberry harvesting robots faced persistent challenges such as low integration of visual perception, fruit-gripper misalignment, empty grasping, and strawberry slippage from the gripper due to insufficient gripping force, all of which compromised harvesting stability and efficiency in orchard environments. To overcome these issues, this paper proposed a visual fault diagnosis and self-recovery framework that integrated multi-task perception with corrective control strategies. At the core of this framework was SRR-Net, an end-to-end multi-task perception model that simultaneously performed strawberry detection, segmentation, and ripeness estimation, thereby unifying visual perception with fault diagnosis. Based on this integrated perception, a relative error compensation method based on the simultaneous target-gripper detection was designed to address positional misalignment, correcting deviations when error exceeded the tolerance threshold. To mitigate empty grasping and fruit-slippage faults, an early abort strategy was implemented. A micro-optical camera embedded in the end-effector provided real-time visual feedback, enabling grasp detection during the deflating stage and strawberry slip prediction during snap-off through MobileNet V3-Small classifier and a time-series LSTM classifier. Experiments demonstrated that SRR-Net maintained high perception accuracy. For detection, it achieved a precision of 0.895 and recall of 0.813 on strawberries, and 0.972/0.958 on hands. In segmentation, it yielded a precision of 0.887 and recall of 0.747 for strawberries, and 0.974/0.947 for hands. For ripeness estimation, SRR-Net attained a mean absolute error of 0.035, while simultaneously supporting multi-task perception and sustaining a competitive inference speed of 163.35 FPS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Strawberry harvesting robots faced persistent challenges such as low integration of visual perception, fruit-gripper misalignment, empty grasping, and strawberry slippage from the gripper due to insufficient gripping force, all of which compromised harvesting stability and efficiency in orchard environments. To overcome these issues, this paper proposed a visual fault diagnosis and self-recovery framework that integrated multi-task perception with corrective control strategies. At the core of this framework was SRR-Net, an end-to-end multi-task perception model that simultaneously performed strawberry detection, segmentation, and ripeness estimation, thereby unifying visual perception with fault diagnosis. Based on this integrated perception, a relative error compensation method based on the simultaneous target-gripper detection was designed to address positional misalignment, correcting deviations when error exceeded the tolerance threshold. To mitigate empty grasping and fruit-slippage faults, an early abort strategy was implemented. A micro-optical camera embedded in the end-effector provided real-time visual feedback, enabling grasp detection during the deflating stage and strawberry slip prediction during snap-off through MobileNet V3-Small classifier and a time-series LSTM classifier. Experiments demonstrated that SRR-Net maintained high perception accuracy. For detection, it achieved a precision of 0.895 and recall of 0.813 on strawberries, and 0.972/0.958 on hands. In segmentation, it yielded a precision of 0.887 and recall of 0.747 for strawberries, and 0.974/0.947 for hands. For ripeness estimation, SRR-Net attained a mean absolute error of 0.035, while simultaneously supporting multi-task perception and sustaining a competitive inference speed of 163.35 FPS."
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-05T13:12:42Z",
                "published_parsed": [
                    2026,
                    1,
                    5,
                    13,
                    12,
                    42,
                    0,
                    5,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO"
                },
                "authors": [
                    {
                        "name": "Meili Sun"
                    },
                    {
                        "name": "Chunjiang Zhao"
                    },
                    {
                        "name": "Lichao Yang"
                    },
                    {
                        "name": "Hao Liu"
                    },
                    {
                        "name": "Shimin Hu"
                    },
                    {
                        "name": "Ya Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Ya Xiong"
                },
                "author": "Ya Xiong"
            },
            {
                "id": "http://arxiv.org/abs/2510.27052v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.27052v3",
                "title": "VISTA Score: Verification In Sequential Turn-based Assessment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VISTA Score: Verification In Sequential Turn-based Assessment"
                },
                "updated": "2026-01-05T13:07:39Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    13,
                    7,
                    39,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.27052v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.27052v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Hallucination--defined here as generating statements unsupported or contradicted by available evidence or conversational context--remains a major obstacle to deploying conversational AI systems in settings that demand factual reliability. Existing metrics either evaluate isolated responses or treat unverifiable content as errors, limiting their use for multi-turn dialogue. We introduce VISTA (Verification In Sequential Turn-based Assessment), a framework for evaluating conversational factuality through claim-level verification and sequential consistency tracking. VISTA decomposes each assistant turn into atomic factual claims, verifies them against trusted sources and dialogue history, and categorizes unverifiable statements (subjective, contradicted, lacking evidence, or abstaining). Across eight large language models and four dialogue factuality benchmarks (AIS, BEGIN, FAITHDIAL, and FADE), VISTA substantially improves hallucination detection over FACTSCORE and LLM-as-Judge baselines. Human evaluation confirms that VISTA's decomposition improves annotator agreement and reveals inconsistencies in existing benchmarks. By modeling factuality as a dynamic property of conversation, VISTA offers a more transparent, human-aligned measure of truthfulness in dialogue systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hallucination--defined here as generating statements unsupported or contradicted by available evidence or conversational context--remains a major obstacle to deploying conversational AI systems in settings that demand factual reliability. Existing metrics either evaluate isolated responses or treat unverifiable content as errors, limiting their use for multi-turn dialogue. We introduce VISTA (Verification In Sequential Turn-based Assessment), a framework for evaluating conversational factuality through claim-level verification and sequential consistency tracking. VISTA decomposes each assistant turn into atomic factual claims, verifies them against trusted sources and dialogue history, and categorizes unverifiable statements (subjective, contradicted, lacking evidence, or abstaining). Across eight large language models and four dialogue factuality benchmarks (AIS, BEGIN, FAITHDIAL, and FADE), VISTA substantially improves hallucination detection over FACTSCORE and LLM-as-Judge baselines. Human evaluation confirms that VISTA's decomposition improves annotator agreement and reveals inconsistencies in existing benchmarks. By modeling factuality as a dynamic property of conversation, VISTA offers a more transparent, human-aligned measure of truthfulness in dialogue systems."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-30T23:45:13Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    23,
                    45,
                    13,
                    3,
                    303,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Ashley Lewis"
                    },
                    {
                        "name": "Andrew Perrault"
                    },
                    {
                        "name": "Eric Fosler-Lussier"
                    },
                    {
                        "name": "Michael White"
                    }
                ],
                "author_detail": {
                    "name": "Michael White"
                },
                "author": "Michael White"
            },
            {
                "id": "http://arxiv.org/abs/2601.02078v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.02078v1",
                "title": "Genie Sim 3.0 : A High-Fidelity Comprehensive Simulation Platform for Humanoid Robot",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Genie Sim 3.0 : A High-Fidelity Comprehensive Simulation Platform for Humanoid Robot"
                },
                "updated": "2026-01-05T12:59:39Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    12,
                    59,
                    39,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.02078v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.02078v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The development of robust and generalizable robot learning models is critically contingent upon the availability of large-scale, diverse training data and reliable evaluation benchmarks. Collecting data in the physical world poses prohibitive costs and scalability challenges, and prevailing simulation benchmarks frequently suffer from fragmentation, narrow scope, or insufficient fidelity to enable effective sim-to-real transfer. To address these challenges, we introduce Genie Sim 3.0, a unified simulation platform for robotic manipulation. We present Genie Sim Generator, a large language model (LLM)-powered tool that constructs high-fidelity scenes from natural language instructions. Its principal strength resides in rapid and multi-dimensional generalization, facilitating the synthesis of diverse environments to support scalable data collection and robust policy evaluation. We introduce the first benchmark that pioneers the application of LLM for automated evaluation. It leverages LLM to mass-generate evaluation scenarios and employs Vision-Language Model (VLM) to establish an automated assessment pipeline. We also release an open-source dataset comprising more than 10,000 hours of synthetic data across over 200 tasks. Through systematic experimentation, we validate the robust zero-shot sim-to-real transfer capability of our open-source dataset, demonstrating that synthetic data can server as an effective substitute for real-world data under controlled conditions for scalable policy training. For code and dataset details, please refer to: https://github.com/AgibotTech/genie_sim.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The development of robust and generalizable robot learning models is critically contingent upon the availability of large-scale, diverse training data and reliable evaluation benchmarks. Collecting data in the physical world poses prohibitive costs and scalability challenges, and prevailing simulation benchmarks frequently suffer from fragmentation, narrow scope, or insufficient fidelity to enable effective sim-to-real transfer. To address these challenges, we introduce Genie Sim 3.0, a unified simulation platform for robotic manipulation. We present Genie Sim Generator, a large language model (LLM)-powered tool that constructs high-fidelity scenes from natural language instructions. Its principal strength resides in rapid and multi-dimensional generalization, facilitating the synthesis of diverse environments to support scalable data collection and robust policy evaluation. We introduce the first benchmark that pioneers the application of LLM for automated evaluation. It leverages LLM to mass-generate evaluation scenarios and employs Vision-Language Model (VLM) to establish an automated assessment pipeline. We also release an open-source dataset comprising more than 10,000 hours of synthetic data across over 200 tasks. Through systematic experimentation, we validate the robust zero-shot sim-to-real transfer capability of our open-source dataset, demonstrating that synthetic data can server as an effective substitute for real-world data under controlled conditions for scalable policy training. For code and dataset details, please refer to: https://github.com/AgibotTech/genie_sim."
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-05T12:59:39Z",
                "published_parsed": [
                    2026,
                    1,
                    5,
                    12,
                    59,
                    39,
                    0,
                    5,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO"
                },
                "authors": [
                    {
                        "name": "Chenghao Yin"
                    },
                    {
                        "name": "Da Huang"
                    },
                    {
                        "name": "Di Yang"
                    },
                    {
                        "name": "Jichao Wang"
                    },
                    {
                        "name": "Nanshu Zhao"
                    },
                    {
                        "name": "Chen Xu"
                    },
                    {
                        "name": "Wenjun Sun"
                    },
                    {
                        "name": "Linjie Hou"
                    },
                    {
                        "name": "Zhijun Li"
                    },
                    {
                        "name": "Junhui Wu"
                    },
                    {
                        "name": "Zhaobo Liu"
                    },
                    {
                        "name": "Zhen Xiao"
                    },
                    {
                        "name": "Sheng Zhang"
                    },
                    {
                        "name": "Lei Bao"
                    },
                    {
                        "name": "Rui Feng"
                    },
                    {
                        "name": "Zhenquan Pang"
                    },
                    {
                        "name": "Jiayu Li"
                    },
                    {
                        "name": "Qian Wang"
                    },
                    {
                        "name": "Maoqing Yao"
                    }
                ],
                "author_detail": {
                    "name": "Maoqing Yao"
                },
                "author": "Maoqing Yao"
            },
            {
                "id": "http://arxiv.org/abs/2601.02077v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.02077v1",
                "title": "Joint Constraints on Neutrinos and Dynamical Dark Energy in Minimally Modified Gravity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Joint Constraints on Neutrinos and Dynamical Dark Energy in Minimally Modified Gravity"
                },
                "updated": "2026-01-05T12:58:36Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    12,
                    58,
                    36,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.02077v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.02077v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The \\(w_{\\dagger}\\)VCDM framework provides a theoretically well-controlled extension of \\(Λ\\)CDM within the class of minimally modified gravity theories, allowing for flexible cosmological background evolution and linear perturbation dynamics while remaining free of pathological instabilities. In this work, we have shown that this scenario remains robust when confronted with current cosmological observations, even in the presence of an extended neutrino sector. Combining \\textit{Planck} CMB data with DESI DR2 BAO and DESY5 supernovae, we obtain stringent constraints on neutrino physics, including \\(\\sum m_ν< 0.11~\\mathrm{eV}\\) (95\\% CL) and \\(N_{\\rm eff} = 2.98^{+0.13}_{-0.14}\\), fully consistent with Standard Model expectations. Crucially, the data exhibit a statistically significant preference for a late-time dark-energy transition, characterized by a robust quintessence--phantom crossing that remains stable across all dataset combinations and neutrino-sector extensions, including the presence of a sterile neutrino. The combined effects of modified late-time expansion and additional relativistic degrees of freedom systematically raise the inferred Hubble constant, substantially alleviating the \\(H_0\\) tension without invoking early dark energy or introducing theoretical instabilities. Overall, the \\(w_{\\dagger}\\)VCDM scenario emerges as a compelling phenomenological framework that simultaneously accommodates current constraints on neutrino physics, provides an excellent fit to recent BAO and supernovae data, and offers a viable pathway toward resolving persistent tensions in the standard cosmological model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The \\(w_{\\dagger}\\)VCDM framework provides a theoretically well-controlled extension of \\(Λ\\)CDM within the class of minimally modified gravity theories, allowing for flexible cosmological background evolution and linear perturbation dynamics while remaining free of pathological instabilities. In this work, we have shown that this scenario remains robust when confronted with current cosmological observations, even in the presence of an extended neutrino sector. Combining \\textit{Planck} CMB data with DESI DR2 BAO and DESY5 supernovae, we obtain stringent constraints on neutrino physics, including \\(\\sum m_ν< 0.11~\\mathrm{eV}\\) (95\\% CL) and \\(N_{\\rm eff} = 2.98^{+0.13}_{-0.14}\\), fully consistent with Standard Model expectations. Crucially, the data exhibit a statistically significant preference for a late-time dark-energy transition, characterized by a robust quintessence--phantom crossing that remains stable across all dataset combinations and neutrino-sector extensions, including the presence of a sterile neutrino. The combined effects of modified late-time expansion and additional relativistic degrees of freedom systematically raise the inferred Hubble constant, substantially alleviating the \\(H_0\\) tension without invoking early dark energy or introducing theoretical instabilities. Overall, the \\(w_{\\dagger}\\)VCDM scenario emerges as a compelling phenomenological framework that simultaneously accommodates current constraints on neutrino physics, provides an excellent fit to recent BAO and supernovae data, and offers a viable pathway toward resolving persistent tensions in the standard cosmological model."
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-05T12:58:36Z",
                "published_parsed": [
                    2026,
                    1,
                    5,
                    12,
                    58,
                    36,
                    0,
                    5,
                    0
                ],
                "arxiv_comment": "15 pages, 4 figures, 3 tables. Comments are welcome!",
                "arxiv_primary_category": {
                    "term": "astro-ph.CO"
                },
                "authors": [
                    {
                        "name": "Artur Ladeira"
                    },
                    {
                        "name": "Rafael C. Nunes"
                    },
                    {
                        "name": "Supriya Pan"
                    },
                    {
                        "name": "Weiqiang Yang"
                    }
                ],
                "author_detail": {
                    "name": "Weiqiang Yang"
                },
                "author": "Weiqiang Yang"
            },
            {
                "id": "http://arxiv.org/abs/2601.02076v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.02076v1",
                "title": "Deferred Commitment Decoding for Diffusion Language Models with Confidence-Aware Sliding Windows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deferred Commitment Decoding for Diffusion Language Models with Confidence-Aware Sliding Windows"
                },
                "updated": "2026-01-05T12:57:33Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    12,
                    57,
                    33,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.02076v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.02076v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Diffusion language models (DLMs) have recently emerged as a strong alternative to autoregressive models by enabling parallel text generation. To improve inference efficiency and KV-cache compatibility, prior work commonly adopts block-based diffusion, decoding tokens block by block. However, this paradigm suffers from a structural limitation that we term Boundary-Induced Context Truncation (BICT): undecoded tokens near block boundaries are forced to commit without access to nearby future context, even when such context could substantially reduce uncertainty. This limitation degrades decoding confidence and generation quality, especially for tasks requiring precise reasoning, such as mathematical problem solving and code generation. We propose Deferred Commitment Decoding (DCD), a novel, training-free decoding strategy that mitigates this issue. DCD maintains a confidence-aware sliding window over masked tokens, resolving low-uncertainty tokens early while deferring high-uncertainty tokens until sufficient contextual evidence becomes available. This design enables effective bidirectional information flow within the decoding window without sacrificing efficiency. Extensive experiments across multiple diffusion language models, benchmarks, and caching configurations show that DCD improves generation accuracy by 1.39% with comparable time on average compared to fixed block-based diffusion methods, with the most significant improvement reaching 9.0%. These results demonstrate that deferring token commitment based on uncertainty is a simple yet effective principle for improving both the quality and efficiency of diffusion language model decoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion language models (DLMs) have recently emerged as a strong alternative to autoregressive models by enabling parallel text generation. To improve inference efficiency and KV-cache compatibility, prior work commonly adopts block-based diffusion, decoding tokens block by block. However, this paradigm suffers from a structural limitation that we term Boundary-Induced Context Truncation (BICT): undecoded tokens near block boundaries are forced to commit without access to nearby future context, even when such context could substantially reduce uncertainty. This limitation degrades decoding confidence and generation quality, especially for tasks requiring precise reasoning, such as mathematical problem solving and code generation. We propose Deferred Commitment Decoding (DCD), a novel, training-free decoding strategy that mitigates this issue. DCD maintains a confidence-aware sliding window over masked tokens, resolving low-uncertainty tokens early while deferring high-uncertainty tokens until sufficient contextual evidence becomes available. This design enables effective bidirectional information flow within the decoding window without sacrificing efficiency. Extensive experiments across multiple diffusion language models, benchmarks, and caching configurations show that DCD improves generation accuracy by 1.39% with comparable time on average compared to fixed block-based diffusion methods, with the most significant improvement reaching 9.0%. These results demonstrate that deferring token commitment based on uncertainty is a simple yet effective principle for improving both the quality and efficiency of diffusion language model decoding."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-05T12:57:33Z",
                "published_parsed": [
                    2026,
                    1,
                    5,
                    12,
                    57,
                    33,
                    0,
                    5,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Yingte Shu"
                    },
                    {
                        "name": "Yuchuan Tian"
                    },
                    {
                        "name": "Chao Xu"
                    },
                    {
                        "name": "Yunhe Wang"
                    },
                    {
                        "name": "Hanting Chen"
                    }
                ],
                "author_detail": {
                    "name": "Hanting Chen"
                },
                "author": "Hanting Chen"
            },
            {
                "id": "http://arxiv.org/abs/2601.02075v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.02075v1",
                "title": "MDAgent2: Large Language Model for Code Generation and Knowledge Q&A in Molecular Dynamics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MDAgent2: Large Language Model for Code Generation and Knowledge Q&A in Molecular Dynamics"
                },
                "updated": "2026-01-05T12:56:51Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    12,
                    56,
                    51,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.02075v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.02075v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Molecular dynamics (MD) simulations are essential for understanding atomic-scale behaviors in materials science, yet writing LAMMPS scripts remains highly specialized and time-consuming tasks. Although LLMs show promise in code generation and domain-specific question answering, their performance in MD scenarios is limited by scarce domain data, the high deployment cost of state-of-the-art LLMs, and low code executability. Building upon our prior MDAgent, we present MDAgent2, the first end-to-end framework capable of performing both knowledge Q&A and code generation within the MD domain. We construct a domain-specific data-construction pipeline that yields three high-quality datasets spanning MD knowledge, question answering, and code generation. Based on these datasets, we adopt a three stage post-training strategy--continued pre-training (CPT), supervised fine-tuning (SFT), and reinforcement learning (RL)--to train two domain-adapted models, MD-Instruct and MD-Code. Furthermore, we introduce MD-GRPO, a closed-loop RL method that leverages simulation outcomes as reward signals and recycles low-reward trajectories for continual refinement. We further build MDAgent2-RUNTIME, a deployable multi-agent system that integrates code generation, execution, evaluation, and self-correction. Together with MD-EvalBench proposed in this work, the first benchmark for LAMMPS code generation and question answering, our models and system achieve performance surpassing several strong baselines.This work systematically demonstrates the adaptability and generalization capability of large language models in industrial simulation tasks, laying a methodological foundation for automatic code generation in AI for Science and industrial-scale simulations. URL: https://github.com/FredericVAN/PKU_MDAgent2",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Molecular dynamics (MD) simulations are essential for understanding atomic-scale behaviors in materials science, yet writing LAMMPS scripts remains highly specialized and time-consuming tasks. Although LLMs show promise in code generation and domain-specific question answering, their performance in MD scenarios is limited by scarce domain data, the high deployment cost of state-of-the-art LLMs, and low code executability. Building upon our prior MDAgent, we present MDAgent2, the first end-to-end framework capable of performing both knowledge Q&A and code generation within the MD domain. We construct a domain-specific data-construction pipeline that yields three high-quality datasets spanning MD knowledge, question answering, and code generation. Based on these datasets, we adopt a three stage post-training strategy--continued pre-training (CPT), supervised fine-tuning (SFT), and reinforcement learning (RL)--to train two domain-adapted models, MD-Instruct and MD-Code. Furthermore, we introduce MD-GRPO, a closed-loop RL method that leverages simulation outcomes as reward signals and recycles low-reward trajectories for continual refinement. We further build MDAgent2-RUNTIME, a deployable multi-agent system that integrates code generation, execution, evaluation, and self-correction. Together with MD-EvalBench proposed in this work, the first benchmark for LAMMPS code generation and question answering, our models and system achieve performance surpassing several strong baselines.This work systematically demonstrates the adaptability and generalization capability of large language models in industrial simulation tasks, laying a methodological foundation for automatic code generation in AI for Science and industrial-scale simulations. URL: https://github.com/FredericVAN/PKU_MDAgent2"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-05T12:56:51Z",
                "published_parsed": [
                    2026,
                    1,
                    5,
                    12,
                    56,
                    51,
                    0,
                    5,
                    0
                ],
                "arxiv_comment": "24 pages,4 figures",
                "arxiv_primary_category": {
                    "term": "cs.CE"
                },
                "authors": [
                    {
                        "name": "Zhuofan Shi"
                    },
                    {
                        "name": "Hubao A"
                    },
                    {
                        "name": "Yufei Shao"
                    },
                    {
                        "name": "Mengyan Dai"
                    },
                    {
                        "name": "Yadong Yu"
                    },
                    {
                        "name": "Pan Xiang"
                    },
                    {
                        "name": "Dongliang Huang"
                    },
                    {
                        "name": "Hongxu An"
                    },
                    {
                        "name": "Chunxiao Xin"
                    },
                    {
                        "name": "Haiyang Shen"
                    },
                    {
                        "name": "Zhenyu Wang"
                    },
                    {
                        "name": "Yunshan Na"
                    },
                    {
                        "name": "Gang Huang"
                    },
                    {
                        "name": "Xiang Jing"
                    }
                ],
                "author_detail": {
                    "name": "Xiang Jing"
                },
                "author": "Xiang Jing"
            },
            {
                "id": "http://arxiv.org/abs/2503.21408v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2503.21408v2",
                "title": "VALLR: Visual ASR Language Model for Lip Reading",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VALLR: Visual ASR Language Model for Lip Reading"
                },
                "updated": "2026-01-05T12:55:42Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    12,
                    55,
                    42,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2503.21408v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2503.21408v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Lip Reading, or Visual Automatic Speech Recognition (V-ASR), is a complex task requiring the interpretation of spoken language exclusively from visual cues, primarily lip movements and facial expressions. This task is especially challenging due to the absence of auditory information and the inherent ambiguity when visually distinguishing phonemes that have overlapping visemes where different phonemes appear identical on the lips. Current methods typically attempt to predict words or characters directly from these visual cues, but this approach frequently encounters high error rates due to coarticulation effects and viseme ambiguity. We propose a novel two-stage, phoneme-centric framework for Visual Automatic Speech Recognition (V-ASR) that addresses these longstanding challenges. First, our model predicts a compact sequence of phonemes from visual inputs using a Video Transformer with a CTC head, thereby reducing the task complexity and achieving robust speaker invariance. This phoneme output then serves as the input to a fine-tuned Large Language Model (LLM), which reconstructs coherent words and sentences by leveraging broader linguistic context. Unlike existing methods that either predict words directly-often faltering on visually similar phonemes-or rely on large-scale multimodal pre-training, our approach explicitly encodes intermediate linguistic structure while remaining highly data efficient. We demonstrate state-of-the-art performance on two challenging datasets, LRS2 and LRS3, where our method achieves significant reductions in Word Error Rate (WER) achieving a SOTA WER of 18.7 on LRS3 despite using 99.4% less labelled data than the next best approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lip Reading, or Visual Automatic Speech Recognition (V-ASR), is a complex task requiring the interpretation of spoken language exclusively from visual cues, primarily lip movements and facial expressions. This task is especially challenging due to the absence of auditory information and the inherent ambiguity when visually distinguishing phonemes that have overlapping visemes where different phonemes appear identical on the lips. Current methods typically attempt to predict words or characters directly from these visual cues, but this approach frequently encounters high error rates due to coarticulation effects and viseme ambiguity. We propose a novel two-stage, phoneme-centric framework for Visual Automatic Speech Recognition (V-ASR) that addresses these longstanding challenges. First, our model predicts a compact sequence of phonemes from visual inputs using a Video Transformer with a CTC head, thereby reducing the task complexity and achieving robust speaker invariance. This phoneme output then serves as the input to a fine-tuned Large Language Model (LLM), which reconstructs coherent words and sentences by leveraging broader linguistic context. Unlike existing methods that either predict words directly-often faltering on visually similar phonemes-or rely on large-scale multimodal pre-training, our approach explicitly encodes intermediate linguistic structure while remaining highly data efficient. We demonstrate state-of-the-art performance on two challenging datasets, LRS2 and LRS3, where our method achieves significant reductions in Word Error Rate (WER) achieving a SOTA WER of 18.7 on LRS3 despite using 99.4% less labelled data than the next best approach."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-03-27T11:52:08Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    11,
                    52,
                    8,
                    3,
                    86,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Marshall Thomas"
                    },
                    {
                        "name": "Edward Fish"
                    },
                    {
                        "name": "Richard Bowden"
                    }
                ],
                "author_detail": {
                    "name": "Richard Bowden"
                },
                "author": "Richard Bowden"
            },
            {
                "id": "http://arxiv.org/abs/2601.02071v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.02071v1",
                "title": "FormuLLA: A Large Language Model Approach to Generating Novel 3D Printable Formulations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FormuLLA: A Large Language Model Approach to Generating Novel 3D Printable Formulations"
                },
                "updated": "2026-01-05T12:50:50Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    12,
                    50,
                    50,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.02071v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.02071v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Pharmaceutical three-dimensional (3D) printing is an advanced fabrication technology with the potential to enable truly personalised dosage forms. Recent studies have integrated artificial intelligence (AI) to accelerate formulation and process development, drastically transforming current approaches to pharmaceutical 3D printing. To date, most AI-driven efforts remain narrowly focused, while failing to account for the broader formulation challenges inherent to the technology. Recent advances in AI have introduced artificial general intelligence concepts, wherein systems extend beyond conventional predictive modelling toward more generalised, human-like reasoning. In this work, we investigate the application of large language models (LLMs), fine-tuned on a fused deposition modelling (FDM) dataset comprising over 1400 formulations, to recommend suitable excipients based on active pharmaceutical ingredient (API) dose, and predict filament mechanical properties. Four LLM architectures were fine-tuned, with systematic evaluation of both fine-tuning and generative parameter configurations. Our results demonstrate that Llama2 was best suited for recommending excipients for FDM formulations. Additionally, model selection and parameterisation significantly influence performance, with smaller LLMs exhibiting instances of catastrophic forgetting. Furthermore, we demonstrate: (i) even with relatively small dataset of over 1400 formulations, it can lead to model catastrophic forgetting; (ii) standard LLM metrics only evaluate linguistic performance but not formulation processability; and (iii) LLMs trained on biomedically-related data do not always produce the best results. Addressing these challenges is essential to advancing LLMs beyond linguistic proficiency and toward reliable systems for pharmaceutical formulation development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pharmaceutical three-dimensional (3D) printing is an advanced fabrication technology with the potential to enable truly personalised dosage forms. Recent studies have integrated artificial intelligence (AI) to accelerate formulation and process development, drastically transforming current approaches to pharmaceutical 3D printing. To date, most AI-driven efforts remain narrowly focused, while failing to account for the broader formulation challenges inherent to the technology. Recent advances in AI have introduced artificial general intelligence concepts, wherein systems extend beyond conventional predictive modelling toward more generalised, human-like reasoning. In this work, we investigate the application of large language models (LLMs), fine-tuned on a fused deposition modelling (FDM) dataset comprising over 1400 formulations, to recommend suitable excipients based on active pharmaceutical ingredient (API) dose, and predict filament mechanical properties. Four LLM architectures were fine-tuned, with systematic evaluation of both fine-tuning and generative parameter configurations. Our results demonstrate that Llama2 was best suited for recommending excipients for FDM formulations. Additionally, model selection and parameterisation significantly influence performance, with smaller LLMs exhibiting instances of catastrophic forgetting. Furthermore, we demonstrate: (i) even with relatively small dataset of over 1400 formulations, it can lead to model catastrophic forgetting; (ii) standard LLM metrics only evaluate linguistic performance but not formulation processability; and (iii) LLMs trained on biomedically-related data do not always produce the best results. Addressing these challenges is essential to advancing LLMs beyond linguistic proficiency and toward reliable systems for pharmaceutical formulation development."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-05T12:50:50Z",
                "published_parsed": [
                    2026,
                    1,
                    5,
                    12,
                    50,
                    50,
                    0,
                    5,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Adeshola Okubena"
                    },
                    {
                        "name": "Yusuf Ali Mohammed"
                    },
                    {
                        "name": "Moe Elbadawi"
                    }
                ],
                "author_detail": {
                    "name": "Moe Elbadawi"
                },
                "author": "Moe Elbadawi"
            },
            {
                "id": "http://arxiv.org/abs/2601.02069v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.02069v1",
                "title": "Reinforcement Learning Based Computationally Efficient Conditional Choice Simulation Estimation of Dynamic Discrete Choice Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning Based Computationally Efficient Conditional Choice Simulation Estimation of Dynamic Discrete Choice Models"
                },
                "updated": "2026-01-05T12:50:03Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    12,
                    50,
                    3,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.02069v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.02069v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Dynamic discrete choice (DDC) models have found widespread application in marketing. However, estimating these becomes challenging in \"big data\" settings with high-dimensional state-action spaces. To address this challenge, this paper develops a Reinforcement Learning (RL)-based two-step (\"computationally light\") Conditional Choice Simulation (CCS) estimation approach that combines the scalability of machine learning with the transparency, explainability, and interpretability of structural models, which is particularly valuable for counterfactual policy analysis. The method is premised on three insights: (1) the CCS (\"forward simulation\") approach is a special case of RL algorithms, (2) starting from an initial state-action pair, CCS updates the corresponding value function only after each simulation path has terminated, whereas RL algorithms may update for all the state-action pairs visited along a simulated path, and (3) RL focuses on inferring an agent's optimal policy with known reward functions, whereas DDC models focus on estimating the reward functions presupposing optimal policies. The procedure's computational efficiency over CCS estimation is demonstrated using Monte Carlo simulations with a canonical machine replacement and a consumer food purchase model. Framing CCS estimation of DDC models as an RL problem increases their applicability and scalability to high-dimensional marketing problems while retaining both interpretability and tractability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic discrete choice (DDC) models have found widespread application in marketing. However, estimating these becomes challenging in \"big data\" settings with high-dimensional state-action spaces. To address this challenge, this paper develops a Reinforcement Learning (RL)-based two-step (\"computationally light\") Conditional Choice Simulation (CCS) estimation approach that combines the scalability of machine learning with the transparency, explainability, and interpretability of structural models, which is particularly valuable for counterfactual policy analysis. The method is premised on three insights: (1) the CCS (\"forward simulation\") approach is a special case of RL algorithms, (2) starting from an initial state-action pair, CCS updates the corresponding value function only after each simulation path has terminated, whereas RL algorithms may update for all the state-action pairs visited along a simulated path, and (3) RL focuses on inferring an agent's optimal policy with known reward functions, whereas DDC models focus on estimating the reward functions presupposing optimal policies. The procedure's computational efficiency over CCS estimation is demonstrated using Monte Carlo simulations with a canonical machine replacement and a consumer food purchase model. Framing CCS estimation of DDC models as an RL problem increases their applicability and scalability to high-dimensional marketing problems while retaining both interpretability and tractability."
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-05T12:50:03Z",
                "published_parsed": [
                    2026,
                    1,
                    5,
                    12,
                    50,
                    3,
                    0,
                    5,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM"
                },
                "authors": [
                    {
                        "name": "Ahmed Khwaja"
                    },
                    {
                        "name": "Sonal Srivastava"
                    }
                ],
                "author_detail": {
                    "name": "Sonal Srivastava"
                },
                "author": "Sonal Srivastava"
            },
            {
                "id": "http://arxiv.org/abs/2512.09972v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.09972v3",
                "title": "SIP-BMM: Constructing the Capability--Efficiency Pareto Set for LLMs via Structural Importance Prior Bayesian Model Merging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SIP-BMM: Constructing the Capability--Efficiency Pareto Set for LLMs via Structural Importance Prior Bayesian Model Merging"
                },
                "updated": "2026-01-05T12:45:09Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    12,
                    45,
                    9,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.09972v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.09972v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Constructing a Pareto set is pivotal for navigating the capability--efficiency trade-offs in Large Language Models (LLMs). However, existing merging techniques remain inadequate for this task. Coarse-grained, model-level methods yield only a sparse set of suboptimal solutions, while fine-grained, layer-wise approaches suffer from the curse of dimensionality, rendering the search space computationally intractable. To resolve this dichotomy, we propose Structural Importance Prior Bayesian Model Merging (SIP-BMM), a framework that automatically constructs the LLM Pareto set. SIP-BMM renders high-dimensional layer-wise search tractable by introducing an importance-aware Sparse Axis-Aligned Subspace Bayesian Optimization (SAASBO) strategy. By leveraging a structural importance prior derived from task-vector differences, our method guides SAASBO to automatically identify critical layers, thereby dramatically reducing the effective dimensionality without sacrificing the granularity of full-model control. The entire process is automated within an evolutionary loop driven by the Log-Noisy Expected Hypervolume Improvement ($q$NEHVI) acquisition function. Experiments demonstrate that SIP-BMM discovers a stronger and denser Pareto front than competitive baselines, enabling agile model selection tailored to diverse operational constraints. Code is available at: https://github.com/MiLab-HITSZ/2026-SIPBMM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Constructing a Pareto set is pivotal for navigating the capability--efficiency trade-offs in Large Language Models (LLMs). However, existing merging techniques remain inadequate for this task. Coarse-grained, model-level methods yield only a sparse set of suboptimal solutions, while fine-grained, layer-wise approaches suffer from the curse of dimensionality, rendering the search space computationally intractable. To resolve this dichotomy, we propose Structural Importance Prior Bayesian Model Merging (SIP-BMM), a framework that automatically constructs the LLM Pareto set. SIP-BMM renders high-dimensional layer-wise search tractable by introducing an importance-aware Sparse Axis-Aligned Subspace Bayesian Optimization (SAASBO) strategy. By leveraging a structural importance prior derived from task-vector differences, our method guides SAASBO to automatically identify critical layers, thereby dramatically reducing the effective dimensionality without sacrificing the granularity of full-model control. The entire process is automated within an evolutionary loop driven by the Log-Noisy Expected Hypervolume Improvement ($q$NEHVI) acquisition function. Experiments demonstrate that SIP-BMM discovers a stronger and denser Pareto front than competitive baselines, enabling agile model selection tailored to diverse operational constraints. Code is available at: https://github.com/MiLab-HITSZ/2026-SIPBMM."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-10T15:32:56Z",
                "published_parsed": [
                    2025,
                    12,
                    10,
                    15,
                    32,
                    56,
                    2,
                    344,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Kesheng Chen"
                    },
                    {
                        "name": "Yamin Hu"
                    },
                    {
                        "name": "Zhenqian Zhu"
                    },
                    {
                        "name": "Wenjian Luo"
                    },
                    {
                        "name": "Yiya Diao"
                    }
                ],
                "author_detail": {
                    "name": "Yiya Diao"
                },
                "author": "Yiya Diao"
            },
            {
                "id": "http://arxiv.org/abs/2601.02065v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.02065v1",
                "title": "Cost-Efficient Cross-Lingual Retrieval-Augmented Generation for Low-Resource Languages: A Case Study in Bengali Agricultural Advisory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cost-Efficient Cross-Lingual Retrieval-Augmented Generation for Low-Resource Languages: A Case Study in Bengali Agricultural Advisory"
                },
                "updated": "2026-01-05T12:41:44Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    12,
                    41,
                    44,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.02065v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.02065v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Access to reliable agricultural advisory remains limited in many developing regions due to a persistent language barrier: authoritative agricultural manuals are predominantly written in English, while farmers primarily communicate in low-resource local languages such as Bengali. Although recent advances in Large Language Models (LLMs) enable natural language interaction, direct generation in low-resource languages often exhibits poor fluency and factual inconsistency, while cloud-based solutions remain cost-prohibitive. This paper presents a cost-efficient, cross-lingual Retrieval-Augmented Generation (RAG) framework for Bengali agricultural advisory that emphasizes factual grounding and practical deployability. The proposed system adopts a translation-centric architecture in which Bengali user queries are translated into English, enriched through domain-specific keyword injection to align colloquial farmer terminology with scientific nomenclature, and answered via dense vector retrieval over a curated corpus of English agricultural manuals (FAO, IRRI). The generated English response is subsequently translated back into Bengali to ensure accessibility. The system is implemented entirely using open-source models and operates on consumer-grade hardware without reliance on paid APIs. Experimental evaluation demonstrates reliable source-grounded responses, robust rejection of out-of-domain queries, and an average end-to-end latency below 20 seconds. The results indicate that cross-lingual retrieval combined with controlled translation offers a practical and scalable solution for agricultural knowledge access in low-resource language settings",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Access to reliable agricultural advisory remains limited in many developing regions due to a persistent language barrier: authoritative agricultural manuals are predominantly written in English, while farmers primarily communicate in low-resource local languages such as Bengali. Although recent advances in Large Language Models (LLMs) enable natural language interaction, direct generation in low-resource languages often exhibits poor fluency and factual inconsistency, while cloud-based solutions remain cost-prohibitive. This paper presents a cost-efficient, cross-lingual Retrieval-Augmented Generation (RAG) framework for Bengali agricultural advisory that emphasizes factual grounding and practical deployability. The proposed system adopts a translation-centric architecture in which Bengali user queries are translated into English, enriched through domain-specific keyword injection to align colloquial farmer terminology with scientific nomenclature, and answered via dense vector retrieval over a curated corpus of English agricultural manuals (FAO, IRRI). The generated English response is subsequently translated back into Bengali to ensure accessibility. The system is implemented entirely using open-source models and operates on consumer-grade hardware without reliance on paid APIs. Experimental evaluation demonstrates reliable source-grounded responses, robust rejection of out-of-domain queries, and an average end-to-end latency below 20 seconds. The results indicate that cross-lingual retrieval combined with controlled translation offers a practical and scalable solution for agricultural knowledge access in low-resource language settings"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-05T12:41:44Z",
                "published_parsed": [
                    2026,
                    1,
                    5,
                    12,
                    41,
                    44,
                    0,
                    5,
                    0
                ],
                "arxiv_comment": "5 pages, 3 figures, 1 table",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Md. Asif Hossain"
                    },
                    {
                        "name": "Nabil Subhan"
                    },
                    {
                        "name": "Mantasha Rahman Mahi"
                    },
                    {
                        "name": "Jannatul Ferdous Nabila"
                    }
                ],
                "author_detail": {
                    "name": "Jannatul Ferdous Nabila"
                },
                "author": "Jannatul Ferdous Nabila"
            },
            {
                "id": "http://arxiv.org/abs/2601.02063v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.02063v1",
                "title": "How much neuroscience does a neuroscientist need to know?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How much neuroscience does a neuroscientist need to know?"
                },
                "updated": "2026-01-05T12:40:00Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    12,
                    40,
                    0,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.02063v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.02063v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "How much of the brain's learned algorithms depend on the fact it is a brain? We argue: a lot, but surprisingly few details matter. We point to simple biological details -- e.g. nonnegative firing and energetic/space budgets in connectionist architectures -- which, when mixed with the requirements of solving a task, produce models that predict brain responses down to single-neuron tuning. We understand this as details constraining the set of plausible algorithms, and their implementations, such that only `brain-like' algorithms are learned. In particular, each biological detail breaks a symmetry in connectionist models (scale, rotation, permutation) leading to interpretable single-neuron responses that are meaningfully characteristic of particular algorithms. This view helps us not only understand the brain's choice of algorithm but also infer algorithm from measured neural responses. Further, this perspective aligns computational neuroscience with mechanistic interpretability in AI, suggesting a more unified approach to studying the mechanisms of intelligence, both natural and artificial.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How much of the brain's learned algorithms depend on the fact it is a brain? We argue: a lot, but surprisingly few details matter. We point to simple biological details -- e.g. nonnegative firing and energetic/space budgets in connectionist architectures -- which, when mixed with the requirements of solving a task, produce models that predict brain responses down to single-neuron tuning. We understand this as details constraining the set of plausible algorithms, and their implementations, such that only `brain-like' algorithms are learned. In particular, each biological detail breaks a symmetry in connectionist models (scale, rotation, permutation) leading to interpretable single-neuron responses that are meaningfully characteristic of particular algorithms. This view helps us not only understand the brain's choice of algorithm but also infer algorithm from measured neural responses. Further, this perspective aligns computational neuroscience with mechanistic interpretability in AI, suggesting a more unified approach to studying the mechanisms of intelligence, both natural and artificial."
                },
                "tags": [
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-05T12:40:00Z",
                "published_parsed": [
                    2026,
                    1,
                    5,
                    12,
                    40,
                    0,
                    0,
                    5,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.NC"
                },
                "authors": [
                    {
                        "name": "James C. R. Whittington"
                    },
                    {
                        "name": "William Dorrell"
                    }
                ],
                "author_detail": {
                    "name": "William Dorrell"
                },
                "author": "William Dorrell"
            },
            {
                "id": "http://arxiv.org/abs/2601.02060v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.02060v1",
                "title": "Perish or Flourish? A Holistic Evaluation of Large Language Models for Code Generation in Functional Programming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Perish or Flourish? A Holistic Evaluation of Large Language Models for Code Generation in Functional Programming"
                },
                "updated": "2026-01-05T12:33:37Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    12,
                    33,
                    37,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.02060v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.02060v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Functional programming provides strong foundations for developing reliable and secure software systems, yet its adoption remains not widespread due to the steep learning curve. Recent advances in Large Language Models (LLMs) for code generation present new opportunities to lower these barriers. However, extensive evaluations of LLMs largely focus on imperative programming languages, and their capabilities in functional programming languages (FP) remain underexplored. To address this gap, we introduce FPEval, a holistic evaluation framework built on FPBench, a new benchmark of 721 programming tasks across three difficulty levels on three mainstream FP languages: Haskell, Ocaml and Scala. FPEval provides compehensive evaluation infrastructures with both test validations with comprehensive test suites and static analysis tools to assess both functional correctness and code style and maintainability. Using this framework, we evaluate state-of-the-art LLMs, including GPT-3.5, GPT-4o, and GPT-5, for code generation in functional programming languages and Java as an imperative baseline. Our results demonstrate that LLM performance in functional programming improves substantially with model advancement; however, error rates remain significantly higher in purely functional languages (Haskell and OCaml) than in hybrid (Scala) or imperative (Java) languages. Moreover, LLMs frequently generate non-idiomatic functional code that follows imperative patterns, raising concerns about code style and long-term maintainability. Finally, we show that LLMs can partially self-repair both correctness and quality issues when provided with static analysis feedback and hand-crafted instructions for common types of issues.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Functional programming provides strong foundations for developing reliable and secure software systems, yet its adoption remains not widespread due to the steep learning curve. Recent advances in Large Language Models (LLMs) for code generation present new opportunities to lower these barriers. However, extensive evaluations of LLMs largely focus on imperative programming languages, and their capabilities in functional programming languages (FP) remain underexplored. To address this gap, we introduce FPEval, a holistic evaluation framework built on FPBench, a new benchmark of 721 programming tasks across three difficulty levels on three mainstream FP languages: Haskell, Ocaml and Scala. FPEval provides compehensive evaluation infrastructures with both test validations with comprehensive test suites and static analysis tools to assess both functional correctness and code style and maintainability. Using this framework, we evaluate state-of-the-art LLMs, including GPT-3.5, GPT-4o, and GPT-5, for code generation in functional programming languages and Java as an imperative baseline. Our results demonstrate that LLM performance in functional programming improves substantially with model advancement; however, error rates remain significantly higher in purely functional languages (Haskell and OCaml) than in hybrid (Scala) or imperative (Java) languages. Moreover, LLMs frequently generate non-idiomatic functional code that follows imperative patterns, raising concerns about code style and long-term maintainability. Finally, we show that LLMs can partially self-repair both correctness and quality issues when provided with static analysis feedback and hand-crafted instructions for common types of issues."
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-05T12:33:37Z",
                "published_parsed": [
                    2026,
                    1,
                    5,
                    12,
                    33,
                    37,
                    0,
                    5,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL"
                },
                "authors": [
                    {
                        "name": "Nguyet-Anh H. Lang"
                    },
                    {
                        "name": "Eric Lang"
                    },
                    {
                        "name": "Thanh Le-Cong"
                    },
                    {
                        "name": "Bach Le"
                    },
                    {
                        "name": "Quyet-Thang Huynh"
                    }
                ],
                "author_detail": {
                    "name": "Quyet-Thang Huynh"
                },
                "author": "Quyet-Thang Huynh"
            },
            {
                "id": "http://arxiv.org/abs/2512.10426v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.10426v2",
                "title": "Differential Privacy for Secure Machine Learning in Healthcare IoT-Cloud Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Differential Privacy for Secure Machine Learning in Healthcare IoT-Cloud Systems"
                },
                "updated": "2026-01-05T12:15:57Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    12,
                    15,
                    57,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.10426v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.10426v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Healthcare has become exceptionally sophisticated, as wearables and connected medical devices are revolutionising remote patient monitoring, emergency response, medication management, diagnosis, and predictive and prescriptive analytics. Internet of Things and Cloud computing integrated systems (IoT-Cloud) facilitate sensing, automation, and processing for these healthcare applications. While real-time response is crucial for alleviating patient emergencies, protecting patient privacy is extremely important in data-driven healthcare. In this paper, we propose a multi-layer IoT, Edge and Cloud architecture to enhance the speed of response for emergency healthcare by distributing tasks based on response criticality and permanence of storage. Privacy of patient data is assured by proposing a Differential Privacy framework across several machine learning models such as K-means, Logistic Regression, Random Forest and Naive Bayes. We establish a comprehensive threat model identifying three adversary classes and evaluate Laplace, Gaussian, and hybrid noise mechanisms across varying privacy budgets, with supervised algorithms achieving up to 86% accuracy. The proposed hybrid Laplace-Gaussian noise mechanism with adaptive budget allocation provides a balanced approach, offering moderate tails and better privacy-utility trade-offs for both low and high dimension datasets. At the practical threshold of $\\varepsilon = 5.0$, supervised algorithms achieve 82-84% accuracy while reducing attribute inference attacks by up to 18% and data reconstruction correlation by 70%. Blockchain security further ensures trusted communication through time-stamping, traceability, and immutability for analytics applications. Edge computing demonstrates 8$\\times$ latency reduction for emergency scenarios, validating the hierarchical architecture for time-critical operations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Healthcare has become exceptionally sophisticated, as wearables and connected medical devices are revolutionising remote patient monitoring, emergency response, medication management, diagnosis, and predictive and prescriptive analytics. Internet of Things and Cloud computing integrated systems (IoT-Cloud) facilitate sensing, automation, and processing for these healthcare applications. While real-time response is crucial for alleviating patient emergencies, protecting patient privacy is extremely important in data-driven healthcare. In this paper, we propose a multi-layer IoT, Edge and Cloud architecture to enhance the speed of response for emergency healthcare by distributing tasks based on response criticality and permanence of storage. Privacy of patient data is assured by proposing a Differential Privacy framework across several machine learning models such as K-means, Logistic Regression, Random Forest and Naive Bayes. We establish a comprehensive threat model identifying three adversary classes and evaluate Laplace, Gaussian, and hybrid noise mechanisms across varying privacy budgets, with supervised algorithms achieving up to 86% accuracy. The proposed hybrid Laplace-Gaussian noise mechanism with adaptive budget allocation provides a balanced approach, offering moderate tails and better privacy-utility trade-offs for both low and high dimension datasets. At the practical threshold of $\\varepsilon = 5.0$, supervised algorithms achieve 82-84% accuracy while reducing attribute inference attacks by up to 18% and data reconstruction correlation by 70%. Blockchain security further ensures trusted communication through time-stamping, traceability, and immutability for analytics applications. Edge computing demonstrates 8$\\times$ latency reduction for emergency scenarios, validating the hierarchical architecture for time-critical operations."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-11T08:37:37Z",
                "published_parsed": [
                    2025,
                    12,
                    11,
                    8,
                    37,
                    37,
                    3,
                    345,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "authors": [
                    {
                        "name": "N Mangala"
                    },
                    {
                        "name": "Murtaza Rangwala"
                    },
                    {
                        "name": "S Aishwarya"
                    },
                    {
                        "name": "B Eswara Reddy"
                    },
                    {
                        "name": "Rajkumar Buyya"
                    },
                    {
                        "name": "KR Venugopal"
                    },
                    {
                        "name": "SS Iyengar"
                    },
                    {
                        "name": "LM Patnaik"
                    }
                ],
                "author_detail": {
                    "name": "LM Patnaik"
                },
                "author": "LM Patnaik"
            },
            {
                "id": "http://arxiv.org/abs/2512.24015v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.24015v2",
                "title": "On Exact Editing of Flow-Based Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Exact Editing of Flow-Based Diffusion Models"
                },
                "updated": "2026-01-05T12:05:00Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    12,
                    5,
                    0,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.24015v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.24015v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Recent methods in flow-based diffusion editing have enabled direct transformations between source and target image distribution without explicit inversion. However, the latent trajectories in these methods often exhibit accumulated velocity errors, leading to semantic inconsistency and loss of structural fidelity. We propose Conditioned Velocity Correction (CVC), a principled framework that reformulates flow-based editing as a distribution transformation problem driven by a known source prior. CVC rethinks the role of velocity in inter-distribution transformation by introducing a dual-perspective velocity conversion mechanism. This mechanism explicitly decomposes the latent evolution into two components: a structure-preserving branch that remains consistent with the source trajectory, and a semantically-guided branch that drives a controlled deviation toward the target distribution. The conditional velocity field exhibits an absolute velocity error relative to the true underlying distribution trajectory, which inherently introduces potential instability and trajectory drift in the latent space. To address this quantifiable deviation and maintain fidelity to the true flow, we apply a posterior-consistent update to the resulting conditional velocity field. This update is derived from Empirical Bayes Inference and Tweedie correction, which ensures a mathematically grounded error compensation over time. Our method yields stable and interpretable latent dynamics, achieving faithful reconstruction alongside smooth local semantic conversion. Comprehensive experiments demonstrate that CVC consistently achieves superior fidelity, better semantic alignment, and more reliable editing behavior across diverse tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent methods in flow-based diffusion editing have enabled direct transformations between source and target image distribution without explicit inversion. However, the latent trajectories in these methods often exhibit accumulated velocity errors, leading to semantic inconsistency and loss of structural fidelity. We propose Conditioned Velocity Correction (CVC), a principled framework that reformulates flow-based editing as a distribution transformation problem driven by a known source prior. CVC rethinks the role of velocity in inter-distribution transformation by introducing a dual-perspective velocity conversion mechanism. This mechanism explicitly decomposes the latent evolution into two components: a structure-preserving branch that remains consistent with the source trajectory, and a semantically-guided branch that drives a controlled deviation toward the target distribution. The conditional velocity field exhibits an absolute velocity error relative to the true underlying distribution trajectory, which inherently introduces potential instability and trajectory drift in the latent space. To address this quantifiable deviation and maintain fidelity to the true flow, we apply a posterior-consistent update to the resulting conditional velocity field. This update is derived from Empirical Bayes Inference and Tweedie correction, which ensures a mathematically grounded error compensation over time. Our method yields stable and interpretable latent dynamics, achieving faithful reconstruction alongside smooth local semantic conversion. Comprehensive experiments demonstrate that CVC consistently achieves superior fidelity, better semantic alignment, and more reliable editing behavior across diverse tasks."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-30T06:29:20Z",
                "published_parsed": [
                    2025,
                    12,
                    30,
                    6,
                    29,
                    20,
                    1,
                    364,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Zixiang Li"
                    },
                    {
                        "name": "Yue Song"
                    },
                    {
                        "name": "Jianing Peng"
                    },
                    {
                        "name": "Ting Liu"
                    },
                    {
                        "name": "Jun Huang"
                    },
                    {
                        "name": "Xiaochao Qu"
                    },
                    {
                        "name": "Luoqi Liu"
                    },
                    {
                        "name": "Wei Wang"
                    },
                    {
                        "name": "Yao Zhao"
                    },
                    {
                        "name": "Yunchao Wei"
                    }
                ],
                "author_detail": {
                    "name": "Yunchao Wei"
                },
                "author": "Yunchao Wei"
            },
            {
                "id": "http://arxiv.org/abs/2601.02045v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.02045v1",
                "title": "The New Compiler Stack: A Survey on the Synergy of LLMs and Compilers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The New Compiler Stack: A Survey on the Synergy of LLMs and Compilers"
                },
                "updated": "2026-01-05T12:02:57Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    12,
                    2,
                    57,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.02045v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.02045v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.1007/s42514-025-00270-x",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "This survey has provided a systematic overview of the emerging field of LLM-enabled compilation by addressing several key research questions. We first answered how LLMs are being integrated by proposing a comprehensive, multi-dimensional taxonomy that categorizes works based on their Design Philosophy (Selector, Translator, Generator), LLM Methodology, their operational Level of Code Abstraction, and the specific Task Type they address. In answering what advancements these approaches offer, we identified three primary benefits: the democratization of compiler development, the discovery of novel optimization strategies, and the broadening of the compiler's traditional scope. Finally, in addressing the field's challenges and opportunities, we highlighted the critical hurdles of ensuring correctness and achieving scalability, while identifying the development of hybrid systems as the most promising path forward. By providing these answers, this survey serves as a foundational roadmap for researchers and practitioners, charting the course for a new generation of LLM-powered, intelligent, adaptive and synergistic compilation tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This survey has provided a systematic overview of the emerging field of LLM-enabled compilation by addressing several key research questions. We first answered how LLMs are being integrated by proposing a comprehensive, multi-dimensional taxonomy that categorizes works based on their Design Philosophy (Selector, Translator, Generator), LLM Methodology, their operational Level of Code Abstraction, and the specific Task Type they address. In answering what advancements these approaches offer, we identified three primary benefits: the democratization of compiler development, the discovery of novel optimization strategies, and the broadening of the compiler's traditional scope. Finally, in addressing the field's challenges and opportunities, we highlighted the critical hurdles of ensuring correctness and achieving scalability, while identifying the development of hybrid systems as the most promising path forward. By providing these answers, this survey serves as a foundational roadmap for researchers and practitioners, charting the course for a new generation of LLM-powered, intelligent, adaptive and synergistic compilation tools."
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-05T12:02:57Z",
                "published_parsed": [
                    2026,
                    1,
                    5,
                    12,
                    2,
                    57,
                    0,
                    5,
                    0
                ],
                "arxiv_comment": "Accepted by CCF Transactions on High Performance Computing",
                "arxiv_primary_category": {
                    "term": "cs.PL"
                },
                "authors": [
                    {
                        "name": "Shuoming Zhang"
                    },
                    {
                        "name": "Jiacheng Zhao"
                    },
                    {
                        "name": "Qiuchu Yu"
                    },
                    {
                        "name": "Chunwei Xia"
                    },
                    {
                        "name": "Zheng Wang"
                    },
                    {
                        "name": "Xiaobing Feng"
                    },
                    {
                        "name": "Huimin Cui"
                    }
                ],
                "author_detail": {
                    "name": "Huimin Cui"
                },
                "author": "Huimin Cui",
                "arxiv_doi": "10.1007/s42514-025-00270-x"
            },
            {
                "id": "http://arxiv.org/abs/2407.05434v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2407.05434v3",
                "title": "LTLBench: Towards Benchmarks for Evaluating Temporal Reasoning in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LTLBench: Towards Benchmarks for Evaluating Temporal Reasoning in Large Language Models"
                },
                "updated": "2026-01-05T11:55:15Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    11,
                    55,
                    15,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2407.05434v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2407.05434v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Temporal Reasoning (TR) is a critical ability for LLMs to understand and reason over temporal information and relationships between events. To study the TR ability in LLMs, prior works provide different ways for evaluating various aspects of TR ability. In this work, we propose an alternative perspective for evaluating TR ability by leveraging Linear Temporal Logic (LTL), and develop a pipeline to automatically synthesize challenges for assessing the TR ability of LLMs. Based on this pipeline, we construct a dataset, namely LTLBench, consisting of $2000$ TR challenges, and benchmark 12 LLMs across 5 different methods. Furthermore, we conduct additional experiments to investigate the impact of increasing the number of formula operators and events on both LLM performance and the complexity of TR problems. We also perform qualitative analyses of their reasoning processes and the effects of varying the number of events and formula operators, which reveal 3 main issues in their temporal reasoning processes and the unexpected performance changes observed as problem complexity increases. We expect this work to provide valuable insights into the TR ability of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Temporal Reasoning (TR) is a critical ability for LLMs to understand and reason over temporal information and relationships between events. To study the TR ability in LLMs, prior works provide different ways for evaluating various aspects of TR ability. In this work, we propose an alternative perspective for evaluating TR ability by leveraging Linear Temporal Logic (LTL), and develop a pipeline to automatically synthesize challenges for assessing the TR ability of LLMs. Based on this pipeline, we construct a dataset, namely LTLBench, consisting of $2000$ TR challenges, and benchmark 12 LLMs across 5 different methods. Furthermore, we conduct additional experiments to investigate the impact of increasing the number of formula operators and events on both LLM performance and the complexity of TR problems. We also perform qualitative analyses of their reasoning processes and the effects of varying the number of events and formula operators, which reveal 3 main issues in their temporal reasoning processes and the unexpected performance changes observed as problem complexity increases. We expect this work to provide valuable insights into the TR ability of LLMs."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2024-07-07T16:37:06Z",
                "published_parsed": [
                    2024,
                    7,
                    7,
                    16,
                    37,
                    6,
                    6,
                    189,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Weizhi Tang"
                    },
                    {
                        "name": "Kwabena Nuamah"
                    },
                    {
                        "name": "Vaishak Belle"
                    }
                ],
                "author_detail": {
                    "name": "Vaishak Belle"
                },
                "author": "Vaishak Belle"
            },
            {
                "id": "http://arxiv.org/abs/2512.23732v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.23732v2",
                "title": "When in Doubt, Consult: Expert Debate for Sexism Detection via Confidence-Based Routin",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When in Doubt, Consult: Expert Debate for Sexism Detection via Confidence-Based Routin"
                },
                "updated": "2026-01-05T11:54:54Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    11,
                    54,
                    54,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.23732v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.23732v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Sexist content online increasingly appears in subtle, context-dependent forms that evade traditional detection methods. Its interpretation often depends on overlapping linguistic, psychological, legal, and cultural dimensions, which produce mixed and sometimes contradictory signals, even in annotated datasets. These inconsistencies, combined with label scarcity and class imbalance, result in unstable decision boundaries and cause fine-tuned models to overlook subtler, underrepresented forms of harm. Together, these limitations point to the need for a design that explicitly addresses the combined effects of (i) underrepresentation, (ii) noise, and (iii) conceptual ambiguity in both data and model predictions. To address these challenges, we propose a two-stage framework that unifies (i) targeted training procedures to adapt supervision to scarce and noisy data with (ii) selective, reasoning-based inference to handle ambiguous or borderline cases. Our training setup applies class-balanced focal loss, class-aware batching, and post-hoc threshold calibration to mitigate label imbalance and noisy supervision. At inference time, a dynamic routing mechanism classifies high-confidence cases directly and escalates uncertain instances to a novel \\textit{Collaborative Expert Judgment} (CEJ) module, which prompts multiple personas and consolidates their reasoning through a judge model. Our approach achieves state-of-the-art results across several benchmarks, with F1 gains of +4.48% and +1.30% on EDOS Tasks A and B, respectively, and a +2.79% improvement in ICM on EXIST 2025 Task 1.1.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sexist content online increasingly appears in subtle, context-dependent forms that evade traditional detection methods. Its interpretation often depends on overlapping linguistic, psychological, legal, and cultural dimensions, which produce mixed and sometimes contradictory signals, even in annotated datasets. These inconsistencies, combined with label scarcity and class imbalance, result in unstable decision boundaries and cause fine-tuned models to overlook subtler, underrepresented forms of harm. Together, these limitations point to the need for a design that explicitly addresses the combined effects of (i) underrepresentation, (ii) noise, and (iii) conceptual ambiguity in both data and model predictions. To address these challenges, we propose a two-stage framework that unifies (i) targeted training procedures to adapt supervision to scarce and noisy data with (ii) selective, reasoning-based inference to handle ambiguous or borderline cases. Our training setup applies class-balanced focal loss, class-aware batching, and post-hoc threshold calibration to mitigate label imbalance and noisy supervision. At inference time, a dynamic routing mechanism classifies high-confidence cases directly and escalates uncertain instances to a novel \\textit{Collaborative Expert Judgment} (CEJ) module, which prompts multiple personas and consolidates their reasoning through a judge model. Our approach achieves state-of-the-art results across several benchmarks, with F1 gains of +4.48% and +1.30% on EDOS Tasks A and B, respectively, and a +2.79% improvement in ICM on EXIST 2025 Task 1.1."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-21T05:48:57Z",
                "published_parsed": [
                    2025,
                    12,
                    21,
                    5,
                    48,
                    57,
                    6,
                    355,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Anwar Alajmi"
                    },
                    {
                        "name": "Gabriele Pergola"
                    }
                ],
                "author_detail": {
                    "name": "Gabriele Pergola"
                },
                "author": "Gabriele Pergola"
            },
            {
                "id": "http://arxiv.org/abs/2512.07404v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.07404v2",
                "title": "On LLMs' Internal Representation of Code Correctness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On LLMs' Internal Representation of Code Correctness"
                },
                "updated": "2026-01-05T11:52:55Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    11,
                    52,
                    55,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.07404v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.07404v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Despite the effectiveness of large language models (LLMs) for code generation, they often output incorrect code. One reason is that model output probabilities are often not well-correlated with correctness, and reflect only the final output of the generation process. Inspired by findings that LLMs internally encode concepts like truthfulness, this paper explores if LLMs similarly represent code correctness. Specifically, we identify a correctness representation inside LLMs by contrasting the hidden states between pairs of correct and incorrect code for the same programming tasks. By experimenting on four LLMs, we show that exploiting this extracted correctness representation outperforms standard log-likelihood ranking, as well as verbalized model confidence. Furthermore, we explore how this internal correctness signal can be used to select higher-quality code samples, without requiring test execution. Ultimately, this work demonstrates how leveraging internal representations can enhance code generation systems and make LLMs more reliable, thus improving confidence in automatically generated code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the effectiveness of large language models (LLMs) for code generation, they often output incorrect code. One reason is that model output probabilities are often not well-correlated with correctness, and reflect only the final output of the generation process. Inspired by findings that LLMs internally encode concepts like truthfulness, this paper explores if LLMs similarly represent code correctness. Specifically, we identify a correctness representation inside LLMs by contrasting the hidden states between pairs of correct and incorrect code for the same programming tasks. By experimenting on four LLMs, we show that exploiting this extracted correctness representation outperforms standard log-likelihood ranking, as well as verbalized model confidence. Furthermore, we explore how this internal correctness signal can be used to select higher-quality code samples, without requiring test execution. Ultimately, this work demonstrates how leveraging internal representations can enhance code generation systems and make LLMs more reliable, thus improving confidence in automatically generated code."
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-08T10:38:03Z",
                "published_parsed": [
                    2025,
                    12,
                    8,
                    10,
                    38,
                    3,
                    0,
                    342,
                    0
                ],
                "arxiv_comment": "Accepted for ICSE'26",
                "arxiv_primary_category": {
                    "term": "cs.SE"
                },
                "authors": [
                    {
                        "name": "Francisco Ribeiro"
                    },
                    {
                        "name": "Claudio Spiess"
                    },
                    {
                        "name": "Prem Devanbu"
                    },
                    {
                        "name": "Sarah Nadi"
                    }
                ],
                "author_detail": {
                    "name": "Sarah Nadi"
                },
                "author": "Sarah Nadi"
            },
            {
                "id": "http://arxiv.org/abs/2503.24191v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2503.24191v2",
                "title": "Beyond Prompts: Space-Time Decoupling Control-Plane Jailbreaks in LLM Structured Output",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Prompts: Space-Time Decoupling Control-Plane Jailbreaks in LLM Structured Output"
                },
                "updated": "2026-01-05T11:49:07Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    11,
                    49,
                    7,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2503.24191v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2503.24191v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Content Warning: This paper may contain unsafe or harmful content generated by LLMs that may be offensive to readers. Large Language Models (LLMs) are extensively used as tooling platforms through structured output APIs to ensure syntax compliance so that robust integration with existing software, like agent systems, can be achieved. However, the feature enabling the functionality of grammar-guided structured output presents significant security vulnerabilities. In this work, we reveal a critical control-plane attack surface orthogonal to traditional data-plane vulnerabilities. We introduce Constrained Decoding Attack (CDA), a novel jailbreak class that weaponizes structured output constraints to bypass both external auditing and internal safety alignment. Unlike prior attacks focused on input prompt designs, CDA operates by embedding malicious intent in schema-level grammar rules (control-plane) while maintaining benign surface prompts (data-plane). We instantiate this with two proof-of-concept attacks: EnumAttack, which embeds malicious content in enum fields; and the more evasive DictAttack, which decouples the malicious payload across a benign prompt and a dictionary-based grammar. Our evaluation spans a broad spectrum of 13 proprietary/open-weight models. In particular, DictAttack achieves 94.3--99.5% ASR across five benchmarks on gpt-5, gemini-2.5-pro, deepseek-r1, and gpt-oss-120b. Furthermore, we demonstrate the significant challenge in defending against these threats: while basic grammar auditing mitigates EnumAttack, the more sophisticated DictAttack maintains a 75.8% ASR even against multiple state-of-the-art jailbreak guardrails. This exposes a critical \"semantic gap\" in current safety architectures and underscores the urgent need for cross-plane defenses that can bridge the data and control planes to secure the LLM generation pipeline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Content Warning: This paper may contain unsafe or harmful content generated by LLMs that may be offensive to readers. Large Language Models (LLMs) are extensively used as tooling platforms through structured output APIs to ensure syntax compliance so that robust integration with existing software, like agent systems, can be achieved. However, the feature enabling the functionality of grammar-guided structured output presents significant security vulnerabilities. In this work, we reveal a critical control-plane attack surface orthogonal to traditional data-plane vulnerabilities. We introduce Constrained Decoding Attack (CDA), a novel jailbreak class that weaponizes structured output constraints to bypass both external auditing and internal safety alignment. Unlike prior attacks focused on input prompt designs, CDA operates by embedding malicious intent in schema-level grammar rules (control-plane) while maintaining benign surface prompts (data-plane). We instantiate this with two proof-of-concept attacks: EnumAttack, which embeds malicious content in enum fields; and the more evasive DictAttack, which decouples the malicious payload across a benign prompt and a dictionary-based grammar. Our evaluation spans a broad spectrum of 13 proprietary/open-weight models. In particular, DictAttack achieves 94.3--99.5% ASR across five benchmarks on gpt-5, gemini-2.5-pro, deepseek-r1, and gpt-oss-120b. Furthermore, we demonstrate the significant challenge in defending against these threats: while basic grammar auditing mitigates EnumAttack, the more sophisticated DictAttack maintains a 75.8% ASR even against multiple state-of-the-art jailbreak guardrails. This exposes a critical \"semantic gap\" in current safety architectures and underscores the urgent need for cross-plane defenses that can bridge the data and control planes to secure the LLM generation pipeline."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-03-31T15:08:06Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    15,
                    8,
                    6,
                    0,
                    90,
                    0
                ],
                "arxiv_comment": "15 pages, 9 figures, 8 tables, Preprint",
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "authors": [
                    {
                        "name": "Shuoming Zhang"
                    },
                    {
                        "name": "Jiacheng Zhao"
                    },
                    {
                        "name": "Hanyuan Dong"
                    },
                    {
                        "name": "Ruiyuan Xu"
                    },
                    {
                        "name": "Zhicheng Li"
                    },
                    {
                        "name": "Yangyu Zhang"
                    },
                    {
                        "name": "Shuaijiang Li"
                    },
                    {
                        "name": "Yuan Wen"
                    },
                    {
                        "name": "Chunwei Xia"
                    },
                    {
                        "name": "Zheng Wang"
                    },
                    {
                        "name": "Xiaobing Feng"
                    },
                    {
                        "name": "Huimin Cui"
                    }
                ],
                "author_detail": {
                    "name": "Huimin Cui"
                },
                "author": "Huimin Cui"
            },
            {
                "id": "http://arxiv.org/abs/2506.13879v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2506.13879v2",
                "title": "Limits to Extracting Neutron-Star Physics Constraints from NICER Pulse Profiles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Limits to Extracting Neutron-Star Physics Constraints from NICER Pulse Profiles"
                },
                "updated": "2026-01-05T11:48:26Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    11,
                    48,
                    26,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2506.13879v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2506.13879v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Modeling energy-dependent X-ray pulse profiles from rotation-powered millisecond pulsars observed with NICER has emerged as a promising avenue for measuring neutron star radii and probing the equation of state of cold, ultra-dense matter. However, pulse profile models have often required an unwieldy number of parameters to account for complex surface emission geometries, introducing the risk of overfitting and degeneracies. To explore the number of model parameters that can be inferred uniquely, we perform a quantitative assessment of the information content in X-ray pulse profiles by applying Fourier methods. We determine the number of independent observables that can be reliably extracted from the pulse shapes, as well as from complementary X-ray spectral data obtained with XMM-Newton, for key NICER targets. Our analysis provides a framework for evaluating the match between model complexity and data constraints. It also demonstrates the importance of incorporating in the model the pulsed components of the magnetospheric non-thermal emission, which may often contribute significantly to the observed spectra. Our results highlight limitations in previous inferences of neutron-star radii from NICER observations, which may have incorporated model complexity not supported by the data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling energy-dependent X-ray pulse profiles from rotation-powered millisecond pulsars observed with NICER has emerged as a promising avenue for measuring neutron star radii and probing the equation of state of cold, ultra-dense matter. However, pulse profile models have often required an unwieldy number of parameters to account for complex surface emission geometries, introducing the risk of overfitting and degeneracies. To explore the number of model parameters that can be inferred uniquely, we perform a quantitative assessment of the information content in X-ray pulse profiles by applying Fourier methods. We determine the number of independent observables that can be reliably extracted from the pulse shapes, as well as from complementary X-ray spectral data obtained with XMM-Newton, for key NICER targets. Our analysis provides a framework for evaluating the match between model complexity and data constraints. It also demonstrates the importance of incorporating in the model the pulsed components of the magnetospheric non-thermal emission, which may often contribute significantly to the observed spectra. Our results highlight limitations in previous inferences of neutron-star radii from NICER observations, which may have incorporated model complexity not supported by the data."
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-06-16T18:00:40Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    18,
                    0,
                    40,
                    0,
                    167,
                    0
                ],
                "arxiv_comment": "Accepted for publication in the Astrophysical Journal",
                "arxiv_primary_category": {
                    "term": "astro-ph.HE"
                },
                "authors": [
                    {
                        "name": "Tolga Guver"
                    },
                    {
                        "name": "Dimitrios Psaltis"
                    },
                    {
                        "name": "Feryal Ozel"
                    },
                    {
                        "name": "Tong Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhao"
                },
                "author": "Tong Zhao"
            },
            {
                "id": "http://arxiv.org/abs/2601.02036v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.02036v1",
                "title": "GDRO: Group-level Reward Post-training Suitable for Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GDRO: Group-level Reward Post-training Suitable for Diffusion Models"
                },
                "updated": "2026-01-05T11:47:18Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    11,
                    47,
                    18,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.02036v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.02036v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Recent advancements adopt online reinforcement learning (RL) from LLMs to text-to-image rectified flow diffusion models for reward alignment. The use of group-level rewards successfully aligns the model with the targeted reward. However, it faces challenges including low efficiency, dependency on stochastic samplers, and reward hacking. The problem is that rectified flow models are fundamentally different from LLMs: 1) For efficiency, online image sampling takes much more time and dominates the time of training. 2) For stochasticity, rectified flow is deterministic once the initial noise is fixed. Aiming at these problems and inspired by the effects of group-level rewards from LLMs, we design Group-level Direct Reward Optimization (GDRO). GDRO is a new post-training paradigm for group-level reward alignment that combines the characteristics of rectified flow models. Through rigorous theoretical analysis, we point out that GDRO supports full offline training that saves the large time cost for image rollout sampling. Also, it is diffusion-sampler-independent, which eliminates the need for the ODE-to-SDE approximation to obtain stochasticity. We also empirically study the reward hacking trap that may mislead the evaluation, and involve this factor in the evaluation using a corrected score that not only considers the original evaluation reward but also the trend of reward hacking. Extensive experiments demonstrate that GDRO effectively and efficiently improves the reward score of the diffusion model through group-wise offline optimization across the OCR and GenEval tasks, while demonstrating strong stability and robustness in mitigating reward hacking.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements adopt online reinforcement learning (RL) from LLMs to text-to-image rectified flow diffusion models for reward alignment. The use of group-level rewards successfully aligns the model with the targeted reward. However, it faces challenges including low efficiency, dependency on stochastic samplers, and reward hacking. The problem is that rectified flow models are fundamentally different from LLMs: 1) For efficiency, online image sampling takes much more time and dominates the time of training. 2) For stochasticity, rectified flow is deterministic once the initial noise is fixed. Aiming at these problems and inspired by the effects of group-level rewards from LLMs, we design Group-level Direct Reward Optimization (GDRO). GDRO is a new post-training paradigm for group-level reward alignment that combines the characteristics of rectified flow models. Through rigorous theoretical analysis, we point out that GDRO supports full offline training that saves the large time cost for image rollout sampling. Also, it is diffusion-sampler-independent, which eliminates the need for the ODE-to-SDE approximation to obtain stochasticity. We also empirically study the reward hacking trap that may mislead the evaluation, and involve this factor in the evaluation using a corrected score that not only considers the original evaluation reward but also the trend of reward hacking. Extensive experiments demonstrate that GDRO effectively and efficiently improves the reward score of the diffusion model through group-wise offline optimization across the OCR and GenEval tasks, while demonstrating strong stability and robustness in mitigating reward hacking."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-05T11:47:18Z",
                "published_parsed": [
                    2026,
                    1,
                    5,
                    11,
                    47,
                    18,
                    0,
                    5,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Yiyang Wang"
                    },
                    {
                        "name": "Xi Chen"
                    },
                    {
                        "name": "Xiaogang Xu"
                    },
                    {
                        "name": "Yu Liu"
                    },
                    {
                        "name": "Hengshuang Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Hengshuang Zhao"
                },
                "author": "Hengshuang Zhao"
            },
            {
                "id": "http://arxiv.org/abs/2508.00454v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2508.00454v3",
                "title": "Learning an Efficient Multi-Turn Dialogue Evaluator from Multiple LLM Judges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning an Efficient Multi-Turn Dialogue Evaluator from Multiple LLM Judges"
                },
                "updated": "2026-01-05T11:45:54Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    11,
                    45,
                    54,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2508.00454v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2508.00454v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Evaluating the conversational abilities of large language models (LLMs) remains a challenging task. Current mainstream approaches primarily rely on the \"LLM-as-a-judge\" paradigm, where an LLM is prompted to serve as an evaluator to assess dialogue quality. However, such methods often suffer from various biases, which undermine the reliability and consistency of the evaluation results. To mitigate these biases, recent methods employ multiple LLMs as judges and aggregate their judgments to select the optimal assessment. Although effective, this multi-judge approach incurs significant computational overhead during inference. In this paper, we propose an efficient dialogue evaluator that captures the collective wisdom of multiple LLM judges by aggregating their preference knowledge into a single model. Our approach preserves the advantages of diverse multi-judge feedback while drastically reducing the evaluation cost, enabling fast, flexible, and fine-grained dialogue quality assessment. Extensive experiments on seven single rating and pairwise comparison dialogue evaluation benchmarks demonstrate that our method outperforms existing baselines across diverse scenarios, showcasing its efficiency and robustness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating the conversational abilities of large language models (LLMs) remains a challenging task. Current mainstream approaches primarily rely on the \"LLM-as-a-judge\" paradigm, where an LLM is prompted to serve as an evaluator to assess dialogue quality. However, such methods often suffer from various biases, which undermine the reliability and consistency of the evaluation results. To mitigate these biases, recent methods employ multiple LLMs as judges and aggregate their judgments to select the optimal assessment. Although effective, this multi-judge approach incurs significant computational overhead during inference. In this paper, we propose an efficient dialogue evaluator that captures the collective wisdom of multiple LLM judges by aggregating their preference knowledge into a single model. Our approach preserves the advantages of diverse multi-judge feedback while drastically reducing the evaluation cost, enabling fast, flexible, and fine-grained dialogue quality assessment. Extensive experiments on seven single rating and pairwise comparison dialogue evaluation benchmarks demonstrate that our method outperforms existing baselines across diverse scenarios, showcasing its efficiency and robustness."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-08-01T09:26:01Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    9,
                    26,
                    1,
                    4,
                    213,
                    0
                ],
                "arxiv_comment": "20 pages, 4 pages, under review",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Yuqi Tang"
                    },
                    {
                        "name": "Kehua Feng"
                    },
                    {
                        "name": "Yunfeng Wang"
                    },
                    {
                        "name": "Zhiwen Chen"
                    },
                    {
                        "name": "Chengfei Lv"
                    },
                    {
                        "name": "Gang Yu"
                    },
                    {
                        "name": "Keyan Ding"
                    },
                    {
                        "name": "Huajun Chen"
                    }
                ],
                "author_detail": {
                    "name": "Huajun Chen"
                },
                "author": "Huajun Chen"
            },
            {
                "id": "http://arxiv.org/abs/2601.02031v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.02031v1",
                "title": "Output Embedding Centering for Stable LLM Pretraining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Output Embedding Centering for Stable LLM Pretraining"
                },
                "updated": "2026-01-05T11:44:05Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    11,
                    44,
                    5,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.02031v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.02031v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Pretraining of large language models is not only expensive but also prone to certain training instabilities. A specific instability that often occurs for large learning rates at the end of training is output logit divergence. The most widely used mitigation strategy, z-loss, merely addresses the symptoms rather than the underlying cause of the problem. In this paper, we analyze the instability from the perspective of the output embeddings' geometry and identify its cause. Based on this, we propose output embedding centering (OEC) as a new mitigation strategy, and prove that it suppresses output logit divergence. OEC can be implemented in two different ways, as a deterministic operation called μ-centering, or a regularization method called μ-loss. Our experiments show that both variants outperform z-loss in terms of training stability and learning rate sensitivity. In particular, they ensure that training converges even for large learning rates when z-loss fails. Furthermore, we find that μ-loss is significantly less sensitive to regularization hyperparameter tuning than z-loss.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pretraining of large language models is not only expensive but also prone to certain training instabilities. A specific instability that often occurs for large learning rates at the end of training is output logit divergence. The most widely used mitigation strategy, z-loss, merely addresses the symptoms rather than the underlying cause of the problem. In this paper, we analyze the instability from the perspective of the output embeddings' geometry and identify its cause. Based on this, we propose output embedding centering (OEC) as a new mitigation strategy, and prove that it suppresses output logit divergence. OEC can be implemented in two different ways, as a deterministic operation called μ-centering, or a regularization method called μ-loss. Our experiments show that both variants outperform z-loss in terms of training stability and learning rate sensitivity. In particular, they ensure that training converges even for large learning rates when z-loss fails. Furthermore, we find that μ-loss is significantly less sensitive to regularization hyperparameter tuning than z-loss."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-05T11:44:05Z",
                "published_parsed": [
                    2026,
                    1,
                    5,
                    11,
                    44,
                    5,
                    0,
                    5,
                    0
                ],
                "arxiv_comment": "11 pages, 5 figures",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Felix Stollenwerk"
                    },
                    {
                        "name": "Anna Lokrantz"
                    },
                    {
                        "name": "Niclas Hertzberg"
                    }
                ],
                "author_detail": {
                    "name": "Niclas Hertzberg"
                },
                "author": "Niclas Hertzberg"
            },
            {
                "id": "http://arxiv.org/abs/2601.02023v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.02023v1",
                "title": "Not All Needles Are Found: How Fact Distribution and Don't Make It Up Prompts Shape Literal Extraction, Logical Inference, and Hallucination Risks in Long-Context LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not All Needles Are Found: How Fact Distribution and Don't Make It Up Prompts Shape Literal Extraction, Logical Inference, and Hallucination Risks in Long-Context LLMs"
                },
                "updated": "2026-01-05T11:30:56Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    11,
                    30,
                    56,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.02023v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.02023v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models (LLMs) increasingly support very long input contexts. Yet it remains unclear how reliably they extract and infer information at scale. Performance varies with context length and strongly interacts with how information is distributed in real-world corpora. Motivated by these observations, we study how fact placement, corpus-level fact distributions, and Don't Make It Up prompts influence model behavior. We introduce an extended needle-in-a-haystack benchmark across four production-scale models: Gemini-2.5-flash, ChatGPT-5-mini, Claude-4.5-haiku, and Deepseek-v3.2-chat. Unlike prior work, we separately evaluate literal extraction, logical inference, and hallucination risk. Our study considers both positional effects and realistic distributions of evidence across long contexts, as well as prompts that explicitly discourage fabrication. We find that longer contexts alone do not guarantee better performance and can be detrimental when relevant evidence is diluted or widely dispersed. Performance varies substantially across models: some show severe degradation under realistic conditions, while others remain more robust at longer context lengths. Anti-hallucination (AH) instructions can make some models overly conservative, sharply reducing accuracy in literal extraction and logical inference. While we do not directly compare retrieval-augmented generation (RAG) and cache-augmented generation (CAG), our results suggest many failures stem from ineffective context utilization. Models often struggle to identify and prioritize relevant information even when it is present. These findings have direct practical implications, as enterprise workflows increasingly involve pasting large volumes of unfiltered documents into LLM prompts. Effective context length and model-specific robustness to long contexts are therefore critical for reliable LLM deployment in research and business.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) increasingly support very long input contexts. Yet it remains unclear how reliably they extract and infer information at scale. Performance varies with context length and strongly interacts with how information is distributed in real-world corpora. Motivated by these observations, we study how fact placement, corpus-level fact distributions, and Don't Make It Up prompts influence model behavior. We introduce an extended needle-in-a-haystack benchmark across four production-scale models: Gemini-2.5-flash, ChatGPT-5-mini, Claude-4.5-haiku, and Deepseek-v3.2-chat. Unlike prior work, we separately evaluate literal extraction, logical inference, and hallucination risk. Our study considers both positional effects and realistic distributions of evidence across long contexts, as well as prompts that explicitly discourage fabrication. We find that longer contexts alone do not guarantee better performance and can be detrimental when relevant evidence is diluted or widely dispersed. Performance varies substantially across models: some show severe degradation under realistic conditions, while others remain more robust at longer context lengths. Anti-hallucination (AH) instructions can make some models overly conservative, sharply reducing accuracy in literal extraction and logical inference. While we do not directly compare retrieval-augmented generation (RAG) and cache-augmented generation (CAG), our results suggest many failures stem from ineffective context utilization. Models often struggle to identify and prioritize relevant information even when it is present. These findings have direct practical implications, as enterprise workflows increasingly involve pasting large volumes of unfiltered documents into LLM prompts. Effective context length and model-specific robustness to long contexts are therefore critical for reliable LLM deployment in research and business."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-05T11:30:56Z",
                "published_parsed": [
                    2026,
                    1,
                    5,
                    11,
                    30,
                    56,
                    0,
                    5,
                    0
                ],
                "arxiv_comment": "25 pages, 8 figures, 3 tables",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Amirali Ebrahimzadeh"
                    },
                    {
                        "name": "Seyyed M. Salili"
                    }
                ],
                "author_detail": {
                    "name": "Seyyed M. Salili"
                },
                "author": "Seyyed M. Salili"
            },
            {
                "id": "http://arxiv.org/abs/2601.02021v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.02021v1",
                "title": "AgentVNE: LLM-Augmented Graph Reinforcement Learning for Affinity-Aware Multi-Agent Placement in Edge Agentic AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AgentVNE: LLM-Augmented Graph Reinforcement Learning for Affinity-Aware Multi-Agent Placement in Edge Agentic AI"
                },
                "updated": "2026-01-05T11:30:04Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    11,
                    30,
                    4,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.02021v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.02021v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The Internet of Agents is propelling edge computing toward agentic AI and edge general intelligence (EGI). However, deploying multi-agent service (MAS) on resource-constrained edge infrastructure presents severe challenges. MAS service workflows are driven by complex cross-node interactions, dynamic memory accumulation, and collaborative tool usage. Exhibiting chain-like topological dependencies and strict affinity constraints, these workflows demand real-time responsiveness that exceeds the capabilities of traditional VNE algorithms designed for static resources. To address this, we propose AgentVNE, a cloud-edge collaborative framework utilizing a dual-layer architecture. First, AgentVNE employs a large language model (LLM) to identify implicit semantic constraints and generate affinity-based resource augmentation to resolve physical dependency issues. Second, it constructs a resource similarity-aware neural network, utilizing a pre-training and PPO fine-tuning strategy to precisely capture topological similarities between dynamic workflows and heterogeneous networks. By coupling semantic perception with topological reasoning, this mechanism effectively bridges the gap between dynamic service requirements and physical infrastructure. Simulation results demonstrate that AgentVNE reduces workflow communication latency to less than 40% of baselines and improves the service acceptance rate by approximately 5%-10% under high-load scenarios. Ultimately, this work provides a foundational solution for the semantic-aware deployment of agentic AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Internet of Agents is propelling edge computing toward agentic AI and edge general intelligence (EGI). However, deploying multi-agent service (MAS) on resource-constrained edge infrastructure presents severe challenges. MAS service workflows are driven by complex cross-node interactions, dynamic memory accumulation, and collaborative tool usage. Exhibiting chain-like topological dependencies and strict affinity constraints, these workflows demand real-time responsiveness that exceeds the capabilities of traditional VNE algorithms designed for static resources. To address this, we propose AgentVNE, a cloud-edge collaborative framework utilizing a dual-layer architecture. First, AgentVNE employs a large language model (LLM) to identify implicit semantic constraints and generate affinity-based resource augmentation to resolve physical dependency issues. Second, it constructs a resource similarity-aware neural network, utilizing a pre-training and PPO fine-tuning strategy to precisely capture topological similarities between dynamic workflows and heterogeneous networks. By coupling semantic perception with topological reasoning, this mechanism effectively bridges the gap between dynamic service requirements and physical infrastructure. Simulation results demonstrate that AgentVNE reduces workflow communication latency to less than 40% of baselines and improves the service acceptance rate by approximately 5%-10% under high-load scenarios. Ultimately, this work provides a foundational solution for the semantic-aware deployment of agentic AI."
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-05T11:30:04Z",
                "published_parsed": [
                    2026,
                    1,
                    5,
                    11,
                    30,
                    4,
                    0,
                    5,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI"
                },
                "authors": [
                    {
                        "name": "Runze Zheng"
                    },
                    {
                        "name": "Yuqing Zheng"
                    },
                    {
                        "name": "Zhengyi Cheng"
                    },
                    {
                        "name": "Long Luo"
                    },
                    {
                        "name": "Haoxiang Luo"
                    },
                    {
                        "name": "Gang Sun"
                    },
                    {
                        "name": "Hongfang Yu"
                    },
                    {
                        "name": "Dusit Niyato"
                    }
                ],
                "author_detail": {
                    "name": "Dusit Niyato"
                },
                "author": "Dusit Niyato"
            },
            {
                "id": "http://arxiv.org/abs/2510.20075v5",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.20075v5",
                "title": "I Large Language Models possono nascondere un testo in un altro testo della stessa lunghezza",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "I Large Language Models possono nascondere un testo in un altro testo della stessa lunghezza"
                },
                "updated": "2026-01-05T11:25:29Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    11,
                    25,
                    29,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.20075v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.20075v5",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "A meaningful text can be hidden inside another, completely different yet still coherent and plausible, text of the same length. For example, a tweet containing a harsh political critique could be embedded in a tweet that celebrates the same political leader, or an ordinary product review could conceal a secret manuscript. This uncanny state of affairs is now possible thanks to Large Language Models, and in this paper we present Calgacus, a simple and efficient protocol to achieve it. We show that even modest 8-billion-parameter open-source LLMs are sufficient to obtain high-quality results, and a message as long as this abstract can be encoded and decoded locally on a laptop in seconds. The existence of such a protocol demonstrates a radical decoupling of text from authorial intent, further eroding trust in written communication, already shaken by the rise of LLM chatbots. We illustrate this with a concrete scenario: a company could covertly deploy an unfiltered LLM by encoding its answers within the compliant responses of a safe model. This possibility raises urgent questions for AI safety and challenges our understanding of what it means for a Large Language Model to know something.\n  --\nUn testo di senso compiuto può essere nascosto all'interno di un altro testo completamente diverso, eppure coerente e plausibile, della stessa lunghezza. Ad esempio, un tweet che celebra un leader politico potrebbe celare un tweet che lo critica duramente, o un'anonima recensione di un prodotto potrebbe in realtà codificare un manoscritto segreto. Questa sconcertante possibilità è oggi alla nostra portata grazie ai Large Language Models (LLM); in questo articolo presentiamo Calgacus, un protocollo semplice ed efficiente per realizzarla. Mostriamo che anche modesti LLM open-source da 8 miliardi di parametri sono sufficienti per ottenere risultati di alta qualità, e che un messaggio lungo quanto questo abstract può essere codificato e decodificato su un comune portatile in pochi secondi. L'esistenza di tale protocollo dimostra un radicale disaccoppiamento del testo dall'intento del suo autore, erodendo ulteriormente la fiducia nella comunicazione scritta, già scossa dall'ascesa dei chatbot basati su LLMs. Illustriamo ciò con uno scenario concreto: un'azienda potrebbe offrire pubblicamente i servizi di un LLM senza filtri nascondendo le sue risposte all'interno di risposte apparentemente innocue generate da un LLM considerato sicuro. Questa possibilità solleva questioni urgenti per la sicurezza dell'Intelligenza Artificiale e sfida la nostra comprensione di cosa significhi, per un Large Language Model, sapere qualcosa.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A meaningful text can be hidden inside another, completely different yet still coherent and plausible, text of the same length. For example, a tweet containing a harsh political critique could be embedded in a tweet that celebrates the same political leader, or an ordinary product review could conceal a secret manuscript. This uncanny state of affairs is now possible thanks to Large Language Models, and in this paper we present Calgacus, a simple and efficient protocol to achieve it. We show that even modest 8-billion-parameter open-source LLMs are sufficient to obtain high-quality results, and a message as long as this abstract can be encoded and decoded locally on a laptop in seconds. The existence of such a protocol demonstrates a radical decoupling of text from authorial intent, further eroding trust in written communication, already shaken by the rise of LLM chatbots. We illustrate this with a concrete scenario: a company could covertly deploy an unfiltered LLM by encoding its answers within the compliant responses of a safe model. This possibility raises urgent questions for AI safety and challenges our understanding of what it means for a Large Language Model to know something.\n  --\nUn testo di senso compiuto può essere nascosto all'interno di un altro testo completamente diverso, eppure coerente e plausibile, della stessa lunghezza. Ad esempio, un tweet che celebra un leader politico potrebbe celare un tweet che lo critica duramente, o un'anonima recensione di un prodotto potrebbe in realtà codificare un manoscritto segreto. Questa sconcertante possibilità è oggi alla nostra portata grazie ai Large Language Models (LLM); in questo articolo presentiamo Calgacus, un protocollo semplice ed efficiente per realizzarla. Mostriamo che anche modesti LLM open-source da 8 miliardi di parametri sono sufficienti per ottenere risultati di alta qualità, e che un messaggio lungo quanto questo abstract può essere codificato e decodificato su un comune portatile in pochi secondi. L'esistenza di tale protocollo dimostra un radicale disaccoppiamento del testo dall'intento del suo autore, erodendo ulteriormente la fiducia nella comunicazione scritta, già scossa dall'ascesa dei chatbot basati su LLMs. Illustriamo ciò con uno scenario concreto: un'azienda potrebbe offrire pubblicamente i servizi di un LLM senza filtri nascondendo le sue risposte all'interno di risposte apparentemente innocue generate da un LLM considerato sicuro. Questa possibilità solleva questioni urgenti per la sicurezza dell'Intelligenza Artificiale e sfida la nostra comprensione di cosa significhi, per un Large Language Model, sapere qualcosa."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-22T23:16:50Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    23,
                    16,
                    50,
                    2,
                    295,
                    0
                ],
                "arxiv_comment": "21 pages, in Italian language, main paper 9 pages. v1-v4 are in English",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Antonio Norelli"
                    },
                    {
                        "name": "Michael Bronstein"
                    }
                ],
                "author_detail": {
                    "name": "Michael Bronstein"
                },
                "author": "Michael Bronstein"
            },
            {
                "id": "http://arxiv.org/abs/2601.02016v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.02016v1",
                "title": "Enhancing Object Detection with Privileged Information: A Model-Agnostic Teacher-Student Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Object Detection with Privileged Information: A Model-Agnostic Teacher-Student Approach"
                },
                "updated": "2026-01-05T11:24:34Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    11,
                    24,
                    34,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.02016v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.02016v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "This paper investigates the integration of the Learning Using Privileged Information (LUPI) paradigm in object detection to exploit fine-grained, descriptive information available during training but not at inference. We introduce a general, model-agnostic methodology for injecting privileged information-such as bounding box masks, saliency maps, and depth cues-into deep learning-based object detectors through a teacher-student architecture. Experiments are conducted across five state-of-the-art object detection models and multiple public benchmarks, including UAV-based litter detection datasets and Pascal VOC 2012, to assess the impact on accuracy, generalization, and computational efficiency. Our results demonstrate that LUPI-trained students consistently outperform their baseline counterparts, achieving significant boosts in detection accuracy with no increase in inference complexity or model size. Performance improvements are especially marked for medium and large objects, while ablation studies reveal that intermediate weighting of teacher guidance optimally balances learning from privileged and standard inputs. The findings affirm that the LUPI framework provides an effective and practical strategy for advancing object detection systems in both resource-constrained and real-world settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates the integration of the Learning Using Privileged Information (LUPI) paradigm in object detection to exploit fine-grained, descriptive information available during training but not at inference. We introduce a general, model-agnostic methodology for injecting privileged information-such as bounding box masks, saliency maps, and depth cues-into deep learning-based object detectors through a teacher-student architecture. Experiments are conducted across five state-of-the-art object detection models and multiple public benchmarks, including UAV-based litter detection datasets and Pascal VOC 2012, to assess the impact on accuracy, generalization, and computational efficiency. Our results demonstrate that LUPI-trained students consistently outperform their baseline counterparts, achieving significant boosts in detection accuracy with no increase in inference complexity or model size. Performance improvements are especially marked for medium and large objects, while ablation studies reveal that intermediate weighting of teacher guidance optimally balances learning from privileged and standard inputs. The findings affirm that the LUPI framework provides an effective and practical strategy for advancing object detection systems in both resource-constrained and real-world settings."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-05T11:24:34Z",
                "published_parsed": [
                    2026,
                    1,
                    5,
                    11,
                    24,
                    34,
                    0,
                    5,
                    0
                ],
                "arxiv_comment": "Code available on GitHub: https://github.com/mbar0075/lupi-for-object-detection",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Matthias Bartolo"
                    },
                    {
                        "name": "Dylan Seychell"
                    },
                    {
                        "name": "Gabriel Hili"
                    },
                    {
                        "name": "Matthew Montebello"
                    },
                    {
                        "name": "Carl James Debono"
                    },
                    {
                        "name": "Saviour Formosa"
                    },
                    {
                        "name": "Konstantinos Makantasis"
                    }
                ],
                "author_detail": {
                    "name": "Konstantinos Makantasis"
                },
                "author": "Konstantinos Makantasis"
            },
            {
                "id": "http://arxiv.org/abs/2508.10710v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2508.10710v2",
                "title": "CountCluster: Training-Free Object Quantity Guidance with Cross-Attention Map Clustering for Text-to-Image Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CountCluster: Training-Free Object Quantity Guidance with Cross-Attention Map Clustering for Text-to-Image Generation"
                },
                "updated": "2026-01-05T11:17:43Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    11,
                    17,
                    43,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2508.10710v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2508.10710v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Diffusion-based text-to-image generation models have demonstrated strong performance in terms of image quality and diversity. However, they still struggle to generate images that accurately reflect the number of objects specified in the input prompt. Several approaches have been proposed that rely on either external counting modules for iterative refinement or quantity representations derived from learned tokens or latent features. However, they still have limitations in accurately reflecting the specified number of objects and overlook an important structural characteristic--The number of object instances in the generated image is largely determined in the early timesteps of the denoising process. To correctly reflect the object quantity for image generation, the highly activated regions in the object cross-attention map at the early timesteps should match the input object quantity, while each region should be clearly separated. To address this issue, we propose \\textit{CountCluster}, a method that guides the object cross-attention map to be clustered according to the specified object count in the input, without relying on any external tools or additional training. The proposed method partitions the object cross-attention map into $k$ clusters at inference time based on attention scores, defines an ideal distribution in which each cluster is spatially well-separated, and optimizes the latent to align with this target distribution. Our method achieves an average improvement of 18.5\\%p in object count accuracy compared to existing methods, and demonstrates superior quantity control performance across a variety of prompts. Code will be released at: https://github.com/JoohyeonL22/CountCluster",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based text-to-image generation models have demonstrated strong performance in terms of image quality and diversity. However, they still struggle to generate images that accurately reflect the number of objects specified in the input prompt. Several approaches have been proposed that rely on either external counting modules for iterative refinement or quantity representations derived from learned tokens or latent features. However, they still have limitations in accurately reflecting the specified number of objects and overlook an important structural characteristic--The number of object instances in the generated image is largely determined in the early timesteps of the denoising process. To correctly reflect the object quantity for image generation, the highly activated regions in the object cross-attention map at the early timesteps should match the input object quantity, while each region should be clearly separated. To address this issue, we propose \\textit{CountCluster}, a method that guides the object cross-attention map to be clustered according to the specified object count in the input, without relying on any external tools or additional training. The proposed method partitions the object cross-attention map into $k$ clusters at inference time based on attention scores, defines an ideal distribution in which each cluster is spatially well-separated, and optimizes the latent to align with this target distribution. Our method achieves an average improvement of 18.5\\%p in object count accuracy compared to existing methods, and demonstrates superior quantity control performance across a variety of prompts. Code will be released at: https://github.com/JoohyeonL22/CountCluster"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-08-14T14:53:53Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    14,
                    53,
                    53,
                    3,
                    226,
                    0
                ],
                "arxiv_comment": "Under review",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Joohyeon Lee"
                    },
                    {
                        "name": "Jin-Seop Lee"
                    },
                    {
                        "name": "Jee-Hyong Lee"
                    }
                ],
                "author_detail": {
                    "name": "Jee-Hyong Lee"
                },
                "author": "Jee-Hyong Lee"
            },
            {
                "id": "http://arxiv.org/abs/2601.02002v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.02002v1",
                "title": "Exploring Approaches for Detecting Memorization of Recommender System Data in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Approaches for Detecting Memorization of Recommender System Data in Large Language Models"
                },
                "updated": "2026-01-05T11:03:56Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    11,
                    3,
                    56,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.02002v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.02002v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) are increasingly applied in recommendation scenarios due to their strong natural language understanding and generation capabilities. However, they are trained on vast corpora whose contents are not publicly disclosed, raising concerns about data leakage. Recent work has shown that the MovieLens-1M dataset is memorized by both the LLaMA and OpenAI model families, but the extraction of such memorized data has so far relied exclusively on manual prompt engineering. In this paper, we pose three main questions: Is it possible to enhance manual prompting? Can LLM memorization be detected through methods beyond manual prompting? And can the detection of data leakage be automated? To address these questions, we evaluate three approaches: (i) jailbreak prompt engineering; (ii) unsupervised latent knowledge discovery, probing internal activations via Contrast-Consistent Search (CCS) and Cluster-Norm; and (iii) Automatic Prompt Engineering (APE), which frames prompt discovery as a meta-learning process that iteratively refines candidate instructions. Experiments on MovieLens-1M using LLaMA models show that jailbreak prompting does not improve the retrieval of memorized items and remains inconsistent; CCS reliably distinguishes genuine from fabricated movie titles but fails on numerical user and rating data; and APE retrieves item-level information with moderate success yet struggles to recover numerical interactions. These findings suggest that automatically optimizing prompts is the most promising strategy for extracting memorized samples.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly applied in recommendation scenarios due to their strong natural language understanding and generation capabilities. However, they are trained on vast corpora whose contents are not publicly disclosed, raising concerns about data leakage. Recent work has shown that the MovieLens-1M dataset is memorized by both the LLaMA and OpenAI model families, but the extraction of such memorized data has so far relied exclusively on manual prompt engineering. In this paper, we pose three main questions: Is it possible to enhance manual prompting? Can LLM memorization be detected through methods beyond manual prompting? And can the detection of data leakage be automated? To address these questions, we evaluate three approaches: (i) jailbreak prompt engineering; (ii) unsupervised latent knowledge discovery, probing internal activations via Contrast-Consistent Search (CCS) and Cluster-Norm; and (iii) Automatic Prompt Engineering (APE), which frames prompt discovery as a meta-learning process that iteratively refines candidate instructions. Experiments on MovieLens-1M using LLaMA models show that jailbreak prompting does not improve the retrieval of memorized items and remains inconsistent; CCS reliably distinguishes genuine from fabricated movie titles but fails on numerical user and rating data; and APE retrieves item-level information with moderate success yet struggles to recover numerical interactions. These findings suggest that automatically optimizing prompts is the most promising strategy for extracting memorized samples."
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-05T11:03:56Z",
                "published_parsed": [
                    2026,
                    1,
                    5,
                    11,
                    3,
                    56,
                    0,
                    5,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR"
                },
                "authors": [
                    {
                        "name": "Antonio Colacicco"
                    },
                    {
                        "name": "Vito Guida"
                    },
                    {
                        "name": "Dario Di Palma"
                    },
                    {
                        "name": "Fedelucio Narducci"
                    },
                    {
                        "name": "Tommaso Di Noia"
                    }
                ],
                "author_detail": {
                    "name": "Tommaso Di Noia"
                },
                "author": "Tommaso Di Noia"
            },
            {
                "id": "http://arxiv.org/abs/2411.12589v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2411.12589v3",
                "title": "ULTra: Unveiling Latent Token Interpretability in Transformer-Based Understanding and Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ULTra: Unveiling Latent Token Interpretability in Transformer-Based Understanding and Segmentation"
                },
                "updated": "2026-01-05T11:02:56Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    11,
                    2,
                    56,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2411.12589v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2411.12589v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Transformers have revolutionized Computer Vision (CV) through self-attention mechanisms. However, their complexity makes latent token representations difficult to interpret. We introduce ULTra, a framework for interpreting Transformer embeddings and uncovering meaningful semantic patterns within them. ULTra enables unsupervised semantic segmentation using pre-trained models without requiring fine-tuning. Additionally, we propose a self-supervised training approach that refines segmentation performance by learning an external transformation matrix without modifying the underlying model. Our method achieves state-of-the-art performance in unsupervised semantic segmentation, outperforming existing segmentation methods. Furthermore, we validate ULTra for model interpretation on both synthetic and real-world scenarios, including Object Selection and interpretable text summarization using LLMs, demonstrating its broad applicability in explaining the semantic structure of latent token representations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers have revolutionized Computer Vision (CV) through self-attention mechanisms. However, their complexity makes latent token representations difficult to interpret. We introduce ULTra, a framework for interpreting Transformer embeddings and uncovering meaningful semantic patterns within them. ULTra enables unsupervised semantic segmentation using pre-trained models without requiring fine-tuning. Additionally, we propose a self-supervised training approach that refines segmentation performance by learning an external transformation matrix without modifying the underlying model. Our method achieves state-of-the-art performance in unsupervised semantic segmentation, outperforming existing segmentation methods. Furthermore, we validate ULTra for model interpretation on both synthetic and real-world scenarios, including Object Selection and interpretable text summarization using LLMs, demonstrating its broad applicability in explaining the semantic structure of latent token representations."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2024-11-15T19:36:50Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    19,
                    36,
                    50,
                    4,
                    320,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "arxiv_journal_ref": "Transactions on Machine Learning Research (TMLR), 2026",
                "authors": [
                    {
                        "name": "Hesam Hosseini"
                    },
                    {
                        "name": "Ghazal Hosseini Mighan"
                    },
                    {
                        "name": "Amirabbas Afzali"
                    },
                    {
                        "name": "Sajjad Amini"
                    },
                    {
                        "name": "Amir Houmansadr"
                    }
                ],
                "author_detail": {
                    "name": "Amir Houmansadr"
                },
                "author": "Amir Houmansadr"
            },
            {
                "id": "http://arxiv.org/abs/2601.01993v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.01993v1",
                "title": "MindChat: A Privacy-preserving Large Language Model for Mental Health Support",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MindChat: A Privacy-preserving Large Language Model for Mental Health Support"
                },
                "updated": "2026-01-05T10:54:18Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    10,
                    54,
                    18,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.01993v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.01993v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models (LLMs) have shown promise for mental health support, yet training such models is constrained by the scarcity and sensitivity of real counseling dialogues. In this article, we present MindChat, a privacy-preserving LLM for mental health support, together with MindCorpus, a synthetic multi-turn counseling dataset constructed via a multi-agent role-playing framework. To synthesize high-quality counseling data, the developed dialogue-construction framework employs a dual closed-loop feedback design to integrate psychological expertise and counseling techniques through role-playing: (i) turn-level critique-and-revision to improve coherence and counseling appropriateness within a session, and (ii) session-level strategy refinement to progressively enrich counselor behaviors across sessions. To mitigate privacy risks under decentralized data ownership, we fine-tune the base model using federated learning with parameter-efficient LoRA adapters and incorporate differentially private optimization to reduce membership and memorization risks. Experiments on synthetic-data quality assessment and counseling capability evaluation show that MindCorpus improves training effectiveness and that MindChat is competitive with existing general and counseling-oriented LLM baselines under both automatic LLM-judge and human evaluation protocols, while exhibiting reduced privacy leakage under membership inference attacks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown promise for mental health support, yet training such models is constrained by the scarcity and sensitivity of real counseling dialogues. In this article, we present MindChat, a privacy-preserving LLM for mental health support, together with MindCorpus, a synthetic multi-turn counseling dataset constructed via a multi-agent role-playing framework. To synthesize high-quality counseling data, the developed dialogue-construction framework employs a dual closed-loop feedback design to integrate psychological expertise and counseling techniques through role-playing: (i) turn-level critique-and-revision to improve coherence and counseling appropriateness within a session, and (ii) session-level strategy refinement to progressively enrich counselor behaviors across sessions. To mitigate privacy risks under decentralized data ownership, we fine-tune the base model using federated learning with parameter-efficient LoRA adapters and incorporate differentially private optimization to reduce membership and memorization risks. Experiments on synthetic-data quality assessment and counseling capability evaluation show that MindCorpus improves training effectiveness and that MindChat is competitive with existing general and counseling-oriented LLM baselines under both automatic LLM-judge and human evaluation protocols, while exhibiting reduced privacy leakage under membership inference attacks."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-05T10:54:18Z",
                "published_parsed": [
                    2026,
                    1,
                    5,
                    10,
                    54,
                    18,
                    0,
                    5,
                    0
                ],
                "arxiv_comment": "33 pages, 16 figures",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Dong Xue"
                    },
                    {
                        "name": "Jicheng Tu"
                    },
                    {
                        "name": "Ming Wang"
                    },
                    {
                        "name": "Xin Yan"
                    },
                    {
                        "name": "Fangzhou Liu"
                    },
                    {
                        "name": "Jie Hu"
                    }
                ],
                "author_detail": {
                    "name": "Jie Hu"
                },
                "author": "Jie Hu"
            },
            {
                "id": "http://arxiv.org/abs/2512.14244v4",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.14244v4",
                "title": "From Context to EDUs: Faithful and Structured Context Compression via Elementary Discourse Unit Decomposition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Context to EDUs: Faithful and Structured Context Compression via Elementary Discourse Unit Decomposition"
                },
                "updated": "2026-01-05T10:52:55Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    10,
                    52,
                    55,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.14244v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.14244v4",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Managing extensive context remains a critical bottleneck for Large Language Models (LLMs), particularly in applications like long-document question answering and autonomous agents where lengthy inputs incur high computational costs and introduce noise. Existing compression techniques often disrupt local coherence through discrete token removal or rely on implicit latent encoding that suffers from positional bias and incompatibility with closed-source APIs. To address these limitations, we introduce the EDU-based Context Compressor, a novel explicit compression framework designed to preserve both global structure and fine-grained details. Our approach reformulates context compression as a structure-then-select process. First, our LingoEDU transforms linear text into a structural relation tree of Elementary Discourse Units (EDUs) which are anchored strictly to source indices to eliminate hallucination. Second, a lightweight ranking module selects query-relevant sub-trees for linearization. To rigorously evaluate structural understanding, we release StructBench, a manually annotated dataset of 248 diverse documents. Empirical results demonstrate that our method achieves state-of-the-art structural prediction accuracy and significantly outperforms frontier LLMs while reducing costs. Furthermore, our structure-aware compression substantially enhances performance across downstream tasks ranging from long-context tasks to complex Deep Search scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Managing extensive context remains a critical bottleneck for Large Language Models (LLMs), particularly in applications like long-document question answering and autonomous agents where lengthy inputs incur high computational costs and introduce noise. Existing compression techniques often disrupt local coherence through discrete token removal or rely on implicit latent encoding that suffers from positional bias and incompatibility with closed-source APIs. To address these limitations, we introduce the EDU-based Context Compressor, a novel explicit compression framework designed to preserve both global structure and fine-grained details. Our approach reformulates context compression as a structure-then-select process. First, our LingoEDU transforms linear text into a structural relation tree of Elementary Discourse Units (EDUs) which are anchored strictly to source indices to eliminate hallucination. Second, a lightweight ranking module selects query-relevant sub-trees for linearization. To rigorously evaluate structural understanding, we release StructBench, a manually annotated dataset of 248 diverse documents. Empirical results demonstrate that our method achieves state-of-the-art structural prediction accuracy and significantly outperforms frontier LLMs while reducing costs. Furthermore, our structure-aware compression substantially enhances performance across downstream tasks ranging from long-context tasks to complex Deep Search scenarios."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-16T09:52:58Z",
                "published_parsed": [
                    2025,
                    12,
                    16,
                    9,
                    52,
                    58,
                    1,
                    350,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Yiqing Zhou"
                    },
                    {
                        "name": "Yu Lei"
                    },
                    {
                        "name": "Shuzheng Si"
                    },
                    {
                        "name": "Qingyan Sun"
                    },
                    {
                        "name": "Wei Wang"
                    },
                    {
                        "name": "Yifei Wu"
                    },
                    {
                        "name": "Hao Wen"
                    },
                    {
                        "name": "Gang Chen"
                    },
                    {
                        "name": "Fanchao Qi"
                    },
                    {
                        "name": "Maosong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Maosong Sun"
                },
                "author": "Maosong Sun"
            },
            {
                "id": "http://arxiv.org/abs/2601.01982v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.01982v1",
                "title": "ChaosBench-Logic: A Benchmark for Logical and Symbolic Reasoning on Chaotic Dynamical Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChaosBench-Logic: A Benchmark for Logical and Symbolic Reasoning on Chaotic Dynamical Systems"
                },
                "updated": "2026-01-05T10:36:40Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    10,
                    36,
                    40,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.01982v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.01982v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models (LLMs) excel at natural language tasks but remain brittle in domains requiring precise logical and symbolic reasoning. Chaotic dynamical systems provide an especially demanding test because chaos is deterministic yet often misinterpreted as randomness or complexity. We introduce ChaosBench-Logic, a benchmark that evaluates LLM reasoning across 30 diverse dynamical systems using a unified first-order logic (FOL) ontology. Each system is annotated with truth assignments for 11 semantic predicates, and 621 questions are generated across seven reasoning categories, including multi-hop implications, cross-system analogies, counterfactual reasoning, bias probes, and multi-turn dialogues. We define metrics for logical accuracy, implication consistency, dialogue coherence, and contradiction, and we release an open-source evaluation pipeline. Initial experiments show that frontier LLMs such as GPT-4, Claude 3.5 Sonnet, Gemini 2.5 Flash, and the open-source LLaMA-3 70B achieve 91-94% per-item accuracy, yet still score 0% on compositional items and exhibit fragile global coherence. Dialogue-level accuracy ranges from 53.1% (GPT-4 CoT) to 75.5% (LLaMA-3 zero-shot). ChaosBench-Logic provides a rigorous testbed for diagnosing such failures and a foundation for developing neuro-symbolic approaches that improve scientific reasoning in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) excel at natural language tasks but remain brittle in domains requiring precise logical and symbolic reasoning. Chaotic dynamical systems provide an especially demanding test because chaos is deterministic yet often misinterpreted as randomness or complexity. We introduce ChaosBench-Logic, a benchmark that evaluates LLM reasoning across 30 diverse dynamical systems using a unified first-order logic (FOL) ontology. Each system is annotated with truth assignments for 11 semantic predicates, and 621 questions are generated across seven reasoning categories, including multi-hop implications, cross-system analogies, counterfactual reasoning, bias probes, and multi-turn dialogues. We define metrics for logical accuracy, implication consistency, dialogue coherence, and contradiction, and we release an open-source evaluation pipeline. Initial experiments show that frontier LLMs such as GPT-4, Claude 3.5 Sonnet, Gemini 2.5 Flash, and the open-source LLaMA-3 70B achieve 91-94% per-item accuracy, yet still score 0% on compositional items and exhibit fragile global coherence. Dialogue-level accuracy ranges from 53.1% (GPT-4 CoT) to 75.5% (LLaMA-3 zero-shot). ChaosBench-Logic provides a rigorous testbed for diagnosing such failures and a foundation for developing neuro-symbolic approaches that improve scientific reasoning in LLMs."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-05T10:36:40Z",
                "published_parsed": [
                    2026,
                    1,
                    5,
                    10,
                    36,
                    40,
                    0,
                    5,
                    0
                ],
                "arxiv_comment": "7 pages, 0 figures , Accepted to AAAI-26 Bridge Program: Logical and Symbolic Reasoning in Language Models (camera-ready)",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Noel Thomas"
                    }
                ],
                "author_detail": {
                    "name": "Noel Thomas"
                },
                "author": "Noel Thomas"
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2601.02360v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.02360v1",
                "title": "Heterogeneous Low-Bandwidth Pre-Training of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Heterogeneous Low-Bandwidth Pre-Training of LLMs"
                },
                "updated": "2026-01-05T18:59:57Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    18,
                    59,
                    57,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.02360v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.02360v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Pre-training large language models (LLMs) increasingly requires distributed compute, yet bandwidth constraints make it difficult to scale beyond well-provisioned datacenters-especially when model parallelism forces frequent, large inter-device communications. We study whether SparseLoCo, a low-communication data parallel method based on infrequent synchronization and sparse pseudo-gradient exchange, can be combined with low-bandwidth pipeline model parallelism via activation and activation-gradient compression. We introduce a heterogeneous distributed training framework where some participants host full replicas on high-bandwidth interconnects, while resource-limited participants are grouped to jointly instantiate a replica using pipeline parallelism with subspace-projected inter-stage communication. To make the recently introduced subspace pipeline compression compatible with SparseLoCo, we study a number of adaptations. Across large-scale language modeling experiments (178M-1B parameters) on standard pretraining corpora, we find that activation compression composes with SparseLoCo at modest cost, while selective (heterogeneous) compression consistently improves the loss-communication tradeoff relative to compressing all replicas-especially at aggressive compression ratios. These results suggest a practical path to incorporating low-bandwidth model parallelism and heterogeneous participants into LLM pre-training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pre-training large language models (LLMs) increasingly requires distributed compute, yet bandwidth constraints make it difficult to scale beyond well-provisioned datacenters-especially when model parallelism forces frequent, large inter-device communications. We study whether SparseLoCo, a low-communication data parallel method based on infrequent synchronization and sparse pseudo-gradient exchange, can be combined with low-bandwidth pipeline model parallelism via activation and activation-gradient compression. We introduce a heterogeneous distributed training framework where some participants host full replicas on high-bandwidth interconnects, while resource-limited participants are grouped to jointly instantiate a replica using pipeline parallelism with subspace-projected inter-stage communication. To make the recently introduced subspace pipeline compression compatible with SparseLoCo, we study a number of adaptations. Across large-scale language modeling experiments (178M-1B parameters) on standard pretraining corpora, we find that activation compression composes with SparseLoCo at modest cost, while selective (heterogeneous) compression consistently improves the loss-communication tradeoff relative to compressing all replicas-especially at aggressive compression ratios. These results suggest a practical path to incorporating low-bandwidth model parallelism and heterogeneous participants into LLM pre-training."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-05T18:59:57Z",
                "published_parsed": [
                    2026,
                    1,
                    5,
                    18,
                    59,
                    57,
                    0,
                    5,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Yazan Obeidi"
                    },
                    {
                        "name": "Amir Sarfi"
                    },
                    {
                        "name": "Joel Lidin"
                    },
                    {
                        "name": "Paul Janson"
                    },
                    {
                        "name": "Eugene Belilovsky"
                    }
                ],
                "author_detail": {
                    "name": "Eugene Belilovsky"
                },
                "author": "Eugene Belilovsky"
            },
            {
                "id": "http://arxiv.org/abs/2601.02346v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.02346v1",
                "title": "Falcon-H1R: Pushing the Reasoning Frontiers with a Hybrid Model for Efficient Test-Time Scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Falcon-H1R: Pushing the Reasoning Frontiers with a Hybrid Model for Efficient Test-Time Scaling"
                },
                "updated": "2026-01-05T18:44:27Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    18,
                    44,
                    27,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.02346v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.02346v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "This work introduces Falcon-H1R, a 7B-parameter reasoning-optimized model that establishes the feasibility of achieving competitive reasoning performance with small language models (SLMs). Falcon-H1R stands out for its parameter efficiency, consistently matching or outperforming SOTA reasoning models that are $2\\times$ to $7\\times$ larger across a variety of reasoning-intensive benchmarks. These results underscore the importance of careful data curation and targeted training strategies (via both efficient SFT and RL scaling) in delivering significant performance gains without increasing model size. Furthermore, Falcon-H1R advances the 3D limits of reasoning efficiency by combining faster inference (through its hybrid-parallel architecture design), token efficiency, and higher accuracy. This unique blend makes Falcon-H1R-7B a practical backbone for scaling advanced reasoning systems, particularly in scenarios requiring extensive chain-of-thoughts generation and parallel test-time scaling. Leveraging the recently introduced DeepConf approach, Falcon-H1R achieves state-of-the-art test-time scaling efficiency, offering substantial improvements in both accuracy and computational cost. As a result, Falcon-H1R demonstrates that compact models, through targeted model training and architectural choices, can deliver robust and scalable reasoning performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work introduces Falcon-H1R, a 7B-parameter reasoning-optimized model that establishes the feasibility of achieving competitive reasoning performance with small language models (SLMs). Falcon-H1R stands out for its parameter efficiency, consistently matching or outperforming SOTA reasoning models that are $2\\times$ to $7\\times$ larger across a variety of reasoning-intensive benchmarks. These results underscore the importance of careful data curation and targeted training strategies (via both efficient SFT and RL scaling) in delivering significant performance gains without increasing model size. Furthermore, Falcon-H1R advances the 3D limits of reasoning efficiency by combining faster inference (through its hybrid-parallel architecture design), token efficiency, and higher accuracy. This unique blend makes Falcon-H1R-7B a practical backbone for scaling advanced reasoning systems, particularly in scenarios requiring extensive chain-of-thoughts generation and parallel test-time scaling. Leveraging the recently introduced DeepConf approach, Falcon-H1R achieves state-of-the-art test-time scaling efficiency, offering substantial improvements in both accuracy and computational cost. As a result, Falcon-H1R demonstrates that compact models, through targeted model training and architectural choices, can deliver robust and scalable reasoning performance."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-05T18:44:27Z",
                "published_parsed": [
                    2026,
                    1,
                    5,
                    18,
                    44,
                    27,
                    0,
                    5,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Falcon LLM Team"
                    },
                    {
                        "name": "Iheb Chaabane"
                    },
                    {
                        "name": "Puneesh Khanna"
                    },
                    {
                        "name": "Suhail Mohmad"
                    },
                    {
                        "name": "Slim Frikha"
                    },
                    {
                        "name": "Shi Hu"
                    },
                    {
                        "name": "Abdalgader Abubaker"
                    },
                    {
                        "name": "Reda Alami"
                    },
                    {
                        "name": "Mikhail Lubinets"
                    },
                    {
                        "name": "Mohamed El Amine Seddik"
                    },
                    {
                        "name": "Hakim Hacid"
                    }
                ],
                "author_detail": {
                    "name": "Hakim Hacid"
                },
                "author": "Hakim Hacid"
            },
            {
                "id": "http://arxiv.org/abs/2601.02337v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.02337v1",
                "title": "Robust Persona-Aware Toxicity Detection with Prompt Optimization and Learned Ensembling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust Persona-Aware Toxicity Detection with Prompt Optimization and Learned Ensembling"
                },
                "updated": "2026-01-05T18:32:45Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    18,
                    32,
                    45,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.02337v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.02337v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Toxicity detection is inherently subjective, shaped by the diverse perspectives and social priors of different demographic groups. While ``pluralistic'' modeling as used in economics and the social sciences aims to capture perspective differences across contexts, current Large Language Model (LLM) prompting techniques have different results across different personas and base models. In this work, we conduct a systematic evaluation of persona-aware toxicity detection, showing that no single prompting method, including our proposed automated prompt optimization strategy, uniformly dominates across all model-persona pairs. To exploit complementary errors, we explore ensembling four prompting variants and propose a lightweight meta-ensemble: an SVM over the 4-bit vector of prompt predictions. Our results demonstrate that the proposed SVM ensemble consistently outperforms individual prompting methods and traditional majority-voting techniques, achieving the strongest overall performance across diverse personas. This work provides one of the first systematic comparisons of persona-conditioned prompting for toxicity detection and offers a robust method for pluralistic evaluation in subjective NLP tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toxicity detection is inherently subjective, shaped by the diverse perspectives and social priors of different demographic groups. While ``pluralistic'' modeling as used in economics and the social sciences aims to capture perspective differences across contexts, current Large Language Model (LLM) prompting techniques have different results across different personas and base models. In this work, we conduct a systematic evaluation of persona-aware toxicity detection, showing that no single prompting method, including our proposed automated prompt optimization strategy, uniformly dominates across all model-persona pairs. To exploit complementary errors, we explore ensembling four prompting variants and propose a lightweight meta-ensemble: an SVM over the 4-bit vector of prompt predictions. Our results demonstrate that the proposed SVM ensemble consistently outperforms individual prompting methods and traditional majority-voting techniques, achieving the strongest overall performance across diverse personas. This work provides one of the first systematic comparisons of persona-conditioned prompting for toxicity detection and offers a robust method for pluralistic evaluation in subjective NLP tasks."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-05T18:32:45Z",
                "published_parsed": [
                    2026,
                    1,
                    5,
                    18,
                    32,
                    45,
                    0,
                    5,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Berk Atil"
                    },
                    {
                        "name": "Rebecca J. Passonneau"
                    },
                    {
                        "name": "Ninareh Mehrabi"
                    }
                ],
                "author_detail": {
                    "name": "Ninareh Mehrabi"
                },
                "author": "Ninareh Mehrabi"
            },
            {
                "id": "http://arxiv.org/abs/2601.02320v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.02320v1",
                "title": "Estimating Text Temperature",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Estimating Text Temperature"
                },
                "updated": "2026-01-05T18:09:41Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    18,
                    9,
                    41,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.02320v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.02320v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Autoregressive language models typically use temperature parameter at inference to shape the probability distribution and control the randomness of the text generated. After the text was generated, this parameter can be estimated using maximum likelihood approach. Following it, we propose a procedure to estimate the temperature of any text, including ones written by humans, with respect to a given language model. We evaluate the temperature estimation capability of a wide selection of small-to-medium LLMs. We then use the best-performing Qwen3 14B to estimate temperatures of popular corpora.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive language models typically use temperature parameter at inference to shape the probability distribution and control the randomness of the text generated. After the text was generated, this parameter can be estimated using maximum likelihood approach. Following it, we propose a procedure to estimate the temperature of any text, including ones written by humans, with respect to a given language model. We evaluate the temperature estimation capability of a wide selection of small-to-medium LLMs. We then use the best-performing Qwen3 14B to estimate temperatures of popular corpora."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-05T18:09:41Z",
                "published_parsed": [
                    2026,
                    1,
                    5,
                    18,
                    9,
                    41,
                    0,
                    5,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Nikolay Mikhaylovskiy"
                    }
                ],
                "author_detail": {
                    "name": "Nikolay Mikhaylovskiy"
                },
                "author": "Nikolay Mikhaylovskiy"
            },
            {
                "id": "http://arxiv.org/abs/2503.18773v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2503.18773v3",
                "title": "BitDecoding: Unlocking Tensor Cores for Long-Context LLMs with Low-Bit KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BitDecoding: Unlocking Tensor Cores for Long-Context LLMs with Low-Bit KV Cache"
                },
                "updated": "2026-01-05T18:08:27Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    18,
                    8,
                    27,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2503.18773v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2503.18773v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The growth of long-context Large Language Models (LLMs) significantly increases memory and bandwidth pressure during autoregressive decoding due to the expanding Key-Value (KV) cache. While accuracy-preserving KV-cache quantization (e.g., 4-bit or 2-bit) reduces memory footprint, existing systems decode inefficiently by relying solely on CUDA cores, underutilizing Tensor Cores-the dominant compute resource on GPUs.\n  We present BitDecoding, the first inference system to efficiently decode low-bit KV caches by cooperatively leveraging CUDA cores and Tensor Cores. BitDecoding smartly induces Tensor-Core-friendly layouts, introduces warp-level dequantization parallelism, and provides unified system support through query transformation, high-performance tensor- and channel-wise quantization, and a software-pipelined dequantization kernel enabling mixed-precision execution. Architecture-aware optimizations further leverage Hopper's warpgroup tensor instructions and Blackwell's NVFP4 (MXFP4) tensor formats.\n  Evaluated on Blackwell, Hopper, and Ampere GPUs, BitDecoding achieves an average 7.5x decoding speedup over FP16 FlashDecoding-v2, up to 8.6x on Blackwell with NVFP4, and up to 4.3x over state-of-the-art approaches. On LLaMA-3.1-8B with a 128K context, BitDecoding reduces single-batch decoding latency by 3x. BitDecoding is open-sourced at https://github.com/OpenBitSys/BitDecoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growth of long-context Large Language Models (LLMs) significantly increases memory and bandwidth pressure during autoregressive decoding due to the expanding Key-Value (KV) cache. While accuracy-preserving KV-cache quantization (e.g., 4-bit or 2-bit) reduces memory footprint, existing systems decode inefficiently by relying solely on CUDA cores, underutilizing Tensor Cores-the dominant compute resource on GPUs.\n  We present BitDecoding, the first inference system to efficiently decode low-bit KV caches by cooperatively leveraging CUDA cores and Tensor Cores. BitDecoding smartly induces Tensor-Core-friendly layouts, introduces warp-level dequantization parallelism, and provides unified system support through query transformation, high-performance tensor- and channel-wise quantization, and a software-pipelined dequantization kernel enabling mixed-precision execution. Architecture-aware optimizations further leverage Hopper's warpgroup tensor instructions and Blackwell's NVFP4 (MXFP4) tensor formats.\n  Evaluated on Blackwell, Hopper, and Ampere GPUs, BitDecoding achieves an average 7.5x decoding speedup over FP16 FlashDecoding-v2, up to 8.6x on Blackwell with NVFP4, and up to 4.3x over state-of-the-art approaches. On LLaMA-3.1-8B with a 128K context, BitDecoding reduces single-batch decoding latency by 3x. BitDecoding is open-sourced at https://github.com/OpenBitSys/BitDecoding."
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-03-24T15:22:41Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    15,
                    22,
                    41,
                    0,
                    83,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR"
                },
                "authors": [
                    {
                        "name": "Dayou Du"
                    },
                    {
                        "name": "Shijie Cao"
                    },
                    {
                        "name": "Jianyi Cheng"
                    },
                    {
                        "name": "Luo Mai"
                    },
                    {
                        "name": "Ting Cao"
                    },
                    {
                        "name": "Mao Yang"
                    }
                ],
                "author_detail": {
                    "name": "Mao Yang"
                },
                "author": "Mao Yang"
            },
            {
                "id": "http://arxiv.org/abs/2601.02314v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.02314v1",
                "title": "Project Ariadne: A Structural Causal Framework for Auditing Faithfulness in LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Project Ariadne: A Structural Causal Framework for Auditing Faithfulness in LLM Agents"
                },
                "updated": "2026-01-05T18:05:29Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    18,
                    5,
                    29,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.02314v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.02314v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "As Large Language Model (LLM) agents are increasingly tasked with high-stakes autonomous decision-making, the transparency of their reasoning processes has become a critical safety concern. While \\textit{Chain-of-Thought} (CoT) prompting allows agents to generate human-readable reasoning traces, it remains unclear whether these traces are \\textbf{faithful} generative drivers of the model's output or merely \\textbf{post-hoc rationalizations}. We introduce \\textbf{Project Ariadne}, a novel XAI framework that utilizes Structural Causal Models (SCMs) and counterfactual logic to audit the causal integrity of agentic reasoning. Unlike existing interpretability methods that rely on surface-level textual similarity, Project Ariadne performs \\textbf{hard interventions} ($do$-calculus) on intermediate reasoning nodes -- systematically inverting logic, negating premises, and reversing factual claims -- to measure the \\textbf{Causal Sensitivity} ($φ$) of the terminal answer. Our empirical evaluation of state-of-the-art models reveals a persistent \\textit{Faithfulness Gap}. We define and detect a widespread failure mode termed \\textbf{Causal Decoupling}, where agents exhibit a violation density ($ρ$) of up to $0.77$ in factual and scientific domains. In these instances, agents arrive at identical conclusions despite contradictory internal logic, proving that their reasoning traces function as \"Reasoning Theater\" while decision-making is governed by latent parametric priors. Our findings suggest that current agentic architectures are inherently prone to unfaithful explanation, and we propose the Ariadne Score as a new benchmark for aligning stated logic with model action.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Model (LLM) agents are increasingly tasked with high-stakes autonomous decision-making, the transparency of their reasoning processes has become a critical safety concern. While \\textit{Chain-of-Thought} (CoT) prompting allows agents to generate human-readable reasoning traces, it remains unclear whether these traces are \\textbf{faithful} generative drivers of the model's output or merely \\textbf{post-hoc rationalizations}. We introduce \\textbf{Project Ariadne}, a novel XAI framework that utilizes Structural Causal Models (SCMs) and counterfactual logic to audit the causal integrity of agentic reasoning. Unlike existing interpretability methods that rely on surface-level textual similarity, Project Ariadne performs \\textbf{hard interventions} ($do$-calculus) on intermediate reasoning nodes -- systematically inverting logic, negating premises, and reversing factual claims -- to measure the \\textbf{Causal Sensitivity} ($φ$) of the terminal answer. Our empirical evaluation of state-of-the-art models reveals a persistent \\textit{Faithfulness Gap}. We define and detect a widespread failure mode termed \\textbf{Causal Decoupling}, where agents exhibit a violation density ($ρ$) of up to $0.77$ in factual and scientific domains. In these instances, agents arrive at identical conclusions despite contradictory internal logic, proving that their reasoning traces function as \"Reasoning Theater\" while decision-making is governed by latent parametric priors. Our findings suggest that current agentic architectures are inherently prone to unfaithful explanation, and we propose the Ariadne Score as a new benchmark for aligning stated logic with model action."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-05T18:05:29Z",
                "published_parsed": [
                    2026,
                    1,
                    5,
                    18,
                    5,
                    29,
                    0,
                    5,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Sourena Khanzadeh"
                    }
                ],
                "author_detail": {
                    "name": "Sourena Khanzadeh"
                },
                "author": "Sourena Khanzadeh"
            },
            {
                "id": "http://arxiv.org/abs/2601.00497v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.00497v2",
                "title": "STELLAR: A Search-Based Testing Framework for Large Language Model Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "STELLAR: A Search-Based Testing Framework for Large Language Model Applications"
                },
                "updated": "2026-01-05T18:03:57Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    18,
                    3,
                    57,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.00497v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.00497v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Model (LLM)-based applications are increasingly deployed across various domains, including customer service, education, and mobility. However, these systems are prone to inaccurate, fictitious, or harmful responses, and their vast, high-dimensional input space makes systematic testing particularly challenging. To address this, we present STELLAR, an automated search-based testing framework for LLM-based applications that systematically uncovers text inputs leading to inappropriate system responses. Our framework models test generation as an optimization problem and discretizes the input space into stylistic, content-related, and perturbation features. Unlike prior work that focuses on prompt optimization or coverage heuristics, our work employs evolutionary optimization to dynamically explore feature combinations that are more likely to expose failures. We evaluate STELLAR on three LLM-based conversational question-answering systems. The first focuses on safety, benchmarking both public and proprietary LLMs against malicious or unsafe prompts. The second and third target navigation, using an open-source and an industrial retrieval-augmented system for in-vehicle venue recommendations. Overall, STELLAR exposes up to 4.3 times (average 2.5 times) more failures than the existing baseline approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM)-based applications are increasingly deployed across various domains, including customer service, education, and mobility. However, these systems are prone to inaccurate, fictitious, or harmful responses, and their vast, high-dimensional input space makes systematic testing particularly challenging. To address this, we present STELLAR, an automated search-based testing framework for LLM-based applications that systematically uncovers text inputs leading to inappropriate system responses. Our framework models test generation as an optimization problem and discretizes the input space into stylistic, content-related, and perturbation features. Unlike prior work that focuses on prompt optimization or coverage heuristics, our work employs evolutionary optimization to dynamically explore feature combinations that are more likely to expose failures. We evaluate STELLAR on three LLM-based conversational question-answering systems. The first focuses on safety, benchmarking both public and proprietary LLMs against malicious or unsafe prompts. The second and third target navigation, using an open-source and an industrial retrieval-augmented system for in-vehicle venue recommendations. Overall, STELLAR exposes up to 4.3 times (average 2.5 times) more failures than the existing baseline approaches."
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-01T22:30:15Z",
                "published_parsed": [
                    2026,
                    1,
                    1,
                    22,
                    30,
                    15,
                    3,
                    1,
                    0
                ],
                "arxiv_comment": "Accepted for publication at the 33th International Conference on Software Analysis, Evolution and Reengineering (SANER 2026)",
                "arxiv_primary_category": {
                    "term": "cs.SE"
                },
                "authors": [
                    {
                        "name": "Lev Sorokin"
                    },
                    {
                        "name": "Ivan Vasilev"
                    },
                    {
                        "name": "Ken E. Friedl"
                    },
                    {
                        "name": "Andrea Stocco"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Stocco"
                },
                "author": "Andrea Stocco"
            },
            {
                "id": "http://arxiv.org/abs/2505.09665v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2505.09665v3",
                "title": "Tales of the 2025 Los Angeles Fire: Hotwash for Public Health Concerns in Reddit via LLM-Enhanced Topic Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tales of the 2025 Los Angeles Fire: Hotwash for Public Health Concerns in Reddit via LLM-Enhanced Topic Modeling"
                },
                "updated": "2026-01-05T18:01:24Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    18,
                    1,
                    24,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2505.09665v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2505.09665v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Wildfires have become increasingly frequent, irregular, and severe in recent years. Understanding how affected populations perceive and respond during wildfire crises is critical for timely and empathetic disaster response. Social media platforms offer a crowd-sourced channel to capture evolving public discourse, providing hyperlocal information and insight into public sentiment. This study analyzes Reddit discourse during the 2025 Los Angeles wildfires, spanning from the onset of the disaster to full containment. We collect 385 posts and 114,879 comments related to the Palisades and Eaton fires. We adopt topic modeling methods to identify the latent topics, enhanced by large language models (LLMs) and human-in-the-loop (HITL) refinement. Furthermore, we develop a hierarchical framework to categorize latent topics, consisting of two main categories, Situational Awareness (SA) and Crisis Narratives (CN). The volume of SA category closely aligns with real-world fire progressions, peaking within the first 2-5 days as the fires reach the maximum extent. The most frequent co-occurring category set of public health and safety, loss and damage, and emergency resources expands on a wide range of health-related latent topics, including environmental health, occupational health, and one health. Grief signals and mental health risks consistently accounted for 60 percentage and 40 percentage of CN instances, respectively, with the highest total volume occurring at night. This study contributes the first annotated social media dataset on the 2025 LA fires, and introduces a scalable multi-layer framework that leverages topic modeling for crisis discourse analysis. By identifying persistent public health concerns, our results can inform more empathetic and adaptive strategies for disaster response, public health communication, and future research in comparable climate-related disaster events.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wildfires have become increasingly frequent, irregular, and severe in recent years. Understanding how affected populations perceive and respond during wildfire crises is critical for timely and empathetic disaster response. Social media platforms offer a crowd-sourced channel to capture evolving public discourse, providing hyperlocal information and insight into public sentiment. This study analyzes Reddit discourse during the 2025 Los Angeles wildfires, spanning from the onset of the disaster to full containment. We collect 385 posts and 114,879 comments related to the Palisades and Eaton fires. We adopt topic modeling methods to identify the latent topics, enhanced by large language models (LLMs) and human-in-the-loop (HITL) refinement. Furthermore, we develop a hierarchical framework to categorize latent topics, consisting of two main categories, Situational Awareness (SA) and Crisis Narratives (CN). The volume of SA category closely aligns with real-world fire progressions, peaking within the first 2-5 days as the fires reach the maximum extent. The most frequent co-occurring category set of public health and safety, loss and damage, and emergency resources expands on a wide range of health-related latent topics, including environmental health, occupational health, and one health. Grief signals and mental health risks consistently accounted for 60 percentage and 40 percentage of CN instances, respectively, with the highest total volume occurring at night. This study contributes the first annotated social media dataset on the 2025 LA fires, and introduces a scalable multi-layer framework that leverages topic modeling for crisis discourse analysis. By identifying persistent public health concerns, our results can inform more empathetic and adaptive strategies for disaster response, public health communication, and future research in comparable climate-related disaster events."
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-05-14T16:31:08Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    16,
                    31,
                    8,
                    2,
                    134,
                    0
                ],
                "arxiv_comment": "Fix typos in Method Section. Add data/code availability",
                "arxiv_primary_category": {
                    "term": "cs.SI"
                },
                "authors": [
                    {
                        "name": "Sulong Zhou"
                    },
                    {
                        "name": "Qunying Huang"
                    },
                    {
                        "name": "Shaoheng Zhou"
                    },
                    {
                        "name": "Yun Hang"
                    },
                    {
                        "name": "Xinyue Ye"
                    },
                    {
                        "name": "Aodong Mei"
                    },
                    {
                        "name": "Kathryn Phung"
                    },
                    {
                        "name": "Yuning Ye"
                    },
                    {
                        "name": "Uma Govindswamy"
                    },
                    {
                        "name": "Zehan Li"
                    }
                ],
                "author_detail": {
                    "name": "Zehan Li"
                },
                "author": "Zehan Li"
            },
            {
                "id": "http://arxiv.org/abs/2411.06254v4",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2411.06254v4",
                "title": "Adaptive Evidence Budgeting for Scalable Long-Document Reranking with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Evidence Budgeting for Scalable Long-Document Reranking with LLMs"
                },
                "updated": "2026-01-05T17:50:15Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    17,
                    50,
                    15,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2411.06254v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2411.06254v4",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Decoder-only LLM rerankers are powerful but often struggle with long documents: inference is costly and relevance signals can be diluted as irrelevant text accumulates in the context window. Motivated by an attention analysis showing that relevance-aligned heads degrade when non-relevant text is appended, we propose EviRerank, a scalable framework that (i) scores document blocks with a lightweight selector (BM25, bi-encoder, or cross-encoder), (ii) constructs a compact evidence context under a strict token budget, and (iii) reranks with a decoder-only LLM. Our key contribution is Adaptive Evidence Budgeting (AEB), an information-density-aware dynamic stopping strategy that avoids low-utility tail blocks, and we further study Summary Augmentation (SA) within the same budget. Across TREC DL'19, DL'23, and MLDR-zh, EviRerank consistently improves over full-document LLM reranking and strong block-selection baselines while substantially reducing the required input length. On TREC DL'19, EviRerank achieves 0.743 nDCG@10 and 0.307 MAP, improving over RankLLaMA (0.701/0.288) by +0.042 nDCG@10 (+6.0%) and +0.019 MAP (+6.6%).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decoder-only LLM rerankers are powerful but often struggle with long documents: inference is costly and relevance signals can be diluted as irrelevant text accumulates in the context window. Motivated by an attention analysis showing that relevance-aligned heads degrade when non-relevant text is appended, we propose EviRerank, a scalable framework that (i) scores document blocks with a lightweight selector (BM25, bi-encoder, or cross-encoder), (ii) constructs a compact evidence context under a strict token budget, and (iii) reranks with a decoder-only LLM. Our key contribution is Adaptive Evidence Budgeting (AEB), an information-density-aware dynamic stopping strategy that avoids low-utility tail blocks, and we further study Summary Augmentation (SA) within the same budget. Across TREC DL'19, DL'23, and MLDR-zh, EviRerank consistently improves over full-document LLM reranking and strong block-selection baselines while substantially reducing the required input length. On TREC DL'19, EviRerank achieves 0.743 nDCG@10 and 0.307 MAP, improving over RankLLaMA (0.701/0.288) by +0.042 nDCG@10 (+6.0%) and +0.019 MAP (+6.6%)."
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2024-11-09T19:03:56Z",
                "published_parsed": [
                    2024,
                    11,
                    9,
                    19,
                    3,
                    56,
                    5,
                    314,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR"
                },
                "authors": [
                    {
                        "name": "Minghan Li"
                    },
                    {
                        "name": "Eric Gaussier"
                    },
                    {
                        "name": "Juntao Li"
                    },
                    {
                        "name": "Guodong Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Guodong Zhou"
                },
                "author": "Guodong Zhou"
            },
            {
                "id": "http://arxiv.org/abs/2601.02304v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.02304v1",
                "title": "Octopus: A Lightweight Entity-Aware System for Multi-Table Data Discovery and Cell-Level Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Octopus: A Lightweight Entity-Aware System for Multi-Table Data Discovery and Cell-Level Retrieval"
                },
                "updated": "2026-01-05T17:43:49Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    17,
                    43,
                    49,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.02304v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.02304v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Tabular data constitute a dominant form of information in modern data lakes and repositories, yet discovering the relevant tables to answer user questions remains challenging. Existing data discovery systems assume that each question can be answered by a single table and often rely on resource-intensive offline preprocessing, such as model training or large-scale content indexing. In practice, however, many questions require information spread across multiple tables -- either independently or through joins -- and users often seek specific cell values rather than entire tables. In this paper, we present Octopus, a lightweight, entity-aware, and training-free system for multi-table data discovery and cell-level value retrieval. Instead of embedding entire questions, Octopus identifies fine-grained entities (column mentions and value mentions) from natural-language queries using an LLM parser. It then matches these entities to table headers through a compact embedding index and scans table contents directly for value occurrences, eliminating the need for heavy content indexing or costly offline stages. The resulting fine-grained alignment not only improves table retrieval accuracy but also facilitates efficient downstream NL2SQL execution by reducing token usage and redundant LLM calls. To evaluate Octopus, we introduce a new benchmark covering both table- and cell-level discovery under multi-table settings, including five datasets for independent discovery and two for join-based discovery. Experimental results show that Octopus consistently outperforms existing systems while achieving substantially lower computational and token costs. Code is available at https://github.com/wenzhilics/octopus.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tabular data constitute a dominant form of information in modern data lakes and repositories, yet discovering the relevant tables to answer user questions remains challenging. Existing data discovery systems assume that each question can be answered by a single table and often rely on resource-intensive offline preprocessing, such as model training or large-scale content indexing. In practice, however, many questions require information spread across multiple tables -- either independently or through joins -- and users often seek specific cell values rather than entire tables. In this paper, we present Octopus, a lightweight, entity-aware, and training-free system for multi-table data discovery and cell-level value retrieval. Instead of embedding entire questions, Octopus identifies fine-grained entities (column mentions and value mentions) from natural-language queries using an LLM parser. It then matches these entities to table headers through a compact embedding index and scans table contents directly for value occurrences, eliminating the need for heavy content indexing or costly offline stages. The resulting fine-grained alignment not only improves table retrieval accuracy but also facilitates efficient downstream NL2SQL execution by reducing token usage and redundant LLM calls. To evaluate Octopus, we introduce a new benchmark covering both table- and cell-level discovery under multi-table settings, including five datasets for independent discovery and two for join-based discovery. Experimental results show that Octopus consistently outperforms existing systems while achieving substantially lower computational and token costs. Code is available at https://github.com/wenzhilics/octopus."
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-05T17:43:49Z",
                "published_parsed": [
                    2026,
                    1,
                    5,
                    17,
                    43,
                    49,
                    0,
                    5,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB"
                },
                "authors": [
                    {
                        "name": "Wen-Zhi Li"
                    },
                    {
                        "name": "Sainyam Galhotra"
                    }
                ],
                "author_detail": {
                    "name": "Sainyam Galhotra"
                },
                "author": "Sainyam Galhotra"
            },
            {
                "id": "http://arxiv.org/abs/2511.04847v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.04847v3",
                "title": "Grounded Test-Time Adaptation for LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Grounded Test-Time Adaptation for LLM Agents"
                },
                "updated": "2026-01-05T17:43:48Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    17,
                    43,
                    48,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.04847v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.04847v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language model (LLM)-based agents struggle to generalize to novel and complex environments, such as unseen websites or new sets of functions, due to a fundamental mismatch between their pre-training and test-time conditions. This challenge stems from two distinct failure modes: a syntactic misunderstanding of environment-specific components like observation formats, and a semantic misunderstanding of state-transition dynamics, which are only revealed at test time. To address these issues, we propose two distinct and complementary strategies for adapting LLM agents by leveraging environment-specific information available during deployment. First, an online distributional adaptation method parameterizes environmental nuances by learning a lightweight adaptation vector that biases the model's output distribution, enabling rapid alignment with an environment response format. Second, a deployment-time dynamics grounding method employs a persona-driven exploration phase to systematically probe and learn the environment's causal dynamics before task execution, equipping the agent with a nonparametric world model. We evaluate these strategies across diverse agentic benchmarks, including function calling and web navigation. Our empirical results show the effectiveness of both strategies across all benchmarks with minimal computational cost. We find that dynamics grounding is particularly effective in complex environments where unpredictable dynamics pose a major obstacle, demonstrating a robust path toward more generalizable and capable LLM-based agents. For example, on the WebArena multi-site split, this method increases the agent's success rate from 2% to 23%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM)-based agents struggle to generalize to novel and complex environments, such as unseen websites or new sets of functions, due to a fundamental mismatch between their pre-training and test-time conditions. This challenge stems from two distinct failure modes: a syntactic misunderstanding of environment-specific components like observation formats, and a semantic misunderstanding of state-transition dynamics, which are only revealed at test time. To address these issues, we propose two distinct and complementary strategies for adapting LLM agents by leveraging environment-specific information available during deployment. First, an online distributional adaptation method parameterizes environmental nuances by learning a lightweight adaptation vector that biases the model's output distribution, enabling rapid alignment with an environment response format. Second, a deployment-time dynamics grounding method employs a persona-driven exploration phase to systematically probe and learn the environment's causal dynamics before task execution, equipping the agent with a nonparametric world model. We evaluate these strategies across diverse agentic benchmarks, including function calling and web navigation. Our empirical results show the effectiveness of both strategies across all benchmarks with minimal computational cost. We find that dynamics grounding is particularly effective in complex environments where unpredictable dynamics pose a major obstacle, demonstrating a robust path toward more generalizable and capable LLM-based agents. For example, on the WebArena multi-site split, this method increases the agent's success rate from 2% to 23%."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-06T22:24:35Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    22,
                    24,
                    35,
                    3,
                    310,
                    0
                ],
                "arxiv_comment": "Our code is available here: https://github.com/r2llab/GTTA",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Arthur Chen"
                    },
                    {
                        "name": "Zuxin Liu"
                    },
                    {
                        "name": "Jianguo Zhang"
                    },
                    {
                        "name": "Akshara Prabhakar"
                    },
                    {
                        "name": "Zhiwei Liu"
                    },
                    {
                        "name": "Shelby Heinecke"
                    },
                    {
                        "name": "Silvio Savarese"
                    },
                    {
                        "name": "Victor Zhong"
                    },
                    {
                        "name": "Caiming Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Caiming Xiong"
                },
                "author": "Caiming Xiong"
            },
            {
                "id": "http://arxiv.org/abs/2510.06478v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.06478v2",
                "title": "Anytime-Valid Answer Sufficiency Certificates for LLM Generation via Sequential Information Lift",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Anytime-Valid Answer Sufficiency Certificates for LLM Generation via Sequential Information Lift"
                },
                "updated": "2026-01-05T17:33:34Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    17,
                    33,
                    34,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.06478v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.06478v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We introduce Sequential-EDFL (Empirical Dynamic Formal Lift), which applies anytime-valid sequential testing to language model generation stopping. Our approach tracks information lift, defined as the log-likelihood ratio between the full model and deliberately weakened \"skeleton\" baselines, using self-normalized empirical-Bernstein e-processes that provide formal delta-level error control regardless of stopping time. This delta guarantee controls premature stopping when information lift is insufficient relative to the skeleton, and it does not imply delta control of factual incorrectness or hallucinations. We handle unknown centering through online mean estimation, combine multiple parameters via mixture e-processes, and support adaptive resets under distributional drift. On six benchmarks, Sequential-EDFL reduces generation length by 22 to 28 percent relative to sequential baselines while maintaining delta-level control with 12 percent computational overhead. We introduce automated skeletons (distilled submodels and randomized logits) and show robustness across skeleton families. Composing EDFL with a lightweight correctness gate (sentence boundaries plus a verifier) improves end-task correctness while preserving anytime-valid guarantees by only delaying stopping. Our certificates control information sufficiency, not factual correctness. Specifically, 10.9 percent of stopped sequences remain incorrect even with the gate (13.2 to 22.7 percent without it). EDFL serves as a first-stage filter that can reduce verification burden: when applied to stopped sequences, the gate validates 83 percent of stops, requiring full verification only for the remaining 17 percent, plus all non-stopped sequences. EDFL is not a standalone solution for safety-critical domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Sequential-EDFL (Empirical Dynamic Formal Lift), which applies anytime-valid sequential testing to language model generation stopping. Our approach tracks information lift, defined as the log-likelihood ratio between the full model and deliberately weakened \"skeleton\" baselines, using self-normalized empirical-Bernstein e-processes that provide formal delta-level error control regardless of stopping time. This delta guarantee controls premature stopping when information lift is insufficient relative to the skeleton, and it does not imply delta control of factual incorrectness or hallucinations. We handle unknown centering through online mean estimation, combine multiple parameters via mixture e-processes, and support adaptive resets under distributional drift. On six benchmarks, Sequential-EDFL reduces generation length by 22 to 28 percent relative to sequential baselines while maintaining delta-level control with 12 percent computational overhead. We introduce automated skeletons (distilled submodels and randomized logits) and show robustness across skeleton families. Composing EDFL with a lightweight correctness gate (sentence boundaries plus a verifier) improves end-task correctness while preserving anytime-valid guarantees by only delaying stopping. Our certificates control information sufficiency, not factual correctness. Specifically, 10.9 percent of stopped sequences remain incorrect even with the gate (13.2 to 22.7 percent without it). EDFL serves as a first-stage filter that can reduce verification burden: when applied to stopped sequences, the gate validates 83 percent of stops, requiring full verification only for the remaining 17 percent, plus all non-stopped sequences. EDFL is not a standalone solution for safety-critical domains."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-07T21:28:53Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    21,
                    28,
                    53,
                    1,
                    280,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Sanjeda Akter"
                    },
                    {
                        "name": "Ibne Farabi Shihab"
                    },
                    {
                        "name": "Anuj Sharma"
                    }
                ],
                "author_detail": {
                    "name": "Anuj Sharma"
                },
                "author": "Anuj Sharma"
            },
            {
                "id": "http://arxiv.org/abs/2601.02298v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.02298v1",
                "title": "Power-of-Two Quantization-Aware-Training (PoT-QAT) in Large Language Models (LLMs)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Power-of-Two Quantization-Aware-Training (PoT-QAT) in Large Language Models (LLMs)"
                },
                "updated": "2026-01-05T17:33:16Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    17,
                    33,
                    16,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.02298v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.02298v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "In Large Language Models (LLMs), the number of parameters has grown exponentially in the past few years, e.g., from 1.5 billion parameters in GPT-2 to 175 billion in GPT-3 to possibly more than trillion in higher versions. This raises a significant challenge for implementation, especially for Edge devices. Unlike cloud computing, memory and processing power for Edge devices are very limited, which necessitates developing novel ideas to make such applications feasible. In this work, we investigate compressing weights with a special quantization that limits numbers to only power-of-two (PoT). This helps save a huge amount of memory as only exponents need to be stored, more importantly, it significantly reduces processing power by replacing costly multiplication with low cost bit shifting. To overcome performance loss due to this strict quantization, we investigate Quantization Aware Training (QAT) to enhance performance through additional training. Results on GPT-2 124M show a major enhancement for quantized PoT model after additional training, with a perplexity enhancement of 66% and BERT-Score loss to baseline GPT-2 of 1%. The memory saving is estimated to be 87.5% while the inference speed is expected to be 3-10x faster with PoT quantization versus full-precision.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In Large Language Models (LLMs), the number of parameters has grown exponentially in the past few years, e.g., from 1.5 billion parameters in GPT-2 to 175 billion in GPT-3 to possibly more than trillion in higher versions. This raises a significant challenge for implementation, especially for Edge devices. Unlike cloud computing, memory and processing power for Edge devices are very limited, which necessitates developing novel ideas to make such applications feasible. In this work, we investigate compressing weights with a special quantization that limits numbers to only power-of-two (PoT). This helps save a huge amount of memory as only exponents need to be stored, more importantly, it significantly reduces processing power by replacing costly multiplication with low cost bit shifting. To overcome performance loss due to this strict quantization, we investigate Quantization Aware Training (QAT) to enhance performance through additional training. Results on GPT-2 124M show a major enhancement for quantized PoT model after additional training, with a perplexity enhancement of 66% and BERT-Score loss to baseline GPT-2 of 1%. The memory saving is estimated to be 87.5% while the inference speed is expected to be 3-10x faster with PoT quantization versus full-precision."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-05T17:33:16Z",
                "published_parsed": [
                    2026,
                    1,
                    5,
                    17,
                    33,
                    16,
                    0,
                    5,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Mahmoud Elgenedy"
                    }
                ],
                "author_detail": {
                    "name": "Mahmoud Elgenedy"
                },
                "author": "Mahmoud Elgenedy"
            },
            {
                "id": "http://arxiv.org/abs/2509.01544v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2509.01544v3",
                "title": "Causal Consistency Regularization: Training Verifiably Sensitive Reasoning in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal Consistency Regularization: Training Verifiably Sensitive Reasoning in Large Language Models"
                },
                "updated": "2026-01-05T17:24:02Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    17,
                    24,
                    2,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2509.01544v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2509.01544v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models can produce correct answers while relying on flawed reasoning traces, partly because common training objectives reward final-answer correctness rather than faithful intermediate reasoning. This undermines trustworthiness in high-stakes settings. We propose Counterfactual Sensitivity Regularization (CSR), a training paradigm that improves reasoning faithfulness by enforcing causal consistency between reasoning steps and outcomes. CSR automatically applies operator-level interventions to reasoning traces, such as swapping \"+\" with \"-\", to generate minimally perturbed counterfactual rationales, and penalizes the model when these logically invalid traces still lead to the original answer. Our implementation is efficient, adding about 9 percent training overhead via a warm-start curriculum and token-subset optimization.\n  We evaluate faithfulness using Counterfactual Outcome Sensitivity (COS), which measures how appropriately answers change under logical perturbations. Across arithmetic (GSM8K), logical deduction (ProofWriter), multi-hop question answering (HotpotQA), and code generation (MBPP), CSR yields improved accuracy versus faithfulness trade-offs, establishing a new Pareto frontier. CSR improves faithfulness over standard fine-tuning and process supervision by up to 70 percentage points, and transfers across model families with 94.2 to 96.7 percent success in structured domains. CSR also complements inference-time methods such as self-consistency. Overall, CSR offers a practical route to more reliable reasoning in structured domains, including mathematics, formal logic, and code, where operators are well-defined and verifiable, covering an estimated 40 to 60 percent of high-stakes reasoning deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models can produce correct answers while relying on flawed reasoning traces, partly because common training objectives reward final-answer correctness rather than faithful intermediate reasoning. This undermines trustworthiness in high-stakes settings. We propose Counterfactual Sensitivity Regularization (CSR), a training paradigm that improves reasoning faithfulness by enforcing causal consistency between reasoning steps and outcomes. CSR automatically applies operator-level interventions to reasoning traces, such as swapping \"+\" with \"-\", to generate minimally perturbed counterfactual rationales, and penalizes the model when these logically invalid traces still lead to the original answer. Our implementation is efficient, adding about 9 percent training overhead via a warm-start curriculum and token-subset optimization.\n  We evaluate faithfulness using Counterfactual Outcome Sensitivity (COS), which measures how appropriately answers change under logical perturbations. Across arithmetic (GSM8K), logical deduction (ProofWriter), multi-hop question answering (HotpotQA), and code generation (MBPP), CSR yields improved accuracy versus faithfulness trade-offs, establishing a new Pareto frontier. CSR improves faithfulness over standard fine-tuning and process supervision by up to 70 percentage points, and transfers across model families with 94.2 to 96.7 percent success in structured domains. CSR also complements inference-time methods such as self-consistency. Overall, CSR offers a practical route to more reliable reasoning in structured domains, including mathematics, formal logic, and code, where operators are well-defined and verifiable, covering an estimated 40 to 60 percent of high-stakes reasoning deployments."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-09-01T15:18:46Z",
                "published_parsed": [
                    2025,
                    9,
                    1,
                    15,
                    18,
                    46,
                    0,
                    244,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Sanjeda Akter"
                    },
                    {
                        "name": "Ibne Farabi Shihab"
                    },
                    {
                        "name": "Anuj Sharma"
                    }
                ],
                "author_detail": {
                    "name": "Anuj Sharma"
                },
                "author": "Anuj Sharma"
            },
            {
                "id": "http://arxiv.org/abs/2601.02285v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.02285v1",
                "title": "pdfQA: Diverse, Challenging, and Realistic Question Answering over PDFs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "pdfQA: Diverse, Challenging, and Realistic Question Answering over PDFs"
                },
                "updated": "2026-01-05T17:15:26Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    17,
                    15,
                    26,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.02285v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.02285v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "PDFs are the second-most used document type on the internet (after HTML). Yet, existing QA datasets commonly start from text sources or only address specific domains. In this paper, we present pdfQA, a multi-domain 2K human-annotated (real-pdfQA) and 2K synthetic dataset (syn-pdfQA) differentiating QA pairs in ten complexity dimensions (e.g., file type, source modality, source position, answer type). We apply and evaluate quality and difficulty filters on both datasets, obtaining valid and challenging QA pairs. We answer the questions with open-source LLMs, revealing existing challenges that correlate with our complexity dimensions. pdfQA presents a basis for end-to-end QA pipeline evaluation, testing diverse skill sets and local optimizations (e.g., in information retrieval or parsing).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PDFs are the second-most used document type on the internet (after HTML). Yet, existing QA datasets commonly start from text sources or only address specific domains. In this paper, we present pdfQA, a multi-domain 2K human-annotated (real-pdfQA) and 2K synthetic dataset (syn-pdfQA) differentiating QA pairs in ten complexity dimensions (e.g., file type, source modality, source position, answer type). We apply and evaluate quality and difficulty filters on both datasets, obtaining valid and challenging QA pairs. We answer the questions with open-source LLMs, revealing existing challenges that correlate with our complexity dimensions. pdfQA presents a basis for end-to-end QA pipeline evaluation, testing diverse skill sets and local optimizations (e.g., in information retrieval or parsing)."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-05T17:15:26Z",
                "published_parsed": [
                    2026,
                    1,
                    5,
                    17,
                    15,
                    26,
                    0,
                    5,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Tobias Schimanski"
                    },
                    {
                        "name": "Imene Kolli"
                    },
                    {
                        "name": "Jingwei Ni"
                    },
                    {
                        "name": "Yu Fan"
                    },
                    {
                        "name": "Ario Saeid Vaghefi"
                    },
                    {
                        "name": "Elliott Ash"
                    },
                    {
                        "name": "Markus Leippold"
                    }
                ],
                "author_detail": {
                    "name": "Markus Leippold"
                },
                "author": "Markus Leippold"
            },
            {
                "id": "http://arxiv.org/abs/2601.02283v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.02283v1",
                "title": "An Automatic Pipeline for the Integration of Python-Based Tools into the Galaxy Platform: Application to the anvi'o Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Automatic Pipeline for the Integration of Python-Based Tools into the Galaxy Platform: Application to the anvi'o Framework"
                },
                "updated": "2026-01-05T17:11:53Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    17,
                    11,
                    53,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.02283v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.02283v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The integration of command-line tools into the Galaxy platform is crucial for making complex computational methods accessible to a broader audience and ensuring reproducible research. However, the manual development of tool wrappers is a time-consuming, error-prone, and knowledge-intensive process. This bottleneck significantly affects the rapid deployment of new and updated tools, creating a gap between tool development and its availability to the scientific community.\n  We have developed a novel, automated approach that directly translates Python tool interfaces into Galaxy-compliant tool wrappers. Our method leverages the argparse library, a standard for command-line argument parsing in Python. By embedding structured metadata within the metavar attribute of input and output arguments, our system programmatically parses the tool's interface to extract all necessary information. This includes parameter types, data formats, help text, and input/output definitions. The system then uses this information to automatically generate a complete and valid Galaxy tool XML wrapper, requiring no manual intervention.\n  To validate the scalability and effectiveness of our approach, we applied it to the anvi'o framework, a comprehensive and complex bioinformatics platform comprising hundreds of individual programs. Our method successfully parsed the argparse definitions for the entire anvi'o suite and generated functional Galaxy tool wrappers. The resulting integration allows for the seamless execution of anvi'o workflows within the Galaxy environment.\n  This work presents a significant advancement in the automation of tool integration for scientific workflow systems. By establishing a convention-based approach using Python's argparse library, we have created a scalable and generalizable solution that dramatically reduces the effort required to make command-line tools available in Galaxy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of command-line tools into the Galaxy platform is crucial for making complex computational methods accessible to a broader audience and ensuring reproducible research. However, the manual development of tool wrappers is a time-consuming, error-prone, and knowledge-intensive process. This bottleneck significantly affects the rapid deployment of new and updated tools, creating a gap between tool development and its availability to the scientific community.\n  We have developed a novel, automated approach that directly translates Python tool interfaces into Galaxy-compliant tool wrappers. Our method leverages the argparse library, a standard for command-line argument parsing in Python. By embedding structured metadata within the metavar attribute of input and output arguments, our system programmatically parses the tool's interface to extract all necessary information. This includes parameter types, data formats, help text, and input/output definitions. The system then uses this information to automatically generate a complete and valid Galaxy tool XML wrapper, requiring no manual intervention.\n  To validate the scalability and effectiveness of our approach, we applied it to the anvi'o framework, a comprehensive and complex bioinformatics platform comprising hundreds of individual programs. Our method successfully parsed the argparse definitions for the entire anvi'o suite and generated functional Galaxy tool wrappers. The resulting integration allows for the seamless execution of anvi'o workflows within the Galaxy environment.\n  This work presents a significant advancement in the automation of tool integration for scientific workflow systems. By establishing a convention-based approach using Python's argparse library, we have created a scalable and generalizable solution that dramatically reduces the effort required to make command-line tools available in Galaxy."
                },
                "tags": [
                    {
                        "term": "cs.SC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-05T17:11:53Z",
                "published_parsed": [
                    2026,
                    1,
                    5,
                    17,
                    11,
                    53,
                    0,
                    5,
                    0
                ],
                "arxiv_comment": "26 pages, 5 figures",
                "arxiv_primary_category": {
                    "term": "cs.SC"
                },
                "authors": [
                    {
                        "name": "Fabio Cumbo"
                    },
                    {
                        "name": "Jayadev Joshi"
                    },
                    {
                        "name": "Daniel Blankenberg"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Blankenberg"
                },
                "author": "Daniel Blankenberg"
            },
            {
                "id": "http://arxiv.org/abs/2601.00469v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.00469v2",
                "title": "DSL or Code? Evaluating the Quality of LLM-Generated Algebraic Specifications: A Case Study in Optimization at Kinaxis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DSL or Code? Evaluating the Quality of LLM-Generated Algebraic Specifications: A Case Study in Optimization at Kinaxis"
                },
                "updated": "2026-01-05T17:09:37Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    17,
                    9,
                    37,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.00469v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.00469v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.1145/3786583.3786879",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "Model-driven engineering (MDE) provides abstraction and analytical rigour, but industrial adoption in many domains has been limited by the cost of developing and maintaining models. Large language models (LLMs) can help shift this cost balance by supporting direct generation of models from natural-language (NL) descriptions. For domain-specific languages (DSLs), however, LLM-generated models may be less accurate than LLM-generated code in mainstream languages such as Python, due to the latter's dominance in LLM training corpora. We investigate this issue in mathematical optimization, with AMPL, a DSL with established industrial use. We introduce EXEOS, an LLM-based approach that derives AMPL models and Python code from NL problem descriptions and iteratively refines them with solver feedback. Using a public optimization dataset and real-world supply-chain cases from our industrial partner Kinaxis, we evaluate generated AMPL models against Python code in terms of executability and correctness. An ablation study with two LLM families shows that AMPL is competitive with, and sometimes better than, Python, and that our design choices in EXEOS improve the quality of generated specifications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model-driven engineering (MDE) provides abstraction and analytical rigour, but industrial adoption in many domains has been limited by the cost of developing and maintaining models. Large language models (LLMs) can help shift this cost balance by supporting direct generation of models from natural-language (NL) descriptions. For domain-specific languages (DSLs), however, LLM-generated models may be less accurate than LLM-generated code in mainstream languages such as Python, due to the latter's dominance in LLM training corpora. We investigate this issue in mathematical optimization, with AMPL, a DSL with established industrial use. We introduce EXEOS, an LLM-based approach that derives AMPL models and Python code from NL problem descriptions and iteratively refines them with solver feedback. Using a public optimization dataset and real-world supply-chain cases from our industrial partner Kinaxis, we evaluate generated AMPL models against Python code in terms of executability and correctness. An ablation study with two LLM families shows that AMPL is competitive with, and sometimes better than, Python, and that our design choices in EXEOS improve the quality of generated specifications."
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-01T20:48:15Z",
                "published_parsed": [
                    2026,
                    1,
                    1,
                    20,
                    48,
                    15,
                    3,
                    1,
                    0
                ],
                "arxiv_comment": "Accepted for publication in ICSE-SEIP 2026",
                "arxiv_primary_category": {
                    "term": "cs.SE"
                },
                "authors": [
                    {
                        "name": "Negin Ayoughi"
                    },
                    {
                        "name": "David Dewar"
                    },
                    {
                        "name": "Shiva Nejati"
                    },
                    {
                        "name": "Mehrdad Sabetzadeh"
                    }
                ],
                "author_detail": {
                    "name": "Mehrdad Sabetzadeh"
                },
                "author": "Mehrdad Sabetzadeh",
                "arxiv_doi": "10.1145/3786583.3786879"
            },
            {
                "id": "http://arxiv.org/abs/2601.02253v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.02253v1",
                "title": "Neuro-Channel Networks: A Multiplication-Free Architecture by Biological Signal Transmission",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neuro-Channel Networks: A Multiplication-Free Architecture by Biological Signal Transmission"
                },
                "updated": "2026-01-05T16:33:13Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    16,
                    33,
                    13,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.02253v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.02253v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The rapid proliferation of Deep Learning is increasingly constrained by its heavy reliance on high-performance hardware, particularly Graphics Processing Units (GPUs). These specialized accelerators are not only prohibitively expensive and energy-intensive but also suffer from significant supply scarcity, limiting the ubiquity of Artificial Intelligence (AI) deployment on edge devices. The core of this inefficiency stems from the standard artificial perceptron's dependence on intensive matrix multiplications. However, biological nervous systems achieve unparalleled efficiency without such arithmetic intensity; synaptic signal transmission is regulated by physical ion channel limits and chemical neurotransmitter levels rather than a process that can be analogous to arithmetic multiplication. Inspired by this biological mechanism, we propose Neuro-Channel Networks (NCN), a novel multiplication-free architecture designed to decouple AI from expensive hardware dependencies. In our model, weights are replaced with Channel Widths that physically limit the signal magnitude, while a secondary parameter acts as a Neurotransmitter to regulate Signal Transmission based on sign logic. The forward pass relies exclusively on addition, subtraction, and bitwise operations (minimum, sign), eliminating floating-point multiplication entirely. In this proof-of-concept study, we demonstrate that NCNs can solve non-linearly separable problems like XOR and the Majority function with 100% accuracy using standard backpropagation, proving their capability to form complex decision boundaries without multiplicative weights. This architecture offers a highly efficient alternative for next-generation neuromorphic hardware, paving the way for running complex models on commodity CPUs or ultra-low-power chips without relying on costly GPU clusters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid proliferation of Deep Learning is increasingly constrained by its heavy reliance on high-performance hardware, particularly Graphics Processing Units (GPUs). These specialized accelerators are not only prohibitively expensive and energy-intensive but also suffer from significant supply scarcity, limiting the ubiquity of Artificial Intelligence (AI) deployment on edge devices. The core of this inefficiency stems from the standard artificial perceptron's dependence on intensive matrix multiplications. However, biological nervous systems achieve unparalleled efficiency without such arithmetic intensity; synaptic signal transmission is regulated by physical ion channel limits and chemical neurotransmitter levels rather than a process that can be analogous to arithmetic multiplication. Inspired by this biological mechanism, we propose Neuro-Channel Networks (NCN), a novel multiplication-free architecture designed to decouple AI from expensive hardware dependencies. In our model, weights are replaced with Channel Widths that physically limit the signal magnitude, while a secondary parameter acts as a Neurotransmitter to regulate Signal Transmission based on sign logic. The forward pass relies exclusively on addition, subtraction, and bitwise operations (minimum, sign), eliminating floating-point multiplication entirely. In this proof-of-concept study, we demonstrate that NCNs can solve non-linearly separable problems like XOR and the Majority function with 100% accuracy using standard backpropagation, proving their capability to form complex decision boundaries without multiplicative weights. This architecture offers a highly efficient alternative for next-generation neuromorphic hardware, paving the way for running complex models on commodity CPUs or ultra-low-power chips without relying on costly GPU clusters."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-05T16:33:13Z",
                "published_parsed": [
                    2026,
                    1,
                    5,
                    16,
                    33,
                    13,
                    0,
                    5,
                    0
                ],
                "arxiv_comment": "9 pages, 4 figures",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Emrah Mete"
                    },
                    {
                        "name": "Emin Erkan Korkmaz"
                    }
                ],
                "author_detail": {
                    "name": "Emin Erkan Korkmaz"
                },
                "author": "Emin Erkan Korkmaz"
            },
            {
                "id": "http://arxiv.org/abs/2411.02570v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2411.02570v3",
                "title": "TI-PREGO: Chain of Thought and In-Context Learning for Online Mistake Detection in PRocedural EGOcentric Videos",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TI-PREGO: Chain of Thought and In-Context Learning for Online Mistake Detection in PRocedural EGOcentric Videos"
                },
                "updated": "2026-01-05T16:20:54Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    16,
                    20,
                    54,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2411.02570v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2411.02570v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.1016/j.cviu.2025.104613",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "Identifying procedural errors online from egocentric videos is a critical yet challenging task across various domains, including manufacturing, healthcare, and skill-based training. The nature of such mistakes is inherently open-set, as unforeseen or novel errors may occur, necessitating robust detection systems that do not rely on prior examples of failure. Currently, however, no technique effectively detects open-set procedural mistakes online.\n  We propose a dual branch architecture to address this problem in an online fashion: one branch continuously performs step recognition from the input egocentric video, while the other anticipates future steps based on the recognition module's output. Mistakes are detected as mismatches between the currently recognized action and the action predicted by the anticipation module. The recognition branch takes input frames, predicts the current action, and aggregates frame-level results into action tokens. The anticipation branch, specifically, leverages the solid pattern-matching capabilities of Large Language Models (LLMs) to predict action tokens based on previously predicted ones.\n  Given the online nature of the task, we also thoroughly benchmark the difficulties associated with per-frame evaluations, particularly the need for accurate and timely predictions in dynamic online scenarios.\n  Extensive experiments on two procedural datasets demonstrate the challenges and opportunities of leveraging a dual-branch architecture for mistake detection, showcasing the effectiveness of our proposed approach. In a thorough evaluation including recognition and anticipation variants and state-of-the-art models, our method reveals its robustness and effectiveness in online applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Identifying procedural errors online from egocentric videos is a critical yet challenging task across various domains, including manufacturing, healthcare, and skill-based training. The nature of such mistakes is inherently open-set, as unforeseen or novel errors may occur, necessitating robust detection systems that do not rely on prior examples of failure. Currently, however, no technique effectively detects open-set procedural mistakes online.\n  We propose a dual branch architecture to address this problem in an online fashion: one branch continuously performs step recognition from the input egocentric video, while the other anticipates future steps based on the recognition module's output. Mistakes are detected as mismatches between the currently recognized action and the action predicted by the anticipation module. The recognition branch takes input frames, predicts the current action, and aggregates frame-level results into action tokens. The anticipation branch, specifically, leverages the solid pattern-matching capabilities of Large Language Models (LLMs) to predict action tokens based on previously predicted ones.\n  Given the online nature of the task, we also thoroughly benchmark the difficulties associated with per-frame evaluations, particularly the need for accurate and timely predictions in dynamic online scenarios.\n  Extensive experiments on two procedural datasets demonstrate the challenges and opportunities of leveraging a dual-branch architecture for mistake detection, showcasing the effectiveness of our proposed approach. In a thorough evaluation including recognition and anticipation variants and state-of-the-art models, our method reveals its robustness and effectiveness in online applications."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2024-11-04T20:03:06Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    20,
                    3,
                    6,
                    0,
                    309,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Leonardo Plini"
                    },
                    {
                        "name": "Luca Scofano"
                    },
                    {
                        "name": "Edoardo De Matteis"
                    },
                    {
                        "name": "Guido Maria D'Amely di Melendugno"
                    },
                    {
                        "name": "Alessandro Flaborea"
                    },
                    {
                        "name": "Andrea Sanchietti"
                    },
                    {
                        "name": "Giovanni Maria Farinella"
                    },
                    {
                        "name": "Fabio Galasso"
                    },
                    {
                        "name": "Antonino Furnari"
                    }
                ],
                "author_detail": {
                    "name": "Antonino Furnari"
                },
                "author": "Antonino Furnari",
                "arxiv_doi": "10.1016/j.cviu.2025.104613"
            },
            {
                "id": "http://arxiv.org/abs/2601.02243v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.02243v1",
                "title": "Optimal Dispatch of Electricity and Water in Renewable-Integrated Desalination Plants",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimal Dispatch of Electricity and Water in Renewable-Integrated Desalination Plants"
                },
                "updated": "2026-01-05T16:19:30Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    16,
                    19,
                    30,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.02243v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.02243v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We develop a mathematical framework for the optimal dispatch of flexible water desalination plants (WDPs) as hybrid generator-load resources. WDPs integrate thermal generation, membrane-based controllable loads, and renewable energy sources, offering unique operational flexibility for power system operations. They can simultaneously participate in two markets: selling desalinated water to a water utility, and bidirectionally transacting electricity with the grid based on their net electricity demand. We formulate the dispatch decision problem of a profit-maximizing WDP, capturing operational, technological, and market-based coupling between water and electricity flows. The threshold-based structure we derive provides computationally tractable coordination suitable for large-scale deployment, offering operational insights into how thermal generation and membrane-based loads complementarily provide continuous bidirectional flexibility. The thresholds are analytically characterized in closed form as explicit functions of technology and tariff parameters. We examine how small changes in the exogenous tariff and technology parameters affect the WDP's profit. Extensive simulations illustrate the optimal WDP's operation, profit, and water-electricity exchange, demonstrating significant improvements relative to benchmark algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We develop a mathematical framework for the optimal dispatch of flexible water desalination plants (WDPs) as hybrid generator-load resources. WDPs integrate thermal generation, membrane-based controllable loads, and renewable energy sources, offering unique operational flexibility for power system operations. They can simultaneously participate in two markets: selling desalinated water to a water utility, and bidirectionally transacting electricity with the grid based on their net electricity demand. We formulate the dispatch decision problem of a profit-maximizing WDP, capturing operational, technological, and market-based coupling between water and electricity flows. The threshold-based structure we derive provides computationally tractable coordination suitable for large-scale deployment, offering operational insights into how thermal generation and membrane-based loads complementarily provide continuous bidirectional flexibility. The thresholds are analytically characterized in closed form as explicit functions of technology and tariff parameters. We examine how small changes in the exogenous tariff and technology parameters affect the WDP's profit. Extensive simulations illustrate the optimal WDP's operation, profit, and water-electricity exchange, demonstrating significant improvements relative to benchmark algorithms."
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-05T16:19:30Z",
                "published_parsed": [
                    2026,
                    1,
                    5,
                    16,
                    19,
                    30,
                    0,
                    5,
                    0
                ],
                "arxiv_comment": "14 pages, 7 figures, 1 table",
                "arxiv_primary_category": {
                    "term": "eess.SY"
                },
                "authors": [
                    {
                        "name": "Ahmed S. Alahmed"
                    },
                    {
                        "name": "Audun Botterud"
                    },
                    {
                        "name": "Saurabh Amin"
                    },
                    {
                        "name": "Ali T. Al-Awami"
                    }
                ],
                "author_detail": {
                    "name": "Ali T. Al-Awami"
                },
                "author": "Ali T. Al-Awami"
            },
            {
                "id": "http://arxiv.org/abs/2601.02242v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.02242v1",
                "title": "VIBE: Visual Instruction Based Editor",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VIBE: Visual Instruction Based Editor"
                },
                "updated": "2026-01-05T16:17:20Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    16,
                    17,
                    20,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.02242v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.02242v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Instruction-based image editing is among the fastest developing areas in generative AI. Over the past year, the field has reached a new level, with dozens of open-source models released alongside highly capable commercial systems. However, only a limited number of open-source approaches currently achieve real-world quality. In addition, diffusion backbones, the dominant choice for these pipelines, are often large and computationally expensive for many deployments and research settings, with widely used variants typically containing 6B to 20B parameters. This paper presents a compact, high-throughput instruction-based image editing pipeline that uses a modern 2B-parameter Qwen3-VL model to guide the editing process and the 1.6B-parameter diffusion model Sana1.5 for image generation. Our design decisions across architecture, data processing, training configuration, and evaluation target low-cost inference and strict source consistency while maintaining high quality across the major edit categories feasible at this scale. Evaluated on the ImgEdit and GEdit benchmarks, the proposed method matches or exceeds the performance of substantially heavier baselines, including models with several times as many parameters and higher inference cost, and is particularly strong on edits that require preserving the input image, such as an attribute adjustment, object removal, background edits, and targeted replacement. The model fits within 24 GB of GPU memory and generates edited images at up to 2K resolution in approximately 4 seconds on an NVIDIA H100 in BF16, without additional inference optimizations or distillation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction-based image editing is among the fastest developing areas in generative AI. Over the past year, the field has reached a new level, with dozens of open-source models released alongside highly capable commercial systems. However, only a limited number of open-source approaches currently achieve real-world quality. In addition, diffusion backbones, the dominant choice for these pipelines, are often large and computationally expensive for many deployments and research settings, with widely used variants typically containing 6B to 20B parameters. This paper presents a compact, high-throughput instruction-based image editing pipeline that uses a modern 2B-parameter Qwen3-VL model to guide the editing process and the 1.6B-parameter diffusion model Sana1.5 for image generation. Our design decisions across architecture, data processing, training configuration, and evaluation target low-cost inference and strict source consistency while maintaining high quality across the major edit categories feasible at this scale. Evaluated on the ImgEdit and GEdit benchmarks, the proposed method matches or exceeds the performance of substantially heavier baselines, including models with several times as many parameters and higher inference cost, and is particularly strong on edits that require preserving the input image, such as an attribute adjustment, object removal, background edits, and targeted replacement. The model fits within 24 GB of GPU memory and generates edited images at up to 2K resolution in approximately 4 seconds on an NVIDIA H100 in BF16, without additional inference optimizations or distillation."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-05T16:17:20Z",
                "published_parsed": [
                    2026,
                    1,
                    5,
                    16,
                    17,
                    20,
                    0,
                    5,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Grigorii Alekseenko"
                    },
                    {
                        "name": "Aleksandr Gordeev"
                    },
                    {
                        "name": "Irina Tolstykh"
                    },
                    {
                        "name": "Bulat Suleimanov"
                    },
                    {
                        "name": "Vladimir Dokholyan"
                    },
                    {
                        "name": "Georgii Fedorov"
                    },
                    {
                        "name": "Sergey Yakubson"
                    },
                    {
                        "name": "Aleksandra Tsybina"
                    },
                    {
                        "name": "Mikhail Chernyshov"
                    },
                    {
                        "name": "Maksim Kuprashevich"
                    }
                ],
                "author_detail": {
                    "name": "Maksim Kuprashevich"
                },
                "author": "Maksim Kuprashevich"
            },
            {
                "id": "http://arxiv.org/abs/2601.02240v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.02240v1",
                "title": "Enabling Deep Reinforcement Learning Research for Energy Saving in Open RAN",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling Deep Reinforcement Learning Research for Energy Saving in Open RAN"
                },
                "updated": "2026-01-05T16:13:48Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    16,
                    13,
                    48,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.02240v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.02240v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.1109/CCNC54725.2025.10975928",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "The growing performance demands and higher deployment densities of next-generation wireless systems emphasize the importance of adopting strategies to manage the energy efficiency of mobile networks. In this demo, we showcase a framework that enables research on Deep Reinforcement Learning (DRL) techniques for improving the energy efficiency of intelligent and programmable Open Radio Access Network (RAN) systems. Using the open-source simulator ns-O-RAN and the reinforcement learning environment Gymnasium, the framework enables to train and evaluate DRL agents that dynamically control the activation and deactivation of cells in a 5G network. We show how to collect data for training and evaluate the impact of DRL on energy efficiency in a realistic 5G network scenario, including users' mobility and handovers, a full protocol stack, and 3rd Generation Partnership Project (3GPP)-compliant channel models. The tool will be open-sourced and a tutorial for energy efficiency testing in ns-O-RAN.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing performance demands and higher deployment densities of next-generation wireless systems emphasize the importance of adopting strategies to manage the energy efficiency of mobile networks. In this demo, we showcase a framework that enables research on Deep Reinforcement Learning (DRL) techniques for improving the energy efficiency of intelligent and programmable Open Radio Access Network (RAN) systems. Using the open-source simulator ns-O-RAN and the reinforcement learning environment Gymnasium, the framework enables to train and evaluate DRL agents that dynamically control the activation and deactivation of cells in a 5G network. We show how to collect data for training and evaluate the impact of DRL on energy efficiency in a realistic 5G network scenario, including users' mobility and handovers, a full protocol stack, and 3rd Generation Partnership Project (3GPP)-compliant channel models. The tool will be open-sourced and a tutorial for energy efficiency testing in ns-O-RAN."
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-05T16:13:48Z",
                "published_parsed": [
                    2026,
                    1,
                    5,
                    16,
                    13,
                    48,
                    0,
                    5,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI"
                },
                "authors": [
                    {
                        "name": "Matteo Bordin"
                    },
                    {
                        "name": "Andrea Lacava"
                    },
                    {
                        "name": "Michele Polese"
                    },
                    {
                        "name": "Francesca Cuomo"
                    },
                    {
                        "name": "Tommaso Melodia"
                    }
                ],
                "author_detail": {
                    "name": "Tommaso Melodia"
                },
                "author": "Tommaso Melodia",
                "arxiv_doi": "10.1109/CCNC54725.2025.10975928"
            },
            {
                "id": "http://arxiv.org/abs/2507.01752v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2507.01752v3",
                "title": "Tuning without Peeking: Provable Generalization Bounds and Robust LLM Post-Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tuning without Peeking: Provable Generalization Bounds and Robust LLM Post-Training"
                },
                "updated": "2026-01-05T16:10:09Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    16,
                    10,
                    9,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2507.01752v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2507.01752v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Gradient-based optimization is the workhorse of deep learning, offering efficient and scalable training via backpropagation. However, exposing gradients during training can leak sensitive information about the underlying data, raising privacy and security concerns such as susceptibility to data poisoning attacks. In contrast, black box optimization methods, which treat the model as an opaque function, relying solely on function evaluations to guide optimization, offer a promising alternative in scenarios where data access is restricted, adversarial risks are high, or overfitting is a concern. This paper introduces BBoxER, an evolutionary black-box method for LLM post-training that induces an information bottleneck via implicit compression of the training data. Leveraging the tractability of information flow, we provide non-vacuous generalization bounds and strong theoretical guarantees for privacy, robustness to data poisoning attacks, and extraction attacks. In experiments with LLMs, we demonstrate empirically that black-box optimization methods, despite the scalability and computational challenges inherent to black-box approaches, are able to learn, showing how a few iterations of BBoxER improve performance, generalize well on a benchmark of reasoning datasets, and are robust to membership inference attacks. This positions BBoxER as an attractive add-on on top of gradient-based optimization, offering suitability for deployment in restricted or privacy-sensitive environments while also providing non-vacuous generalization guarantees.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gradient-based optimization is the workhorse of deep learning, offering efficient and scalable training via backpropagation. However, exposing gradients during training can leak sensitive information about the underlying data, raising privacy and security concerns such as susceptibility to data poisoning attacks. In contrast, black box optimization methods, which treat the model as an opaque function, relying solely on function evaluations to guide optimization, offer a promising alternative in scenarios where data access is restricted, adversarial risks are high, or overfitting is a concern. This paper introduces BBoxER, an evolutionary black-box method for LLM post-training that induces an information bottleneck via implicit compression of the training data. Leveraging the tractability of information flow, we provide non-vacuous generalization bounds and strong theoretical guarantees for privacy, robustness to data poisoning attacks, and extraction attacks. In experiments with LLMs, we demonstrate empirically that black-box optimization methods, despite the scalability and computational challenges inherent to black-box approaches, are able to learn, showing how a few iterations of BBoxER improve performance, generalize well on a benchmark of reasoning datasets, and are robust to membership inference attacks. This positions BBoxER as an attractive add-on on top of gradient-based optimization, offering suitability for deployment in restricted or privacy-sensitive environments while also providing non-vacuous generalization guarantees."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-07-02T14:29:30Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    14,
                    29,
                    30,
                    2,
                    183,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Ismail Labiad"
                    },
                    {
                        "name": "Mathurin Videau"
                    },
                    {
                        "name": "Matthieu Kowalski"
                    },
                    {
                        "name": "Marc Schoenauer"
                    },
                    {
                        "name": "Alessandro Leite"
                    },
                    {
                        "name": "Julia Kempe"
                    },
                    {
                        "name": "Olivier Teytaud"
                    }
                ],
                "author_detail": {
                    "name": "Olivier Teytaud"
                },
                "author": "Olivier Teytaud"
            },
            {
                "id": "http://arxiv.org/abs/2601.02232v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.02232v1",
                "title": "ELLA: Efficient Lifelong Learning for Adapters in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ELLA: Efficient Lifelong Learning for Adapters in Large Language Models"
                },
                "updated": "2026-01-05T15:58:08Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    15,
                    58,
                    8,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.02232v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.02232v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) suffer severe catastrophic forgetting when adapted sequentially to new tasks in a continual learning (CL) setting. Existing approaches are fundamentally limited: replay-based methods are impractical and privacy-violating, while strict orthogonality-based methods collapse under scale: each new task is projected onto an orthogonal complement, progressively reducing the residual degrees of freedom and eliminating forward transfer by forbidding overlap in shared representations. In this work, we introduce ELLA, a training framework built on the principle of selective subspace de-correlation. Rather than forbidding all overlap, ELLA explicitly characterizes the structure of past updates and penalizes alignments along their high-energy, task-specific directions, while preserving freedom in the low-energy residual subspaces to enable transfer. Formally, this is realized via a lightweight regularizer on a single aggregated update matrix. We prove this mechanism corresponds to an anisotropic shrinkage operator that bounds interference, yielding a penalty that is both memory- and compute-constant regardless of task sequence length. ELLA requires no data replay, no architectural expansion, and negligible storage. Empirically, it achieves state-of-the-art CL performance on three popular benchmarks, with relative accuracy gains of up to $9.6\\%$ and a $35\\times$ smaller memory footprint. Further, ELLA scales robustly across architectures and actively enhances the model's zero-shot generalization performance on unseen tasks, establishing a principled and scalable solution for constructive lifelong LLM adaptation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) suffer severe catastrophic forgetting when adapted sequentially to new tasks in a continual learning (CL) setting. Existing approaches are fundamentally limited: replay-based methods are impractical and privacy-violating, while strict orthogonality-based methods collapse under scale: each new task is projected onto an orthogonal complement, progressively reducing the residual degrees of freedom and eliminating forward transfer by forbidding overlap in shared representations. In this work, we introduce ELLA, a training framework built on the principle of selective subspace de-correlation. Rather than forbidding all overlap, ELLA explicitly characterizes the structure of past updates and penalizes alignments along their high-energy, task-specific directions, while preserving freedom in the low-energy residual subspaces to enable transfer. Formally, this is realized via a lightweight regularizer on a single aggregated update matrix. We prove this mechanism corresponds to an anisotropic shrinkage operator that bounds interference, yielding a penalty that is both memory- and compute-constant regardless of task sequence length. ELLA requires no data replay, no architectural expansion, and negligible storage. Empirically, it achieves state-of-the-art CL performance on three popular benchmarks, with relative accuracy gains of up to $9.6\\%$ and a $35\\times$ smaller memory footprint. Further, ELLA scales robustly across architectures and actively enhances the model's zero-shot generalization performance on unseen tasks, establishing a principled and scalable solution for constructive lifelong LLM adaptation."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-05T15:58:08Z",
                "published_parsed": [
                    2026,
                    1,
                    5,
                    15,
                    58,
                    8,
                    0,
                    5,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Shristi Das Biswas"
                    },
                    {
                        "name": "Yue Zhang"
                    },
                    {
                        "name": "Anwesan Pal"
                    },
                    {
                        "name": "Radhika Bhargava"
                    },
                    {
                        "name": "Kaushik Roy"
                    }
                ],
                "author_detail": {
                    "name": "Kaushik Roy"
                },
                "author": "Kaushik Roy"
            },
            {
                "id": "http://arxiv.org/abs/2601.02227v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.02227v1",
                "title": "Ultra-low-power Monostatic Backscatter Platform with Phase-Aware Channel Estimation and System-Level Validation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ultra-low-power Monostatic Backscatter Platform with Phase-Aware Channel Estimation and System-Level Validation"
                },
                "updated": "2026-01-05T15:55:46Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    15,
                    55,
                    46,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.02227v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.02227v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "This paper presents a novel channel-estimation (CE) method that mitigates residual phase drifts in backscatter links and a full hardware and signal-processing pipeline for a single-antenna monostatic system. The platform comprises a semi-passive tag, a software-defined radio (SDR) reader, and a 2x1 planar Yagi-Uda array (7 dBi with higher than 30 dB isolation) operating at 2.4 ~ 2.5 GHz. The developed backscatter fading model accounts for round-trip propagation and temporal correlation, and employs an analytically derived resource-optimal pilot allocation strategy. At the receiver, optimized least square (LS) and linear minimum mean square error (LMMSE) CE with pilot-aided carrier frequency offset (CFO) compensation feed a zero-forcing (ZF) equalizer to suppress ISI. The prototype delivers 500 kbps at 1 m with power of 158 uW (SDR baseband) and 10 uW (RF switch), yielding 320 pJ/bit. OOK and BPSK modulations achieve measured EVMs of 2.97 % and 4.02 %, respectively. Performance is validated by BER measurements and successful reconstruction of a full-color image in an over-the-air experiment. The results demonstrate an ultra-low-power, multimedia-capable backscatter IoT link and provide practical hardware-software co-design guidance for scalable deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a novel channel-estimation (CE) method that mitigates residual phase drifts in backscatter links and a full hardware and signal-processing pipeline for a single-antenna monostatic system. The platform comprises a semi-passive tag, a software-defined radio (SDR) reader, and a 2x1 planar Yagi-Uda array (7 dBi with higher than 30 dB isolation) operating at 2.4 ~ 2.5 GHz. The developed backscatter fading model accounts for round-trip propagation and temporal correlation, and employs an analytically derived resource-optimal pilot allocation strategy. At the receiver, optimized least square (LS) and linear minimum mean square error (LMMSE) CE with pilot-aided carrier frequency offset (CFO) compensation feed a zero-forcing (ZF) equalizer to suppress ISI. The prototype delivers 500 kbps at 1 m with power of 158 uW (SDR baseband) and 10 uW (RF switch), yielding 320 pJ/bit. OOK and BPSK modulations achieve measured EVMs of 2.97 % and 4.02 %, respectively. Performance is validated by BER measurements and successful reconstruction of a full-color image in an over-the-air experiment. The results demonstrate an ultra-low-power, multimedia-capable backscatter IoT link and provide practical hardware-software co-design guidance for scalable deployments."
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-05T15:55:46Z",
                "published_parsed": [
                    2026,
                    1,
                    5,
                    15,
                    55,
                    46,
                    0,
                    5,
                    0
                ],
                "arxiv_comment": "19 pages, 18 figures",
                "arxiv_primary_category": {
                    "term": "eess.SP"
                },
                "authors": [
                    {
                        "name": "Hanyeol Ryu"
                    },
                    {
                        "name": "Sangkil Kim"
                    }
                ],
                "author_detail": {
                    "name": "Sangkil Kim"
                },
                "author": "Sangkil Kim"
            },
            {
                "id": "http://arxiv.org/abs/2601.02225v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.02225v1",
                "title": "Backscatter-Assisted High-Speed Rail Communications in Straight Tunnel Environments: Effects of Tag Number and Phase Control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Backscatter-Assisted High-Speed Rail Communications in Straight Tunnel Environments: Effects of Tag Number and Phase Control"
                },
                "updated": "2026-01-05T15:52:27Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    15,
                    52,
                    27,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.02225v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.02225v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/0.1109/LWC.2025.3646864",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "Backscatter communication is a promising technology to enhance the signal strength received by the receiver in straight tunnel environments. The impact of the number of tags and their phase adjustment on system performance remains a challenging issue though. Therefore, in this paper, we investigate the channel gain of backscatter-assisted communication with multiple tags in straight tunnels. In particular, we derive the probabilities that the backscatter link gain is greater than the direct link under adjustable and random phase assumptions by applying the Gaussian and Gamma approximations to derive tractable expressions. The simulation results show that phaseadjustable tags significantly improve the channel gain of the backscatter links compared to the random phase case. Moreover, the number of tags has an upper threshold for an effective tag deployment pattern. These insights provide valuable guidelines for the efficient design of backscatter communication systems in tunnel environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Backscatter communication is a promising technology to enhance the signal strength received by the receiver in straight tunnel environments. The impact of the number of tags and their phase adjustment on system performance remains a challenging issue though. Therefore, in this paper, we investigate the channel gain of backscatter-assisted communication with multiple tags in straight tunnels. In particular, we derive the probabilities that the backscatter link gain is greater than the direct link under adjustable and random phase assumptions by applying the Gaussian and Gamma approximations to derive tractable expressions. The simulation results show that phaseadjustable tags significantly improve the channel gain of the backscatter links compared to the random phase case. Moreover, the number of tags has an upper threshold for an effective tag deployment pattern. These insights provide valuable guidelines for the efficient design of backscatter communication systems in tunnel environments."
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-05T15:52:27Z",
                "published_parsed": [
                    2026,
                    1,
                    5,
                    15,
                    52,
                    27,
                    0,
                    5,
                    0
                ],
                "arxiv_comment": "6 pages, 5 figures",
                "arxiv_primary_category": {
                    "term": "eess.SP"
                },
                "arxiv_journal_ref": "IEEE Wireless Communications Letters, 2025",
                "authors": [
                    {
                        "name": "Yunping Mu"
                    },
                    {
                        "name": "Gongpu Wang"
                    },
                    {
                        "name": "Ruisi He"
                    },
                    {
                        "name": "Theodoros A. Tsiftsis"
                    },
                    {
                        "name": "Saman Atapattu"
                    },
                    {
                        "name": "Chintha Tellambura"
                    }
                ],
                "author_detail": {
                    "name": "Chintha Tellambura"
                },
                "author": "Chintha Tellambura",
                "arxiv_doi": "0.1109/LWC.2025.3646864"
            },
            {
                "id": "http://arxiv.org/abs/2601.02224v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.02224v1",
                "title": "From XAI to Stories: A Factorial Study of LLM-Generated Explanation Quality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From XAI to Stories: A Factorial Study of LLM-Generated Explanation Quality"
                },
                "updated": "2026-01-05T15:52:20Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    15,
                    52,
                    20,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.02224v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.02224v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Explainable AI (XAI) methods like SHAP and LIME produce numerical feature attributions that remain inaccessible to non expert users. Prior work has shown that Large Language Models (LLMs) can transform these outputs into natural language explanations (NLEs), but it remains unclear which factors contribute to high-quality explanations. We present a systematic factorial study investigating how Forecasting model choice, XAI method, LLM selection, and prompting strategy affect NLE quality. Our design spans four models (XGBoost (XGB), Random Forest (RF), Multilayer Perceptron (MLP), and SARIMAX - comparing black-box Machine-Learning (ML) against classical time-series approaches), three XAI conditions (SHAP, LIME, and a no-XAI baseline), three LLMs (GPT-4o, Llama-3-8B, DeepSeek-R1), and eight prompting strategies. Using G-Eval, an LLM-as-a-judge evaluation method, with dual LLM judges and four evaluation criteria, we evaluate 660 explanations for time-series forecasting. Our results suggest that: (1) XAI provides only small improvements over no-XAI baselines, and only for expert audiences; (2) LLM choice dominates all other factors, with DeepSeek-R1 outperforming GPT-4o and Llama-3; (3) we observe an interpretability paradox: in our setting, SARIMAX yielded lower NLE quality than ML models despite higher prediction accuracy; (4) zero-shot prompting is competitive with self-consistency at 7-times lower cost; and (5) chain-of-thought hurts rather than helps.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explainable AI (XAI) methods like SHAP and LIME produce numerical feature attributions that remain inaccessible to non expert users. Prior work has shown that Large Language Models (LLMs) can transform these outputs into natural language explanations (NLEs), but it remains unclear which factors contribute to high-quality explanations. We present a systematic factorial study investigating how Forecasting model choice, XAI method, LLM selection, and prompting strategy affect NLE quality. Our design spans four models (XGBoost (XGB), Random Forest (RF), Multilayer Perceptron (MLP), and SARIMAX - comparing black-box Machine-Learning (ML) against classical time-series approaches), three XAI conditions (SHAP, LIME, and a no-XAI baseline), three LLMs (GPT-4o, Llama-3-8B, DeepSeek-R1), and eight prompting strategies. Using G-Eval, an LLM-as-a-judge evaluation method, with dual LLM judges and four evaluation criteria, we evaluate 660 explanations for time-series forecasting. Our results suggest that: (1) XAI provides only small improvements over no-XAI baselines, and only for expert audiences; (2) LLM choice dominates all other factors, with DeepSeek-R1 outperforming GPT-4o and Llama-3; (3) we observe an interpretability paradox: in our setting, SARIMAX yielded lower NLE quality than ML models despite higher prediction accuracy; (4) zero-shot prompting is competitive with self-consistency at 7-times lower cost; and (5) chain-of-thought hurts rather than helps."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-05T15:52:20Z",
                "published_parsed": [
                    2026,
                    1,
                    5,
                    15,
                    52,
                    20,
                    0,
                    5,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Fabian Lukassen"
                    },
                    {
                        "name": "Jan Herrmann"
                    },
                    {
                        "name": "Christoph Weisser"
                    },
                    {
                        "name": "Benjamin Saefken"
                    },
                    {
                        "name": "Thomas Kneib"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Kneib"
                },
                "author": "Thomas Kneib"
            },
            {
                "id": "http://arxiv.org/abs/2509.25664v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2509.25664v2",
                "title": "QFrBLiMP: a Quebec-French Benchmark of Linguistic Minimal Pairs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QFrBLiMP: a Quebec-French Benchmark of Linguistic Minimal Pairs"
                },
                "updated": "2026-01-05T15:50:05Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    15,
                    50,
                    5,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2509.25664v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2509.25664v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "In this paper, we introduce the Quebec-French Benchmark of Linguistic Minimal Pairs (QFrBLiMP), a corpus designed to evaluate LLMs' linguistic knowledge of prominent grammatical phenomena in Quebec-French. QFrBLiMP comprises 1,761 minimal pairs annotated with 20 LPs. Specifically, these minimal pairs have been created by manually modifying sentences extracted from an official online resource maintained by a Québec government institution. Each pair is annotated by 12 Quebec-French native speakers, who select the sentence they consider grammatical from the two. These annotations are used to compare the competency of LLMs with that of humans. We evaluate different LLMs on QFrBLiMP and MultiBLiMP-Fr by observing the rate of higher probabilities assigned to the sentences of each minimal pair for each category. We find that while grammatical competence scales with model size, a clear hierarchy of difficulty emerges. All benchmarked models consistently fail on phenomena requiring deep semantic understanding, revealing a critical limitation. Finally, our statistical analysis comparing QFrBLiMP and MultiBLiMP reveals a significant performance degradation for most models on Quebec-French; however, the most capable models remain within the statistical significance interval, demonstrating cross-dialectal robustness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce the Quebec-French Benchmark of Linguistic Minimal Pairs (QFrBLiMP), a corpus designed to evaluate LLMs' linguistic knowledge of prominent grammatical phenomena in Quebec-French. QFrBLiMP comprises 1,761 minimal pairs annotated with 20 LPs. Specifically, these minimal pairs have been created by manually modifying sentences extracted from an official online resource maintained by a Québec government institution. Each pair is annotated by 12 Quebec-French native speakers, who select the sentence they consider grammatical from the two. These annotations are used to compare the competency of LLMs with that of humans. We evaluate different LLMs on QFrBLiMP and MultiBLiMP-Fr by observing the rate of higher probabilities assigned to the sentences of each minimal pair for each category. We find that while grammatical competence scales with model size, a clear hierarchy of difficulty emerges. All benchmarked models consistently fail on phenomena requiring deep semantic understanding, revealing a critical limitation. Finally, our statistical analysis comparing QFrBLiMP and MultiBLiMP reveals a significant performance degradation for most models on Quebec-French; however, the most capable models remain within the statistical significance interval, demonstrating cross-dialectal robustness."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-09-30T02:00:26Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    2,
                    0,
                    26,
                    1,
                    273,
                    0
                ],
                "arxiv_comment": "Acceptged to EACL 2026",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "David Beauchemin"
                    },
                    {
                        "name": "Pier-Luc Veilleux"
                    },
                    {
                        "name": "Johanna-Pascale Roy"
                    },
                    {
                        "name": "Richard Khoury"
                    }
                ],
                "author_detail": {
                    "name": "Richard Khoury"
                },
                "author": "Richard Khoury"
            },
            {
                "id": "http://arxiv.org/abs/2512.15231v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.15231v2",
                "title": "CangLing-KnowFlow: A Unified Knowledge-and-Flow-fused Agent for Comprehensive Remote Sensing Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CangLing-KnowFlow: A Unified Knowledge-and-Flow-fused Agent for Comprehensive Remote Sensing Applications"
                },
                "updated": "2026-01-05T15:48:10Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    15,
                    48,
                    10,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.15231v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.15231v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The automated and intelligent processing of massive remote sensing (RS) datasets is critical in Earth observation (EO). Existing automated systems are normally task-specific, lacking a unified framework to manage diverse, end-to-end workflows--from data preprocessing to advanced interpretation--across diverse RS applications. To address this gap, this paper introduces CangLing-KnowFlow, a unified intelligent agent framework that integrates a Procedural Knowledge Base (PKB), Dynamic Workflow Adjustment, and an Evolutionary Memory Module. The PKB, comprising 1,008 expert-validated workflow cases across 162 practical RS tasks, guides planning and substantially reduces hallucinations common in general-purpose agents. During runtime failures, the Dynamic Workflow Adjustment autonomously diagnoses and replans recovery strategies, while the Evolutionary Memory Module continuously learns from these events, iteratively enhancing the agent's knowledge and performance. This synergy enables CangLing-KnowFlow to adapt, learn, and operate reliably across diverse, complex tasks. We evaluated CangLing-KnowFlow on the KnowFlow-Bench, a novel benchmark of 324 workflows inspired by real-world applications, testing its performance across 13 top Large Language Model (LLM) backbones, from open-source to commercial. Across all complex tasks, CangLing-KnowFlow surpassed the Reflexion baseline by at least 4% in Task Success Rate. As the first most comprehensive validation along this emerging field, this research demonstrates the great potential of CangLing-KnowFlow as a robust, efficient, and scalable automated solution for complex EO challenges by leveraging expert knowledge (Knowledge) into adaptive and verifiable procedures (Flow).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The automated and intelligent processing of massive remote sensing (RS) datasets is critical in Earth observation (EO). Existing automated systems are normally task-specific, lacking a unified framework to manage diverse, end-to-end workflows--from data preprocessing to advanced interpretation--across diverse RS applications. To address this gap, this paper introduces CangLing-KnowFlow, a unified intelligent agent framework that integrates a Procedural Knowledge Base (PKB), Dynamic Workflow Adjustment, and an Evolutionary Memory Module. The PKB, comprising 1,008 expert-validated workflow cases across 162 practical RS tasks, guides planning and substantially reduces hallucinations common in general-purpose agents. During runtime failures, the Dynamic Workflow Adjustment autonomously diagnoses and replans recovery strategies, while the Evolutionary Memory Module continuously learns from these events, iteratively enhancing the agent's knowledge and performance. This synergy enables CangLing-KnowFlow to adapt, learn, and operate reliably across diverse, complex tasks. We evaluated CangLing-KnowFlow on the KnowFlow-Bench, a novel benchmark of 324 workflows inspired by real-world applications, testing its performance across 13 top Large Language Model (LLM) backbones, from open-source to commercial. Across all complex tasks, CangLing-KnowFlow surpassed the Reflexion baseline by at least 4% in Task Success Rate. As the first most comprehensive validation along this emerging field, this research demonstrates the great potential of CangLing-KnowFlow as a robust, efficient, and scalable automated solution for complex EO challenges by leveraging expert knowledge (Knowledge) into adaptive and verifiable procedures (Flow)."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-17T09:31:57Z",
                "published_parsed": [
                    2025,
                    12,
                    17,
                    9,
                    31,
                    57,
                    2,
                    351,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Zhengchao Chen"
                    },
                    {
                        "name": "Haoran Wang"
                    },
                    {
                        "name": "Jing Yao"
                    },
                    {
                        "name": "Pedram Ghamisi"
                    },
                    {
                        "name": "Jun Zhou"
                    },
                    {
                        "name": "Peter M. Atkinson"
                    },
                    {
                        "name": "Bing Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Bing Zhang"
                },
                "author": "Bing Zhang"
            },
            {
                "id": "http://arxiv.org/abs/2601.02219v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.02219v1",
                "title": "Beam-Brainstorm: A Generative Site-Specific Beamforming Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beam-Brainstorm: A Generative Site-Specific Beamforming Approach"
                },
                "updated": "2026-01-05T15:46:18Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    15,
                    46,
                    18,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.02219v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.02219v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Accurately understanding the propagation environment is a fundamental challenge in site-specific beamforming (SSBF). This paper proposes a novel generative SSBF (GenSSBF) solution, which represents a paradigm shift from conventional unstructured prediction to joint-structure modeling. First, considering the fundamental differences between beam generation and conventional image synthesis, a unified GenSSBF framework is proposed, which includes a site profile, a wireless prompting module, and a generator. Second, a beam-brainstorm (BBS) solution is proposed as an instantiation of this GenSSBF framework. Specifically, the site profile is configured by transforming channel data from spatial domain to a reversible latent space via discrete Fourier transform (DFT). To facilitate practical deployment, the wireless prompt is constructed from the reference signal received power (RSRP) measured using a small number of DFT-beams. Finally, the generator is developed using a customized conditional diffusion model. Rather than relying on a meticulously designed global codebook, BBS directly generates diverse and high-fidelity user-specific beams guided by the wireless prompts. Simulation results on accurate ray-tracing datasets demonstrate that BBS can achieve near-optimal beamforming gain while drastically reducing the beam sweeping overhead, even in low signal-to-noise ratio (SNR) environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurately understanding the propagation environment is a fundamental challenge in site-specific beamforming (SSBF). This paper proposes a novel generative SSBF (GenSSBF) solution, which represents a paradigm shift from conventional unstructured prediction to joint-structure modeling. First, considering the fundamental differences between beam generation and conventional image synthesis, a unified GenSSBF framework is proposed, which includes a site profile, a wireless prompting module, and a generator. Second, a beam-brainstorm (BBS) solution is proposed as an instantiation of this GenSSBF framework. Specifically, the site profile is configured by transforming channel data from spatial domain to a reversible latent space via discrete Fourier transform (DFT). To facilitate practical deployment, the wireless prompt is constructed from the reference signal received power (RSRP) measured using a small number of DFT-beams. Finally, the generator is developed using a customized conditional diffusion model. Rather than relying on a meticulously designed global codebook, BBS directly generates diverse and high-fidelity user-specific beams guided by the wireless prompts. Simulation results on accurate ray-tracing datasets demonstrate that BBS can achieve near-optimal beamforming gain while drastically reducing the beam sweeping overhead, even in low signal-to-noise ratio (SNR) environments."
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-05T15:46:18Z",
                "published_parsed": [
                    2026,
                    1,
                    5,
                    15,
                    46,
                    18,
                    0,
                    5,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP"
                },
                "authors": [
                    {
                        "name": "Zihao Zhou"
                    },
                    {
                        "name": "Zhaolin Wang"
                    },
                    {
                        "name": "Yuanwei Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yuanwei Liu"
                },
                "author": "Yuanwei Liu"
            },
            {
                "id": "http://arxiv.org/abs/2512.20182v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20182v2",
                "title": "FaithLens: Detecting and Explaining Faithfulness Hallucination",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FaithLens: Detecting and Explaining Faithfulness Hallucination"
                },
                "updated": "2026-01-05T15:43:00Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    15,
                    43,
                    0,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20182v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20182v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Recognizing whether outputs from large language models (LLMs) contain faithfulness hallucination is crucial for real-world applications, e.g., retrieval-augmented generation and summarization. In this paper, we introduce FaithLens, a cost-efficient and effective faithfulness hallucination detection model that can jointly provide binary predictions and corresponding explanations to improve trustworthiness. To achieve this, we first synthesize training data with explanations via advanced LLMs and apply a well-defined data filtering strategy to ensure label correctness, explanation quality, and data diversity. Subsequently, we fine-tune the model on these well-curated training data as a cold start and further optimize it with rule-based reinforcement learning, using rewards for both prediction correctness and explanation quality. Results on 12 diverse tasks show that the 8B-parameter FaithLens outperforms advanced models such as GPT-4.1 and o3. Also, FaithLens can produce high-quality explanations, delivering a distinctive balance of trustworthiness, efficiency, and effectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recognizing whether outputs from large language models (LLMs) contain faithfulness hallucination is crucial for real-world applications, e.g., retrieval-augmented generation and summarization. In this paper, we introduce FaithLens, a cost-efficient and effective faithfulness hallucination detection model that can jointly provide binary predictions and corresponding explanations to improve trustworthiness. To achieve this, we first synthesize training data with explanations via advanced LLMs and apply a well-defined data filtering strategy to ensure label correctness, explanation quality, and data diversity. Subsequently, we fine-tune the model on these well-curated training data as a cold start and further optimize it with rule-based reinforcement learning, using rewards for both prediction correctness and explanation quality. Results on 12 diverse tasks show that the 8B-parameter FaithLens outperforms advanced models such as GPT-4.1 and o3. Also, FaithLens can produce high-quality explanations, delivering a distinctive balance of trustworthiness, efficiency, and effectiveness."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T09:20:32Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    9,
                    20,
                    32,
                    1,
                    357,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Shuzheng Si"
                    },
                    {
                        "name": "Qingyi Wang"
                    },
                    {
                        "name": "Haozhe Zhao"
                    },
                    {
                        "name": "Yuzhuo Bai"
                    },
                    {
                        "name": "Guanqiao Chen"
                    },
                    {
                        "name": "Kangyang Luo"
                    },
                    {
                        "name": "Gang Chen"
                    },
                    {
                        "name": "Fanchao Qi"
                    },
                    {
                        "name": "Minjia Zhang"
                    },
                    {
                        "name": "Baobao Chang"
                    },
                    {
                        "name": "Maosong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Maosong Sun"
                },
                "author": "Maosong Sun"
            },
            {
                "id": "http://arxiv.org/abs/2601.02215v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.02215v1",
                "title": "LLM-Empowered Functional Safety and Security by Design in Automotive Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Empowered Functional Safety and Security by Design in Automotive Systems"
                },
                "updated": "2026-01-05T15:37:08Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    15,
                    37,
                    8,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.02215v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.02215v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "This paper presents LLM-empowered workflow to support Software Defined Vehicle (SDV) software development, covering the aspects of security-aware system topology design, as well as event-driven decision-making code analysis. For code analysis we adopt event chains model which provides formal foundations to systematic validation of functional safety, taking into account the semantic validity of messages exchanged between key components, including both CAN and Vehicle Signal Specification (VSS). Analysis of security aspects for topology relies on synergy with Model-Driven Engineering (MDE) approach and Object Constraint Language (OCL) rules. Both locally deployable and proprietary solution are taken into account for evaluation within Advanced Driver-Assistance Systems (ADAS)-related scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents LLM-empowered workflow to support Software Defined Vehicle (SDV) software development, covering the aspects of security-aware system topology design, as well as event-driven decision-making code analysis. For code analysis we adopt event chains model which provides formal foundations to systematic validation of functional safety, taking into account the semantic validity of messages exchanged between key components, including both CAN and Vehicle Signal Specification (VSS). Analysis of security aspects for topology relies on synergy with Model-Driven Engineering (MDE) approach and Object Constraint Language (OCL) rules. Both locally deployable and proprietary solution are taken into account for evaluation within Advanced Driver-Assistance Systems (ADAS)-related scenarios."
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-05T15:37:08Z",
                "published_parsed": [
                    2026,
                    1,
                    5,
                    15,
                    37,
                    8,
                    0,
                    5,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE"
                },
                "authors": [
                    {
                        "name": "Nenad Petrovic"
                    },
                    {
                        "name": "Vahid Zolfaghari"
                    },
                    {
                        "name": "Fengjunjie Pan"
                    },
                    {
                        "name": "Alois Knoll"
                    }
                ],
                "author_detail": {
                    "name": "Alois Knoll"
                },
                "author": "Alois Knoll"
            },
            {
                "id": "http://arxiv.org/abs/2601.02213v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.02213v1",
                "title": "Quantized SO(3)-Equivariant Graph Neural Networks for Efficient Molecular Property Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantized SO(3)-Equivariant Graph Neural Networks for Efficient Molecular Property Prediction"
                },
                "updated": "2026-01-05T15:36:04Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    15,
                    36,
                    4,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.02213v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.02213v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Deploying 3D graph neural networks (GNNs) that are equivariant to 3D rotations (the group SO(3)) on edge devices is challenging due to their high computational cost. This paper addresses the problem by compressing and accelerating an SO(3)-equivariant GNN using low-bit quantization techniques. Specifically, we introduce three innovations for quantized equivariant transformers: (1) a magnitude-direction decoupled quantization scheme that separately quantizes the norm and orientation of equivariant (vector) features, (2) a branch-separated quantization-aware training strategy that treats invariant and equivariant feature channels differently in an attention-based $SO(3)$-GNN, and (3) a robustness-enhancing attention normalization mechanism that stabilizes low-precision attention computations. Experiments on the QM9 and rMD17 molecular benchmarks demonstrate that our 8-bit models achieve accuracy on energy and force predictions comparable to full-precision baselines with markedly improved efficiency. We also conduct ablation studies to quantify the contribution of each component to maintain accuracy and equivariance under quantization, using the Local error of equivariance (LEE) metric. The proposed techniques enable the deployment of symmetry-aware GNNs in practical chemistry applications with 2.37--2.73x faster inference and 4x smaller model size, without sacrificing accuracy or physical symmetry.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying 3D graph neural networks (GNNs) that are equivariant to 3D rotations (the group SO(3)) on edge devices is challenging due to their high computational cost. This paper addresses the problem by compressing and accelerating an SO(3)-equivariant GNN using low-bit quantization techniques. Specifically, we introduce three innovations for quantized equivariant transformers: (1) a magnitude-direction decoupled quantization scheme that separately quantizes the norm and orientation of equivariant (vector) features, (2) a branch-separated quantization-aware training strategy that treats invariant and equivariant feature channels differently in an attention-based $SO(3)$-GNN, and (3) a robustness-enhancing attention normalization mechanism that stabilizes low-precision attention computations. Experiments on the QM9 and rMD17 molecular benchmarks demonstrate that our 8-bit models achieve accuracy on energy and force predictions comparable to full-precision baselines with markedly improved efficiency. We also conduct ablation studies to quantify the contribution of each component to maintain accuracy and equivariance under quantization, using the Local error of equivariance (LEE) metric. The proposed techniques enable the deployment of symmetry-aware GNNs in practical chemistry applications with 2.37--2.73x faster inference and 4x smaller model size, without sacrificing accuracy or physical symmetry."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-05T15:36:04Z",
                "published_parsed": [
                    2026,
                    1,
                    5,
                    15,
                    36,
                    4,
                    0,
                    5,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Haoyu Zhou"
                    },
                    {
                        "name": "Ping Xue"
                    },
                    {
                        "name": "Tianfan Fu"
                    },
                    {
                        "name": "Hao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Zhang"
                },
                "author": "Hao Zhang"
            },
            {
                "id": "http://arxiv.org/abs/2601.02203v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.02203v1",
                "title": "Parameter-Efficient Domain Adaption for CSI Crowd-Counting via Self-Supervised Learning with Adapter Modules",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parameter-Efficient Domain Adaption for CSI Crowd-Counting via Self-Supervised Learning with Adapter Modules"
                },
                "updated": "2026-01-05T15:27:04Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    15,
                    27,
                    4,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.02203v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.02203v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Device-free crowd-counting using WiFi Channel State Information (CSI) is a key enabling technology for a new generation of privacy-preserving Internet of Things (IoT) applications. However, practical deployment is severely hampered by the domain shift problem, where models trained in one environment fail to generalise to another. To overcome this, we propose a novel two-stage framework centred on a CSI-ResNet-A architecture. This model is pre-trained via self-supervised contrastive learning to learn domain-invariant representations and leverages lightweight Adapter modules for highly efficient fine-tuning. The resulting event sequence is then processed by a stateful counting machine to produce a final, stable occupancy estimate. We validate our framework extensively. On our WiFlow dataset, our unsupervised approach excels in a 10-shot learning scenario, achieving a final Mean Absolute Error (MAE) of just 0.44--a task where supervised baselines fail. To formally quantify robustness, we introduce the Generalisation Index (GI), on which our model scores near-perfectly, confirming its ability to generalise. Furthermore, our framework sets a new state-of-the-art public WiAR benchmark with 98.8\\% accuracy. Our ablation studies reveal the core strength of our design: adapter-based fine-tuning achieves performance within 1\\% of a full fine-tune (98.84\\% vs. 99.67\\%) while training 97.2\\% fewer parameters. Our work provides a practical and scalable solution for developing robust sensing systems ready for real-world IoT deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Device-free crowd-counting using WiFi Channel State Information (CSI) is a key enabling technology for a new generation of privacy-preserving Internet of Things (IoT) applications. However, practical deployment is severely hampered by the domain shift problem, where models trained in one environment fail to generalise to another. To overcome this, we propose a novel two-stage framework centred on a CSI-ResNet-A architecture. This model is pre-trained via self-supervised contrastive learning to learn domain-invariant representations and leverages lightweight Adapter modules for highly efficient fine-tuning. The resulting event sequence is then processed by a stateful counting machine to produce a final, stable occupancy estimate. We validate our framework extensively. On our WiFlow dataset, our unsupervised approach excels in a 10-shot learning scenario, achieving a final Mean Absolute Error (MAE) of just 0.44--a task where supervised baselines fail. To formally quantify robustness, we introduce the Generalisation Index (GI), on which our model scores near-perfectly, confirming its ability to generalise. Furthermore, our framework sets a new state-of-the-art public WiAR benchmark with 98.8\\% accuracy. Our ablation studies reveal the core strength of our design: adapter-based fine-tuning achieves performance within 1\\% of a full fine-tune (98.84\\% vs. 99.67\\%) while training 97.2\\% fewer parameters. Our work provides a practical and scalable solution for developing robust sensing systems ready for real-world IoT deployments."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-05T15:27:04Z",
                "published_parsed": [
                    2026,
                    1,
                    5,
                    15,
                    27,
                    4,
                    0,
                    5,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Oliver Custance"
                    },
                    {
                        "name": "Saad Khan"
                    },
                    {
                        "name": "Simon Parkinson"
                    },
                    {
                        "name": "Quan Z. Sheng"
                    }
                ],
                "author_detail": {
                    "name": "Quan Z. Sheng"
                },
                "author": "Quan Z. Sheng"
            },
            {
                "id": "http://arxiv.org/abs/2511.08873v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.08873v2",
                "title": "UCO: A Multi-Turn Interactive Reinforcement Learning Method for Adaptive Teaching with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UCO: A Multi-Turn Interactive Reinforcement Learning Method for Adaptive Teaching with Large Language Models"
                },
                "updated": "2026-01-05T15:26:03Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    15,
                    26,
                    3,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.08873v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.08873v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models (LLMs) are shifting from answer providers to intelligent tutors in educational settings, yet current supervised fine-tuning methods only learn surface teaching patterns without dynamic adaptation capabilities. Recent reinforcement learning approaches address this limitation but face two critical challenges. First, they evaluate teaching effectiveness solely based on whether students produce correct outputs, unable to distinguish whether students genuinely understand or echo teacher-provided answers during interaction. Second, they cannot perceive students' evolving cognitive states in real time through interactive dialogue, thus failing to adapt teaching strategies to match students' cognitive levels dynamically. We propose the Unidirectional Cognitive Optimization (UCO) method to address these challenges. UCO uses a multi-turn interactive reinforcement learning paradigm where the innovation lies in two synergistic reward functions: the Progress Reward captures students' cognitive advancement, evaluating whether students truly transition from confusion to comprehension, while the Scaffold Reward dynamically identifies each student's Zone of Proximal Development (ZPD), encouraging teachers to maintain productive teaching within this zone. We evaluate UCO by comparing it against 11 baseline models on BigMath and MathTutorBench benchmarks. Experimental results demonstrate that our UCO model outperforms all models of equivalent scale and achieves performance comparable to advanced closed-source models. The code and data are available at https://github.com/Mind-Lab-ECNU/UCO.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are shifting from answer providers to intelligent tutors in educational settings, yet current supervised fine-tuning methods only learn surface teaching patterns without dynamic adaptation capabilities. Recent reinforcement learning approaches address this limitation but face two critical challenges. First, they evaluate teaching effectiveness solely based on whether students produce correct outputs, unable to distinguish whether students genuinely understand or echo teacher-provided answers during interaction. Second, they cannot perceive students' evolving cognitive states in real time through interactive dialogue, thus failing to adapt teaching strategies to match students' cognitive levels dynamically. We propose the Unidirectional Cognitive Optimization (UCO) method to address these challenges. UCO uses a multi-turn interactive reinforcement learning paradigm where the innovation lies in two synergistic reward functions: the Progress Reward captures students' cognitive advancement, evaluating whether students truly transition from confusion to comprehension, while the Scaffold Reward dynamically identifies each student's Zone of Proximal Development (ZPD), encouraging teachers to maintain productive teaching within this zone. We evaluate UCO by comparing it against 11 baseline models on BigMath and MathTutorBench benchmarks. Experimental results demonstrate that our UCO model outperforms all models of equivalent scale and achieves performance comparable to advanced closed-source models. The code and data are available at https://github.com/Mind-Lab-ECNU/UCO."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-12T01:27:02Z",
                "published_parsed": [
                    2025,
                    11,
                    12,
                    1,
                    27,
                    2,
                    2,
                    316,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Shouang Wei"
                    },
                    {
                        "name": "Min Zhang"
                    },
                    {
                        "name": "Xin Lin"
                    },
                    {
                        "name": "Bo Jiang"
                    },
                    {
                        "name": "Kun Kuang"
                    },
                    {
                        "name": "Zhongxiang Dai"
                    }
                ],
                "author_detail": {
                    "name": "Zhongxiang Dai"
                },
                "author": "Zhongxiang Dai"
            },
            {
                "id": "http://arxiv.org/abs/2601.02200v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.02200v1",
                "title": "Code for Machines, Not Just Humans: Quantifying AI-Friendliness with Code Health Metrics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code for Machines, Not Just Humans: Quantifying AI-Friendliness with Code Health Metrics"
                },
                "updated": "2026-01-05T15:23:55Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    15,
                    23,
                    55,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.02200v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.02200v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We are entering a hybrid era in which human developers and AI coding agents work in the same codebases. While industry practice has long optimized code for human comprehension, it is increasingly important to ensure that LLMs with different capabilities can edit code reliably. In this study, we investigate the concept of ``AI-friendly code'' via LLM-based refactoring on a dataset of 5,000 Python files from competitive programming. We find a meaningful association between CodeHealth, a quality metric calibrated for human comprehension, and semantic preservation after AI refactoring. Our findings confirm that human-friendly code is also more compatible with AI tooling. These results suggest that organizations can use CodeHealth to guide where AI interventions are lower risk and where additional human oversight is warranted. Investing in maintainability not only helps humans; it also prepares for large-scale AI adoption.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We are entering a hybrid era in which human developers and AI coding agents work in the same codebases. While industry practice has long optimized code for human comprehension, it is increasingly important to ensure that LLMs with different capabilities can edit code reliably. In this study, we investigate the concept of ``AI-friendly code'' via LLM-based refactoring on a dataset of 5,000 Python files from competitive programming. We find a meaningful association between CodeHealth, a quality metric calibrated for human comprehension, and semantic preservation after AI refactoring. Our findings confirm that human-friendly code is also more compatible with AI tooling. These results suggest that organizations can use CodeHealth to guide where AI interventions are lower risk and where additional human oversight is warranted. Investing in maintainability not only helps humans; it also prepares for large-scale AI adoption."
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-05T15:23:55Z",
                "published_parsed": [
                    2026,
                    1,
                    5,
                    15,
                    23,
                    55,
                    0,
                    5,
                    0
                ],
                "arxiv_comment": "Accepted for the 3rd ACM International Conference on AI Foundation Models and Software Engineering (FORGE 2026)",
                "arxiv_primary_category": {
                    "term": "cs.SE"
                },
                "authors": [
                    {
                        "name": "Markus Borg"
                    },
                    {
                        "name": "Nadim Hagatulah"
                    },
                    {
                        "name": "Adam Tornhill"
                    },
                    {
                        "name": "Emma Söderberg"
                    }
                ],
                "author_detail": {
                    "name": "Emma Söderberg"
                },
                "author": "Emma Söderberg"
            },
            {
                "id": "http://arxiv.org/abs/2503.22458v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2503.22458v2",
                "title": "Evaluating LLM-based Agents for Multi-Turn Conversations: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating LLM-based Agents for Multi-Turn Conversations: A Survey"
                },
                "updated": "2026-01-05T15:14:04Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    15,
                    14,
                    4,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2503.22458v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2503.22458v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "This survey examines evaluation methods for large language model (LLM)-based agents in multi-turn conversational settings. Using a PRISMA-inspired framework, we systematically reviewed nearly 250 scholarly sources, capturing the state of the art from various venues of publication, and establishing a solid foundation for our analysis. Our study offers a structured approach by developing two interrelated taxonomy systems: one that defines \\emph{what to evaluate} and another that explains \\emph{how to evaluate}. The first taxonomy identifies key components of LLM-based agents for multi-turn conversations and their evaluation dimensions, including task completion, response quality, user experience, memory and context retention, as well as planning and tool integration. These components ensure that the performance of conversational agents is assessed in a holistic and meaningful manner. The second taxonomy system focuses on the evaluation methodologies. It categorizes approaches into annotation-based evaluations, automated metrics, hybrid strategies that combine human assessments with quantitative measures, and self-judging methods utilizing LLMs. This framework not only captures traditional metrics derived from language understanding, such as BLEU and ROUGE scores, but also incorporates advanced techniques that reflect the dynamic, interactive nature of multi-turn dialogues.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This survey examines evaluation methods for large language model (LLM)-based agents in multi-turn conversational settings. Using a PRISMA-inspired framework, we systematically reviewed nearly 250 scholarly sources, capturing the state of the art from various venues of publication, and establishing a solid foundation for our analysis. Our study offers a structured approach by developing two interrelated taxonomy systems: one that defines \\emph{what to evaluate} and another that explains \\emph{how to evaluate}. The first taxonomy identifies key components of LLM-based agents for multi-turn conversations and their evaluation dimensions, including task completion, response quality, user experience, memory and context retention, as well as planning and tool integration. These components ensure that the performance of conversational agents is assessed in a holistic and meaningful manner. The second taxonomy system focuses on the evaluation methodologies. It categorizes approaches into annotation-based evaluations, automated metrics, hybrid strategies that combine human assessments with quantitative measures, and self-judging methods utilizing LLMs. This framework not only captures traditional metrics derived from language understanding, such as BLEU and ROUGE scores, but also incorporates advanced techniques that reflect the dynamic, interactive nature of multi-turn dialogues."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-03-28T14:08:40Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    14,
                    8,
                    40,
                    4,
                    87,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Shengyue Guan"
                    },
                    {
                        "name": "Jindong Wang"
                    },
                    {
                        "name": "Jiang Bian"
                    },
                    {
                        "name": "Bin Zhu"
                    },
                    {
                        "name": "Jian-guang Lou"
                    },
                    {
                        "name": "Haoyi Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Haoyi Xiong"
                },
                "author": "Haoyi Xiong"
            },
            {
                "id": "http://arxiv.org/abs/2601.02186v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.02186v1",
                "title": "Toward Global Large Language Models in Medicine",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toward Global Large Language Models in Medicine"
                },
                "updated": "2026-01-05T15:05:49Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    15,
                    5,
                    49,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.02186v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.02186v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Despite continuous advances in medical technology, the global distribution of health care resources remains uneven. The development of large language models (LLMs) has transformed the landscape of medicine and holds promise for improving health care quality and expanding access to medical information globally. However, existing LLMs are primarily trained on high-resource languages, limiting their applicability in global medical scenarios. To address this gap, we constructed GlobMed, a large multilingual medical dataset, containing over 500,000 entries spanning 12 languages, including four low-resource languages. Building on this, we established GlobMed-Bench, which systematically assesses 56 state-of-the-art proprietary and open-weight LLMs across multiple multilingual medical tasks, revealing significant performance disparities across languages, particularly for low-resource languages. Additionally, we introduced GlobMed-LLMs, a suite of multilingual medical LLMs trained on GlobMed, with parameters ranging from 1.7B to 8B. GlobMed-LLMs achieved an average performance improvement of over 40% relative to baseline models, with a more than threefold increase in performance on low-resource languages. Together, these resources provide an important foundation for advancing the equitable development and application of LLMs globally, enabling broader language communities to benefit from technological advances.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite continuous advances in medical technology, the global distribution of health care resources remains uneven. The development of large language models (LLMs) has transformed the landscape of medicine and holds promise for improving health care quality and expanding access to medical information globally. However, existing LLMs are primarily trained on high-resource languages, limiting their applicability in global medical scenarios. To address this gap, we constructed GlobMed, a large multilingual medical dataset, containing over 500,000 entries spanning 12 languages, including four low-resource languages. Building on this, we established GlobMed-Bench, which systematically assesses 56 state-of-the-art proprietary and open-weight LLMs across multiple multilingual medical tasks, revealing significant performance disparities across languages, particularly for low-resource languages. Additionally, we introduced GlobMed-LLMs, a suite of multilingual medical LLMs trained on GlobMed, with parameters ranging from 1.7B to 8B. GlobMed-LLMs achieved an average performance improvement of over 40% relative to baseline models, with a more than threefold increase in performance on low-resource languages. Together, these resources provide an important foundation for advancing the equitable development and application of LLMs globally, enabling broader language communities to benefit from technological advances."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-05T15:05:49Z",
                "published_parsed": [
                    2026,
                    1,
                    5,
                    15,
                    5,
                    49,
                    0,
                    5,
                    0
                ],
                "arxiv_comment": "182 pages, 65 figures",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Rui Yang"
                    },
                    {
                        "name": "Huitao Li"
                    },
                    {
                        "name": "Weihao Xuan"
                    },
                    {
                        "name": "Heli Qi"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Kunyu Yu"
                    },
                    {
                        "name": "Yingjian Chen"
                    },
                    {
                        "name": "Rongrong Wang"
                    },
                    {
                        "name": "Jacques Behmoaras"
                    },
                    {
                        "name": "Tianxi Cai"
                    },
                    {
                        "name": "Bibhas Chakraborty"
                    },
                    {
                        "name": "Qingyu Chen"
                    },
                    {
                        "name": "Lionel Tim-Ee Cheng"
                    },
                    {
                        "name": "Marie-Louise Damwanza"
                    },
                    {
                        "name": "Chido Dzinotyiwei"
                    },
                    {
                        "name": "Aosong Feng"
                    },
                    {
                        "name": "Chuan Hong"
                    },
                    {
                        "name": "Yusuke Iwasawa"
                    },
                    {
                        "name": "Yuhe Ke"
                    },
                    {
                        "name": "Linah Kitala"
                    },
                    {
                        "name": "Taehoon Ko"
                    },
                    {
                        "name": "Jisan Lee"
                    },
                    {
                        "name": "Irene Li"
                    },
                    {
                        "name": "Jonathan Chong Kai Liew"
                    },
                    {
                        "name": "Hongfang Liu"
                    },
                    {
                        "name": "Lian Leng Low"
                    },
                    {
                        "name": "Edison Marrese-Taylor"
                    },
                    {
                        "name": "Yutaka Matsuo"
                    },
                    {
                        "name": "Isheanesu Misi"
                    },
                    {
                        "name": "Yilin Ning"
                    },
                    {
                        "name": "Jasmine Chiat Ling Ong"
                    },
                    {
                        "name": "Marcus Eng Hock Ong"
                    },
                    {
                        "name": "Enrico Petretto"
                    },
                    {
                        "name": "Hossein Rouhizadeh"
                    },
                    {
                        "name": "Abiram Sandralegar"
                    },
                    {
                        "name": "Oren Schreier"
                    },
                    {
                        "name": "Iain Bee Huat Tan"
                    },
                    {
                        "name": "Patrick Tan"
                    },
                    {
                        "name": "Daniel Shu Wei Ting"
                    },
                    {
                        "name": "Junjue Wang"
                    },
                    {
                        "name": "Chunhua Weng"
                    },
                    {
                        "name": "Matthew Yu Heng Wong"
                    },
                    {
                        "name": "Fang Wu"
                    },
                    {
                        "name": "Yunze Xiao"
                    },
                    {
                        "name": "Xuhai Xu"
                    },
                    {
                        "name": "Qingcheng Zeng"
                    },
                    {
                        "name": "Zhuo Zheng"
                    },
                    {
                        "name": "Yifan Peng"
                    },
                    {
                        "name": "Douglas Teodoro"
                    },
                    {
                        "name": "Nan Liu"
                    }
                ],
                "author_detail": {
                    "name": "Nan Liu"
                },
                "author": "Nan Liu"
            },
            {
                "id": "http://arxiv.org/abs/2601.02184v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.02184v1",
                "title": "Differential Barometric Altimetry for Submeter Vertical Localization and Floor Recognition Indoors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Differential Barometric Altimetry for Submeter Vertical Localization and Floor Recognition Indoors"
                },
                "updated": "2026-01-05T15:03:55Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    15,
                    3,
                    55,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.02184v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.02184v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Accurate altitude estimation and reliable floor recognition are critical for mobile robot localization and navigation within complex multi-storey environments. In this paper, we present a robust, low-cost vertical estimation framework leveraging differential barometric sensing integrated within a fully ROS-compliant software package. Our system simultaneously publishes real-time altitude data from both a stationary base station and a mobile sensor, enabling precise and drift-free vertical localization. Empirical evaluations conducted in challenging scenarios -- such as fully enclosed stairwells and elevators, demonstrate that our proposed barometric pipeline achieves sub-meter vertical accuracy (RMSE: 0.29 m) and perfect (100%) floor-level identification. In contrast, our results confirm that standalone height estimates, obtained solely from visual- or LiDAR-based SLAM odometry, are insufficient for reliable vertical localization. The proposed ROS-compatible barometric module thus provides a practical and cost-effective solution for robust vertical awareness in real-world robotic deployments. The implementation of our method is released as open source at https://github.com/witsir/differential-barometric.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate altitude estimation and reliable floor recognition are critical for mobile robot localization and navigation within complex multi-storey environments. In this paper, we present a robust, low-cost vertical estimation framework leveraging differential barometric sensing integrated within a fully ROS-compliant software package. Our system simultaneously publishes real-time altitude data from both a stationary base station and a mobile sensor, enabling precise and drift-free vertical localization. Empirical evaluations conducted in challenging scenarios -- such as fully enclosed stairwells and elevators, demonstrate that our proposed barometric pipeline achieves sub-meter vertical accuracy (RMSE: 0.29 m) and perfect (100%) floor-level identification. In contrast, our results confirm that standalone height estimates, obtained solely from visual- or LiDAR-based SLAM odometry, are insufficient for reliable vertical localization. The proposed ROS-compatible barometric module thus provides a practical and cost-effective solution for robust vertical awareness in real-world robotic deployments. The implementation of our method is released as open source at https://github.com/witsir/differential-barometric."
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-05T15:03:55Z",
                "published_parsed": [
                    2026,
                    1,
                    5,
                    15,
                    3,
                    55,
                    0,
                    5,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO"
                },
                "authors": [
                    {
                        "name": "Yuhang Zhang"
                    },
                    {
                        "name": "Sören Schwertfeger"
                    }
                ],
                "author_detail": {
                    "name": "Sören Schwertfeger"
                },
                "author": "Sören Schwertfeger"
            },
            {
                "id": "http://arxiv.org/abs/2601.02179v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.02179v1",
                "title": "Confidence Estimation for LLMs in Multi-turn Interactions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Confidence Estimation for LLMs in Multi-turn Interactions"
                },
                "updated": "2026-01-05T14:58:04Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    14,
                    58,
                    4,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.02179v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.02179v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "While confidence estimation is a promising direction for mitigating hallucinations in Large Language Models (LLMs), current research dominantly focuses on single-turn settings. The dynamics of model confidence in multi-turn conversations, where context accumulates and ambiguity is progressively resolved, remain largely unexplored. Reliable confidence estimation in multi-turn settings is critical for many downstream applications, such as autonomous agents and human-in-the-loop systems. This work presents the first systematic study of confidence estimation in multi-turn interactions, establishing a formal evaluation framework grounded in two key desiderata: per-turn calibration and monotonicity of confidence as more information becomes available. To facilitate this, we introduce novel metrics, including a length-normalized Expected Calibration Error (InfoECE), and a new \"Hinter-Guesser\" paradigm for generating controlled evaluation datasets. Our experiments reveal that widely-used confidence techniques struggle with calibration and monotonicity in multi-turn dialogues. We propose P(Sufficient), a logit-based probe that achieves comparatively better performance, although the task remains far from solved. Our work provides a foundational methodology for developing more reliable and trustworthy conversational agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While confidence estimation is a promising direction for mitigating hallucinations in Large Language Models (LLMs), current research dominantly focuses on single-turn settings. The dynamics of model confidence in multi-turn conversations, where context accumulates and ambiguity is progressively resolved, remain largely unexplored. Reliable confidence estimation in multi-turn settings is critical for many downstream applications, such as autonomous agents and human-in-the-loop systems. This work presents the first systematic study of confidence estimation in multi-turn interactions, establishing a formal evaluation framework grounded in two key desiderata: per-turn calibration and monotonicity of confidence as more information becomes available. To facilitate this, we introduce novel metrics, including a length-normalized Expected Calibration Error (InfoECE), and a new \"Hinter-Guesser\" paradigm for generating controlled evaluation datasets. Our experiments reveal that widely-used confidence techniques struggle with calibration and monotonicity in multi-turn dialogues. We propose P(Sufficient), a logit-based probe that achieves comparatively better performance, although the task remains far from solved. Our work provides a foundational methodology for developing more reliable and trustworthy conversational agents."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-05T14:58:04Z",
                "published_parsed": [
                    2026,
                    1,
                    5,
                    14,
                    58,
                    4,
                    0,
                    5,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Caiqi Zhang"
                    },
                    {
                        "name": "Ruihan Yang"
                    },
                    {
                        "name": "Xiaochen Zhu"
                    },
                    {
                        "name": "Chengzu Li"
                    },
                    {
                        "name": "Tiancheng Hu"
                    },
                    {
                        "name": "Yijiang River Dong"
                    },
                    {
                        "name": "Deqing Yang"
                    },
                    {
                        "name": "Nigel Collier"
                    }
                ],
                "author_detail": {
                    "name": "Nigel Collier"
                },
                "author": "Nigel Collier"
            },
            {
                "id": "http://arxiv.org/abs/2601.02175v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.02175v1",
                "title": "Single- and Multi-Objective Stochastic Optimization for Next-Generation Networks in the Generative AI and Quantum Computing Era",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Single- and Multi-Objective Stochastic Optimization for Next-Generation Networks in the Generative AI and Quantum Computing Era"
                },
                "updated": "2026-01-05T14:54:27Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    14,
                    54,
                    27,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.02175v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.02175v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Next Generation (NG) networks move beyond simply connecting devices to creating an ecosystem of connected intelligence, especially with the support of generative Artificial Intelligence (AI) and quantum computation. These systems are expected to handle large-scale deployments and high-density networks with diverse functionalities. As a result, there is an increasing demand for efficient and intelligent algorithms that can operate under uncertainty from both propagation environments and networking systems. Traditional optimization methods often depend on accurate theoretical models of data transmission, but in real-world NG scenarios, they suffer from high computational complexity in large-scale settings. Stochastic Optimization (SO) algorithms, designed to accommodate extremely high density and extensive network scalability, have emerged as a powerful solution for optimizing wireless networks. This includes various categories that range from model-based approaches to learning-based approaches. These techniques are capable of converging within a feasible time frame while addressing complex, large-scale optimization problems. However, there is currently limited research on SO applied for NG networks, especially the upcoming Sixth-Generation (6G). In this survey, we emphasize the relationship between NG systems and SO by eight open questions involving the background, key features, and lesson learned. Overall, our study starts by providing a detailed overview of both areas, covering fundamental and widely used SO techniques, spanning from single to multi-objective signal processing. Next, we explore how different algorithms can solve NG challenges, such as load balancing, optimizing energy efficiency, improving spectral efficiency, or handling multiple performance trade-offs. Lastly, we highlight the challenges in the current research and propose new directions for future studies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Next Generation (NG) networks move beyond simply connecting devices to creating an ecosystem of connected intelligence, especially with the support of generative Artificial Intelligence (AI) and quantum computation. These systems are expected to handle large-scale deployments and high-density networks with diverse functionalities. As a result, there is an increasing demand for efficient and intelligent algorithms that can operate under uncertainty from both propagation environments and networking systems. Traditional optimization methods often depend on accurate theoretical models of data transmission, but in real-world NG scenarios, they suffer from high computational complexity in large-scale settings. Stochastic Optimization (SO) algorithms, designed to accommodate extremely high density and extensive network scalability, have emerged as a powerful solution for optimizing wireless networks. This includes various categories that range from model-based approaches to learning-based approaches. These techniques are capable of converging within a feasible time frame while addressing complex, large-scale optimization problems. However, there is currently limited research on SO applied for NG networks, especially the upcoming Sixth-Generation (6G). In this survey, we emphasize the relationship between NG systems and SO by eight open questions involving the background, key features, and lesson learned. Overall, our study starts by providing a detailed overview of both areas, covering fundamental and widely used SO techniques, spanning from single to multi-objective signal processing. Next, we explore how different algorithms can solve NG challenges, such as load balancing, optimizing energy efficiency, improving spectral efficiency, or handling multiple performance trade-offs. Lastly, we highlight the challenges in the current research and propose new directions for future studies."
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-05T14:54:27Z",
                "published_parsed": [
                    2026,
                    1,
                    5,
                    14,
                    54,
                    27,
                    0,
                    5,
                    0
                ],
                "arxiv_comment": "30 pages, 19 figures, and 8 tables. Submitted for publication",
                "arxiv_primary_category": {
                    "term": "cs.IT"
                },
                "authors": [
                    {
                        "name": "Trinh Van Chien"
                    },
                    {
                        "name": "Bui Trong Duc"
                    },
                    {
                        "name": "Nguyen Xuan Tung"
                    },
                    {
                        "name": "Van Duc Nguyen"
                    },
                    {
                        "name": "Waqas Khalid"
                    },
                    {
                        "name": "Symeon Chatzinotas"
                    },
                    {
                        "name": "Lajos Hanzo"
                    }
                ],
                "author_detail": {
                    "name": "Lajos Hanzo"
                },
                "author": "Lajos Hanzo"
            },
            {
                "id": "http://arxiv.org/abs/2601.02167v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.02167v1",
                "title": "LocoScooter: Designing a Stationary Scooter-Based Locomotion System for Navigation in Virtual Reality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LocoScooter: Designing a Stationary Scooter-Based Locomotion System for Navigation in Virtual Reality"
                },
                "updated": "2026-01-05T14:42:47Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    14,
                    42,
                    47,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.02167v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.02167v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Virtual locomotion remains a challenge in VR, especially in space-limited environments where room-scale walking is impractical. We present LocoScooter, a low-cost, deployable locomotion interface combining foot-sliding on a compact treadmill with handlebar steering inspired by scooter riding. Built from commodity hardware, it supports embodied navigation through familiar, physically engaging movement. In a within-subject study (N = 14), LocoScooter significantly improved immersion, enjoyment, and bodily involvement over joystick navigation, while maintaining comparable efficiency and usability. Despite higher physical demand, users did not report increased fatigue, suggesting familiar movements can enrich VR navigation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Virtual locomotion remains a challenge in VR, especially in space-limited environments where room-scale walking is impractical. We present LocoScooter, a low-cost, deployable locomotion interface combining foot-sliding on a compact treadmill with handlebar steering inspired by scooter riding. Built from commodity hardware, it supports embodied navigation through familiar, physically engaging movement. In a within-subject study (N = 14), LocoScooter significantly improved immersion, enjoyment, and bodily involvement over joystick navigation, while maintaining comparable efficiency and usability. Despite higher physical demand, users did not report increased fatigue, suggesting familiar movements can enrich VR navigation."
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-05T14:42:47Z",
                "published_parsed": [
                    2026,
                    1,
                    5,
                    14,
                    42,
                    47,
                    0,
                    5,
                    0
                ],
                "arxiv_comment": "11 pages, 10 figures, conditionally accpeted by IEEE Transactions on Visualization and Computer Graphics (IEEE VR 2026)",
                "arxiv_primary_category": {
                    "term": "cs.HC"
                },
                "authors": [
                    {
                        "name": "Wei He"
                    },
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Per Ola Kristensson"
                    },
                    {
                        "name": "Ge Lin Kan"
                    }
                ],
                "author_detail": {
                    "name": "Ge Lin Kan"
                },
                "author": "Ge Lin Kan"
            },
            {
                "id": "http://arxiv.org/abs/2601.02163v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.02163v1",
                "title": "EverMemOS: A Self-Organizing Memory Operating System for Structured Long-Horizon Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EverMemOS: A Self-Organizing Memory Operating System for Structured Long-Horizon Reasoning"
                },
                "updated": "2026-01-05T14:39:43Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    14,
                    39,
                    43,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.02163v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.02163v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) are increasingly deployed as long-term interactive agents, yet their limited context windows make it difficult to sustain coherent behavior over extended interactions. Existing memory systems often store isolated records and retrieve fragments, limiting their ability to consolidate evolving user states and resolve conflicts. We introduce EverMemOS, a self-organizing memory operating system that implements an engram-inspired lifecycle for computational memory. Episodic Trace Formation converts dialogue streams into MemCells that capture episodic traces, atomic facts, and time-bounded Foresight signals. Semantic Consolidation organizes MemCells into thematic MemScenes, distilling stable semantic structures and updating user profiles. Reconstructive Recollection performs MemScene-guided agentic retrieval to compose the necessary and sufficient context for downstream reasoning. Experiments on LoCoMo and LongMemEval show that EverMemOS achieves state-of-the-art performance on memory-augmented reasoning tasks. We further report a profile study on PersonaMem v2 and qualitative case studies illustrating chat-oriented capabilities such as user profiling and Foresight. Code is available at https://github.com/EverMind-AI/EverMemOS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly deployed as long-term interactive agents, yet their limited context windows make it difficult to sustain coherent behavior over extended interactions. Existing memory systems often store isolated records and retrieve fragments, limiting their ability to consolidate evolving user states and resolve conflicts. We introduce EverMemOS, a self-organizing memory operating system that implements an engram-inspired lifecycle for computational memory. Episodic Trace Formation converts dialogue streams into MemCells that capture episodic traces, atomic facts, and time-bounded Foresight signals. Semantic Consolidation organizes MemCells into thematic MemScenes, distilling stable semantic structures and updating user profiles. Reconstructive Recollection performs MemScene-guided agentic retrieval to compose the necessary and sufficient context for downstream reasoning. Experiments on LoCoMo and LongMemEval show that EverMemOS achieves state-of-the-art performance on memory-augmented reasoning tasks. We further report a profile study on PersonaMem v2 and qualitative case studies illustrating chat-oriented capabilities such as user profiling and Foresight. Code is available at https://github.com/EverMind-AI/EverMemOS."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-05T14:39:43Z",
                "published_parsed": [
                    2026,
                    1,
                    5,
                    14,
                    39,
                    43,
                    0,
                    5,
                    0
                ],
                "arxiv_comment": "16 pages, 6 figures, 12 tables. Code available at https://github.com/EverMind-AI/EverMemOS",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Chuanrui Hu"
                    },
                    {
                        "name": "Xingze Gao"
                    },
                    {
                        "name": "Zuyi Zhou"
                    },
                    {
                        "name": "Dannong Xu"
                    },
                    {
                        "name": "Yi Bai"
                    },
                    {
                        "name": "Xintong Li"
                    },
                    {
                        "name": "Hui Zhang"
                    },
                    {
                        "name": "Tong Li"
                    },
                    {
                        "name": "Chong Zhang"
                    },
                    {
                        "name": "Lidong Bing"
                    },
                    {
                        "name": "Yafeng Deng"
                    }
                ],
                "author_detail": {
                    "name": "Yafeng Deng"
                },
                "author": "Yafeng Deng"
            },
            {
                "id": "http://arxiv.org/abs/2504.11320v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2504.11320v2",
                "title": "Optimizing LLM Inference: Fluid-Guided Online Scheduling with Memory Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing LLM Inference: Fluid-Guided Online Scheduling with Memory Constraints"
                },
                "updated": "2026-01-05T14:10:45Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    14,
                    10,
                    45,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2504.11320v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2504.11320v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) power many modern applications, but their inference procedure poses unique scheduling challenges: the Key-Value (KV) cache grows dynamically during response generation, and memory overflow triggers eviction that can cascade into system-wide failures. Even when memory capacity exceeds the theoretical requirement, conventional scheduling algorithms fail because they do not account for this dynamic memory growth -- a system that should be stable can become unstable under poor scheduling.\n  This paper formulates LLM inference optimization as a multi-stage online scheduling problem. We develop a fluid dynamics approximation to establish a tractable benchmark and derive the Waiting for Accumulated Inference Threshold (WAIT) algorithm. WAIT uses threshold-based batching to prevent eviction by keeping the system near load balance, achieving near-optimal throughput when output lengths are known.\n  For practical settings where output lengths are unknown at arrival, we introduce Nested WAIT. Rather than predicting output lengths, Nested WAIT classifies prompts on-the-fly: short prompts complete early and exit, while longer prompts naturally advance to later segments. A safety buffer provides high-probability protection against memory overflow with only logarithmic overhead.\n  Theoretical analysis establishes near-optimal performance in the asymptotic regime. Experiments on Llama-7B with an A100 GPU demonstrate that our approach achieves superior throughput and reduced latency compared to vLLM and Sarathi. This work applies operations research principles to establish a theoretical framework for LLM deployment under memory constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) power many modern applications, but their inference procedure poses unique scheduling challenges: the Key-Value (KV) cache grows dynamically during response generation, and memory overflow triggers eviction that can cascade into system-wide failures. Even when memory capacity exceeds the theoretical requirement, conventional scheduling algorithms fail because they do not account for this dynamic memory growth -- a system that should be stable can become unstable under poor scheduling.\n  This paper formulates LLM inference optimization as a multi-stage online scheduling problem. We develop a fluid dynamics approximation to establish a tractable benchmark and derive the Waiting for Accumulated Inference Threshold (WAIT) algorithm. WAIT uses threshold-based batching to prevent eviction by keeping the system near load balance, achieving near-optimal throughput when output lengths are known.\n  For practical settings where output lengths are unknown at arrival, we introduce Nested WAIT. Rather than predicting output lengths, Nested WAIT classifies prompts on-the-fly: short prompts complete early and exit, while longer prompts naturally advance to later segments. A safety buffer provides high-probability protection against memory overflow with only logarithmic overhead.\n  Theoretical analysis establishes near-optimal performance in the asymptotic regime. Experiments on Llama-7B with an A100 GPU demonstrate that our approach achieves superior throughput and reduced latency compared to vLLM and Sarathi. This work applies operations research principles to establish a theoretical framework for LLM deployment under memory constraints."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-04-15T16:00:21Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    16,
                    0,
                    21,
                    1,
                    105,
                    0
                ],
                "arxiv_comment": "49 pages, 18 figures",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Ruicheng Ao"
                    },
                    {
                        "name": "Gan Luo"
                    },
                    {
                        "name": "David Simchi-Levi"
                    },
                    {
                        "name": "Xinshang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xinshang Wang"
                },
                "author": "Xinshang Wang"
            },
            {
                "id": "http://arxiv.org/abs/2510.18039v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.18039v2",
                "title": "Presenting Large Language Models as Companions Affects What Mental Capacities People Attribute to Them",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Presenting Large Language Models as Companions Affects What Mental Capacities People Attribute to Them"
                },
                "updated": "2026-01-05T14:03:10Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    14,
                    3,
                    10,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.18039v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.18039v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "How might messages about large language models (LLMs) found in public discourse influence the way people think about and interact with these models? To explore this question, we randomly assigned participants (N = 470) to watch short informational videos presenting LLMs as either machines, tools, or companions -- or to watch no video. We then assessed how strongly they believed LLMs to possess various mental capacities, such as the ability have intentions or remember things. We found that participants who watched video messages presenting LLMs as companions reported believing that LLMs more fully possessed these capacities than did participants in other groups. In a follow-up study (N = 604), we replicated these findings and found nuanced effects on how these videos also impact people's reliance on LLM-generated responses when seeking out factual information. Together, these studies suggest that messages about LLMs -- beyond technical advances -- may shape what people believe about these systems and how they rely on LLM-generated responses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How might messages about large language models (LLMs) found in public discourse influence the way people think about and interact with these models? To explore this question, we randomly assigned participants (N = 470) to watch short informational videos presenting LLMs as either machines, tools, or companions -- or to watch no video. We then assessed how strongly they believed LLMs to possess various mental capacities, such as the ability have intentions or remember things. We found that participants who watched video messages presenting LLMs as companions reported believing that LLMs more fully possessed these capacities than did participants in other groups. In a follow-up study (N = 604), we replicated these findings and found nuanced effects on how these videos also impact people's reliance on LLM-generated responses when seeking out factual information. Together, these studies suggest that messages about LLMs -- beyond technical advances -- may shape what people believe about these systems and how they rely on LLM-generated responses."
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-20T19:25:24Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    19,
                    25,
                    24,
                    0,
                    293,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC"
                },
                "authors": [
                    {
                        "name": "Allison Chen"
                    },
                    {
                        "name": "Sunnie S. Y. Kim"
                    },
                    {
                        "name": "Angel Franyutti"
                    },
                    {
                        "name": "Amaya Dharmasiri"
                    },
                    {
                        "name": "Kushin Mukherjee"
                    },
                    {
                        "name": "Olga Russakovsky"
                    },
                    {
                        "name": "Judith E. Fan"
                    }
                ],
                "author_detail": {
                    "name": "Judith E. Fan"
                },
                "author": "Judith E. Fan"
            },
            {
                "id": "http://arxiv.org/abs/2509.14704v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2509.14704v2",
                "title": "Japanese Children's Riddles as a Benchmark for Machine Insight and Metacognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Japanese Children's Riddles as a Benchmark for Machine Insight and Metacognition"
                },
                "updated": "2026-01-05T13:57:38Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    13,
                    57,
                    38,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2509.14704v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2509.14704v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Benchmark saturation and contamination have obscured genuine advances in reasoning for large language models (LLMs). We introduce NazoNazo Benchmark, a low-cost, renewable test built from Japanese children's riddles that demand insight-based reasoning, or representational shifts rather than knowledge recall. We evaluate 38 frontier LLMs (2023-2025) on 201 riddles and a 120-item human-comparison subset, finding that non-reasoning models average 7.6%, reasoning models 17.6%, and humans ~53% accuracy. Importantly, thought-log analysis reveals that reasoning in Japanese did not necessarily improve accuracy, indicating that language understanding alone is insufficient for insight reasoning. Notably, models sometimes generated correct candidates but failed to endorse them, suggesting weak metacognitive control rather than a lack of knowledge. This \"verification failure\" indicates that CoT outputs can reflect genuine intermediate reasoning states rather than post-hoc rationalizations. By exposing this metacognitive bottleneck - models' inability to recognize when they are right - the benchmark provides a scalable, cross-linguistic testbed for studying machine insight, confidence calibration, and self-evaluation. NazoNazo Benchmark thus offers not only a fresh challenge to current LLMs but also a concrete target for developing AI metacognitive psychology and enhancing machine Aha! capability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmark saturation and contamination have obscured genuine advances in reasoning for large language models (LLMs). We introduce NazoNazo Benchmark, a low-cost, renewable test built from Japanese children's riddles that demand insight-based reasoning, or representational shifts rather than knowledge recall. We evaluate 38 frontier LLMs (2023-2025) on 201 riddles and a 120-item human-comparison subset, finding that non-reasoning models average 7.6%, reasoning models 17.6%, and humans ~53% accuracy. Importantly, thought-log analysis reveals that reasoning in Japanese did not necessarily improve accuracy, indicating that language understanding alone is insufficient for insight reasoning. Notably, models sometimes generated correct candidates but failed to endorse them, suggesting weak metacognitive control rather than a lack of knowledge. This \"verification failure\" indicates that CoT outputs can reflect genuine intermediate reasoning states rather than post-hoc rationalizations. By exposing this metacognitive bottleneck - models' inability to recognize when they are right - the benchmark provides a scalable, cross-linguistic testbed for studying machine insight, confidence calibration, and self-evaluation. NazoNazo Benchmark thus offers not only a fresh challenge to current LLMs but also a concrete target for developing AI metacognitive psychology and enhancing machine Aha! capability."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-09-18T07:50:04Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    7,
                    50,
                    4,
                    3,
                    261,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Masaharu Mizumoto"
                    },
                    {
                        "name": "Dat Nguyen"
                    },
                    {
                        "name": "Zhiheng Han"
                    },
                    {
                        "name": "Jiyuan Fang"
                    },
                    {
                        "name": "Heyuan Guan"
                    },
                    {
                        "name": "Xingfu Li"
                    },
                    {
                        "name": "Naoya Shiraishi"
                    },
                    {
                        "name": "Yo Nakawake"
                    },
                    {
                        "name": "Le Minh Nguyen"
                    }
                ],
                "author_detail": {
                    "name": "Le Minh Nguyen"
                },
                "author": "Le Minh Nguyen"
            },
            {
                "id": "http://arxiv.org/abs/2601.02123v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.02123v1",
                "title": "DeCode: Decoupling Content and Delivery for Medical QA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeCode: Decoupling Content and Delivery for Medical QA"
                },
                "updated": "2026-01-05T13:54:38Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    13,
                    54,
                    38,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.02123v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.02123v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models (LLMs) exhibit strong medical knowledge and can generate factually accurate responses. However, existing models often fail to account for individual patient contexts, producing answers that are clinically correct yet poorly aligned with patients' needs. In this work, we introduce DeCode, a training-free, model-agnostic framework that adapts existing LLMs to produce contextualized answers in clinical settings. We evaluate DeCode on OpenAI HealthBench, a comprehensive and challenging benchmark designed to assess clinical relevance and validity of LLM responses. DeCode improves the previous state of the art from $28.4\\%$ to $49.8\\%$, corresponding to a $75\\%$ relative improvement. Experimental results suggest the effectiveness of DeCode in improving clinical question answering of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) exhibit strong medical knowledge and can generate factually accurate responses. However, existing models often fail to account for individual patient contexts, producing answers that are clinically correct yet poorly aligned with patients' needs. In this work, we introduce DeCode, a training-free, model-agnostic framework that adapts existing LLMs to produce contextualized answers in clinical settings. We evaluate DeCode on OpenAI HealthBench, a comprehensive and challenging benchmark designed to assess clinical relevance and validity of LLM responses. DeCode improves the previous state of the art from $28.4\\%$ to $49.8\\%$, corresponding to a $75\\%$ relative improvement. Experimental results suggest the effectiveness of DeCode in improving clinical question answering of LLMs."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-05T13:54:38Z",
                "published_parsed": [
                    2026,
                    1,
                    5,
                    13,
                    54,
                    38,
                    0,
                    5,
                    0
                ],
                "arxiv_comment": "Preprint",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Po-Jen Ko"
                    },
                    {
                        "name": "Chen-Han Tsai"
                    },
                    {
                        "name": "Yu-Shao Peng"
                    }
                ],
                "author_detail": {
                    "name": "Yu-Shao Peng"
                },
                "author": "Yu-Shao Peng"
            },
            {
                "id": "http://arxiv.org/abs/2512.22792v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.22792v2",
                "title": "A Universal and Robust Framework for Multiple Gas Recognition Based-on Spherical Normalization-Coupled Mahalanobis Algorithm",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Universal and Robust Framework for Multiple Gas Recognition Based-on Spherical Normalization-Coupled Mahalanobis Algorithm"
                },
                "updated": "2026-01-05T13:46:47Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    13,
                    46,
                    47,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.22792v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.22792v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Electronic nose (E-nose) systems face two interconnected challenges in open-set gas recognition: feature distribution shift caused by signal drift and decision boundary failure induced by unknown gas interference. Existing methods predominantly rely on Euclidean distance or conventional classifiers, failing to account for anisotropic feature distributions and dynamic signal intensity variations. To address these issues, this study proposes the Spherical Normalization coupled Mahalanobis (SNM) module, a universal post-processing module for open-set gas recognition. First, it achieves geometric decoupling through cascaded batch and L2 normalization, projecting features onto a unit hypersphere to eliminate signal intensity fluctuations. Second, it utilizes Mahalanobis distance to construct adaptive ellipsoidal decision boundaries that conform to the anisotropic feature geometry. The architecture-agnostic SNM-Module seamlessly integrates with mainstream backbones including Convolutional Neural Network (CNN), Recurrent Neural Network (RNN), and Transformer. Experiments on the public Vergara dataset demonstrate that the Transformer+SNM configuration achieves near-theoretical-limit performance in discriminating among multiple target gases, with an AUROC of 0.9977 and an unknown gas detection rate of 99.57% at 5% false positive rate, significantly outperforming state-of-the-art methods with a 3.0% AUROC improvement and 91.0% standard deviation reduction compared to Class Anchor Clustering (CAC). The module maintains exceptional robustness across five sensor positions, with standard deviations below 0.0028. This work effectively addresses the critical challenge of simultaneously achieving high accuracy and high stability in open-set gas recognition, providing solid support for industrial E-nose deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electronic nose (E-nose) systems face two interconnected challenges in open-set gas recognition: feature distribution shift caused by signal drift and decision boundary failure induced by unknown gas interference. Existing methods predominantly rely on Euclidean distance or conventional classifiers, failing to account for anisotropic feature distributions and dynamic signal intensity variations. To address these issues, this study proposes the Spherical Normalization coupled Mahalanobis (SNM) module, a universal post-processing module for open-set gas recognition. First, it achieves geometric decoupling through cascaded batch and L2 normalization, projecting features onto a unit hypersphere to eliminate signal intensity fluctuations. Second, it utilizes Mahalanobis distance to construct adaptive ellipsoidal decision boundaries that conform to the anisotropic feature geometry. The architecture-agnostic SNM-Module seamlessly integrates with mainstream backbones including Convolutional Neural Network (CNN), Recurrent Neural Network (RNN), and Transformer. Experiments on the public Vergara dataset demonstrate that the Transformer+SNM configuration achieves near-theoretical-limit performance in discriminating among multiple target gases, with an AUROC of 0.9977 and an unknown gas detection rate of 99.57% at 5% false positive rate, significantly outperforming state-of-the-art methods with a 3.0% AUROC improvement and 91.0% standard deviation reduction compared to Class Anchor Clustering (CAC). The module maintains exceptional robustness across five sensor positions, with standard deviations below 0.0028. This work effectively addresses the critical challenge of simultaneously achieving high accuracy and high stability in open-set gas recognition, providing solid support for industrial E-nose deployment."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-28T05:33:05Z",
                "published_parsed": [
                    2025,
                    12,
                    28,
                    5,
                    33,
                    5,
                    6,
                    362,
                    0
                ],
                "arxiv_comment": "27 pages, 8 figures, 4 tables",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Shuai Chen"
                    },
                    {
                        "name": "Yang Song"
                    },
                    {
                        "name": "Chen Wang"
                    },
                    {
                        "name": "Ziran Wang"
                    }
                ],
                "author_detail": {
                    "name": "Ziran Wang"
                },
                "author": "Ziran Wang"
            },
            {
                "id": "http://arxiv.org/abs/2512.23260v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.23260v2",
                "title": "Interpretable Safety Alignment via SAE-Constructed Low-Rank Subspace Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interpretable Safety Alignment via SAE-Constructed Low-Rank Subspace Adaptation"
                },
                "updated": "2026-01-05T13:39:39Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    13,
                    39,
                    39,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.23260v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.23260v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Safety alignment -- training large language models (LLMs) to refuse harmful requests while remaining helpful -- is critical for responsible deployment. Prior work established that safety behaviors are governed by low-rank structures, suggesting parameter-efficient fine-tuning (PEFT) should be well-suited for alignment. However, Low-Rank Adaptation (LoRA) consistently underperforms full fine-tuning and reinforcement learning on safety benchmarks. We attribute this gap to semantic entanglement: safety-relevant directions are intertwined with unrelated concepts due to polysemanticity, impeding implicit subspace identification. To address this, we propose SAILS (Safety Alignment via Interpretable Low-rank Subspace), which leverages Sparse Autoencoders (SAEs) to disentangle representations into monosemantic features, constructs an interpretable safety subspace from SAE decoder directions, and uses it to initialize LoRA adapters. Theoretically, we prove that SAE-based identification achieves arbitrarily small recovery error under monosemanticity assumptions, while direct identification suffers an irreducible error floor. Empirically, SAILS achieves up to 99.6% safety rate on Gemma-2-9B -- exceeding full fine-tuning by 7.4 points and matching RLHF-based models -- while updating only 0.19% of parameters and providing interpretability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Safety alignment -- training large language models (LLMs) to refuse harmful requests while remaining helpful -- is critical for responsible deployment. Prior work established that safety behaviors are governed by low-rank structures, suggesting parameter-efficient fine-tuning (PEFT) should be well-suited for alignment. However, Low-Rank Adaptation (LoRA) consistently underperforms full fine-tuning and reinforcement learning on safety benchmarks. We attribute this gap to semantic entanglement: safety-relevant directions are intertwined with unrelated concepts due to polysemanticity, impeding implicit subspace identification. To address this, we propose SAILS (Safety Alignment via Interpretable Low-rank Subspace), which leverages Sparse Autoencoders (SAEs) to disentangle representations into monosemantic features, constructs an interpretable safety subspace from SAE decoder directions, and uses it to initialize LoRA adapters. Theoretically, we prove that SAE-based identification achieves arbitrarily small recovery error under monosemanticity assumptions, while direct identification suffers an irreducible error floor. Empirically, SAILS achieves up to 99.6% safety rate on Gemma-2-9B -- exceeding full fine-tuning by 7.4 points and matching RLHF-based models -- while updating only 0.19% of parameters and providing interpretability."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-29T07:39:49Z",
                "published_parsed": [
                    2025,
                    12,
                    29,
                    7,
                    39,
                    49,
                    0,
                    363,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Dianyun Wang"
                    },
                    {
                        "name": "Qingsen Ma"
                    },
                    {
                        "name": "Yuhu Shang"
                    },
                    {
                        "name": "Zhifeng Lu"
                    },
                    {
                        "name": "Zhenbo Xu"
                    },
                    {
                        "name": "Lechen Ning"
                    },
                    {
                        "name": "Huijia Wu"
                    },
                    {
                        "name": "Zhaofeng He"
                    }
                ],
                "author_detail": {
                    "name": "Zhaofeng He"
                },
                "author": "Zhaofeng He"
            },
            {
                "id": "http://arxiv.org/abs/2506.05623v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2506.05623v2",
                "title": "Deployability-Centric Infrastructure-as-Code Generation: Fail, Learn, Refine, and Succeed through LLM-Empowered DevOps Simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deployability-Centric Infrastructure-as-Code Generation: Fail, Learn, Refine, and Succeed through LLM-Empowered DevOps Simulation"
                },
                "updated": "2026-01-05T13:38:13Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    13,
                    38,
                    13,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2506.05623v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2506.05623v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Infrastructure-as-Code (IaC) generation holds significant promise for automating cloud infrastructure provisioning. Recent advances in Large Language Models (LLMs) present a promising opportunity to democratize IaC development by generating deployable infrastructure templates from natural language descriptions. However, current evaluation focuses on syntactic correctness while ignoring deployability, the critical measure of the utility of IaC configuration files. Six state-of-the-art LLMs performed poorly on deployability, achieving only 20.8$\\sim$30.2% deployment success rate on the first attempt. In this paper, we construct DPIaC-Eval, the first deployability-centric IaC template benchmark consisting of 153 real-world scenarios cross 58 unique services. Also, we propose an LLM-based deployability-centric framework, dubbed IaCGen, that uses iterative feedback mechanism encompassing format verification, syntax checking, and live deployment stages, thereby closely mirroring the real DevOps workflows. Results show that IaCGen can make 54.6$\\sim$91.6% generated IaC templates from all evaluated models deployable in the first 10 iterations. Additionally, human-in-the-loop feedback that provide direct guidance for the deployability errors, can further boost the performance to over 90% passItr@25 on all evaluated LLMs. Furthermore, we explore the trustworthiness of the generated IaC templates on user intent alignment and security compliance. The poor performance (25.2% user requirement coverage and 8.4% security compliance rate) indicates a critical need for continued research in this domain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Infrastructure-as-Code (IaC) generation holds significant promise for automating cloud infrastructure provisioning. Recent advances in Large Language Models (LLMs) present a promising opportunity to democratize IaC development by generating deployable infrastructure templates from natural language descriptions. However, current evaluation focuses on syntactic correctness while ignoring deployability, the critical measure of the utility of IaC configuration files. Six state-of-the-art LLMs performed poorly on deployability, achieving only 20.8$\\sim$30.2% deployment success rate on the first attempt. In this paper, we construct DPIaC-Eval, the first deployability-centric IaC template benchmark consisting of 153 real-world scenarios cross 58 unique services. Also, we propose an LLM-based deployability-centric framework, dubbed IaCGen, that uses iterative feedback mechanism encompassing format verification, syntax checking, and live deployment stages, thereby closely mirroring the real DevOps workflows. Results show that IaCGen can make 54.6$\\sim$91.6% generated IaC templates from all evaluated models deployable in the first 10 iterations. Additionally, human-in-the-loop feedback that provide direct guidance for the deployability errors, can further boost the performance to over 90% passItr@25 on all evaluated LLMs. Furthermore, we explore the trustworthiness of the generated IaC templates on user intent alignment and security compliance. The poor performance (25.2% user requirement coverage and 8.4% security compliance rate) indicates a critical need for continued research in this domain."
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-06-05T22:53:12Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    22,
                    53,
                    12,
                    3,
                    156,
                    0
                ],
                "arxiv_comment": "Accepted by FSE 2026",
                "arxiv_primary_category": {
                    "term": "cs.SE"
                },
                "authors": [
                    {
                        "name": "Tianyi Zhang"
                    },
                    {
                        "name": "Shidong Pan"
                    },
                    {
                        "name": "Zejun Zhang"
                    },
                    {
                        "name": "Zhenchang Xing"
                    },
                    {
                        "name": "Xiaoyu Sun"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoyu Sun"
                },
                "author": "Xiaoyu Sun"
            },
            {
                "id": "http://arxiv.org/abs/2511.06779v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.06779v2",
                "title": "Pedagogical Reflections on the Holistic Cognitive Development (HCD) Framework and AI-Augmented Learning in Creative Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pedagogical Reflections on the Holistic Cognitive Development (HCD) Framework and AI-Augmented Learning in Creative Computing"
                },
                "updated": "2026-01-05T13:31:19Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    13,
                    31,
                    19,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.06779v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.06779v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "This paper presents an expanded account of the Holistic Cognitive Development (HCD) framework for reflective and creative learning in computing education. The HCD framework integrates design thinking, experiential learning, and reflective practice into a unified constructivist pedagogy emphasizing autonomy, ownership, and scaffolding. It is applied across courses in game design (CS3247, CS4350), virtual reality (CS4240), and extended reality systems, where students engage in iterative cycles of thinking, creating, criticizing, and reflecting. The paper also examines how AI-augmented systems such as iReflect, ReflexAI, and Knowledge Graph-enhanced LLM feedback tools operationalize the HCD framework through scalable, personalized feedback. Empirical findings demonstrate improved reflective depth, feedback quality, and learner autonomy. The work advocates a balance of supportive autonomy in supervision, where students practice self-directed inquiry while guided through structured reflection and feedback.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents an expanded account of the Holistic Cognitive Development (HCD) framework for reflective and creative learning in computing education. The HCD framework integrates design thinking, experiential learning, and reflective practice into a unified constructivist pedagogy emphasizing autonomy, ownership, and scaffolding. It is applied across courses in game design (CS3247, CS4350), virtual reality (CS4240), and extended reality systems, where students engage in iterative cycles of thinking, creating, criticizing, and reflecting. The paper also examines how AI-augmented systems such as iReflect, ReflexAI, and Knowledge Graph-enhanced LLM feedback tools operationalize the HCD framework through scalable, personalized feedback. Empirical findings demonstrate improved reflective depth, feedback quality, and learner autonomy. The work advocates a balance of supportive autonomy in supervision, where students practice self-directed inquiry while guided through structured reflection and feedback."
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-10T07:07:37Z",
                "published_parsed": [
                    2025,
                    11,
                    10,
                    7,
                    7,
                    37,
                    0,
                    314,
                    0
                ],
                "arxiv_comment": "Short Abstract",
                "arxiv_primary_category": {
                    "term": "cs.MM"
                },
                "authors": [
                    {
                        "name": "Anand Bhojan"
                    }
                ],
                "author_detail": {
                    "name": "Anand Bhojan"
                },
                "author": "Anand Bhojan"
            },
            {
                "id": "http://arxiv.org/abs/2510.23506v4",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.23506v4",
                "title": "Emotion-Coherent Reasoning for Multimodal LLMs via Emotional Rationale Verifier",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emotion-Coherent Reasoning for Multimodal LLMs via Emotional Rationale Verifier"
                },
                "updated": "2026-01-05T13:24:37Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    13,
                    24,
                    37,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.23506v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.23506v4",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The recent advancement of Multimodal Large Language Models (MLLMs) is transforming human-computer interaction (HCI) from surface-level exchanges into more nuanced and emotionally intelligent communication. To realize this shift, emotion understanding becomes essential allowing systems to capture subtle cues underlying user intent. Furthermore, providing faithful explanations for predicted emotions is crucial to ensure interpretability and build user trust. However, current MLLM-based methods often generate emotion explanations that diverge from the target labels and sometimes even contradict their own predicted emotions. This inconsistency poses a critical risk for misunderstanding and erodes reliability in interactive settings. To address this, we propose a novel approach: the Emotional Rationale Verifier (ERV) and an Explanation Reward. Our method guides the model to produce reasoning that is explicitly consistent with the target emotion during multimodal emotion recognition without modifying the model architecture or requiring additional paired video-description annotations. Our method significantly improves faithful explanation-prediction consistency and explanation emotion accuracy on the MAFW and DFEW datasets. Through extensive experiments and human evaluations, we show that our approach not only enhances alignment between explanation and prediction but also empowers MLLMs to deliver emotionally coherent, trustworthy interactions, marking a key step toward truly human-like HCI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent advancement of Multimodal Large Language Models (MLLMs) is transforming human-computer interaction (HCI) from surface-level exchanges into more nuanced and emotionally intelligent communication. To realize this shift, emotion understanding becomes essential allowing systems to capture subtle cues underlying user intent. Furthermore, providing faithful explanations for predicted emotions is crucial to ensure interpretability and build user trust. However, current MLLM-based methods often generate emotion explanations that diverge from the target labels and sometimes even contradict their own predicted emotions. This inconsistency poses a critical risk for misunderstanding and erodes reliability in interactive settings. To address this, we propose a novel approach: the Emotional Rationale Verifier (ERV) and an Explanation Reward. Our method guides the model to produce reasoning that is explicitly consistent with the target emotion during multimodal emotion recognition without modifying the model architecture or requiring additional paired video-description annotations. Our method significantly improves faithful explanation-prediction consistency and explanation emotion accuracy on the MAFW and DFEW datasets. Through extensive experiments and human evaluations, we show that our approach not only enhances alignment between explanation and prediction but also empowers MLLMs to deliver emotionally coherent, trustworthy interactions, marking a key step toward truly human-like HCI systems."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-27T16:40:17Z",
                "published_parsed": [
                    2025,
                    10,
                    27,
                    16,
                    40,
                    17,
                    0,
                    300,
                    0
                ],
                "arxiv_comment": "15 pages, 11 figures",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Hyeongseop Rha"
                    },
                    {
                        "name": "Jeong Hun Yeo"
                    },
                    {
                        "name": "Yeonju Kim"
                    },
                    {
                        "name": "Yong Man Ro"
                    }
                ],
                "author_detail": {
                    "name": "Yong Man Ro"
                },
                "author": "Yong Man Ro"
            },
            {
                "id": "http://arxiv.org/abs/2512.20957v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20957v3",
                "title": "One Tool Is Enough: Reinforcement Learning for Repository-Level LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One Tool Is Enough: Reinforcement Learning for Repository-Level LLM Agents"
                },
                "updated": "2026-01-05T13:23:35Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    13,
                    23,
                    35,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20957v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20957v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Locating the files and functions requiring modification in large open-source software (OSS) repositories is challenging due to their scale and structural complexity. Existing large language model (LLM)-based methods typically treat this as a repository-level retrieval task and rely on multiple auxiliary tools, which overlook code execution logic and complicate model control. We propose RepoNavigator, an LLM agent equipped with a single execution-aware tool-jumping to the definition of an invoked symbol. This unified design reflects the actual flow of code execution while simplifying tool manipulation. RepoNavigator is trained end-to-end via Reinforcement Learning (RL) directly from a pretrained model, without any closed-source distillation. Experiments demonstrate that RL-trained RepoNavigator achieves state-of-the-art performance, with the 7B model outperforming 14B baselines, the 14B model surpassing 32B competitors, and even the 32B model exceeding closed-source models such as Claude-3.7. These results confirm that integrating a single, structurally grounded tool with RL training provides an efficient and scalable solution for repository-level issue localization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Locating the files and functions requiring modification in large open-source software (OSS) repositories is challenging due to their scale and structural complexity. Existing large language model (LLM)-based methods typically treat this as a repository-level retrieval task and rely on multiple auxiliary tools, which overlook code execution logic and complicate model control. We propose RepoNavigator, an LLM agent equipped with a single execution-aware tool-jumping to the definition of an invoked symbol. This unified design reflects the actual flow of code execution while simplifying tool manipulation. RepoNavigator is trained end-to-end via Reinforcement Learning (RL) directly from a pretrained model, without any closed-source distillation. Experiments demonstrate that RL-trained RepoNavigator achieves state-of-the-art performance, with the 7B model outperforming 14B baselines, the 14B model surpassing 32B competitors, and even the 32B model exceeding closed-source models such as Claude-3.7. These results confirm that integrating a single, structurally grounded tool with RL training provides an efficient and scalable solution for repository-level issue localization."
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-24T05:27:53Z",
                "published_parsed": [
                    2025,
                    12,
                    24,
                    5,
                    27,
                    53,
                    2,
                    358,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE"
                },
                "authors": [
                    {
                        "name": "Zhaoxi Zhang"
                    },
                    {
                        "name": "Yitong Duan"
                    },
                    {
                        "name": "Yanzhi Zhang"
                    },
                    {
                        "name": "Yiming Xu"
                    },
                    {
                        "name": "Jiyan He"
                    },
                    {
                        "name": "Yunfang Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yunfang Wu"
                },
                "author": "Yunfang Wu"
            },
            {
                "id": "http://arxiv.org/abs/2512.22673v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.22673v2",
                "title": "TravelBench: A Broader Real-World Benchmark for Multi-Turn and Tool-Using Travel Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TravelBench: A Broader Real-World Benchmark for Multi-Turn and Tool-Using Travel Planning"
                },
                "updated": "2026-01-05T13:19:13Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    13,
                    19,
                    13,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.22673v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.22673v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Travel planning is a natural real-world task to test large language models (LLMs) planning and tool-use abilities. Although prior work has studied LLM performance on travel planning, existing settings still differ from real-world needs, mainly due to limited domain coverage, insufficient modeling of users' implicit preferences in multi-turn conversations, and a lack of clear evaluation of agents' capability boundaries. To mitigate these gaps, we propose \\textbf{TravelBench}, a benchmark for fully real-world travel planning. We collect user queries, user profile and tools from real scenarios, and construct three subtasks-Single-Turn, Multi-Turn, and Unsolvable-to evaluate agent's three core capabilities in real settings: (1) solving problems autonomously, (2) interacting with users over multiple turns to refine requirements, and (3) recognizing the limits of own abilities. To enable stable tool invocation and reproducible evaluation, we cache real tool-call results and build a sandbox environment that integrates ten travel-related tools. Agents can combine these tools to solve most practical travel planning problems, and our systematic verification demonstrates the stability of the proposed benchmark. We further evaluate multiple LLMs on TravelBench and conduct an in-depth analysis of their behaviors and performance. TravelBench provides a practical and reproducible evaluation benchmark to advance research on LLM agents for travel planning.\\footnote{Our code and data will be available after internal review.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Travel planning is a natural real-world task to test large language models (LLMs) planning and tool-use abilities. Although prior work has studied LLM performance on travel planning, existing settings still differ from real-world needs, mainly due to limited domain coverage, insufficient modeling of users' implicit preferences in multi-turn conversations, and a lack of clear evaluation of agents' capability boundaries. To mitigate these gaps, we propose \\textbf{TravelBench}, a benchmark for fully real-world travel planning. We collect user queries, user profile and tools from real scenarios, and construct three subtasks-Single-Turn, Multi-Turn, and Unsolvable-to evaluate agent's three core capabilities in real settings: (1) solving problems autonomously, (2) interacting with users over multiple turns to refine requirements, and (3) recognizing the limits of own abilities. To enable stable tool invocation and reproducible evaluation, we cache real tool-call results and build a sandbox environment that integrates ten travel-related tools. Agents can combine these tools to solve most practical travel planning problems, and our systematic verification demonstrates the stability of the proposed benchmark. We further evaluate multiple LLMs on TravelBench and conduct an in-depth analysis of their behaviors and performance. TravelBench provides a practical and reproducible evaluation benchmark to advance research on LLM agents for travel planning.\\footnote{Our code and data will be available after internal review."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-27T18:25:14Z",
                "published_parsed": [
                    2025,
                    12,
                    27,
                    18,
                    25,
                    14,
                    5,
                    361,
                    0
                ],
                "arxiv_comment": "In progress",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Xiang Cheng"
                    },
                    {
                        "name": "Yulan Hu"
                    },
                    {
                        "name": "Xiangwen Zhang"
                    },
                    {
                        "name": "Lu Xu"
                    },
                    {
                        "name": "Zheng Pan"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Yong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Liu"
                },
                "author": "Yong Liu"
            },
            {
                "id": "http://arxiv.org/abs/2512.11667v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.11667v2",
                "title": "Toward Scalable VR-Cloud Gaming: An Attention-aware Adaptive Resource Allocation Framework for 6G Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toward Scalable VR-Cloud Gaming: An Attention-aware Adaptive Resource Allocation Framework for 6G Networks"
                },
                "updated": "2026-01-05T13:18:26Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    13,
                    18,
                    26,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.11667v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.11667v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Virtual Reality Cloud Gaming (VR-CG) represents a demanding class of immersive applications, requiring high bandwidth, ultra-low latency, and intelligent resource management to ensure optimal user experience. In this paper, we propose a scalable and QoE-aware multi-stage optimization framework for resource allocation in VR-CG over 6G networks. Our solution decomposes the joint resource allocation problem into three interdependent stages: (i) user association and communication resource allocation; (ii) VR-CG game engine placement with adaptive multipath routing; and (iii) attention-aware scheduling and wireless resource allocation based on motion-to-photon latency. For each stage, we design specialized heuristic algorithms that achieve near-optimal performance while significantly reducing computational time. We introduce a novel user-centric QoE model based on visual attention to virtual objects, guiding adaptive resolution and frame rate selection. A dataset-driven evaluation demonstrates that, when compared against state-of-the-art approaches, our framework improves QoE by up to 50\\%, reduces communication resource usage by 75\\%, and achieves up to 35\\% cost savings, while maintaining an average optimality gap of 5\\%. Our proposed heuristics solve large-scale scenarios in under 0.1 seconds, highlighting their potential for real-time deployment in next-generation mobile networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Virtual Reality Cloud Gaming (VR-CG) represents a demanding class of immersive applications, requiring high bandwidth, ultra-low latency, and intelligent resource management to ensure optimal user experience. In this paper, we propose a scalable and QoE-aware multi-stage optimization framework for resource allocation in VR-CG over 6G networks. Our solution decomposes the joint resource allocation problem into three interdependent stages: (i) user association and communication resource allocation; (ii) VR-CG game engine placement with adaptive multipath routing; and (iii) attention-aware scheduling and wireless resource allocation based on motion-to-photon latency. For each stage, we design specialized heuristic algorithms that achieve near-optimal performance while significantly reducing computational time. We introduce a novel user-centric QoE model based on visual attention to virtual objects, guiding adaptive resolution and frame rate selection. A dataset-driven evaluation demonstrates that, when compared against state-of-the-art approaches, our framework improves QoE by up to 50\\%, reduces communication resource usage by 75\\%, and achieves up to 35\\% cost savings, while maintaining an average optimality gap of 5\\%. Our proposed heuristics solve large-scale scenarios in under 0.1 seconds, highlighting their potential for real-time deployment in next-generation mobile networks."
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-12T15:46:53Z",
                "published_parsed": [
                    2025,
                    12,
                    12,
                    15,
                    46,
                    53,
                    4,
                    346,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI"
                },
                "authors": [
                    {
                        "name": "Gabriel Almeida"
                    },
                    {
                        "name": "João Paulo Esper"
                    },
                    {
                        "name": "Cleverson Nahum"
                    },
                    {
                        "name": "Aldebaro Klautau"
                    },
                    {
                        "name": "Kleber Vieira Cardoso"
                    }
                ],
                "author_detail": {
                    "name": "Kleber Vieira Cardoso"
                },
                "author": "Kleber Vieira Cardoso"
            },
            {
                "id": "http://arxiv.org/abs/2601.02091v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.02091v1",
                "title": "MCD-Net: A Lightweight Deep Learning Baseline for Optical-Only Moraine Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MCD-Net: A Lightweight Deep Learning Baseline for Optical-Only Moraine Segmentation"
                },
                "updated": "2026-01-05T13:18:11Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    13,
                    18,
                    11,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.02091v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.02091v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Glacial segmentation is essential for reconstructing past glacier dynamics and evaluating climate-driven landscape change. However, weak optical contrast and the limited availability of high-resolution DEMs hinder automated mapping. This study introduces the first large-scale optical-only moraine segmentation dataset, comprising 3,340 manually annotated high-resolution images from Google Earth covering glaciated regions of Sichuan and Yunnan, China. We develop MCD-Net, a lightweight baseline that integrates a MobileNetV2 encoder, a Convolutional Block Attention Module (CBAM), and a DeepLabV3+ decoder. Benchmarking against deeper backbones (ResNet152, Xception) shows that MCD-Net achieves 62.3\\% mean Intersection over Union (mIoU) and 72.8\\% Dice coefficient while reducing computational cost by more than 60\\%. Although ridge delineation remains constrained by sub-pixel width and spectral ambiguity, the results demonstrate that optical imagery alone can provide reliable moraine-body segmentation. The dataset and code are publicly available at https://github.com/Lyra-alpha/MCD-Net, establishing a reproducible benchmark for moraine-specific segmentation and offering a deployable baseline for high-altitude glacial monitoring.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Glacial segmentation is essential for reconstructing past glacier dynamics and evaluating climate-driven landscape change. However, weak optical contrast and the limited availability of high-resolution DEMs hinder automated mapping. This study introduces the first large-scale optical-only moraine segmentation dataset, comprising 3,340 manually annotated high-resolution images from Google Earth covering glaciated regions of Sichuan and Yunnan, China. We develop MCD-Net, a lightweight baseline that integrates a MobileNetV2 encoder, a Convolutional Block Attention Module (CBAM), and a DeepLabV3+ decoder. Benchmarking against deeper backbones (ResNet152, Xception) shows that MCD-Net achieves 62.3\\% mean Intersection over Union (mIoU) and 72.8\\% Dice coefficient while reducing computational cost by more than 60\\%. Although ridge delineation remains constrained by sub-pixel width and spectral ambiguity, the results demonstrate that optical imagery alone can provide reliable moraine-body segmentation. The dataset and code are publicly available at https://github.com/Lyra-alpha/MCD-Net, establishing a reproducible benchmark for moraine-specific segmentation and offering a deployable baseline for high-altitude glacial monitoring."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-05T13:18:11Z",
                "published_parsed": [
                    2026,
                    1,
                    5,
                    13,
                    18,
                    11,
                    0,
                    5,
                    0
                ],
                "arxiv_comment": "13 pages, 10 figures. This manuscript is under review at IEEE Transactions on Geoscience and Remote Sensing",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Zhehuan Cao"
                    },
                    {
                        "name": "Fiseha Berhanu Tesema"
                    },
                    {
                        "name": "Ping Fu"
                    },
                    {
                        "name": "Jianfeng Ren"
                    },
                    {
                        "name": "Ahmed Nasr"
                    }
                ],
                "author_detail": {
                    "name": "Ahmed Nasr"
                },
                "author": "Ahmed Nasr"
            },
            {
                "id": "http://arxiv.org/abs/2601.02082v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.02082v1",
                "title": "Realistic adversarial scenario generation via human-like pedestrian model for autonomous vehicle control parameter optimisation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Realistic adversarial scenario generation via human-like pedestrian model for autonomous vehicle control parameter optimisation"
                },
                "updated": "2026-01-05T13:10:32Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    13,
                    10,
                    32,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.02082v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.02082v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Autonomous vehicles (AVs) are rapidly advancing and are expected to play a central role in future mobility. Ensuring their safe deployment requires reliable interaction with other road users, not least pedestrians. Direct testing on public roads is costly and unsafe for rare but critical interactions, making simulation a practical alternative. Within simulation-based testing, adversarial scenarios are widely used to probe safety limits, but many prioritise difficulty over realism, producing exaggerated behaviours which may result in AV controllers that are overly conservative. We propose an alternative method, instead using a cognitively inspired pedestrian model featuring both inter-individual and intra-individual variability to generate behaviourally plausible adversarial scenarios. We provide a proof of concept demonstration of this method's potential for AV control optimisation, in closed-loop testing and tuning of an AV controller. Our results show that replacing the rule-based CARLA pedestrian with the human-like model yields more realistic gap acceptance patterns and smoother vehicle decelerations. Unsafe interactions occur only for certain pedestrian individuals and conditions, underscoring the importance of human variability in AV testing. Adversarial scenarios generated by this model can be used to optimise AV control towards safer and more efficient behaviour. Overall, this work illustrates how incorporating human-like road user models into simulation-based adversarial testing can enhance the credibility of AV evaluation and provide a practical basis to behaviourally informed controller optimisation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous vehicles (AVs) are rapidly advancing and are expected to play a central role in future mobility. Ensuring their safe deployment requires reliable interaction with other road users, not least pedestrians. Direct testing on public roads is costly and unsafe for rare but critical interactions, making simulation a practical alternative. Within simulation-based testing, adversarial scenarios are widely used to probe safety limits, but many prioritise difficulty over realism, producing exaggerated behaviours which may result in AV controllers that are overly conservative. We propose an alternative method, instead using a cognitively inspired pedestrian model featuring both inter-individual and intra-individual variability to generate behaviourally plausible adversarial scenarios. We provide a proof of concept demonstration of this method's potential for AV control optimisation, in closed-loop testing and tuning of an AV controller. Our results show that replacing the rule-based CARLA pedestrian with the human-like model yields more realistic gap acceptance patterns and smoother vehicle decelerations. Unsafe interactions occur only for certain pedestrian individuals and conditions, underscoring the importance of human variability in AV testing. Adversarial scenarios generated by this model can be used to optimise AV control towards safer and more efficient behaviour. Overall, this work illustrates how incorporating human-like road user models into simulation-based adversarial testing can enhance the credibility of AV evaluation and provide a practical basis to behaviourally informed controller optimisation."
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-05T13:10:32Z",
                "published_parsed": [
                    2026,
                    1,
                    5,
                    13,
                    10,
                    32,
                    0,
                    5,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC"
                },
                "authors": [
                    {
                        "name": "Yueyang Wang"
                    },
                    {
                        "name": "Mehmet Dogar"
                    },
                    {
                        "name": "Gustav Markkula"
                    }
                ],
                "author_detail": {
                    "name": "Gustav Markkula"
                },
                "author": "Gustav Markkula"
            },
            {
                "id": "http://arxiv.org/abs/2510.27052v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.27052v3",
                "title": "VISTA Score: Verification In Sequential Turn-based Assessment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VISTA Score: Verification In Sequential Turn-based Assessment"
                },
                "updated": "2026-01-05T13:07:39Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    13,
                    7,
                    39,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.27052v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.27052v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Hallucination--defined here as generating statements unsupported or contradicted by available evidence or conversational context--remains a major obstacle to deploying conversational AI systems in settings that demand factual reliability. Existing metrics either evaluate isolated responses or treat unverifiable content as errors, limiting their use for multi-turn dialogue. We introduce VISTA (Verification In Sequential Turn-based Assessment), a framework for evaluating conversational factuality through claim-level verification and sequential consistency tracking. VISTA decomposes each assistant turn into atomic factual claims, verifies them against trusted sources and dialogue history, and categorizes unverifiable statements (subjective, contradicted, lacking evidence, or abstaining). Across eight large language models and four dialogue factuality benchmarks (AIS, BEGIN, FAITHDIAL, and FADE), VISTA substantially improves hallucination detection over FACTSCORE and LLM-as-Judge baselines. Human evaluation confirms that VISTA's decomposition improves annotator agreement and reveals inconsistencies in existing benchmarks. By modeling factuality as a dynamic property of conversation, VISTA offers a more transparent, human-aligned measure of truthfulness in dialogue systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hallucination--defined here as generating statements unsupported or contradicted by available evidence or conversational context--remains a major obstacle to deploying conversational AI systems in settings that demand factual reliability. Existing metrics either evaluate isolated responses or treat unverifiable content as errors, limiting their use for multi-turn dialogue. We introduce VISTA (Verification In Sequential Turn-based Assessment), a framework for evaluating conversational factuality through claim-level verification and sequential consistency tracking. VISTA decomposes each assistant turn into atomic factual claims, verifies them against trusted sources and dialogue history, and categorizes unverifiable statements (subjective, contradicted, lacking evidence, or abstaining). Across eight large language models and four dialogue factuality benchmarks (AIS, BEGIN, FAITHDIAL, and FADE), VISTA substantially improves hallucination detection over FACTSCORE and LLM-as-Judge baselines. Human evaluation confirms that VISTA's decomposition improves annotator agreement and reveals inconsistencies in existing benchmarks. By modeling factuality as a dynamic property of conversation, VISTA offers a more transparent, human-aligned measure of truthfulness in dialogue systems."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-30T23:45:13Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    23,
                    45,
                    13,
                    3,
                    303,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Ashley Lewis"
                    },
                    {
                        "name": "Andrew Perrault"
                    },
                    {
                        "name": "Eric Fosler-Lussier"
                    },
                    {
                        "name": "Michael White"
                    }
                ],
                "author_detail": {
                    "name": "Michael White"
                },
                "author": "Michael White"
            },
            {
                "id": "http://arxiv.org/abs/2601.02078v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.02078v1",
                "title": "Genie Sim 3.0 : A High-Fidelity Comprehensive Simulation Platform for Humanoid Robot",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Genie Sim 3.0 : A High-Fidelity Comprehensive Simulation Platform for Humanoid Robot"
                },
                "updated": "2026-01-05T12:59:39Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    12,
                    59,
                    39,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.02078v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.02078v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The development of robust and generalizable robot learning models is critically contingent upon the availability of large-scale, diverse training data and reliable evaluation benchmarks. Collecting data in the physical world poses prohibitive costs and scalability challenges, and prevailing simulation benchmarks frequently suffer from fragmentation, narrow scope, or insufficient fidelity to enable effective sim-to-real transfer. To address these challenges, we introduce Genie Sim 3.0, a unified simulation platform for robotic manipulation. We present Genie Sim Generator, a large language model (LLM)-powered tool that constructs high-fidelity scenes from natural language instructions. Its principal strength resides in rapid and multi-dimensional generalization, facilitating the synthesis of diverse environments to support scalable data collection and robust policy evaluation. We introduce the first benchmark that pioneers the application of LLM for automated evaluation. It leverages LLM to mass-generate evaluation scenarios and employs Vision-Language Model (VLM) to establish an automated assessment pipeline. We also release an open-source dataset comprising more than 10,000 hours of synthetic data across over 200 tasks. Through systematic experimentation, we validate the robust zero-shot sim-to-real transfer capability of our open-source dataset, demonstrating that synthetic data can server as an effective substitute for real-world data under controlled conditions for scalable policy training. For code and dataset details, please refer to: https://github.com/AgibotTech/genie_sim.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The development of robust and generalizable robot learning models is critically contingent upon the availability of large-scale, diverse training data and reliable evaluation benchmarks. Collecting data in the physical world poses prohibitive costs and scalability challenges, and prevailing simulation benchmarks frequently suffer from fragmentation, narrow scope, or insufficient fidelity to enable effective sim-to-real transfer. To address these challenges, we introduce Genie Sim 3.0, a unified simulation platform for robotic manipulation. We present Genie Sim Generator, a large language model (LLM)-powered tool that constructs high-fidelity scenes from natural language instructions. Its principal strength resides in rapid and multi-dimensional generalization, facilitating the synthesis of diverse environments to support scalable data collection and robust policy evaluation. We introduce the first benchmark that pioneers the application of LLM for automated evaluation. It leverages LLM to mass-generate evaluation scenarios and employs Vision-Language Model (VLM) to establish an automated assessment pipeline. We also release an open-source dataset comprising more than 10,000 hours of synthetic data across over 200 tasks. Through systematic experimentation, we validate the robust zero-shot sim-to-real transfer capability of our open-source dataset, demonstrating that synthetic data can server as an effective substitute for real-world data under controlled conditions for scalable policy training. For code and dataset details, please refer to: https://github.com/AgibotTech/genie_sim."
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-05T12:59:39Z",
                "published_parsed": [
                    2026,
                    1,
                    5,
                    12,
                    59,
                    39,
                    0,
                    5,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO"
                },
                "authors": [
                    {
                        "name": "Chenghao Yin"
                    },
                    {
                        "name": "Da Huang"
                    },
                    {
                        "name": "Di Yang"
                    },
                    {
                        "name": "Jichao Wang"
                    },
                    {
                        "name": "Nanshu Zhao"
                    },
                    {
                        "name": "Chen Xu"
                    },
                    {
                        "name": "Wenjun Sun"
                    },
                    {
                        "name": "Linjie Hou"
                    },
                    {
                        "name": "Zhijun Li"
                    },
                    {
                        "name": "Junhui Wu"
                    },
                    {
                        "name": "Zhaobo Liu"
                    },
                    {
                        "name": "Zhen Xiao"
                    },
                    {
                        "name": "Sheng Zhang"
                    },
                    {
                        "name": "Lei Bao"
                    },
                    {
                        "name": "Rui Feng"
                    },
                    {
                        "name": "Zhenquan Pang"
                    },
                    {
                        "name": "Jiayu Li"
                    },
                    {
                        "name": "Qian Wang"
                    },
                    {
                        "name": "Maoqing Yao"
                    }
                ],
                "author_detail": {
                    "name": "Maoqing Yao"
                },
                "author": "Maoqing Yao"
            },
            {
                "id": "http://arxiv.org/abs/2601.02075v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.02075v1",
                "title": "MDAgent2: Large Language Model for Code Generation and Knowledge Q&A in Molecular Dynamics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MDAgent2: Large Language Model for Code Generation and Knowledge Q&A in Molecular Dynamics"
                },
                "updated": "2026-01-05T12:56:51Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    12,
                    56,
                    51,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.02075v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.02075v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Molecular dynamics (MD) simulations are essential for understanding atomic-scale behaviors in materials science, yet writing LAMMPS scripts remains highly specialized and time-consuming tasks. Although LLMs show promise in code generation and domain-specific question answering, their performance in MD scenarios is limited by scarce domain data, the high deployment cost of state-of-the-art LLMs, and low code executability. Building upon our prior MDAgent, we present MDAgent2, the first end-to-end framework capable of performing both knowledge Q&A and code generation within the MD domain. We construct a domain-specific data-construction pipeline that yields three high-quality datasets spanning MD knowledge, question answering, and code generation. Based on these datasets, we adopt a three stage post-training strategy--continued pre-training (CPT), supervised fine-tuning (SFT), and reinforcement learning (RL)--to train two domain-adapted models, MD-Instruct and MD-Code. Furthermore, we introduce MD-GRPO, a closed-loop RL method that leverages simulation outcomes as reward signals and recycles low-reward trajectories for continual refinement. We further build MDAgent2-RUNTIME, a deployable multi-agent system that integrates code generation, execution, evaluation, and self-correction. Together with MD-EvalBench proposed in this work, the first benchmark for LAMMPS code generation and question answering, our models and system achieve performance surpassing several strong baselines.This work systematically demonstrates the adaptability and generalization capability of large language models in industrial simulation tasks, laying a methodological foundation for automatic code generation in AI for Science and industrial-scale simulations. URL: https://github.com/FredericVAN/PKU_MDAgent2",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Molecular dynamics (MD) simulations are essential for understanding atomic-scale behaviors in materials science, yet writing LAMMPS scripts remains highly specialized and time-consuming tasks. Although LLMs show promise in code generation and domain-specific question answering, their performance in MD scenarios is limited by scarce domain data, the high deployment cost of state-of-the-art LLMs, and low code executability. Building upon our prior MDAgent, we present MDAgent2, the first end-to-end framework capable of performing both knowledge Q&A and code generation within the MD domain. We construct a domain-specific data-construction pipeline that yields three high-quality datasets spanning MD knowledge, question answering, and code generation. Based on these datasets, we adopt a three stage post-training strategy--continued pre-training (CPT), supervised fine-tuning (SFT), and reinforcement learning (RL)--to train two domain-adapted models, MD-Instruct and MD-Code. Furthermore, we introduce MD-GRPO, a closed-loop RL method that leverages simulation outcomes as reward signals and recycles low-reward trajectories for continual refinement. We further build MDAgent2-RUNTIME, a deployable multi-agent system that integrates code generation, execution, evaluation, and self-correction. Together with MD-EvalBench proposed in this work, the first benchmark for LAMMPS code generation and question answering, our models and system achieve performance surpassing several strong baselines.This work systematically demonstrates the adaptability and generalization capability of large language models in industrial simulation tasks, laying a methodological foundation for automatic code generation in AI for Science and industrial-scale simulations. URL: https://github.com/FredericVAN/PKU_MDAgent2"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-05T12:56:51Z",
                "published_parsed": [
                    2026,
                    1,
                    5,
                    12,
                    56,
                    51,
                    0,
                    5,
                    0
                ],
                "arxiv_comment": "24 pages,4 figures",
                "arxiv_primary_category": {
                    "term": "cs.CE"
                },
                "authors": [
                    {
                        "name": "Zhuofan Shi"
                    },
                    {
                        "name": "Hubao A"
                    },
                    {
                        "name": "Yufei Shao"
                    },
                    {
                        "name": "Mengyan Dai"
                    },
                    {
                        "name": "Yadong Yu"
                    },
                    {
                        "name": "Pan Xiang"
                    },
                    {
                        "name": "Dongliang Huang"
                    },
                    {
                        "name": "Hongxu An"
                    },
                    {
                        "name": "Chunxiao Xin"
                    },
                    {
                        "name": "Haiyang Shen"
                    },
                    {
                        "name": "Zhenyu Wang"
                    },
                    {
                        "name": "Yunshan Na"
                    },
                    {
                        "name": "Gang Huang"
                    },
                    {
                        "name": "Xiang Jing"
                    }
                ],
                "author_detail": {
                    "name": "Xiang Jing"
                },
                "author": "Xiang Jing"
            },
            {
                "id": "http://arxiv.org/abs/2503.21408v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2503.21408v2",
                "title": "VALLR: Visual ASR Language Model for Lip Reading",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VALLR: Visual ASR Language Model for Lip Reading"
                },
                "updated": "2026-01-05T12:55:42Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    12,
                    55,
                    42,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2503.21408v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2503.21408v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Lip Reading, or Visual Automatic Speech Recognition (V-ASR), is a complex task requiring the interpretation of spoken language exclusively from visual cues, primarily lip movements and facial expressions. This task is especially challenging due to the absence of auditory information and the inherent ambiguity when visually distinguishing phonemes that have overlapping visemes where different phonemes appear identical on the lips. Current methods typically attempt to predict words or characters directly from these visual cues, but this approach frequently encounters high error rates due to coarticulation effects and viseme ambiguity. We propose a novel two-stage, phoneme-centric framework for Visual Automatic Speech Recognition (V-ASR) that addresses these longstanding challenges. First, our model predicts a compact sequence of phonemes from visual inputs using a Video Transformer with a CTC head, thereby reducing the task complexity and achieving robust speaker invariance. This phoneme output then serves as the input to a fine-tuned Large Language Model (LLM), which reconstructs coherent words and sentences by leveraging broader linguistic context. Unlike existing methods that either predict words directly-often faltering on visually similar phonemes-or rely on large-scale multimodal pre-training, our approach explicitly encodes intermediate linguistic structure while remaining highly data efficient. We demonstrate state-of-the-art performance on two challenging datasets, LRS2 and LRS3, where our method achieves significant reductions in Word Error Rate (WER) achieving a SOTA WER of 18.7 on LRS3 despite using 99.4% less labelled data than the next best approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lip Reading, or Visual Automatic Speech Recognition (V-ASR), is a complex task requiring the interpretation of spoken language exclusively from visual cues, primarily lip movements and facial expressions. This task is especially challenging due to the absence of auditory information and the inherent ambiguity when visually distinguishing phonemes that have overlapping visemes where different phonemes appear identical on the lips. Current methods typically attempt to predict words or characters directly from these visual cues, but this approach frequently encounters high error rates due to coarticulation effects and viseme ambiguity. We propose a novel two-stage, phoneme-centric framework for Visual Automatic Speech Recognition (V-ASR) that addresses these longstanding challenges. First, our model predicts a compact sequence of phonemes from visual inputs using a Video Transformer with a CTC head, thereby reducing the task complexity and achieving robust speaker invariance. This phoneme output then serves as the input to a fine-tuned Large Language Model (LLM), which reconstructs coherent words and sentences by leveraging broader linguistic context. Unlike existing methods that either predict words directly-often faltering on visually similar phonemes-or rely on large-scale multimodal pre-training, our approach explicitly encodes intermediate linguistic structure while remaining highly data efficient. We demonstrate state-of-the-art performance on two challenging datasets, LRS2 and LRS3, where our method achieves significant reductions in Word Error Rate (WER) achieving a SOTA WER of 18.7 on LRS3 despite using 99.4% less labelled data than the next best approach."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-03-27T11:52:08Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    11,
                    52,
                    8,
                    3,
                    86,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Marshall Thomas"
                    },
                    {
                        "name": "Edward Fish"
                    },
                    {
                        "name": "Richard Bowden"
                    }
                ],
                "author_detail": {
                    "name": "Richard Bowden"
                },
                "author": "Richard Bowden"
            },
            {
                "id": "http://arxiv.org/abs/2601.02071v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.02071v1",
                "title": "FormuLLA: A Large Language Model Approach to Generating Novel 3D Printable Formulations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FormuLLA: A Large Language Model Approach to Generating Novel 3D Printable Formulations"
                },
                "updated": "2026-01-05T12:50:50Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    12,
                    50,
                    50,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.02071v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.02071v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Pharmaceutical three-dimensional (3D) printing is an advanced fabrication technology with the potential to enable truly personalised dosage forms. Recent studies have integrated artificial intelligence (AI) to accelerate formulation and process development, drastically transforming current approaches to pharmaceutical 3D printing. To date, most AI-driven efforts remain narrowly focused, while failing to account for the broader formulation challenges inherent to the technology. Recent advances in AI have introduced artificial general intelligence concepts, wherein systems extend beyond conventional predictive modelling toward more generalised, human-like reasoning. In this work, we investigate the application of large language models (LLMs), fine-tuned on a fused deposition modelling (FDM) dataset comprising over 1400 formulations, to recommend suitable excipients based on active pharmaceutical ingredient (API) dose, and predict filament mechanical properties. Four LLM architectures were fine-tuned, with systematic evaluation of both fine-tuning and generative parameter configurations. Our results demonstrate that Llama2 was best suited for recommending excipients for FDM formulations. Additionally, model selection and parameterisation significantly influence performance, with smaller LLMs exhibiting instances of catastrophic forgetting. Furthermore, we demonstrate: (i) even with relatively small dataset of over 1400 formulations, it can lead to model catastrophic forgetting; (ii) standard LLM metrics only evaluate linguistic performance but not formulation processability; and (iii) LLMs trained on biomedically-related data do not always produce the best results. Addressing these challenges is essential to advancing LLMs beyond linguistic proficiency and toward reliable systems for pharmaceutical formulation development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pharmaceutical three-dimensional (3D) printing is an advanced fabrication technology with the potential to enable truly personalised dosage forms. Recent studies have integrated artificial intelligence (AI) to accelerate formulation and process development, drastically transforming current approaches to pharmaceutical 3D printing. To date, most AI-driven efforts remain narrowly focused, while failing to account for the broader formulation challenges inherent to the technology. Recent advances in AI have introduced artificial general intelligence concepts, wherein systems extend beyond conventional predictive modelling toward more generalised, human-like reasoning. In this work, we investigate the application of large language models (LLMs), fine-tuned on a fused deposition modelling (FDM) dataset comprising over 1400 formulations, to recommend suitable excipients based on active pharmaceutical ingredient (API) dose, and predict filament mechanical properties. Four LLM architectures were fine-tuned, with systematic evaluation of both fine-tuning and generative parameter configurations. Our results demonstrate that Llama2 was best suited for recommending excipients for FDM formulations. Additionally, model selection and parameterisation significantly influence performance, with smaller LLMs exhibiting instances of catastrophic forgetting. Furthermore, we demonstrate: (i) even with relatively small dataset of over 1400 formulations, it can lead to model catastrophic forgetting; (ii) standard LLM metrics only evaluate linguistic performance but not formulation processability; and (iii) LLMs trained on biomedically-related data do not always produce the best results. Addressing these challenges is essential to advancing LLMs beyond linguistic proficiency and toward reliable systems for pharmaceutical formulation development."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-05T12:50:50Z",
                "published_parsed": [
                    2026,
                    1,
                    5,
                    12,
                    50,
                    50,
                    0,
                    5,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Adeshola Okubena"
                    },
                    {
                        "name": "Yusuf Ali Mohammed"
                    },
                    {
                        "name": "Moe Elbadawi"
                    }
                ],
                "author_detail": {
                    "name": "Moe Elbadawi"
                },
                "author": "Moe Elbadawi"
            },
            {
                "id": "http://arxiv.org/abs/2512.09972v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.09972v3",
                "title": "SIP-BMM: Constructing the Capability--Efficiency Pareto Set for LLMs via Structural Importance Prior Bayesian Model Merging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SIP-BMM: Constructing the Capability--Efficiency Pareto Set for LLMs via Structural Importance Prior Bayesian Model Merging"
                },
                "updated": "2026-01-05T12:45:09Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    12,
                    45,
                    9,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.09972v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.09972v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Constructing a Pareto set is pivotal for navigating the capability--efficiency trade-offs in Large Language Models (LLMs). However, existing merging techniques remain inadequate for this task. Coarse-grained, model-level methods yield only a sparse set of suboptimal solutions, while fine-grained, layer-wise approaches suffer from the curse of dimensionality, rendering the search space computationally intractable. To resolve this dichotomy, we propose Structural Importance Prior Bayesian Model Merging (SIP-BMM), a framework that automatically constructs the LLM Pareto set. SIP-BMM renders high-dimensional layer-wise search tractable by introducing an importance-aware Sparse Axis-Aligned Subspace Bayesian Optimization (SAASBO) strategy. By leveraging a structural importance prior derived from task-vector differences, our method guides SAASBO to automatically identify critical layers, thereby dramatically reducing the effective dimensionality without sacrificing the granularity of full-model control. The entire process is automated within an evolutionary loop driven by the Log-Noisy Expected Hypervolume Improvement ($q$NEHVI) acquisition function. Experiments demonstrate that SIP-BMM discovers a stronger and denser Pareto front than competitive baselines, enabling agile model selection tailored to diverse operational constraints. Code is available at: https://github.com/MiLab-HITSZ/2026-SIPBMM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Constructing a Pareto set is pivotal for navigating the capability--efficiency trade-offs in Large Language Models (LLMs). However, existing merging techniques remain inadequate for this task. Coarse-grained, model-level methods yield only a sparse set of suboptimal solutions, while fine-grained, layer-wise approaches suffer from the curse of dimensionality, rendering the search space computationally intractable. To resolve this dichotomy, we propose Structural Importance Prior Bayesian Model Merging (SIP-BMM), a framework that automatically constructs the LLM Pareto set. SIP-BMM renders high-dimensional layer-wise search tractable by introducing an importance-aware Sparse Axis-Aligned Subspace Bayesian Optimization (SAASBO) strategy. By leveraging a structural importance prior derived from task-vector differences, our method guides SAASBO to automatically identify critical layers, thereby dramatically reducing the effective dimensionality without sacrificing the granularity of full-model control. The entire process is automated within an evolutionary loop driven by the Log-Noisy Expected Hypervolume Improvement ($q$NEHVI) acquisition function. Experiments demonstrate that SIP-BMM discovers a stronger and denser Pareto front than competitive baselines, enabling agile model selection tailored to diverse operational constraints. Code is available at: https://github.com/MiLab-HITSZ/2026-SIPBMM."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-10T15:32:56Z",
                "published_parsed": [
                    2025,
                    12,
                    10,
                    15,
                    32,
                    56,
                    2,
                    344,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Kesheng Chen"
                    },
                    {
                        "name": "Yamin Hu"
                    },
                    {
                        "name": "Zhenqian Zhu"
                    },
                    {
                        "name": "Wenjian Luo"
                    },
                    {
                        "name": "Yiya Diao"
                    }
                ],
                "author_detail": {
                    "name": "Yiya Diao"
                },
                "author": "Yiya Diao"
            },
            {
                "id": "http://arxiv.org/abs/2601.02065v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.02065v1",
                "title": "Cost-Efficient Cross-Lingual Retrieval-Augmented Generation for Low-Resource Languages: A Case Study in Bengali Agricultural Advisory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cost-Efficient Cross-Lingual Retrieval-Augmented Generation for Low-Resource Languages: A Case Study in Bengali Agricultural Advisory"
                },
                "updated": "2026-01-05T12:41:44Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    12,
                    41,
                    44,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.02065v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.02065v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Access to reliable agricultural advisory remains limited in many developing regions due to a persistent language barrier: authoritative agricultural manuals are predominantly written in English, while farmers primarily communicate in low-resource local languages such as Bengali. Although recent advances in Large Language Models (LLMs) enable natural language interaction, direct generation in low-resource languages often exhibits poor fluency and factual inconsistency, while cloud-based solutions remain cost-prohibitive. This paper presents a cost-efficient, cross-lingual Retrieval-Augmented Generation (RAG) framework for Bengali agricultural advisory that emphasizes factual grounding and practical deployability. The proposed system adopts a translation-centric architecture in which Bengali user queries are translated into English, enriched through domain-specific keyword injection to align colloquial farmer terminology with scientific nomenclature, and answered via dense vector retrieval over a curated corpus of English agricultural manuals (FAO, IRRI). The generated English response is subsequently translated back into Bengali to ensure accessibility. The system is implemented entirely using open-source models and operates on consumer-grade hardware without reliance on paid APIs. Experimental evaluation demonstrates reliable source-grounded responses, robust rejection of out-of-domain queries, and an average end-to-end latency below 20 seconds. The results indicate that cross-lingual retrieval combined with controlled translation offers a practical and scalable solution for agricultural knowledge access in low-resource language settings",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Access to reliable agricultural advisory remains limited in many developing regions due to a persistent language barrier: authoritative agricultural manuals are predominantly written in English, while farmers primarily communicate in low-resource local languages such as Bengali. Although recent advances in Large Language Models (LLMs) enable natural language interaction, direct generation in low-resource languages often exhibits poor fluency and factual inconsistency, while cloud-based solutions remain cost-prohibitive. This paper presents a cost-efficient, cross-lingual Retrieval-Augmented Generation (RAG) framework for Bengali agricultural advisory that emphasizes factual grounding and practical deployability. The proposed system adopts a translation-centric architecture in which Bengali user queries are translated into English, enriched through domain-specific keyword injection to align colloquial farmer terminology with scientific nomenclature, and answered via dense vector retrieval over a curated corpus of English agricultural manuals (FAO, IRRI). The generated English response is subsequently translated back into Bengali to ensure accessibility. The system is implemented entirely using open-source models and operates on consumer-grade hardware without reliance on paid APIs. Experimental evaluation demonstrates reliable source-grounded responses, robust rejection of out-of-domain queries, and an average end-to-end latency below 20 seconds. The results indicate that cross-lingual retrieval combined with controlled translation offers a practical and scalable solution for agricultural knowledge access in low-resource language settings"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-05T12:41:44Z",
                "published_parsed": [
                    2026,
                    1,
                    5,
                    12,
                    41,
                    44,
                    0,
                    5,
                    0
                ],
                "arxiv_comment": "5 pages, 3 figures, 1 table",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Md. Asif Hossain"
                    },
                    {
                        "name": "Nabil Subhan"
                    },
                    {
                        "name": "Mantasha Rahman Mahi"
                    },
                    {
                        "name": "Jannatul Ferdous Nabila"
                    }
                ],
                "author_detail": {
                    "name": "Jannatul Ferdous Nabila"
                },
                "author": "Jannatul Ferdous Nabila"
            },
            {
                "id": "http://arxiv.org/abs/2601.02061v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.02061v1",
                "title": "Higher-Order Action Regularization in Deep Reinforcement Learning: From Continuous Control to Building Energy Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Higher-Order Action Regularization in Deep Reinforcement Learning: From Continuous Control to Building Energy Management"
                },
                "updated": "2026-01-05T12:35:33Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    12,
                    35,
                    33,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.02061v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.02061v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Deep reinforcement learning agents often exhibit erratic, high-frequency control behaviors that hinder real-world deployment due to excessive energy consumption and mechanical wear. We systematically investigate action smoothness regularization through higher-order derivative penalties, progressing from theoretical understanding in continuous control benchmarks to practical validation in building energy management. Our comprehensive evaluation across four continuous control environments demonstrates that third-order derivative penalties (jerk minimization) consistently achieve superior smoothness while maintaining competitive performance. We extend these findings to HVAC control systems where smooth policies reduce equipment switching by 60%, translating to significant operational benefits. Our work establishes higher-order action regularization as an effective bridge between RL optimization and operational constraints in energy-critical applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep reinforcement learning agents often exhibit erratic, high-frequency control behaviors that hinder real-world deployment due to excessive energy consumption and mechanical wear. We systematically investigate action smoothness regularization through higher-order derivative penalties, progressing from theoretical understanding in continuous control benchmarks to practical validation in building energy management. Our comprehensive evaluation across four continuous control environments demonstrates that third-order derivative penalties (jerk minimization) consistently achieve superior smoothness while maintaining competitive performance. We extend these findings to HVAC control systems where smooth policies reduce equipment switching by 60%, translating to significant operational benefits. Our work establishes higher-order action regularization as an effective bridge between RL optimization and operational constraints in energy-critical applications."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-05T12:35:33Z",
                "published_parsed": [
                    2026,
                    1,
                    5,
                    12,
                    35,
                    33,
                    0,
                    5,
                    0
                ],
                "arxiv_comment": "6 pages, accepted at NeurIPS workshop 2025",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Faizan Ahmed"
                    },
                    {
                        "name": "Aniket Dixit"
                    },
                    {
                        "name": "James Brusey"
                    }
                ],
                "author_detail": {
                    "name": "James Brusey"
                },
                "author": "James Brusey"
            },
            {
                "id": "http://arxiv.org/abs/2601.02060v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.02060v1",
                "title": "Perish or Flourish? A Holistic Evaluation of Large Language Models for Code Generation in Functional Programming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Perish or Flourish? A Holistic Evaluation of Large Language Models for Code Generation in Functional Programming"
                },
                "updated": "2026-01-05T12:33:37Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    12,
                    33,
                    37,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.02060v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.02060v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Functional programming provides strong foundations for developing reliable and secure software systems, yet its adoption remains not widespread due to the steep learning curve. Recent advances in Large Language Models (LLMs) for code generation present new opportunities to lower these barriers. However, extensive evaluations of LLMs largely focus on imperative programming languages, and their capabilities in functional programming languages (FP) remain underexplored. To address this gap, we introduce FPEval, a holistic evaluation framework built on FPBench, a new benchmark of 721 programming tasks across three difficulty levels on three mainstream FP languages: Haskell, Ocaml and Scala. FPEval provides compehensive evaluation infrastructures with both test validations with comprehensive test suites and static analysis tools to assess both functional correctness and code style and maintainability. Using this framework, we evaluate state-of-the-art LLMs, including GPT-3.5, GPT-4o, and GPT-5, for code generation in functional programming languages and Java as an imperative baseline. Our results demonstrate that LLM performance in functional programming improves substantially with model advancement; however, error rates remain significantly higher in purely functional languages (Haskell and OCaml) than in hybrid (Scala) or imperative (Java) languages. Moreover, LLMs frequently generate non-idiomatic functional code that follows imperative patterns, raising concerns about code style and long-term maintainability. Finally, we show that LLMs can partially self-repair both correctness and quality issues when provided with static analysis feedback and hand-crafted instructions for common types of issues.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Functional programming provides strong foundations for developing reliable and secure software systems, yet its adoption remains not widespread due to the steep learning curve. Recent advances in Large Language Models (LLMs) for code generation present new opportunities to lower these barriers. However, extensive evaluations of LLMs largely focus on imperative programming languages, and their capabilities in functional programming languages (FP) remain underexplored. To address this gap, we introduce FPEval, a holistic evaluation framework built on FPBench, a new benchmark of 721 programming tasks across three difficulty levels on three mainstream FP languages: Haskell, Ocaml and Scala. FPEval provides compehensive evaluation infrastructures with both test validations with comprehensive test suites and static analysis tools to assess both functional correctness and code style and maintainability. Using this framework, we evaluate state-of-the-art LLMs, including GPT-3.5, GPT-4o, and GPT-5, for code generation in functional programming languages and Java as an imperative baseline. Our results demonstrate that LLM performance in functional programming improves substantially with model advancement; however, error rates remain significantly higher in purely functional languages (Haskell and OCaml) than in hybrid (Scala) or imperative (Java) languages. Moreover, LLMs frequently generate non-idiomatic functional code that follows imperative patterns, raising concerns about code style and long-term maintainability. Finally, we show that LLMs can partially self-repair both correctness and quality issues when provided with static analysis feedback and hand-crafted instructions for common types of issues."
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-05T12:33:37Z",
                "published_parsed": [
                    2026,
                    1,
                    5,
                    12,
                    33,
                    37,
                    0,
                    5,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL"
                },
                "authors": [
                    {
                        "name": "Nguyet-Anh H. Lang"
                    },
                    {
                        "name": "Eric Lang"
                    },
                    {
                        "name": "Thanh Le-Cong"
                    },
                    {
                        "name": "Bach Le"
                    },
                    {
                        "name": "Quyet-Thang Huynh"
                    }
                ],
                "author_detail": {
                    "name": "Quyet-Thang Huynh"
                },
                "author": "Quyet-Thang Huynh"
            },
            {
                "id": "http://arxiv.org/abs/2601.02053v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.02053v1",
                "title": "Ageing Monitoring for Commercial Microcontrollers Based on Timing Windows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ageing Monitoring for Commercial Microcontrollers Based on Timing Windows"
                },
                "updated": "2026-01-05T12:22:27Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    12,
                    22,
                    27,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.02053v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.02053v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Microcontrollers are increasingly present in embedded deployments and dependable applications, for which malfunctions due to hardware ageing can have severe impact. The lack of deployable techniques for ageing monitoring on these devices has spread the application of guard bands to prevent timing errors due to degradation. Applying this static technique can limit performance and lead to sudden failures as devices age. In this paper, we follow a software-based self-testing approach to design monitoring of hardware degradation for microcontrollers. Deployable in the field, our technique leverages timing windows of variable lengths to determine the maximum operational frequency of the devices. We empirically validate the method on real hardware and find that it consistently detects temperature-induced degradations in maximum operating frequency of up to 13.79 % across devices for 60 °C temperature increase.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Microcontrollers are increasingly present in embedded deployments and dependable applications, for which malfunctions due to hardware ageing can have severe impact. The lack of deployable techniques for ageing monitoring on these devices has spread the application of guard bands to prevent timing errors due to degradation. Applying this static technique can limit performance and lead to sudden failures as devices age. In this paper, we follow a software-based self-testing approach to design monitoring of hardware degradation for microcontrollers. Deployable in the field, our technique leverages timing windows of variable lengths to determine the maximum operational frequency of the devices. We empirically validate the method on real hardware and find that it consistently detects temperature-induced degradations in maximum operating frequency of up to 13.79 % across devices for 60 °C temperature increase."
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-05T12:22:27Z",
                "published_parsed": [
                    2026,
                    1,
                    5,
                    12,
                    22,
                    27,
                    0,
                    5,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR"
                },
                "authors": [
                    {
                        "name": "Leandro Lanzieri"
                    },
                    {
                        "name": "Jiri Kral"
                    },
                    {
                        "name": "Goerschwin Fey"
                    },
                    {
                        "name": "Holger Schlarb"
                    },
                    {
                        "name": "Thomas C. Schmidt"
                    }
                ],
                "author_detail": {
                    "name": "Thomas C. Schmidt"
                },
                "author": "Thomas C. Schmidt"
            },
            {
                "id": "http://arxiv.org/abs/2601.02050v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.02050v1",
                "title": "Explore the Ideology of Deep Learning in ENSO Forecasts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explore the Ideology of Deep Learning in ENSO Forecasts"
                },
                "updated": "2026-01-05T12:15:39Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    12,
                    15,
                    39,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.02050v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.02050v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The El Ni{~n}o-Southern Oscillation (ENSO) exerts profound influence on global climate variability, yet its prediction remains a grand challenge. Recent advances in deep learning have significantly improved forecasting skill, but the opacity of these models hampers scientific trust and operational deployment. Here, we introduce a mathematically grounded interpretability framework based on bounded variation function. By rescuing the \"dead\" neurons from the saturation zone of the activation function, we enhance the model's expressive capacity. Our analysis reveals that ENSO predictability emerges dominantly from the tropical Pacific, with contributions from the Indian and Atlantic Oceans, consistent with physical understanding. Controlled experiments affirm the robustness of our method and its alignment with established predictors. Notably, we probe the persistent Spring Predictability Barrier (SPB), finding that despite expanded sensitivity during spring, predictive performance declines-likely due to suboptimal variable selection. These results suggest that incorporating additional ocean-atmosphere variables may help transcend SPB limitations and advance long-range ENSO prediction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The El Ni{~n}o-Southern Oscillation (ENSO) exerts profound influence on global climate variability, yet its prediction remains a grand challenge. Recent advances in deep learning have significantly improved forecasting skill, but the opacity of these models hampers scientific trust and operational deployment. Here, we introduce a mathematically grounded interpretability framework based on bounded variation function. By rescuing the \"dead\" neurons from the saturation zone of the activation function, we enhance the model's expressive capacity. Our analysis reveals that ENSO predictability emerges dominantly from the tropical Pacific, with contributions from the Indian and Atlantic Oceans, consistent with physical understanding. Controlled experiments affirm the robustness of our method and its alignment with established predictors. Notably, we probe the persistent Spring Predictability Barrier (SPB), finding that despite expanded sensitivity during spring, predictive performance declines-likely due to suboptimal variable selection. These results suggest that incorporating additional ocean-atmosphere variables may help transcend SPB limitations and advance long-range ENSO prediction."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-05T12:15:39Z",
                "published_parsed": [
                    2026,
                    1,
                    5,
                    12,
                    15,
                    39,
                    0,
                    5,
                    0
                ],
                "arxiv_comment": "5 figures. Code available at https://github.com/liuxingguo9349/pptv-enso-env",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Yanhai Gan"
                    },
                    {
                        "name": "Yipeng Chen"
                    },
                    {
                        "name": "Ning Li"
                    },
                    {
                        "name": "Xingguo Liu"
                    },
                    {
                        "name": "Junyu Dong"
                    },
                    {
                        "name": "Xianyao Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xianyao Chen"
                },
                "author": "Xianyao Chen"
            },
            {
                "id": "http://arxiv.org/abs/2601.02045v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.02045v1",
                "title": "The New Compiler Stack: A Survey on the Synergy of LLMs and Compilers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The New Compiler Stack: A Survey on the Synergy of LLMs and Compilers"
                },
                "updated": "2026-01-05T12:02:57Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    12,
                    2,
                    57,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.02045v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.02045v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.1007/s42514-025-00270-x",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "This survey has provided a systematic overview of the emerging field of LLM-enabled compilation by addressing several key research questions. We first answered how LLMs are being integrated by proposing a comprehensive, multi-dimensional taxonomy that categorizes works based on their Design Philosophy (Selector, Translator, Generator), LLM Methodology, their operational Level of Code Abstraction, and the specific Task Type they address. In answering what advancements these approaches offer, we identified three primary benefits: the democratization of compiler development, the discovery of novel optimization strategies, and the broadening of the compiler's traditional scope. Finally, in addressing the field's challenges and opportunities, we highlighted the critical hurdles of ensuring correctness and achieving scalability, while identifying the development of hybrid systems as the most promising path forward. By providing these answers, this survey serves as a foundational roadmap for researchers and practitioners, charting the course for a new generation of LLM-powered, intelligent, adaptive and synergistic compilation tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This survey has provided a systematic overview of the emerging field of LLM-enabled compilation by addressing several key research questions. We first answered how LLMs are being integrated by proposing a comprehensive, multi-dimensional taxonomy that categorizes works based on their Design Philosophy (Selector, Translator, Generator), LLM Methodology, their operational Level of Code Abstraction, and the specific Task Type they address. In answering what advancements these approaches offer, we identified three primary benefits: the democratization of compiler development, the discovery of novel optimization strategies, and the broadening of the compiler's traditional scope. Finally, in addressing the field's challenges and opportunities, we highlighted the critical hurdles of ensuring correctness and achieving scalability, while identifying the development of hybrid systems as the most promising path forward. By providing these answers, this survey serves as a foundational roadmap for researchers and practitioners, charting the course for a new generation of LLM-powered, intelligent, adaptive and synergistic compilation tools."
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-05T12:02:57Z",
                "published_parsed": [
                    2026,
                    1,
                    5,
                    12,
                    2,
                    57,
                    0,
                    5,
                    0
                ],
                "arxiv_comment": "Accepted by CCF Transactions on High Performance Computing",
                "arxiv_primary_category": {
                    "term": "cs.PL"
                },
                "authors": [
                    {
                        "name": "Shuoming Zhang"
                    },
                    {
                        "name": "Jiacheng Zhao"
                    },
                    {
                        "name": "Qiuchu Yu"
                    },
                    {
                        "name": "Chunwei Xia"
                    },
                    {
                        "name": "Zheng Wang"
                    },
                    {
                        "name": "Xiaobing Feng"
                    },
                    {
                        "name": "Huimin Cui"
                    }
                ],
                "author_detail": {
                    "name": "Huimin Cui"
                },
                "author": "Huimin Cui",
                "arxiv_doi": "10.1007/s42514-025-00270-x"
            },
            {
                "id": "http://arxiv.org/abs/2407.05434v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2407.05434v3",
                "title": "LTLBench: Towards Benchmarks for Evaluating Temporal Reasoning in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LTLBench: Towards Benchmarks for Evaluating Temporal Reasoning in Large Language Models"
                },
                "updated": "2026-01-05T11:55:15Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    11,
                    55,
                    15,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2407.05434v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2407.05434v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Temporal Reasoning (TR) is a critical ability for LLMs to understand and reason over temporal information and relationships between events. To study the TR ability in LLMs, prior works provide different ways for evaluating various aspects of TR ability. In this work, we propose an alternative perspective for evaluating TR ability by leveraging Linear Temporal Logic (LTL), and develop a pipeline to automatically synthesize challenges for assessing the TR ability of LLMs. Based on this pipeline, we construct a dataset, namely LTLBench, consisting of $2000$ TR challenges, and benchmark 12 LLMs across 5 different methods. Furthermore, we conduct additional experiments to investigate the impact of increasing the number of formula operators and events on both LLM performance and the complexity of TR problems. We also perform qualitative analyses of their reasoning processes and the effects of varying the number of events and formula operators, which reveal 3 main issues in their temporal reasoning processes and the unexpected performance changes observed as problem complexity increases. We expect this work to provide valuable insights into the TR ability of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Temporal Reasoning (TR) is a critical ability for LLMs to understand and reason over temporal information and relationships between events. To study the TR ability in LLMs, prior works provide different ways for evaluating various aspects of TR ability. In this work, we propose an alternative perspective for evaluating TR ability by leveraging Linear Temporal Logic (LTL), and develop a pipeline to automatically synthesize challenges for assessing the TR ability of LLMs. Based on this pipeline, we construct a dataset, namely LTLBench, consisting of $2000$ TR challenges, and benchmark 12 LLMs across 5 different methods. Furthermore, we conduct additional experiments to investigate the impact of increasing the number of formula operators and events on both LLM performance and the complexity of TR problems. We also perform qualitative analyses of their reasoning processes and the effects of varying the number of events and formula operators, which reveal 3 main issues in their temporal reasoning processes and the unexpected performance changes observed as problem complexity increases. We expect this work to provide valuable insights into the TR ability of LLMs."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2024-07-07T16:37:06Z",
                "published_parsed": [
                    2024,
                    7,
                    7,
                    16,
                    37,
                    6,
                    6,
                    189,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Weizhi Tang"
                    },
                    {
                        "name": "Kwabena Nuamah"
                    },
                    {
                        "name": "Vaishak Belle"
                    }
                ],
                "author_detail": {
                    "name": "Vaishak Belle"
                },
                "author": "Vaishak Belle"
            },
            {
                "id": "http://arxiv.org/abs/2512.07404v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.07404v2",
                "title": "On LLMs' Internal Representation of Code Correctness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On LLMs' Internal Representation of Code Correctness"
                },
                "updated": "2026-01-05T11:52:55Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    11,
                    52,
                    55,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.07404v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.07404v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Despite the effectiveness of large language models (LLMs) for code generation, they often output incorrect code. One reason is that model output probabilities are often not well-correlated with correctness, and reflect only the final output of the generation process. Inspired by findings that LLMs internally encode concepts like truthfulness, this paper explores if LLMs similarly represent code correctness. Specifically, we identify a correctness representation inside LLMs by contrasting the hidden states between pairs of correct and incorrect code for the same programming tasks. By experimenting on four LLMs, we show that exploiting this extracted correctness representation outperforms standard log-likelihood ranking, as well as verbalized model confidence. Furthermore, we explore how this internal correctness signal can be used to select higher-quality code samples, without requiring test execution. Ultimately, this work demonstrates how leveraging internal representations can enhance code generation systems and make LLMs more reliable, thus improving confidence in automatically generated code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the effectiveness of large language models (LLMs) for code generation, they often output incorrect code. One reason is that model output probabilities are often not well-correlated with correctness, and reflect only the final output of the generation process. Inspired by findings that LLMs internally encode concepts like truthfulness, this paper explores if LLMs similarly represent code correctness. Specifically, we identify a correctness representation inside LLMs by contrasting the hidden states between pairs of correct and incorrect code for the same programming tasks. By experimenting on four LLMs, we show that exploiting this extracted correctness representation outperforms standard log-likelihood ranking, as well as verbalized model confidence. Furthermore, we explore how this internal correctness signal can be used to select higher-quality code samples, without requiring test execution. Ultimately, this work demonstrates how leveraging internal representations can enhance code generation systems and make LLMs more reliable, thus improving confidence in automatically generated code."
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-08T10:38:03Z",
                "published_parsed": [
                    2025,
                    12,
                    8,
                    10,
                    38,
                    3,
                    0,
                    342,
                    0
                ],
                "arxiv_comment": "Accepted for ICSE'26",
                "arxiv_primary_category": {
                    "term": "cs.SE"
                },
                "authors": [
                    {
                        "name": "Francisco Ribeiro"
                    },
                    {
                        "name": "Claudio Spiess"
                    },
                    {
                        "name": "Prem Devanbu"
                    },
                    {
                        "name": "Sarah Nadi"
                    }
                ],
                "author_detail": {
                    "name": "Sarah Nadi"
                },
                "author": "Sarah Nadi"
            },
            {
                "id": "http://arxiv.org/abs/2503.24191v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2503.24191v2",
                "title": "Beyond Prompts: Space-Time Decoupling Control-Plane Jailbreaks in LLM Structured Output",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Prompts: Space-Time Decoupling Control-Plane Jailbreaks in LLM Structured Output"
                },
                "updated": "2026-01-05T11:49:07Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    11,
                    49,
                    7,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2503.24191v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2503.24191v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Content Warning: This paper may contain unsafe or harmful content generated by LLMs that may be offensive to readers. Large Language Models (LLMs) are extensively used as tooling platforms through structured output APIs to ensure syntax compliance so that robust integration with existing software, like agent systems, can be achieved. However, the feature enabling the functionality of grammar-guided structured output presents significant security vulnerabilities. In this work, we reveal a critical control-plane attack surface orthogonal to traditional data-plane vulnerabilities. We introduce Constrained Decoding Attack (CDA), a novel jailbreak class that weaponizes structured output constraints to bypass both external auditing and internal safety alignment. Unlike prior attacks focused on input prompt designs, CDA operates by embedding malicious intent in schema-level grammar rules (control-plane) while maintaining benign surface prompts (data-plane). We instantiate this with two proof-of-concept attacks: EnumAttack, which embeds malicious content in enum fields; and the more evasive DictAttack, which decouples the malicious payload across a benign prompt and a dictionary-based grammar. Our evaluation spans a broad spectrum of 13 proprietary/open-weight models. In particular, DictAttack achieves 94.3--99.5% ASR across five benchmarks on gpt-5, gemini-2.5-pro, deepseek-r1, and gpt-oss-120b. Furthermore, we demonstrate the significant challenge in defending against these threats: while basic grammar auditing mitigates EnumAttack, the more sophisticated DictAttack maintains a 75.8% ASR even against multiple state-of-the-art jailbreak guardrails. This exposes a critical \"semantic gap\" in current safety architectures and underscores the urgent need for cross-plane defenses that can bridge the data and control planes to secure the LLM generation pipeline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Content Warning: This paper may contain unsafe or harmful content generated by LLMs that may be offensive to readers. Large Language Models (LLMs) are extensively used as tooling platforms through structured output APIs to ensure syntax compliance so that robust integration with existing software, like agent systems, can be achieved. However, the feature enabling the functionality of grammar-guided structured output presents significant security vulnerabilities. In this work, we reveal a critical control-plane attack surface orthogonal to traditional data-plane vulnerabilities. We introduce Constrained Decoding Attack (CDA), a novel jailbreak class that weaponizes structured output constraints to bypass both external auditing and internal safety alignment. Unlike prior attacks focused on input prompt designs, CDA operates by embedding malicious intent in schema-level grammar rules (control-plane) while maintaining benign surface prompts (data-plane). We instantiate this with two proof-of-concept attacks: EnumAttack, which embeds malicious content in enum fields; and the more evasive DictAttack, which decouples the malicious payload across a benign prompt and a dictionary-based grammar. Our evaluation spans a broad spectrum of 13 proprietary/open-weight models. In particular, DictAttack achieves 94.3--99.5% ASR across five benchmarks on gpt-5, gemini-2.5-pro, deepseek-r1, and gpt-oss-120b. Furthermore, we demonstrate the significant challenge in defending against these threats: while basic grammar auditing mitigates EnumAttack, the more sophisticated DictAttack maintains a 75.8% ASR even against multiple state-of-the-art jailbreak guardrails. This exposes a critical \"semantic gap\" in current safety architectures and underscores the urgent need for cross-plane defenses that can bridge the data and control planes to secure the LLM generation pipeline."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-03-31T15:08:06Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    15,
                    8,
                    6,
                    0,
                    90,
                    0
                ],
                "arxiv_comment": "15 pages, 9 figures, 8 tables, Preprint",
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "authors": [
                    {
                        "name": "Shuoming Zhang"
                    },
                    {
                        "name": "Jiacheng Zhao"
                    },
                    {
                        "name": "Hanyuan Dong"
                    },
                    {
                        "name": "Ruiyuan Xu"
                    },
                    {
                        "name": "Zhicheng Li"
                    },
                    {
                        "name": "Yangyu Zhang"
                    },
                    {
                        "name": "Shuaijiang Li"
                    },
                    {
                        "name": "Yuan Wen"
                    },
                    {
                        "name": "Chunwei Xia"
                    },
                    {
                        "name": "Zheng Wang"
                    },
                    {
                        "name": "Xiaobing Feng"
                    },
                    {
                        "name": "Huimin Cui"
                    }
                ],
                "author_detail": {
                    "name": "Huimin Cui"
                },
                "author": "Huimin Cui"
            },
            {
                "id": "http://arxiv.org/abs/2408.09881v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2408.09881v3",
                "title": "Uncertainty Quantification of Surrogate Models using Conformal Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncertainty Quantification of Surrogate Models using Conformal Prediction"
                },
                "updated": "2026-01-05T11:48:30Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    11,
                    48,
                    30,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2408.09881v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2408.09881v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.1088/2632-2153/ae2e7b",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "Data-driven surrogate models offer quick approximations to complex numerical and experimental systems but typically lack uncertainty quantification, limiting their reliability in safety-critical applications. While Bayesian methods provide uncertainty estimates, they offer no statistical guarantees and struggle with high-dimensional spatio-temporal problems due to computational costs. We present a conformal prediction (CP) framework that provides statistically guaranteed marginal coverage for surrogate models in a model-agnostic manner with near-zero computational cost. Our approach handles high-dimensional spatio-temporal outputs by performing cell-wise calibration while preserving the tensorial structure of predictions. Through extensive empirical evaluation across diverse applications including fluid dynamics, magnetohydrodynamics, weather forecasting, and fusion diagnostics, we demonstrate that CP achieves empirical coverage with valid error bars regardless of model architecture, training regime, or output dimensionality. We evaluate three nonconformity scores (conformalised quantile regression, absolute error residual, and standard deviation) for both deterministic and probabilistic models, showing that guaranteed coverage holds even for out-of-distribution predictions where models are deployed on physics regimes different from training data. Calibration requires only seconds to minutes on standard hardware. The framework enables rigorous validation of pre-trained surrogate models for downstream applications without retraining. While CP provides marginal rather than conditional coverage and assumes exchangeability between calibration and test data, our method circumvents the curse of dimensionality inherent in traditional uncertainty quantification approaches, offering a practical tool for trustworthy deployment of machine learning in physical sciences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data-driven surrogate models offer quick approximations to complex numerical and experimental systems but typically lack uncertainty quantification, limiting their reliability in safety-critical applications. While Bayesian methods provide uncertainty estimates, they offer no statistical guarantees and struggle with high-dimensional spatio-temporal problems due to computational costs. We present a conformal prediction (CP) framework that provides statistically guaranteed marginal coverage for surrogate models in a model-agnostic manner with near-zero computational cost. Our approach handles high-dimensional spatio-temporal outputs by performing cell-wise calibration while preserving the tensorial structure of predictions. Through extensive empirical evaluation across diverse applications including fluid dynamics, magnetohydrodynamics, weather forecasting, and fusion diagnostics, we demonstrate that CP achieves empirical coverage with valid error bars regardless of model architecture, training regime, or output dimensionality. We evaluate three nonconformity scores (conformalised quantile regression, absolute error residual, and standard deviation) for both deterministic and probabilistic models, showing that guaranteed coverage holds even for out-of-distribution predictions where models are deployed on physics regimes different from training data. Calibration requires only seconds to minutes on standard hardware. The framework enables rigorous validation of pre-trained surrogate models for downstream applications without retraining. While CP provides marginal rather than conditional coverage and assumes exchangeability between calibration and test data, our method circumvents the curse of dimensionality inherent in traditional uncertainty quantification approaches, offering a practical tool for trustworthy deployment of machine learning in physical sciences."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ao-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2024-08-19T10:46:19Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    10,
                    46,
                    19,
                    0,
                    232,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Vignesh Gopakumar"
                    },
                    {
                        "name": "Ander Gray"
                    },
                    {
                        "name": "Joel Oskarsson"
                    },
                    {
                        "name": "Lorenzo Zanisi"
                    },
                    {
                        "name": "Daniel Giles"
                    },
                    {
                        "name": "Matt J. Kusner"
                    },
                    {
                        "name": "Stanislas Pamela"
                    },
                    {
                        "name": "Marc Peter Deisenroth"
                    }
                ],
                "author_detail": {
                    "name": "Marc Peter Deisenroth"
                },
                "author": "Marc Peter Deisenroth",
                "arxiv_doi": "10.1088/2632-2153/ae2e7b"
            },
            {
                "id": "http://arxiv.org/abs/2601.02036v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.02036v1",
                "title": "GDRO: Group-level Reward Post-training Suitable for Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GDRO: Group-level Reward Post-training Suitable for Diffusion Models"
                },
                "updated": "2026-01-05T11:47:18Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    11,
                    47,
                    18,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.02036v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.02036v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Recent advancements adopt online reinforcement learning (RL) from LLMs to text-to-image rectified flow diffusion models for reward alignment. The use of group-level rewards successfully aligns the model with the targeted reward. However, it faces challenges including low efficiency, dependency on stochastic samplers, and reward hacking. The problem is that rectified flow models are fundamentally different from LLMs: 1) For efficiency, online image sampling takes much more time and dominates the time of training. 2) For stochasticity, rectified flow is deterministic once the initial noise is fixed. Aiming at these problems and inspired by the effects of group-level rewards from LLMs, we design Group-level Direct Reward Optimization (GDRO). GDRO is a new post-training paradigm for group-level reward alignment that combines the characteristics of rectified flow models. Through rigorous theoretical analysis, we point out that GDRO supports full offline training that saves the large time cost for image rollout sampling. Also, it is diffusion-sampler-independent, which eliminates the need for the ODE-to-SDE approximation to obtain stochasticity. We also empirically study the reward hacking trap that may mislead the evaluation, and involve this factor in the evaluation using a corrected score that not only considers the original evaluation reward but also the trend of reward hacking. Extensive experiments demonstrate that GDRO effectively and efficiently improves the reward score of the diffusion model through group-wise offline optimization across the OCR and GenEval tasks, while demonstrating strong stability and robustness in mitigating reward hacking.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements adopt online reinforcement learning (RL) from LLMs to text-to-image rectified flow diffusion models for reward alignment. The use of group-level rewards successfully aligns the model with the targeted reward. However, it faces challenges including low efficiency, dependency on stochastic samplers, and reward hacking. The problem is that rectified flow models are fundamentally different from LLMs: 1) For efficiency, online image sampling takes much more time and dominates the time of training. 2) For stochasticity, rectified flow is deterministic once the initial noise is fixed. Aiming at these problems and inspired by the effects of group-level rewards from LLMs, we design Group-level Direct Reward Optimization (GDRO). GDRO is a new post-training paradigm for group-level reward alignment that combines the characteristics of rectified flow models. Through rigorous theoretical analysis, we point out that GDRO supports full offline training that saves the large time cost for image rollout sampling. Also, it is diffusion-sampler-independent, which eliminates the need for the ODE-to-SDE approximation to obtain stochasticity. We also empirically study the reward hacking trap that may mislead the evaluation, and involve this factor in the evaluation using a corrected score that not only considers the original evaluation reward but also the trend of reward hacking. Extensive experiments demonstrate that GDRO effectively and efficiently improves the reward score of the diffusion model through group-wise offline optimization across the OCR and GenEval tasks, while demonstrating strong stability and robustness in mitigating reward hacking."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-05T11:47:18Z",
                "published_parsed": [
                    2026,
                    1,
                    5,
                    11,
                    47,
                    18,
                    0,
                    5,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Yiyang Wang"
                    },
                    {
                        "name": "Xi Chen"
                    },
                    {
                        "name": "Xiaogang Xu"
                    },
                    {
                        "name": "Yu Liu"
                    },
                    {
                        "name": "Hengshuang Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Hengshuang Zhao"
                },
                "author": "Hengshuang Zhao"
            },
            {
                "id": "http://arxiv.org/abs/2508.00454v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2508.00454v3",
                "title": "Learning an Efficient Multi-Turn Dialogue Evaluator from Multiple LLM Judges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning an Efficient Multi-Turn Dialogue Evaluator from Multiple LLM Judges"
                },
                "updated": "2026-01-05T11:45:54Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    11,
                    45,
                    54,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2508.00454v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2508.00454v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Evaluating the conversational abilities of large language models (LLMs) remains a challenging task. Current mainstream approaches primarily rely on the \"LLM-as-a-judge\" paradigm, where an LLM is prompted to serve as an evaluator to assess dialogue quality. However, such methods often suffer from various biases, which undermine the reliability and consistency of the evaluation results. To mitigate these biases, recent methods employ multiple LLMs as judges and aggregate their judgments to select the optimal assessment. Although effective, this multi-judge approach incurs significant computational overhead during inference. In this paper, we propose an efficient dialogue evaluator that captures the collective wisdom of multiple LLM judges by aggregating their preference knowledge into a single model. Our approach preserves the advantages of diverse multi-judge feedback while drastically reducing the evaluation cost, enabling fast, flexible, and fine-grained dialogue quality assessment. Extensive experiments on seven single rating and pairwise comparison dialogue evaluation benchmarks demonstrate that our method outperforms existing baselines across diverse scenarios, showcasing its efficiency and robustness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating the conversational abilities of large language models (LLMs) remains a challenging task. Current mainstream approaches primarily rely on the \"LLM-as-a-judge\" paradigm, where an LLM is prompted to serve as an evaluator to assess dialogue quality. However, such methods often suffer from various biases, which undermine the reliability and consistency of the evaluation results. To mitigate these biases, recent methods employ multiple LLMs as judges and aggregate their judgments to select the optimal assessment. Although effective, this multi-judge approach incurs significant computational overhead during inference. In this paper, we propose an efficient dialogue evaluator that captures the collective wisdom of multiple LLM judges by aggregating their preference knowledge into a single model. Our approach preserves the advantages of diverse multi-judge feedback while drastically reducing the evaluation cost, enabling fast, flexible, and fine-grained dialogue quality assessment. Extensive experiments on seven single rating and pairwise comparison dialogue evaluation benchmarks demonstrate that our method outperforms existing baselines across diverse scenarios, showcasing its efficiency and robustness."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-08-01T09:26:01Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    9,
                    26,
                    1,
                    4,
                    213,
                    0
                ],
                "arxiv_comment": "20 pages, 4 pages, under review",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Yuqi Tang"
                    },
                    {
                        "name": "Kehua Feng"
                    },
                    {
                        "name": "Yunfeng Wang"
                    },
                    {
                        "name": "Zhiwen Chen"
                    },
                    {
                        "name": "Chengfei Lv"
                    },
                    {
                        "name": "Gang Yu"
                    },
                    {
                        "name": "Keyan Ding"
                    },
                    {
                        "name": "Huajun Chen"
                    }
                ],
                "author_detail": {
                    "name": "Huajun Chen"
                },
                "author": "Huajun Chen"
            },
            {
                "id": "http://arxiv.org/abs/2601.02031v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.02031v1",
                "title": "Output Embedding Centering for Stable LLM Pretraining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Output Embedding Centering for Stable LLM Pretraining"
                },
                "updated": "2026-01-05T11:44:05Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    11,
                    44,
                    5,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.02031v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.02031v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Pretraining of large language models is not only expensive but also prone to certain training instabilities. A specific instability that often occurs for large learning rates at the end of training is output logit divergence. The most widely used mitigation strategy, z-loss, merely addresses the symptoms rather than the underlying cause of the problem. In this paper, we analyze the instability from the perspective of the output embeddings' geometry and identify its cause. Based on this, we propose output embedding centering (OEC) as a new mitigation strategy, and prove that it suppresses output logit divergence. OEC can be implemented in two different ways, as a deterministic operation called μ-centering, or a regularization method called μ-loss. Our experiments show that both variants outperform z-loss in terms of training stability and learning rate sensitivity. In particular, they ensure that training converges even for large learning rates when z-loss fails. Furthermore, we find that μ-loss is significantly less sensitive to regularization hyperparameter tuning than z-loss.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pretraining of large language models is not only expensive but also prone to certain training instabilities. A specific instability that often occurs for large learning rates at the end of training is output logit divergence. The most widely used mitigation strategy, z-loss, merely addresses the symptoms rather than the underlying cause of the problem. In this paper, we analyze the instability from the perspective of the output embeddings' geometry and identify its cause. Based on this, we propose output embedding centering (OEC) as a new mitigation strategy, and prove that it suppresses output logit divergence. OEC can be implemented in two different ways, as a deterministic operation called μ-centering, or a regularization method called μ-loss. Our experiments show that both variants outperform z-loss in terms of training stability and learning rate sensitivity. In particular, they ensure that training converges even for large learning rates when z-loss fails. Furthermore, we find that μ-loss is significantly less sensitive to regularization hyperparameter tuning than z-loss."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-05T11:44:05Z",
                "published_parsed": [
                    2026,
                    1,
                    5,
                    11,
                    44,
                    5,
                    0,
                    5,
                    0
                ],
                "arxiv_comment": "11 pages, 5 figures",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Felix Stollenwerk"
                    },
                    {
                        "name": "Anna Lokrantz"
                    },
                    {
                        "name": "Niclas Hertzberg"
                    }
                ],
                "author_detail": {
                    "name": "Niclas Hertzberg"
                },
                "author": "Niclas Hertzberg"
            },
            {
                "id": "http://arxiv.org/abs/2512.20773v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20773v2",
                "title": "Adversarial Training for Failure-Sensitive User Simulation in Mental Health Dialogue Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adversarial Training for Failure-Sensitive User Simulation in Mental Health Dialogue Optimization"
                },
                "updated": "2026-01-05T11:37:37Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    11,
                    37,
                    37,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20773v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20773v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Realistic user simulation is crucial for training and evaluating task-oriented dialogue (TOD) systems, yet creating simulators that accurately replicate human behavior remains challenging. A key property of effective simulators is their ability to expose failure modes of the systems they evaluate. We present an adversarial training framework that iteratively improves user simulator realism through a competitive dynamic between a generator (user simulator) and a discriminator. Applied to mental health support chatbots, our approach demonstrates that fine-tuned simulators dramatically outperform zero-shot base models at surfacing system issues, and adversarial training further enhances diversity, distributional alignment, and predictive validity. The resulting simulator achieves a strong correlation between simulated and real failure occurrence rates across diverse chatbot configurations while maintaining low distributional divergence of failure modes. Discriminator accuracy decreases drastically after three adversarial iterations, suggesting improved realism. These results provide evidence that adversarial training is a promising approach for creating realistic user simulators in mental health support TOD domains, enabling rapid, reliable, and cost-effective system evaluation before deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Realistic user simulation is crucial for training and evaluating task-oriented dialogue (TOD) systems, yet creating simulators that accurately replicate human behavior remains challenging. A key property of effective simulators is their ability to expose failure modes of the systems they evaluate. We present an adversarial training framework that iteratively improves user simulator realism through a competitive dynamic between a generator (user simulator) and a discriminator. Applied to mental health support chatbots, our approach demonstrates that fine-tuned simulators dramatically outperform zero-shot base models at surfacing system issues, and adversarial training further enhances diversity, distributional alignment, and predictive validity. The resulting simulator achieves a strong correlation between simulated and real failure occurrence rates across diverse chatbot configurations while maintaining low distributional divergence of failure modes. Discriminator accuracy decreases drastically after three adversarial iterations, suggesting improved realism. These results provide evidence that adversarial training is a promising approach for creating realistic user simulators in mental health support TOD domains, enabling rapid, reliable, and cost-effective system evaluation before deployment."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T21:21:08Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    21,
                    21,
                    8,
                    1,
                    357,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Ziyi Zhu"
                    },
                    {
                        "name": "Olivier Tieleman"
                    },
                    {
                        "name": "Caitlin A. Stamatis"
                    },
                    {
                        "name": "Luka Smyth"
                    },
                    {
                        "name": "Thomas D. Hull"
                    },
                    {
                        "name": "Daniel R. Cahn"
                    },
                    {
                        "name": "Matteo Malgaroli"
                    }
                ],
                "author_detail": {
                    "name": "Matteo Malgaroli"
                },
                "author": "Matteo Malgaroli"
            },
            {
                "id": "http://arxiv.org/abs/2601.02023v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.02023v1",
                "title": "Not All Needles Are Found: How Fact Distribution and Don't Make It Up Prompts Shape Literal Extraction, Logical Inference, and Hallucination Risks in Long-Context LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not All Needles Are Found: How Fact Distribution and Don't Make It Up Prompts Shape Literal Extraction, Logical Inference, and Hallucination Risks in Long-Context LLMs"
                },
                "updated": "2026-01-05T11:30:56Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    11,
                    30,
                    56,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.02023v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.02023v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models (LLMs) increasingly support very long input contexts. Yet it remains unclear how reliably they extract and infer information at scale. Performance varies with context length and strongly interacts with how information is distributed in real-world corpora. Motivated by these observations, we study how fact placement, corpus-level fact distributions, and Don't Make It Up prompts influence model behavior. We introduce an extended needle-in-a-haystack benchmark across four production-scale models: Gemini-2.5-flash, ChatGPT-5-mini, Claude-4.5-haiku, and Deepseek-v3.2-chat. Unlike prior work, we separately evaluate literal extraction, logical inference, and hallucination risk. Our study considers both positional effects and realistic distributions of evidence across long contexts, as well as prompts that explicitly discourage fabrication. We find that longer contexts alone do not guarantee better performance and can be detrimental when relevant evidence is diluted or widely dispersed. Performance varies substantially across models: some show severe degradation under realistic conditions, while others remain more robust at longer context lengths. Anti-hallucination (AH) instructions can make some models overly conservative, sharply reducing accuracy in literal extraction and logical inference. While we do not directly compare retrieval-augmented generation (RAG) and cache-augmented generation (CAG), our results suggest many failures stem from ineffective context utilization. Models often struggle to identify and prioritize relevant information even when it is present. These findings have direct practical implications, as enterprise workflows increasingly involve pasting large volumes of unfiltered documents into LLM prompts. Effective context length and model-specific robustness to long contexts are therefore critical for reliable LLM deployment in research and business.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) increasingly support very long input contexts. Yet it remains unclear how reliably they extract and infer information at scale. Performance varies with context length and strongly interacts with how information is distributed in real-world corpora. Motivated by these observations, we study how fact placement, corpus-level fact distributions, and Don't Make It Up prompts influence model behavior. We introduce an extended needle-in-a-haystack benchmark across four production-scale models: Gemini-2.5-flash, ChatGPT-5-mini, Claude-4.5-haiku, and Deepseek-v3.2-chat. Unlike prior work, we separately evaluate literal extraction, logical inference, and hallucination risk. Our study considers both positional effects and realistic distributions of evidence across long contexts, as well as prompts that explicitly discourage fabrication. We find that longer contexts alone do not guarantee better performance and can be detrimental when relevant evidence is diluted or widely dispersed. Performance varies substantially across models: some show severe degradation under realistic conditions, while others remain more robust at longer context lengths. Anti-hallucination (AH) instructions can make some models overly conservative, sharply reducing accuracy in literal extraction and logical inference. While we do not directly compare retrieval-augmented generation (RAG) and cache-augmented generation (CAG), our results suggest many failures stem from ineffective context utilization. Models often struggle to identify and prioritize relevant information even when it is present. These findings have direct practical implications, as enterprise workflows increasingly involve pasting large volumes of unfiltered documents into LLM prompts. Effective context length and model-specific robustness to long contexts are therefore critical for reliable LLM deployment in research and business."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-05T11:30:56Z",
                "published_parsed": [
                    2026,
                    1,
                    5,
                    11,
                    30,
                    56,
                    0,
                    5,
                    0
                ],
                "arxiv_comment": "25 pages, 8 figures, 3 tables",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Amirali Ebrahimzadeh"
                    },
                    {
                        "name": "Seyyed M. Salili"
                    }
                ],
                "author_detail": {
                    "name": "Seyyed M. Salili"
                },
                "author": "Seyyed M. Salili"
            },
            {
                "id": "http://arxiv.org/abs/2601.02021v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.02021v1",
                "title": "AgentVNE: LLM-Augmented Graph Reinforcement Learning for Affinity-Aware Multi-Agent Placement in Edge Agentic AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AgentVNE: LLM-Augmented Graph Reinforcement Learning for Affinity-Aware Multi-Agent Placement in Edge Agentic AI"
                },
                "updated": "2026-01-05T11:30:04Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    11,
                    30,
                    4,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.02021v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.02021v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The Internet of Agents is propelling edge computing toward agentic AI and edge general intelligence (EGI). However, deploying multi-agent service (MAS) on resource-constrained edge infrastructure presents severe challenges. MAS service workflows are driven by complex cross-node interactions, dynamic memory accumulation, and collaborative tool usage. Exhibiting chain-like topological dependencies and strict affinity constraints, these workflows demand real-time responsiveness that exceeds the capabilities of traditional VNE algorithms designed for static resources. To address this, we propose AgentVNE, a cloud-edge collaborative framework utilizing a dual-layer architecture. First, AgentVNE employs a large language model (LLM) to identify implicit semantic constraints and generate affinity-based resource augmentation to resolve physical dependency issues. Second, it constructs a resource similarity-aware neural network, utilizing a pre-training and PPO fine-tuning strategy to precisely capture topological similarities between dynamic workflows and heterogeneous networks. By coupling semantic perception with topological reasoning, this mechanism effectively bridges the gap between dynamic service requirements and physical infrastructure. Simulation results demonstrate that AgentVNE reduces workflow communication latency to less than 40% of baselines and improves the service acceptance rate by approximately 5%-10% under high-load scenarios. Ultimately, this work provides a foundational solution for the semantic-aware deployment of agentic AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Internet of Agents is propelling edge computing toward agentic AI and edge general intelligence (EGI). However, deploying multi-agent service (MAS) on resource-constrained edge infrastructure presents severe challenges. MAS service workflows are driven by complex cross-node interactions, dynamic memory accumulation, and collaborative tool usage. Exhibiting chain-like topological dependencies and strict affinity constraints, these workflows demand real-time responsiveness that exceeds the capabilities of traditional VNE algorithms designed for static resources. To address this, we propose AgentVNE, a cloud-edge collaborative framework utilizing a dual-layer architecture. First, AgentVNE employs a large language model (LLM) to identify implicit semantic constraints and generate affinity-based resource augmentation to resolve physical dependency issues. Second, it constructs a resource similarity-aware neural network, utilizing a pre-training and PPO fine-tuning strategy to precisely capture topological similarities between dynamic workflows and heterogeneous networks. By coupling semantic perception with topological reasoning, this mechanism effectively bridges the gap between dynamic service requirements and physical infrastructure. Simulation results demonstrate that AgentVNE reduces workflow communication latency to less than 40% of baselines and improves the service acceptance rate by approximately 5%-10% under high-load scenarios. Ultimately, this work provides a foundational solution for the semantic-aware deployment of agentic AI."
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-05T11:30:04Z",
                "published_parsed": [
                    2026,
                    1,
                    5,
                    11,
                    30,
                    4,
                    0,
                    5,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI"
                },
                "authors": [
                    {
                        "name": "Runze Zheng"
                    },
                    {
                        "name": "Yuqing Zheng"
                    },
                    {
                        "name": "Zhengyi Cheng"
                    },
                    {
                        "name": "Long Luo"
                    },
                    {
                        "name": "Haoxiang Luo"
                    },
                    {
                        "name": "Gang Sun"
                    },
                    {
                        "name": "Hongfang Yu"
                    },
                    {
                        "name": "Dusit Niyato"
                    }
                ],
                "author_detail": {
                    "name": "Dusit Niyato"
                },
                "author": "Dusit Niyato"
            },
            {
                "id": "http://arxiv.org/abs/2510.20075v5",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.20075v5",
                "title": "I Large Language Models possono nascondere un testo in un altro testo della stessa lunghezza",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "I Large Language Models possono nascondere un testo in un altro testo della stessa lunghezza"
                },
                "updated": "2026-01-05T11:25:29Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    11,
                    25,
                    29,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.20075v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.20075v5",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "A meaningful text can be hidden inside another, completely different yet still coherent and plausible, text of the same length. For example, a tweet containing a harsh political critique could be embedded in a tweet that celebrates the same political leader, or an ordinary product review could conceal a secret manuscript. This uncanny state of affairs is now possible thanks to Large Language Models, and in this paper we present Calgacus, a simple and efficient protocol to achieve it. We show that even modest 8-billion-parameter open-source LLMs are sufficient to obtain high-quality results, and a message as long as this abstract can be encoded and decoded locally on a laptop in seconds. The existence of such a protocol demonstrates a radical decoupling of text from authorial intent, further eroding trust in written communication, already shaken by the rise of LLM chatbots. We illustrate this with a concrete scenario: a company could covertly deploy an unfiltered LLM by encoding its answers within the compliant responses of a safe model. This possibility raises urgent questions for AI safety and challenges our understanding of what it means for a Large Language Model to know something.\n  --\nUn testo di senso compiuto può essere nascosto all'interno di un altro testo completamente diverso, eppure coerente e plausibile, della stessa lunghezza. Ad esempio, un tweet che celebra un leader politico potrebbe celare un tweet che lo critica duramente, o un'anonima recensione di un prodotto potrebbe in realtà codificare un manoscritto segreto. Questa sconcertante possibilità è oggi alla nostra portata grazie ai Large Language Models (LLM); in questo articolo presentiamo Calgacus, un protocollo semplice ed efficiente per realizzarla. Mostriamo che anche modesti LLM open-source da 8 miliardi di parametri sono sufficienti per ottenere risultati di alta qualità, e che un messaggio lungo quanto questo abstract può essere codificato e decodificato su un comune portatile in pochi secondi. L'esistenza di tale protocollo dimostra un radicale disaccoppiamento del testo dall'intento del suo autore, erodendo ulteriormente la fiducia nella comunicazione scritta, già scossa dall'ascesa dei chatbot basati su LLMs. Illustriamo ciò con uno scenario concreto: un'azienda potrebbe offrire pubblicamente i servizi di un LLM senza filtri nascondendo le sue risposte all'interno di risposte apparentemente innocue generate da un LLM considerato sicuro. Questa possibilità solleva questioni urgenti per la sicurezza dell'Intelligenza Artificiale e sfida la nostra comprensione di cosa significhi, per un Large Language Model, sapere qualcosa.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A meaningful text can be hidden inside another, completely different yet still coherent and plausible, text of the same length. For example, a tweet containing a harsh political critique could be embedded in a tweet that celebrates the same political leader, or an ordinary product review could conceal a secret manuscript. This uncanny state of affairs is now possible thanks to Large Language Models, and in this paper we present Calgacus, a simple and efficient protocol to achieve it. We show that even modest 8-billion-parameter open-source LLMs are sufficient to obtain high-quality results, and a message as long as this abstract can be encoded and decoded locally on a laptop in seconds. The existence of such a protocol demonstrates a radical decoupling of text from authorial intent, further eroding trust in written communication, already shaken by the rise of LLM chatbots. We illustrate this with a concrete scenario: a company could covertly deploy an unfiltered LLM by encoding its answers within the compliant responses of a safe model. This possibility raises urgent questions for AI safety and challenges our understanding of what it means for a Large Language Model to know something.\n  --\nUn testo di senso compiuto può essere nascosto all'interno di un altro testo completamente diverso, eppure coerente e plausibile, della stessa lunghezza. Ad esempio, un tweet che celebra un leader politico potrebbe celare un tweet che lo critica duramente, o un'anonima recensione di un prodotto potrebbe in realtà codificare un manoscritto segreto. Questa sconcertante possibilità è oggi alla nostra portata grazie ai Large Language Models (LLM); in questo articolo presentiamo Calgacus, un protocollo semplice ed efficiente per realizzarla. Mostriamo che anche modesti LLM open-source da 8 miliardi di parametri sono sufficienti per ottenere risultati di alta qualità, e che un messaggio lungo quanto questo abstract può essere codificato e decodificato su un comune portatile in pochi secondi. L'esistenza di tale protocollo dimostra un radicale disaccoppiamento del testo dall'intento del suo autore, erodendo ulteriormente la fiducia nella comunicazione scritta, già scossa dall'ascesa dei chatbot basati su LLMs. Illustriamo ciò con uno scenario concreto: un'azienda potrebbe offrire pubblicamente i servizi di un LLM senza filtri nascondendo le sue risposte all'interno di risposte apparentemente innocue generate da un LLM considerato sicuro. Questa possibilità solleva questioni urgenti per la sicurezza dell'Intelligenza Artificiale e sfida la nostra comprensione di cosa significhi, per un Large Language Model, sapere qualcosa."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-22T23:16:50Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    23,
                    16,
                    50,
                    2,
                    295,
                    0
                ],
                "arxiv_comment": "21 pages, in Italian language, main paper 9 pages. v1-v4 are in English",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Antonio Norelli"
                    },
                    {
                        "name": "Michael Bronstein"
                    }
                ],
                "author_detail": {
                    "name": "Michael Bronstein"
                },
                "author": "Michael Bronstein"
            },
            {
                "id": "http://arxiv.org/abs/2601.02002v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.02002v1",
                "title": "Exploring Approaches for Detecting Memorization of Recommender System Data in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Approaches for Detecting Memorization of Recommender System Data in Large Language Models"
                },
                "updated": "2026-01-05T11:03:56Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    11,
                    3,
                    56,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.02002v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.02002v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) are increasingly applied in recommendation scenarios due to their strong natural language understanding and generation capabilities. However, they are trained on vast corpora whose contents are not publicly disclosed, raising concerns about data leakage. Recent work has shown that the MovieLens-1M dataset is memorized by both the LLaMA and OpenAI model families, but the extraction of such memorized data has so far relied exclusively on manual prompt engineering. In this paper, we pose three main questions: Is it possible to enhance manual prompting? Can LLM memorization be detected through methods beyond manual prompting? And can the detection of data leakage be automated? To address these questions, we evaluate three approaches: (i) jailbreak prompt engineering; (ii) unsupervised latent knowledge discovery, probing internal activations via Contrast-Consistent Search (CCS) and Cluster-Norm; and (iii) Automatic Prompt Engineering (APE), which frames prompt discovery as a meta-learning process that iteratively refines candidate instructions. Experiments on MovieLens-1M using LLaMA models show that jailbreak prompting does not improve the retrieval of memorized items and remains inconsistent; CCS reliably distinguishes genuine from fabricated movie titles but fails on numerical user and rating data; and APE retrieves item-level information with moderate success yet struggles to recover numerical interactions. These findings suggest that automatically optimizing prompts is the most promising strategy for extracting memorized samples.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly applied in recommendation scenarios due to their strong natural language understanding and generation capabilities. However, they are trained on vast corpora whose contents are not publicly disclosed, raising concerns about data leakage. Recent work has shown that the MovieLens-1M dataset is memorized by both the LLaMA and OpenAI model families, but the extraction of such memorized data has so far relied exclusively on manual prompt engineering. In this paper, we pose three main questions: Is it possible to enhance manual prompting? Can LLM memorization be detected through methods beyond manual prompting? And can the detection of data leakage be automated? To address these questions, we evaluate three approaches: (i) jailbreak prompt engineering; (ii) unsupervised latent knowledge discovery, probing internal activations via Contrast-Consistent Search (CCS) and Cluster-Norm; and (iii) Automatic Prompt Engineering (APE), which frames prompt discovery as a meta-learning process that iteratively refines candidate instructions. Experiments on MovieLens-1M using LLaMA models show that jailbreak prompting does not improve the retrieval of memorized items and remains inconsistent; CCS reliably distinguishes genuine from fabricated movie titles but fails on numerical user and rating data; and APE retrieves item-level information with moderate success yet struggles to recover numerical interactions. These findings suggest that automatically optimizing prompts is the most promising strategy for extracting memorized samples."
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-05T11:03:56Z",
                "published_parsed": [
                    2026,
                    1,
                    5,
                    11,
                    3,
                    56,
                    0,
                    5,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR"
                },
                "authors": [
                    {
                        "name": "Antonio Colacicco"
                    },
                    {
                        "name": "Vito Guida"
                    },
                    {
                        "name": "Dario Di Palma"
                    },
                    {
                        "name": "Fedelucio Narducci"
                    },
                    {
                        "name": "Tommaso Di Noia"
                    }
                ],
                "author_detail": {
                    "name": "Tommaso Di Noia"
                },
                "author": "Tommaso Di Noia"
            },
            {
                "id": "http://arxiv.org/abs/2411.12589v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2411.12589v3",
                "title": "ULTra: Unveiling Latent Token Interpretability in Transformer-Based Understanding and Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ULTra: Unveiling Latent Token Interpretability in Transformer-Based Understanding and Segmentation"
                },
                "updated": "2026-01-05T11:02:56Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    11,
                    2,
                    56,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2411.12589v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2411.12589v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Transformers have revolutionized Computer Vision (CV) through self-attention mechanisms. However, their complexity makes latent token representations difficult to interpret. We introduce ULTra, a framework for interpreting Transformer embeddings and uncovering meaningful semantic patterns within them. ULTra enables unsupervised semantic segmentation using pre-trained models without requiring fine-tuning. Additionally, we propose a self-supervised training approach that refines segmentation performance by learning an external transformation matrix without modifying the underlying model. Our method achieves state-of-the-art performance in unsupervised semantic segmentation, outperforming existing segmentation methods. Furthermore, we validate ULTra for model interpretation on both synthetic and real-world scenarios, including Object Selection and interpretable text summarization using LLMs, demonstrating its broad applicability in explaining the semantic structure of latent token representations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers have revolutionized Computer Vision (CV) through self-attention mechanisms. However, their complexity makes latent token representations difficult to interpret. We introduce ULTra, a framework for interpreting Transformer embeddings and uncovering meaningful semantic patterns within them. ULTra enables unsupervised semantic segmentation using pre-trained models without requiring fine-tuning. Additionally, we propose a self-supervised training approach that refines segmentation performance by learning an external transformation matrix without modifying the underlying model. Our method achieves state-of-the-art performance in unsupervised semantic segmentation, outperforming existing segmentation methods. Furthermore, we validate ULTra for model interpretation on both synthetic and real-world scenarios, including Object Selection and interpretable text summarization using LLMs, demonstrating its broad applicability in explaining the semantic structure of latent token representations."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2024-11-15T19:36:50Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    19,
                    36,
                    50,
                    4,
                    320,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "arxiv_journal_ref": "Transactions on Machine Learning Research (TMLR), 2026",
                "authors": [
                    {
                        "name": "Hesam Hosseini"
                    },
                    {
                        "name": "Ghazal Hosseini Mighan"
                    },
                    {
                        "name": "Amirabbas Afzali"
                    },
                    {
                        "name": "Sajjad Amini"
                    },
                    {
                        "name": "Amir Houmansadr"
                    }
                ],
                "author_detail": {
                    "name": "Amir Houmansadr"
                },
                "author": "Amir Houmansadr"
            },
            {
                "id": "http://arxiv.org/abs/2601.01993v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.01993v1",
                "title": "MindChat: A Privacy-preserving Large Language Model for Mental Health Support",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MindChat: A Privacy-preserving Large Language Model for Mental Health Support"
                },
                "updated": "2026-01-05T10:54:18Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    10,
                    54,
                    18,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.01993v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.01993v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models (LLMs) have shown promise for mental health support, yet training such models is constrained by the scarcity and sensitivity of real counseling dialogues. In this article, we present MindChat, a privacy-preserving LLM for mental health support, together with MindCorpus, a synthetic multi-turn counseling dataset constructed via a multi-agent role-playing framework. To synthesize high-quality counseling data, the developed dialogue-construction framework employs a dual closed-loop feedback design to integrate psychological expertise and counseling techniques through role-playing: (i) turn-level critique-and-revision to improve coherence and counseling appropriateness within a session, and (ii) session-level strategy refinement to progressively enrich counselor behaviors across sessions. To mitigate privacy risks under decentralized data ownership, we fine-tune the base model using federated learning with parameter-efficient LoRA adapters and incorporate differentially private optimization to reduce membership and memorization risks. Experiments on synthetic-data quality assessment and counseling capability evaluation show that MindCorpus improves training effectiveness and that MindChat is competitive with existing general and counseling-oriented LLM baselines under both automatic LLM-judge and human evaluation protocols, while exhibiting reduced privacy leakage under membership inference attacks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown promise for mental health support, yet training such models is constrained by the scarcity and sensitivity of real counseling dialogues. In this article, we present MindChat, a privacy-preserving LLM for mental health support, together with MindCorpus, a synthetic multi-turn counseling dataset constructed via a multi-agent role-playing framework. To synthesize high-quality counseling data, the developed dialogue-construction framework employs a dual closed-loop feedback design to integrate psychological expertise and counseling techniques through role-playing: (i) turn-level critique-and-revision to improve coherence and counseling appropriateness within a session, and (ii) session-level strategy refinement to progressively enrich counselor behaviors across sessions. To mitigate privacy risks under decentralized data ownership, we fine-tune the base model using federated learning with parameter-efficient LoRA adapters and incorporate differentially private optimization to reduce membership and memorization risks. Experiments on synthetic-data quality assessment and counseling capability evaluation show that MindCorpus improves training effectiveness and that MindChat is competitive with existing general and counseling-oriented LLM baselines under both automatic LLM-judge and human evaluation protocols, while exhibiting reduced privacy leakage under membership inference attacks."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-05T10:54:18Z",
                "published_parsed": [
                    2026,
                    1,
                    5,
                    10,
                    54,
                    18,
                    0,
                    5,
                    0
                ],
                "arxiv_comment": "33 pages, 16 figures",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Dong Xue"
                    },
                    {
                        "name": "Jicheng Tu"
                    },
                    {
                        "name": "Ming Wang"
                    },
                    {
                        "name": "Xin Yan"
                    },
                    {
                        "name": "Fangzhou Liu"
                    },
                    {
                        "name": "Jie Hu"
                    }
                ],
                "author_detail": {
                    "name": "Jie Hu"
                },
                "author": "Jie Hu"
            },
            {
                "id": "http://arxiv.org/abs/2512.14244v4",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.14244v4",
                "title": "From Context to EDUs: Faithful and Structured Context Compression via Elementary Discourse Unit Decomposition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Context to EDUs: Faithful and Structured Context Compression via Elementary Discourse Unit Decomposition"
                },
                "updated": "2026-01-05T10:52:55Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    10,
                    52,
                    55,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.14244v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.14244v4",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Managing extensive context remains a critical bottleneck for Large Language Models (LLMs), particularly in applications like long-document question answering and autonomous agents where lengthy inputs incur high computational costs and introduce noise. Existing compression techniques often disrupt local coherence through discrete token removal or rely on implicit latent encoding that suffers from positional bias and incompatibility with closed-source APIs. To address these limitations, we introduce the EDU-based Context Compressor, a novel explicit compression framework designed to preserve both global structure and fine-grained details. Our approach reformulates context compression as a structure-then-select process. First, our LingoEDU transforms linear text into a structural relation tree of Elementary Discourse Units (EDUs) which are anchored strictly to source indices to eliminate hallucination. Second, a lightweight ranking module selects query-relevant sub-trees for linearization. To rigorously evaluate structural understanding, we release StructBench, a manually annotated dataset of 248 diverse documents. Empirical results demonstrate that our method achieves state-of-the-art structural prediction accuracy and significantly outperforms frontier LLMs while reducing costs. Furthermore, our structure-aware compression substantially enhances performance across downstream tasks ranging from long-context tasks to complex Deep Search scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Managing extensive context remains a critical bottleneck for Large Language Models (LLMs), particularly in applications like long-document question answering and autonomous agents where lengthy inputs incur high computational costs and introduce noise. Existing compression techniques often disrupt local coherence through discrete token removal or rely on implicit latent encoding that suffers from positional bias and incompatibility with closed-source APIs. To address these limitations, we introduce the EDU-based Context Compressor, a novel explicit compression framework designed to preserve both global structure and fine-grained details. Our approach reformulates context compression as a structure-then-select process. First, our LingoEDU transforms linear text into a structural relation tree of Elementary Discourse Units (EDUs) which are anchored strictly to source indices to eliminate hallucination. Second, a lightweight ranking module selects query-relevant sub-trees for linearization. To rigorously evaluate structural understanding, we release StructBench, a manually annotated dataset of 248 diverse documents. Empirical results demonstrate that our method achieves state-of-the-art structural prediction accuracy and significantly outperforms frontier LLMs while reducing costs. Furthermore, our structure-aware compression substantially enhances performance across downstream tasks ranging from long-context tasks to complex Deep Search scenarios."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-16T09:52:58Z",
                "published_parsed": [
                    2025,
                    12,
                    16,
                    9,
                    52,
                    58,
                    1,
                    350,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Yiqing Zhou"
                    },
                    {
                        "name": "Yu Lei"
                    },
                    {
                        "name": "Shuzheng Si"
                    },
                    {
                        "name": "Qingyan Sun"
                    },
                    {
                        "name": "Wei Wang"
                    },
                    {
                        "name": "Yifei Wu"
                    },
                    {
                        "name": "Hao Wen"
                    },
                    {
                        "name": "Gang Chen"
                    },
                    {
                        "name": "Fanchao Qi"
                    },
                    {
                        "name": "Maosong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Maosong Sun"
                },
                "author": "Maosong Sun"
            },
            {
                "id": "http://arxiv.org/abs/2601.01982v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.01982v1",
                "title": "ChaosBench-Logic: A Benchmark for Logical and Symbolic Reasoning on Chaotic Dynamical Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChaosBench-Logic: A Benchmark for Logical and Symbolic Reasoning on Chaotic Dynamical Systems"
                },
                "updated": "2026-01-05T10:36:40Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    10,
                    36,
                    40,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.01982v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.01982v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models (LLMs) excel at natural language tasks but remain brittle in domains requiring precise logical and symbolic reasoning. Chaotic dynamical systems provide an especially demanding test because chaos is deterministic yet often misinterpreted as randomness or complexity. We introduce ChaosBench-Logic, a benchmark that evaluates LLM reasoning across 30 diverse dynamical systems using a unified first-order logic (FOL) ontology. Each system is annotated with truth assignments for 11 semantic predicates, and 621 questions are generated across seven reasoning categories, including multi-hop implications, cross-system analogies, counterfactual reasoning, bias probes, and multi-turn dialogues. We define metrics for logical accuracy, implication consistency, dialogue coherence, and contradiction, and we release an open-source evaluation pipeline. Initial experiments show that frontier LLMs such as GPT-4, Claude 3.5 Sonnet, Gemini 2.5 Flash, and the open-source LLaMA-3 70B achieve 91-94% per-item accuracy, yet still score 0% on compositional items and exhibit fragile global coherence. Dialogue-level accuracy ranges from 53.1% (GPT-4 CoT) to 75.5% (LLaMA-3 zero-shot). ChaosBench-Logic provides a rigorous testbed for diagnosing such failures and a foundation for developing neuro-symbolic approaches that improve scientific reasoning in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) excel at natural language tasks but remain brittle in domains requiring precise logical and symbolic reasoning. Chaotic dynamical systems provide an especially demanding test because chaos is deterministic yet often misinterpreted as randomness or complexity. We introduce ChaosBench-Logic, a benchmark that evaluates LLM reasoning across 30 diverse dynamical systems using a unified first-order logic (FOL) ontology. Each system is annotated with truth assignments for 11 semantic predicates, and 621 questions are generated across seven reasoning categories, including multi-hop implications, cross-system analogies, counterfactual reasoning, bias probes, and multi-turn dialogues. We define metrics for logical accuracy, implication consistency, dialogue coherence, and contradiction, and we release an open-source evaluation pipeline. Initial experiments show that frontier LLMs such as GPT-4, Claude 3.5 Sonnet, Gemini 2.5 Flash, and the open-source LLaMA-3 70B achieve 91-94% per-item accuracy, yet still score 0% on compositional items and exhibit fragile global coherence. Dialogue-level accuracy ranges from 53.1% (GPT-4 CoT) to 75.5% (LLaMA-3 zero-shot). ChaosBench-Logic provides a rigorous testbed for diagnosing such failures and a foundation for developing neuro-symbolic approaches that improve scientific reasoning in LLMs."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-05T10:36:40Z",
                "published_parsed": [
                    2026,
                    1,
                    5,
                    10,
                    36,
                    40,
                    0,
                    5,
                    0
                ],
                "arxiv_comment": "7 pages, 0 figures , Accepted to AAAI-26 Bridge Program: Logical and Symbolic Reasoning in Language Models (camera-ready)",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Noel Thomas"
                    }
                ],
                "author_detail": {
                    "name": "Noel Thomas"
                },
                "author": "Noel Thomas"
            },
            {
                "id": "http://arxiv.org/abs/2601.01969v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.01969v1",
                "title": "What you reward is what you learn: Comparing rewards for online speech policy optimization in public HRI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What you reward is what you learn: Comparing rewards for online speech policy optimization in public HRI"
                },
                "updated": "2026-01-05T10:22:58Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    10,
                    22,
                    58,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.01969v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.01969v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Designing policies that are both efficient and acceptable for conversational service robots in open and diverse environments is non-trivial. Unlike fixed, hand-tuned parameters, online learning can adapt to non-stationary conditions. In this paper, we study how to adapt a social robot's speech policy in the wild. During a 12-day in-situ deployment with over 1,400 public encounters, we cast online policy optimization as a multi-armed bandit problem and use Thompson sampling to select among six actions defined by speech rate (slow/normal/fast) and verbosity (concise/detailed). We compare three complementary binary rewards--Ru (user rating), Rc (conversation closure), and Rt (>=2 turns)--and show that each induces distinct arm distributions and interaction behaviors. We complement the online results with offline evaluations that analyze contextual factors (e.g., crowd level, group size) using video-annotated data. Taken together, we distill ready-to-use design lessons for deploying online optimization of speech policies in real public HRI settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Designing policies that are both efficient and acceptable for conversational service robots in open and diverse environments is non-trivial. Unlike fixed, hand-tuned parameters, online learning can adapt to non-stationary conditions. In this paper, we study how to adapt a social robot's speech policy in the wild. During a 12-day in-situ deployment with over 1,400 public encounters, we cast online policy optimization as a multi-armed bandit problem and use Thompson sampling to select among six actions defined by speech rate (slow/normal/fast) and verbosity (concise/detailed). We compare three complementary binary rewards--Ru (user rating), Rc (conversation closure), and Rt (>=2 turns)--and show that each induces distinct arm distributions and interaction behaviors. We complement the online results with offline evaluations that analyze contextual factors (e.g., crowd level, group size) using video-annotated data. Taken together, we distill ready-to-use design lessons for deploying online optimization of speech policies in real public HRI settings."
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-05T10:22:58Z",
                "published_parsed": [
                    2026,
                    1,
                    5,
                    10,
                    22,
                    58,
                    0,
                    5,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO"
                },
                "authors": [
                    {
                        "name": "Sichao Song"
                    },
                    {
                        "name": "Yuki Okafuji"
                    },
                    {
                        "name": "Kaito Ariu"
                    },
                    {
                        "name": "Amy Koike"
                    }
                ],
                "author_detail": {
                    "name": "Amy Koike"
                },
                "author": "Amy Koike"
            },
            {
                "id": "http://arxiv.org/abs/2601.01966v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.01966v1",
                "title": "Refinement Provenance Inference: Detecting LLM-Refined Training Prompts from Model Behavior",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Refinement Provenance Inference: Detecting LLM-Refined Training Prompts from Model Behavior"
                },
                "updated": "2026-01-05T10:16:41Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    10,
                    16,
                    41,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.01966v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.01966v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Instruction tuning increasingly relies on LLM-based prompt refinement, where prompts in the training corpus are selectively rewritten by an external refiner to improve clarity and instruction alignment. This motivates an instance-level audit problem: for a fine-tuned model and a training prompt-response pair, can we infer whether the model was trained on the original prompt or its LLM-refined version within a mixed corpus? This matters for dataset governance and dispute resolution when training data are contested. However, it is non-trivial in practice: refined and raw instances are interleaved in the training corpus with unknown, source-dependent mixture ratios, making it harder to develop provenance methods that generalize across models and training setups. In this paper, we formalize this audit task as Refinement Provenance Inference (RPI) and show that prompt refinement yields stable, detectable shifts in teacher-forced token distributions, even when semantic differences are not obvious. Building on this phenomenon, we propose RePro, a logit-based provenance framework that fuses teacher-forced likelihood features with logit-ranking signals. During training, RePro learns a transferable representation via shadow fine-tuning, and uses a lightweight linear head to infer provenance on unseen victims without training-data access. Empirically, RePro consistently attains strong performance and transfers well across refiners, suggesting that it exploits refiner-agnostic distribution shifts rather than rewrite-style artifacts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction tuning increasingly relies on LLM-based prompt refinement, where prompts in the training corpus are selectively rewritten by an external refiner to improve clarity and instruction alignment. This motivates an instance-level audit problem: for a fine-tuned model and a training prompt-response pair, can we infer whether the model was trained on the original prompt or its LLM-refined version within a mixed corpus? This matters for dataset governance and dispute resolution when training data are contested. However, it is non-trivial in practice: refined and raw instances are interleaved in the training corpus with unknown, source-dependent mixture ratios, making it harder to develop provenance methods that generalize across models and training setups. In this paper, we formalize this audit task as Refinement Provenance Inference (RPI) and show that prompt refinement yields stable, detectable shifts in teacher-forced token distributions, even when semantic differences are not obvious. Building on this phenomenon, we propose RePro, a logit-based provenance framework that fuses teacher-forced likelihood features with logit-ranking signals. During training, RePro learns a transferable representation via shadow fine-tuning, and uses a lightweight linear head to infer provenance on unseen victims without training-data access. Empirically, RePro consistently attains strong performance and transfers well across refiners, suggesting that it exploits refiner-agnostic distribution shifts rather than rewrite-style artifacts."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-05T10:16:41Z",
                "published_parsed": [
                    2026,
                    1,
                    5,
                    10,
                    16,
                    41,
                    0,
                    5,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Bo Yin"
                    },
                    {
                        "name": "Qi Li"
                    },
                    {
                        "name": "Runpeng Yu"
                    },
                    {
                        "name": "Xinchao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xinchao Wang"
                },
                "author": "Xinchao Wang"
            },
            {
                "id": "http://arxiv.org/abs/2506.09040v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2506.09040v2",
                "title": "Autoregressive Semantic Visual Reconstruction Helps VLMs Understand Better",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive Semantic Visual Reconstruction Helps VLMs Understand Better"
                },
                "updated": "2026-01-05T10:14:19Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    10,
                    14,
                    19,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2506.09040v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2506.09040v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Typical large vision-language models (LVLMs) apply autoregressive supervision solely to textual sequences, without fully incorporating the visual modality into the learning process. This results in three key limitations: (1) an inability to utilize images without accompanying captions, (2) the risk that captions omit critical visual details, and (3) the challenge that certain vision-centric content cannot be adequately conveyed through text. As a result, current LVLMs often prioritize vision-to-language alignment while potentially overlooking fine-grained visual information. While some prior works have explored autoregressive image generation, effectively leveraging autoregressive visual supervision to enhance image understanding remains an open challenge. In this paper, we introduce Autoregressive Semantic Visual Reconstruction (ASVR), which enables joint learning of visual and textual modalities within a unified autoregressive framework. We show that autoregressively reconstructing the raw visual appearance of images does not enhance and may even impair multimodal understanding. In contrast, autoregressively reconstructing the semantic representation of images consistently improves comprehension. Notably, we find that even when models are given continuous image features as input, they can effectively reconstruct discrete semantic tokens, resulting in stable and consistent improvements across a wide range of multimodal understanding benchmarks. Our approach delivers significant performance gains across varying data scales (556k-2M) and types of LLM bacbones. Specifically, ASVR improves LLaVA-1.5 by 5% in average scores across 14 multimodal benchmarks. The code is available at https://github.com/AlenjandroWang/ASVR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Typical large vision-language models (LVLMs) apply autoregressive supervision solely to textual sequences, without fully incorporating the visual modality into the learning process. This results in three key limitations: (1) an inability to utilize images without accompanying captions, (2) the risk that captions omit critical visual details, and (3) the challenge that certain vision-centric content cannot be adequately conveyed through text. As a result, current LVLMs often prioritize vision-to-language alignment while potentially overlooking fine-grained visual information. While some prior works have explored autoregressive image generation, effectively leveraging autoregressive visual supervision to enhance image understanding remains an open challenge. In this paper, we introduce Autoregressive Semantic Visual Reconstruction (ASVR), which enables joint learning of visual and textual modalities within a unified autoregressive framework. We show that autoregressively reconstructing the raw visual appearance of images does not enhance and may even impair multimodal understanding. In contrast, autoregressively reconstructing the semantic representation of images consistently improves comprehension. Notably, we find that even when models are given continuous image features as input, they can effectively reconstruct discrete semantic tokens, resulting in stable and consistent improvements across a wide range of multimodal understanding benchmarks. Our approach delivers significant performance gains across varying data scales (556k-2M) and types of LLM bacbones. Specifically, ASVR improves LLaVA-1.5 by 5% in average scores across 14 multimodal benchmarks. The code is available at https://github.com/AlenjandroWang/ASVR."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-06-10T17:57:50Z",
                "published_parsed": [
                    2025,
                    6,
                    10,
                    17,
                    57,
                    50,
                    1,
                    161,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Dianyi Wang"
                    },
                    {
                        "name": "Wei Song"
                    },
                    {
                        "name": "Yikun Wang"
                    },
                    {
                        "name": "Siyuan Wang"
                    },
                    {
                        "name": "Kaicheng Yu"
                    },
                    {
                        "name": "Zhongyu Wei"
                    },
                    {
                        "name": "Jiaqi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jiaqi Wang"
                },
                "author": "Jiaqi Wang"
            },
            {
                "id": "http://arxiv.org/abs/2601.01954v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.01954v1",
                "title": "Reporting LLM Prompting in Automated Software Engineering: A Guideline Based on Current Practices and Expectations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reporting LLM Prompting in Automated Software Engineering: A Guideline Based on Current Practices and Expectations"
                },
                "updated": "2026-01-05T10:01:20Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    10,
                    1,
                    20,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.01954v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.01954v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models, particularly decoder-only generative models such as GPT, are increasingly used to automate Software Engineering tasks. These models are primarily guided through natural language prompts, making prompt engineering a critical factor in system performance and behavior. Despite their growing role in SE research, prompt-related decisions are rarely documented in a systematic or transparent manner, hindering reproducibility and comparability across studies. To address this gap, we conducted a two-phase empirical study. First, we analyzed nearly 300 papers published at the top-3 SE conferences since 2022 to assess how prompt design, testing, and optimization are currently reported. Second, we surveyed 105 program committee members from these conferences to capture their expectations for prompt reporting in LLM-driven research. Based on the findings, we derived a structured guideline that distinguishes essential, desirable, and exceptional reporting elements. Our results reveal significant misalignment between current practices and reviewer expectations, particularly regarding version disclosure, prompt justification, and threats to validity. We present our guideline as a step toward improving transparency, reproducibility, and methodological rigor in LLM-based SE research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models, particularly decoder-only generative models such as GPT, are increasingly used to automate Software Engineering tasks. These models are primarily guided through natural language prompts, making prompt engineering a critical factor in system performance and behavior. Despite their growing role in SE research, prompt-related decisions are rarely documented in a systematic or transparent manner, hindering reproducibility and comparability across studies. To address this gap, we conducted a two-phase empirical study. First, we analyzed nearly 300 papers published at the top-3 SE conferences since 2022 to assess how prompt design, testing, and optimization are currently reported. Second, we surveyed 105 program committee members from these conferences to capture their expectations for prompt reporting in LLM-driven research. Based on the findings, we derived a structured guideline that distinguishes essential, desirable, and exceptional reporting elements. Our results reveal significant misalignment between current practices and reviewer expectations, particularly regarding version disclosure, prompt justification, and threats to validity. We present our guideline as a step toward improving transparency, reproducibility, and methodological rigor in LLM-based SE research."
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-05T10:01:20Z",
                "published_parsed": [
                    2026,
                    1,
                    5,
                    10,
                    1,
                    20,
                    0,
                    5,
                    0
                ],
                "arxiv_comment": "To be published at The 3rd ACM International Conference on AI Foundation Models and Software Engineering FORGE 2026",
                "arxiv_primary_category": {
                    "term": "cs.SE"
                },
                "authors": [
                    {
                        "name": "Alexander Korn"
                    },
                    {
                        "name": "Lea Zaruchas"
                    },
                    {
                        "name": "Chetan Arora"
                    },
                    {
                        "name": "Andreas Metzger"
                    },
                    {
                        "name": "Sven Smolka"
                    },
                    {
                        "name": "Fanyu Wang"
                    },
                    {
                        "name": "Andreas Vogelsang"
                    }
                ],
                "author_detail": {
                    "name": "Andreas Vogelsang"
                },
                "author": "Andreas Vogelsang"
            },
            {
                "id": "http://arxiv.org/abs/2601.01952v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.01952v1",
                "title": "Context-Adaptive Requirements Defect Prediction through Human-LLM Collaboration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context-Adaptive Requirements Defect Prediction through Human-LLM Collaboration"
                },
                "updated": "2026-01-05T10:00:14Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    10,
                    0,
                    14,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.01952v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.01952v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Automated requirements assessment traditionally relies on universal patterns as proxies for defectiveness, implemented through rule-based heuristics or machine learning classifiers trained on large annotated datasets. However, what constitutes a \"defect\" is inherently context-dependent and varies across projects, domains, and stakeholder interpretations. In this paper, we propose a Human-LLM Collaboration (HLC) approach that treats defect prediction as an adaptive process rather than a static classification task. HLC leverages LLM Chain-of-Thought reasoning in a feedback loop: users validate predictions alongside their explanations, and these validated examples adaptively guide future predictions through few-shot learning. We evaluate this approach using the weak word smell on the QuRE benchmark of 1,266 annotated Mercedes-Benz requirements. Our results show that HLC effectively adapts to the provision of validated examples, with rapid performance gains from as few as 20 validated examples. Incorporating validated explanations, not just labels, enables HLC to substantially outperform both standard few-shot prompting and fine-tuned BERT models while maintaining high recall. These results highlight how the in-context and Chain-of-Thought learning capabilities of LLMs enable adaptive classification approaches that move beyond one-size-fits-all models, creating opportunities for tools that learn continuously from stakeholder feedback.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated requirements assessment traditionally relies on universal patterns as proxies for defectiveness, implemented through rule-based heuristics or machine learning classifiers trained on large annotated datasets. However, what constitutes a \"defect\" is inherently context-dependent and varies across projects, domains, and stakeholder interpretations. In this paper, we propose a Human-LLM Collaboration (HLC) approach that treats defect prediction as an adaptive process rather than a static classification task. HLC leverages LLM Chain-of-Thought reasoning in a feedback loop: users validate predictions alongside their explanations, and these validated examples adaptively guide future predictions through few-shot learning. We evaluate this approach using the weak word smell on the QuRE benchmark of 1,266 annotated Mercedes-Benz requirements. Our results show that HLC effectively adapts to the provision of validated examples, with rapid performance gains from as few as 20 validated examples. Incorporating validated explanations, not just labels, enables HLC to substantially outperform both standard few-shot prompting and fine-tuned BERT models while maintaining high recall. These results highlight how the in-context and Chain-of-Thought learning capabilities of LLMs enable adaptive classification approaches that move beyond one-size-fits-all models, creating opportunities for tools that learn continuously from stakeholder feedback."
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-05T10:00:14Z",
                "published_parsed": [
                    2026,
                    1,
                    5,
                    10,
                    0,
                    14,
                    0,
                    5,
                    0
                ],
                "arxiv_comment": "Accepted at ICSE-NIER 2026",
                "arxiv_primary_category": {
                    "term": "cs.SE"
                },
                "authors": [
                    {
                        "name": "Max Unterbusch"
                    },
                    {
                        "name": "Andreas Vogelsang"
                    }
                ],
                "author_detail": {
                    "name": "Andreas Vogelsang"
                },
                "author": "Andreas Vogelsang"
            },
            {
                "id": "http://arxiv.org/abs/2509.26013v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2509.26013v2",
                "title": "User-Centric Comparison of 5G NTN and DVB-S2/RCS2 Using OpenAirInterface and OpenSAND",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "User-Centric Comparison of 5G NTN and DVB-S2/RCS2 Using OpenAirInterface and OpenSAND"
                },
                "updated": "2026-01-05T09:56:43Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    9,
                    56,
                    43,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2509.26013v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2509.26013v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.1109/FNWF66845.2025.11317602",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "The integration of satellite networks into next-generation mobile communication systems has gained considerable momentum with the advent of 5G Non-Terrestrial Networks (5G-NTN). Since established technologies like DVB-S2/RCS2 are already widely used for satellite broadband, a detailed comparison with emerging 5G NTN solutions is necessary to understand their relative merits and guide deployment decisions. This paper presents a user-centric, end-to-end evaluation of these technologies under realistic traffic conditions, showing how differences in architecture and protocols impact application-layer performance. Utilizing the 6G Sandbox platform, we employ OpenAirInterface to emulate 5G NTN and OpenSAND for DVB-S2/RCS2, replicating transparent payload GEO satellite scenarios under uniform downlink conditions. A range of real-world applications, such as web browsing, file downloads, and video streaming, are tested across both systems and systematically analyzed. While the emulation lacks real-time capability, it reveals key strengths and limitations of each approach, helping identify suitable deployment scenarios for 5G NTN and DVB-S2/RCS2.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of satellite networks into next-generation mobile communication systems has gained considerable momentum with the advent of 5G Non-Terrestrial Networks (5G-NTN). Since established technologies like DVB-S2/RCS2 are already widely used for satellite broadband, a detailed comparison with emerging 5G NTN solutions is necessary to understand their relative merits and guide deployment decisions. This paper presents a user-centric, end-to-end evaluation of these technologies under realistic traffic conditions, showing how differences in architecture and protocols impact application-layer performance. Utilizing the 6G Sandbox platform, we employ OpenAirInterface to emulate 5G NTN and OpenSAND for DVB-S2/RCS2, replicating transparent payload GEO satellite scenarios under uniform downlink conditions. A range of real-world applications, such as web browsing, file downloads, and video streaming, are tested across both systems and systematically analyzed. While the emulation lacks real-time capability, it reveals key strengths and limitations of each approach, helping identify suitable deployment scenarios for 5G NTN and DVB-S2/RCS2."
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-09-30T09:42:07Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    9,
                    42,
                    7,
                    1,
                    273,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI"
                },
                "arxiv_journal_ref": "2025 IEEE Future Networks World Forum (FNWF)",
                "authors": [
                    {
                        "name": "Sumit Kumar"
                    },
                    {
                        "name": "Juan Carlos Estrada-Jimenez"
                    },
                    {
                        "name": "Ion Turcanu"
                    }
                ],
                "author_detail": {
                    "name": "Ion Turcanu"
                },
                "author": "Ion Turcanu",
                "arxiv_doi": "10.1109/FNWF66845.2025.11317602"
            },
            {
                "id": "http://arxiv.org/abs/2601.01948v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.01948v1",
                "title": "Learning Diffusion Policy from Primitive Skills for Robot Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Diffusion Policy from Primitive Skills for Robot Manipulation"
                },
                "updated": "2026-01-05T09:56:24Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    9,
                    56,
                    24,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.01948v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.01948v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Diffusion policies (DP) have recently shown great promise for generating actions in robotic manipulation. However, existing approaches often rely on global instructions to produce short-term control signals, which can result in misalignment in action generation. We conjecture that the primitive skills, referred to as fine-grained, short-horizon manipulations, such as ``move up'' and ``open the gripper'', provide a more intuitive and effective interface for robot learning. To bridge this gap, we propose SDP, a skill-conditioned DP that integrates interpretable skill learning with conditional action planning. SDP abstracts eight reusable primitive skills across tasks and employs a vision-language model to extract discrete representations from visual observations and language instructions. Based on them, a lightweight router network is designed to assign a desired primitive skill for each state, which helps construct a single-skill policy to generate skill-aligned actions. By decomposing complex tasks into a sequence of primitive skills and selecting a single-skill policy, SDP ensures skill-consistent behavior across diverse tasks. Extensive experiments on two challenging simulation benchmarks and real-world robot deployments demonstrate that SDP consistently outperforms SOTA methods, providing a new paradigm for skill-based robot learning with diffusion policies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion policies (DP) have recently shown great promise for generating actions in robotic manipulation. However, existing approaches often rely on global instructions to produce short-term control signals, which can result in misalignment in action generation. We conjecture that the primitive skills, referred to as fine-grained, short-horizon manipulations, such as ``move up'' and ``open the gripper'', provide a more intuitive and effective interface for robot learning. To bridge this gap, we propose SDP, a skill-conditioned DP that integrates interpretable skill learning with conditional action planning. SDP abstracts eight reusable primitive skills across tasks and employs a vision-language model to extract discrete representations from visual observations and language instructions. Based on them, a lightweight router network is designed to assign a desired primitive skill for each state, which helps construct a single-skill policy to generate skill-aligned actions. By decomposing complex tasks into a sequence of primitive skills and selecting a single-skill policy, SDP ensures skill-consistent behavior across diverse tasks. Extensive experiments on two challenging simulation benchmarks and real-world robot deployments demonstrate that SDP consistently outperforms SOTA methods, providing a new paradigm for skill-based robot learning with diffusion policies."
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-05T09:56:24Z",
                "published_parsed": [
                    2026,
                    1,
                    5,
                    9,
                    56,
                    24,
                    0,
                    5,
                    0
                ],
                "arxiv_comment": "Accepted to AAAI2026",
                "arxiv_primary_category": {
                    "term": "cs.RO"
                },
                "authors": [
                    {
                        "name": "Zhihao Gu"
                    },
                    {
                        "name": "Ming Yang"
                    },
                    {
                        "name": "Difan Zou"
                    },
                    {
                        "name": "Dong Xu"
                    }
                ],
                "author_detail": {
                    "name": "Dong Xu"
                },
                "author": "Dong Xu"
            },
            {
                "id": "http://arxiv.org/abs/2601.01946v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.01946v1",
                "title": "From Metrics to Meaning: Insights from a Mixed-Methods Field Experiment on Retail Robot Deployment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Metrics to Meaning: Insights from a Mixed-Methods Field Experiment on Retail Robot Deployment"
                },
                "updated": "2026-01-05T09:54:33Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    9,
                    54,
                    33,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.01946v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.01946v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We report a mixed-methods field experiment of a conversational service robot deployed under everyday staffing discretion in a live bedding store. Over 12 days we alternated three conditions--Baseline (no robot), Robot-only, and Robot+Fixture--and video-annotated the service funnel from passersby to purchase. An explanatory sequential design then used six post-experiment staff interviews to interpret the quantitative patterns.\n  Quantitatively, the robot increased stopping per passerby (highest with the fixture), yet clerk-led downstream steps per stopper--clerk approach, store entry, assisted experience, and purchase--decreased. Interviews explained this divergence: clerks avoided interrupting ongoing robot-customer talk, struggled with ambiguous timing amid conversational latency, and noted child-centered attraction that often satisfied curiosity at the doorway. The fixture amplified visibility but also anchored encounters at the threshold, creating a well-defined micro-space where needs could ``close'' without moving inside.\n  We synthesize these strands into an integrative account from the initial show of interest on the part of a customer to their entering the store and derive actionable guidance. The results advance the understanding of interactions between customers, staff members, and the robot and offer practical recommendations for deploying service robots in high-touch retail.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We report a mixed-methods field experiment of a conversational service robot deployed under everyday staffing discretion in a live bedding store. Over 12 days we alternated three conditions--Baseline (no robot), Robot-only, and Robot+Fixture--and video-annotated the service funnel from passersby to purchase. An explanatory sequential design then used six post-experiment staff interviews to interpret the quantitative patterns.\n  Quantitatively, the robot increased stopping per passerby (highest with the fixture), yet clerk-led downstream steps per stopper--clerk approach, store entry, assisted experience, and purchase--decreased. Interviews explained this divergence: clerks avoided interrupting ongoing robot-customer talk, struggled with ambiguous timing amid conversational latency, and noted child-centered attraction that often satisfied curiosity at the doorway. The fixture amplified visibility but also anchored encounters at the threshold, creating a well-defined micro-space where needs could ``close'' without moving inside.\n  We synthesize these strands into an integrative account from the initial show of interest on the part of a customer to their entering the store and derive actionable guidance. The results advance the understanding of interactions between customers, staff members, and the robot and offer practical recommendations for deploying service robots in high-touch retail."
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-05T09:54:33Z",
                "published_parsed": [
                    2026,
                    1,
                    5,
                    9,
                    54,
                    33,
                    0,
                    5,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO"
                },
                "authors": [
                    {
                        "name": "Sichao Song"
                    },
                    {
                        "name": "Yuki Okafuji"
                    },
                    {
                        "name": "Takuya Iwamoto"
                    },
                    {
                        "name": "Jun Baba"
                    },
                    {
                        "name": "Hiroshi Ishiguro"
                    }
                ],
                "author_detail": {
                    "name": "Hiroshi Ishiguro"
                },
                "author": "Hiroshi Ishiguro"
            },
            {
                "id": "http://arxiv.org/abs/2601.01931v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.01931v1",
                "title": "DéjàQ: Open-Ended Evolution of Diverse, Learnable and Verifiable Problems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DéjàQ: Open-Ended Evolution of Diverse, Learnable and Verifiable Problems"
                },
                "updated": "2026-01-05T09:27:49Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    9,
                    27,
                    49,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.01931v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.01931v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Recent advances in reasoning models have yielded impressive results in mathematics and coding. However, most approaches rely on static datasets, which have been suggested to encourage memorisation and limit generalisation. We introduce DéjàQ, a framework that departs from this paradigm by jointly evolving a diverse set of synthetic mathematical problems alongside model training. This evolutionary process adapts to the model's ability throughout training, optimising problems for learnability. We propose two LLM-driven mutation strategies in which the model itself mutates the training data, either by altering contextual details or by directly modifying problem structure. We find that the model can generate novel and meaningful problems, and that these LLM-driven mutations improve RL training. We analyse key aspects of DéjàQ, including the validity of generated problems and computational overhead. Our results underscore the potential of dynamically evolving training data to enhance mathematical reasoning and indicate broader applicability, which we will support by open-sourcing our code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in reasoning models have yielded impressive results in mathematics and coding. However, most approaches rely on static datasets, which have been suggested to encourage memorisation and limit generalisation. We introduce DéjàQ, a framework that departs from this paradigm by jointly evolving a diverse set of synthetic mathematical problems alongside model training. This evolutionary process adapts to the model's ability throughout training, optimising problems for learnability. We propose two LLM-driven mutation strategies in which the model itself mutates the training data, either by altering contextual details or by directly modifying problem structure. We find that the model can generate novel and meaningful problems, and that these LLM-driven mutations improve RL training. We analyse key aspects of DéjàQ, including the validity of generated problems and computational overhead. Our results underscore the potential of dynamically evolving training data to enhance mathematical reasoning and indicate broader applicability, which we will support by open-sourcing our code."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-05T09:27:49Z",
                "published_parsed": [
                    2026,
                    1,
                    5,
                    9,
                    27,
                    49,
                    0,
                    5,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Willem Röpke"
                    },
                    {
                        "name": "Samuel Coward"
                    },
                    {
                        "name": "Andrei Lupu"
                    },
                    {
                        "name": "Thomas Foster"
                    },
                    {
                        "name": "Tim Rocktäschel"
                    },
                    {
                        "name": "Jakob Foerster"
                    }
                ],
                "author_detail": {
                    "name": "Jakob Foerster"
                },
                "author": "Jakob Foerster"
            },
            {
                "id": "http://arxiv.org/abs/2601.01925v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.01925v1",
                "title": "AR-MOT: Autoregressive Multi-object Tracking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AR-MOT: Autoregressive Multi-object Tracking"
                },
                "updated": "2026-01-05T09:17:28Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    9,
                    17,
                    28,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.01925v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.01925v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "As multi-object tracking (MOT) tasks continue to evolve toward more general and multi-modal scenarios, the rigid and task-specific architectures of existing MOT methods increasingly hinder their applicability across diverse tasks and limit flexibility in adapting to new tracking formulations. Most approaches rely on fixed output heads and bespoke tracking pipelines, making them difficult to extend to more complex or instruction-driven tasks. To address these limitations, we propose AR-MOT, a novel autoregressive paradigm that formulates MOT as a sequence generation task within a large language model (LLM) framework. This design enables the model to output structured results through flexible sequence construction, without requiring any task-specific heads. To enhance region-level visual perception, we introduce an Object Tokenizer based on a pretrained detector. To mitigate the misalignment between global and regional features, we propose a Region-Aware Alignment (RAA) module, and to support long-term tracking, we design a Temporal Memory Fusion (TMF) module that caches historical object tokens. AR-MOT offers strong potential for extensibility, as new modalities or instructions can be integrated by simply modifying the output sequence format without altering the model architecture. Extensive experiments on MOT17 and DanceTrack validate the feasibility of our approach, achieving performance comparable to state-of-the-art methods while laying the foundation for more general and flexible MOT systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As multi-object tracking (MOT) tasks continue to evolve toward more general and multi-modal scenarios, the rigid and task-specific architectures of existing MOT methods increasingly hinder their applicability across diverse tasks and limit flexibility in adapting to new tracking formulations. Most approaches rely on fixed output heads and bespoke tracking pipelines, making them difficult to extend to more complex or instruction-driven tasks. To address these limitations, we propose AR-MOT, a novel autoregressive paradigm that formulates MOT as a sequence generation task within a large language model (LLM) framework. This design enables the model to output structured results through flexible sequence construction, without requiring any task-specific heads. To enhance region-level visual perception, we introduce an Object Tokenizer based on a pretrained detector. To mitigate the misalignment between global and regional features, we propose a Region-Aware Alignment (RAA) module, and to support long-term tracking, we design a Temporal Memory Fusion (TMF) module that caches historical object tokens. AR-MOT offers strong potential for extensibility, as new modalities or instructions can be integrated by simply modifying the output sequence format without altering the model architecture. Extensive experiments on MOT17 and DanceTrack validate the feasibility of our approach, achieving performance comparable to state-of-the-art methods while laying the foundation for more general and flexible MOT systems."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-05T09:17:28Z",
                "published_parsed": [
                    2026,
                    1,
                    5,
                    9,
                    17,
                    28,
                    0,
                    5,
                    0
                ],
                "arxiv_comment": "12 pages, 5 figures",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Lianjie Jia"
                    },
                    {
                        "name": "Yuhan Wu"
                    },
                    {
                        "name": "Binghao Ran"
                    },
                    {
                        "name": "Yifan Wang"
                    },
                    {
                        "name": "Lijun Wang"
                    },
                    {
                        "name": "Huchuan Lu"
                    }
                ],
                "author_detail": {
                    "name": "Huchuan Lu"
                },
                "author": "Huchuan Lu"
            },
            {
                "id": "http://arxiv.org/abs/2510.23163v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.23163v2",
                "title": "Beyond Direct Generation: A Decomposed Approach to Well-Crafted Screenwriting with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Direct Generation: A Decomposed Approach to Well-Crafted Screenwriting with LLMs"
                },
                "updated": "2026-01-05T09:07:45Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    9,
                    7,
                    45,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.23163v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.23163v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The screenplay serves as the foundation for television production, defining narrative structure, character development, and dialogue. While Large Language Models (LLMs) show great potential in creative writing, direct end-to-end generation approaches often fail to produce well-crafted screenplays. We argue this failure stems from forcing a single model to simultaneously master two disparate capabilities: creative narrative construction and rigid format adherence. The resulting outputs may mimic superficial style but lack the deep structural integrity and storytelling substance required for professional use. To enable LLMs to generate high-quality screenplays, we introduce Dual-Stage Refinement (DSR), a decomposed framework that decouples creative narrative generation from format conversion. The first stage transforms a brief outline into rich, novel-style prose. The second stage refines this narrative into a professionally formatted screenplay. This separation enables the model to specialize in one distinct capability at each stage. A key challenge in implementing DSR is the scarcity of paired outline-to-novel training data. We address this through hybrid data synthesis: reverse synthesis deconstructs existing screenplays into structured inputs, while forward synthesis leverages these inputs to generate high-quality narrative texts as training targets. Blind evaluations by professional screenwriters show that DSR achieves a 75% win rate against strong baselines like Gemini-2.5-Pro and reaches 82.7% of human-level performance. Our work demonstrates that decomposed generation architecture with tailored data synthesis effectively specializes LLMs in complex creative domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The screenplay serves as the foundation for television production, defining narrative structure, character development, and dialogue. While Large Language Models (LLMs) show great potential in creative writing, direct end-to-end generation approaches often fail to produce well-crafted screenplays. We argue this failure stems from forcing a single model to simultaneously master two disparate capabilities: creative narrative construction and rigid format adherence. The resulting outputs may mimic superficial style but lack the deep structural integrity and storytelling substance required for professional use. To enable LLMs to generate high-quality screenplays, we introduce Dual-Stage Refinement (DSR), a decomposed framework that decouples creative narrative generation from format conversion. The first stage transforms a brief outline into rich, novel-style prose. The second stage refines this narrative into a professionally formatted screenplay. This separation enables the model to specialize in one distinct capability at each stage. A key challenge in implementing DSR is the scarcity of paired outline-to-novel training data. We address this through hybrid data synthesis: reverse synthesis deconstructs existing screenplays into structured inputs, while forward synthesis leverages these inputs to generate high-quality narrative texts as training targets. Blind evaluations by professional screenwriters show that DSR achieves a 75% win rate against strong baselines like Gemini-2.5-Pro and reaches 82.7% of human-level performance. Our work demonstrates that decomposed generation architecture with tailored data synthesis effectively specializes LLMs in complex creative domains."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-27T09:41:29Z",
                "published_parsed": [
                    2025,
                    10,
                    27,
                    9,
                    41,
                    29,
                    0,
                    300,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Hang Lei"
                    },
                    {
                        "name": "Shengyi Zong"
                    },
                    {
                        "name": "Zhaoyan Li"
                    },
                    {
                        "name": "Ziren Zhou"
                    },
                    {
                        "name": "Hao Liu"
                    }
                ],
                "author_detail": {
                    "name": "Hao Liu"
                },
                "author": "Hao Liu"
            },
            {
                "id": "http://arxiv.org/abs/2507.20704v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2507.20704v2",
                "title": "Text2VLM: Adapting Text-Only Datasets to Evaluate Alignment Training in Visual Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text2VLM: Adapting Text-Only Datasets to Evaluate Alignment Training in Visual Language Models"
                },
                "updated": "2026-01-05T09:07:09Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    9,
                    7,
                    9,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2507.20704v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2507.20704v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The increasing integration of Visual Language Models (VLMs) into AI systems necessitates robust model alignment, especially when handling multimodal content that combines text and images. Existing evaluation datasets heavily lean towards text-only prompts, leaving visual vulnerabilities under evaluated. To address this gap, we propose \\textbf{Text2VLM}, a novel multi-stage pipeline that adapts text-only datasets into multimodal formats, specifically designed to evaluate the resilience of VLMs against typographic prompt injection attacks. The Text2VLM pipeline identifies harmful content in the original text and converts it into a typographic image, creating a multimodal prompt for VLMs. Also, our evaluation of open-source VLMs highlights their increased susceptibility to prompt injection when visual inputs are introduced, revealing critical weaknesses in the current models' alignment. This is in addition to a significant performance gap compared to closed-source frontier models. We validate Text2VLM through human evaluations, ensuring the alignment of extracted salient concepts; text summarization and output classification align with human expectations. Text2VLM provides a scalable tool for comprehensive safety assessment, contributing to the development of more robust safety mechanisms for VLMs. By enhancing the evaluation of multimodal vulnerabilities, Text2VLM plays a role in advancing the safe deployment of VLMs in diverse, real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing integration of Visual Language Models (VLMs) into AI systems necessitates robust model alignment, especially when handling multimodal content that combines text and images. Existing evaluation datasets heavily lean towards text-only prompts, leaving visual vulnerabilities under evaluated. To address this gap, we propose \\textbf{Text2VLM}, a novel multi-stage pipeline that adapts text-only datasets into multimodal formats, specifically designed to evaluate the resilience of VLMs against typographic prompt injection attacks. The Text2VLM pipeline identifies harmful content in the original text and converts it into a typographic image, creating a multimodal prompt for VLMs. Also, our evaluation of open-source VLMs highlights their increased susceptibility to prompt injection when visual inputs are introduced, revealing critical weaknesses in the current models' alignment. This is in addition to a significant performance gap compared to closed-source frontier models. We validate Text2VLM through human evaluations, ensuring the alignment of extracted salient concepts; text summarization and output classification align with human expectations. Text2VLM provides a scalable tool for comprehensive safety assessment, contributing to the development of more robust safety mechanisms for VLMs. By enhancing the evaluation of multimodal vulnerabilities, Text2VLM plays a role in advancing the safe deployment of VLMs in diverse, real-world applications."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-07-28T10:57:44Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    10,
                    57,
                    44,
                    0,
                    209,
                    0
                ],
                "arxiv_comment": "9 pages, 9 figures. Jake Thomas served as Editor for this manuscript",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Gabriel Downer"
                    },
                    {
                        "name": "Sean Craven"
                    },
                    {
                        "name": "Damian Ruck"
                    },
                    {
                        "name": "Jake Thomas"
                    }
                ],
                "author_detail": {
                    "name": "Jake Thomas"
                },
                "author": "Jake Thomas"
            },
            {
                "id": "http://arxiv.org/abs/2512.20293v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20293v2",
                "title": "AprielGuard",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AprielGuard"
                },
                "updated": "2026-01-05T09:05:32Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    9,
                    5,
                    32,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20293v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20293v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Safeguarding large language models (LLMs) against unsafe or adversarial behavior is critical as they are increasingly deployed in conversational and agentic settings. Existing moderation tools often treat safety risks (e.g. toxicity, bias) and adversarial threats (e.g. prompt injections, jailbreaks) as separate problems, limiting their robustness and generalizability. We introduce AprielGuard, an 8B parameter safeguard model that unify these dimensions within a single taxonomy and learning framework. AprielGuard is trained on a diverse mix of open and synthetic data covering standalone prompts, multi-turn conversations, and agentic workflows, augmented with structured reasoning traces to improve interpretability. Across multiple public and proprietary benchmarks, AprielGuard achieves strong performance in detecting harmful content and adversarial manipulations, outperforming existing opensource guardrails such as Llama-Guard and Granite Guardian, particularly in multi-step and reasoning intensive scenarios. By releasing the model, we aim to advance transparent and reproducible research on reliable safeguards for LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Safeguarding large language models (LLMs) against unsafe or adversarial behavior is critical as they are increasingly deployed in conversational and agentic settings. Existing moderation tools often treat safety risks (e.g. toxicity, bias) and adversarial threats (e.g. prompt injections, jailbreaks) as separate problems, limiting their robustness and generalizability. We introduce AprielGuard, an 8B parameter safeguard model that unify these dimensions within a single taxonomy and learning framework. AprielGuard is trained on a diverse mix of open and synthetic data covering standalone prompts, multi-turn conversations, and agentic workflows, augmented with structured reasoning traces to improve interpretability. Across multiple public and proprietary benchmarks, AprielGuard achieves strong performance in detecting harmful content and adversarial manipulations, outperforming existing opensource guardrails such as Llama-Guard and Granite Guardian, particularly in multi-step and reasoning intensive scenarios. By releasing the model, we aim to advance transparent and reproducible research on reliable safeguards for LLMs."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T12:01:32Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    12,
                    1,
                    32,
                    1,
                    357,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Jaykumar Kasundra"
                    },
                    {
                        "name": "Anjaneya Praharaj"
                    },
                    {
                        "name": "Sourabh Surana"
                    },
                    {
                        "name": "Lakshmi Sirisha Chodisetty"
                    },
                    {
                        "name": "Sourav Sharma"
                    },
                    {
                        "name": "Abhigya Verma"
                    },
                    {
                        "name": "Abhishek Bhardwaj"
                    },
                    {
                        "name": "Debasish Kanhar"
                    },
                    {
                        "name": "Aakash Bhagat"
                    },
                    {
                        "name": "Khalil Slimi"
                    },
                    {
                        "name": "Seganrasan Subramanian"
                    },
                    {
                        "name": "Sathwik Tejaswi Madhusudhan"
                    },
                    {
                        "name": "Ranga Prasad Chenna"
                    },
                    {
                        "name": "Srinivas Sunkara"
                    }
                ],
                "author_detail": {
                    "name": "Srinivas Sunkara"
                },
                "author": "Srinivas Sunkara"
            },
            {
                "id": "http://arxiv.org/abs/2512.04415v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.04415v3",
                "title": "RoboBPP: Benchmarking Robotic Online Bin Packing with Physics-based Simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RoboBPP: Benchmarking Robotic Online Bin Packing with Physics-based Simulation"
                },
                "updated": "2026-01-05T09:02:01Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    9,
                    2,
                    1,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.04415v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.04415v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Physical feasibility in 3D bin packing is a key requirement in modern industrial logistics and robotic automation. With the growing adoption of industrial automation, online bin packing has gained increasing attention. However, inconsistencies in problem settings, test datasets, and evaluation metrics have hindered progress in the field, and there is a lack of a comprehensive benchmarking system. Direct testing on real hardware is costly, and building a realistic simulation environment is also challenging. To address these limitations, we introduce RoboBPP, a benchmarking system designed for robotic online bin packing. RoboBPP integrates a physics-based simulator to assess physical feasibility. In our simulation environment, we introduce a robotic arm and boxes at real-world scales to replicate real industrial packing workflows. By simulating conditions that arise in real industrial applications, we ensure that evaluated algorithms are practically deployable. In addition, prior studies often rely on synthetic datasets whose distributions differ from real-world industrial data. To address this issue, we collect three datasets from real industrial workflows, including assembly-line production, logistics packing, and furniture manufacturing. The benchmark comprises three carefully designed test settings and extends existing evaluation metrics with new metrics for structural stability and operational safety. We design a scoring system and derive a range of insights from the evaluation results. RoboBPP is fully open-source and is equipped with visualization tools and an online leaderboard, providing a reproducible and extensible foundation for future research and industrial applications (https://robot-bin-packing-benchmark.github.io).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Physical feasibility in 3D bin packing is a key requirement in modern industrial logistics and robotic automation. With the growing adoption of industrial automation, online bin packing has gained increasing attention. However, inconsistencies in problem settings, test datasets, and evaluation metrics have hindered progress in the field, and there is a lack of a comprehensive benchmarking system. Direct testing on real hardware is costly, and building a realistic simulation environment is also challenging. To address these limitations, we introduce RoboBPP, a benchmarking system designed for robotic online bin packing. RoboBPP integrates a physics-based simulator to assess physical feasibility. In our simulation environment, we introduce a robotic arm and boxes at real-world scales to replicate real industrial packing workflows. By simulating conditions that arise in real industrial applications, we ensure that evaluated algorithms are practically deployable. In addition, prior studies often rely on synthetic datasets whose distributions differ from real-world industrial data. To address this issue, we collect three datasets from real industrial workflows, including assembly-line production, logistics packing, and furniture manufacturing. The benchmark comprises three carefully designed test settings and extends existing evaluation metrics with new metrics for structural stability and operational safety. We design a scoring system and derive a range of insights from the evaluation results. RoboBPP is fully open-source and is equipped with visualization tools and an online leaderboard, providing a reproducible and extensible foundation for future research and industrial applications (https://robot-bin-packing-benchmark.github.io)."
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-04T03:24:03Z",
                "published_parsed": [
                    2025,
                    12,
                    4,
                    3,
                    24,
                    3,
                    3,
                    338,
                    0
                ],
                "arxiv_comment": "Under review at the International Journal of Robotics Research (IJRR)",
                "arxiv_primary_category": {
                    "term": "cs.RO"
                },
                "authors": [
                    {
                        "name": "Zhoufeng Wang"
                    },
                    {
                        "name": "Hang Zhao"
                    },
                    {
                        "name": "Juzhan Xu"
                    },
                    {
                        "name": "Shishun Zhang"
                    },
                    {
                        "name": "Zeyu Xiong"
                    },
                    {
                        "name": "Ruizhen Hu"
                    },
                    {
                        "name": "Chenyang Zhu"
                    },
                    {
                        "name": "Zecui Zeng"
                    },
                    {
                        "name": "Kai Xu"
                    }
                ],
                "author_detail": {
                    "name": "Kai Xu"
                },
                "author": "Kai Xu"
            },
            {
                "id": "http://arxiv.org/abs/2601.01915v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.01915v1",
                "title": "TalkPhoto: A Versatile Training-Free Conversational Assistant for Intelligent Image Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TalkPhoto: A Versatile Training-Free Conversational Assistant for Intelligent Image Editing"
                },
                "updated": "2026-01-05T09:00:32Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    9,
                    0,
                    32,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.01915v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.01915v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Thanks to the powerful language comprehension capabilities of Large Language Models (LLMs), existing instruction-based image editing methods have introduced Multimodal Large Language Models (MLLMs) to promote information exchange between instructions and images, ensuring the controllability and flexibility of image editing. However, these frameworks often build a multi-instruction dataset to train the model to handle multiple editing tasks, which is not only time-consuming and labor-intensive but also fails to achieve satisfactory results. In this paper, we present TalkPhoto, a versatile training-free image editing framework that facilitates precise image manipulation through conversational interaction. We instruct the open-source LLM with a specially designed prompt template to analyze user needs after receiving instructions and hierarchically invoke existing advanced editing methods, all without additional training. Moreover, we implement a plug-and-play and efficient invocation of image editing methods, allowing complex and unseen editing tasks to be integrated into the current framework, achieving stable and high-quality editing results. Extensive experiments demonstrate that our method not only provides more accurate invocation with fewer token consumption but also achieves higher editing quality across various image editing tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Thanks to the powerful language comprehension capabilities of Large Language Models (LLMs), existing instruction-based image editing methods have introduced Multimodal Large Language Models (MLLMs) to promote information exchange between instructions and images, ensuring the controllability and flexibility of image editing. However, these frameworks often build a multi-instruction dataset to train the model to handle multiple editing tasks, which is not only time-consuming and labor-intensive but also fails to achieve satisfactory results. In this paper, we present TalkPhoto, a versatile training-free image editing framework that facilitates precise image manipulation through conversational interaction. We instruct the open-source LLM with a specially designed prompt template to analyze user needs after receiving instructions and hierarchically invoke existing advanced editing methods, all without additional training. Moreover, we implement a plug-and-play and efficient invocation of image editing methods, allowing complex and unseen editing tasks to be integrated into the current framework, achieving stable and high-quality editing results. Extensive experiments demonstrate that our method not only provides more accurate invocation with fewer token consumption but also achieves higher editing quality across various image editing tasks."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-05T09:00:32Z",
                "published_parsed": [
                    2026,
                    1,
                    5,
                    9,
                    0,
                    32,
                    0,
                    5,
                    0
                ],
                "arxiv_comment": "a Conversational Assistant for Intelligent Image Editing",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Yujie Hu"
                    },
                    {
                        "name": "Zecheng Tang"
                    },
                    {
                        "name": "Xu Jiang"
                    },
                    {
                        "name": "Weiqi Li"
                    },
                    {
                        "name": "Jian Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jian Zhang"
                },
                "author": "Jian Zhang"
            }
        ]
    }
]