[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2503.18893v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18893v1",
                "updated": "2025-03-24T17:06:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    17,
                    6,
                    37,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T17:06:37Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    17,
                    6,
                    37,
                    0,
                    83,
                    0
                ],
                "title": "xKV: Cross-Layer SVD for KV-Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "xKV: Cross-Layer SVD for KV-Cache Compression"
                },
                "summary": "Large Language Models (LLMs) with long context windows enable powerful\napplications but come at the cost of high memory consumption to store the Key\nand Value states (KV-Cache). Recent studies attempted to merge KV-cache from\nmultiple layers into shared representations, yet these approaches either\nrequire expensive pretraining or rely on assumptions of high per-token cosine\nsimilarity across layers which generally does not hold in practice. We find\nthat the dominant singular vectors are remarkably well-aligned across multiple\nlayers of the KV-Cache. Exploiting this insight, we propose xKV, a simple\npost-training method that applies Singular Value Decomposition (SVD) on the\nKV-Cache of grouped layers. xKV consolidates the KV-Cache of multiple layers\ninto a shared low-rank subspace, significantly reducing KV-Cache sizes. Through\nextensive evaluations on the RULER long-context benchmark with widely-used LLMs\n(e.g., Llama-3.1 and Qwen2.5), xKV achieves up to 6.8x higher compression rates\nthan state-of-the-art inter-layer technique while improving accuracy by 2.7%.\nMoreover, xKV is compatible with the emerging Multi-Head Latent Attention (MLA)\n(e.g., DeepSeek-Coder-V2), yielding a notable 3x compression rates on coding\ntasks without performance degradation. These results highlight xKV's strong\ncapability and versatility in addressing memory bottlenecks for long-context\nLLM inference. Our code is publicly available at:\nhttps://github.com/abdelfattah-lab/xKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) with long context windows enable powerful\napplications but come at the cost of high memory consumption to store the Key\nand Value states (KV-Cache). Recent studies attempted to merge KV-cache from\nmultiple layers into shared representations, yet these approaches either\nrequire expensive pretraining or rely on assumptions of high per-token cosine\nsimilarity across layers which generally does not hold in practice. We find\nthat the dominant singular vectors are remarkably well-aligned across multiple\nlayers of the KV-Cache. Exploiting this insight, we propose xKV, a simple\npost-training method that applies Singular Value Decomposition (SVD) on the\nKV-Cache of grouped layers. xKV consolidates the KV-Cache of multiple layers\ninto a shared low-rank subspace, significantly reducing KV-Cache sizes. Through\nextensive evaluations on the RULER long-context benchmark with widely-used LLMs\n(e.g., Llama-3.1 and Qwen2.5), xKV achieves up to 6.8x higher compression rates\nthan state-of-the-art inter-layer technique while improving accuracy by 2.7%.\nMoreover, xKV is compatible with the emerging Multi-Head Latent Attention (MLA)\n(e.g., DeepSeek-Coder-V2), yielding a notable 3x compression rates on coding\ntasks without performance degradation. These results highlight xKV's strong\ncapability and versatility in addressing memory bottlenecks for long-context\nLLM inference. Our code is publicly available at:\nhttps://github.com/abdelfattah-lab/xKV."
                },
                "authors": [
                    {
                        "name": "Chi-Chih Chang"
                    },
                    {
                        "name": "Chien-Yu Lin"
                    },
                    {
                        "name": "Yash Akhauri"
                    },
                    {
                        "name": "Wei-Cheng Lin"
                    },
                    {
                        "name": "Kai-Chiang Wu"
                    },
                    {
                        "name": "Luis Ceze"
                    },
                    {
                        "name": "Mohamed S. Abdelfattah"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed S. Abdelfattah"
                },
                "author": "Mohamed S. Abdelfattah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18893v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18893v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13064v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13064v2",
                "updated": "2025-03-24T16:47:48Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    16,
                    47,
                    48,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-17T11:10:49Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    11,
                    10,
                    49,
                    0,
                    76,
                    0
                ],
                "title": "HERMES: High-Performance RISC-V Memory Hierarchy for ML Workloads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HERMES: High-Performance RISC-V Memory Hierarchy for ML Workloads"
                },
                "summary": "The growth of machine learning (ML) workloads has underscored the importance\nof efficient memory hierarchies to address bandwidth, latency, and scalability\nchallenges. HERMES focuses on optimizing memory subsystems for RISC-V\narchitectures to meet the computational needs of ML models such as CNNs, RNNs,\nand Transformers. This project explores state-of-the-art techniques such as\nadvanced prefetching, tensor-aware caching, and hybrid memory models. The\ncornerstone of HERMES is the integration of shared L3 caches with fine-grained\ncoherence protocols equipped with specialized pathways to deep-learning\naccelerators such as Gemmini. Simulation tools like Gem5 and DRAMSim2 were used\nto evaluate baseline performance and scalability under representative ML\nworkloads. The findings of this study highlight the design choices, and the\nanticipated challenges, paving the way for low-latency scalable memory\noperations for ML applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growth of machine learning (ML) workloads has underscored the importance\nof efficient memory hierarchies to address bandwidth, latency, and scalability\nchallenges. HERMES focuses on optimizing memory subsystems for RISC-V\narchitectures to meet the computational needs of ML models such as CNNs, RNNs,\nand Transformers. This project explores state-of-the-art techniques such as\nadvanced prefetching, tensor-aware caching, and hybrid memory models. The\ncornerstone of HERMES is the integration of shared L3 caches with fine-grained\ncoherence protocols equipped with specialized pathways to deep-learning\naccelerators such as Gemmini. Simulation tools like Gem5 and DRAMSim2 were used\nto evaluate baseline performance and scalability under representative ML\nworkloads. The findings of this study highlight the design choices, and the\nanticipated challenges, paving the way for low-latency scalable memory\noperations for ML applications."
                },
                "authors": [
                    {
                        "name": "Pranav Suryadevara"
                    }
                ],
                "author_detail": {
                    "name": "Pranav Suryadevara"
                },
                "author": "Pranav Suryadevara",
                "arxiv_comment": "5 pages, 5 figures. Individual Project",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13064v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13064v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "B.3.2; C.1.3; C.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18869v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18869v1",
                "updated": "2025-03-24T16:44:32Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    16,
                    44,
                    32,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T16:44:32Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    16,
                    44,
                    32,
                    0,
                    83,
                    0
                ],
                "title": "Reimagining Memory Access for LLM Inference: Compression-Aware Memory\n  Controller Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reimagining Memory Access for LLM Inference: Compression-Aware Memory\n  Controller Design"
                },
                "summary": "The efficiency of Large Language Model~(LLM) inference is often constrained\nby substantial memory bandwidth and capacity demands. Existing techniques, such\nas pruning, quantization, and mixture of experts/depth, reduce memory capacity\nand/or bandwidth consumption at the cost of slight degradation in inference\nquality. This paper introduces a design solution that further alleviates memory\nbottlenecks by enhancing the on-chip memory controller in AI accelerators to\nachieve two main objectives: (1) significantly reducing memory capacity and\nbandwidth usage through lossless block compression~(e.g., LZ4 and ZSTD) of\nmodel weights and key-value (KV) cache without compromising inference quality,\nand (2) enabling memory bandwidth and energy consumption to scale\nproportionally with context-dependent dynamic quantization. These goals are\naccomplished by equipping the on-chip memory controller with mechanisms to\nimprove fine-grained bit-level accessibility and compressibility of weights and\nKV cache through LLM-aware configuration of in-memory placement and\nrepresentation. Experimental results on publicly available LLMs demonstrate the\neffectiveness of this approach, showing memory footprint reductions of 25.2\\%\nfor model weights and 46.9\\% for KV cache. In addition, our hardware prototype\nat 4\\,GHz and 32 lanes (7\\,nm) achieves 8\\,TB/s throughput with a modest area\noverhead (under 3.8\\,mm\\(^2\\)), which underscores the viability of LLM-aware\nmemory control as a key to efficient large-scale inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The efficiency of Large Language Model~(LLM) inference is often constrained\nby substantial memory bandwidth and capacity demands. Existing techniques, such\nas pruning, quantization, and mixture of experts/depth, reduce memory capacity\nand/or bandwidth consumption at the cost of slight degradation in inference\nquality. This paper introduces a design solution that further alleviates memory\nbottlenecks by enhancing the on-chip memory controller in AI accelerators to\nachieve two main objectives: (1) significantly reducing memory capacity and\nbandwidth usage through lossless block compression~(e.g., LZ4 and ZSTD) of\nmodel weights and key-value (KV) cache without compromising inference quality,\nand (2) enabling memory bandwidth and energy consumption to scale\nproportionally with context-dependent dynamic quantization. These goals are\naccomplished by equipping the on-chip memory controller with mechanisms to\nimprove fine-grained bit-level accessibility and compressibility of weights and\nKV cache through LLM-aware configuration of in-memory placement and\nrepresentation. Experimental results on publicly available LLMs demonstrate the\neffectiveness of this approach, showing memory footprint reductions of 25.2\\%\nfor model weights and 46.9\\% for KV cache. In addition, our hardware prototype\nat 4\\,GHz and 32 lanes (7\\,nm) achieves 8\\,TB/s throughput with a modest area\noverhead (under 3.8\\,mm\\(^2\\)), which underscores the viability of LLM-aware\nmemory control as a key to efficient large-scale inference."
                },
                "authors": [
                    {
                        "name": "Rui Xie"
                    },
                    {
                        "name": "Asad Ul Haq"
                    },
                    {
                        "name": "Linsen Ma"
                    },
                    {
                        "name": "Yunhua Fang"
                    },
                    {
                        "name": "Zirak Burzin Engineer"
                    },
                    {
                        "name": "Liu Liu"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "author": "Tong Zhang",
                "arxiv_comment": "9 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18869v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18869v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18862v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18862v1",
                "updated": "2025-03-24T16:38:31Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    16,
                    38,
                    31,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T16:38:31Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    16,
                    38,
                    31,
                    0,
                    83,
                    0
                ],
                "title": "Exploring the Integration of Key-Value Attention Into Pure and Hybrid\n  Transformers for Semantic Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the Integration of Key-Value Attention Into Pure and Hybrid\n  Transformers for Semantic Segmentation"
                },
                "summary": "While CNNs were long considered state of the art for image processing, the\nintroduction of Transformer architectures has challenged this position. While\nachieving excellent results in image classification and segmentation,\nTransformers remain inherently reliant on large training datasets and remain\ncomputationally expensive. A newly introduced Transformer derivative named KV\nTransformer shows promising results in synthetic, NLP, and image classification\ntasks, while reducing complexity and memory usage. This is especially conducive\nto use cases where local inference is required, such as medical screening\napplications. We endeavoured to further evaluate the merit of KV Transformers\non semantic segmentation tasks, specifically in the domain of medical imaging.\nBy directly comparing traditional and KV variants of the same base\narchitectures, we provide further insight into the practical tradeoffs of\nreduced model complexity. We observe a notable reduction in parameter count and\nmultiply accumulate operations, while achieving similar performance from most\nof the KV variant models when directly compared to their QKV implementation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While CNNs were long considered state of the art for image processing, the\nintroduction of Transformer architectures has challenged this position. While\nachieving excellent results in image classification and segmentation,\nTransformers remain inherently reliant on large training datasets and remain\ncomputationally expensive. A newly introduced Transformer derivative named KV\nTransformer shows promising results in synthetic, NLP, and image classification\ntasks, while reducing complexity and memory usage. This is especially conducive\nto use cases where local inference is required, such as medical screening\napplications. We endeavoured to further evaluate the merit of KV Transformers\non semantic segmentation tasks, specifically in the domain of medical imaging.\nBy directly comparing traditional and KV variants of the same base\narchitectures, we provide further insight into the practical tradeoffs of\nreduced model complexity. We observe a notable reduction in parameter count and\nmultiply accumulate operations, while achieving similar performance from most\nof the KV variant models when directly compared to their QKV implementation."
                },
                "authors": [
                    {
                        "name": "DeShin Hwa"
                    },
                    {
                        "name": "Tobias Holmes"
                    },
                    {
                        "name": "Klaus Drechsler"
                    }
                ],
                "author_detail": {
                    "name": "Klaus Drechsler"
                },
                "author": "Klaus Drechsler",
                "arxiv_doi": "10.1007/978-3-658-47422-5_71",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-658-47422-5_71",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.18862v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18862v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "6 pages, 3 figures, Preprint. Final version published in:\n  Bildverarbeitung f\\\"ur die Medizin 2025, Springer. DOI:\n  https://doi.org/10.1007/978-3-658-47422-5_71",
                "arxiv_journal_ref": "Bildverarbeitung f\\\"ur die Medizin 2025. BVM 2025. Informatik\n  aktuell. Springer Vieweg, Wiesbaden, pp 305-310",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18773v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18773v1",
                "updated": "2025-03-24T15:22:41Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    15,
                    22,
                    41,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T15:22:41Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    15,
                    22,
                    41,
                    0,
                    83,
                    0
                ],
                "title": "BitDecoding: Unlocking Tensor Cores for Long-Context LLMs Decoding with\n  Low-Bit KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BitDecoding: Unlocking Tensor Cores for Long-Context LLMs Decoding with\n  Low-Bit KV Cache"
                },
                "summary": "The growing adoption of long-context Large Language Models (LLMs) has\nintroduced significant memory and computational challenges in autoregressive\ndecoding due to the expanding Key-Value (KV) cache. KV cache quantization has\nemerged as a promising solution, with prior work showing that 4-bit or even\n2-bit quantization can maintain model accuracy while reducing memory costs.\nHowever, despite these benefits, preliminary implementations for the low-bit KV\ncache struggle to deliver the expected speedup due to quantization and\ndequantization overheads and the lack of Tensor Cores utilization. In this\nwork, we propose BitDecoding, a GPU-optimized framework that unlocks Tensor\nCores for efficient decoding with low-bit KV cache. Efficiently leveraging\nTensor Cores for low-bit KV cache is challenging due to the dynamic nature of\nKV cache generation at each decoding step. BitDecoding addresses these\nchallenges with a Tensor Cores-Centric BitFusion Scheme that ensures data\nlayout compatibility to enable high utilization of Tensor Cores. Additionally,\nBitDecoding incorporates a warp-efficient parallel decoding kernel and a\nfine-grained asynchronous pipeline, minimizing dequantization overhead and\nimproving computational efficiency. Experiments show that BitDecoding achieves\nup to 7.5x speedup on RTX 4090, 4.8x on A100, and 8.9x on H100, compared to\nFP16 FlashDecoding-v2. It also outperforms the state-of-the-art low-bit KV\ncache implementation (QServe) by up to 4.3x. On LLaMA-3.1-8B with a 128K\nsequence length, BitDecoding reduces single-batch decoding latency by 3x,\ndemonstrating its effectiveness in long-context generation scenarios. The code\nis available at https://github.com/DD-DuDa/BitDecoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing adoption of long-context Large Language Models (LLMs) has\nintroduced significant memory and computational challenges in autoregressive\ndecoding due to the expanding Key-Value (KV) cache. KV cache quantization has\nemerged as a promising solution, with prior work showing that 4-bit or even\n2-bit quantization can maintain model accuracy while reducing memory costs.\nHowever, despite these benefits, preliminary implementations for the low-bit KV\ncache struggle to deliver the expected speedup due to quantization and\ndequantization overheads and the lack of Tensor Cores utilization. In this\nwork, we propose BitDecoding, a GPU-optimized framework that unlocks Tensor\nCores for efficient decoding with low-bit KV cache. Efficiently leveraging\nTensor Cores for low-bit KV cache is challenging due to the dynamic nature of\nKV cache generation at each decoding step. BitDecoding addresses these\nchallenges with a Tensor Cores-Centric BitFusion Scheme that ensures data\nlayout compatibility to enable high utilization of Tensor Cores. Additionally,\nBitDecoding incorporates a warp-efficient parallel decoding kernel and a\nfine-grained asynchronous pipeline, minimizing dequantization overhead and\nimproving computational efficiency. Experiments show that BitDecoding achieves\nup to 7.5x speedup on RTX 4090, 4.8x on A100, and 8.9x on H100, compared to\nFP16 FlashDecoding-v2. It also outperforms the state-of-the-art low-bit KV\ncache implementation (QServe) by up to 4.3x. On LLaMA-3.1-8B with a 128K\nsequence length, BitDecoding reduces single-batch decoding latency by 3x,\ndemonstrating its effectiveness in long-context generation scenarios. The code\nis available at https://github.com/DD-DuDa/BitDecoding."
                },
                "authors": [
                    {
                        "name": "Dayou Du"
                    },
                    {
                        "name": "Shijie Cao"
                    },
                    {
                        "name": "Jianyi Cheng"
                    },
                    {
                        "name": "Ting Cao"
                    },
                    {
                        "name": "Mao Yang"
                    }
                ],
                "author_detail": {
                    "name": "Mao Yang"
                },
                "author": "Mao Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18773v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18773v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05941v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05941v2",
                "updated": "2025-03-24T13:09:03Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    13,
                    9,
                    3,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-07T21:16:41Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    21,
                    16,
                    41,
                    4,
                    66,
                    0
                ],
                "title": "Choosing Augmentation Parameters in OSQP- A New Approach based on\n  Conjugate Directions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Choosing Augmentation Parameters in OSQP- A New Approach based on\n  Conjugate Directions"
                },
                "summary": "This work proposes a new method to select the augmentation parameters in the\noperator splitting quadratic program (OSQP) algorithm so as to reduce the\ncomputation time of overall algorithm. The selection is based upon the\ninformation of conjugate directions of the coefficient matrix of a linear\nsystem of equations present in the algorithm. This selection makes it possible\nto cache these conjugate directions, instead of computing them at each\niteration, resulting in faster computation of the solution of the linear system\nthus reducing the overall computation time. This reduction is demonstrated by a\nnumerical example.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work proposes a new method to select the augmentation parameters in the\noperator splitting quadratic program (OSQP) algorithm so as to reduce the\ncomputation time of overall algorithm. The selection is based upon the\ninformation of conjugate directions of the coefficient matrix of a linear\nsystem of equations present in the algorithm. This selection makes it possible\nto cache these conjugate directions, instead of computing them at each\niteration, resulting in faster computation of the solution of the linear system\nthus reducing the overall computation time. This reduction is demonstrated by a\nnumerical example."
                },
                "authors": [
                    {
                        "name": "Avinash Kumar"
                    }
                ],
                "author_detail": {
                    "name": "Avinash Kumar"
                },
                "author": "Avinash Kumar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05941v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05941v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18599v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18599v1",
                "updated": "2025-03-24T11:56:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    11,
                    56,
                    50,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T11:56:50Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    11,
                    56,
                    50,
                    0,
                    83,
                    0
                ],
                "title": "Oaken: Fast and Efficient LLM Serving with Online-Offline Hybrid KV\n  Cache Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Oaken: Fast and Efficient LLM Serving with Online-Offline Hybrid KV\n  Cache Quantization"
                },
                "summary": "Modern Large Language Model serving system batches multiple requests to\nachieve high throughput, while batching attention operations is challenging,\nrendering memory bandwidth a critical bottleneck. The community relies on\nhigh-end GPUs with multiple high-bandwidth memory channels. Unfortunately,\nHBM's high bandwidth often comes at the expense of limited memory capacity,\nwhich reduces core utilization and increases costs. Recent advancements\nenabling longer contexts for LLMs have substantially increased the key-value\ncache size, further intensifying the pressures on memory capacity. The\nliterature has explored KV cache quantization techniques, which commonly use\nlow bitwidth for most values, selectively using higher bitwidth for outlier\nvalues. While this approach helps achieve high accuracy and low bitwidth\nsimultaneously, it comes with the limitation that cost for online outlier\ndetection is excessively high, negating the advantages. We propose Oaken, an\nacceleration solution that achieves high accuracy and high performance\nsimultaneously through co-designing algorithm and hardware. To effectively find\na sweet spot in the accuracy-performance trade-off space of KV cache\nquantization, Oaken employs an online-offline hybrid approach, setting outlier\nthresholds offline, which are then used to determine the quantization scale\nonline. To translate the proposed algorithmic technique into tangible\nperformance gains, Oaken also comes with custom quantization engines and memory\nmanagement units that can be integrated with any LLM accelerators. We built an\nOaken accelerator on top of an LLM accelerator, LPU, and conducted a\ncomprehensive evaluation. Our experiments show that for a batch size of 256,\nOaken achieves up to 1.58x throughput improvement over NVIDIA A100 GPU,\nincurring a minimal accuracy loss of only 0.54\\% on average, compared to\nstate-of-the-art KV cache quantization techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern Large Language Model serving system batches multiple requests to\nachieve high throughput, while batching attention operations is challenging,\nrendering memory bandwidth a critical bottleneck. The community relies on\nhigh-end GPUs with multiple high-bandwidth memory channels. Unfortunately,\nHBM's high bandwidth often comes at the expense of limited memory capacity,\nwhich reduces core utilization and increases costs. Recent advancements\nenabling longer contexts for LLMs have substantially increased the key-value\ncache size, further intensifying the pressures on memory capacity. The\nliterature has explored KV cache quantization techniques, which commonly use\nlow bitwidth for most values, selectively using higher bitwidth for outlier\nvalues. While this approach helps achieve high accuracy and low bitwidth\nsimultaneously, it comes with the limitation that cost for online outlier\ndetection is excessively high, negating the advantages. We propose Oaken, an\nacceleration solution that achieves high accuracy and high performance\nsimultaneously through co-designing algorithm and hardware. To effectively find\na sweet spot in the accuracy-performance trade-off space of KV cache\nquantization, Oaken employs an online-offline hybrid approach, setting outlier\nthresholds offline, which are then used to determine the quantization scale\nonline. To translate the proposed algorithmic technique into tangible\nperformance gains, Oaken also comes with custom quantization engines and memory\nmanagement units that can be integrated with any LLM accelerators. We built an\nOaken accelerator on top of an LLM accelerator, LPU, and conducted a\ncomprehensive evaluation. Our experiments show that for a batch size of 256,\nOaken achieves up to 1.58x throughput improvement over NVIDIA A100 GPU,\nincurring a minimal accuracy loss of only 0.54\\% on average, compared to\nstate-of-the-art KV cache quantization techniques."
                },
                "authors": [
                    {
                        "name": "Minsu Kim"
                    },
                    {
                        "name": "Seongmin Hong"
                    },
                    {
                        "name": "RyeoWook Ko"
                    },
                    {
                        "name": "Soongyu Choi"
                    },
                    {
                        "name": "Hunjong Lee"
                    },
                    {
                        "name": "Junsoo Kim"
                    },
                    {
                        "name": "Joo-Young Kim"
                    },
                    {
                        "name": "Jongse Park"
                    }
                ],
                "author_detail": {
                    "name": "Jongse Park"
                },
                "author": "Jongse Park",
                "arxiv_comment": "15 pages, 14 figures, and 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18599v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18599v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17333v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17333v2",
                "updated": "2025-03-24T11:00:35Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    11,
                    0,
                    35,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-21T17:33:03Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    17,
                    33,
                    3,
                    4,
                    80,
                    0
                ],
                "title": "Register Dispersion: Reducing the Footprint of the Vector Register File\n  in Vector Engines of Low-Cost RISC-V CPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Register Dispersion: Reducing the Footprint of the Vector Register File\n  in Vector Engines of Low-Cost RISC-V CPUs"
                },
                "summary": "The deployment of Machine Learning (ML) applications at the edge on\nresource-constrained devices has accentuated the need for efficient ML\nprocessing on low-cost processors. While traditional CPUs provide programming\nflexibility, their general-purpose architecture often lacks the throughput\nrequired for complex ML models. The augmentation of a RISC-V processor with a\nvector unit can provide substantial data-level parallelism. However, increasing\nthe data-level parallelism supported by vector processing would make the Vector\nRegister File (VRF) a major area consumer in ultra low-cost processors, since\n32 vector registers are required for RISC-V Vector ISA compliance. This work\nleverages the insight that many ML vectorized kernels require a small number of\nactive vector registers, and proposes the use of a physically smaller VRF that\ndynamically caches only the vector registers currently accessed by the\napplication. This approach, called Register Dispersion, maps the architectural\nvector registers to a smaller set of physical registers. The proposed\nISA-compliant VRF is significantly smaller than a full-size VRF and operates\nlike a conventional cache, i.e., it only stores the most recently accessed\nvector registers. Essential registers remain readily accessible within the\ncompact VRF, while the others are offloaded to the cache/memory sub-system. The\ncompact VRF design is demonstrated to yield substantial area and power savings,\nas compared to using a full VRF, with no or minimal impact on performance. This\neffective trade-off renders the inclusion of vector units in low-cost\nprocessors feasible and practical.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of Machine Learning (ML) applications at the edge on\nresource-constrained devices has accentuated the need for efficient ML\nprocessing on low-cost processors. While traditional CPUs provide programming\nflexibility, their general-purpose architecture often lacks the throughput\nrequired for complex ML models. The augmentation of a RISC-V processor with a\nvector unit can provide substantial data-level parallelism. However, increasing\nthe data-level parallelism supported by vector processing would make the Vector\nRegister File (VRF) a major area consumer in ultra low-cost processors, since\n32 vector registers are required for RISC-V Vector ISA compliance. This work\nleverages the insight that many ML vectorized kernels require a small number of\nactive vector registers, and proposes the use of a physically smaller VRF that\ndynamically caches only the vector registers currently accessed by the\napplication. This approach, called Register Dispersion, maps the architectural\nvector registers to a smaller set of physical registers. The proposed\nISA-compliant VRF is significantly smaller than a full-size VRF and operates\nlike a conventional cache, i.e., it only stores the most recently accessed\nvector registers. Essential registers remain readily accessible within the\ncompact VRF, while the others are offloaded to the cache/memory sub-system. The\ncompact VRF design is demonstrated to yield substantial area and power savings,\nas compared to using a full VRF, with no or minimal impact on performance. This\neffective trade-off renders the inclusion of vector units in low-cost\nprocessors feasible and practical."
                },
                "authors": [
                    {
                        "name": "Vasileios Titopoulos"
                    },
                    {
                        "name": "George Alexakis"
                    },
                    {
                        "name": "Kosmas Alexandridis"
                    },
                    {
                        "name": "Chrysostomos Nicopoulos"
                    },
                    {
                        "name": "Giorgos Dimitrakopoulos"
                    }
                ],
                "author_detail": {
                    "name": "Giorgos Dimitrakopoulos"
                },
                "author": "Giorgos Dimitrakopoulos",
                "arxiv_comment": "22nd ACM International Conference on Computing Frontiers (CF' 25)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17333v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17333v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17038v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17038v2",
                "updated": "2025-03-24T07:29:28Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    7,
                    29,
                    28,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-21T10:48:35Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    10,
                    48,
                    35,
                    4,
                    80,
                    0
                ],
                "title": "Arm DynamIQ Shared Unit and Real-Time: An Empirical Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Arm DynamIQ Shared Unit and Real-Time: An Empirical Evaluation"
                },
                "summary": "The increasing complexity of embedded hardware platforms poses significant\nchallenges for real-time workloads. Architectural features such as Intel RDT,\nArm QoS, and Arm MPAM are either unavailable on commercial embedded platforms\nor designed primarily for server environments optimized for average-case\nperformance and might fail to deliver the expected real-time guarantees. Arm\nDynamIQ Shared Unit (DSU) includes isolation features-among others, hardware\nper-way cache partitioning-that can improve the real-time guarantees of complex\nembedded multicore systems and facilitate real-time analysis. However, the DSU\nalso targets average cases, and its real-time capabilities have not yet been\nevaluated. This paper presents the first comprehensive analysis of three\nreal-world deployments of the Arm DSU on Rockchip RK3568, Rockchip RK3588, and\nNVIDIA Orin platforms. We integrate support for the DSU at the operating system\nand hypervisor level and conduct a large-scale evaluation using both synthetic\nand real-world benchmarks with varying types and intensities of interference.\nOur results make extensive use of performance counters and indicate that,\nalthough effective, the quality of partitioning and isolation provided by the\nDSU depends on the type and the intensity of the interfering workloads. In\naddition, we uncover and analyze in detail the correlation between benchmarks\nand different types and intensities of interference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing complexity of embedded hardware platforms poses significant\nchallenges for real-time workloads. Architectural features such as Intel RDT,\nArm QoS, and Arm MPAM are either unavailable on commercial embedded platforms\nor designed primarily for server environments optimized for average-case\nperformance and might fail to deliver the expected real-time guarantees. Arm\nDynamIQ Shared Unit (DSU) includes isolation features-among others, hardware\nper-way cache partitioning-that can improve the real-time guarantees of complex\nembedded multicore systems and facilitate real-time analysis. However, the DSU\nalso targets average cases, and its real-time capabilities have not yet been\nevaluated. This paper presents the first comprehensive analysis of three\nreal-world deployments of the Arm DSU on Rockchip RK3568, Rockchip RK3588, and\nNVIDIA Orin platforms. We integrate support for the DSU at the operating system\nand hypervisor level and conduct a large-scale evaluation using both synthetic\nand real-world benchmarks with varying types and intensities of interference.\nOur results make extensive use of performance counters and indicate that,\nalthough effective, the quality of partitioning and isolation provided by the\nDSU depends on the type and the intensity of the interfering workloads. In\naddition, we uncover and analyze in detail the correlation between benchmarks\nand different types and intensities of interference."
                },
                "authors": [
                    {
                        "name": "Ashutosh Pradhan"
                    },
                    {
                        "name": "Daniele Ottaviano"
                    },
                    {
                        "name": "Yi Jiang"
                    },
                    {
                        "name": "Haozheng Huang"
                    },
                    {
                        "name": "Alexander Zuepke"
                    },
                    {
                        "name": "Andrea Bastoni"
                    },
                    {
                        "name": "Marco Caccamo"
                    }
                ],
                "author_detail": {
                    "name": "Marco Caccamo"
                },
                "author": "Marco Caccamo",
                "arxiv_comment": "Accepted for publication in the Proceedings of the 31st IEEE\n  Real-Time and Embedded Technology and Applications Symposium (RTAS 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17038v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17038v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68M20",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.3; C.4; D.4.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18334v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18334v1",
                "updated": "2025-03-24T04:32:35Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    4,
                    32,
                    35,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T04:32:35Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    4,
                    32,
                    35,
                    0,
                    83,
                    0
                ],
                "title": "Mitigating Cache Noise in Test-Time Adaptation for Large Vision-Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigating Cache Noise in Test-Time Adaptation for Large Vision-Language\n  Models"
                },
                "summary": "Test-time adaptation (TTA) of visual language models has recently attracted\nsignificant attention as a solution to the performance degradation caused by\ndistribution shifts in downstream tasks. However, existing cache-based TTA\nmethods have certain limitations. They mainly rely on the accuracy of cached\nfeature labels, and the presence of noisy pseudo-labels can cause these\nfeatures to deviate from their true distribution. This makes cache retrieval\nmethods based on similarity matching highly sensitive to outliers or extreme\nsamples. Moreover, current methods lack effective mechanisms to model class\ndistributions, which limits their ability to fully exploit the potential of\ncached information. To address these challenges, we introduce a comprehensive\nand reliable caching mechanism and propose a novel zero-shot TTA method called\n``Cache, Residual, Gaussian\" (CRG). This method not only employs learnable\nresidual parameters to better align positive and negative visual prototypes\nwith text prototypes, thereby optimizing the quality of cached features, but\nalso incorporates Gaussian Discriminant Analysis (GDA) to dynamically model\nintra-class feature distributions, further mitigating the impact of noisy\nfeatures. Experimental results on 13 benchmarks demonstrate that CRG\noutperforms state-of-the-art TTA methods, showcasing exceptional robustness and\nadaptability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time adaptation (TTA) of visual language models has recently attracted\nsignificant attention as a solution to the performance degradation caused by\ndistribution shifts in downstream tasks. However, existing cache-based TTA\nmethods have certain limitations. They mainly rely on the accuracy of cached\nfeature labels, and the presence of noisy pseudo-labels can cause these\nfeatures to deviate from their true distribution. This makes cache retrieval\nmethods based on similarity matching highly sensitive to outliers or extreme\nsamples. Moreover, current methods lack effective mechanisms to model class\ndistributions, which limits their ability to fully exploit the potential of\ncached information. To address these challenges, we introduce a comprehensive\nand reliable caching mechanism and propose a novel zero-shot TTA method called\n``Cache, Residual, Gaussian\" (CRG). This method not only employs learnable\nresidual parameters to better align positive and negative visual prototypes\nwith text prototypes, thereby optimizing the quality of cached features, but\nalso incorporates Gaussian Discriminant Analysis (GDA) to dynamically model\nintra-class feature distributions, further mitigating the impact of noisy\nfeatures. Experimental results on 13 benchmarks demonstrate that CRG\noutperforms state-of-the-art TTA methods, showcasing exceptional robustness and\nadaptability."
                },
                "authors": [
                    {
                        "name": "Haotian Zhai"
                    },
                    {
                        "name": "Xinyu Chen"
                    },
                    {
                        "name": "Can Zhang"
                    },
                    {
                        "name": "Tianming Sha"
                    },
                    {
                        "name": "Ruirui Li"
                    }
                ],
                "author_detail": {
                    "name": "Ruirui Li"
                },
                "author": "Ruirui Li",
                "arxiv_comment": "Accepted by ICME 2025 and ICLR 2025 Workshop on Foundation Models in\n  the Wild",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18334v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18334v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16653v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16653v2",
                "updated": "2025-03-24T03:18:49Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    3,
                    18,
                    49,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-20T19:10:37Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    19,
                    10,
                    37,
                    3,
                    79,
                    0
                ],
                "title": "iFlame: Interleaving Full and Linear Attention for Efficient Mesh\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "iFlame: Interleaving Full and Linear Attention for Efficient Mesh\n  Generation"
                },
                "summary": "This paper propose iFlame, a novel transformer-based network architecture for\nmesh generation. While attention-based models have demonstrated remarkable\nperformance in mesh generation, their quadratic computational complexity limits\nscalability, particularly for high-resolution 3D data. Conversely, linear\nattention mechanisms offer lower computational costs but often struggle to\ncapture long-range dependencies, resulting in suboptimal outcomes. To address\nthis trade-off, we propose an interleaving autoregressive mesh generation\nframework that combines the efficiency of linear attention with the expressive\npower of full attention mechanisms. To further enhance efficiency and leverage\nthe inherent structure of mesh representations, we integrate this interleaving\napproach into an hourglass architecture, which significantly boosts efficiency.\nOur approach reduces training time while achieving performance comparable to\npure attention-based models. To improve inference efficiency, we implemented a\ncaching algorithm that almost doubles the speed and reduces the KV cache size\nby seven-eighths compared to the original Transformer. We evaluate our\nframework on ShapeNet and Objaverse, demonstrating its ability to generate\nhigh-quality 3D meshes efficiently. Our results indicate that the proposed\ninterleaving framework effectively balances computational efficiency and\ngenerative performance, making it a practical solution for mesh generation. The\ntraining takes only 2 days with 4 GPUs on 39k data with a maximum of 4k faces\non Objaverse.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper propose iFlame, a novel transformer-based network architecture for\nmesh generation. While attention-based models have demonstrated remarkable\nperformance in mesh generation, their quadratic computational complexity limits\nscalability, particularly for high-resolution 3D data. Conversely, linear\nattention mechanisms offer lower computational costs but often struggle to\ncapture long-range dependencies, resulting in suboptimal outcomes. To address\nthis trade-off, we propose an interleaving autoregressive mesh generation\nframework that combines the efficiency of linear attention with the expressive\npower of full attention mechanisms. To further enhance efficiency and leverage\nthe inherent structure of mesh representations, we integrate this interleaving\napproach into an hourglass architecture, which significantly boosts efficiency.\nOur approach reduces training time while achieving performance comparable to\npure attention-based models. To improve inference efficiency, we implemented a\ncaching algorithm that almost doubles the speed and reduces the KV cache size\nby seven-eighths compared to the original Transformer. We evaluate our\nframework on ShapeNet and Objaverse, demonstrating its ability to generate\nhigh-quality 3D meshes efficiently. Our results indicate that the proposed\ninterleaving framework effectively balances computational efficiency and\ngenerative performance, making it a practical solution for mesh generation. The\ntraining takes only 2 days with 4 GPUs on 39k data with a maximum of 4k faces\non Objaverse."
                },
                "authors": [
                    {
                        "name": "Hanxiao Wang"
                    },
                    {
                        "name": "Biao Zhang"
                    },
                    {
                        "name": "Weize Quan"
                    },
                    {
                        "name": "Dong-Ming Yan"
                    },
                    {
                        "name": "Peter Wonka"
                    }
                ],
                "author_detail": {
                    "name": "Peter Wonka"
                },
                "author": "Peter Wonka",
                "arxiv_comment": "Project website: https://wanghanxiao123.github.io/iFa/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16653v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16653v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18292v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18292v1",
                "updated": "2025-03-24T02:28:04Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    2,
                    28,
                    4,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T02:28:04Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    2,
                    28,
                    4,
                    0,
                    83,
                    0
                ],
                "title": "Jenga: Effective Memory Management for Serving LLM with Heterogeneity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Jenga: Effective Memory Management for Serving LLM with Heterogeneity"
                },
                "summary": "Large language models (LLMs) are widely used but expensive to run, especially\nas inference workloads grow. To lower costs, maximizing the request batch size\nby managing GPU memory efficiently is crucial. While PagedAttention has\nrecently been proposed to improve the efficiency of memory management, we find\nthat the growing heterogeneity in the embeddings dimensions, attention, and\naccess patterns of modern LLM architectures introduces new challenges for\nmemory allocation.\n  In this paper, we present Jenga, a novel memory allocation framework for\nheterogeneous embeddings in LLMs. Jenga tackles two key challenges: (1)\nminimizing memory fragmentation when managing embeddings of different sizes,\nand (2) enabling flexible caching and eviction policies tailored to the\nspecific token-dependency patterns of various layers. Jenga employs a two-level\nmemory allocator, leveraging the least common multiple (LCM) of embedding sizes\nto optimize memory usage and providing APIs to express layer-specific caching\nlogic to enhance memory reuse.\n  We implemente Jenga on vLLM, a state-of-the-art LLM inference engine, and\nevaluate it with diverse LLMs, datasets, and GPU configurations. Evaluations\nshow that Jenga improves GPU memory utilization by up to 79.6%, and increases\nserving throughput by up to 4.92x (1.80x on average).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are widely used but expensive to run, especially\nas inference workloads grow. To lower costs, maximizing the request batch size\nby managing GPU memory efficiently is crucial. While PagedAttention has\nrecently been proposed to improve the efficiency of memory management, we find\nthat the growing heterogeneity in the embeddings dimensions, attention, and\naccess patterns of modern LLM architectures introduces new challenges for\nmemory allocation.\n  In this paper, we present Jenga, a novel memory allocation framework for\nheterogeneous embeddings in LLMs. Jenga tackles two key challenges: (1)\nminimizing memory fragmentation when managing embeddings of different sizes,\nand (2) enabling flexible caching and eviction policies tailored to the\nspecific token-dependency patterns of various layers. Jenga employs a two-level\nmemory allocator, leveraging the least common multiple (LCM) of embedding sizes\nto optimize memory usage and providing APIs to express layer-specific caching\nlogic to enhance memory reuse.\n  We implemente Jenga on vLLM, a state-of-the-art LLM inference engine, and\nevaluate it with diverse LLMs, datasets, and GPU configurations. Evaluations\nshow that Jenga improves GPU memory utilization by up to 79.6%, and increases\nserving throughput by up to 4.92x (1.80x on average)."
                },
                "authors": [
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Kuntai Du"
                    },
                    {
                        "name": "Shu Liu"
                    },
                    {
                        "name": "Woosuk Kwon"
                    },
                    {
                        "name": "Xiangxi Mo"
                    },
                    {
                        "name": "Yufeng Wang"
                    },
                    {
                        "name": "Xiaoxuan Liu"
                    },
                    {
                        "name": "Kaichao You"
                    },
                    {
                        "name": "Zhuohan Li"
                    },
                    {
                        "name": "Mingsheng Long"
                    },
                    {
                        "name": "Jidong Zhai"
                    },
                    {
                        "name": "Joseph Gonzalez"
                    },
                    {
                        "name": "Ion Stoica"
                    }
                ],
                "author_detail": {
                    "name": "Ion Stoica"
                },
                "author": "Ion Stoica",
                "arxiv_comment": "16 pages, 19 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18292v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18292v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20504v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20504v5",
                "updated": "2025-03-24T02:17:34Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    2,
                    17,
                    34,
                    0,
                    83,
                    0
                ],
                "published": "2024-12-29T15:42:24Z",
                "published_parsed": [
                    2024,
                    12,
                    29,
                    15,
                    42,
                    24,
                    6,
                    364,
                    0
                ],
                "title": "ReTaKe: Reducing Temporal and Knowledge Redundancy for Long Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReTaKe: Reducing Temporal and Knowledge Redundancy for Long Video\n  Understanding"
                },
                "summary": "Video Large Language Models (VideoLLMs) have made significant strides in\nvideo understanding but struggle with long videos due to the limitations of\ntheir backbone LLMs. Existing solutions rely on length extrapolation, which is\nmemory-constrained, or visual token compression, which primarily leverages\nlow-level temporal redundancy while overlooking the more effective high-level\nknowledge redundancy. To address this, we propose $\\textbf{ReTaKe}$, a\ntraining-free method with two novel modules DPSelect and PivotKV, to jointly\nreduce both temporal visual redundancy and knowledge redundancy for video\ncompression. To align with the way of human temporal perception, DPSelect\nidentifies keyframes based on inter-frame distance peaks. To leverage LLMs'\nlearned prior knowledge, PivotKV marks the keyframes as pivots and compress\nnon-pivot frames by pruning low-attention tokens in their KV cache. ReTaKe\nenables VideoLLMs to process 8 times longer frames (up to 2048), outperforming\nsimilar-sized models by 3-5% and even rivaling much larger ones on VideoMME,\nMLVU, LongVideoBench, and LVBench. Moreover, by overlapping compression\noperations with prefilling, ReTaKe introduces only ~10% prefilling latency\noverhead while reducing decoding latency by ~20%. Our code is available at\nhttps://github.com/SCZwangxiao/video-ReTaKe.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Large Language Models (VideoLLMs) have made significant strides in\nvideo understanding but struggle with long videos due to the limitations of\ntheir backbone LLMs. Existing solutions rely on length extrapolation, which is\nmemory-constrained, or visual token compression, which primarily leverages\nlow-level temporal redundancy while overlooking the more effective high-level\nknowledge redundancy. To address this, we propose $\\textbf{ReTaKe}$, a\ntraining-free method with two novel modules DPSelect and PivotKV, to jointly\nreduce both temporal visual redundancy and knowledge redundancy for video\ncompression. To align with the way of human temporal perception, DPSelect\nidentifies keyframes based on inter-frame distance peaks. To leverage LLMs'\nlearned prior knowledge, PivotKV marks the keyframes as pivots and compress\nnon-pivot frames by pruning low-attention tokens in their KV cache. ReTaKe\nenables VideoLLMs to process 8 times longer frames (up to 2048), outperforming\nsimilar-sized models by 3-5% and even rivaling much larger ones on VideoMME,\nMLVU, LongVideoBench, and LVBench. Moreover, by overlapping compression\noperations with prefilling, ReTaKe introduces only ~10% prefilling latency\noverhead while reducing decoding latency by ~20%. Our code is available at\nhttps://github.com/SCZwangxiao/video-ReTaKe."
                },
                "authors": [
                    {
                        "name": "Xiao Wang"
                    },
                    {
                        "name": "Qingyi Si"
                    },
                    {
                        "name": "Jianlong Wu"
                    },
                    {
                        "name": "Shiyu Zhu"
                    },
                    {
                        "name": "Li Cao"
                    },
                    {
                        "name": "Liqiang Nie"
                    }
                ],
                "author_detail": {
                    "name": "Liqiang Nie"
                },
                "author": "Liqiang Nie",
                "arxiv_comment": "Rewrite the methods section. Add more ablation studies and results in\n  LongVideoBench. Update metadata",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20504v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20504v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18278v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18278v1",
                "updated": "2025-03-24T01:47:26Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    1,
                    47,
                    26,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T01:47:26Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    1,
                    47,
                    26,
                    0,
                    83,
                    0
                ],
                "title": "TopV: Compatible Token Pruning with Inference Time Optimization for Fast\n  and Low-Memory Multimodal Vision Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TopV: Compatible Token Pruning with Inference Time Optimization for Fast\n  and Low-Memory Multimodal Vision Language Model"
                },
                "summary": "Vision-Language Models (VLMs) demand substantial computational resources\nduring inference, largely due to the extensive visual input tokens for\nrepresenting visual information. Previous studies have noted that visual tokens\ntend to receive less attention than text tokens, suggesting their lower\nimportance during inference and potential for pruning. However, their methods\nencounter several challenges: reliance on greedy heuristic criteria for token\nimportance and incompatibility with FlashAttention and KV cache. To address\nthese issues, we introduce \\textbf{TopV}, a compatible \\textbf{TO}ken\n\\textbf{P}runing with inference Time Optimization for fast and low-memory\n\\textbf{V}LM, achieving efficient pruning without additional training or\nfine-tuning. Instead of relying on attention scores, we formulate token pruning\nas an optimization problem, accurately identifying important visual tokens\nwhile remaining compatible with FlashAttention. Additionally, since we only\nperform this pruning once during the prefilling stage, it effectively reduces\nKV cache size. Our optimization framework incorporates a visual-aware cost\nfunction considering factors such as Feature Similarity, Relative Spatial\nDistance, and Absolute Central Distance, to measure the importance of each\nsource visual token, enabling effective pruning of low-importance tokens.\nExtensive experiments demonstrate that our method outperforms previous token\npruning methods, validating the effectiveness and efficiency of our approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language Models (VLMs) demand substantial computational resources\nduring inference, largely due to the extensive visual input tokens for\nrepresenting visual information. Previous studies have noted that visual tokens\ntend to receive less attention than text tokens, suggesting their lower\nimportance during inference and potential for pruning. However, their methods\nencounter several challenges: reliance on greedy heuristic criteria for token\nimportance and incompatibility with FlashAttention and KV cache. To address\nthese issues, we introduce \\textbf{TopV}, a compatible \\textbf{TO}ken\n\\textbf{P}runing with inference Time Optimization for fast and low-memory\n\\textbf{V}LM, achieving efficient pruning without additional training or\nfine-tuning. Instead of relying on attention scores, we formulate token pruning\nas an optimization problem, accurately identifying important visual tokens\nwhile remaining compatible with FlashAttention. Additionally, since we only\nperform this pruning once during the prefilling stage, it effectively reduces\nKV cache size. Our optimization framework incorporates a visual-aware cost\nfunction considering factors such as Feature Similarity, Relative Spatial\nDistance, and Absolute Central Distance, to measure the importance of each\nsource visual token, enabling effective pruning of low-importance tokens.\nExtensive experiments demonstrate that our method outperforms previous token\npruning methods, validating the effectiveness and efficiency of our approach."
                },
                "authors": [
                    {
                        "name": "Cheng Yang"
                    },
                    {
                        "name": "Yang Sui"
                    },
                    {
                        "name": "Jinqi Xiao"
                    },
                    {
                        "name": "Lingyi Huang"
                    },
                    {
                        "name": "Yu Gong"
                    },
                    {
                        "name": "Chendi Li"
                    },
                    {
                        "name": "Jinghua Yan"
                    },
                    {
                        "name": "Yu Bai"
                    },
                    {
                        "name": "Ponnuswamy Sadayappan"
                    },
                    {
                        "name": "Xia Hu"
                    },
                    {
                        "name": "Bo Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Bo Yuan"
                },
                "author": "Bo Yuan",
                "arxiv_comment": "Accepted by CVPR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18278v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18278v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18265v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18265v1",
                "updated": "2025-03-24T01:15:43Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    1,
                    15,
                    43,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T01:15:43Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    1,
                    15,
                    43,
                    0,
                    83,
                    0
                ],
                "title": "Risk Management for Distributed Arbitrage Systems: Integrating\n  Artificial Intelligence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Risk Management for Distributed Arbitrage Systems: Integrating\n  Artificial Intelligence"
                },
                "summary": "Effective risk management solutions become absolutely crucial when financial\nmarkets embrace distributed technology and decentralized financing (DeFi). This\nstudy offers a thorough survey and comparative analysis of the integration of\nartificial intelligence (AI) in risk management for distributed arbitrage\nsystems. We examine several modern caching techniques namely in memory caching,\ndistributed caching, and proxy caching and their functions in enhancing\nperformance in decentralized settings. Through literature review we examine the\nutilization of AI techniques for alleviating risks related to market\nvolatility, liquidity challenges, operational failures, regulatory compliance,\nand security threats. This comparison research evaluates various case studies\nfrom prominent DeFi technologies, emphasizing critical performance metrics like\nlatency reduction, load balancing, and system resilience. Additionally, we\nexamine the problems and trade offs associated with these technologies,\nemphasizing their effects on consistency, scalability, and fault tolerance. By\nmeticulously analyzing real world applications, specifically centering on the\nAave platform as our principal case study, we illustrate how the purposeful\namalgamation of AI with contemporary caching methodologies has revolutionized\nrisk management in distributed arbitrage systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective risk management solutions become absolutely crucial when financial\nmarkets embrace distributed technology and decentralized financing (DeFi). This\nstudy offers a thorough survey and comparative analysis of the integration of\nartificial intelligence (AI) in risk management for distributed arbitrage\nsystems. We examine several modern caching techniques namely in memory caching,\ndistributed caching, and proxy caching and their functions in enhancing\nperformance in decentralized settings. Through literature review we examine the\nutilization of AI techniques for alleviating risks related to market\nvolatility, liquidity challenges, operational failures, regulatory compliance,\nand security threats. This comparison research evaluates various case studies\nfrom prominent DeFi technologies, emphasizing critical performance metrics like\nlatency reduction, load balancing, and system resilience. Additionally, we\nexamine the problems and trade offs associated with these technologies,\nemphasizing their effects on consistency, scalability, and fault tolerance. By\nmeticulously analyzing real world applications, specifically centering on the\nAave platform as our principal case study, we illustrate how the purposeful\namalgamation of AI with contemporary caching methodologies has revolutionized\nrisk management in distributed arbitrage systems."
                },
                "authors": [
                    {
                        "name": "Akaash Vishal Hazarika"
                    },
                    {
                        "name": "Mahak Shah"
                    },
                    {
                        "name": "Swapnil Patil"
                    },
                    {
                        "name": "Pradyumna Shukla"
                    }
                ],
                "author_detail": {
                    "name": "Pradyumna Shukla"
                },
                "author": "Pradyumna Shukla",
                "arxiv_comment": "International Conference on AI and Financial Innovation AIFI-2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18265v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18265v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.11; G.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18191v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18191v1",
                "updated": "2025-03-23T20:18:16Z",
                "updated_parsed": [
                    2025,
                    3,
                    23,
                    20,
                    18,
                    16,
                    6,
                    82,
                    0
                ],
                "published": "2025-03-23T20:18:16Z",
                "published_parsed": [
                    2025,
                    3,
                    23,
                    20,
                    18,
                    16,
                    6,
                    82,
                    0
                ],
                "title": "Enabling the Write-Back Page Cache with Strong Consistency in\n  Distributed Userspace File Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling the Write-Back Page Cache with Strong Consistency in\n  Distributed Userspace File Systems"
                },
                "summary": "The large-scale, multi-tenant nature of cloud computing requires distributed\nfile systems that offer stability, adaptability, and compatibility. FUSE-based\ndistributed file systems have emerged as a popular solution for the cloud,\noffering fast deployment, fault isolation, and POSIX compliance. However,\nFUSE's performance limitations, particularly its inability to reconcile page\ncaching with strong consistency in distributed environments, remain a\npersistent problem. Existing approaches either sacrifice consistency for\nperformance or rely on inefficient caching, limiting their practicality.\n  To this end, we present DistFUSE, the first FUSE-based distributed file\nsystem that relies on a write-back kernel-based page cache for performance and\nprovides strong consistency. DistFUSE achieves this by offloading userspace\nlock management to the kernel driver, allowing coordinated access to the\nkernel's page cache across nodes. This design eliminates blind local cache\nupdates and ensures cluster-wide consistency without compromising performance.\nOur evaluation shows DistFUSE improves throughput by up to 75% compared to\nbaseline approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The large-scale, multi-tenant nature of cloud computing requires distributed\nfile systems that offer stability, adaptability, and compatibility. FUSE-based\ndistributed file systems have emerged as a popular solution for the cloud,\noffering fast deployment, fault isolation, and POSIX compliance. However,\nFUSE's performance limitations, particularly its inability to reconcile page\ncaching with strong consistency in distributed environments, remain a\npersistent problem. Existing approaches either sacrifice consistency for\nperformance or rely on inefficient caching, limiting their practicality.\n  To this end, we present DistFUSE, the first FUSE-based distributed file\nsystem that relies on a write-back kernel-based page cache for performance and\nprovides strong consistency. DistFUSE achieves this by offloading userspace\nlock management to the kernel driver, allowing coordinated access to the\nkernel's page cache across nodes. This design eliminates blind local cache\nupdates and ensures cluster-wide consistency without compromising performance.\nOur evaluation shows DistFUSE improves throughput by up to 75% compared to\nbaseline approaches."
                },
                "authors": [
                    {
                        "name": "Haoyu Li"
                    },
                    {
                        "name": "Jingkai Fu"
                    },
                    {
                        "name": "Qing Li"
                    },
                    {
                        "name": "Windsor Hsu"
                    },
                    {
                        "name": "Asaf Cidon"
                    }
                ],
                "author_detail": {
                    "name": "Asaf Cidon"
                },
                "author": "Asaf Cidon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18191v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18191v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18030v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18030v1",
                "updated": "2025-03-23T11:07:24Z",
                "updated_parsed": [
                    2025,
                    3,
                    23,
                    11,
                    7,
                    24,
                    6,
                    82,
                    0
                ],
                "published": "2025-03-23T11:07:24Z",
                "published_parsed": [
                    2025,
                    3,
                    23,
                    11,
                    7,
                    24,
                    6,
                    82,
                    0
                ],
                "title": "Formal Verification of Parameterized Systems based on Induction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Formal Verification of Parameterized Systems based on Induction"
                },
                "summary": "Parameterized systems play a crucial role in the computer field, and their\nsecurity is of great significance. Formal verification of parameterized\nprotocols is especially challenging due to its \"parameterized\" feature, which\nbrings complexity and undecidability. Existing automated parameterized\nverification methods have limitations, such as facing difficulties in\nautomatically deriving parameterized invariants constrained by mixed Forall and\nExists quantifiers, or having challenges in completing the parameterized\nverification of large and complex protocols. This paper proposes a formal\nverification framework for parameterized systems based on induction, named\nwiseParaverifier. It starts from small concretizations of protocols, analyzes\ninductive counterexamples, and constructs counterexample formulas to guide the\nentire process of parameterized verification. It also presents a heuristic\nGeneralize method to quickly find auxiliary invariants, a method for promoting\ncomplex mixed quantifiers and merging parameterized invariants, and uses\nsymmetric reduction ideas to accelerate the verification process. Experimental\nresults show that wiseParaverifier can successfully complete automatic\ninductive verification on 7 cache coherence protocols and 10 distributed\nprotocols. It has strong verification capabilities and migration capabilities,\nand can provide concise and readable verification results, which is helpful for\nlearners to understand protocol behaviors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parameterized systems play a crucial role in the computer field, and their\nsecurity is of great significance. Formal verification of parameterized\nprotocols is especially challenging due to its \"parameterized\" feature, which\nbrings complexity and undecidability. Existing automated parameterized\nverification methods have limitations, such as facing difficulties in\nautomatically deriving parameterized invariants constrained by mixed Forall and\nExists quantifiers, or having challenges in completing the parameterized\nverification of large and complex protocols. This paper proposes a formal\nverification framework for parameterized systems based on induction, named\nwiseParaverifier. It starts from small concretizations of protocols, analyzes\ninductive counterexamples, and constructs counterexample formulas to guide the\nentire process of parameterized verification. It also presents a heuristic\nGeneralize method to quickly find auxiliary invariants, a method for promoting\ncomplex mixed quantifiers and merging parameterized invariants, and uses\nsymmetric reduction ideas to accelerate the verification process. Experimental\nresults show that wiseParaverifier can successfully complete automatic\ninductive verification on 7 cache coherence protocols and 10 distributed\nprotocols. It has strong verification capabilities and migration capabilities,\nand can provide concise and readable verification results, which is helpful for\nlearners to understand protocol behaviors."
                },
                "authors": [
                    {
                        "name": "Jiaqi Xiu"
                    },
                    {
                        "name": "Yongjian Li"
                    }
                ],
                "author_detail": {
                    "name": "Yongjian Li"
                },
                "author": "Yongjian Li",
                "arxiv_comment": "9 pages,2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18030v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18030v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.10425v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.10425v2",
                "updated": "2025-03-23T06:14:35Z",
                "updated_parsed": [
                    2025,
                    3,
                    23,
                    6,
                    14,
                    35,
                    6,
                    82,
                    0
                ],
                "published": "2023-12-16T11:40:49Z",
                "published_parsed": [
                    2023,
                    12,
                    16,
                    11,
                    40,
                    49,
                    5,
                    350,
                    0
                ],
                "title": "Knowledge Rumination for Client Utility Evaluation in Heterogeneous\n  Federated Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Rumination for Client Utility Evaluation in Heterogeneous\n  Federated Learning"
                },
                "summary": "Federated Learning (FL) allows several clients to cooperatively train machine\nlearning models without disclosing the raw data. In practical applications,\nasynchronous FL (AFL) can address the straggler effect compared to synchronous\nFL. However, Non-IID data and stale models pose significant challenges to AFL,\nas they can diminish the practicality of the global model and even lead to\ntraining failures. In this work, we propose a novel AFL framework called\nFederated Historical Learning (FedHist), which effectively addresses the\nchallenges posed by both Non-IID data and gradient staleness based on the\nconcept of knowledge rumination. FedHist enhances the stability of local\ngradients by performing weighted fusion with historical global gradients cached\non the server. Relying on hindsight, it assigns aggregation weights to each\nparticipant in a multi-dimensional manner during each communication round. To\nfurther enhance the efficiency and stability of the training process, we\nintroduce an intelligent $\\ell_2$-norm amplification scheme, which dynamically\nregulates the learning progress based on the $\\ell_2$-norms of the submitted\ngradients. Extensive experiments indicate FedHist outperforms state-of-the-art\nmethods in terms of convergence performance and test accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) allows several clients to cooperatively train machine\nlearning models without disclosing the raw data. In practical applications,\nasynchronous FL (AFL) can address the straggler effect compared to synchronous\nFL. However, Non-IID data and stale models pose significant challenges to AFL,\nas they can diminish the practicality of the global model and even lead to\ntraining failures. In this work, we propose a novel AFL framework called\nFederated Historical Learning (FedHist), which effectively addresses the\nchallenges posed by both Non-IID data and gradient staleness based on the\nconcept of knowledge rumination. FedHist enhances the stability of local\ngradients by performing weighted fusion with historical global gradients cached\non the server. Relying on hindsight, it assigns aggregation weights to each\nparticipant in a multi-dimensional manner during each communication round. To\nfurther enhance the efficiency and stability of the training process, we\nintroduce an intelligent $\\ell_2$-norm amplification scheme, which dynamically\nregulates the learning progress based on the $\\ell_2$-norms of the submitted\ngradients. Extensive experiments indicate FedHist outperforms state-of-the-art\nmethods in terms of convergence performance and test accuracy."
                },
                "authors": [
                    {
                        "name": "Xiaorui Jiang"
                    },
                    {
                        "name": "Yu Gao"
                    },
                    {
                        "name": "Hengwei Xu"
                    },
                    {
                        "name": "Qi Zhang"
                    },
                    {
                        "name": "Yong Liao"
                    },
                    {
                        "name": "Pengyuan Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Pengyuan Zhou"
                },
                "author": "Pengyuan Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.10425v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.10425v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17922v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17922v1",
                "updated": "2025-03-23T03:36:52Z",
                "updated_parsed": [
                    2025,
                    3,
                    23,
                    3,
                    36,
                    52,
                    6,
                    82,
                    0
                ],
                "published": "2025-03-23T03:36:52Z",
                "published_parsed": [
                    2025,
                    3,
                    23,
                    3,
                    36,
                    52,
                    6,
                    82,
                    0
                ],
                "title": "WindowKV: Task-Adaptive Group-Wise KV Cache Window Selection for\n  Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WindowKV: Task-Adaptive Group-Wise KV Cache Window Selection for\n  Efficient LLM Inference"
                },
                "summary": "With the advancements in long-context inference capabilities of large\nlanguage models (LLMs), the KV cache has become one of the foundational\ncomponents. However, its substantial GPU memory consumption makes KV cache\ncompression a key technique for enabling efficient LLM inference in industrial\nscenarios. While recent studies have focused on optimizing the memory occupied\nby the KV cache, they overlook two critical factors: preserving semantic\ncoherence and considering task-specific characteristic during compression. To\naddress these limitations, we propose a novel task-adaptive KV cache window\nselection method, WindowKV. WindowKV dynamically selects local semantic windows\nconsisting of consecutive tokens, according to task-specific characteristics,\nensuring the retained KV cache captures continuous, essential context.\nAdditionally, we introduce an intra-group layer KV cache indices sharing\nstrategy to reduce computational overhead, achieving a balance between\nperformance and efficiency. We rigorously evaluate WindowKV on the LongBench\nbenchmark, and the results demonstrate that it maintains a performance\ncomparable to full KV cache retention while using only 12% of the original KV\ncache, significantly reducing memory requirements. Furthermore, our method also\nachieves state-of-the-art results in the Needle-in-a-Haystack evaluation,\nhighlighting its effectiveness and robustness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the advancements in long-context inference capabilities of large\nlanguage models (LLMs), the KV cache has become one of the foundational\ncomponents. However, its substantial GPU memory consumption makes KV cache\ncompression a key technique for enabling efficient LLM inference in industrial\nscenarios. While recent studies have focused on optimizing the memory occupied\nby the KV cache, they overlook two critical factors: preserving semantic\ncoherence and considering task-specific characteristic during compression. To\naddress these limitations, we propose a novel task-adaptive KV cache window\nselection method, WindowKV. WindowKV dynamically selects local semantic windows\nconsisting of consecutive tokens, according to task-specific characteristics,\nensuring the retained KV cache captures continuous, essential context.\nAdditionally, we introduce an intra-group layer KV cache indices sharing\nstrategy to reduce computational overhead, achieving a balance between\nperformance and efficiency. We rigorously evaluate WindowKV on the LongBench\nbenchmark, and the results demonstrate that it maintains a performance\ncomparable to full KV cache retention while using only 12% of the original KV\ncache, significantly reducing memory requirements. Furthermore, our method also\nachieves state-of-the-art results in the Needle-in-a-Haystack evaluation,\nhighlighting its effectiveness and robustness."
                },
                "authors": [
                    {
                        "name": "Youhui Zuo"
                    },
                    {
                        "name": "Sibo Wei"
                    },
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Zhuorui Liu"
                    },
                    {
                        "name": "Wenpeng Lu"
                    },
                    {
                        "name": "Dawei Song"
                    }
                ],
                "author_detail": {
                    "name": "Dawei Song"
                },
                "author": "Dawei Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17922v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17922v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17913v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17913v1",
                "updated": "2025-03-23T03:20:25Z",
                "updated_parsed": [
                    2025,
                    3,
                    23,
                    3,
                    20,
                    25,
                    6,
                    82,
                    0
                ],
                "published": "2025-03-23T03:20:25Z",
                "published_parsed": [
                    2025,
                    3,
                    23,
                    3,
                    20,
                    25,
                    6,
                    82,
                    0
                ],
                "title": "Cache-Aware Cooperative Multicast Beamforming in Dynamic\n  Satellite-Terrestrial Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-Aware Cooperative Multicast Beamforming in Dynamic\n  Satellite-Terrestrial Networks"
                },
                "summary": "With the burgeoning demand for data-intensive services, satellite-terrestrial\nnetworks (STNs) face increasing backhaul link congestion, deteriorating user\nquality of service (QoS), and escalating power consumption. Cache-aided STNs\nare acknowledged as a promising paradigm for accelerating content delivery to\nusers and alleviating the load of backhaul links. However, the dynamic nature\nof low earth orbit (LEO) satellites and the complex interference among\nsatellite beams and terrestrial base stations pose challenges in effectively\nmanaging limited edge resources. To address these issues, this paper proposes a\nmethod for dynamically scheduling caching and communication resources, aiming\nto reduce network costs in terms of transmission power consumption and backhaul\ntraffic, while meeting user QoS demands and resource constraints. We formulate\na mixed timescale problem to jointly optimize cache placement, LEO satellite\nbeam direction, and cooperative multicast beamforming among satellite beams and\nbase stations. To tackle this intricate problem, we propose a two-stage\nsolution framework, where the primary problem is decoupled into a short-term\ncontent delivery subproblem and a long-term cache placement subproblem. The\nformer subproblem is solved by designing an alternating optimization approach\nwith whale optimization and successive convex approximation methods according\nto the cache placement state, while cache content in STNs is updated using an\niterative algorithm that utilizes historical information. Simulation results\ndemonstrate the effectiveness of our proposed algorithms, showcasing their\nconvergence and significantly reducing transmission power consumption and\nbackhaul traffic by up to 52%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the burgeoning demand for data-intensive services, satellite-terrestrial\nnetworks (STNs) face increasing backhaul link congestion, deteriorating user\nquality of service (QoS), and escalating power consumption. Cache-aided STNs\nare acknowledged as a promising paradigm for accelerating content delivery to\nusers and alleviating the load of backhaul links. However, the dynamic nature\nof low earth orbit (LEO) satellites and the complex interference among\nsatellite beams and terrestrial base stations pose challenges in effectively\nmanaging limited edge resources. To address these issues, this paper proposes a\nmethod for dynamically scheduling caching and communication resources, aiming\nto reduce network costs in terms of transmission power consumption and backhaul\ntraffic, while meeting user QoS demands and resource constraints. We formulate\na mixed timescale problem to jointly optimize cache placement, LEO satellite\nbeam direction, and cooperative multicast beamforming among satellite beams and\nbase stations. To tackle this intricate problem, we propose a two-stage\nsolution framework, where the primary problem is decoupled into a short-term\ncontent delivery subproblem and a long-term cache placement subproblem. The\nformer subproblem is solved by designing an alternating optimization approach\nwith whale optimization and successive convex approximation methods according\nto the cache placement state, while cache content in STNs is updated using an\niterative algorithm that utilizes historical information. Simulation results\ndemonstrate the effectiveness of our proposed algorithms, showcasing their\nconvergence and significantly reducing transmission power consumption and\nbackhaul traffic by up to 52%."
                },
                "authors": [
                    {
                        "name": "Shuo Yuan"
                    },
                    {
                        "name": "Yaohua Sun"
                    },
                    {
                        "name": "Mugen Peng"
                    }
                ],
                "author_detail": {
                    "name": "Mugen Peng"
                },
                "author": "Mugen Peng",
                "arxiv_doi": "10.1109/TVT.2024.3463548",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TVT.2024.3463548",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.17913v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17913v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by IEEE Transactions on Vehicular Technology",
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17911v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17911v1",
                "updated": "2025-03-23T03:16:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    23,
                    3,
                    16,
                    50,
                    6,
                    82,
                    0
                ],
                "published": "2025-03-23T03:16:50Z",
                "published_parsed": [
                    2025,
                    3,
                    23,
                    3,
                    16,
                    50,
                    6,
                    82,
                    0
                ],
                "title": "VSAG: An Optimized Search Framework for Graph-based Approximate Nearest\n  Neighbor Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VSAG: An Optimized Search Framework for Graph-based Approximate Nearest\n  Neighbor Search"
                },
                "summary": "Approximate nearest neighbor search (ANNS) is a fundamental problem in vector\ndatabases and AI infrastructures. Recent graph-based ANNS algorithms have\nachieved high search accuracy with practical efficiency. Despite the\nadvancements, these algorithms still face performance bottlenecks in\nproduction, due to the random memory access patterns of graph-based search and\nthe high computational overheads of vector distance. In addition, the\nperformance of a graph-based ANNS algorithm is highly sensitive to parameters,\nwhile selecting the optimal parameters is cost-prohibitive, e.g., manual tuning\nrequires repeatedly re-building the index.\n  This paper introduces VSAG, an open-source framework that aims to enhance the\nin production performance of graph-based ANNS algorithms. VSAG has been\ndeployed at scale in the services of Ant Group, and it incorporates three key\noptimizations: (i) efficient memory access: it reduces L3 cache misses with\npre-fetching and cache-friendly vector organization; (ii) automated parameter\ntuning: it automatically selects performance-optimal parameters without\nrequiring index rebuilding; (iii) efficient distance computation: it leverages\nmodern hardware, scalar quantization, and smartly switches to low-precision\nrepresentation to dramatically reduce the distance computation costs. We\nevaluate VSAG on real-world datasets. The experimental results show that VSAG\nachieves the state-of-the-art performance and provides up to 4x speedup over\nHNSWlib (an industry-standard library) while ensuring the same accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Approximate nearest neighbor search (ANNS) is a fundamental problem in vector\ndatabases and AI infrastructures. Recent graph-based ANNS algorithms have\nachieved high search accuracy with practical efficiency. Despite the\nadvancements, these algorithms still face performance bottlenecks in\nproduction, due to the random memory access patterns of graph-based search and\nthe high computational overheads of vector distance. In addition, the\nperformance of a graph-based ANNS algorithm is highly sensitive to parameters,\nwhile selecting the optimal parameters is cost-prohibitive, e.g., manual tuning\nrequires repeatedly re-building the index.\n  This paper introduces VSAG, an open-source framework that aims to enhance the\nin production performance of graph-based ANNS algorithms. VSAG has been\ndeployed at scale in the services of Ant Group, and it incorporates three key\noptimizations: (i) efficient memory access: it reduces L3 cache misses with\npre-fetching and cache-friendly vector organization; (ii) automated parameter\ntuning: it automatically selects performance-optimal parameters without\nrequiring index rebuilding; (iii) efficient distance computation: it leverages\nmodern hardware, scalar quantization, and smartly switches to low-precision\nrepresentation to dramatically reduce the distance computation costs. We\nevaluate VSAG on real-world datasets. The experimental results show that VSAG\nachieves the state-of-the-art performance and provides up to 4x speedup over\nHNSWlib (an industry-standard library) while ensuring the same accuracy."
                },
                "authors": [
                    {
                        "name": "Xiaoyao Zhong"
                    },
                    {
                        "name": "Haotian Li"
                    },
                    {
                        "name": "Jiabao Jin"
                    },
                    {
                        "name": "Mingyu Yang"
                    },
                    {
                        "name": "Deming Chu"
                    },
                    {
                        "name": "Xiangyu Wang"
                    },
                    {
                        "name": "Zhitao Shen"
                    },
                    {
                        "name": "Wei Jia"
                    },
                    {
                        "name": "George Gu"
                    },
                    {
                        "name": "Yi Xie"
                    },
                    {
                        "name": "Xuemin Lin"
                    },
                    {
                        "name": "Heng Tao Shen"
                    },
                    {
                        "name": "Jingkuan Song"
                    },
                    {
                        "name": "Peng Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Peng Cheng"
                },
                "author": "Peng Cheng",
                "arxiv_comment": "16 pages, the report of open-source library VSAG\n  (https://github.com/antgroup/vsag)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17911v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17911v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17895v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17895v1",
                "updated": "2025-03-23T01:17:08Z",
                "updated_parsed": [
                    2025,
                    3,
                    23,
                    1,
                    17,
                    8,
                    6,
                    82,
                    0
                ],
                "published": "2025-03-23T01:17:08Z",
                "published_parsed": [
                    2025,
                    3,
                    23,
                    1,
                    17,
                    8,
                    6,
                    82,
                    0
                ],
                "title": "Orientation-Dependent \\b{eta}-Ga2O3 Heterojunction Diode with Atomic\n  Layer Deposition (ALD) Grown NiO",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Orientation-Dependent \\b{eta}-Ga2O3 Heterojunction Diode with Atomic\n  Layer Deposition (ALD) Grown NiO"
                },
                "summary": "This work reports the demonstration of ALD-deposited NiO/\\b{eta}-Ga2O3\nheterojunction diodes (HJDs) on low doped drift layer and highly doped (001) &\n(100) n+ substrates with experimental observation of a parallel-plane junction\nelectric field as high as 7.5 MV/cm, revealing a crystal orientation dependence\nin \\b{eta}-Ga2O3. We use a novel metalorganic precursor\nbis(1,4-di-tert-butyl-1,3-diazadienyl) (nickel Ni(tBu2DAD)2) with ozone (O3) to\ndeposit NiO. The NiO/\\b{eta}-Ga2O3 HJD on 7.7 {\\mu}m-thick HVPE-grown drift\nregion exhibited an on-state current density of ~20 A/cm2 at 5 V, ~10-8 A/cm2\nreverse leakage at low reverse bias(-5 V), and a rectifying ratio(Jon/Joff) of\n~109. The HJD broke down at ~2.2 kV reverse bias, corresponding to a ~3.4 MV/cm\nparallel-plane junction electric field, with a noise floor reverse leakage\n(10-8~10-6 A/cm2, nA) at 80% of the device catastrophic breakdown voltage. The\nNiO/\\b{eta}-Ga2O3 HJDs on n+ (001) & (100) highly-doped substrates exhibited\nbreakdown voltages at 12.5-16.0 V and 28.5-70.5 V, respectively, with extracted\ncritical electric field (EC) at 2.30-2.76 MV/cm, and 4.33-7.50 MV/cm, revealing\na substrate crystal orientation dependence on breakdown electric field for\n\\b{eta}-Ga2O3. The 7.5 MV/cm EC reported here is one of the highest\nparallel-plane junction electric fields reported in literature.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work reports the demonstration of ALD-deposited NiO/\\b{eta}-Ga2O3\nheterojunction diodes (HJDs) on low doped drift layer and highly doped (001) &\n(100) n+ substrates with experimental observation of a parallel-plane junction\nelectric field as high as 7.5 MV/cm, revealing a crystal orientation dependence\nin \\b{eta}-Ga2O3. We use a novel metalorganic precursor\nbis(1,4-di-tert-butyl-1,3-diazadienyl) (nickel Ni(tBu2DAD)2) with ozone (O3) to\ndeposit NiO. The NiO/\\b{eta}-Ga2O3 HJD on 7.7 {\\mu}m-thick HVPE-grown drift\nregion exhibited an on-state current density of ~20 A/cm2 at 5 V, ~10-8 A/cm2\nreverse leakage at low reverse bias(-5 V), and a rectifying ratio(Jon/Joff) of\n~109. The HJD broke down at ~2.2 kV reverse bias, corresponding to a ~3.4 MV/cm\nparallel-plane junction electric field, with a noise floor reverse leakage\n(10-8~10-6 A/cm2, nA) at 80% of the device catastrophic breakdown voltage. The\nNiO/\\b{eta}-Ga2O3 HJDs on n+ (001) & (100) highly-doped substrates exhibited\nbreakdown voltages at 12.5-16.0 V and 28.5-70.5 V, respectively, with extracted\ncritical electric field (EC) at 2.30-2.76 MV/cm, and 4.33-7.50 MV/cm, revealing\na substrate crystal orientation dependence on breakdown electric field for\n\\b{eta}-Ga2O3. The 7.5 MV/cm EC reported here is one of the highest\nparallel-plane junction electric fields reported in literature."
                },
                "authors": [
                    {
                        "name": "Yizheng Liu"
                    },
                    {
                        "name": "Shane M. W. Witsell"
                    },
                    {
                        "name": "John F. Conley"
                    },
                    {
                        "name": "Sriram Krishnamoorthy"
                    }
                ],
                "author_detail": {
                    "name": "Sriram Krishnamoorthy"
                },
                "author": "Sriram Krishnamoorthy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17895v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17895v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17603v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17603v1",
                "updated": "2025-03-22T01:17:56Z",
                "updated_parsed": [
                    2025,
                    3,
                    22,
                    1,
                    17,
                    56,
                    5,
                    81,
                    0
                ],
                "published": "2025-03-22T01:17:56Z",
                "published_parsed": [
                    2025,
                    3,
                    22,
                    1,
                    17,
                    56,
                    5,
                    81,
                    0
                ],
                "title": "A Generative Caching System for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Generative Caching System for Large Language Models"
                },
                "summary": "Caching has the potential to be of significant benefit for accessing large\nlanguage models (LLMs) due to their high latencies which typically range from a\nsmall number of seconds to well over a minute. Furthermore, many LLMs charge\nmoney for queries; caching thus has a clear monetary benefit. This paper\npresents a new caching system for improving user experiences with LLMs. In\naddition to reducing both latencies and monetary costs for accessing LLMs, our\nsystem also provides important features that go beyond the performance benefits\ntypically associated with caches. A key feature we provide is generative\ncaching, wherein multiple cached responses can be synthesized to provide\nanswers to queries which have never been seen before. Our generative caches\nfunction as repositories of valuable information which can be mined and\nanalyzed. We also improve upon past semantic caching techniques by tailoring\nthe caching algorithms to optimally balance cost and latency reduction with the\nquality of responses provided. Performance tests indicate that our caches are\nconsiderably faster than GPTcache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caching has the potential to be of significant benefit for accessing large\nlanguage models (LLMs) due to their high latencies which typically range from a\nsmall number of seconds to well over a minute. Furthermore, many LLMs charge\nmoney for queries; caching thus has a clear monetary benefit. This paper\npresents a new caching system for improving user experiences with LLMs. In\naddition to reducing both latencies and monetary costs for accessing LLMs, our\nsystem also provides important features that go beyond the performance benefits\ntypically associated with caches. A key feature we provide is generative\ncaching, wherein multiple cached responses can be synthesized to provide\nanswers to queries which have never been seen before. Our generative caches\nfunction as repositories of valuable information which can be mined and\nanalyzed. We also improve upon past semantic caching techniques by tailoring\nthe caching algorithms to optimally balance cost and latency reduction with the\nquality of responses provided. Performance tests indicate that our caches are\nconsiderably faster than GPTcache."
                },
                "authors": [
                    {
                        "name": "Arun Iyengar"
                    },
                    {
                        "name": "Ashish Kundu"
                    },
                    {
                        "name": "Ramana Kompella"
                    },
                    {
                        "name": "Sai Nandan Mamidi"
                    }
                ],
                "author_detail": {
                    "name": "Sai Nandan Mamidi"
                },
                "author": "Sai Nandan Mamidi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17603v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17603v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17602v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17602v1",
                "updated": "2025-03-22T01:16:24Z",
                "updated_parsed": [
                    2025,
                    3,
                    22,
                    1,
                    16,
                    24,
                    5,
                    81,
                    0
                ],
                "published": "2025-03-22T01:16:24Z",
                "published_parsed": [
                    2025,
                    3,
                    22,
                    1,
                    16,
                    24,
                    5,
                    81,
                    0
                ],
                "title": "Multiport Support for Vortex OpenGPU Memory Hierarchy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multiport Support for Vortex OpenGPU Memory Hierarchy"
                },
                "summary": "Modern day applications have grown in size and require more computational\npower. The rise of machine learning and AI increased the need for parallel\ncomputation, which has increased the need for GPGPUs. With the increasing\ndemand for computational power, GPGPUs' SIMT architecture has solved this with\nan increase in the number of threads and the number of cores in a GPU,\nincreasing the throughput of these processors to match the demand of the\napplications. However, this created a larger demand for the memory, making the\nmemory bandwidth a bottleneck. The introduction of High-Bandwidth Memory (HBM)\nwith its increased number of memory ports offers a potential solution for the\nGPU to exploit its memory parallelism to increase the memory bandwidth.\nHowever, effectively leveraging HBM's memory parallelism to maximize bandwidth\npresents a unique and complex challenge for GPU architectures on how to\ndistribute those ports among the streaming multiprocessors in the GPGPU. In\nthis work, we extend the Vortex OpenGPU microarchitecture to incorporate a\nmultiport memory hierarchy, spanning from the L1 cache to the last-level cache\n(LLC). In addition, we propose various arbitration strategies to optimize\nmemory transfers across the cache hierarchy. The results have shown that an\nincrease in memory ports increases IPC, achieving an average speedup of 2.34x\nwith 8 memory ports in the tested configuration while showing relatively small\narea overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern day applications have grown in size and require more computational\npower. The rise of machine learning and AI increased the need for parallel\ncomputation, which has increased the need for GPGPUs. With the increasing\ndemand for computational power, GPGPUs' SIMT architecture has solved this with\nan increase in the number of threads and the number of cores in a GPU,\nincreasing the throughput of these processors to match the demand of the\napplications. However, this created a larger demand for the memory, making the\nmemory bandwidth a bottleneck. The introduction of High-Bandwidth Memory (HBM)\nwith its increased number of memory ports offers a potential solution for the\nGPU to exploit its memory parallelism to increase the memory bandwidth.\nHowever, effectively leveraging HBM's memory parallelism to maximize bandwidth\npresents a unique and complex challenge for GPU architectures on how to\ndistribute those ports among the streaming multiprocessors in the GPGPU. In\nthis work, we extend the Vortex OpenGPU microarchitecture to incorporate a\nmultiport memory hierarchy, spanning from the L1 cache to the last-level cache\n(LLC). In addition, we propose various arbitration strategies to optimize\nmemory transfers across the cache hierarchy. The results have shown that an\nincrease in memory ports increases IPC, achieving an average speedup of 2.34x\nwith 8 memory ports in the tested configuration while showing relatively small\narea overhead."
                },
                "authors": [
                    {
                        "name": "Injae Shin"
                    },
                    {
                        "name": "Blaise Tine"
                    }
                ],
                "author_detail": {
                    "name": "Blaise Tine"
                },
                "author": "Blaise Tine",
                "arxiv_comment": "OSSMPIC2025, 1st workshop on Open Source Solutions for Massively\n  Parallel Integrated Circuits",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17602v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17602v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07578v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07578v2",
                "updated": "2025-03-21T21:10:02Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    21,
                    10,
                    2,
                    4,
                    80,
                    0
                ],
                "published": "2025-02-11T14:25:20Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    14,
                    25,
                    20,
                    1,
                    42,
                    0
                ],
                "title": "PIM Is All You Need: A CXL-Enabled GPU-Free System for Large Language\n  Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PIM Is All You Need: A CXL-Enabled GPU-Free System for Large Language\n  Model Inference"
                },
                "summary": "Large Language Model (LLM) inference uses an autoregressive manner to\ngenerate one token at a time, which exhibits notably lower operational\nintensity compared to earlier Machine Learning (ML) models such as encoder-only\ntransformers and Convolutional Neural Networks. At the same time, LLMs possess\nlarge parameter sizes and use key-value caches to store context information.\nModern LLMs support context windows with up to 1 million tokens to generate\nversatile text, audio, and video content. A large key-value cache unique to\neach prompt requires a large memory capacity, limiting the inference batch\nsize. Both low operational intensity and limited batch size necessitate a high\nmemory bandwidth. However, contemporary hardware systems for ML model\ndeployment, such as GPUs and TPUs, are primarily optimized for compute\nthroughput. This mismatch challenges the efficient deployment of advanced LLMs\nand makes users pay for expensive compute resources that are poorly utilized\nfor the memory-bound LLM inference tasks.\n  We propose CENT, a CXL-ENabled GPU-Free sysTem for LLM inference, which\nharnesses CXL memory expansion capabilities to accommodate substantial LLM\nsizes, and utilizes near-bank processing units to deliver high memory\nbandwidth, eliminating the need for expensive GPUs. CENT exploits a scalable\nCXL network to support peer-to-peer and collective communication primitives\nacross CXL devices. We implement various parallelism strategies to distribute\nLLMs across these devices. Compared to GPU baselines with maximum supported\nbatch sizes and similar average power, CENT achieves 2.3$\\times$ higher\nthroughput and consumes 2.3$\\times$ less energy. CENT enhances the Total Cost\nof Ownership (TCO), generating 5.2$\\times$ more tokens per dollar than GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) inference uses an autoregressive manner to\ngenerate one token at a time, which exhibits notably lower operational\nintensity compared to earlier Machine Learning (ML) models such as encoder-only\ntransformers and Convolutional Neural Networks. At the same time, LLMs possess\nlarge parameter sizes and use key-value caches to store context information.\nModern LLMs support context windows with up to 1 million tokens to generate\nversatile text, audio, and video content. A large key-value cache unique to\neach prompt requires a large memory capacity, limiting the inference batch\nsize. Both low operational intensity and limited batch size necessitate a high\nmemory bandwidth. However, contemporary hardware systems for ML model\ndeployment, such as GPUs and TPUs, are primarily optimized for compute\nthroughput. This mismatch challenges the efficient deployment of advanced LLMs\nand makes users pay for expensive compute resources that are poorly utilized\nfor the memory-bound LLM inference tasks.\n  We propose CENT, a CXL-ENabled GPU-Free sysTem for LLM inference, which\nharnesses CXL memory expansion capabilities to accommodate substantial LLM\nsizes, and utilizes near-bank processing units to deliver high memory\nbandwidth, eliminating the need for expensive GPUs. CENT exploits a scalable\nCXL network to support peer-to-peer and collective communication primitives\nacross CXL devices. We implement various parallelism strategies to distribute\nLLMs across these devices. Compared to GPU baselines with maximum supported\nbatch sizes and similar average power, CENT achieves 2.3$\\times$ higher\nthroughput and consumes 2.3$\\times$ less energy. CENT enhances the Total Cost\nof Ownership (TCO), generating 5.2$\\times$ more tokens per dollar than GPUs."
                },
                "authors": [
                    {
                        "name": "Yufeng Gu"
                    },
                    {
                        "name": "Alireza Khadem"
                    },
                    {
                        "name": "Sumanth Umesh"
                    },
                    {
                        "name": "Ning Liang"
                    },
                    {
                        "name": "Xavier Servot"
                    },
                    {
                        "name": "Onur Mutlu"
                    },
                    {
                        "name": "Ravi Iyer"
                    },
                    {
                        "name": "Reetuparna Das"
                    }
                ],
                "author_detail": {
                    "name": "Reetuparna Das"
                },
                "author": "Reetuparna Das",
                "arxiv_doi": "10.1145/3676641.3716267",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3676641.3716267",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.07578v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07578v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "In Proceedings of the 30th ACM International Conference on\n  Architectural Support for Programming Languages and Operating Systems, Volume\n  2 (ASPLOS'25)",
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09003v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09003v2",
                "updated": "2025-03-21T19:26:12Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    19,
                    26,
                    12,
                    4,
                    80,
                    0
                ],
                "published": "2025-02-13T06:44:33Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    6,
                    44,
                    33,
                    3,
                    44,
                    0
                ],
                "title": "RoSTE: An Efficient Quantization-Aware Supervised Fine-Tuning Approach\n  for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RoSTE: An Efficient Quantization-Aware Supervised Fine-Tuning Approach\n  for Large Language Models"
                },
                "summary": "Supervised fine-tuning is a standard method for adapting pre-trained large\nlanguage models (LLMs) to downstream tasks. Quantization has been recently\nstudied as a post-training technique for efficient LLM deployment. To obtain\nquantized fine-tuned LLMs, conventional pipelines would first fine-tune the\npre-trained models, followed by post-training quantization. This often yields\nsuboptimal performance as it fails to leverage the synergy between fine-tuning\nand quantization. To effectively realize low-bit quantization of weights,\nactivations, and KV caches in LLMs, we propose an algorithm named Rotated\nStraight-Through-Estimator (RoSTE), which combines quantization-aware\nsupervised fine-tuning (QA-SFT) with an adaptive rotation strategy that\nidentifies an effective rotation configuration to reduce activation outliers.\nWe provide theoretical insights on RoSTE by analyzing its prediction error when\napplied to an overparameterized least square quantized training problem. Our\nfindings reveal that the prediction error is directly proportional to the\nquantization error of the converged weights, which can be effectively managed\nthrough an optimized rotation configuration. Experiments on Pythia, Qwen and\nLlama models of different sizes demonstrate the effectiveness of RoSTE.\nCompared to existing post-SFT quantization baselines, our method consistently\nachieves superior performances across various tasks and different LLM\narchitectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Supervised fine-tuning is a standard method for adapting pre-trained large\nlanguage models (LLMs) to downstream tasks. Quantization has been recently\nstudied as a post-training technique for efficient LLM deployment. To obtain\nquantized fine-tuned LLMs, conventional pipelines would first fine-tune the\npre-trained models, followed by post-training quantization. This often yields\nsuboptimal performance as it fails to leverage the synergy between fine-tuning\nand quantization. To effectively realize low-bit quantization of weights,\nactivations, and KV caches in LLMs, we propose an algorithm named Rotated\nStraight-Through-Estimator (RoSTE), which combines quantization-aware\nsupervised fine-tuning (QA-SFT) with an adaptive rotation strategy that\nidentifies an effective rotation configuration to reduce activation outliers.\nWe provide theoretical insights on RoSTE by analyzing its prediction error when\napplied to an overparameterized least square quantized training problem. Our\nfindings reveal that the prediction error is directly proportional to the\nquantization error of the converged weights, which can be effectively managed\nthrough an optimized rotation configuration. Experiments on Pythia, Qwen and\nLlama models of different sizes demonstrate the effectiveness of RoSTE.\nCompared to existing post-SFT quantization baselines, our method consistently\nachieves superior performances across various tasks and different LLM\narchitectures."
                },
                "authors": [
                    {
                        "name": "Quan Wei"
                    },
                    {
                        "name": "Chung-Yiu Yau"
                    },
                    {
                        "name": "Hoi-To Wai"
                    },
                    {
                        "name": "Yang Katie Zhao"
                    },
                    {
                        "name": "Dongyeop Kang"
                    },
                    {
                        "name": "Youngsuk Park"
                    },
                    {
                        "name": "Mingyi Hong"
                    }
                ],
                "author_detail": {
                    "name": "Mingyi Hong"
                },
                "author": "Mingyi Hong",
                "arxiv_comment": "20 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09003v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09003v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12444v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12444v3",
                "updated": "2025-03-21T15:52:39Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    15,
                    52,
                    39,
                    4,
                    80,
                    0
                ],
                "published": "2024-12-17T01:12:35Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    1,
                    12,
                    35,
                    1,
                    352,
                    0
                ],
                "title": "LazyDiT: Lazy Learning for the Acceleration of Diffusion Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LazyDiT: Lazy Learning for the Acceleration of Diffusion Transformers"
                },
                "summary": "Diffusion Transformers have emerged as the preeminent models for a wide array\nof generative tasks, demonstrating superior performance and efficacy across\nvarious applications. The promising results come at the cost of slow inference,\nas each denoising step requires running the whole transformer model with a\nlarge amount of parameters. In this paper, we show that performing the full\ncomputation of the model at each diffusion step is unnecessary, as some\ncomputations can be skipped by lazily reusing the results of previous steps.\nFurthermore, we show that the lower bound of similarity between outputs at\nconsecutive steps is notably high, and this similarity can be linearly\napproximated using the inputs. To verify our demonstrations, we propose the\n\\textbf{LazyDiT}, a lazy learning framework that efficiently leverages cached\nresults from earlier steps to skip redundant computations. Specifically, we\nincorporate lazy learning layers into the model, effectively trained to\nmaximize laziness, enabling dynamic skipping of redundant computations.\nExperimental results show that LazyDiT outperforms the DDIM sampler across\nmultiple diffusion transformer models at various resolutions. Furthermore, we\nimplement our method on mobile devices, achieving better performance than DDIM\nwith similar latency. Code: https://github.com/shawnricecake/lazydit",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers have emerged as the preeminent models for a wide array\nof generative tasks, demonstrating superior performance and efficacy across\nvarious applications. The promising results come at the cost of slow inference,\nas each denoising step requires running the whole transformer model with a\nlarge amount of parameters. In this paper, we show that performing the full\ncomputation of the model at each diffusion step is unnecessary, as some\ncomputations can be skipped by lazily reusing the results of previous steps.\nFurthermore, we show that the lower bound of similarity between outputs at\nconsecutive steps is notably high, and this similarity can be linearly\napproximated using the inputs. To verify our demonstrations, we propose the\n\\textbf{LazyDiT}, a lazy learning framework that efficiently leverages cached\nresults from earlier steps to skip redundant computations. Specifically, we\nincorporate lazy learning layers into the model, effectively trained to\nmaximize laziness, enabling dynamic skipping of redundant computations.\nExperimental results show that LazyDiT outperforms the DDIM sampler across\nmultiple diffusion transformer models at various resolutions. Furthermore, we\nimplement our method on mobile devices, achieving better performance than DDIM\nwith similar latency. Code: https://github.com/shawnricecake/lazydit"
                },
                "authors": [
                    {
                        "name": "Xuan Shen"
                    },
                    {
                        "name": "Zhao Song"
                    },
                    {
                        "name": "Yufa Zhou"
                    },
                    {
                        "name": "Bo Chen"
                    },
                    {
                        "name": "Yanyu Li"
                    },
                    {
                        "name": "Yifan Gong"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Hao Tan"
                    },
                    {
                        "name": "Jason Kuen"
                    },
                    {
                        "name": "Henghui Ding"
                    },
                    {
                        "name": "Zhihao Shu"
                    },
                    {
                        "name": "Wei Niu"
                    },
                    {
                        "name": "Pu Zhao"
                    },
                    {
                        "name": "Yanzhi Wang"
                    },
                    {
                        "name": "Jiuxiang Gu"
                    }
                ],
                "author_detail": {
                    "name": "Jiuxiang Gu"
                },
                "author": "Jiuxiang Gu",
                "arxiv_comment": "Accepted by AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12444v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12444v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15102v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15102v3",
                "updated": "2025-03-21T15:47:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    15,
                    47,
                    53,
                    4,
                    80,
                    0
                ],
                "published": "2024-11-22T18:06:14Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    18,
                    6,
                    14,
                    4,
                    327,
                    0
                ],
                "title": "AttriBoT: A Bag of Tricks for Efficiently Approximating Leave-One-Out\n  Context Attribution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AttriBoT: A Bag of Tricks for Efficiently Approximating Leave-One-Out\n  Context Attribution"
                },
                "summary": "The influence of contextual input on the behavior of large language models\n(LLMs) has prompted the development of context attribution methods that aim to\nquantify each context span's effect on an LLM's generations. The leave-one-out\n(LOO) error, which measures the change in the likelihood of the LLM's response\nwhen a given span of the context is removed, provides a principled way to\nperform context attribution, but can be prohibitively expensive to compute for\nlarge models. In this work, we introduce AttriBoT, a series of novel techniques\nfor efficiently computing an approximation of the LOO error for context\nattribution. Specifically, AttriBoT uses cached activations to avoid redundant\noperations, performs hierarchical attribution to reduce computation, and\nemulates the behavior of large target models with smaller proxy models. Taken\ntogether, AttriBoT can provide a >300x speedup while remaining more faithful to\na target model's LOO error than prior context attribution methods. This stark\nincrease in performance makes computing context attributions for a given\nresponse 30x faster than generating the response itself, empowering real-world\napplications that require computing attributions at scale. We release a\nuser-friendly and efficient implementation of AttriBoT to enable efficient LLM\ninterpretability as well as encourage future development of efficient context\nattribution methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The influence of contextual input on the behavior of large language models\n(LLMs) has prompted the development of context attribution methods that aim to\nquantify each context span's effect on an LLM's generations. The leave-one-out\n(LOO) error, which measures the change in the likelihood of the LLM's response\nwhen a given span of the context is removed, provides a principled way to\nperform context attribution, but can be prohibitively expensive to compute for\nlarge models. In this work, we introduce AttriBoT, a series of novel techniques\nfor efficiently computing an approximation of the LOO error for context\nattribution. Specifically, AttriBoT uses cached activations to avoid redundant\noperations, performs hierarchical attribution to reduce computation, and\nemulates the behavior of large target models with smaller proxy models. Taken\ntogether, AttriBoT can provide a >300x speedup while remaining more faithful to\na target model's LOO error than prior context attribution methods. This stark\nincrease in performance makes computing context attributions for a given\nresponse 30x faster than generating the response itself, empowering real-world\napplications that require computing attributions at scale. We release a\nuser-friendly and efficient implementation of AttriBoT to enable efficient LLM\ninterpretability as well as encourage future development of efficient context\nattribution methods."
                },
                "authors": [
                    {
                        "name": "Fengyuan Liu"
                    },
                    {
                        "name": "Nikhil Kandpal"
                    },
                    {
                        "name": "Colin Raffel"
                    }
                ],
                "author_detail": {
                    "name": "Colin Raffel"
                },
                "author": "Colin Raffel",
                "arxiv_comment": "24 pages, 11 figures, ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15102v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15102v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00876v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00876v4",
                "updated": "2025-03-21T13:30:33Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    13,
                    30,
                    33,
                    4,
                    80,
                    0
                ],
                "published": "2024-12-01T16:32:31Z",
                "published_parsed": [
                    2024,
                    12,
                    1,
                    16,
                    32,
                    31,
                    6,
                    336,
                    0
                ],
                "title": "Dynamic-LLaVA: Efficient Multimodal Large Language Models via Dynamic\n  Vision-language Context Sparsification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic-LLaVA: Efficient Multimodal Large Language Models via Dynamic\n  Vision-language Context Sparsification"
                },
                "summary": "Multimodal Large Language Models (MLLMs) have achieved remarkable success in\nvision understanding, reasoning, and interaction. However, the inference\ncomputation and memory increase progressively with the generation of output\ntokens during decoding, directly affecting the efficacy of MLLMs. Existing\nmethods attempt to reduce the vision context redundancy to achieve efficient\nMLLMs. Unfortunately, the efficiency benefits of the vision context reduction\nin the prefill stage gradually diminish during the decoding stage. To address\nthis problem, we proposed a dynamic vision-language context sparsification\nframework Dynamic-LLaVA, which dynamically reduces the redundancy of vision\ncontext in the prefill stage and decreases the memory and computation overhead\nof the generated language context during decoding. Dynamic-LLaVA designs a\ntailored sparsification inference scheme for different inference modes, i.e.,\nprefill, decoding with and without KV cache, to achieve efficient inference of\nMLLMs. In practice, Dynamic-LLaVA can reduce computation consumption by\n$\\sim$75\\% in the prefill stage. Meanwhile, throughout the entire generation\nprocess of MLLMs, Dynamic-LLaVA reduces the $\\sim$50\\% computation consumption\nunder decoding without KV cache, while saving $\\sim$50\\% GPU memory overhead\nwhen decoding with KV cache, due to the vision-language context sparsification.\nExtensive experiments also demonstrate that Dynamic-LLaVA achieves efficient\ninference for MLLMs with negligible understanding and generation ability\ndegradation or even performance gains compared to the full-context inference\nbaselines. Code is available at https://github.com/Osilly/dynamic_llava .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) have achieved remarkable success in\nvision understanding, reasoning, and interaction. However, the inference\ncomputation and memory increase progressively with the generation of output\ntokens during decoding, directly affecting the efficacy of MLLMs. Existing\nmethods attempt to reduce the vision context redundancy to achieve efficient\nMLLMs. Unfortunately, the efficiency benefits of the vision context reduction\nin the prefill stage gradually diminish during the decoding stage. To address\nthis problem, we proposed a dynamic vision-language context sparsification\nframework Dynamic-LLaVA, which dynamically reduces the redundancy of vision\ncontext in the prefill stage and decreases the memory and computation overhead\nof the generated language context during decoding. Dynamic-LLaVA designs a\ntailored sparsification inference scheme for different inference modes, i.e.,\nprefill, decoding with and without KV cache, to achieve efficient inference of\nMLLMs. In practice, Dynamic-LLaVA can reduce computation consumption by\n$\\sim$75\\% in the prefill stage. Meanwhile, throughout the entire generation\nprocess of MLLMs, Dynamic-LLaVA reduces the $\\sim$50\\% computation consumption\nunder decoding without KV cache, while saving $\\sim$50\\% GPU memory overhead\nwhen decoding with KV cache, due to the vision-language context sparsification.\nExtensive experiments also demonstrate that Dynamic-LLaVA achieves efficient\ninference for MLLMs with negligible understanding and generation ability\ndegradation or even performance gains compared to the full-context inference\nbaselines. Code is available at https://github.com/Osilly/dynamic_llava ."
                },
                "authors": [
                    {
                        "name": "Wenxuan Huang"
                    },
                    {
                        "name": "Zijie Zhai"
                    },
                    {
                        "name": "Yunhang Shen"
                    },
                    {
                        "name": "Shaosheng Cao"
                    },
                    {
                        "name": "Fei Zhao"
                    },
                    {
                        "name": "Xiangfeng Xu"
                    },
                    {
                        "name": "Zheyu Ye"
                    },
                    {
                        "name": "Yao Hu"
                    },
                    {
                        "name": "Shaohui Lin"
                    }
                ],
                "author_detail": {
                    "name": "Shaohui Lin"
                },
                "author": "Shaohui Lin",
                "arxiv_comment": "Accepted to ICLR 2025. Code is available at\n  https://github.com/Osilly/dynamic_llava",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00876v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00876v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09398v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09398v3",
                "updated": "2025-03-21T12:51:15Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    12,
                    51,
                    15,
                    4,
                    80,
                    0
                ],
                "published": "2024-09-14T10:15:37Z",
                "published_parsed": [
                    2024,
                    9,
                    14,
                    10,
                    15,
                    37,
                    5,
                    258,
                    0
                ],
                "title": "Language-Queried Target Sound Extraction Without Parallel Training Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language-Queried Target Sound Extraction Without Parallel Training Data"
                },
                "summary": "Language-queried target sound extraction (TSE) aims to extract specific\nsounds from mixtures based on language queries. Traditional fully-supervised\ntraining schemes require extensively annotated parallel audio-text data, which\nare labor-intensive. We introduce a parallel-data-free training scheme,\nrequiring only unlabelled audio clips for TSE model training by utilizing the\ncontrastive language-audio pre-trained model (CLAP). In a vanilla\nparallel-data-free training stage, target audio is encoded using the\npre-trained CLAP audio encoder to form a condition embedding, while during\ntesting, user language queries are encoded by CLAP text encoder as the\ncondition embedding. This vanilla approach assumes perfect alignment between\ntext and audio embeddings, which is unrealistic. Two major challenges arise\nfrom training-testing mismatch: the persistent modality gap between text and\naudio and the risk of overfitting due to the exposure of rich acoustic details\nin target audio embedding during training. To address this, we propose a\nretrieval-augmented strategy. Specifically, we create an embedding cache using\naudio captions generated by a large language model (LLM). During training,\ntarget audio embeddings retrieve text embeddings from this cache to use as\ncondition embeddings, ensuring consistent modalities between training and\ntesting and eliminating information leakage. Extensive experiment results show\nthat our retrieval-augmented approach achieves consistent and notable\nperformance improvements over existing state-of-the-art with better\ngeneralizability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language-queried target sound extraction (TSE) aims to extract specific\nsounds from mixtures based on language queries. Traditional fully-supervised\ntraining schemes require extensively annotated parallel audio-text data, which\nare labor-intensive. We introduce a parallel-data-free training scheme,\nrequiring only unlabelled audio clips for TSE model training by utilizing the\ncontrastive language-audio pre-trained model (CLAP). In a vanilla\nparallel-data-free training stage, target audio is encoded using the\npre-trained CLAP audio encoder to form a condition embedding, while during\ntesting, user language queries are encoded by CLAP text encoder as the\ncondition embedding. This vanilla approach assumes perfect alignment between\ntext and audio embeddings, which is unrealistic. Two major challenges arise\nfrom training-testing mismatch: the persistent modality gap between text and\naudio and the risk of overfitting due to the exposure of rich acoustic details\nin target audio embedding during training. To address this, we propose a\nretrieval-augmented strategy. Specifically, we create an embedding cache using\naudio captions generated by a large language model (LLM). During training,\ntarget audio embeddings retrieve text embeddings from this cache to use as\ncondition embeddings, ensuring consistent modalities between training and\ntesting and eliminating information leakage. Extensive experiment results show\nthat our retrieval-augmented approach achieves consistent and notable\nperformance improvements over existing state-of-the-art with better\ngeneralizability."
                },
                "authors": [
                    {
                        "name": "Hao Ma"
                    },
                    {
                        "name": "Zhiyuan Peng"
                    },
                    {
                        "name": "Xu Li"
                    },
                    {
                        "name": "Yukai Li"
                    },
                    {
                        "name": "Mingjie Shao"
                    },
                    {
                        "name": "Qiuqiang Kong"
                    },
                    {
                        "name": "Ju Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ju Liu"
                },
                "author": "Ju Liu",
                "arxiv_comment": "Accepted by ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09398v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09398v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16870v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16870v1",
                "updated": "2025-03-21T05:58:18Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    5,
                    58,
                    18,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-21T05:58:18Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    5,
                    58,
                    18,
                    4,
                    80,
                    0
                ],
                "title": "Sparse Logit Sampling: Accelerating Knowledge Distillation in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse Logit Sampling: Accelerating Knowledge Distillation in LLMs"
                },
                "summary": "Knowledge distillation can be a cost-effective technique to distill knowledge\nin Large Language Models, if the teacher output logits can be pre-computed and\ncached. However, successfully applying this to pre-training remains largely\nunexplored. In this work, we prove that naive approaches for sparse knowledge\ndistillation such as caching Top-K probabilities, while intuitive, provide\nbiased estimates of teacher probability distribution to the student, resulting\nin suboptimal performance and calibration. We propose an\nimportance-sampling-based method `Random Sampling Knowledge Distillation',\nwhich provides unbiased estimates, preserves the gradient in expectation, and\nrequires storing significantly sparser logits. Our method enables faster\ntraining of student models with marginal overhead (<10%) compared to\ncross-entropy based training, while maintaining competitive performance\ncompared to full distillation, across a range of model sizes from 300M to 3B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge distillation can be a cost-effective technique to distill knowledge\nin Large Language Models, if the teacher output logits can be pre-computed and\ncached. However, successfully applying this to pre-training remains largely\nunexplored. In this work, we prove that naive approaches for sparse knowledge\ndistillation such as caching Top-K probabilities, while intuitive, provide\nbiased estimates of teacher probability distribution to the student, resulting\nin suboptimal performance and calibration. We propose an\nimportance-sampling-based method `Random Sampling Knowledge Distillation',\nwhich provides unbiased estimates, preserves the gradient in expectation, and\nrequires storing significantly sparser logits. Our method enables faster\ntraining of student models with marginal overhead (<10%) compared to\ncross-entropy based training, while maintaining competitive performance\ncompared to full distillation, across a range of model sizes from 300M to 3B."
                },
                "authors": [
                    {
                        "name": "Anshumann"
                    },
                    {
                        "name": "Mohd Abbas Zaidi"
                    },
                    {
                        "name": "Akhil Kedia"
                    },
                    {
                        "name": "Jinwoo Ahn"
                    },
                    {
                        "name": "Taehwak Kwon"
                    },
                    {
                        "name": "Kangwook Lee"
                    },
                    {
                        "name": "Haejun Lee"
                    },
                    {
                        "name": "Joohyung Lee"
                    }
                ],
                "author_detail": {
                    "name": "Joohyung Lee"
                },
                "author": "Joohyung Lee",
                "arxiv_comment": "Anshumann, Mohd Abbas Zaidi and Akhil Kedia have Equal Contribution",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16870v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16870v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16131v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16131v2",
                "updated": "2025-03-21T01:59:12Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    1,
                    59,
                    12,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-20T13:25:03Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    13,
                    25,
                    3,
                    3,
                    79,
                    0
                ],
                "title": "MKG-Rank: Enhancing Large Language Models with Knowledge Graph for\n  Multilingual Medical Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MKG-Rank: Enhancing Large Language Models with Knowledge Graph for\n  Multilingual Medical Question Answering"
                },
                "summary": "Large Language Models (LLMs) have shown remarkable progress in medical\nquestion answering (QA), yet their effectiveness remains predominantly limited\nto English due to imbalanced multilingual training data and scarce medical\nresources for low-resource languages. To address this critical language gap in\nmedical QA, we propose Multilingual Knowledge Graph-based Retrieval Ranking\n(MKG-Rank), a knowledge graph-enhanced framework that enables English-centric\nLLMs to perform multilingual medical QA. Through a word-level translation\nmechanism, our framework efficiently integrates comprehensive English-centric\nmedical knowledge graphs into LLM reasoning at a low cost, mitigating\ncross-lingual semantic distortion and achieving precise medical QA across\nlanguage barriers. To enhance efficiency, we introduce caching and multi-angle\nranking strategies to optimize the retrieval process, significantly reducing\nresponse times and prioritizing relevant medical knowledge. Extensive\nevaluations on multilingual medical QA benchmarks across Chinese, Japanese,\nKorean, and Swahili demonstrate that MKG-Rank consistently outperforms\nzero-shot LLMs, achieving maximum 35.03% increase in accuracy, while\nmaintaining an average retrieval time of only 0.0009 seconds.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown remarkable progress in medical\nquestion answering (QA), yet their effectiveness remains predominantly limited\nto English due to imbalanced multilingual training data and scarce medical\nresources for low-resource languages. To address this critical language gap in\nmedical QA, we propose Multilingual Knowledge Graph-based Retrieval Ranking\n(MKG-Rank), a knowledge graph-enhanced framework that enables English-centric\nLLMs to perform multilingual medical QA. Through a word-level translation\nmechanism, our framework efficiently integrates comprehensive English-centric\nmedical knowledge graphs into LLM reasoning at a low cost, mitigating\ncross-lingual semantic distortion and achieving precise medical QA across\nlanguage barriers. To enhance efficiency, we introduce caching and multi-angle\nranking strategies to optimize the retrieval process, significantly reducing\nresponse times and prioritizing relevant medical knowledge. Extensive\nevaluations on multilingual medical QA benchmarks across Chinese, Japanese,\nKorean, and Swahili demonstrate that MKG-Rank consistently outperforms\nzero-shot LLMs, achieving maximum 35.03% increase in accuracy, while\nmaintaining an average retrieval time of only 0.0009 seconds."
                },
                "authors": [
                    {
                        "name": "Feiyang Li"
                    },
                    {
                        "name": "Yingjian Chen"
                    },
                    {
                        "name": "Haoran Liu"
                    },
                    {
                        "name": "Rui Yang"
                    },
                    {
                        "name": "Han Yuan"
                    },
                    {
                        "name": "Yuang Jiang"
                    },
                    {
                        "name": "Tianxiao Li"
                    },
                    {
                        "name": "Edison Marrese Taylor"
                    },
                    {
                        "name": "Hossein Rouhizadeh"
                    },
                    {
                        "name": "Yusuke Iwasawa"
                    },
                    {
                        "name": "Douglas Teodoro"
                    },
                    {
                        "name": "Yutaka Matsuo"
                    },
                    {
                        "name": "Irene Li"
                    }
                ],
                "author_detail": {
                    "name": "Irene Li"
                },
                "author": "Irene Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16131v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16131v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02430v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02430v3",
                "updated": "2025-03-20T21:49:15Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    21,
                    49,
                    15,
                    3,
                    79,
                    0
                ],
                "published": "2025-02-04T15:55:10Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    15,
                    55,
                    10,
                    1,
                    35,
                    0
                ],
                "title": "A Scalable Crawling Algorithm Utilizing Noisy Change-Indicating Signals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Scalable Crawling Algorithm Utilizing Noisy Change-Indicating Signals"
                },
                "summary": "Web refresh crawling is the problem of keeping a cache of web pages fresh,\nthat is, having the most recent copy available when a page is requested, given\na limited bandwidth available to the crawler. Under the assumption that the\nchange and request events, resp., to each web page follow independent Poisson\nprocesses, the optimal scheduling policy was derived by Azar et al. 2018. In\nthis paper, we study an extension of this problem where side information\nindicating content changes, such as various types of web pings, for example,\nsignals from sitemaps, content delivery networks, etc., is available.\nIncorporating such side information into the crawling policy is challenging,\nbecause (i) the signals can be noisy with false positive events and with\nmissing change events; and (ii) the crawler should achieve a fair performance\nover web pages regardless of the quality of the side information, which might\ndiffer from web page to web page. We propose a scalable crawling algorithm\nwhich (i) uses the noisy side information in an optimal way under mild\nassumptions; (ii) can be deployed without heavy centralized computation; (iii)\nis able to crawl web pages at a constant total rate without spikes in the total\nbandwidth usage over any time interval, and automatically adapt to the new\noptimal solution when the total bandwidth changes without centralized\ncomputation. Experiments clearly demonstrate the versatility of our approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Web refresh crawling is the problem of keeping a cache of web pages fresh,\nthat is, having the most recent copy available when a page is requested, given\na limited bandwidth available to the crawler. Under the assumption that the\nchange and request events, resp., to each web page follow independent Poisson\nprocesses, the optimal scheduling policy was derived by Azar et al. 2018. In\nthis paper, we study an extension of this problem where side information\nindicating content changes, such as various types of web pings, for example,\nsignals from sitemaps, content delivery networks, etc., is available.\nIncorporating such side information into the crawling policy is challenging,\nbecause (i) the signals can be noisy with false positive events and with\nmissing change events; and (ii) the crawler should achieve a fair performance\nover web pages regardless of the quality of the side information, which might\ndiffer from web page to web page. We propose a scalable crawling algorithm\nwhich (i) uses the noisy side information in an optimal way under mild\nassumptions; (ii) can be deployed without heavy centralized computation; (iii)\nis able to crawl web pages at a constant total rate without spikes in the total\nbandwidth usage over any time interval, and automatically adapt to the new\noptimal solution when the total bandwidth changes without centralized\ncomputation. Experiments clearly demonstrate the versatility of our approach."
                },
                "authors": [
                    {
                        "name": "Rbert Busa-Fekete"
                    },
                    {
                        "name": "Julian Zimmert"
                    },
                    {
                        "name": "Andrs Gyrgy"
                    },
                    {
                        "name": "Linhai Qiu"
                    },
                    {
                        "name": "Tzu-Wei Sung"
                    },
                    {
                        "name": "Hao Shen"
                    },
                    {
                        "name": "Hyomin Choi"
                    },
                    {
                        "name": "Sharmila Subramaniam"
                    },
                    {
                        "name": "Li Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Li Xiao"
                },
                "author": "Li Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02430v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02430v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16588v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16588v1",
                "updated": "2025-03-20T17:37:15Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    17,
                    37,
                    15,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T17:37:15Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    17,
                    37,
                    15,
                    3,
                    79,
                    0
                ],
                "title": "A Unified Framework for Quantitative Cache Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Unified Framework for Quantitative Cache Analysis"
                },
                "summary": "In this work we unify two existing lines of work towards cache analysis for\nnon-LRU policies. To this end, we extend the notion of competitiveness to\nblock-wise competitiveness and systematically analyze the competitiveness and\nblock competitiveness of FIFO and MRU relative to LRU for arbitrary\nassociativities. We show how competitiveness and block competitiveness can be\nexploited in state-of-the-art WCET analysis based on the results of existing\npersistence analyses for LRU. Unlike prior work, our approach is applicable to\nmicroarchitectures that exhibit timing anomalies. We experimentally evaluate\nthe precision and cost of our approach on benchmarks from TACLeBench. The\nexperiments demonstrate that quantitative cache analysis for FIFO and MRU comes\nclose to the precision of LRU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work we unify two existing lines of work towards cache analysis for\nnon-LRU policies. To this end, we extend the notion of competitiveness to\nblock-wise competitiveness and systematically analyze the competitiveness and\nblock competitiveness of FIFO and MRU relative to LRU for arbitrary\nassociativities. We show how competitiveness and block competitiveness can be\nexploited in state-of-the-art WCET analysis based on the results of existing\npersistence analyses for LRU. Unlike prior work, our approach is applicable to\nmicroarchitectures that exhibit timing anomalies. We experimentally evaluate\nthe precision and cost of our approach on benchmarks from TACLeBench. The\nexperiments demonstrate that quantitative cache analysis for FIFO and MRU comes\nclose to the precision of LRU."
                },
                "authors": [
                    {
                        "name": "Sophie Kahlen"
                    },
                    {
                        "name": "Jan Reineke"
                    }
                ],
                "author_detail": {
                    "name": "Jan Reineke"
                },
                "author": "Jan Reineke",
                "arxiv_comment": "Extended version of RTAS 2025 paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16588v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16588v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.3.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16302v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16302v1",
                "updated": "2025-03-20T16:23:44Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    16,
                    23,
                    44,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T16:23:44Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    16,
                    23,
                    44,
                    3,
                    79,
                    0
                ],
                "title": "Unleashing Vecset Diffusion Model for Fast Shape Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unleashing Vecset Diffusion Model for Fast Shape Generation"
                },
                "summary": "3D shape generation has greatly flourished through the development of\nso-called \"native\" 3D diffusion, particularly through the Vecset Diffusion\nModel (VDM). While recent advancements have shown promising results in\ngenerating high-resolution 3D shapes, VDM still struggles with high-speed\ngeneration. Challenges exist because of difficulties not only in accelerating\ndiffusion sampling but also VAE decoding in VDM, areas under-explored in\nprevious works. To address these challenges, we present FlashVDM, a systematic\nframework for accelerating both VAE and DiT in VDM. For DiT, FlashVDM enables\nflexible diffusion sampling with as few as 5 inference steps and comparable\nquality, which is made possible by stabilizing consistency distillation with\nour newly introduced Progressive Flow Distillation. For VAE, we introduce a\nlightning vecset decoder equipped with Adaptive KV Selection, Hierarchical\nVolume Decoding, and Efficient Network Design. By exploiting the locality of\nthe vecset and the sparsity of shape surface in the volume, our decoder\ndrastically lowers FLOPs, minimizing the overall decoding overhead. We apply\nFlashVDM to Hunyuan3D-2 to obtain Hunyuan3D-2 Turbo. Through systematic\nevaluation, we show that our model significantly outperforms existing fast 3D\ngeneration methods, achieving comparable performance to the state-of-the-art\nwhile reducing inference time by over 45x for reconstruction and 32x for\ngeneration. Code and models are available at\nhttps://github.com/Tencent/FlashVDM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D shape generation has greatly flourished through the development of\nso-called \"native\" 3D diffusion, particularly through the Vecset Diffusion\nModel (VDM). While recent advancements have shown promising results in\ngenerating high-resolution 3D shapes, VDM still struggles with high-speed\ngeneration. Challenges exist because of difficulties not only in accelerating\ndiffusion sampling but also VAE decoding in VDM, areas under-explored in\nprevious works. To address these challenges, we present FlashVDM, a systematic\nframework for accelerating both VAE and DiT in VDM. For DiT, FlashVDM enables\nflexible diffusion sampling with as few as 5 inference steps and comparable\nquality, which is made possible by stabilizing consistency distillation with\nour newly introduced Progressive Flow Distillation. For VAE, we introduce a\nlightning vecset decoder equipped with Adaptive KV Selection, Hierarchical\nVolume Decoding, and Efficient Network Design. By exploiting the locality of\nthe vecset and the sparsity of shape surface in the volume, our decoder\ndrastically lowers FLOPs, minimizing the overall decoding overhead. We apply\nFlashVDM to Hunyuan3D-2 to obtain Hunyuan3D-2 Turbo. Through systematic\nevaluation, we show that our model significantly outperforms existing fast 3D\ngeneration methods, achieving comparable performance to the state-of-the-art\nwhile reducing inference time by over 45x for reconstruction and 32x for\ngeneration. Code and models are available at\nhttps://github.com/Tencent/FlashVDM."
                },
                "authors": [
                    {
                        "name": "Zeqiang Lai"
                    },
                    {
                        "name": "Yunfei Zhao"
                    },
                    {
                        "name": "Zibo Zhao"
                    },
                    {
                        "name": "Haolin Liu"
                    },
                    {
                        "name": "Fuyun Wang"
                    },
                    {
                        "name": "Huiwen Shi"
                    },
                    {
                        "name": "Xianghui Yang"
                    },
                    {
                        "name": "Qinxiang Lin"
                    },
                    {
                        "name": "Jinwei Huang"
                    },
                    {
                        "name": "Yuhong Liu"
                    },
                    {
                        "name": "Jie Jiang"
                    },
                    {
                        "name": "Chunchao Guo"
                    },
                    {
                        "name": "Xiangyu Yue"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyu Yue"
                },
                "author": "Xiangyu Yue",
                "arxiv_comment": "Technical report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16302v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16302v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16257v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16257v1",
                "updated": "2025-03-20T15:52:43Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    15,
                    52,
                    43,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T15:52:43Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    15,
                    52,
                    43,
                    3,
                    79,
                    0
                ],
                "title": "Plug-and-Play 1.x-Bit KV Cache Quantization for Video Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Plug-and-Play 1.x-Bit KV Cache Quantization for Video Large Language\n  Models"
                },
                "summary": "Video large language models (VideoLLMs) have demonstrated the capability to\nprocess longer video inputs and enable complex reasoning and analysis. However,\ndue to the thousands of visual tokens from the video frames, key-value (KV)\ncache can significantly increase memory requirements, becoming a bottleneck for\ninference speed and memory usage. KV cache quantization is a widely used\napproach to address this problem. In this paper, we find that 2-bit KV\nquantization of VideoLLMs can hardly hurt the model performance, while the\nlimit of KV cache quantization in even lower bits has not been investigated. To\nbridge this gap, we introduce VidKV, a plug-and-play KV cache quantization\nmethod to compress the KV cache to lower than 2 bits. Specifically, (1) for\nkey, we propose a mixed-precision quantization strategy in the channel\ndimension, where we perform 2-bit quantization for anomalous channels and 1-bit\nquantization combined with FFT for normal channels; (2) for value, we implement\n1.58-bit quantization while selectively filtering semantically salient visual\ntokens for targeted preservation, for a better trade-off between precision and\nmodel performance. Importantly, our findings suggest that the value cache of\nVideoLLMs should be quantized in a per-channel fashion instead of the per-token\nfashion proposed by prior KV cache quantization works for LLMs. Empirically,\nextensive results with LLaVA-OV-7B and Qwen2.5-VL-7B on six benchmarks show\nthat VidKV effectively compresses the KV cache to 1.5-bit and 1.58-bit\nprecision with almost no performance drop compared to the FP16 counterparts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video large language models (VideoLLMs) have demonstrated the capability to\nprocess longer video inputs and enable complex reasoning and analysis. However,\ndue to the thousands of visual tokens from the video frames, key-value (KV)\ncache can significantly increase memory requirements, becoming a bottleneck for\ninference speed and memory usage. KV cache quantization is a widely used\napproach to address this problem. In this paper, we find that 2-bit KV\nquantization of VideoLLMs can hardly hurt the model performance, while the\nlimit of KV cache quantization in even lower bits has not been investigated. To\nbridge this gap, we introduce VidKV, a plug-and-play KV cache quantization\nmethod to compress the KV cache to lower than 2 bits. Specifically, (1) for\nkey, we propose a mixed-precision quantization strategy in the channel\ndimension, where we perform 2-bit quantization for anomalous channels and 1-bit\nquantization combined with FFT for normal channels; (2) for value, we implement\n1.58-bit quantization while selectively filtering semantically salient visual\ntokens for targeted preservation, for a better trade-off between precision and\nmodel performance. Importantly, our findings suggest that the value cache of\nVideoLLMs should be quantized in a per-channel fashion instead of the per-token\nfashion proposed by prior KV cache quantization works for LLMs. Empirically,\nextensive results with LLaVA-OV-7B and Qwen2.5-VL-7B on six benchmarks show\nthat VidKV effectively compresses the KV cache to 1.5-bit and 1.58-bit\nprecision with almost no performance drop compared to the FP16 counterparts."
                },
                "authors": [
                    {
                        "name": "Keda Tao"
                    },
                    {
                        "name": "Haoxuan You"
                    },
                    {
                        "name": "Yang Sui"
                    },
                    {
                        "name": "Can Qin"
                    },
                    {
                        "name": "Huan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Huan Wang"
                },
                "author": "Huan Wang",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16257v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16257v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16163v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16163v1",
                "updated": "2025-03-20T14:01:56Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    14,
                    1,
                    56,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T14:01:56Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    14,
                    1,
                    56,
                    3,
                    79,
                    0
                ],
                "title": "SpeCache: Speculative Key-Value Caching for Efficient Generation of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpeCache: Speculative Key-Value Caching for Efficient Generation of LLMs"
                },
                "summary": "Transformer-based large language models (LLMs) have already achieved\nremarkable results on long-text tasks, but the limited GPU memory (VRAM)\nresources struggle to accommodate the linearly growing demand for key-value\n(KV) cache as the sequence length increases, which has become a bottleneck for\nthe application of LLMs on long sequences. Existing KV cache compression\nmethods include eviction, merging, or quantization of the KV cache to reduce\nits size. However, compression results in irreversible information forgetting,\npotentially affecting the accuracy of subsequent decoding. In this paper, we\npropose SpeCache, which takes full advantage of the large and easily expandable\nCPU memory to offload the complete KV cache, and dynamically fetches KV pairs\nback in each decoding step based on their importance measured by low-bit KV\ncache copy in VRAM. To avoid inference latency caused by CPU-GPU communication,\nSpeCache speculatively predicts the KV pairs that the next token might attend\nto, allowing us to prefetch them before the next decoding step which enables\nparallelization of prefetching and computation. Experiments on LongBench and\nNeedle-in-a-Haystack benchmarks verify that SpeCache effectively reduces VRAM\nusage while avoiding information forgetting for long sequences without\nre-training, even with a 10x high KV cache compression ratio.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models (LLMs) have already achieved\nremarkable results on long-text tasks, but the limited GPU memory (VRAM)\nresources struggle to accommodate the linearly growing demand for key-value\n(KV) cache as the sequence length increases, which has become a bottleneck for\nthe application of LLMs on long sequences. Existing KV cache compression\nmethods include eviction, merging, or quantization of the KV cache to reduce\nits size. However, compression results in irreversible information forgetting,\npotentially affecting the accuracy of subsequent decoding. In this paper, we\npropose SpeCache, which takes full advantage of the large and easily expandable\nCPU memory to offload the complete KV cache, and dynamically fetches KV pairs\nback in each decoding step based on their importance measured by low-bit KV\ncache copy in VRAM. To avoid inference latency caused by CPU-GPU communication,\nSpeCache speculatively predicts the KV pairs that the next token might attend\nto, allowing us to prefetch them before the next decoding step which enables\nparallelization of prefetching and computation. Experiments on LongBench and\nNeedle-in-a-Haystack benchmarks verify that SpeCache effectively reduces VRAM\nusage while avoiding information forgetting for long sequences without\nre-training, even with a 10x high KV cache compression ratio."
                },
                "authors": [
                    {
                        "name": "Shibo Jie"
                    },
                    {
                        "name": "Yehui Tang"
                    },
                    {
                        "name": "Kai Han"
                    },
                    {
                        "name": "Zhi-Hong Deng"
                    },
                    {
                        "name": "Jing Han"
                    }
                ],
                "author_detail": {
                    "name": "Jing Han"
                },
                "author": "Jing Han",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16163v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16163v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16112v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16112v1",
                "updated": "2025-03-20T13:00:36Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    13,
                    0,
                    36,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T13:00:36Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    13,
                    0,
                    36,
                    3,
                    79,
                    0
                ],
                "title": "PromptMobile: Efficient Promptus for Low Bandwidth Mobile Video\n  Streaming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PromptMobile: Efficient Promptus for Low Bandwidth Mobile Video\n  Streaming"
                },
                "summary": "Traditional video compression algorithms exhibit significant quality\ndegradation at extremely low bitrates. Promptus emerges as a new paradigm for\nvideo streaming, substantially cutting down the bandwidth essential for video\nstreaming. However, Promptus is computationally intensive and can not run in\nreal-time on mobile devices. This paper presents PromptMobile, an efficient\nacceleration framework tailored for on-device Promptus. Specifically, we\npropose (1) a two-stage efficient generation framework to reduce computational\ncost by 8.1x, (2) a fine-grained inter-frame caching to reduce redundant\ncomputations by 16.6\\%, (3) system-level optimizations to further enhance\nefficiency. The evaluations demonstrate that compared with the original\nPromptus, PromptMobile achieves a 13.6x increase in image generation speed.\nCompared with other streaming methods, PromptMobile achives an average LPIPS\nimprovement of 0.016 (compared with H.265), reducing 60\\% of severely distorted\nframes (compared to VQGAN).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional video compression algorithms exhibit significant quality\ndegradation at extremely low bitrates. Promptus emerges as a new paradigm for\nvideo streaming, substantially cutting down the bandwidth essential for video\nstreaming. However, Promptus is computationally intensive and can not run in\nreal-time on mobile devices. This paper presents PromptMobile, an efficient\nacceleration framework tailored for on-device Promptus. Specifically, we\npropose (1) a two-stage efficient generation framework to reduce computational\ncost by 8.1x, (2) a fine-grained inter-frame caching to reduce redundant\ncomputations by 16.6\\%, (3) system-level optimizations to further enhance\nefficiency. The evaluations demonstrate that compared with the original\nPromptus, PromptMobile achieves a 13.6x increase in image generation speed.\nCompared with other streaming methods, PromptMobile achives an average LPIPS\nimprovement of 0.016 (compared with H.265), reducing 60\\% of severely distorted\nframes (compared to VQGAN)."
                },
                "authors": [
                    {
                        "name": "Liming Liu"
                    },
                    {
                        "name": "Jiangkai Wu"
                    },
                    {
                        "name": "Haoyang Wang"
                    },
                    {
                        "name": "Peiheng Wang"
                    },
                    {
                        "name": "Xinggong Zhang"
                    },
                    {
                        "name": "Zongming Guo"
                    }
                ],
                "author_detail": {
                    "name": "Zongming Guo"
                },
                "author": "Zongming Guo",
                "arxiv_comment": "7 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16112v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16112v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15927v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15927v1",
                "updated": "2025-03-20T08:07:31Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    8,
                    7,
                    31,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T08:07:31Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    8,
                    7,
                    31,
                    3,
                    79,
                    0
                ],
                "title": "BlockDance: Reuse Structurally Similar Spatio-Temporal Features to\n  Accelerate Diffusion Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BlockDance: Reuse Structurally Similar Spatio-Temporal Features to\n  Accelerate Diffusion Transformers"
                },
                "summary": "Diffusion models have demonstrated impressive generation capabilities,\nparticularly with recent advancements leveraging transformer architectures to\nimprove both visual and artistic quality. However, Diffusion Transformers\n(DiTs) continue to encounter challenges related to low inference speed,\nprimarily due to the iterative denoising process. To address this issue, we\npropose BlockDance, a training-free approach that explores feature similarities\nat adjacent time steps to accelerate DiTs. Unlike previous feature-reuse\nmethods that lack tailored reuse strategies for features at different scales,\nBlockDance prioritizes the identification of the most structurally similar\nfeatures, referred to as Structurally Similar Spatio-Temporal (STSS) features.\nThese features are primarily located within the structure-focused blocks of the\ntransformer during the later stages of denoising. BlockDance caches and reuses\nthese highly similar features to mitigate redundant computation, thereby\naccelerating DiTs while maximizing consistency with the generated results of\nthe original model. Furthermore, considering the diversity of generated content\nand the varying distributions of redundant features, we introduce\nBlockDance-Ada, a lightweight decision-making network tailored for\ninstance-specific acceleration. BlockDance-Ada dynamically allocates resources\nand provides superior content quality. Both BlockDance and BlockDance-Ada have\nproven effective across various generation tasks and models, achieving\naccelerations between 25% and 50% while maintaining generation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have demonstrated impressive generation capabilities,\nparticularly with recent advancements leveraging transformer architectures to\nimprove both visual and artistic quality. However, Diffusion Transformers\n(DiTs) continue to encounter challenges related to low inference speed,\nprimarily due to the iterative denoising process. To address this issue, we\npropose BlockDance, a training-free approach that explores feature similarities\nat adjacent time steps to accelerate DiTs. Unlike previous feature-reuse\nmethods that lack tailored reuse strategies for features at different scales,\nBlockDance prioritizes the identification of the most structurally similar\nfeatures, referred to as Structurally Similar Spatio-Temporal (STSS) features.\nThese features are primarily located within the structure-focused blocks of the\ntransformer during the later stages of denoising. BlockDance caches and reuses\nthese highly similar features to mitigate redundant computation, thereby\naccelerating DiTs while maximizing consistency with the generated results of\nthe original model. Furthermore, considering the diversity of generated content\nand the varying distributions of redundant features, we introduce\nBlockDance-Ada, a lightweight decision-making network tailored for\ninstance-specific acceleration. BlockDance-Ada dynamically allocates resources\nand provides superior content quality. Both BlockDance and BlockDance-Ada have\nproven effective across various generation tasks and models, achieving\naccelerations between 25% and 50% while maintaining generation quality."
                },
                "authors": [
                    {
                        "name": "Hui Zhang"
                    },
                    {
                        "name": "Tingwei Gao"
                    },
                    {
                        "name": "Jie Shao"
                    },
                    {
                        "name": "Zuxuan Wu"
                    }
                ],
                "author_detail": {
                    "name": "Zuxuan Wu"
                },
                "author": "Zuxuan Wu",
                "arxiv_comment": "Accepted by CVPR2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15927v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15927v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.18921v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.18921v2",
                "updated": "2025-03-20T05:23:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    5,
                    23,
                    42,
                    3,
                    79,
                    0
                ],
                "published": "2024-07-09T13:47:05Z",
                "published_parsed": [
                    2024,
                    7,
                    9,
                    13,
                    47,
                    5,
                    1,
                    191,
                    0
                ],
                "title": "Mobile Edge Intelligence for Large Language Models: A Contemporary\n  Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mobile Edge Intelligence for Large Language Models: A Contemporary\n  Survey"
                },
                "summary": "On-device large language models (LLMs), referring to running LLMs on edge\ndevices, have raised considerable interest since they are more cost-effective,\nlatency-efficient, and privacy-preserving compared with the cloud paradigm.\nNonetheless, the performance of on-device LLMs is intrinsically constrained by\nresource limitations on edge devices. Sitting between cloud and on-device AI,\nmobile edge intelligence (MEI) presents a viable solution by provisioning AI\ncapabilities at the edge of mobile networks, enabling end users to offload\nheavy AI computation to capable edge servers nearby. This article provides a\ncontemporary survey on harnessing MEI for LLMs. We begin by illustrating\nseveral killer applications to demonstrate the urgent need for deploying LLMs\nat the network edge. Next, we present the preliminaries of LLMs and MEI,\nfollowed by resource-efficient LLM techniques. We then present an architectural\noverview of MEI for LLMs (MEI4LLM), outlining its core components and how it\nsupports the deployment of LLMs. Subsequently, we delve into various aspects of\nMEI4LLM, extensively covering edge LLM caching and delivery, edge LLM training,\nand edge LLM inference. Finally, we identify future research opportunities. We\nhope this article inspires researchers in the field to leverage mobile edge\ncomputing to facilitate LLM deployment, thereby unleashing the potential of\nLLMs across various privacy- and delay-sensitive applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On-device large language models (LLMs), referring to running LLMs on edge\ndevices, have raised considerable interest since they are more cost-effective,\nlatency-efficient, and privacy-preserving compared with the cloud paradigm.\nNonetheless, the performance of on-device LLMs is intrinsically constrained by\nresource limitations on edge devices. Sitting between cloud and on-device AI,\nmobile edge intelligence (MEI) presents a viable solution by provisioning AI\ncapabilities at the edge of mobile networks, enabling end users to offload\nheavy AI computation to capable edge servers nearby. This article provides a\ncontemporary survey on harnessing MEI for LLMs. We begin by illustrating\nseveral killer applications to demonstrate the urgent need for deploying LLMs\nat the network edge. Next, we present the preliminaries of LLMs and MEI,\nfollowed by resource-efficient LLM techniques. We then present an architectural\noverview of MEI for LLMs (MEI4LLM), outlining its core components and how it\nsupports the deployment of LLMs. Subsequently, we delve into various aspects of\nMEI4LLM, extensively covering edge LLM caching and delivery, edge LLM training,\nand edge LLM inference. Finally, we identify future research opportunities. We\nhope this article inspires researchers in the field to leverage mobile edge\ncomputing to facilitate LLM deployment, thereby unleashing the potential of\nLLMs across various privacy- and delay-sensitive applications."
                },
                "authors": [
                    {
                        "name": "Guanqiao Qu"
                    },
                    {
                        "name": "Qiyuan Chen"
                    },
                    {
                        "name": "Wei Wei"
                    },
                    {
                        "name": "Zheng Lin"
                    },
                    {
                        "name": "Xianhao Chen"
                    },
                    {
                        "name": "Kaibin Huang"
                    }
                ],
                "author_detail": {
                    "name": "Kaibin Huang"
                },
                "author": "Kaibin Huang",
                "arxiv_comment": "42 pages, 17 figures. This paper has been accepted by IEEE\n  Communications Surveys & Tutorials",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.18921v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.18921v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15908v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15908v2",
                "updated": "2025-03-19T10:19:30Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    10,
                    19,
                    30,
                    2,
                    78,
                    0
                ],
                "published": "2024-10-21T11:29:49Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    11,
                    29,
                    49,
                    0,
                    295,
                    0
                ],
                "title": "Formalising CXL Cache Coherence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Formalising CXL Cache Coherence"
                },
                "summary": "We report our experience formally modelling and verifying CXL.cache, the\ninter-device cache coherence protocol of the Compute Express Link standard. We\nhave used the Isabelle proof assistant to create a formal model for CXL.cache\nbased on the prose English specification. This led to us identifying and\nproposing fixes to several problems we identified as unclear, ambiguous or\ninaccurate, some of which could lead to incoherence if left unfixed. Nearly all\nour issues and proposed fixes have been confirmed and tentatively accepted by\nthe CXL consortium for adoption, save for one which is still under discussion.\nTo validate the faithfulness of our model we performed scenario verification of\nessential restrictions such as \"Snoop-pushes-GO\", and produced a fully\nmechanised proof of a coherence property of the model. The considerable size of\nthis proof, comprising tens of thousands of lemmas, prompted us to develop new\nproof automation tools, which we have made available for other Isabelle users\nworking with similarly cumbersome proofs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We report our experience formally modelling and verifying CXL.cache, the\ninter-device cache coherence protocol of the Compute Express Link standard. We\nhave used the Isabelle proof assistant to create a formal model for CXL.cache\nbased on the prose English specification. This led to us identifying and\nproposing fixes to several problems we identified as unclear, ambiguous or\ninaccurate, some of which could lead to incoherence if left unfixed. Nearly all\nour issues and proposed fixes have been confirmed and tentatively accepted by\nthe CXL consortium for adoption, save for one which is still under discussion.\nTo validate the faithfulness of our model we performed scenario verification of\nessential restrictions such as \"Snoop-pushes-GO\", and produced a fully\nmechanised proof of a coherence property of the model. The considerable size of\nthis proof, comprising tens of thousands of lemmas, prompted us to develop new\nproof automation tools, which we have made available for other Isabelle users\nworking with similarly cumbersome proofs."
                },
                "authors": [
                    {
                        "name": "Chengsong Tan"
                    },
                    {
                        "name": "Alastair F. Donaldson"
                    },
                    {
                        "name": "John Wickerson"
                    }
                ],
                "author_detail": {
                    "name": "John Wickerson"
                },
                "author": "John Wickerson",
                "arxiv_doi": "10.1145/3676641.3715999",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3676641.3715999",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2410.15908v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15908v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "14 pages",
                "arxiv_journal_ref": "Proceedings of the 30th ACM International Conference on\n  Architectural Support for Programming Languages and Operating Systems, ASPLOS\n  2025, Rotterdam, Netherlands",
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68 (Primary)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.1; F.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14881v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14881v1",
                "updated": "2025-03-19T04:18:57Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    4,
                    18,
                    57,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-19T04:18:57Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    4,
                    18,
                    57,
                    2,
                    78,
                    0
                ],
                "title": "Exploring the Limits of KV Cache Compression in Visual Autoregressive\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the Limits of KV Cache Compression in Visual Autoregressive\n  Transformers"
                },
                "summary": "A fundamental challenge in Visual Autoregressive models is the substantial\nmemory overhead required during inference to store previously generated\nrepresentations. Despite various attempts to mitigate this issue through\ncompression techniques, prior works have not explicitly formalized the problem\nof KV-cache compression in this context. In this work, we take the first step\nin formally defining the KV-cache compression problem for Visual Autoregressive\ntransformers. We then establish a fundamental negative result, proving that any\nmechanism for sequential visual token generation under attention-based\narchitectures must use at least $\\Omega(n^2 d)$ memory, when $d = \\Omega(\\log\nn)$, where $n$ is the number of tokens generated and $d$ is the embedding\ndimensionality. This result demonstrates that achieving truly sub-quadratic\nmemory usage is impossible without additional structural constraints. Our proof\nis constructed via a reduction from a computational lower bound problem,\nleveraging randomized embedding techniques inspired by dimensionality reduction\nprinciples. Finally, we discuss how sparsity priors on visual representations\ncan influence memory efficiency, presenting both impossibility results and\npotential directions for mitigating memory overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A fundamental challenge in Visual Autoregressive models is the substantial\nmemory overhead required during inference to store previously generated\nrepresentations. Despite various attempts to mitigate this issue through\ncompression techniques, prior works have not explicitly formalized the problem\nof KV-cache compression in this context. In this work, we take the first step\nin formally defining the KV-cache compression problem for Visual Autoregressive\ntransformers. We then establish a fundamental negative result, proving that any\nmechanism for sequential visual token generation under attention-based\narchitectures must use at least $\\Omega(n^2 d)$ memory, when $d = \\Omega(\\log\nn)$, where $n$ is the number of tokens generated and $d$ is the embedding\ndimensionality. This result demonstrates that achieving truly sub-quadratic\nmemory usage is impossible without additional structural constraints. Our proof\nis constructed via a reduction from a computational lower bound problem,\nleveraging randomized embedding techniques inspired by dimensionality reduction\nprinciples. Finally, we discuss how sparsity priors on visual representations\ncan influence memory efficiency, presenting both impossibility results and\npotential directions for mitigating memory overhead."
                },
                "authors": [
                    {
                        "name": "Bo Chen"
                    },
                    {
                        "name": "Xiaoyu Li"
                    },
                    {
                        "name": "Yekun Ke"
                    },
                    {
                        "name": "Yingyu Liang"
                    },
                    {
                        "name": "Zhenmei Shi"
                    },
                    {
                        "name": "Zhao Song"
                    }
                ],
                "author_detail": {
                    "name": "Zhao Song"
                },
                "author": "Zhao Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14881v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14881v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14805v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14805v1",
                "updated": "2025-03-19T00:30:43Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    0,
                    30,
                    43,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-19T00:30:43Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    0,
                    30,
                    43,
                    2,
                    78,
                    0
                ],
                "title": "Degradation of 2.4-kV $Ga_{2}O_{3}$ Schottky Barrier Diode at High\n  Temperatures up to 500 C",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Degradation of 2.4-kV $Ga_{2}O_{3}$ Schottky Barrier Diode at High\n  Temperatures up to 500 C"
                },
                "summary": "Ga2O3 Schottky barrier diodes featuring a field plate and a composite\nSiO2/SiNx dielectric layer beneath the field plate were fabricated, achieving a\nbreakdown voltage of 2.4 kV at room temperature. Electrical performance and\ndegradation were analyzed via I-V and C-V measurements from 25 {\\deg}C to 500\n{\\deg}C, revealing temperature-dependent transport, interface stability, and\ndevice stability. Upon returning to room temperature, the diodes exhibited\nnearly unchanged forward characteristics, while the breakdown voltage declined\nsignificantly from 2.4 kV to 700 V. This behavior indicates a\ntemperature-induced reduction in the barrier height. Detailed analysis revealed\nthat variable range hopping (VRH) dominated the leakage mechanism at moderate\ntemperatures, while thermal emission (TE) became increasingly significant at\ntemperatures exceeding 400 {\\deg}C.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ga2O3 Schottky barrier diodes featuring a field plate and a composite\nSiO2/SiNx dielectric layer beneath the field plate were fabricated, achieving a\nbreakdown voltage of 2.4 kV at room temperature. Electrical performance and\ndegradation were analyzed via I-V and C-V measurements from 25 {\\deg}C to 500\n{\\deg}C, revealing temperature-dependent transport, interface stability, and\ndevice stability. Upon returning to room temperature, the diodes exhibited\nnearly unchanged forward characteristics, while the breakdown voltage declined\nsignificantly from 2.4 kV to 700 V. This behavior indicates a\ntemperature-induced reduction in the barrier height. Detailed analysis revealed\nthat variable range hopping (VRH) dominated the leakage mechanism at moderate\ntemperatures, while thermal emission (TE) became increasingly significant at\ntemperatures exceeding 400 {\\deg}C."
                },
                "authors": [
                    {
                        "name": "Hunter Ellis"
                    },
                    {
                        "name": "Wei Jia"
                    },
                    {
                        "name": "Imteaz Rahaman"
                    },
                    {
                        "name": "Apostoli Hillas"
                    },
                    {
                        "name": "Botong Li"
                    },
                    {
                        "name": "Michael A. Scarpulla"
                    },
                    {
                        "name": "Berardi Sensale Rodriguez"
                    },
                    {
                        "name": "Kai Fu"
                    }
                ],
                "author_detail": {
                    "name": "Kai Fu"
                },
                "author": "Kai Fu",
                "arxiv_comment": "7 Pages, 9 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14805v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14805v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14708v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14708v1",
                "updated": "2025-03-18T20:16:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    20,
                    16,
                    50,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-18T20:16:50Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    20,
                    16,
                    50,
                    1,
                    77,
                    0
                ],
                "title": "NeCTAr: A Heterogeneous RISC-V SoC for Language Model Inference in Intel\n  16",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NeCTAr: A Heterogeneous RISC-V SoC for Language Model Inference in Intel\n  16"
                },
                "summary": "This paper introduces NeCTAr (Near-Cache Transformer Accelerator), a 16nm\nheterogeneous multicore RISC-V SoC for sparse and dense machine learning\nkernels with both near-core and near-memory accelerators. A prototype chip runs\nat 400MHz at 0.85V and performs matrix-vector multiplications with 109 GOPs/W.\nThe effectiveness of the design is demonstrated by running inference on a\nsparse language model, ReLU-Llama.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces NeCTAr (Near-Cache Transformer Accelerator), a 16nm\nheterogeneous multicore RISC-V SoC for sparse and dense machine learning\nkernels with both near-core and near-memory accelerators. A prototype chip runs\nat 400MHz at 0.85V and performs matrix-vector multiplications with 109 GOPs/W.\nThe effectiveness of the design is demonstrated by running inference on a\nsparse language model, ReLU-Llama."
                },
                "authors": [
                    {
                        "name": "Viansa Schmulbach"
                    },
                    {
                        "name": "Jason Kim"
                    },
                    {
                        "name": "Ethan Gao"
                    },
                    {
                        "name": "Lucy Revina"
                    },
                    {
                        "name": "Nikhil Jha"
                    },
                    {
                        "name": "Ethan Wu"
                    },
                    {
                        "name": "Borivoje Nikolic"
                    }
                ],
                "author_detail": {
                    "name": "Borivoje Nikolic"
                },
                "author": "Borivoje Nikolic",
                "arxiv_doi": "10.1109/HCS61935.2024.10665203",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/HCS61935.2024.10665203",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.14708v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14708v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14647v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14647v1",
                "updated": "2025-03-18T18:52:03Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    18,
                    52,
                    3,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-18T18:52:03Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    18,
                    52,
                    3,
                    1,
                    77,
                    0
                ],
                "title": "Towards More Economical Context-Augmented LLM Generation by Reusing\n  Stored KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards More Economical Context-Augmented LLM Generation by Reusing\n  Stored KV Cache"
                },
                "summary": "Across large language model (LLM) applications, we observe an emerging trend\nfor reusing KV caches to save the prefill delays of processing repeated input\ntexts in different LLM inputs. This has led to a broad design space, including\ncolocating stored KV caches with (or close to) GPUs to various KV cache\ncompression. However, a key question remains unanswered: can these delay\nreductions also be economically favorable? Specifically, we ask whether a\ndeveloper can use public cloud services to store precomputed KV caches and\nreuse them to save delay without incurring more costs in terms of compute,\nstorage, and network. To answer this question, we propose an validated\nanalytical model for the cloud cost (in compute, storage, and network) of\nstoring and reusing KV caches based on various workload parameters, such as\nreuse frequency, generated text lengths, model sizes, etc. Preliminary results\nshow that KV cache reusing is able to save both delay and cloud cost across a\nrange of workloads with long context. And we call more efforts on building more\neconomical context augmented LLM by KV cache reusing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Across large language model (LLM) applications, we observe an emerging trend\nfor reusing KV caches to save the prefill delays of processing repeated input\ntexts in different LLM inputs. This has led to a broad design space, including\ncolocating stored KV caches with (or close to) GPUs to various KV cache\ncompression. However, a key question remains unanswered: can these delay\nreductions also be economically favorable? Specifically, we ask whether a\ndeveloper can use public cloud services to store precomputed KV caches and\nreuse them to save delay without incurring more costs in terms of compute,\nstorage, and network. To answer this question, we propose an validated\nanalytical model for the cloud cost (in compute, storage, and network) of\nstoring and reusing KV caches based on various workload parameters, such as\nreuse frequency, generated text lengths, model sizes, etc. Preliminary results\nshow that KV cache reusing is able to save both delay and cloud cost across a\nrange of workloads with long context. And we call more efforts on building more\neconomical context augmented LLM by KV cache reusing."
                },
                "authors": [
                    {
                        "name": "Hanchen Li"
                    },
                    {
                        "name": "Yuhan Liu"
                    },
                    {
                        "name": "Yihua Cheng"
                    },
                    {
                        "name": "Kuntai Du"
                    },
                    {
                        "name": "Junchen Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Junchen Jiang"
                },
                "author": "Junchen Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14647v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14647v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08640v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08640v2",
                "updated": "2025-03-18T17:13:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    17,
                    13,
                    42,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-11T17:30:58Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    17,
                    30,
                    58,
                    1,
                    70,
                    0
                ],
                "title": "Efficient Many-Shot In-Context Learning with Dynamic Block-Sparse\n  Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Many-Shot In-Context Learning with Dynamic Block-Sparse\n  Attention"
                },
                "summary": "Many-shot in-context learning has recently shown promise as an alternative to\nfinetuning, with the major advantage that the same model can be served for\nmultiple tasks. However, this shifts the computational burden from\ntraining-time to inference-time, making deployment of many-shot ICL challenging\nto justify in-practice. This cost is further increased if a custom\ndemonstration set is retrieved for each inference example. We present Dynamic\nBlock-Sparse Attention, a training-free framework for retrieval-based many-shot\nin-context learning. By combining carefully designed block-sparse attention and\nretrieval of cached groups of demonstrations, we achieve comparable per-example\nlatency to finetuning while maintaining on average >95% of the best method's\naccuracy across strong ICL and finetuning baselines. We hope that this will\nfurther enable the deployment of many-shot ICL at scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many-shot in-context learning has recently shown promise as an alternative to\nfinetuning, with the major advantage that the same model can be served for\nmultiple tasks. However, this shifts the computational burden from\ntraining-time to inference-time, making deployment of many-shot ICL challenging\nto justify in-practice. This cost is further increased if a custom\ndemonstration set is retrieved for each inference example. We present Dynamic\nBlock-Sparse Attention, a training-free framework for retrieval-based many-shot\nin-context learning. By combining carefully designed block-sparse attention and\nretrieval of cached groups of demonstrations, we achieve comparable per-example\nlatency to finetuning while maintaining on average >95% of the best method's\naccuracy across strong ICL and finetuning baselines. We hope that this will\nfurther enable the deployment of many-shot ICL at scale."
                },
                "authors": [
                    {
                        "name": "Emily Xiao"
                    },
                    {
                        "name": "Chin-Jou Li"
                    },
                    {
                        "name": "Yilin Zhang"
                    },
                    {
                        "name": "Graham Neubig"
                    },
                    {
                        "name": "Amanda Bertsch"
                    }
                ],
                "author_detail": {
                    "name": "Amanda Bertsch"
                },
                "author": "Amanda Bertsch",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08640v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08640v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09573v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09573v2",
                "updated": "2025-03-18T15:58:18Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    15,
                    58,
                    18,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-12T17:43:40Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    17,
                    43,
                    40,
                    2,
                    71,
                    0
                ],
                "title": "Block Diffusion: Interpolating Between Autoregressive and Diffusion\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Block Diffusion: Interpolating Between Autoregressive and Diffusion\n  Language Models"
                },
                "summary": "Diffusion language models offer unique benefits over autoregressive models\ndue to their potential for parallelized generation and controllability, yet\nthey lag in likelihood modeling and are limited to fixed-length generation. In\nthis work, we introduce a class of block diffusion language models that\ninterpolate between discrete denoising diffusion and autoregressive models.\nBlock diffusion overcomes key limitations of both approaches by supporting\nflexible-length generation and improving inference efficiency with KV caching\nand parallel token sampling. We propose a recipe for building effective block\ndiffusion models that includes an efficient training algorithm, estimators of\ngradient variance, and data-driven noise schedules to minimize the variance.\nBlock diffusion sets a new state-of-the-art performance among diffusion models\non language modeling benchmarks and enables generation of arbitrary-length\nsequences. We provide the code, along with the model weights and blog post on\nthe project page: https://m-arriola.com/bd3lms/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion language models offer unique benefits over autoregressive models\ndue to their potential for parallelized generation and controllability, yet\nthey lag in likelihood modeling and are limited to fixed-length generation. In\nthis work, we introduce a class of block diffusion language models that\ninterpolate between discrete denoising diffusion and autoregressive models.\nBlock diffusion overcomes key limitations of both approaches by supporting\nflexible-length generation and improving inference efficiency with KV caching\nand parallel token sampling. We propose a recipe for building effective block\ndiffusion models that includes an efficient training algorithm, estimators of\ngradient variance, and data-driven noise schedules to minimize the variance.\nBlock diffusion sets a new state-of-the-art performance among diffusion models\non language modeling benchmarks and enables generation of arbitrary-length\nsequences. We provide the code, along with the model weights and blog post on\nthe project page: https://m-arriola.com/bd3lms/"
                },
                "authors": [
                    {
                        "name": "Marianne Arriola"
                    },
                    {
                        "name": "Aaron Gokaslan"
                    },
                    {
                        "name": "Justin T Chiu"
                    },
                    {
                        "name": "Zhihan Yang"
                    },
                    {
                        "name": "Zhixuan Qi"
                    },
                    {
                        "name": "Jiaqi Han"
                    },
                    {
                        "name": "Subham Sekhar Sahoo"
                    },
                    {
                        "name": "Volodymyr Kuleshov"
                    }
                ],
                "author_detail": {
                    "name": "Volodymyr Kuleshov"
                },
                "author": "Volodymyr Kuleshov",
                "arxiv_comment": "ICLR 2025 Oral. We provide the code at\n  https://github.com/kuleshov-group/bd3lms",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09573v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09573v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.18753v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.18753v2",
                "updated": "2025-03-18T09:43:33Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    9,
                    43,
                    33,
                    1,
                    77,
                    0
                ],
                "published": "2024-07-26T14:08:53Z",
                "published_parsed": [
                    2024,
                    7,
                    26,
                    14,
                    8,
                    53,
                    4,
                    208,
                    0
                ],
                "title": "Suffixient Arrays: a New Efficient Suffix Array Compression Technique",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Suffixient Arrays: a New Efficient Suffix Array Compression Technique"
                },
                "summary": "The Suffix Array is a classic text index enabling on-line pattern matching\nqueries via simple binary search. The main drawback of the Suffix Array is that\nit takes linear space in the text's length, even if the text itself is\nextremely compressible. Several works in the literature showed that the Suffix\nArray can be compressed, but they all rely on complex succinct data structures\nwhich in practice tend to exhibit poor cache locality and thus significantly\nslow down queries. In this paper, we propose a new simple and very efficient\nsolution to this problem by presenting the \\emph{Suffixient Array}: a tiny\nsubset of the Suffix Array \\emph{sufficient} to locate on-line one pattern\noccurrence (in general, all its Maximal Exact Matches) via binary search,\nprovided that random access to the text is available. We prove that: (i) the\nSuffixient Array length $\\chi$ is a strong repetitiveness measure, (ii) unlike\nmost existing repetition-aware indexes such as the $r$-index, our new index is\nefficient in the I/O model, and (iii) Suffixient Arrays can be computed in\nlinear time and compressed working space. We show experimentally that, when\nusing well-established compressed random access data structures on repetitive\ncollections, the Suffixient Array $\\SuA$ is \\emph{simultaneously} (i) faster\nand orders of magnitude smaller than the Suffix Array $\\SA$ and (ii) smaller\nand \\emph{one to two orders of magnitude faster} than the $r$-index. With an\naverage pattern matching query time as low as 3.5 ns per character, our new\nindex gets very close to the ultimate lower bound: the RAM throughput of our\nworkstation (1.18 ns per character).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Suffix Array is a classic text index enabling on-line pattern matching\nqueries via simple binary search. The main drawback of the Suffix Array is that\nit takes linear space in the text's length, even if the text itself is\nextremely compressible. Several works in the literature showed that the Suffix\nArray can be compressed, but they all rely on complex succinct data structures\nwhich in practice tend to exhibit poor cache locality and thus significantly\nslow down queries. In this paper, we propose a new simple and very efficient\nsolution to this problem by presenting the \\emph{Suffixient Array}: a tiny\nsubset of the Suffix Array \\emph{sufficient} to locate on-line one pattern\noccurrence (in general, all its Maximal Exact Matches) via binary search,\nprovided that random access to the text is available. We prove that: (i) the\nSuffixient Array length $\\chi$ is a strong repetitiveness measure, (ii) unlike\nmost existing repetition-aware indexes such as the $r$-index, our new index is\nefficient in the I/O model, and (iii) Suffixient Arrays can be computed in\nlinear time and compressed working space. We show experimentally that, when\nusing well-established compressed random access data structures on repetitive\ncollections, the Suffixient Array $\\SuA$ is \\emph{simultaneously} (i) faster\nand orders of magnitude smaller than the Suffix Array $\\SA$ and (ii) smaller\nand \\emph{one to two orders of magnitude faster} than the $r$-index. With an\naverage pattern matching query time as low as 3.5 ns per character, our new\nindex gets very close to the ultimate lower bound: the RAM throughput of our\nworkstation (1.18 ns per character)."
                },
                "authors": [
                    {
                        "name": "Davide Cenzato"
                    },
                    {
                        "name": "Lore Depuydt"
                    },
                    {
                        "name": "Travis Gagie"
                    },
                    {
                        "name": "Sung-Hwan Kim"
                    },
                    {
                        "name": "Giovanni Manzini"
                    },
                    {
                        "name": "Francisco Olivares"
                    },
                    {
                        "name": "Nicola Prezza"
                    }
                ],
                "author_detail": {
                    "name": "Nicola Prezza"
                },
                "author": "Nicola Prezza",
                "arxiv_comment": "40 pages, 7 figure, 1 table and 7 pseudocodes",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.18753v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.18753v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13145v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13145v2",
                "updated": "2025-03-18T07:02:33Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    7,
                    2,
                    33,
                    1,
                    77,
                    0
                ],
                "published": "2025-02-18T18:59:57Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    18,
                    59,
                    57,
                    1,
                    49,
                    0
                ],
                "title": "Multimodal Mamba: Decoder-only Multimodal State Space Model via\n  Quadratic to Linear Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Mamba: Decoder-only Multimodal State Space Model via\n  Quadratic to Linear Distillation"
                },
                "summary": "Recent Multimodal Large Language Models (MLLMs) have achieved remarkable\nperformance but face deployment challenges due to their quadratic computational\ncomplexity, growing Key-Value cache requirements, and reliance on separate\nvision encoders. We propose mmMamba, a framework for developing\nlinear-complexity native multimodal state space models through progressive\ndistillation from existing MLLMs using moderate academic computational\nresources. Our approach enables the direct conversion of trained decoder-only\nMLLMs to linear-complexity architectures without requiring pre-trained\nRNN-based LLM or vision encoders. We propose an seeding strategy to carve Mamba\nfrom trained Transformer and a three-stage distillation recipe, which can\neffectively transfer the knowledge from Transformer to Mamba while preserving\nmultimodal capabilities. Our method also supports flexible hybrid architectures\nthat combine Transformer and Mamba layers for customizable\nefficiency-performance trade-offs. Distilled from the Transformer-based\ndecoder-only HoVLE, mmMamba-linear achieves competitive performance against\nexisting linear and quadratic-complexity VLMs, while mmMamba-hybrid further\nimproves performance significantly, approaching HoVLE's capabilities. At 103K\ntokens, mmMamba-linear demonstrates 20.6$\\times$ speedup and 75.8% GPU memory\nreduction compared to HoVLE, while mmMamba-hybrid achieves 13.5$\\times$ speedup\nand 60.2% memory savings. Code and models are released at\nhttps://github.com/hustvl/mmMamba",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent Multimodal Large Language Models (MLLMs) have achieved remarkable\nperformance but face deployment challenges due to their quadratic computational\ncomplexity, growing Key-Value cache requirements, and reliance on separate\nvision encoders. We propose mmMamba, a framework for developing\nlinear-complexity native multimodal state space models through progressive\ndistillation from existing MLLMs using moderate academic computational\nresources. Our approach enables the direct conversion of trained decoder-only\nMLLMs to linear-complexity architectures without requiring pre-trained\nRNN-based LLM or vision encoders. We propose an seeding strategy to carve Mamba\nfrom trained Transformer and a three-stage distillation recipe, which can\neffectively transfer the knowledge from Transformer to Mamba while preserving\nmultimodal capabilities. Our method also supports flexible hybrid architectures\nthat combine Transformer and Mamba layers for customizable\nefficiency-performance trade-offs. Distilled from the Transformer-based\ndecoder-only HoVLE, mmMamba-linear achieves competitive performance against\nexisting linear and quadratic-complexity VLMs, while mmMamba-hybrid further\nimproves performance significantly, approaching HoVLE's capabilities. At 103K\ntokens, mmMamba-linear demonstrates 20.6$\\times$ speedup and 75.8% GPU memory\nreduction compared to HoVLE, while mmMamba-hybrid achieves 13.5$\\times$ speedup\nand 60.2% memory savings. Code and models are released at\nhttps://github.com/hustvl/mmMamba"
                },
                "authors": [
                    {
                        "name": "Bencheng Liao"
                    },
                    {
                        "name": "Hongyuan Tao"
                    },
                    {
                        "name": "Qian Zhang"
                    },
                    {
                        "name": "Tianheng Cheng"
                    },
                    {
                        "name": "Yingyue Li"
                    },
                    {
                        "name": "Haoran Yin"
                    },
                    {
                        "name": "Wenyu Liu"
                    },
                    {
                        "name": "Xinggang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xinggang Wang"
                },
                "author": "Xinggang Wang",
                "arxiv_comment": "Code and model are available at https://github.com/hustvl/mmMamba",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13145v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13145v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19108v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19108v2",
                "updated": "2025-03-18T04:49:23Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    4,
                    49,
                    23,
                    1,
                    77,
                    0
                ],
                "published": "2024-11-28T12:50:05Z",
                "published_parsed": [
                    2024,
                    11,
                    28,
                    12,
                    50,
                    5,
                    3,
                    333,
                    0
                ],
                "title": "Timestep Embedding Tells: It's Time to Cache for Video Diffusion Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Timestep Embedding Tells: It's Time to Cache for Video Diffusion Model"
                },
                "summary": "As a fundamental backbone for video generation, diffusion models are\nchallenged by low inference speed due to the sequential nature of denoising.\nPrevious methods speed up the models by caching and reusing model outputs at\nuniformly selected timesteps. However, such a strategy neglects the fact that\ndifferences among model outputs are not uniform across timesteps, which hinders\nselecting the appropriate model outputs to cache, leading to a poor balance\nbetween inference efficiency and visual quality. In this study, we introduce\nTimestep Embedding Aware Cache (TeaCache), a training-free caching approach\nthat estimates and leverages the fluctuating differences among model outputs\nacross timesteps. Rather than directly using the time-consuming model outputs,\nTeaCache focuses on model inputs, which have a strong correlation with the\nmodeloutputs while incurring negligible computational cost. TeaCache first\nmodulates the noisy inputs using the timestep embeddings to ensure their\ndifferences better approximating those of model outputs. TeaCache then\nintroduces a rescaling strategy to refine the estimated differences and\nutilizes them to indicate output caching. Experiments show that TeaCache\nachieves up to 4.41x acceleration over Open-Sora-Plan with negligible (-0.07%\nVbench score) degradation of visual quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As a fundamental backbone for video generation, diffusion models are\nchallenged by low inference speed due to the sequential nature of denoising.\nPrevious methods speed up the models by caching and reusing model outputs at\nuniformly selected timesteps. However, such a strategy neglects the fact that\ndifferences among model outputs are not uniform across timesteps, which hinders\nselecting the appropriate model outputs to cache, leading to a poor balance\nbetween inference efficiency and visual quality. In this study, we introduce\nTimestep Embedding Aware Cache (TeaCache), a training-free caching approach\nthat estimates and leverages the fluctuating differences among model outputs\nacross timesteps. Rather than directly using the time-consuming model outputs,\nTeaCache focuses on model inputs, which have a strong correlation with the\nmodeloutputs while incurring negligible computational cost. TeaCache first\nmodulates the noisy inputs using the timestep embeddings to ensure their\ndifferences better approximating those of model outputs. TeaCache then\nintroduces a rescaling strategy to refine the estimated differences and\nutilizes them to indicate output caching. Experiments show that TeaCache\nachieves up to 4.41x acceleration over Open-Sora-Plan with negligible (-0.07%\nVbench score) degradation of visual quality."
                },
                "authors": [
                    {
                        "name": "Feng Liu"
                    },
                    {
                        "name": "Shiwei Zhang"
                    },
                    {
                        "name": "Xiaofeng Wang"
                    },
                    {
                        "name": "Yujie Wei"
                    },
                    {
                        "name": "Haonan Qiu"
                    },
                    {
                        "name": "Yuzhong Zhao"
                    },
                    {
                        "name": "Yingya Zhang"
                    },
                    {
                        "name": "Qixiang Ye"
                    },
                    {
                        "name": "Fang Wan"
                    }
                ],
                "author_detail": {
                    "name": "Fang Wan"
                },
                "author": "Fang Wan",
                "arxiv_comment": "Accepted in CVPR 2025. Project: https://liewfeng.github.io/TeaCache",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19108v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19108v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.10511v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.10511v3",
                "updated": "2025-03-18T01:58:36Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    1,
                    58,
                    36,
                    1,
                    77,
                    0
                ],
                "published": "2024-06-15T05:28:55Z",
                "published_parsed": [
                    2024,
                    6,
                    15,
                    5,
                    28,
                    55,
                    5,
                    167,
                    0
                ],
                "title": "Efficient Hardware Accelerator Based on Medium Granularity Dataflow for\n  SpTRSV",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Hardware Accelerator Based on Medium Granularity Dataflow for\n  SpTRSV"
                },
                "summary": "Sparse triangular solve (SpTRSV) is widely used in various domains. Numerous\nstudies have been conducted using CPUs, GPUs, and specific hardware\naccelerators, where dataflows can be categorized into coarse and fine\ngranularity. Coarse dataflows offer good spatial locality but suffer from low\nparallelism, while fine dataflows provide high parallelism but disrupt the\nspatial structure, leading to increased nodes and poor data reuse. This paper\nproposes a novel hardware accelerator for SpTRSV or SpTRSV-like DAGs. The\naccelerator implements a medium granularity dataflow through hardware-software\ncodesign and achieves both excellent spatial locality and high parallelism.\nAdditionally, a partial sum caching mechanism is introduced to reduce the\nblocking frequency of processing elements (PEs), and a reordering algorithm of\nintra-node edges computation is developed to enhance data reuse. Experimental\nresults on 245 benchmarks with node counts reaching up to 85,392 demonstrate\nthat this work achieves average performance improvements of 7.0$\\times$ (up to\n27.8$\\times$) over CPUs and 5.8$\\times$ (up to 98.8$\\times$) over GPUs.\nCompared to the state-of-the-art technique (DPU-v2), this work shows a\n2.5$\\times$ (up to 5.9$\\times$) average performance improvement and 1.7$\\times$\n(up to 4.1$\\times$) average energy efficiency enhancement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse triangular solve (SpTRSV) is widely used in various domains. Numerous\nstudies have been conducted using CPUs, GPUs, and specific hardware\naccelerators, where dataflows can be categorized into coarse and fine\ngranularity. Coarse dataflows offer good spatial locality but suffer from low\nparallelism, while fine dataflows provide high parallelism but disrupt the\nspatial structure, leading to increased nodes and poor data reuse. This paper\nproposes a novel hardware accelerator for SpTRSV or SpTRSV-like DAGs. The\naccelerator implements a medium granularity dataflow through hardware-software\ncodesign and achieves both excellent spatial locality and high parallelism.\nAdditionally, a partial sum caching mechanism is introduced to reduce the\nblocking frequency of processing elements (PEs), and a reordering algorithm of\nintra-node edges computation is developed to enhance data reuse. Experimental\nresults on 245 benchmarks with node counts reaching up to 85,392 demonstrate\nthat this work achieves average performance improvements of 7.0$\\times$ (up to\n27.8$\\times$) over CPUs and 5.8$\\times$ (up to 98.8$\\times$) over GPUs.\nCompared to the state-of-the-art technique (DPU-v2), this work shows a\n2.5$\\times$ (up to 5.9$\\times$) average performance improvement and 1.7$\\times$\n(up to 4.1$\\times$) average energy efficiency enhancement."
                },
                "authors": [
                    {
                        "name": "Qian Chen"
                    },
                    {
                        "name": "Xiaofeng Yang"
                    },
                    {
                        "name": "Shengli Lu"
                    }
                ],
                "author_detail": {
                    "name": "Shengli Lu"
                },
                "author": "Shengli Lu",
                "arxiv_doi": "10.1109/TVLSI.2024.3497166",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TVLSI.2024.3497166",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2406.10511v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.10511v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "IEEE Trans. Very Large Scale Integr. (VLSI) Syst. 33 (2025)\n  807-820",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13773v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13773v2",
                "updated": "2025-03-24T18:50:09Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    18,
                    50,
                    9,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-17T23:38:29Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    23,
                    38,
                    29,
                    0,
                    76,
                    0
                ],
                "title": "Mitigating KV Cache Competition to Enhance User Experience in LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigating KV Cache Competition to Enhance User Experience in LLM\n  Inference"
                },
                "summary": "In Large Language Model (LLM) serving, the KV-cache (KVC) bottleneck causes\nhigh tail Time-to-First-Token (TTFT) and Time-Between-Tokens (TBT), impairing\nuser experience, particularly in time-sensitive applications. However,\nsatisfying both TTFT and TBT service-level objectives (SLOs) is challenging. To\naddress this, we propose a system, named CacheOPT for mitigating KV Cache\ncompetition, based on key insights from our measurements, incorporating novel\ncomponents. First, it estimates a request's output length, bounding the\ndeviation with a high specified probability, adjusted based on the request\narrival rate. Second, it allocates the estimated KVC demand to a request, and\nreuses other requests' allocated KVC to avoid preemptions while reducing\nwaiting time. Third, it proactively allocates KVC before instead of at the time\na request exhausts its allocation and reserves KVC globally to prevent\npreemptions. Fourth, it chooses a request that has long TBT SLO, long job\nremaining time and short preemption time to preempt. Fifth, it selects the\nshortest-latency strategy between swapping and recomputation for preemptions.\nExperiments show that CacheOPT achieves up to 3.29$\\times$ and 2.83$\\times$\nlower tail TBT and tail TTFT, 47\\% and 53\\% higher TTFT and TBT SLO\nattainments, and supports up to 1.58$\\times$ higher request arrival rate than\nthe state-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In Large Language Model (LLM) serving, the KV-cache (KVC) bottleneck causes\nhigh tail Time-to-First-Token (TTFT) and Time-Between-Tokens (TBT), impairing\nuser experience, particularly in time-sensitive applications. However,\nsatisfying both TTFT and TBT service-level objectives (SLOs) is challenging. To\naddress this, we propose a system, named CacheOPT for mitigating KV Cache\ncompetition, based on key insights from our measurements, incorporating novel\ncomponents. First, it estimates a request's output length, bounding the\ndeviation with a high specified probability, adjusted based on the request\narrival rate. Second, it allocates the estimated KVC demand to a request, and\nreuses other requests' allocated KVC to avoid preemptions while reducing\nwaiting time. Third, it proactively allocates KVC before instead of at the time\na request exhausts its allocation and reserves KVC globally to prevent\npreemptions. Fourth, it chooses a request that has long TBT SLO, long job\nremaining time and short preemption time to preempt. Fifth, it selects the\nshortest-latency strategy between swapping and recomputation for preemptions.\nExperiments show that CacheOPT achieves up to 3.29$\\times$ and 2.83$\\times$\nlower tail TBT and tail TTFT, 47\\% and 53\\% higher TTFT and TBT SLO\nattainments, and supports up to 1.58$\\times$ higher request arrival rate than\nthe state-of-the-art methods."
                },
                "authors": [
                    {
                        "name": "Haiying Shen"
                    },
                    {
                        "name": "Tanmoy Sen"
                    },
                    {
                        "name": "Masahiro Tanaka"
                    }
                ],
                "author_detail": {
                    "name": "Masahiro Tanaka"
                },
                "author": "Masahiro Tanaka",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13773v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13773v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13737v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13737v1",
                "updated": "2025-03-17T21:47:43Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    21,
                    47,
                    43,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T21:47:43Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    21,
                    47,
                    43,
                    0,
                    76,
                    0
                ],
                "title": "AccelGen: Heterogeneous SLO-Guaranteed High-Throughput LLM Inference\n  Serving for Diverse Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AccelGen: Heterogeneous SLO-Guaranteed High-Throughput LLM Inference\n  Serving for Diverse Applications"
                },
                "summary": "In this paper, we consider a mixed-prompt scenario for a large language model\n(LLM) inference serving system that supports diverse applications with both\nshort prompts and long prompts and heterogeneous SLOs for iteration time. To\nimprove throughput when handling long prompts, previous research introduces a\nchunking method, but has not addressed heterogeneous SLOs. To address the\nlimitation, we propose AccelGen, a high-throughput LLM inference serving system\nwith heterogeneous SLO guarantees for diverse applications. AccelGen introduces\nfour core components: (1) SLO-guaranteed dynamic chunking, which dynamically\nadjusts chunk sizes to maximize GPU compute utilization while meeting\niteration-level SLOs; (2) Iteration-level SLO-based task prioritization, which\nprioritizes tight-SLO requests and batches requests with similar SLOs; (3)\nMulti-resource-aware batching, which selects queued requests to maximize the\nutilizations of both GPU compute resource and key-value cache (KVC).\nTrace-driven real experiments demonstrate that AccelGen achieves 1.42-11.21X\nhigher throughput, 1.43-13.71X higher goodput, 37-90% higher SLO attainment,\nand 1.61-12.22X lower response latency compared to the state-of-the-art\napproaches. It achieves performance near the Oracle, which optimally maximizes\ngoodput.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we consider a mixed-prompt scenario for a large language model\n(LLM) inference serving system that supports diverse applications with both\nshort prompts and long prompts and heterogeneous SLOs for iteration time. To\nimprove throughput when handling long prompts, previous research introduces a\nchunking method, but has not addressed heterogeneous SLOs. To address the\nlimitation, we propose AccelGen, a high-throughput LLM inference serving system\nwith heterogeneous SLO guarantees for diverse applications. AccelGen introduces\nfour core components: (1) SLO-guaranteed dynamic chunking, which dynamically\nadjusts chunk sizes to maximize GPU compute utilization while meeting\niteration-level SLOs; (2) Iteration-level SLO-based task prioritization, which\nprioritizes tight-SLO requests and batches requests with similar SLOs; (3)\nMulti-resource-aware batching, which selects queued requests to maximize the\nutilizations of both GPU compute resource and key-value cache (KVC).\nTrace-driven real experiments demonstrate that AccelGen achieves 1.42-11.21X\nhigher throughput, 1.43-13.71X higher goodput, 37-90% higher SLO attainment,\nand 1.61-12.22X lower response latency compared to the state-of-the-art\napproaches. It achieves performance near the Oracle, which optimally maximizes\ngoodput."
                },
                "authors": [
                    {
                        "name": "Haiying Shen"
                    },
                    {
                        "name": "Tanmoy Sen"
                    }
                ],
                "author_detail": {
                    "name": "Tanmoy Sen"
                },
                "author": "Tanmoy Sen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13737v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13737v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13723v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13723v1",
                "updated": "2025-03-17T21:11:30Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    21,
                    11,
                    30,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T21:11:30Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    21,
                    11,
                    30,
                    0,
                    76,
                    0
                ],
                "title": "Fast Maximum Likelihood Positioning for a Staggered Layer Scintillation\n  PET Detector",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast Maximum Likelihood Positioning for a Staggered Layer Scintillation\n  PET Detector"
                },
                "summary": "In this study, we propose a fast implementation of a Maximum Likelihood\nPositioning (MLP) algorithm to estimate the energy and identify the active\nscintillator pixel in staggered layer scintillation detectors for PET. The\nstaggered layer design with pixelated scintillators enables the determination\nof the gamma's depth of interaction and facilitates an iteration-free\nformulation of the MLP algorithm. The efficacy of the algorithm optimization\nwas tested on a scintillation detector block designed for an ultra-high field\nBrainPET 7T, comprising three scintillator pixel layers. The three layers\ncontain 24 x 24, 24 x 23 and 23 x 22 scintillator pixels, respectively, with a\npixel pitch of 2 mm in both directions and layer thicknesses of 9, 8 and 7 mm.\nCalibration measurements, in combination with an automated calibration script,\nwere used to obtain the expected counts of scintillation photons required in\nthe MLP algorithm. Using Single-Instruction-Multiple-Data parallelization,\nmulti-threading and optimized cache lines, a maximum processing speed of\napproximately 22.5 million singles per second was achieved on a platform with\nfour Intel Xeon Platinum 8168 CPUs and 60 threads, encompassing all required\nprocessing steps. The automatic calibration failed for 1 to 15 individual\nscintillator pixels in approximately 10 per cent of the 120 scintillation\ndetector blocks, necessitating manual correction. After applying the energy\ncorrection to the positioned single events, an energy resolution of of 12 +/- 2\nper cent FWHM was obtained for the entire scintillation block. This value is\nvery close to the energy resolutions measured for the individual scintillator\npixels, proving that the MLP accurately identifies the scintillating pixel and\nthat the energy correction method effectively compensates for the light\ncollection variations of the SiPM array.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this study, we propose a fast implementation of a Maximum Likelihood\nPositioning (MLP) algorithm to estimate the energy and identify the active\nscintillator pixel in staggered layer scintillation detectors for PET. The\nstaggered layer design with pixelated scintillators enables the determination\nof the gamma's depth of interaction and facilitates an iteration-free\nformulation of the MLP algorithm. The efficacy of the algorithm optimization\nwas tested on a scintillation detector block designed for an ultra-high field\nBrainPET 7T, comprising three scintillator pixel layers. The three layers\ncontain 24 x 24, 24 x 23 and 23 x 22 scintillator pixels, respectively, with a\npixel pitch of 2 mm in both directions and layer thicknesses of 9, 8 and 7 mm.\nCalibration measurements, in combination with an automated calibration script,\nwere used to obtain the expected counts of scintillation photons required in\nthe MLP algorithm. Using Single-Instruction-Multiple-Data parallelization,\nmulti-threading and optimized cache lines, a maximum processing speed of\napproximately 22.5 million singles per second was achieved on a platform with\nfour Intel Xeon Platinum 8168 CPUs and 60 threads, encompassing all required\nprocessing steps. The automatic calibration failed for 1 to 15 individual\nscintillator pixels in approximately 10 per cent of the 120 scintillation\ndetector blocks, necessitating manual correction. After applying the energy\ncorrection to the positioned single events, an energy resolution of of 12 +/- 2\nper cent FWHM was obtained for the entire scintillation block. This value is\nvery close to the energy resolutions measured for the individual scintillator\npixels, proving that the MLP accurately identifies the scintillating pixel and\nthat the energy correction method effectively compensates for the light\ncollection variations of the SiPM array."
                },
                "authors": [
                    {
                        "name": "Christoph W. Lerche"
                    },
                    {
                        "name": "Wenwei Bi"
                    },
                    {
                        "name": "Mirjam Schoeneck"
                    },
                    {
                        "name": "Debora Niekaemper"
                    },
                    {
                        "name": "Qi Liu"
                    },
                    {
                        "name": "Elisabeth Pfaehler"
                    },
                    {
                        "name": "Lutz Tellmann"
                    },
                    {
                        "name": "Juergen J. Scheins"
                    },
                    {
                        "name": "N. Jon Shah"
                    }
                ],
                "author_detail": {
                    "name": "N. Jon Shah"
                },
                "arxiv_affiliation": "Department of Neurology RWTH Aachen University Aachen Germany",
                "author": "N. Jon Shah",
                "arxiv_comment": "20 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13723v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13723v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "92C55 (Primary) 94A08 (Secondary)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13873v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13873v2",
                "updated": "2025-03-17T20:31:46Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    20,
                    31,
                    46,
                    0,
                    76,
                    0
                ],
                "published": "2025-02-19T16:54:58Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    16,
                    54,
                    58,
                    2,
                    50,
                    0
                ],
                "title": "NVR: Vector Runahead on NPUs for Sparse Memory Access",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NVR: Vector Runahead on NPUs for Sparse Memory Access"
                },
                "summary": "Deep Neural Networks are increasingly leveraging sparsity to reduce the\nscaling up of model parameter size. However, reducing wall-clock time through\nsparsity and pruning remains challenging due to irregular memory access\npatterns, leading to frequent cache misses. In this paper, we present NPU\nVector Runahead (NVR), a prefetching mechanism tailored for NPUs to address\ncache miss problems in sparse DNN workloads. Rather than optimising memory\npatterns with high overhead and poor portability, NVR adapts runahead execution\nto the unique architecture of NPUs. NVR provides a general micro-architectural\nsolution for sparse DNN workloads without requiring compiler or algorithmic\nsupport, operating as a decoupled, speculative, lightweight hardware sub-thread\nalongside the NPU, with minimal hardware overhead (under 5%). NVR achieves an\naverage 90% reduction in cache misses compared to SOTA prefetching in\ngeneral-purpose processors, delivering 4x average speedup on sparse workloads\nversus NPUs without prefetching. Moreover, we investigate the advantages of\nincorporating a small cache (16KB) into the NPU combined with NVR. Our\nevaluation shows that expanding this modest cache delivers 5x higher\nperformance benefits than increasing the L2 cache size by the same amount.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Neural Networks are increasingly leveraging sparsity to reduce the\nscaling up of model parameter size. However, reducing wall-clock time through\nsparsity and pruning remains challenging due to irregular memory access\npatterns, leading to frequent cache misses. In this paper, we present NPU\nVector Runahead (NVR), a prefetching mechanism tailored for NPUs to address\ncache miss problems in sparse DNN workloads. Rather than optimising memory\npatterns with high overhead and poor portability, NVR adapts runahead execution\nto the unique architecture of NPUs. NVR provides a general micro-architectural\nsolution for sparse DNN workloads without requiring compiler or algorithmic\nsupport, operating as a decoupled, speculative, lightweight hardware sub-thread\nalongside the NPU, with minimal hardware overhead (under 5%). NVR achieves an\naverage 90% reduction in cache misses compared to SOTA prefetching in\ngeneral-purpose processors, delivering 4x average speedup on sparse workloads\nversus NPUs without prefetching. Moreover, we investigate the advantages of\nincorporating a small cache (16KB) into the NPU combined with NVR. Our\nevaluation shows that expanding this modest cache delivers 5x higher\nperformance benefits than increasing the L2 cache size by the same amount."
                },
                "authors": [
                    {
                        "name": "Hui Wang"
                    },
                    {
                        "name": "Zhengpeng Zhao"
                    },
                    {
                        "name": "Jing Wang"
                    },
                    {
                        "name": "Yushu Du"
                    },
                    {
                        "name": "Yuan Cheng"
                    },
                    {
                        "name": "Bing Guo"
                    },
                    {
                        "name": "He Xiao"
                    },
                    {
                        "name": "Chenhao Ma"
                    },
                    {
                        "name": "Xiaomeng Han"
                    },
                    {
                        "name": "Dean You"
                    },
                    {
                        "name": "Jiapeng Guan"
                    },
                    {
                        "name": "Ran Wei"
                    },
                    {
                        "name": "Dawei Yang"
                    },
                    {
                        "name": "Zhe Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Zhe Jiang"
                },
                "author": "Zhe Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13873v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13873v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13679v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13679v1",
                "updated": "2025-03-17T19:32:26Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    19,
                    32,
                    26,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T19:32:26Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    19,
                    32,
                    26,
                    0,
                    76,
                    0
                ],
                "title": "PrETi: Predicting Execution Time in Early Stage with LLVM and Machine\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PrETi: Predicting Execution Time in Early Stage with LLVM and Machine\n  Learning"
                },
                "summary": "We introduce preti, a novel framework for predicting software execution time\nduring the early stages of development. preti leverages an LLVM-based\nsimulation environment to extract timing-related runtime information, such as\nthe count of executed LLVM IR instructions. This information, combined with\nhistorical execution time data, is utilized to train machine learning models\nfor accurate time prediction. To further enhance prediction accuracy, our\napproach incorporates simulations of cache accesses and branch prediction. The\nevaluations on public benchmarks demonstrate that preti achieves an average\nAbsolute Percentage Error (APE) of 11.98\\%, surpassing state-of-the-art\nmethods. These results underscore the effectiveness and efficiency of preti as\na robust solution for early-stage timing analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce preti, a novel framework for predicting software execution time\nduring the early stages of development. preti leverages an LLVM-based\nsimulation environment to extract timing-related runtime information, such as\nthe count of executed LLVM IR instructions. This information, combined with\nhistorical execution time data, is utilized to train machine learning models\nfor accurate time prediction. To further enhance prediction accuracy, our\napproach incorporates simulations of cache accesses and branch prediction. The\nevaluations on public benchmarks demonstrate that preti achieves an average\nAbsolute Percentage Error (APE) of 11.98\\%, surpassing state-of-the-art\nmethods. These results underscore the effectiveness and efficiency of preti as\na robust solution for early-stage timing analysis."
                },
                "authors": [
                    {
                        "name": "Risheng Xu"
                    },
                    {
                        "name": "Philipp Sieweck"
                    },
                    {
                        "name": "Hermann von Hasseln"
                    },
                    {
                        "name": "Dirk Nowotka"
                    }
                ],
                "author_detail": {
                    "name": "Dirk Nowotka"
                },
                "author": "Dirk Nowotka",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13679v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13679v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16525v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16525v1",
                "updated": "2025-03-17T16:43:35Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    16,
                    43,
                    35,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T16:43:35Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    16,
                    43,
                    35,
                    0,
                    76,
                    0
                ],
                "title": "KVShare: Semantic-Aware Key-Value Cache Sharing for Efficient Large\n  Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVShare: Semantic-Aware Key-Value Cache Sharing for Efficient Large\n  Language Model Inference"
                },
                "summary": "This paper presents KVShare, a multi-user Key-Value (KV) Cache sharing\ntechnology based on semantic similarity, designed to enhance the inference\nefficiency of Large Language Models (LLMs) and Multimodal Large Language Models\n(MLLMs). Addressing the limitations of existing prefix caching (strict text\nprefix matching) and semantic caching (loss of response diversity), KVShare\nachieves fine-grained KV cache reuse through semantic alignment algorithms and\ndifferential editing operations. Experiments on real-world user conversation\ndatasets demonstrate that KVShare improves KV cache hit rates by over 60%,\nwhile maintaining output quality comparable to full computation (no significant\ndegradation in BLEU and Rouge-L metrics). This approach effectively reduces GPU\nresource consumption and is applicable to scenarios with repetitive queries,\nsuch as healthcare and education.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents KVShare, a multi-user Key-Value (KV) Cache sharing\ntechnology based on semantic similarity, designed to enhance the inference\nefficiency of Large Language Models (LLMs) and Multimodal Large Language Models\n(MLLMs). Addressing the limitations of existing prefix caching (strict text\nprefix matching) and semantic caching (loss of response diversity), KVShare\nachieves fine-grained KV cache reuse through semantic alignment algorithms and\ndifferential editing operations. Experiments on real-world user conversation\ndatasets demonstrate that KVShare improves KV cache hit rates by over 60%,\nwhile maintaining output quality comparable to full computation (no significant\ndegradation in BLEU and Rouge-L metrics). This approach effectively reduces GPU\nresource consumption and is applicable to scenarios with repetitive queries,\nsuch as healthcare and education."
                },
                "authors": [
                    {
                        "name": "Huan Yang"
                    },
                    {
                        "name": "Renji Zhang"
                    },
                    {
                        "name": "Deyu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Deyu Zhang"
                },
                "author": "Deyu Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16525v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16525v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13275v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13275v1",
                "updated": "2025-03-17T15:27:02Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    15,
                    27,
                    2,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T15:27:02Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    15,
                    27,
                    2,
                    0,
                    76,
                    0
                ],
                "title": "Knowledge-Aware Iterative Retrieval for Multi-Agent Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge-Aware Iterative Retrieval for Multi-Agent Systems"
                },
                "summary": "We introduce a novel large language model (LLM)-driven agent framework, which\niteratively refines queries and filters contextual evidence by leveraging\ndynamically evolving knowledge. A defining feature of the system is its\ndecoupling of external sources from an internal knowledge cache that is\nprogressively updated to guide both query generation and evidence selection.\nThis design mitigates bias-reinforcement loops and enables dynamic, trackable\nsearch exploration paths, thereby optimizing the trade-off between exploring\ndiverse information and maintaining accuracy through autonomous agent\ndecision-making. Our approach is evaluated on a broad range of open-domain\nquestion answering benchmarks, including multi-step tasks that mirror\nreal-world scenarios where integrating information from multiple sources is\ncritical, especially given the vulnerabilities of LLMs that lack explicit\nreasoning or planning capabilities. The results show that the proposed system\nnot only outperforms single-step baselines regardless of task difficulty but\nalso, compared to conventional iterative retrieval methods, demonstrates\npronounced advantages in complex tasks through precise evidence-based reasoning\nand enhanced efficiency. The proposed system supports both competitive and\ncollaborative sharing of updated context, enabling multi-agent extension. The\nbenefits of multi-agent configurations become especially prominent as task\ndifficulty increases. The number of convergence steps scales with task\ndifficulty, suggesting cost-effective scalability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a novel large language model (LLM)-driven agent framework, which\niteratively refines queries and filters contextual evidence by leveraging\ndynamically evolving knowledge. A defining feature of the system is its\ndecoupling of external sources from an internal knowledge cache that is\nprogressively updated to guide both query generation and evidence selection.\nThis design mitigates bias-reinforcement loops and enables dynamic, trackable\nsearch exploration paths, thereby optimizing the trade-off between exploring\ndiverse information and maintaining accuracy through autonomous agent\ndecision-making. Our approach is evaluated on a broad range of open-domain\nquestion answering benchmarks, including multi-step tasks that mirror\nreal-world scenarios where integrating information from multiple sources is\ncritical, especially given the vulnerabilities of LLMs that lack explicit\nreasoning or planning capabilities. The results show that the proposed system\nnot only outperforms single-step baselines regardless of task difficulty but\nalso, compared to conventional iterative retrieval methods, demonstrates\npronounced advantages in complex tasks through precise evidence-based reasoning\nand enhanced efficiency. The proposed system supports both competitive and\ncollaborative sharing of updated context, enabling multi-agent extension. The\nbenefits of multi-agent configurations become especially prominent as task\ndifficulty increases. The number of convergence steps scales with task\ndifficulty, suggesting cost-effective scalability."
                },
                "authors": [
                    {
                        "name": "Seyoung Song"
                    }
                ],
                "author_detail": {
                    "name": "Seyoung Song"
                },
                "author": "Seyoung Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13275v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13275v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.0; I.2.7; I.2.11; H.3.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.12991v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.12991v1",
                "updated": "2025-03-17T09:46:35Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    9,
                    46,
                    35,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T09:46:35Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    9,
                    46,
                    35,
                    0,
                    76,
                    0
                ],
                "title": "Tuning the CMS Coffea-casa facility for 200 Gbps Challenge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tuning the CMS Coffea-casa facility for 200 Gbps Challenge"
                },
                "summary": "As a part of the IRIS-HEP \"Analysis Grand Challenge\" activities, the\nCoffea-casa AF team executed a \"200 Gbps Challenge\". One of the goals of this\nchallenge was to provide a setup for execution of a test notebook-style\nanalysis on the facility that could process a 200 TB CMS NanoAOD dataset in 20\nminutes.\n  We describe the solutions we deployed at the facility to execute the\nchallenge tasks. The facility was configured to provide 2000+ cores for quick\nturn-around, low-latency analysis. To reach the highest event processing rates\nwe tested different scaling backends, both scaling over HTCondor and Kubernetes\nresources and using Dask and Taskvine schedulers. This configuration also\nallowed us to compare two different services for managing Dask clusters, Dask\nlabextention, and Dask Gateway server, under extreme conditions.\n  A robust set of XCache servers with a redirector were deployed in Kubernetes\nto cache the dataset to minimize wide-area network traffic. The XCache servers\nwere backed with solid-state NVME drives deployed within the Kubernetes cluster\nnodes. All data access was authenticated using scitokens and was transparent to\nthe user. To ensure we could track and measure data throughput precisely, we\nused our existing Prometheus monitoring stack to monitor the XCache pod\nthroughput on the Kubernetes network layer. Using the rate query across all of\nthe 8 XCache pods we were able to view a stacked cumulative graph of the total\nthroughput for each XCache. This monitoring setup allowed us to ensure uniform\ndata rates across all nodes while verifying we had reached the 200 Gbps\nbenchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As a part of the IRIS-HEP \"Analysis Grand Challenge\" activities, the\nCoffea-casa AF team executed a \"200 Gbps Challenge\". One of the goals of this\nchallenge was to provide a setup for execution of a test notebook-style\nanalysis on the facility that could process a 200 TB CMS NanoAOD dataset in 20\nminutes.\n  We describe the solutions we deployed at the facility to execute the\nchallenge tasks. The facility was configured to provide 2000+ cores for quick\nturn-around, low-latency analysis. To reach the highest event processing rates\nwe tested different scaling backends, both scaling over HTCondor and Kubernetes\nresources and using Dask and Taskvine schedulers. This configuration also\nallowed us to compare two different services for managing Dask clusters, Dask\nlabextention, and Dask Gateway server, under extreme conditions.\n  A robust set of XCache servers with a redirector were deployed in Kubernetes\nto cache the dataset to minimize wide-area network traffic. The XCache servers\nwere backed with solid-state NVME drives deployed within the Kubernetes cluster\nnodes. All data access was authenticated using scitokens and was transparent to\nthe user. To ensure we could track and measure data throughput precisely, we\nused our existing Prometheus monitoring stack to monitor the XCache pod\nthroughput on the Kubernetes network layer. Using the rate query across all of\nthe 8 XCache pods we were able to view a stacked cumulative graph of the total\nthroughput for each XCache. This monitoring setup allowed us to ensure uniform\ndata rates across all nodes while verifying we had reached the 200 Gbps\nbenchmark."
                },
                "authors": [
                    {
                        "name": "Sam Albin"
                    },
                    {
                        "name": "Garhan Attebury"
                    },
                    {
                        "name": "Kenneth Bloom"
                    },
                    {
                        "name": "Brian Paul Bockelman"
                    },
                    {
                        "name": "Benjamin Tovar Lopez"
                    },
                    {
                        "name": "Carl Lundstedt"
                    },
                    {
                        "name": "Oksana Shadura"
                    },
                    {
                        "name": "John Thiltges"
                    },
                    {
                        "name": "Derek Weitzel"
                    },
                    {
                        "name": "Andrew Wightman"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Wightman"
                },
                "arxiv_affiliation": "University of Nebraska-Lincoln",
                "author": "Andrew Wightman",
                "arxiv_comment": "Draft submitted to EPJ journal (CHEP 2024 conference proceedings)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.12991v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.12991v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-ex",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.12988v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.12988v1",
                "updated": "2025-03-17T09:44:17Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    9,
                    44,
                    17,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T09:44:17Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    9,
                    44,
                    17,
                    0,
                    76,
                    0
                ],
                "title": "ROMA: a Read-Only-Memory-based Accelerator for QLoRA-based On-Device LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ROMA: a Read-Only-Memory-based Accelerator for QLoRA-based On-Device LLM"
                },
                "summary": "As large language models (LLMs) demonstrate powerful capabilities, deploying\nthem on edge devices has become increasingly crucial, offering advantages in\nprivacy and real-time interaction. QLoRA has emerged as the standard approach\nfor on-device LLMs, leveraging quantized models to reduce memory and\ncomputational costs while utilizing LoRA for task-specific adaptability. In\nthis work, we propose ROMA, a QLoRA accelerator with a hybrid storage\narchitecture that uses ROM for quantized base models and SRAM for LoRA weights\nand KV cache. Our insight is that the quantized base model is stable and\nconverged, making it well-suited for ROM storage. Meanwhile, LoRA modules offer\nthe flexibility to adapt to new data without requiring updates to the base\nmodel. To further reduce the area cost of ROM, we introduce a novel B-ROM\ndesign and integrate it with the compute unit to form a fused cell for\nefficient use of chip resources. ROMA can effectively store both a 4-bit 3B and\na 2-bit 8B LLaMA model entirely on-chip, achieving a notable generation speed\nexceeding 20,000 tokens/s without requiring external memory.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) demonstrate powerful capabilities, deploying\nthem on edge devices has become increasingly crucial, offering advantages in\nprivacy and real-time interaction. QLoRA has emerged as the standard approach\nfor on-device LLMs, leveraging quantized models to reduce memory and\ncomputational costs while utilizing LoRA for task-specific adaptability. In\nthis work, we propose ROMA, a QLoRA accelerator with a hybrid storage\narchitecture that uses ROM for quantized base models and SRAM for LoRA weights\nand KV cache. Our insight is that the quantized base model is stable and\nconverged, making it well-suited for ROM storage. Meanwhile, LoRA modules offer\nthe flexibility to adapt to new data without requiring updates to the base\nmodel. To further reduce the area cost of ROM, we introduce a novel B-ROM\ndesign and integrate it with the compute unit to form a fused cell for\nefficient use of chip resources. ROMA can effectively store both a 4-bit 3B and\na 2-bit 8B LLaMA model entirely on-chip, achieving a notable generation speed\nexceeding 20,000 tokens/s without requiring external memory."
                },
                "authors": [
                    {
                        "name": "Wenqiang Wang"
                    },
                    {
                        "name": "Yijia Zhang"
                    },
                    {
                        "name": "Zikai Zhang"
                    },
                    {
                        "name": "Guanting Huo"
                    },
                    {
                        "name": "Hao Liang"
                    },
                    {
                        "name": "Shijie Cao"
                    },
                    {
                        "name": "Ningyi Xu"
                    }
                ],
                "author_detail": {
                    "name": "Ningyi Xu"
                },
                "author": "Ningyi Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.12988v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.12988v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08407v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08407v2",
                "updated": "2025-03-17T03:30:29Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    3,
                    30,
                    29,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-11T13:10:41Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    13,
                    10,
                    41,
                    1,
                    70,
                    0
                ],
                "title": "WildSeg3D: Segment Any 3D Objects in the Wild from 2D Images",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WildSeg3D: Segment Any 3D Objects in the Wild from 2D Images"
                },
                "summary": "Recent advances in interactive 3D segmentation from 2D images have\ndemonstrated impressive performance. However, current models typically require\nextensive scene-specific training to accurately reconstruct and segment\nobjects, which limits their applicability in real-time scenarios. In this\npaper, we introduce WildSeg3D, an efficient approach that enables the\nsegmentation of arbitrary 3D objects across diverse environments using a\nfeed-forward mechanism. A key challenge of this feed-forward approach lies in\nthe accumulation of 3D alignment errors across multiple 2D views, which can\nlead to inaccurate 3D segmentation results. To address this issue, we propose\nDynamic Global Aligning (DGA), a technique that improves the accuracy of global\nmulti-view alignment by focusing on difficult-to-match 3D points across images,\nusing a dynamic adjustment function. Additionally, for real-time interactive\nsegmentation, we introduce Multi-view Group Mapping (MGM), a method that\nutilizes an object mask cache to integrate multi-view segmentations and respond\nrapidly to user prompts. WildSeg3D demonstrates robust generalization across\narbitrary scenes, thereby eliminating the need for scene-specific training.\nSpecifically, WildSeg3D not only attains the accuracy of state-of-the-art\n(SOTA) methods but also achieves a $40\\times$ speedup compared to existing SOTA\nmodels. Our code will be publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in interactive 3D segmentation from 2D images have\ndemonstrated impressive performance. However, current models typically require\nextensive scene-specific training to accurately reconstruct and segment\nobjects, which limits their applicability in real-time scenarios. In this\npaper, we introduce WildSeg3D, an efficient approach that enables the\nsegmentation of arbitrary 3D objects across diverse environments using a\nfeed-forward mechanism. A key challenge of this feed-forward approach lies in\nthe accumulation of 3D alignment errors across multiple 2D views, which can\nlead to inaccurate 3D segmentation results. To address this issue, we propose\nDynamic Global Aligning (DGA), a technique that improves the accuracy of global\nmulti-view alignment by focusing on difficult-to-match 3D points across images,\nusing a dynamic adjustment function. Additionally, for real-time interactive\nsegmentation, we introduce Multi-view Group Mapping (MGM), a method that\nutilizes an object mask cache to integrate multi-view segmentations and respond\nrapidly to user prompts. WildSeg3D demonstrates robust generalization across\narbitrary scenes, thereby eliminating the need for scene-specific training.\nSpecifically, WildSeg3D not only attains the accuracy of state-of-the-art\n(SOTA) methods but also achieves a $40\\times$ speedup compared to existing SOTA\nmodels. Our code will be publicly available."
                },
                "authors": [
                    {
                        "name": "Yansong Guo"
                    },
                    {
                        "name": "Jie Hu"
                    },
                    {
                        "name": "Yansong Qu"
                    },
                    {
                        "name": "Liujuan Cao"
                    }
                ],
                "author_detail": {
                    "name": "Liujuan Cao"
                },
                "author": "Liujuan Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08407v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08407v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.12491v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.12491v1",
                "updated": "2025-03-16T12:49:44Z",
                "updated_parsed": [
                    2025,
                    3,
                    16,
                    12,
                    49,
                    44,
                    6,
                    75,
                    0
                ],
                "published": "2025-03-16T12:49:44Z",
                "published_parsed": [
                    2025,
                    3,
                    16,
                    12,
                    49,
                    44,
                    6,
                    75,
                    0
                ],
                "title": "CAKE: Cascading and Adaptive KV Cache Eviction with Layer Preferences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAKE: Cascading and Adaptive KV Cache Eviction with Layer Preferences"
                },
                "summary": "Large language models (LLMs) excel at processing long sequences, boosting\ndemand for key-value (KV) caching. While recent efforts to evict KV cache have\nalleviated the inference burden, they often fail to allocate resources\nrationally across layers with different attention patterns. In this paper, we\nintroduce Cascading and Adaptive KV cache Eviction (CAKE), a novel approach\nthat frames KV cache eviction as a \"cake-slicing problem.\" CAKE assesses\nlayer-specific preferences by considering attention dynamics in both spatial\nand temporal dimensions, allocates rational cache size for layers accordingly,\nand manages memory constraints in a cascading manner. This approach enables a\nglobal view of cache allocation, adaptively distributing resources across\ndiverse attention mechanisms while maintaining memory budgets. CAKE also\nemploys a new eviction indicator that considers the shifting importance of\ntokens over time, addressing limitations in existing methods that overlook\ntemporal dynamics. Comprehensive experiments on LongBench and NeedleBench show\nthat CAKE maintains model performance with only 3.2% of the KV cache and\nconsistently outperforms current baselines across various models and memory\nconstraints, particularly in low-memory settings. Additionally, CAKE achieves\nover 10x speedup in decoding latency compared to full cache when processing\ncontexts of 128K tokens with FlashAttention-2. Our code is available at\nhttps://github.com/antgroup/cakekv.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) excel at processing long sequences, boosting\ndemand for key-value (KV) caching. While recent efforts to evict KV cache have\nalleviated the inference burden, they often fail to allocate resources\nrationally across layers with different attention patterns. In this paper, we\nintroduce Cascading and Adaptive KV cache Eviction (CAKE), a novel approach\nthat frames KV cache eviction as a \"cake-slicing problem.\" CAKE assesses\nlayer-specific preferences by considering attention dynamics in both spatial\nand temporal dimensions, allocates rational cache size for layers accordingly,\nand manages memory constraints in a cascading manner. This approach enables a\nglobal view of cache allocation, adaptively distributing resources across\ndiverse attention mechanisms while maintaining memory budgets. CAKE also\nemploys a new eviction indicator that considers the shifting importance of\ntokens over time, addressing limitations in existing methods that overlook\ntemporal dynamics. Comprehensive experiments on LongBench and NeedleBench show\nthat CAKE maintains model performance with only 3.2% of the KV cache and\nconsistently outperforms current baselines across various models and memory\nconstraints, particularly in low-memory settings. Additionally, CAKE achieves\nover 10x speedup in decoding latency compared to full cache when processing\ncontexts of 128K tokens with FlashAttention-2. Our code is available at\nhttps://github.com/antgroup/cakekv."
                },
                "authors": [
                    {
                        "name": "Ziran Qin"
                    },
                    {
                        "name": "Yuchen Cao"
                    },
                    {
                        "name": "Mingbao Lin"
                    },
                    {
                        "name": "Wen Hu"
                    },
                    {
                        "name": "Shixuan Fan"
                    },
                    {
                        "name": "Ke Cheng"
                    },
                    {
                        "name": "Weiyao Lin"
                    },
                    {
                        "name": "Jianguo Li"
                    }
                ],
                "author_detail": {
                    "name": "Jianguo Li"
                },
                "author": "Jianguo Li",
                "arxiv_comment": "Accepted by ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.12491v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.12491v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.12450v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.12450v1",
                "updated": "2025-03-16T10:54:59Z",
                "updated_parsed": [
                    2025,
                    3,
                    16,
                    10,
                    54,
                    59,
                    6,
                    75,
                    0
                ],
                "published": "2025-03-16T10:54:59Z",
                "published_parsed": [
                    2025,
                    3,
                    16,
                    10,
                    54,
                    59,
                    6,
                    75,
                    0
                ],
                "title": "LazyMAR: Accelerating Masked Autoregressive Models via Feature Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LazyMAR: Accelerating Masked Autoregressive Models via Feature Caching"
                },
                "summary": "Masked Autoregressive (MAR) models have emerged as a promising approach in\nimage generation, expected to surpass traditional autoregressive models in\ncomputational efficiency by leveraging the capability of parallel decoding.\nHowever, their dependence on bidirectional self-attention inherently conflicts\nwith conventional KV caching mechanisms, creating unexpected computational\nbottlenecks that undermine their expected efficiency. To address this problem,\nthis paper studies the caching mechanism for MAR by leveraging two types of\nredundancy: Token Redundancy indicates that a large portion of tokens have very\nsimilar representations in the adjacent decoding steps, which allows us to\nfirst cache them in previous steps and then reuse them in the later steps.\nCondition Redundancy indicates that the difference between conditional and\nunconditional output in classifier-free guidance exhibits very similar values\nin adjacent steps. Based on these two redundancies, we propose LazyMAR, which\nintroduces two caching mechanisms to handle them one by one. LazyMAR is\ntraining-free and plug-and-play for all MAR models. Experimental results\ndemonstrate that our method achieves 2.83 times acceleration with almost no\ndrop in generation quality. Our codes will be released in\nhttps://github.com/feihongyan1/LazyMAR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Masked Autoregressive (MAR) models have emerged as a promising approach in\nimage generation, expected to surpass traditional autoregressive models in\ncomputational efficiency by leveraging the capability of parallel decoding.\nHowever, their dependence on bidirectional self-attention inherently conflicts\nwith conventional KV caching mechanisms, creating unexpected computational\nbottlenecks that undermine their expected efficiency. To address this problem,\nthis paper studies the caching mechanism for MAR by leveraging two types of\nredundancy: Token Redundancy indicates that a large portion of tokens have very\nsimilar representations in the adjacent decoding steps, which allows us to\nfirst cache them in previous steps and then reuse them in the later steps.\nCondition Redundancy indicates that the difference between conditional and\nunconditional output in classifier-free guidance exhibits very similar values\nin adjacent steps. Based on these two redundancies, we propose LazyMAR, which\nintroduces two caching mechanisms to handle them one by one. LazyMAR is\ntraining-free and plug-and-play for all MAR models. Experimental results\ndemonstrate that our method achieves 2.83 times acceleration with almost no\ndrop in generation quality. Our codes will be released in\nhttps://github.com/feihongyan1/LazyMAR."
                },
                "authors": [
                    {
                        "name": "Feihong Yan"
                    },
                    {
                        "name": "Qingyan Wei"
                    },
                    {
                        "name": "Jiayi Tang"
                    },
                    {
                        "name": "Jiajun Li"
                    },
                    {
                        "name": "Yulin Wang"
                    },
                    {
                        "name": "Xuming Hu"
                    },
                    {
                        "name": "Huiqi Li"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "10 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.12450v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.12450v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.12150v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.12150v1",
                "updated": "2025-03-15T14:13:23Z",
                "updated_parsed": [
                    2025,
                    3,
                    15,
                    14,
                    13,
                    23,
                    5,
                    74,
                    0
                ],
                "published": "2025-03-15T14:13:23Z",
                "published_parsed": [
                    2025,
                    3,
                    15,
                    14,
                    13,
                    23,
                    5,
                    74,
                    0
                ],
                "title": "Point-Cache: Test-time Dynamic and Hierarchical Cache for Robust and\n  Generalizable Point Cloud Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Point-Cache: Test-time Dynamic and Hierarchical Cache for Robust and\n  Generalizable Point Cloud Analysis"
                },
                "summary": "This paper proposes a general solution to enable point cloud recognition\nmodels to handle distribution shifts at test time. Unlike prior methods, which\nrely heavily on training data-often inaccessible during online inference-and\nare limited to recognizing a fixed set of point cloud classes predefined during\ntraining, we explore a more practical and challenging scenario: adapting the\nmodel solely based on online test data to recognize both previously seen\nclasses and novel, unseen classes at test time. To this end, we develop\nPoint-Cache, a hierarchical cache model that captures essential clues of online\ntest samples, particularly focusing on the global structure of point clouds and\ntheir local-part details. Point-Cache, which serves as a rich 3D knowledge\nbase, is dynamically managed to prioritize the inclusion of high-quality\nsamples. Designed as a plug-and-play module, our method can be flexibly\nintegrated into large multimodal 3D models to support open-vocabulary point\ncloud recognition. Notably, our solution operates with efficiency comparable to\nzero-shot inference, as it is entirely training-free. Point-Cache demonstrates\nsubstantial gains across 8 challenging benchmarks and 4 representative large 3D\nmodels, highlighting its effectiveness. Code is available at\nhttps://github.com/auniquesun/Point-Cache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes a general solution to enable point cloud recognition\nmodels to handle distribution shifts at test time. Unlike prior methods, which\nrely heavily on training data-often inaccessible during online inference-and\nare limited to recognizing a fixed set of point cloud classes predefined during\ntraining, we explore a more practical and challenging scenario: adapting the\nmodel solely based on online test data to recognize both previously seen\nclasses and novel, unseen classes at test time. To this end, we develop\nPoint-Cache, a hierarchical cache model that captures essential clues of online\ntest samples, particularly focusing on the global structure of point clouds and\ntheir local-part details. Point-Cache, which serves as a rich 3D knowledge\nbase, is dynamically managed to prioritize the inclusion of high-quality\nsamples. Designed as a plug-and-play module, our method can be flexibly\nintegrated into large multimodal 3D models to support open-vocabulary point\ncloud recognition. Notably, our solution operates with efficiency comparable to\nzero-shot inference, as it is entirely training-free. Point-Cache demonstrates\nsubstantial gains across 8 challenging benchmarks and 4 representative large 3D\nmodels, highlighting its effectiveness. Code is available at\nhttps://github.com/auniquesun/Point-Cache."
                },
                "authors": [
                    {
                        "name": "Hongyu Sun"
                    },
                    {
                        "name": "Qiuhong Ke"
                    },
                    {
                        "name": "Ming Cheng"
                    },
                    {
                        "name": "Yongcai Wang"
                    },
                    {
                        "name": "Deying Li"
                    },
                    {
                        "name": "Chenhui Gou"
                    },
                    {
                        "name": "Jianfei Cai"
                    }
                ],
                "author_detail": {
                    "name": "Jianfei Cai"
                },
                "author": "Jianfei Cai",
                "arxiv_comment": "Accepted by CVPR 2025; 24 pages, 14 figures, 18 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.12150v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.12150v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11972v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11972v1",
                "updated": "2025-03-15T02:48:27Z",
                "updated_parsed": [
                    2025,
                    3,
                    15,
                    2,
                    48,
                    27,
                    5,
                    74,
                    0
                ],
                "published": "2025-03-15T02:48:27Z",
                "published_parsed": [
                    2025,
                    3,
                    15,
                    2,
                    48,
                    27,
                    5,
                    74,
                    0
                ],
                "title": "MoDM: Efficient Serving for Image Generation via Mixture-of-Diffusion\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoDM: Efficient Serving for Image Generation via Mixture-of-Diffusion\n  Models"
                },
                "summary": "Diffusion-based text-to-image generation models trade latency for quality:\nsmall models are fast but generate lower-quality images, while large models\nproduce better images but are slow.\n  We present MoDM, a novel caching-based serving system for diffusion models\nthat dynamically balances latency and quality through a mixture of diffusion\nmodels. Unlike prior approaches that rely on model-specific internal features,\nMoDM caches final images, allowing seamless retrieval and reuse across multiple\ndiffusion model families.\n  This design enables adaptive serving by dynamically balancing latency and\nimage quality: using smaller models for cache-hit requests to reduce latency\nwhile reserving larger models for cache-miss requests to maintain quality.\nSmall model image quality is preserved using retrieved cached images.\n  We design a global monitor that optimally allocates GPU resources and\nbalances inference workload, ensuring high throughput while meeting\nservice-level objectives under varying request rates. Our evaluations show that\nMoDM significantly reduces average serving time by 2.5x while retaining image\nquality, making it a practical solution for scalable and resource-efficient\nmodel deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based text-to-image generation models trade latency for quality:\nsmall models are fast but generate lower-quality images, while large models\nproduce better images but are slow.\n  We present MoDM, a novel caching-based serving system for diffusion models\nthat dynamically balances latency and quality through a mixture of diffusion\nmodels. Unlike prior approaches that rely on model-specific internal features,\nMoDM caches final images, allowing seamless retrieval and reuse across multiple\ndiffusion model families.\n  This design enables adaptive serving by dynamically balancing latency and\nimage quality: using smaller models for cache-hit requests to reduce latency\nwhile reserving larger models for cache-miss requests to maintain quality.\nSmall model image quality is preserved using retrieved cached images.\n  We design a global monitor that optimally allocates GPU resources and\nbalances inference workload, ensuring high throughput while meeting\nservice-level objectives under varying request rates. Our evaluations show that\nMoDM significantly reduces average serving time by 2.5x while retaining image\nquality, making it a practical solution for scalable and resource-efficient\nmodel deployment."
                },
                "authors": [
                    {
                        "name": "Yuchen Xia"
                    },
                    {
                        "name": "Divyam Sharma"
                    },
                    {
                        "name": "Yichao Yuan"
                    },
                    {
                        "name": "Souvik Kundu"
                    },
                    {
                        "name": "Nishil Talati"
                    }
                ],
                "author_detail": {
                    "name": "Nishil Talati"
                },
                "author": "Nishil Talati",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11972v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11972v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11946v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11946v1",
                "updated": "2025-03-15T01:35:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    15,
                    1,
                    35,
                    53,
                    5,
                    74,
                    0
                ],
                "published": "2025-03-15T01:35:53Z",
                "published_parsed": [
                    2025,
                    3,
                    15,
                    1,
                    35,
                    53,
                    5,
                    74,
                    0
                ],
                "title": "CCRSat: A Collaborative Computation Reuse Framework for Satellite Edge\n  Computing Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CCRSat: A Collaborative Computation Reuse Framework for Satellite Edge\n  Computing Networks"
                },
                "summary": "In satellite computing applications, such as remote sensing, tasks often\ninvolve similar or identical input data, leading to the same processing\nresults. Computation reuse is an emerging paradigm that leverages the execution\nresults of previous tasks to enhance the utilization of computational\nresources. While this paradigm has been extensively studied in terrestrial\nnetworks with abundant computing and caching resources, such as named data\nnetworking (NDN), it is essential to develop a framework appropriate for\nresource-constrained satellite networks, which are expected to have longer task\ncompletion times. In this paper, we propose CCRSat, a collaborative computation\nreuse framework for satellite edge computing networks. CCRSat initially\nimplements local computation reuse on an independent satellite, utilizing a\nsatellite reuse state (SRS) to assess the efficiency of computation reuse.\nAdditionally, an inter-satellite computation reuse algorithm is introduced,\nwhich utilizes the collaborative sharing of similarity in previously processed\ndata among multiple satellites. The evaluation results tested on real-world\ndatasets demonstrate that, compared to comparative scenarios, our proposed\nCCRSat can significantly reduce task completion time by up to 62.1% and\ncomputational resource consumption by up to 28.8%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In satellite computing applications, such as remote sensing, tasks often\ninvolve similar or identical input data, leading to the same processing\nresults. Computation reuse is an emerging paradigm that leverages the execution\nresults of previous tasks to enhance the utilization of computational\nresources. While this paradigm has been extensively studied in terrestrial\nnetworks with abundant computing and caching resources, such as named data\nnetworking (NDN), it is essential to develop a framework appropriate for\nresource-constrained satellite networks, which are expected to have longer task\ncompletion times. In this paper, we propose CCRSat, a collaborative computation\nreuse framework for satellite edge computing networks. CCRSat initially\nimplements local computation reuse on an independent satellite, utilizing a\nsatellite reuse state (SRS) to assess the efficiency of computation reuse.\nAdditionally, an inter-satellite computation reuse algorithm is introduced,\nwhich utilizes the collaborative sharing of similarity in previously processed\ndata among multiple satellites. The evaluation results tested on real-world\ndatasets demonstrate that, compared to comparative scenarios, our proposed\nCCRSat can significantly reduce task completion time by up to 62.1% and\ncomputational resource consumption by up to 28.8%."
                },
                "authors": [
                    {
                        "name": "Ye Zhang"
                    },
                    {
                        "name": "Zhishu Shen"
                    },
                    {
                        "name": "Dawen Jiang"
                    },
                    {
                        "name": "Xiangrui Liu"
                    },
                    {
                        "name": "Qiushi Zheng"
                    },
                    {
                        "name": "Jiong Jin"
                    }
                ],
                "author_detail": {
                    "name": "Jiong Jin"
                },
                "author": "Jiong Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11946v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11946v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.06348v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.06348v2",
                "updated": "2025-03-15T00:49:55Z",
                "updated_parsed": [
                    2025,
                    3,
                    15,
                    0,
                    49,
                    55,
                    5,
                    74,
                    0
                ],
                "published": "2024-03-11T00:30:25Z",
                "published_parsed": [
                    2024,
                    3,
                    11,
                    0,
                    30,
                    25,
                    0,
                    71,
                    0
                ],
                "title": "Accelerating Sparse Tensor Decomposition Using Adaptive Linearized\n  Representation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Sparse Tensor Decomposition Using Adaptive Linearized\n  Representation"
                },
                "summary": "High-dimensional sparse data emerge in many critical application domains such\nas healthcare and cybersecurity. To extract meaningful insights from massive\nvolumes of these multi-dimensional data, scientists employ unsupervised\nanalysis tools based on tensor decomposition (TD) methods. However, real-world\nsparse tensors exhibit highly irregular shapes and data distributions, which\npose significant challenges for making efficient use of modern parallel\nprocessors. This study breaks the prevailing assumption that compressing sparse\ntensors into coarse-grained structures or along a particular dimension/mode is\nmore efficient than keeping them in a fine-grained, mode-agnostic form. Our\nnovel sparse tensor representation, Adaptive Linearized Tensor Order (ALTO),\nencodes tensors in a compact format that can be easily streamed from memory and\nis amenable to both caching and parallel execution. In contrast to existing\ncompressed tensor formats, ALTO constructs one tensor copy that is agnostic to\nboth the mode orientation and the irregular distribution of nonzero elements.\nTo demonstrate the efficacy of ALTO, we propose a set of parallel TD algorithms\nthat exploit the inherent data reuse of tensor computations to substantially\nreduce synchronization overhead, decrease memory footprint, and improve\nparallel performance. Additionally, we characterize the major execution\nbottlenecks of TD methods on the latest Intel Xeon Scalable processors and\nintroduce dynamic adaptation heuristics to automatically select the best\nalgorithm based on the sparse tensor characteristics. Across a diverse set of\nreal-world data sets, ALTO outperforms the state-of-the-art approaches,\nachieving more than an order-of-magnitude speedup over the best mode-agnostic\nformats. Compared to the best mode-specific formats, ALTO achieves 5.1X\ngeometric mean speedup at a fraction (25%) of their storage costs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-dimensional sparse data emerge in many critical application domains such\nas healthcare and cybersecurity. To extract meaningful insights from massive\nvolumes of these multi-dimensional data, scientists employ unsupervised\nanalysis tools based on tensor decomposition (TD) methods. However, real-world\nsparse tensors exhibit highly irregular shapes and data distributions, which\npose significant challenges for making efficient use of modern parallel\nprocessors. This study breaks the prevailing assumption that compressing sparse\ntensors into coarse-grained structures or along a particular dimension/mode is\nmore efficient than keeping them in a fine-grained, mode-agnostic form. Our\nnovel sparse tensor representation, Adaptive Linearized Tensor Order (ALTO),\nencodes tensors in a compact format that can be easily streamed from memory and\nis amenable to both caching and parallel execution. In contrast to existing\ncompressed tensor formats, ALTO constructs one tensor copy that is agnostic to\nboth the mode orientation and the irregular distribution of nonzero elements.\nTo demonstrate the efficacy of ALTO, we propose a set of parallel TD algorithms\nthat exploit the inherent data reuse of tensor computations to substantially\nreduce synchronization overhead, decrease memory footprint, and improve\nparallel performance. Additionally, we characterize the major execution\nbottlenecks of TD methods on the latest Intel Xeon Scalable processors and\nintroduce dynamic adaptation heuristics to automatically select the best\nalgorithm based on the sparse tensor characteristics. Across a diverse set of\nreal-world data sets, ALTO outperforms the state-of-the-art approaches,\nachieving more than an order-of-magnitude speedup over the best mode-agnostic\nformats. Compared to the best mode-specific formats, ALTO achieves 5.1X\ngeometric mean speedup at a fraction (25%) of their storage costs."
                },
                "authors": [
                    {
                        "name": "Jan Laukemann"
                    },
                    {
                        "name": "Ahmed E. Helal"
                    },
                    {
                        "name": "S. Isaac Geronimo Anderson"
                    },
                    {
                        "name": "Fabio Checconi"
                    },
                    {
                        "name": "Yongseok Soh"
                    },
                    {
                        "name": "Jesmin Jahan Tithi"
                    },
                    {
                        "name": "Teresa Ranadive"
                    },
                    {
                        "name": "Brian J Gravelle"
                    },
                    {
                        "name": "Fabrizio Petrini"
                    },
                    {
                        "name": "Jee Choi"
                    }
                ],
                "author_detail": {
                    "name": "Jee Choi"
                },
                "author": "Jee Choi",
                "arxiv_comment": "Accepted to TPDS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.06348v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.06348v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11816v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11816v1",
                "updated": "2025-03-14T19:02:16Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    19,
                    2,
                    16,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T19:02:16Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    19,
                    2,
                    16,
                    4,
                    73,
                    0
                ],
                "title": "Key, Value, Compress: A Systematic Exploration of KV Cache Compression\n  Techniques",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key, Value, Compress: A Systematic Exploration of KV Cache Compression\n  Techniques"
                },
                "summary": "Large language models (LLMs) have demonstrated exceptional capabilities in\ngenerating text, images, and video content. However, as context length grows,\nthe computational cost of attention increases quadratically with the number of\ntokens, presenting significant efficiency challenges. This paper presents an\nanalysis of various Key-Value (KV) cache compression strategies, offering a\ncomprehensive taxonomy that categorizes these methods by their underlying\nprinciples and implementation techniques. Furthermore, we evaluate their impact\non performance and inference latency, providing critical insights into their\neffectiveness. Our findings highlight the trade-offs involved in KV cache\ncompression and its influence on handling long-context scenarios, paving the\nway for more efficient LLM implementations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated exceptional capabilities in\ngenerating text, images, and video content. However, as context length grows,\nthe computational cost of attention increases quadratically with the number of\ntokens, presenting significant efficiency challenges. This paper presents an\nanalysis of various Key-Value (KV) cache compression strategies, offering a\ncomprehensive taxonomy that categorizes these methods by their underlying\nprinciples and implementation techniques. Furthermore, we evaluate their impact\non performance and inference latency, providing critical insights into their\neffectiveness. Our findings highlight the trade-offs involved in KV cache\ncompression and its influence on handling long-context scenarios, paving the\nway for more efficient LLM implementations."
                },
                "authors": [
                    {
                        "name": "Neusha Javidnia"
                    },
                    {
                        "name": "Bita Darvish Rouhani"
                    },
                    {
                        "name": "Farinaz Koushanfar"
                    }
                ],
                "author_detail": {
                    "name": "Farinaz Koushanfar"
                },
                "author": "Farinaz Koushanfar",
                "arxiv_comment": "Invited paper to IEEE Custom Integrated Circuits Conference (CICC)\n  2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11816v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11816v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11750v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11750v1",
                "updated": "2025-03-14T17:57:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    17,
                    57,
                    42,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T17:57:42Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    17,
                    57,
                    42,
                    4,
                    73,
                    0
                ],
                "title": "Making Every Step Effective: Jailbreaking Large Vision-Language Models\n  Through Hierarchical KV Equalization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Making Every Step Effective: Jailbreaking Large Vision-Language Models\n  Through Hierarchical KV Equalization"
                },
                "summary": "In the realm of large vision-language models (LVLMs), adversarial jailbreak\nattacks serve as a red-teaming approach to identify safety vulnerabilities of\nthese models and their associated defense mechanisms. However, we identify a\ncritical limitation: not every adversarial optimization step leads to a\npositive outcome, and indiscriminately accepting optimization results at each\nstep may reduce the overall attack success rate. To address this challenge, we\nintroduce HKVE (Hierarchical Key-Value Equalization), an innovative\njailbreaking framework that selectively accepts gradient optimization results\nbased on the distribution of attention scores across different layers, ensuring\nthat every optimization step positively contributes to the attack. Extensive\nexperiments demonstrate HKVE's significant effectiveness, achieving attack\nsuccess rates of 75.08% on MiniGPT4, 85.84% on LLaVA and 81.00% on Qwen-VL,\nsubstantially outperforming existing methods by margins of 20.43\\%, 21.01\\% and\n26.43\\% respectively. Furthermore, making every step effective not only leads\nto an increase in attack success rate but also allows for a reduction in the\nnumber of iterations, thereby lowering computational costs. Warning: This paper\ncontains potentially harmful example data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the realm of large vision-language models (LVLMs), adversarial jailbreak\nattacks serve as a red-teaming approach to identify safety vulnerabilities of\nthese models and their associated defense mechanisms. However, we identify a\ncritical limitation: not every adversarial optimization step leads to a\npositive outcome, and indiscriminately accepting optimization results at each\nstep may reduce the overall attack success rate. To address this challenge, we\nintroduce HKVE (Hierarchical Key-Value Equalization), an innovative\njailbreaking framework that selectively accepts gradient optimization results\nbased on the distribution of attention scores across different layers, ensuring\nthat every optimization step positively contributes to the attack. Extensive\nexperiments demonstrate HKVE's significant effectiveness, achieving attack\nsuccess rates of 75.08% on MiniGPT4, 85.84% on LLaVA and 81.00% on Qwen-VL,\nsubstantially outperforming existing methods by margins of 20.43\\%, 21.01\\% and\n26.43\\% respectively. Furthermore, making every step effective not only leads\nto an increase in attack success rate but also allows for a reduction in the\nnumber of iterations, thereby lowering computational costs. Warning: This paper\ncontains potentially harmful example data."
                },
                "authors": [
                    {
                        "name": "Shuyang Hao"
                    },
                    {
                        "name": "Yiwei Wang"
                    },
                    {
                        "name": "Bryan Hooi"
                    },
                    {
                        "name": "Jun Liu"
                    },
                    {
                        "name": "Muhao Chen"
                    },
                    {
                        "name": "Zi Huang"
                    },
                    {
                        "name": "Yujun Cai"
                    }
                ],
                "author_detail": {
                    "name": "Yujun Cai"
                },
                "author": "Yujun Cai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11750v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11750v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01066v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01066v2",
                "updated": "2025-03-14T16:57:12Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    16,
                    57,
                    12,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-03T00:14:34Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    0,
                    14,
                    34,
                    0,
                    62,
                    0
                ],
                "title": "Alchemist: Towards the Design of Efficient Online Continual Learning\n  System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Alchemist: Towards the Design of Efficient Online Continual Learning\n  System"
                },
                "summary": "Continual learning has become a promising solution to refine large language\nmodels incrementally by leveraging user feedback. In particular, online\ncontinual learning - iteratively training the model with small batches of user\nfeedback - has demonstrated notable performance improvements. However, the\nexisting practice of separating training and serving processes forces the\nonline trainer to recompute the intermediate results already done during\nserving. Such redundant computations can account for 30%-42% of total training\ntime.\n  In this paper, we propose Alchemist, to the best of our knowledge, the first\nonline continual learning system that efficiently reuses serving activations to\nincrease training throughput. Alchemist introduces two key techniques: (1)\nrecording and storing activations and KV cache only during the prefill phase to\nminimize latency and memory overhead; and (2) smart activation offloading and\nhedging. Evaluations with inputs of varied token length sampled from ShareGPT\ndataset show that compared with a separate training cluster, Alchemist\nsignificantly increases training throughput by up to 1.72x, reduces up to 47%\nmemory usage during training, and supports up to 2x more training tokens - all\nwhile maintaining negligible impact on serving latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continual learning has become a promising solution to refine large language\nmodels incrementally by leveraging user feedback. In particular, online\ncontinual learning - iteratively training the model with small batches of user\nfeedback - has demonstrated notable performance improvements. However, the\nexisting practice of separating training and serving processes forces the\nonline trainer to recompute the intermediate results already done during\nserving. Such redundant computations can account for 30%-42% of total training\ntime.\n  In this paper, we propose Alchemist, to the best of our knowledge, the first\nonline continual learning system that efficiently reuses serving activations to\nincrease training throughput. Alchemist introduces two key techniques: (1)\nrecording and storing activations and KV cache only during the prefill phase to\nminimize latency and memory overhead; and (2) smart activation offloading and\nhedging. Evaluations with inputs of varied token length sampled from ShareGPT\ndataset show that compared with a separate training cluster, Alchemist\nsignificantly increases training throughput by up to 1.72x, reduces up to 47%\nmemory usage during training, and supports up to 2x more training tokens - all\nwhile maintaining negligible impact on serving latency."
                },
                "authors": [
                    {
                        "name": "Yuyang Huang"
                    },
                    {
                        "name": "Yuhan Liu"
                    },
                    {
                        "name": "Haryadi S. Gunawi"
                    },
                    {
                        "name": "Beibin Li"
                    },
                    {
                        "name": "Changho Hwang"
                    }
                ],
                "author_detail": {
                    "name": "Changho Hwang"
                },
                "author": "Changho Hwang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01066v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01066v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11460v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11460v1",
                "updated": "2025-03-14T14:47:55Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    14,
                    47,
                    55,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T14:47:55Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    14,
                    47,
                    55,
                    4,
                    73,
                    0
                ],
                "title": "ARCAS: Adaptive Runtime System for Chiplet-Aware Scheduling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ARCAS: Adaptive Runtime System for Chiplet-Aware Scheduling"
                },
                "summary": "The growing disparity between CPU core counts and available memory bandwidth\nhas intensified memory contention in servers. This particularly affects highly\nparallelizable applications, which must achieve efficient cache utilization to\nmaintain performance as CPU core counts grow. Optimizing cache utilization,\nhowever, is complex for recent chiplet-based CPUs, whose partitioned L3 caches\nlead to varying latencies and bandwidths, even within a single NUMA domain.\nClassical NUMA optimizations and task scheduling approaches unfortunately fail\nto address the performance issues of chiplet-based CPUs.\n  We describe Adaptive Runtime system for Chiplet-Aware Scheduling (ARCAS), a\nnew runtime system designed for chiplet-based CPUs. ARCAS combines\nchiplet-aware task scheduling heuristics, hardware-aware memory allocation, and\nfine-grained performance monitoring to optimize workload execution. It\nimplements a lightweight concurrency model that combines user-level thread\nfeatures-such as individual stacks, per-task scheduling, and state\nmanagement-with coroutine-like behavior, allowing tasks to suspend and resume\nexecution at defined points while efficiently managing task migration across\nchiplets. Our evaluation across diverse scenarios shows ARCAS's effectiveness\nfor optimizing the performance of memory-intensive parallel applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing disparity between CPU core counts and available memory bandwidth\nhas intensified memory contention in servers. This particularly affects highly\nparallelizable applications, which must achieve efficient cache utilization to\nmaintain performance as CPU core counts grow. Optimizing cache utilization,\nhowever, is complex for recent chiplet-based CPUs, whose partitioned L3 caches\nlead to varying latencies and bandwidths, even within a single NUMA domain.\nClassical NUMA optimizations and task scheduling approaches unfortunately fail\nto address the performance issues of chiplet-based CPUs.\n  We describe Adaptive Runtime system for Chiplet-Aware Scheduling (ARCAS), a\nnew runtime system designed for chiplet-based CPUs. ARCAS combines\nchiplet-aware task scheduling heuristics, hardware-aware memory allocation, and\nfine-grained performance monitoring to optimize workload execution. It\nimplements a lightweight concurrency model that combines user-level thread\nfeatures-such as individual stacks, per-task scheduling, and state\nmanagement-with coroutine-like behavior, allowing tasks to suspend and resume\nexecution at defined points while efficiently managing task migration across\nchiplets. Our evaluation across diverse scenarios shows ARCAS's effectiveness\nfor optimizing the performance of memory-intensive parallel applications."
                },
                "authors": [
                    {
                        "name": "Alessandro Fogli"
                    },
                    {
                        "name": "Bo Zhao"
                    },
                    {
                        "name": "Peter Pietzuch"
                    },
                    {
                        "name": "Jana Giceva"
                    }
                ],
                "author_detail": {
                    "name": "Jana Giceva"
                },
                "author": "Jana Giceva",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11460v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11460v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11426v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11426v1",
                "updated": "2025-03-14T14:14:05Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    14,
                    14,
                    5,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T14:14:05Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    14,
                    14,
                    5,
                    4,
                    73,
                    0
                ],
                "title": "Text Compression for Efficient Language Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text Compression for Efficient Language Generation"
                },
                "summary": "We challenge the prevailing assumption that LLMs must rely fully on sub-word\ntokens for high-quality text generation. To this end, we propose the\n\"Generative Pretrained Thoughtformer\" (GPTHF), a hierarchical transformer\nlanguage model capable of text generation by compressing text into sentence\nembeddings and employing a sentence attention mechanism. GPTHF retains GPT's\narchitecture, modifying only token interactions via dynamic sparse attention\nmasks.\n  Our experiments show that GPTHF achieves an up to an order of magnitude\nimprovement in FLOPs efficiency and a threefold increase in runtime speed\ncompared to equally-sized GPT models in the low-size regime. This is achieved\nthrough a unique generation method that caches and reuses sentence embeddings,\nallowing significant portions of the input to bypass large parts of the\nnetwork.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We challenge the prevailing assumption that LLMs must rely fully on sub-word\ntokens for high-quality text generation. To this end, we propose the\n\"Generative Pretrained Thoughtformer\" (GPTHF), a hierarchical transformer\nlanguage model capable of text generation by compressing text into sentence\nembeddings and employing a sentence attention mechanism. GPTHF retains GPT's\narchitecture, modifying only token interactions via dynamic sparse attention\nmasks.\n  Our experiments show that GPTHF achieves an up to an order of magnitude\nimprovement in FLOPs efficiency and a threefold increase in runtime speed\ncompared to equally-sized GPT models in the low-size regime. This is achieved\nthrough a unique generation method that caches and reuses sentence embeddings,\nallowing significant portions of the input to bypass large parts of the\nnetwork."
                },
                "authors": [
                    {
                        "name": "David Gu"
                    },
                    {
                        "name": "Peter Belcak"
                    },
                    {
                        "name": "Roger Wattenhofer"
                    }
                ],
                "author_detail": {
                    "name": "Roger Wattenhofer"
                },
                "author": "Roger Wattenhofer",
                "arxiv_comment": "accepted to NAACL SRW 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11426v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11426v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11132v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11132v1",
                "updated": "2025-03-14T06:49:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    6,
                    49,
                    37,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T06:49:37Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    6,
                    49,
                    37,
                    4,
                    73,
                    0
                ],
                "title": "X-EcoMLA: Upcycling Pre-Trained Attention into MLA for Efficient and\n  Extreme KV Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "X-EcoMLA: Upcycling Pre-Trained Attention into MLA for Efficient and\n  Extreme KV Compression"
                },
                "summary": "Multi-head latent attention (MLA) is designed to optimize KV cache memory\nthrough low-rank key-value joint compression. Rather than caching keys and\nvalues separately, MLA stores their compressed latent representations, reducing\nmemory overhead while maintaining the performance. While MLA improves memory\nefficiency without compromising language model accuracy, its major limitation\nlies in its integration during the pre-training phase, requiring models to be\ntrained from scratch. This raises a key question: can we use MLA's benefits\nfully or partially in models that have already been pre-trained with different\nattention mechanisms? In this paper, we propose X-EcoMLA to deploy post\ntraining distillation to enable the upcycling of Transformer-based attention\ninto an efficient hybrid (i.e., combination of regular attention and MLA\nlayers) or full MLA variant through lightweight post-training adaptation,\nbypassing the need for extensive pre-training. We demonstrate that leveraging\nthe dark knowledge of a well-trained model can enhance training accuracy and\nenable extreme KV cache compression in MLA without compromising model\nperformance. Our results show that using an 8B teacher model allows us to\ncompress the KV cache size of the Llama3.2-1B-Inst baseline by 6.4x while\npreserving 100% of its average score across multiple tasks on the LM Harness\nEvaluation benchmark. This is achieved with only 3.6B training tokens and about\n70 GPU hours on AMD MI300 GPUs, compared to the 370K GPU hours required for\npre-training the Llama3.2-1B model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-head latent attention (MLA) is designed to optimize KV cache memory\nthrough low-rank key-value joint compression. Rather than caching keys and\nvalues separately, MLA stores their compressed latent representations, reducing\nmemory overhead while maintaining the performance. While MLA improves memory\nefficiency without compromising language model accuracy, its major limitation\nlies in its integration during the pre-training phase, requiring models to be\ntrained from scratch. This raises a key question: can we use MLA's benefits\nfully or partially in models that have already been pre-trained with different\nattention mechanisms? In this paper, we propose X-EcoMLA to deploy post\ntraining distillation to enable the upcycling of Transformer-based attention\ninto an efficient hybrid (i.e., combination of regular attention and MLA\nlayers) or full MLA variant through lightweight post-training adaptation,\nbypassing the need for extensive pre-training. We demonstrate that leveraging\nthe dark knowledge of a well-trained model can enhance training accuracy and\nenable extreme KV cache compression in MLA without compromising model\nperformance. Our results show that using an 8B teacher model allows us to\ncompress the KV cache size of the Llama3.2-1B-Inst baseline by 6.4x while\npreserving 100% of its average score across multiple tasks on the LM Harness\nEvaluation benchmark. This is achieved with only 3.6B training tokens and about\n70 GPU hours on AMD MI300 GPUs, compared to the 370K GPU hours required for\npre-training the Llama3.2-1B model."
                },
                "authors": [
                    {
                        "name": "Guihong Li"
                    },
                    {
                        "name": "Mehdi Rezagholizadeh"
                    },
                    {
                        "name": "Mingyu Yang"
                    },
                    {
                        "name": "Vikram Appia"
                    },
                    {
                        "name": "Emad Barsoum"
                    }
                ],
                "author_detail": {
                    "name": "Emad Barsoum"
                },
                "author": "Emad Barsoum",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11132v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11132v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11108v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11108v1",
                "updated": "2025-03-14T06:01:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    6,
                    1,
                    42,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T06:01:42Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    6,
                    1,
                    42,
                    4,
                    73,
                    0
                ],
                "title": "Limits of KV Cache Compression for Tensor Attention based Autoregressive\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Limits of KV Cache Compression for Tensor Attention based Autoregressive\n  Transformers"
                },
                "summary": "The key-value (KV) cache in autoregressive transformers presents a\nsignificant bottleneck during inference, which restricts the context length\ncapabilities of large language models (LLMs). While previous work analyzes the\nfundamental space complexity barriers in standard attention mechanism [Haris\nand Onak, 2025], our work generalizes the space complexity barriers result to\ntensor attention version. Our theoretical contributions rely on a novel\nreduction from communication complexity and deduce the memory lower bound for\ntensor-structured attention mechanisms when $d = \\Omega(\\log n)$. In the low\ndimensional regime where $d = o(\\log n)$, we analyze the theoretical bounds of\nthe space complexity as well. Overall, our work provides a theoretical\nfoundation for us to understand the compression-expressivity tradeoff in tensor\nattention mechanisms and offers more perspectives in developing more\nmemory-efficient transformer architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The key-value (KV) cache in autoregressive transformers presents a\nsignificant bottleneck during inference, which restricts the context length\ncapabilities of large language models (LLMs). While previous work analyzes the\nfundamental space complexity barriers in standard attention mechanism [Haris\nand Onak, 2025], our work generalizes the space complexity barriers result to\ntensor attention version. Our theoretical contributions rely on a novel\nreduction from communication complexity and deduce the memory lower bound for\ntensor-structured attention mechanisms when $d = \\Omega(\\log n)$. In the low\ndimensional regime where $d = o(\\log n)$, we analyze the theoretical bounds of\nthe space complexity as well. Overall, our work provides a theoretical\nfoundation for us to understand the compression-expressivity tradeoff in tensor\nattention mechanisms and offers more perspectives in developing more\nmemory-efficient transformer architectures."
                },
                "authors": [
                    {
                        "name": "Yifang Chen"
                    },
                    {
                        "name": "Xiaoyu Li"
                    },
                    {
                        "name": "Yingyu Liang"
                    },
                    {
                        "name": "Zhenmei Shi"
                    },
                    {
                        "name": "Zhao Song"
                    },
                    {
                        "name": "Yu Tian"
                    }
                ],
                "author_detail": {
                    "name": "Yu Tian"
                },
                "author": "Yu Tian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11108v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11108v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10589v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10589v1",
                "updated": "2025-03-13T17:40:07Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    17,
                    40,
                    7,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T17:40:07Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    17,
                    40,
                    7,
                    3,
                    72,
                    0
                ],
                "title": "Long Context Tuning for Video Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long Context Tuning for Video Generation"
                },
                "summary": "Recent advances in video generation can produce realistic, minute-long\nsingle-shot videos with scalable diffusion transformers. However, real-world\nnarrative videos require multi-shot scenes with visual and dynamic consistency\nacross shots. In this work, we introduce Long Context Tuning (LCT), a training\nparadigm that expands the context window of pre-trained single-shot video\ndiffusion models to learn scene-level consistency directly from data. Our\nmethod expands full attention mechanisms from individual shots to encompass all\nshots within a scene, incorporating interleaved 3D position embedding and an\nasynchronous noise strategy, enabling both joint and auto-regressive shot\ngeneration without additional parameters. Models with bidirectional attention\nafter LCT can further be fine-tuned with context-causal attention, facilitating\nauto-regressive generation with efficient KV-cache. Experiments demonstrate\nsingle-shot models after LCT can produce coherent multi-shot scenes and exhibit\nemerging capabilities, including compositional generation and interactive shot\nextension, paving the way for more practical visual content creation. See\nhttps://guoyww.github.io/projects/long-context-video/ for more details.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in video generation can produce realistic, minute-long\nsingle-shot videos with scalable diffusion transformers. However, real-world\nnarrative videos require multi-shot scenes with visual and dynamic consistency\nacross shots. In this work, we introduce Long Context Tuning (LCT), a training\nparadigm that expands the context window of pre-trained single-shot video\ndiffusion models to learn scene-level consistency directly from data. Our\nmethod expands full attention mechanisms from individual shots to encompass all\nshots within a scene, incorporating interleaved 3D position embedding and an\nasynchronous noise strategy, enabling both joint and auto-regressive shot\ngeneration without additional parameters. Models with bidirectional attention\nafter LCT can further be fine-tuned with context-causal attention, facilitating\nauto-regressive generation with efficient KV-cache. Experiments demonstrate\nsingle-shot models after LCT can produce coherent multi-shot scenes and exhibit\nemerging capabilities, including compositional generation and interactive shot\nextension, paving the way for more practical visual content creation. See\nhttps://guoyww.github.io/projects/long-context-video/ for more details."
                },
                "authors": [
                    {
                        "name": "Yuwei Guo"
                    },
                    {
                        "name": "Ceyuan Yang"
                    },
                    {
                        "name": "Ziyan Yang"
                    },
                    {
                        "name": "Zhibei Ma"
                    },
                    {
                        "name": "Zhijie Lin"
                    },
                    {
                        "name": "Zhenheng Yang"
                    },
                    {
                        "name": "Dahua Lin"
                    },
                    {
                        "name": "Lu Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Lu Jiang"
                },
                "author": "Lu Jiang",
                "arxiv_comment": "Project Page: https://guoyww.github.io/projects/long-context-video/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10589v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10589v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10568v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10568v1",
                "updated": "2025-03-13T17:19:51Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    17,
                    19,
                    51,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T17:19:51Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    17,
                    19,
                    51,
                    3,
                    72,
                    0
                ],
                "title": "Autoregressive Image Generation with Randomized Parallel Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive Image Generation with Randomized Parallel Decoding"
                },
                "summary": "We introduce ARPG, a novel visual autoregressive model that enables\nrandomized parallel generation, addressing the inherent limitations of\nconventional raster-order approaches, which hinder inference efficiency and\nzero-shot generalization due to their sequential, predefined token generation\norder. Our key insight is that effective random-order modeling necessitates\nexplicit guidance for determining the position of the next predicted token. To\nthis end, we propose a novel guided decoding framework that decouples\npositional guidance from content representation, encoding them separately as\nqueries and key-value pairs. By directly incorporating this guidance into the\ncausal attention mechanism, our approach enables fully random-order training\nand generation, eliminating the need for bidirectional attention. Consequently,\nARPG readily generalizes to zero-shot tasks such as image inpainting,\noutpainting, and resolution expansion. Furthermore, it supports parallel\ninference by concurrently processing multiple queries using a shared KV cache.\nOn the ImageNet-1K 256 benchmark, our approach attains an FID of 1.94 with only\n64 sampling steps, achieving over a 20-fold increase in throughput while\nreducing memory consumption by over 75% compared to representative recent\nautoregressive models at a similar scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce ARPG, a novel visual autoregressive model that enables\nrandomized parallel generation, addressing the inherent limitations of\nconventional raster-order approaches, which hinder inference efficiency and\nzero-shot generalization due to their sequential, predefined token generation\norder. Our key insight is that effective random-order modeling necessitates\nexplicit guidance for determining the position of the next predicted token. To\nthis end, we propose a novel guided decoding framework that decouples\npositional guidance from content representation, encoding them separately as\nqueries and key-value pairs. By directly incorporating this guidance into the\ncausal attention mechanism, our approach enables fully random-order training\nand generation, eliminating the need for bidirectional attention. Consequently,\nARPG readily generalizes to zero-shot tasks such as image inpainting,\noutpainting, and resolution expansion. Furthermore, it supports parallel\ninference by concurrently processing multiple queries using a shared KV cache.\nOn the ImageNet-1K 256 benchmark, our approach attains an FID of 1.94 with only\n64 sampling steps, achieving over a 20-fold increase in throughput while\nreducing memory consumption by over 75% compared to representative recent\nautoregressive models at a similar scale."
                },
                "authors": [
                    {
                        "name": "Haopeng Li"
                    },
                    {
                        "name": "Jinyue Yang"
                    },
                    {
                        "name": "Guoqi Li"
                    },
                    {
                        "name": "Huan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Huan Wang"
                },
                "author": "Huan Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10568v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10568v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07720v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07720v2",
                "updated": "2025-03-13T16:29:17Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    16,
                    29,
                    17,
                    3,
                    72,
                    0
                ],
                "published": "2024-12-10T18:13:20Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    13,
                    20,
                    1,
                    345,
                    0
                ],
                "title": "ACDiT: Interpolating Autoregressive Conditional Modeling and Diffusion\n  Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ACDiT: Interpolating Autoregressive Conditional Modeling and Diffusion\n  Transformer"
                },
                "summary": "We present ACDiT, a novel Autoregressive blockwise Conditional Diffusion\nTransformer, that innovatively combines autoregressive and diffusion paradigms\nfor modeling continuous visual information. By introducing a block-wise\nautoregressive unit, ACDiT offers a flexible interpolation between token-wise\nautoregression and full-sequence diffusion, bypassing the limitations of\ndiscrete tokenization. The generation of each block is formulated as a\nconditional diffusion process, conditioned on prior blocks. ACDiT is easy to\nimplement, as simple as creating a Skip-Causal Attention Mask (SCAM) on\nstandard diffusion transformer during training. During inference, the process\niterates between diffusion denoising and autoregressive decoding that can make\nfull use of KV-Cache. We show that ACDiT performs best among all autoregressive\nbaselines under similar model scales on image and video generation tasks. We\nalso demonstrate that benefiting from autoregressive modeling, pretrained ACDiT\ncan be transferred in visual understanding tasks despite being trained with the\ndiffusion objective. The analysis of the trade-off between autoregressive\nmodeling and diffusion demonstrates the potential of ACDiT to be used in\nlong-horizon visual generation tasks. We hope that ACDiT offers a novel\nperspective on visual autoregressive generation and unlocks new avenues for\nunified models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present ACDiT, a novel Autoregressive blockwise Conditional Diffusion\nTransformer, that innovatively combines autoregressive and diffusion paradigms\nfor modeling continuous visual information. By introducing a block-wise\nautoregressive unit, ACDiT offers a flexible interpolation between token-wise\nautoregression and full-sequence diffusion, bypassing the limitations of\ndiscrete tokenization. The generation of each block is formulated as a\nconditional diffusion process, conditioned on prior blocks. ACDiT is easy to\nimplement, as simple as creating a Skip-Causal Attention Mask (SCAM) on\nstandard diffusion transformer during training. During inference, the process\niterates between diffusion denoising and autoregressive decoding that can make\nfull use of KV-Cache. We show that ACDiT performs best among all autoregressive\nbaselines under similar model scales on image and video generation tasks. We\nalso demonstrate that benefiting from autoregressive modeling, pretrained ACDiT\ncan be transferred in visual understanding tasks despite being trained with the\ndiffusion objective. The analysis of the trade-off between autoregressive\nmodeling and diffusion demonstrates the potential of ACDiT to be used in\nlong-horizon visual generation tasks. We hope that ACDiT offers a novel\nperspective on visual autoregressive generation and unlocks new avenues for\nunified models."
                },
                "authors": [
                    {
                        "name": "Jinyi Hu"
                    },
                    {
                        "name": "Shengding Hu"
                    },
                    {
                        "name": "Yuxuan Song"
                    },
                    {
                        "name": "Yufei Huang"
                    },
                    {
                        "name": "Mingxuan Wang"
                    },
                    {
                        "name": "Hao Zhou"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Wei-Ying Ma"
                    },
                    {
                        "name": "Maosong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Maosong Sun"
                },
                "author": "Maosong Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07720v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07720v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10501v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10501v1",
                "updated": "2025-03-13T16:04:31Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    16,
                    4,
                    31,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T16:04:31Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    16,
                    4,
                    31,
                    3,
                    72,
                    0
                ],
                "title": "TokenCarve: Information-Preserving Visual Token Compression in\n  Multimodal Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TokenCarve: Information-Preserving Visual Token Compression in\n  Multimodal Large Language Models"
                },
                "summary": "Multimodal Large Language Models (MLLMs) are becoming increasingly popular,\nwhile the high computational cost associated with multimodal data input,\nparticularly from visual tokens, poses a significant challenge. Existing\ntraining-based token compression methods improve inference efficiency but\nrequire costly retraining, while training-free methods struggle to maintain\nperformance when aggressively reducing token counts. In this study, we reveal\nthat the performance degradation of MLLM closely correlates with the\naccelerated loss of information in the attention output matrix. This insight\nintroduces a novel information-preserving perspective, making it possible to\nmaintain performance even under extreme token compression. Based on this\nfinding, we propose TokenCarve, a training-free, plug-and-play, two-stage token\ncompression framework. The first stage employs an\nInformation-Preservation-Guided Selection (IPGS) strategy to prune\nlow-information tokens, while the second stage further leverages IPGS to guide\ntoken merging, minimizing information loss. Extensive experiments on 11\ndatasets and 2 model variants demonstrate the effectiveness of TokenCarve. It\ncan even reduce the number of visual tokens to 22.2% of the original count,\nachieving a 1.23x speedup in inference, a 64% reduction in KV cache storage,\nand only a 1.54% drop in accuracy. Our code is available at\nhttps://github.com/ShawnTan86/TokenCarve.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) are becoming increasingly popular,\nwhile the high computational cost associated with multimodal data input,\nparticularly from visual tokens, poses a significant challenge. Existing\ntraining-based token compression methods improve inference efficiency but\nrequire costly retraining, while training-free methods struggle to maintain\nperformance when aggressively reducing token counts. In this study, we reveal\nthat the performance degradation of MLLM closely correlates with the\naccelerated loss of information in the attention output matrix. This insight\nintroduces a novel information-preserving perspective, making it possible to\nmaintain performance even under extreme token compression. Based on this\nfinding, we propose TokenCarve, a training-free, plug-and-play, two-stage token\ncompression framework. The first stage employs an\nInformation-Preservation-Guided Selection (IPGS) strategy to prune\nlow-information tokens, while the second stage further leverages IPGS to guide\ntoken merging, minimizing information loss. Extensive experiments on 11\ndatasets and 2 model variants demonstrate the effectiveness of TokenCarve. It\ncan even reduce the number of visual tokens to 22.2% of the original count,\nachieving a 1.23x speedup in inference, a 64% reduction in KV cache storage,\nand only a 1.54% drop in accuracy. Our code is available at\nhttps://github.com/ShawnTan86/TokenCarve."
                },
                "authors": [
                    {
                        "name": "Xudong Tan"
                    },
                    {
                        "name": "Peng Ye"
                    },
                    {
                        "name": "Chongjun Tu"
                    },
                    {
                        "name": "Jianjian Cao"
                    },
                    {
                        "name": "Yaoxin Yang"
                    },
                    {
                        "name": "Lin Zhang"
                    },
                    {
                        "name": "Dongzhan Zhou"
                    },
                    {
                        "name": "Tao Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tao Chen"
                },
                "author": "Tao Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10501v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10501v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10494v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10494v1",
                "updated": "2025-03-13T15:57:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    15,
                    57,
                    50,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T15:57:50Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    15,
                    57,
                    50,
                    3,
                    72,
                    0
                ],
                "title": "Source-primed Multi-turn Conversation Helps Large Language Models\n  Translate Documents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Source-primed Multi-turn Conversation Helps Large Language Models\n  Translate Documents"
                },
                "summary": "LLMs have paved the way for truly simple document-level machine translation,\nbut challenges such as omission errors remain. In this paper, we study a simple\nmethod for handling document-level machine translation, by leveraging previous\ncontexts in a multi-turn conversational manner. Specifically, by decomposing\ndocuments into segments and iteratively translating them while maintaining\nprevious turns, this method ensures coherent translations without additional\ntraining, and can fully re-use the KV cache of previous turns thus minimizing\ncomputational overhead. We further propose a `source-primed' method that first\nprovides the whole source document before multi-turn translation. We\nempirically show this multi-turn method outperforms both translating entire\ndocuments in a single turn and translating each segment independently according\nto multiple automatic metrics in representative LLMs, establishing a strong\nbaseline for document-level translation using LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs have paved the way for truly simple document-level machine translation,\nbut challenges such as omission errors remain. In this paper, we study a simple\nmethod for handling document-level machine translation, by leveraging previous\ncontexts in a multi-turn conversational manner. Specifically, by decomposing\ndocuments into segments and iteratively translating them while maintaining\nprevious turns, this method ensures coherent translations without additional\ntraining, and can fully re-use the KV cache of previous turns thus minimizing\ncomputational overhead. We further propose a `source-primed' method that first\nprovides the whole source document before multi-turn translation. We\nempirically show this multi-turn method outperforms both translating entire\ndocuments in a single turn and translating each segment independently according\nto multiple automatic metrics in representative LLMs, establishing a strong\nbaseline for document-level translation using LLMs."
                },
                "authors": [
                    {
                        "name": "Hanxu Hu"
                    },
                    {
                        "name": "Jannis Vamvas"
                    },
                    {
                        "name": "Rico Sennrich"
                    }
                ],
                "author_detail": {
                    "name": "Rico Sennrich"
                },
                "author": "Rico Sennrich",
                "arxiv_comment": "9 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10494v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10494v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10337v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10337v1",
                "updated": "2025-03-13T13:15:28Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    13,
                    15,
                    28,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T13:15:28Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    13,
                    15,
                    28,
                    3,
                    72,
                    0
                ],
                "title": "KV-Distill: Nearly Lossless Learnable Context Compression for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV-Distill: Nearly Lossless Learnable Context Compression for LLMs"
                },
                "summary": "Sequence-to-sequence tasks often benefit from long contexts, but the\nquadratic complexity of self-attention in standard Transformers renders this\nnon-trivial. During generation, temporary representations -stored in the\nso-called KV cache-account for a large portion of GPU memory usage and scale\nlinearly with context length. We introduce KV-Distill, a Transformer\ncompression framework that distills long context KV caches into significantly\nshorter representations in a question-independent fashion. KV-Distill can be\ntrained as a parameter-efficient adaptor for pretrained models, and enables the\ncompression of arbitrary spans of a context while preserving pre-trained model\ncapabilities. We treat a compressed-uncompressed cache as a student-teacher\npairing and apply a KL-type divergence to match the generated outputs.\nKV-Distill outperforms other compression techniques in worst-case extractive\ntasks and approaches uncompressed performance in long context question\nanswering and summarization, and it can be fine-tuned on domain-specific\ncontexts to reduce lengths by up to 99% while preserving downstream\nperformance. We demonstrate the generalizability of KV-Distill across various\nmodel sizes and architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequence-to-sequence tasks often benefit from long contexts, but the\nquadratic complexity of self-attention in standard Transformers renders this\nnon-trivial. During generation, temporary representations -stored in the\nso-called KV cache-account for a large portion of GPU memory usage and scale\nlinearly with context length. We introduce KV-Distill, a Transformer\ncompression framework that distills long context KV caches into significantly\nshorter representations in a question-independent fashion. KV-Distill can be\ntrained as a parameter-efficient adaptor for pretrained models, and enables the\ncompression of arbitrary spans of a context while preserving pre-trained model\ncapabilities. We treat a compressed-uncompressed cache as a student-teacher\npairing and apply a KL-type divergence to match the generated outputs.\nKV-Distill outperforms other compression techniques in worst-case extractive\ntasks and approaches uncompressed performance in long context question\nanswering and summarization, and it can be fine-tuned on domain-specific\ncontexts to reduce lengths by up to 99% while preserving downstream\nperformance. We demonstrate the generalizability of KV-Distill across various\nmodel sizes and architectures."
                },
                "authors": [
                    {
                        "name": "Vivek Chari"
                    },
                    {
                        "name": "Guanghui Qin"
                    },
                    {
                        "name": "Benjamin Van Durme"
                    }
                ],
                "author_detail": {
                    "name": "Benjamin Van Durme"
                },
                "author": "Benjamin Van Durme",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10337v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10337v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10270v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10270v1",
                "updated": "2025-03-13T11:26:45Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    11,
                    26,
                    45,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T11:26:45Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    11,
                    26,
                    45,
                    3,
                    72,
                    0
                ],
                "title": "EEdit : Rethinking the Spatial and Temporal Redundancy for Efficient\n  Image Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EEdit : Rethinking the Spatial and Temporal Redundancy for Efficient\n  Image Editing"
                },
                "summary": "Inversion-based image editing is rapidly gaining momentum while suffering\nfrom significant computation overhead, hindering its application in real-time\ninteractive scenarios. In this paper, we rethink that the redundancy in\ninversion-based image editing exists in both the spatial and temporal\ndimensions, such as the unnecessary computation in unedited regions and the\nredundancy in the inversion progress. To tackle these challenges, we propose a\npractical framework, named EEdit, to achieve efficient image editing.\nSpecifically, we introduce three techniques to solve them one by one. For\nspatial redundancy, spatial locality caching is introduced to compute the\nedited region and its neighboring regions while skipping the unedited regions,\nand token indexing preprocessing is designed to further accelerate the caching.\nFor temporal redundancy, inversion step skipping is proposed to reuse the\nlatent for efficient editing. Our experiments demonstrate an average of 2.46\n$\\times$ acceleration without performance drop in a wide range of editing tasks\nincluding prompt-guided image editing, dragging and image composition. Our\ncodes are available at https://github.com/yuriYanZeXuan/EEdit",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inversion-based image editing is rapidly gaining momentum while suffering\nfrom significant computation overhead, hindering its application in real-time\ninteractive scenarios. In this paper, we rethink that the redundancy in\ninversion-based image editing exists in both the spatial and temporal\ndimensions, such as the unnecessary computation in unedited regions and the\nredundancy in the inversion progress. To tackle these challenges, we propose a\npractical framework, named EEdit, to achieve efficient image editing.\nSpecifically, we introduce three techniques to solve them one by one. For\nspatial redundancy, spatial locality caching is introduced to compute the\nedited region and its neighboring regions while skipping the unedited regions,\nand token indexing preprocessing is designed to further accelerate the caching.\nFor temporal redundancy, inversion step skipping is proposed to reuse the\nlatent for efficient editing. Our experiments demonstrate an average of 2.46\n$\\times$ acceleration without performance drop in a wide range of editing tasks\nincluding prompt-guided image editing, dragging and image composition. Our\ncodes are available at https://github.com/yuriYanZeXuan/EEdit"
                },
                "authors": [
                    {
                        "name": "Zexuan Yan"
                    },
                    {
                        "name": "Yue Ma"
                    },
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Wenteng Chen"
                    },
                    {
                        "name": "Qifeng Chen"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "17 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10270v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10270v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07752v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07752v3",
                "updated": "2025-03-13T11:14:49Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    11,
                    14,
                    49,
                    3,
                    72,
                    0
                ],
                "published": "2024-12-10T18:50:37Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    50,
                    37,
                    1,
                    345,
                    0
                ],
                "title": "FlashRNN: I/O-Aware Optimization of Traditional RNNs on modern hardware",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlashRNN: I/O-Aware Optimization of Traditional RNNs on modern hardware"
                },
                "summary": "While Transformers and other sequence-parallelizable neural network\narchitectures seem like the current state of the art in sequence modeling, they\nspecifically lack state-tracking capabilities. These are important for\ntime-series tasks and logical reasoning. Traditional RNNs like LSTMs and GRUs,\nas well as modern variants like sLSTM do have these capabilities at the cost of\nstrictly sequential processing. While this is often seen as a strong\nlimitation, we show how fast these networks can get with our\nhardware-optimization FlashRNN in Triton and CUDA, optimizing kernels to the\nregister level on modern GPUs. We extend traditional RNNs with a\nparallelization variant that processes multiple RNNs of smaller hidden state in\nparallel, similar to the head-wise processing in Transformers. To enable\nflexibility on different GPU variants, we introduce a new optimization\nframework for hardware-internal cache sizes, memory and compute handling. It\nmodels the hardware in a setting using polyhedral-like constraints, including\nthe notion of divisibility. This speeds up the solution process in our\nConstrINT library for general integer constraint satisfaction problems (integer\nCSPs). We show that our kernels can achieve 50x speed-ups over a vanilla\nPyTorch implementation and allow 40x larger hidden sizes compared to our Triton\nimplementation. Our open-source kernels and the optimization library are\nreleased here to boost research in the direction of state-tracking enabled RNNs\nand sequence modeling: https://github.com/NX-AI/flashrnn",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Transformers and other sequence-parallelizable neural network\narchitectures seem like the current state of the art in sequence modeling, they\nspecifically lack state-tracking capabilities. These are important for\ntime-series tasks and logical reasoning. Traditional RNNs like LSTMs and GRUs,\nas well as modern variants like sLSTM do have these capabilities at the cost of\nstrictly sequential processing. While this is often seen as a strong\nlimitation, we show how fast these networks can get with our\nhardware-optimization FlashRNN in Triton and CUDA, optimizing kernels to the\nregister level on modern GPUs. We extend traditional RNNs with a\nparallelization variant that processes multiple RNNs of smaller hidden state in\nparallel, similar to the head-wise processing in Transformers. To enable\nflexibility on different GPU variants, we introduce a new optimization\nframework for hardware-internal cache sizes, memory and compute handling. It\nmodels the hardware in a setting using polyhedral-like constraints, including\nthe notion of divisibility. This speeds up the solution process in our\nConstrINT library for general integer constraint satisfaction problems (integer\nCSPs). We show that our kernels can achieve 50x speed-ups over a vanilla\nPyTorch implementation and allow 40x larger hidden sizes compared to our Triton\nimplementation. Our open-source kernels and the optimization library are\nreleased here to boost research in the direction of state-tracking enabled RNNs\nand sequence modeling: https://github.com/NX-AI/flashrnn"
                },
                "authors": [
                    {
                        "name": "Korbinian Pppel"
                    },
                    {
                        "name": "Maximilian Beck"
                    },
                    {
                        "name": "Sepp Hochreiter"
                    }
                ],
                "author_detail": {
                    "name": "Sepp Hochreiter"
                },
                "author": "Sepp Hochreiter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07752v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07752v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10074v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10074v1",
                "updated": "2025-03-13T05:43:14Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    5,
                    43,
                    14,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T05:43:14Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    5,
                    43,
                    14,
                    3,
                    72,
                    0
                ],
                "title": "Demoting Security via Exploitation of Cache Demote Operation in Intel's\n  Latest ISA Extension",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demoting Security via Exploitation of Cache Demote Operation in Intel's\n  Latest ISA Extension"
                },
                "summary": "ISA extensions are increasingly adopted to boost the performance of\nspecialized workloads without requiring an entire architectural redesign.\nHowever, these enhancements can inadvertently expose new attack surfaces in the\nmicroarchitecture. In this paper, we investigate Intel's recently introduced\ncldemote extension, which promotes efficient data sharing by transferring cache\nlines from upper-level caches to the Last Level Cache (LLC). Despite its\nperformance benefits, we uncover critical properties-unprivileged access,\ninter-cache state transition, and fault suppression-that render cldemote\nexploitable for microarchitectural attacks. We propose two new attack\nprimitives, Flush+Demote and Demote+Time, built on our analysis. Flush+Demote\nconstructs a covert channel with a bandwidth of 2.84 Mbps and a bit error rate\nof 0.018%, while Demote+Time derandomizes the kernel base address in 2.49 ms on\nLinux. Furthermore, we show that leveraging cldemote accelerates eviction set\nconstruction in non-inclusive LLC designs by obviating the need for helper\nthreads or extensive cache conflicts, thereby reducing construction time by 36%\nyet retaining comparable success rates. Finally, we examine how ISA extensions\ncontribute to broader microarchitectural attacks, identifying five key\nexploitable characteristics and categorizing four distinct attack types. We\nalso discuss potential countermeasures, highlighting the far-reaching security\nimplications of emerging ISA extensions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ISA extensions are increasingly adopted to boost the performance of\nspecialized workloads without requiring an entire architectural redesign.\nHowever, these enhancements can inadvertently expose new attack surfaces in the\nmicroarchitecture. In this paper, we investigate Intel's recently introduced\ncldemote extension, which promotes efficient data sharing by transferring cache\nlines from upper-level caches to the Last Level Cache (LLC). Despite its\nperformance benefits, we uncover critical properties-unprivileged access,\ninter-cache state transition, and fault suppression-that render cldemote\nexploitable for microarchitectural attacks. We propose two new attack\nprimitives, Flush+Demote and Demote+Time, built on our analysis. Flush+Demote\nconstructs a covert channel with a bandwidth of 2.84 Mbps and a bit error rate\nof 0.018%, while Demote+Time derandomizes the kernel base address in 2.49 ms on\nLinux. Furthermore, we show that leveraging cldemote accelerates eviction set\nconstruction in non-inclusive LLC designs by obviating the need for helper\nthreads or extensive cache conflicts, thereby reducing construction time by 36%\nyet retaining comparable success rates. Finally, we examine how ISA extensions\ncontribute to broader microarchitectural attacks, identifying five key\nexploitable characteristics and categorizing four distinct attack types. We\nalso discuss potential countermeasures, highlighting the far-reaching security\nimplications of emerging ISA extensions."
                },
                "authors": [
                    {
                        "name": "Taehun Kim"
                    },
                    {
                        "name": "Hyerean Jang"
                    },
                    {
                        "name": "Youngjoo Shin"
                    }
                ],
                "author_detail": {
                    "name": "Youngjoo Shin"
                },
                "author": "Youngjoo Shin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10074v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10074v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17599v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17599v2",
                "updated": "2025-03-13T04:04:08Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    4,
                    4,
                    8,
                    3,
                    72,
                    0
                ],
                "published": "2025-02-24T19:34:52Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    19,
                    34,
                    52,
                    0,
                    55,
                    0
                ],
                "title": "MEDA: Dynamic KV Cache Allocation for Efficient Multimodal Long-Context\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MEDA: Dynamic KV Cache Allocation for Efficient Multimodal Long-Context\n  Inference"
                },
                "summary": "Long-context Multimodal Large Language Models (MLLMs) that incorporate long\ntext-image and text-video modalities, demand substantial resources as their\nmultimodal Key-Value (KV) caches grow with increasing input lengths,\nchallenging inference efficiency. Existing methods for KV cache compression, in\nboth text-only and multimodal LLMs, have neglected attention density variations\nacross layers, thus often adopting uniform or progressive reduction strategies\nfor layer-wise cache allocation. In this work, we propose MEDA, a dynamic\nlayer-wise KV cache allocation method for efficient multimodal long-context\ninference. As its core, MEDA utilizes cross-modal attention entropy to\ndetermine the KV cache size at each MLLMs layer. Given the dynamically\nallocated KV cache size at each layer, MEDA also employs a KV pair selection\nscheme to identify which KV pairs to select and a KV pair merging strategy that\nmerges the selected and non-selected ones to preserve information from the\nentire context. MEDA achieves up to 72% KV cache memory reduction and 2.82\ntimes faster decoding speed, while maintaining or enhancing performance on\nvarious multimodal tasks in long-context settings, including multi-images and\nlong-video scenarios. Our code is released at\nhttps://github.com/AIoT-MLSys-Lab/MEDA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context Multimodal Large Language Models (MLLMs) that incorporate long\ntext-image and text-video modalities, demand substantial resources as their\nmultimodal Key-Value (KV) caches grow with increasing input lengths,\nchallenging inference efficiency. Existing methods for KV cache compression, in\nboth text-only and multimodal LLMs, have neglected attention density variations\nacross layers, thus often adopting uniform or progressive reduction strategies\nfor layer-wise cache allocation. In this work, we propose MEDA, a dynamic\nlayer-wise KV cache allocation method for efficient multimodal long-context\ninference. As its core, MEDA utilizes cross-modal attention entropy to\ndetermine the KV cache size at each MLLMs layer. Given the dynamically\nallocated KV cache size at each layer, MEDA also employs a KV pair selection\nscheme to identify which KV pairs to select and a KV pair merging strategy that\nmerges the selected and non-selected ones to preserve information from the\nentire context. MEDA achieves up to 72% KV cache memory reduction and 2.82\ntimes faster decoding speed, while maintaining or enhancing performance on\nvarious multimodal tasks in long-context settings, including multi-images and\nlong-video scenarios. Our code is released at\nhttps://github.com/AIoT-MLSys-Lab/MEDA."
                },
                "authors": [
                    {
                        "name": "Zhongwei Wan"
                    },
                    {
                        "name": "Hui Shen"
                    },
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Che Liu"
                    },
                    {
                        "name": "Zheda Mai"
                    },
                    {
                        "name": "Mi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Mi Zhang"
                },
                "author": "Mi Zhang",
                "arxiv_comment": "NAACL 2025 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17599v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17599v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10714v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10714v1",
                "updated": "2025-03-13T03:36:03Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    3,
                    36,
                    3,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T03:36:03Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    3,
                    36,
                    3,
                    3,
                    72,
                    0
                ],
                "title": "ZeroMerge: Parameter-Free KV Cache Compression for Memory-Efficient\n  Long-Context LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZeroMerge: Parameter-Free KV Cache Compression for Memory-Efficient\n  Long-Context LLMs"
                },
                "summary": "The linear growth of key-value (KV) cache memory and quadratic computational\ncomplexity pose significant bottlenecks for large language models (LLMs) in\nlong-context processing. While existing KV cache optimization methods address\nthese challenges through token pruning or feature merging, they often suffer\nfrom irreversible information loss or require costly parameter retraining. We\npropose ZeroMerge, a dynamic zero-shot compression framework that achieves\nefficient cache management through three key innovations: (1) Fine-grained\nmemory allocation guided by multi-dimensional token importance metrics at\nhead-level granularity, (2) A residual merging mechanism that preserves\ncritical context through compensated attention scoring, and (3) Parameter-free\nadaptation compatible with diverse LLM architectures without retraining.\nComprehensive evaluations across LLaMA-2 model demonstrate that ZeroMerge\nmaintains full-cache performance at 5\\% compression ratios while doubling\ninference throughput at 40K token lengths. The method effectively balances\nmemory efficiency, generation quality, and deployment flexibility, advancing\npractical long-context LLM applications. The code is available at\nhttps://github.com/SusCom-Lab/ZeroMerge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The linear growth of key-value (KV) cache memory and quadratic computational\ncomplexity pose significant bottlenecks for large language models (LLMs) in\nlong-context processing. While existing KV cache optimization methods address\nthese challenges through token pruning or feature merging, they often suffer\nfrom irreversible information loss or require costly parameter retraining. We\npropose ZeroMerge, a dynamic zero-shot compression framework that achieves\nefficient cache management through three key innovations: (1) Fine-grained\nmemory allocation guided by multi-dimensional token importance metrics at\nhead-level granularity, (2) A residual merging mechanism that preserves\ncritical context through compensated attention scoring, and (3) Parameter-free\nadaptation compatible with diverse LLM architectures without retraining.\nComprehensive evaluations across LLaMA-2 model demonstrate that ZeroMerge\nmaintains full-cache performance at 5\\% compression ratios while doubling\ninference throughput at 40K token lengths. The method effectively balances\nmemory efficiency, generation quality, and deployment flexibility, advancing\npractical long-context LLM applications. The code is available at\nhttps://github.com/SusCom-Lab/ZeroMerge."
                },
                "authors": [
                    {
                        "name": "Xin Liu"
                    },
                    {
                        "name": "Pei Liu"
                    },
                    {
                        "name": "Guoming Tang"
                    }
                ],
                "author_detail": {
                    "name": "Guoming Tang"
                },
                "author": "Guoming Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10714v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10714v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.13035v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.13035v3",
                "updated": "2025-03-13T03:16:43Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    3,
                    16,
                    43,
                    3,
                    72,
                    0
                ],
                "published": "2024-06-18T20:01:51Z",
                "published_parsed": [
                    2024,
                    6,
                    18,
                    20,
                    1,
                    51,
                    1,
                    170,
                    0
                ],
                "title": "D2O: Dynamic Discriminative Operations for Efficient Long-Context\n  Inference of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "D2O: Dynamic Discriminative Operations for Efficient Long-Context\n  Inference of Large Language Models"
                },
                "summary": "Generative inference in Large Language Models (LLMs) is impeded by the\ngrowing memory demands of Key-Value (KV) cache, especially for longer\nsequences. Traditional KV cache eviction strategies, which discard less\ncritical KV pairs based on attention scores, often degrade generation quality,\nleading to issues such as context loss or hallucinations. In this work, we\nintroduce Dynamic Discriminative Operations (D2O), a KV cache compression\nmethod that optimizes KV cache size dynamically and discriminatively at two\nlevels without fine-tuning, while preserving essential context. At layer level,\nD2O leverages the varying densities of attention weights between shallow and\ndeep layers to dynamically determine which layers should avoid excessive\neviction via a novel dynamic allocation strategy to minimize information loss.\nAt token level, D2O incorporates a compensation mechanism that maintains a\nsimilarity threshold to re-discriminate the importance of currently discarded\ntokens, determining whether they should be recalled and merged with similar\ntokens. We conduct experiments on various benchmarks and LLM architectures. Our\nresults show that D2O not only achieves significant memory savings and enhances\ninference throughput by more than 3$\\times$ but also maintains high-quality\nlong-text generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative inference in Large Language Models (LLMs) is impeded by the\ngrowing memory demands of Key-Value (KV) cache, especially for longer\nsequences. Traditional KV cache eviction strategies, which discard less\ncritical KV pairs based on attention scores, often degrade generation quality,\nleading to issues such as context loss or hallucinations. In this work, we\nintroduce Dynamic Discriminative Operations (D2O), a KV cache compression\nmethod that optimizes KV cache size dynamically and discriminatively at two\nlevels without fine-tuning, while preserving essential context. At layer level,\nD2O leverages the varying densities of attention weights between shallow and\ndeep layers to dynamically determine which layers should avoid excessive\neviction via a novel dynamic allocation strategy to minimize information loss.\nAt token level, D2O incorporates a compensation mechanism that maintains a\nsimilarity threshold to re-discriminate the importance of currently discarded\ntokens, determining whether they should be recalled and merged with similar\ntokens. We conduct experiments on various benchmarks and LLM architectures. Our\nresults show that D2O not only achieves significant memory savings and enhances\ninference throughput by more than 3$\\times$ but also maintains high-quality\nlong-text generation."
                },
                "authors": [
                    {
                        "name": "Zhongwei Wan"
                    },
                    {
                        "name": "Xinjian Wu"
                    },
                    {
                        "name": "Yu Zhang"
                    },
                    {
                        "name": "Yi Xin"
                    },
                    {
                        "name": "Chaofan Tao"
                    },
                    {
                        "name": "Zhihong Zhu"
                    },
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Siqi Luo"
                    },
                    {
                        "name": "Jing Xiong"
                    },
                    {
                        "name": "Longyue Wang"
                    },
                    {
                        "name": "Mi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Mi Zhang"
                },
                "author": "Mi Zhang",
                "arxiv_comment": "ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.13035v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.13035v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.14361v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.14361v3",
                "updated": "2025-03-12T18:14:21Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    18,
                    14,
                    21,
                    2,
                    71,
                    0
                ],
                "published": "2024-01-25T18:07:50Z",
                "published_parsed": [
                    2024,
                    1,
                    25,
                    18,
                    7,
                    50,
                    3,
                    25,
                    0
                ],
                "title": "MoE-Infinity: Efficient MoE Inference on Personal Machines with\n  Sparsity-Aware Expert Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoE-Infinity: Efficient MoE Inference on Personal Machines with\n  Sparsity-Aware Expert Cache"
                },
                "summary": "This paper presents MoE-Infinity, an efficient MoE inference system designed\nfor personal machines with limited GPU memory capacity. The key idea for\nMoE-Infinity is that on personal machines, which are often single-user\nenvironments, MoE-based LLMs typically operate with a batch size of one. In\nthis setting, MoE models exhibit a high degree of activation sparsity, meaning\na small number of experts are frequently reused in generating tokens during the\ndecode phase. Leveraging this idea, we design a sparsity-aware expert cache,\nwhich can trace the sparse activation of experts during inference and carefully\nselect the trace that represents the sparsity pattern. By analyzing these\nselected traces, MoE-Infinity guides the replacement and prefetching of the\nexpert cache, providing 3.1-16.7x per-token latency improvements over numerous\nstate-of-the-art systems, including vLLM, Ollama, DeepSpeed and BrainStorm\nacross various MoE models (DeepSeek and Mixtral) when handling different LLM\ntasks. MoE-Infinity's source code is publicly available at\nhttps://github.com/EfficientMoE/MoE-Infinity",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents MoE-Infinity, an efficient MoE inference system designed\nfor personal machines with limited GPU memory capacity. The key idea for\nMoE-Infinity is that on personal machines, which are often single-user\nenvironments, MoE-based LLMs typically operate with a batch size of one. In\nthis setting, MoE models exhibit a high degree of activation sparsity, meaning\na small number of experts are frequently reused in generating tokens during the\ndecode phase. Leveraging this idea, we design a sparsity-aware expert cache,\nwhich can trace the sparse activation of experts during inference and carefully\nselect the trace that represents the sparsity pattern. By analyzing these\nselected traces, MoE-Infinity guides the replacement and prefetching of the\nexpert cache, providing 3.1-16.7x per-token latency improvements over numerous\nstate-of-the-art systems, including vLLM, Ollama, DeepSpeed and BrainStorm\nacross various MoE models (DeepSeek and Mixtral) when handling different LLM\ntasks. MoE-Infinity's source code is publicly available at\nhttps://github.com/EfficientMoE/MoE-Infinity"
                },
                "authors": [
                    {
                        "name": "Leyang Xue"
                    },
                    {
                        "name": "Yao Fu"
                    },
                    {
                        "name": "Zhan Lu"
                    },
                    {
                        "name": "Luo Mai"
                    },
                    {
                        "name": "Mahesh Marina"
                    }
                ],
                "author_detail": {
                    "name": "Mahesh Marina"
                },
                "author": "Mahesh Marina",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.14361v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.14361v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18914v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18914v2",
                "updated": "2025-03-12T17:59:18Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    17,
                    59,
                    18,
                    2,
                    71,
                    0
                ],
                "published": "2024-12-25T14:14:31Z",
                "published_parsed": [
                    2024,
                    12,
                    25,
                    14,
                    14,
                    31,
                    2,
                    360,
                    0
                ],
                "title": "PRISM: Efficient Long-Range Reasoning With Short-Context LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PRISM: Efficient Long-Range Reasoning With Short-Context LLMs"
                },
                "summary": "Long-range tasks demand reasoning over long inputs. Current solutions require\nlarge compute budgets, training data, model weight access, or complex\ntask-specific designs. We introduce PRISM, which processes information as a\nstream of chunks while maintaining a structured in-context memory specified\nwith a typed hierarchical schema. PRISM outperforms baselines on diverse tasks\nwhile using at least 4x shorter contexts than long-context models. This\napproach is token-efficient, producing concise outputs and efficiently\nleveraging key-value (KV) caches to reduce costs by up to 54% compared to\nalternative short-context methods. PRISM scales down to tiny chunks (<500\ntokens) without increasing encoding costs or sacrificing quality, and\ngeneralizes to new tasks with minimal effort by automatically generating\nschemas from task descriptions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-range tasks demand reasoning over long inputs. Current solutions require\nlarge compute budgets, training data, model weight access, or complex\ntask-specific designs. We introduce PRISM, which processes information as a\nstream of chunks while maintaining a structured in-context memory specified\nwith a typed hierarchical schema. PRISM outperforms baselines on diverse tasks\nwhile using at least 4x shorter contexts than long-context models. This\napproach is token-efficient, producing concise outputs and efficiently\nleveraging key-value (KV) caches to reduce costs by up to 54% compared to\nalternative short-context methods. PRISM scales down to tiny chunks (<500\ntokens) without increasing encoding costs or sacrificing quality, and\ngeneralizes to new tasks with minimal effort by automatically generating\nschemas from task descriptions."
                },
                "authors": [
                    {
                        "name": "Dulhan Jayalath"
                    },
                    {
                        "name": "James Bradley Wendt"
                    },
                    {
                        "name": "Nicholas Monath"
                    },
                    {
                        "name": "Sandeep Tata"
                    },
                    {
                        "name": "Beliz Gunel"
                    }
                ],
                "author_detail": {
                    "name": "Beliz Gunel"
                },
                "author": "Beliz Gunel",
                "arxiv_comment": "28 pages, 7 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18914v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18914v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09218v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09218v1",
                "updated": "2025-03-12T10:05:05Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    10,
                    5,
                    5,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-12T10:05:05Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    10,
                    5,
                    5,
                    2,
                    71,
                    0
                ],
                "title": "N2C2: Nearest Neighbor Enhanced Confidence Calibration for Cross-Lingual\n  In-Context Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "N2C2: Nearest Neighbor Enhanced Confidence Calibration for Cross-Lingual\n  In-Context Learning"
                },
                "summary": "Recent advancements of in-context learning (ICL) show language models can\nsignificantly improve their performance when demonstrations are provided.\nHowever, little attention has been paid to model calibration and prediction\nconfidence of ICL in cross-lingual scenarios. To bridge this gap, we conduct a\nthorough analysis of ICL for cross-lingual sentiment classification. Our\nfindings suggest that ICL performs poorly in cross-lingual scenarios,\nexhibiting low accuracy and presenting high calibration errors. In response, we\npropose a novel approach, N2C2, which employs a -nearest neighbors augmented\nclassifier for prediction confidence calibration. N2C2 narrows the prediction\ngap by leveraging a datastore of cached few-shot instances. Specifically, N2C2\nintegrates the predictions from the datastore and incorporates confidence-aware\ndistribution, semantically consistent retrieval representation, and adaptive\nneighbor combination modules to effectively utilize the limited number of\nsupporting instances. Evaluation on two multilingual sentiment classification\ndatasets demonstrates that N2C2 outperforms traditional ICL. It surpasses fine\ntuning, prompt tuning and recent state-of-the-art methods in terms of accuracy\nand calibration errors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements of in-context learning (ICL) show language models can\nsignificantly improve their performance when demonstrations are provided.\nHowever, little attention has been paid to model calibration and prediction\nconfidence of ICL in cross-lingual scenarios. To bridge this gap, we conduct a\nthorough analysis of ICL for cross-lingual sentiment classification. Our\nfindings suggest that ICL performs poorly in cross-lingual scenarios,\nexhibiting low accuracy and presenting high calibration errors. In response, we\npropose a novel approach, N2C2, which employs a -nearest neighbors augmented\nclassifier for prediction confidence calibration. N2C2 narrows the prediction\ngap by leveraging a datastore of cached few-shot instances. Specifically, N2C2\nintegrates the predictions from the datastore and incorporates confidence-aware\ndistribution, semantically consistent retrieval representation, and adaptive\nneighbor combination modules to effectively utilize the limited number of\nsupporting instances. Evaluation on two multilingual sentiment classification\ndatasets demonstrates that N2C2 outperforms traditional ICL. It surpasses fine\ntuning, prompt tuning and recent state-of-the-art methods in terms of accuracy\nand calibration errors."
                },
                "authors": [
                    {
                        "name": "Jie He"
                    },
                    {
                        "name": "Simon Yu"
                    },
                    {
                        "name": "Deyi Xiong"
                    },
                    {
                        "name": "Vctor Gutirrez-Basulto"
                    },
                    {
                        "name": "Jeff Z. Pan"
                    }
                ],
                "author_detail": {
                    "name": "Jeff Z. Pan"
                },
                "author": "Jeff Z. Pan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09218v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09218v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17363v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17363v3",
                "updated": "2025-03-12T07:23:32Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    7,
                    23,
                    32,
                    2,
                    71,
                    0
                ],
                "published": "2025-02-24T17:40:09Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    17,
                    40,
                    9,
                    0,
                    55,
                    0
                ],
                "title": "KV-Edit: Training-Free Image Editing for Precise Background Preservation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV-Edit: Training-Free Image Editing for Precise Background Preservation"
                },
                "summary": "Background consistency remains a significant challenge in image editing\ntasks. Despite extensive developments, existing works still face a trade-off\nbetween maintaining similarity to the original image and generating content\nthat aligns with the target. Here, we propose KV-Edit, a training-free approach\nthat uses KV cache in DiTs to maintain background consistency, where background\ntokens are preserved rather than regenerated, eliminating the need for complex\nmechanisms or expensive training, ultimately generating new content that\nseamlessly integrates with the background within user-provided regions. We\nfurther explore the memory consumption of the KV cache during editing and\noptimize the space complexity to $O(1)$ using an inversion-free method. Our\napproach is compatible with any DiT-based generative model without additional\ntraining. Experiments demonstrate that KV-Edit significantly outperforms\nexisting approaches in terms of both background and image quality, even\nsurpassing training-based methods. Project webpage is available at\nhttps://xilluill.github.io/projectpages/KV-Edit",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Background consistency remains a significant challenge in image editing\ntasks. Despite extensive developments, existing works still face a trade-off\nbetween maintaining similarity to the original image and generating content\nthat aligns with the target. Here, we propose KV-Edit, a training-free approach\nthat uses KV cache in DiTs to maintain background consistency, where background\ntokens are preserved rather than regenerated, eliminating the need for complex\nmechanisms or expensive training, ultimately generating new content that\nseamlessly integrates with the background within user-provided regions. We\nfurther explore the memory consumption of the KV cache during editing and\noptimize the space complexity to $O(1)$ using an inversion-free method. Our\napproach is compatible with any DiT-based generative model without additional\ntraining. Experiments demonstrate that KV-Edit significantly outperforms\nexisting approaches in terms of both background and image quality, even\nsurpassing training-based methods. Project webpage is available at\nhttps://xilluill.github.io/projectpages/KV-Edit"
                },
                "authors": [
                    {
                        "name": "Tianrui Zhu"
                    },
                    {
                        "name": "Shiyi Zhang"
                    },
                    {
                        "name": "Jiawei Shao"
                    },
                    {
                        "name": "Yansong Tang"
                    }
                ],
                "author_detail": {
                    "name": "Yansong Tang"
                },
                "author": "Yansong Tang",
                "arxiv_comment": "Project webpage is available at\n  https://xilluill.github.io/projectpages/KV-Edit",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17363v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17363v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19355v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19355v2",
                "updated": "2025-03-12T03:40:38Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    3,
                    40,
                    38,
                    2,
                    71,
                    0
                ],
                "published": "2024-10-25T07:24:38Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    7,
                    24,
                    38,
                    4,
                    299,
                    0
                ],
                "title": "FasterCache: Training-Free Video Diffusion Model Acceleration with High\n  Quality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FasterCache: Training-Free Video Diffusion Model Acceleration with High\n  Quality"
                },
                "summary": "In this paper, we present \\textbf{\\textit{FasterCache}}, a novel\ntraining-free strategy designed to accelerate the inference of video diffusion\nmodels with high-quality generation. By analyzing existing cache-based methods,\nwe observe that \\textit{directly reusing adjacent-step features degrades video\nquality due to the loss of subtle variations}. We further perform a pioneering\ninvestigation of the acceleration potential of classifier-free guidance (CFG)\nand reveal significant redundancy between conditional and unconditional\nfeatures within the same timestep. Capitalizing on these observations, we\nintroduce FasterCache to substantially accelerate diffusion-based video\ngeneration. Our key contributions include a dynamic feature reuse strategy that\npreserves both feature distinction and temporal continuity, and CFG-Cache which\noptimizes the reuse of conditional and unconditional outputs to further enhance\ninference speed without compromising video quality. We empirically evaluate\nFasterCache on recent video diffusion models. Experimental results show that\nFasterCache can significantly accelerate video generation (\\eg 1.67$\\times$\nspeedup on Vchitect-2.0) while keeping video quality comparable to the\nbaseline, and consistently outperform existing methods in both inference speed\nand video quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present \\textbf{\\textit{FasterCache}}, a novel\ntraining-free strategy designed to accelerate the inference of video diffusion\nmodels with high-quality generation. By analyzing existing cache-based methods,\nwe observe that \\textit{directly reusing adjacent-step features degrades video\nquality due to the loss of subtle variations}. We further perform a pioneering\ninvestigation of the acceleration potential of classifier-free guidance (CFG)\nand reveal significant redundancy between conditional and unconditional\nfeatures within the same timestep. Capitalizing on these observations, we\nintroduce FasterCache to substantially accelerate diffusion-based video\ngeneration. Our key contributions include a dynamic feature reuse strategy that\npreserves both feature distinction and temporal continuity, and CFG-Cache which\noptimizes the reuse of conditional and unconditional outputs to further enhance\ninference speed without compromising video quality. We empirically evaluate\nFasterCache on recent video diffusion models. Experimental results show that\nFasterCache can significantly accelerate video generation (\\eg 1.67$\\times$\nspeedup on Vchitect-2.0) while keeping video quality comparable to the\nbaseline, and consistently outperform existing methods in both inference speed\nand video quality."
                },
                "authors": [
                    {
                        "name": "Zhengyao Lv"
                    },
                    {
                        "name": "Chenyang Si"
                    },
                    {
                        "name": "Junhao Song"
                    },
                    {
                        "name": "Zhenyu Yang"
                    },
                    {
                        "name": "Yu Qiao"
                    },
                    {
                        "name": "Ziwei Liu"
                    },
                    {
                        "name": "Kwan-Yee K. Wong"
                    }
                ],
                "author_detail": {
                    "name": "Kwan-Yee K. Wong"
                },
                "author": "Kwan-Yee K. Wong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19355v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19355v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08966v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08966v1",
                "updated": "2025-03-12T00:12:39Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    0,
                    12,
                    39,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-12T00:12:39Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    0,
                    12,
                    39,
                    2,
                    71,
                    0
                ],
                "title": "Performance Models for a Two-tiered Storage System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance Models for a Two-tiered Storage System"
                },
                "summary": "This work describes the design, implementation and performance analysis of a\ndistributed two-tiered storage software. The first tier functions as a\ndistributed software cache implemented using solid-state devices~(NVMes) and\nthe second tier consists of multiple hard disks~(HDDs). We describe an online\nlearning algorithm that manages data movement between the tiers. The software\nis hybrid, i.e. both distributed and multi-threaded. The end-to-end performance\nmodel of the two-tier system was developed using queuing networks and\nbehavioral models of storage devices. We identified significant parameters that\naffect the performance of storage devices and created behavioral models for\neach device. The performance of the software was evaluated on a many-core\ncluster using non-trivial read/write workloads. The paper provides examples to\nillustrate the use of these models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work describes the design, implementation and performance analysis of a\ndistributed two-tiered storage software. The first tier functions as a\ndistributed software cache implemented using solid-state devices~(NVMes) and\nthe second tier consists of multiple hard disks~(HDDs). We describe an online\nlearning algorithm that manages data movement between the tiers. The software\nis hybrid, i.e. both distributed and multi-threaded. The end-to-end performance\nmodel of the two-tier system was developed using queuing networks and\nbehavioral models of storage devices. We identified significant parameters that\naffect the performance of storage devices and created behavioral models for\neach device. The performance of the software was evaluated on a many-core\ncluster using non-trivial read/write workloads. The paper provides examples to\nillustrate the use of these models."
                },
                "authors": [
                    {
                        "name": "Aparna Sasidharan"
                    },
                    {
                        "name": "Xian-He"
                    },
                    {
                        "name": "Jay Lofstead"
                    },
                    {
                        "name": "Scott Klasky"
                    }
                ],
                "author_detail": {
                    "name": "Scott Klasky"
                },
                "author": "Scott Klasky",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08966v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08966v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08941v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08941v1",
                "updated": "2025-03-11T22:44:38Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    22,
                    44,
                    38,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T22:44:38Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    22,
                    44,
                    38,
                    1,
                    70,
                    0
                ],
                "title": "BCZT/LSMO/BCZT multilayer films for high temperature energy storage\n  capacitors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BCZT/LSMO/BCZT multilayer films for high temperature energy storage\n  capacitors"
                },
                "summary": "Ba0.85Ca0.15Zr0.1Ti0.9O3/La0.8Sr0.2MnO3/Ba0.85Ca0.15Zr0.1Ti0.9O3\n(BCZT/LSMO/BCZT) sandwich films were elaborated using the sol-gel spin coating\nprocess. The dielectric properties displayed excellent thermal stability with\nthe temperature coefficient of capacitance, TCC, remaining within 10% between\n-50 C and 300 C. The high energy storage density, Wrec, of 11.8 J/cm3 observed\nin this sandwich films, is nearly twice as high as that of the BCZT films, with\nan efficiency, n, of 77% under a weak electric field of 800 kV/cm. Furthermore,\nthe stability of Wrec and n was observed along the studied temperature interval\nmaking them promising candidates for high-temperature energy storage\ncapacitors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ba0.85Ca0.15Zr0.1Ti0.9O3/La0.8Sr0.2MnO3/Ba0.85Ca0.15Zr0.1Ti0.9O3\n(BCZT/LSMO/BCZT) sandwich films were elaborated using the sol-gel spin coating\nprocess. The dielectric properties displayed excellent thermal stability with\nthe temperature coefficient of capacitance, TCC, remaining within 10% between\n-50 C and 300 C. The high energy storage density, Wrec, of 11.8 J/cm3 observed\nin this sandwich films, is nearly twice as high as that of the BCZT films, with\nan efficiency, n, of 77% under a weak electric field of 800 kV/cm. Furthermore,\nthe stability of Wrec and n was observed along the studied temperature interval\nmaking them promising candidates for high-temperature energy storage\ncapacitors."
                },
                "authors": [
                    {
                        "name": "Afaak Lakouader"
                    },
                    {
                        "name": "Abdelilah Lahmar"
                    },
                    {
                        "name": "Spela Kunej"
                    },
                    {
                        "name": "Daoud Mezzane"
                    },
                    {
                        "name": "Jamal Belhadi"
                    },
                    {
                        "name": "El Hassan Choukri"
                    },
                    {
                        "name": "Lahoucine Hajji"
                    },
                    {
                        "name": "Mbarek Amjoud"
                    },
                    {
                        "name": "Zdravko Kutnjak"
                    },
                    {
                        "name": "Igor A. Lukyanchuk"
                    },
                    {
                        "name": "Mimoun El Marssi"
                    }
                ],
                "author_detail": {
                    "name": "Mimoun El Marssi"
                },
                "author": "Mimoun El Marssi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08941v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08941v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08879v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08879v1",
                "updated": "2025-03-11T20:45:02Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    20,
                    45,
                    2,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T20:45:02Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    20,
                    45,
                    2,
                    1,
                    70,
                    0
                ],
                "title": "LLMs Know What to Drop: Self-Attention Guided KV Cache Eviction for\n  Efficient Long-Context Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs Know What to Drop: Self-Attention Guided KV Cache Eviction for\n  Efficient Long-Context Inference"
                },
                "summary": "Efficient long-context inference is critical as large language models (LLMs)\nadopt context windows of ranging from 128K to 1M tokens. However, the growing\nkey-value (KV) cache and the high computational complexity of attention create\nsignificant bottlenecks in memory usage and latency. In this paper, we find\nthat attention in diverse long-context tasks exhibits sparsity, and LLMs\nimplicitly \"know\" which tokens can be dropped or evicted at the head level\nafter the pre-filling stage. Based on this insight, we propose Self-Attention\nGuided Eviction~(SAGE-KV), a simple and effective KV eviction cache method for\nlong-context inference. After prefilling, our method performs a one-time top-k\nselection at both the token and head levels to compress the KV cache, enabling\nefficient inference with the reduced cache. Evaluations on LongBench and three\nlong-context LLMs (Llama3.1-8B-Instruct-128k, Llama3-8B-Prolong-512k-Instruct,\nand Qwen2.5-7B-Instruct-128k) show that SAGE-KV maintains accuracy comparable\nto full attention while significantly improving efficiency. Specifically,\nSAGE-KV achieves 4x higher memory efficiency with improved accuracy over the\nstatic KV cache selection method StreamLLM, and 2x higher memory efficiency\nwith better accuracy than the dynamic KV cache selection method Quest.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient long-context inference is critical as large language models (LLMs)\nadopt context windows of ranging from 128K to 1M tokens. However, the growing\nkey-value (KV) cache and the high computational complexity of attention create\nsignificant bottlenecks in memory usage and latency. In this paper, we find\nthat attention in diverse long-context tasks exhibits sparsity, and LLMs\nimplicitly \"know\" which tokens can be dropped or evicted at the head level\nafter the pre-filling stage. Based on this insight, we propose Self-Attention\nGuided Eviction~(SAGE-KV), a simple and effective KV eviction cache method for\nlong-context inference. After prefilling, our method performs a one-time top-k\nselection at both the token and head levels to compress the KV cache, enabling\nefficient inference with the reduced cache. Evaluations on LongBench and three\nlong-context LLMs (Llama3.1-8B-Instruct-128k, Llama3-8B-Prolong-512k-Instruct,\nand Qwen2.5-7B-Instruct-128k) show that SAGE-KV maintains accuracy comparable\nto full attention while significantly improving efficiency. Specifically,\nSAGE-KV achieves 4x higher memory efficiency with improved accuracy over the\nstatic KV cache selection method StreamLLM, and 2x higher memory efficiency\nwith better accuracy than the dynamic KV cache selection method Quest."
                },
                "authors": [
                    {
                        "name": "Guangtao Wang"
                    },
                    {
                        "name": "Shubhangi Upasani"
                    },
                    {
                        "name": "Chen Wu"
                    },
                    {
                        "name": "Darshan Gandhi"
                    },
                    {
                        "name": "Jonathan Li"
                    },
                    {
                        "name": "Changran Hu"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Urmish Thakker"
                    }
                ],
                "author_detail": {
                    "name": "Urmish Thakker"
                },
                "author": "Urmish Thakker",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08879v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08879v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08461v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08461v1",
                "updated": "2025-03-11T14:10:58Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    14,
                    10,
                    58,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T14:10:58Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    14,
                    10,
                    58,
                    1,
                    70,
                    0
                ],
                "title": "FastCache: Optimizing Multimodal LLM Serving through Lightweight\n  KV-Cache Compression Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FastCache: Optimizing Multimodal LLM Serving through Lightweight\n  KV-Cache Compression Framework"
                },
                "summary": "Multi-modal Large Language Models (MLLMs) serving systems commonly employ\nKV-cache compression to reduce memory footprint. However, existing compression\nmethods introduce significant processing overhead and queuing delays,\nparticularly in concurrent serving scenarios. We present \\texttt{FastCache}, a\nnovel serving framework that effectively addresses these challenges through two\nkey innovations: (1) a dynamic batching strategy that optimizes request\nscheduling across prefill, compression, and decode stages, and (2) an efficient\nKV-cache memory pool mechanism that eliminates memory fragmentation while\nmaintaining high GPU utilization. Our comprehensive experiments on the GQA and\nMileBench datasets demonstrate that \\texttt{FastCache} achieves up to\n19.3$\\times$ reduction in Time-To-First-Token (TTFT) and 12.1$\\times$\nimprovement in throughput compared to state-of-the-art baselines. The system\nmaintains stable performance under high-concurrency scenarios (up to 40 req/s)\nwhile reducing average memory consumption by 20\\%. These results establish\n\\texttt{FastCache} as an efficient solution for real-world LLM serving systems\nwith KV-cache compression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-modal Large Language Models (MLLMs) serving systems commonly employ\nKV-cache compression to reduce memory footprint. However, existing compression\nmethods introduce significant processing overhead and queuing delays,\nparticularly in concurrent serving scenarios. We present \\texttt{FastCache}, a\nnovel serving framework that effectively addresses these challenges through two\nkey innovations: (1) a dynamic batching strategy that optimizes request\nscheduling across prefill, compression, and decode stages, and (2) an efficient\nKV-cache memory pool mechanism that eliminates memory fragmentation while\nmaintaining high GPU utilization. Our comprehensive experiments on the GQA and\nMileBench datasets demonstrate that \\texttt{FastCache} achieves up to\n19.3$\\times$ reduction in Time-To-First-Token (TTFT) and 12.1$\\times$\nimprovement in throughput compared to state-of-the-art baselines. The system\nmaintains stable performance under high-concurrency scenarios (up to 40 req/s)\nwhile reducing average memory consumption by 20\\%. These results establish\n\\texttt{FastCache} as an efficient solution for real-world LLM serving systems\nwith KV-cache compression."
                },
                "authors": [
                    {
                        "name": "Jianian Zhu"
                    },
                    {
                        "name": "Hang Wu"
                    },
                    {
                        "name": "Haojie Wang"
                    },
                    {
                        "name": "Yinghui Li"
                    },
                    {
                        "name": "Biao Hou"
                    },
                    {
                        "name": "Ruixuan Li"
                    },
                    {
                        "name": "Jidong Zhai"
                    }
                ],
                "author_detail": {
                    "name": "Jidong Zhai"
                },
                "author": "Jidong Zhai",
                "arxiv_comment": "14 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08461v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08461v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10319v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10319v2",
                "updated": "2025-03-11T14:02:04Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    14,
                    2,
                    4,
                    1,
                    70,
                    0
                ],
                "published": "2024-12-13T17:59:52Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    17,
                    59,
                    52,
                    4,
                    348,
                    0
                ],
                "title": "SCBench: A KV Cache-Centric Analysis of Long-Context Methods",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SCBench: A KV Cache-Centric Analysis of Long-Context Methods"
                },
                "summary": "Long-context LLMs have enabled numerous downstream applications but also\nintroduced significant challenges related to computational and memory\nefficiency. To address these challenges, optimizations for long-context\ninference have been developed, centered around the KV cache. However, existing\nbenchmarks often evaluate in single-request, neglecting the full lifecycle of\nthe KV cache in real-world use. This oversight is particularly critical, as KV\ncache reuse has become widely adopted in LLMs inference frameworks, such as\nvLLM and SGLang, as well as by LLM providers, including OpenAI, Microsoft,\nGoogle, and Anthropic. To address this gap, we introduce\nSCBench(SharedContextBench), a comprehensive benchmark for evaluating\nlong-context methods from a KV cachecentric perspective: 1) KV cache\ngeneration, 2) KV cache compression, 3) KV cache retrieval, 4) KV cache\nloading. Specifically, SCBench uses test examples with shared context, ranging\n12 tasks with two shared context modes, covering four categories of\nlong-context capabilities: string retrieval, semantic retrieval, global\ninformation, and multi-task. With it, we provide an extensive KV cache-centric\nanalysis of eight categories long-context solutions, including Gated Linear\nRNNs, Mamba-Attention hybrids, and efficient methods such as sparse attention,\nKV cache dropping, quantization, retrieval, loading, and prompt compression.\nThe evaluation is conducted on 8 long-context LLMs. Our findings show that\nsub-O(n) memory methods suffer in multi-turn scenarios, while sparse encoding\nwith O(n) memory and sub-O(n^2) pre-filling computation perform robustly.\nDynamic sparsity yields more expressive KV caches than static patterns, and\nlayer-level sparsity in hybrid architectures reduces memory usage with strong\nperformance. Additionally, we identify attention distribution shift issues in\nlong-generation scenarios. https://aka.ms/SCBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context LLMs have enabled numerous downstream applications but also\nintroduced significant challenges related to computational and memory\nefficiency. To address these challenges, optimizations for long-context\ninference have been developed, centered around the KV cache. However, existing\nbenchmarks often evaluate in single-request, neglecting the full lifecycle of\nthe KV cache in real-world use. This oversight is particularly critical, as KV\ncache reuse has become widely adopted in LLMs inference frameworks, such as\nvLLM and SGLang, as well as by LLM providers, including OpenAI, Microsoft,\nGoogle, and Anthropic. To address this gap, we introduce\nSCBench(SharedContextBench), a comprehensive benchmark for evaluating\nlong-context methods from a KV cachecentric perspective: 1) KV cache\ngeneration, 2) KV cache compression, 3) KV cache retrieval, 4) KV cache\nloading. Specifically, SCBench uses test examples with shared context, ranging\n12 tasks with two shared context modes, covering four categories of\nlong-context capabilities: string retrieval, semantic retrieval, global\ninformation, and multi-task. With it, we provide an extensive KV cache-centric\nanalysis of eight categories long-context solutions, including Gated Linear\nRNNs, Mamba-Attention hybrids, and efficient methods such as sparse attention,\nKV cache dropping, quantization, retrieval, loading, and prompt compression.\nThe evaluation is conducted on 8 long-context LLMs. Our findings show that\nsub-O(n) memory methods suffer in multi-turn scenarios, while sparse encoding\nwith O(n) memory and sub-O(n^2) pre-filling computation perform robustly.\nDynamic sparsity yields more expressive KV caches than static patterns, and\nlayer-level sparsity in hybrid architectures reduces memory usage with strong\nperformance. Additionally, we identify attention distribution shift issues in\nlong-generation scenarios. https://aka.ms/SCBench."
                },
                "authors": [
                    {
                        "name": "Yucheng Li"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Qianhui Wu"
                    },
                    {
                        "name": "Xufang Luo"
                    },
                    {
                        "name": "Surin Ahn"
                    },
                    {
                        "name": "Chengruidong Zhang"
                    },
                    {
                        "name": "Amir H. Abdi"
                    },
                    {
                        "name": "Dongsheng Li"
                    },
                    {
                        "name": "Jianfeng Gao"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Lili Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Lili Qiu"
                },
                "author": "Lili Qiu",
                "arxiv_comment": "Accepted at ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10319v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10319v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00857v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00857v3",
                "updated": "2025-03-11T13:13:11Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    13,
                    13,
                    11,
                    1,
                    70,
                    0
                ],
                "published": "2024-12-01T15:45:26Z",
                "published_parsed": [
                    2024,
                    12,
                    1,
                    15,
                    45,
                    26,
                    6,
                    336,
                    0
                ],
                "title": "Coherent Video Inpainting Using Optical Flow-Guided Efficient Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coherent Video Inpainting Using Optical Flow-Guided Efficient Diffusion"
                },
                "summary": "The text-guided video inpainting technique has significantly improved the\nperformance of content generation applications. A recent family for these\nimprovements uses diffusion models, which have become essential for achieving\nhigh-quality video inpainting results, yet they still face performance\nbottlenecks in temporal consistency and computational efficiency. This\nmotivates us to propose a new video inpainting framework using optical\nFlow-guided Efficient Diffusion (FloED) for higher video coherence.\nSpecifically, FloED employs a dual-branch architecture, where the time-agnostic\nflow branch restores corrupted flow first, and the multi-scale flow adapters\nprovide motion guidance to the main inpainting branch. Besides, a training-free\nlatent interpolation method is proposed to accelerate the multi-step denoising\nprocess using flow warping. With the flow attention cache mechanism, FLoED\nefficiently reduces the computational cost of incorporating optical flow.\nExtensive experiments on background restoration and object removal tasks show\nthat FloED outperforms state-of-the-art diffusion-based methods in both quality\nand efficiency. Our codes and models will be made publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The text-guided video inpainting technique has significantly improved the\nperformance of content generation applications. A recent family for these\nimprovements uses diffusion models, which have become essential for achieving\nhigh-quality video inpainting results, yet they still face performance\nbottlenecks in temporal consistency and computational efficiency. This\nmotivates us to propose a new video inpainting framework using optical\nFlow-guided Efficient Diffusion (FloED) for higher video coherence.\nSpecifically, FloED employs a dual-branch architecture, where the time-agnostic\nflow branch restores corrupted flow first, and the multi-scale flow adapters\nprovide motion guidance to the main inpainting branch. Besides, a training-free\nlatent interpolation method is proposed to accelerate the multi-step denoising\nprocess using flow warping. With the flow attention cache mechanism, FLoED\nefficiently reduces the computational cost of incorporating optical flow.\nExtensive experiments on background restoration and object removal tasks show\nthat FloED outperforms state-of-the-art diffusion-based methods in both quality\nand efficiency. Our codes and models will be made publicly available."
                },
                "authors": [
                    {
                        "name": "Bohai Gu"
                    },
                    {
                        "name": "Hao Luo"
                    },
                    {
                        "name": "Song Guo"
                    },
                    {
                        "name": "Peiran Dong"
                    },
                    {
                        "name": "Qihua Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Qihua Zhou"
                },
                "author": "Qihua Zhou",
                "arxiv_comment": "Project page: https://nevsnev.github.io/FloED/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00857v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00857v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07635v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07635v5",
                "updated": "2025-03-11T09:17:02Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    9,
                    17,
                    2,
                    1,
                    70,
                    0
                ],
                "published": "2024-11-12T08:30:59Z",
                "published_parsed": [
                    2024,
                    11,
                    12,
                    8,
                    30,
                    59,
                    1,
                    317,
                    0
                ],
                "title": "Breaking the Low-Rank Dilemma of Linear Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breaking the Low-Rank Dilemma of Linear Attention"
                },
                "summary": "The Softmax attention mechanism in Transformer models is notoriously\ncomputationally expensive, particularly due to its quadratic complexity, posing\nsignificant challenges in vision applications. In contrast, linear attention\nprovides a far more efficient solution by reducing the complexity to linear\nlevels. However, compared to Softmax attention, linear attention often\nexperiences significant performance degradation. Our experiments indicate that\nthis performance drop is due to the low-rank nature of linear attention's\nfeature map, which hinders its ability to adequately model complex spatial\ninformation. In this paper, to break the low-rank dilemma of linear attention,\nwe conduct rank analysis from two perspectives: the KV buffer and the output\nfeatures. Consequently, we introduce Rank-Augmented Linear Attention (RALA),\nwhich rivals the performance of Softmax attention while maintaining linear\ncomplexity and high efficiency. Based on RALA, we construct the Rank-Augmented\nVision Linear Transformer (RAVLT). Extensive experiments demonstrate that RAVLT\nachieves excellent performance across various vision tasks. Specifically,\nwithout using any additional labels, data, or supervision during training,\nRAVLT achieves an 84.4% Top-1 accuracy on ImageNet-1k with only 26M parameters\nand 4.6G FLOPs. This result significantly surpasses previous linear attention\nmechanisms, fully illustrating the potential of RALA. Code will be available at\nhttps://github.com/qhfan/RALA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Softmax attention mechanism in Transformer models is notoriously\ncomputationally expensive, particularly due to its quadratic complexity, posing\nsignificant challenges in vision applications. In contrast, linear attention\nprovides a far more efficient solution by reducing the complexity to linear\nlevels. However, compared to Softmax attention, linear attention often\nexperiences significant performance degradation. Our experiments indicate that\nthis performance drop is due to the low-rank nature of linear attention's\nfeature map, which hinders its ability to adequately model complex spatial\ninformation. In this paper, to break the low-rank dilemma of linear attention,\nwe conduct rank analysis from two perspectives: the KV buffer and the output\nfeatures. Consequently, we introduce Rank-Augmented Linear Attention (RALA),\nwhich rivals the performance of Softmax attention while maintaining linear\ncomplexity and high efficiency. Based on RALA, we construct the Rank-Augmented\nVision Linear Transformer (RAVLT). Extensive experiments demonstrate that RAVLT\nachieves excellent performance across various vision tasks. Specifically,\nwithout using any additional labels, data, or supervision during training,\nRAVLT achieves an 84.4% Top-1 accuracy on ImageNet-1k with only 26M parameters\nand 4.6G FLOPs. This result significantly surpasses previous linear attention\nmechanisms, fully illustrating the potential of RALA. Code will be available at\nhttps://github.com/qhfan/RALA."
                },
                "authors": [
                    {
                        "name": "Qihang Fan"
                    },
                    {
                        "name": "Huaibo Huang"
                    },
                    {
                        "name": "Ran He"
                    }
                ],
                "author_detail": {
                    "name": "Ran He"
                },
                "author": "Ran He",
                "arxiv_comment": "The paper is accepted by CVPR2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07635v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07635v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.06304v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.06304v2",
                "updated": "2025-03-11T03:26:20Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    3,
                    26,
                    20,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-08T18:42:34Z",
                "published_parsed": [
                    2025,
                    3,
                    8,
                    18,
                    42,
                    34,
                    5,
                    67,
                    0
                ],
                "title": "Optimization and Benchmarking of Monolithically Stackable Gain Cell\n  Memory for Last-Level Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimization and Benchmarking of Monolithically Stackable Gain Cell\n  Memory for Last-Level Cache"
                },
                "summary": "The Last Level Cache (LLC) is the processor's critical bridge between on-chip\nand off-chip memory levels - optimized for high density, high bandwidth, and\nlow operation energy. To date, high-density (HD) SRAM has been the conventional\ndevice of choice; however, with the slowing of transistor scaling, as reflected\nin the industry's almost identical HD SRAM cell size from 5 nm to 3 nm,\nalternative solutions such as 3D stacking with advanced packaging like hybrid\nbonding are pursued (as demonstrated in AMD's V-cache). Escalating data demands\nnecessitate ultra-large on-chip caches to decrease costly off-chip memory\nmovement, pushing the exploration of device technology toward monolithic 3D\n(M3D) integration where transistors can be stacked in the back-end-of-line\n(BEOL) at the interconnect level. M3D integration requires fabrication\ntechniques compatible with a low thermal budget (<400 degC). Among promising\nBEOL device candidates are amorphous oxide semiconductor (AOS) transistors,\nparticularly desirable for their ultra-low leakage (<fA/um), enabling\npersistent data retention (>seconds) when used in a gain-cell configuration.\nThis paper examines device, circuit, and system-level tradeoffs when optimizing\nBEOL-compatible AOS-based 2-transistor gain cell (2T-GC) for LLC. A cache\nearly-exploration tool, NS-Cache, is developed to model caches in advanced 7\nand 3 nm nodes and is integrated with the Gem5 simulator to systematically\nbenchmark the impact of the newfound density/performance when compared to\nHD-SRAM, MRAM, and 1T1C eDRAM alternatives for LLC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Last Level Cache (LLC) is the processor's critical bridge between on-chip\nand off-chip memory levels - optimized for high density, high bandwidth, and\nlow operation energy. To date, high-density (HD) SRAM has been the conventional\ndevice of choice; however, with the slowing of transistor scaling, as reflected\nin the industry's almost identical HD SRAM cell size from 5 nm to 3 nm,\nalternative solutions such as 3D stacking with advanced packaging like hybrid\nbonding are pursued (as demonstrated in AMD's V-cache). Escalating data demands\nnecessitate ultra-large on-chip caches to decrease costly off-chip memory\nmovement, pushing the exploration of device technology toward monolithic 3D\n(M3D) integration where transistors can be stacked in the back-end-of-line\n(BEOL) at the interconnect level. M3D integration requires fabrication\ntechniques compatible with a low thermal budget (<400 degC). Among promising\nBEOL device candidates are amorphous oxide semiconductor (AOS) transistors,\nparticularly desirable for their ultra-low leakage (<fA/um), enabling\npersistent data retention (>seconds) when used in a gain-cell configuration.\nThis paper examines device, circuit, and system-level tradeoffs when optimizing\nBEOL-compatible AOS-based 2-transistor gain cell (2T-GC) for LLC. A cache\nearly-exploration tool, NS-Cache, is developed to model caches in advanced 7\nand 3 nm nodes and is integrated with the Gem5 simulator to systematically\nbenchmark the impact of the newfound density/performance when compared to\nHD-SRAM, MRAM, and 1T1C eDRAM alternatives for LLC."
                },
                "authors": [
                    {
                        "name": "Faaiq Waqar"
                    },
                    {
                        "name": "Jungyoun Kwak"
                    },
                    {
                        "name": "Junmo Lee"
                    },
                    {
                        "name": "Minji Shon"
                    },
                    {
                        "name": "Mohammadhosein Gholamrezaei"
                    },
                    {
                        "name": "Kevin Skadron"
                    },
                    {
                        "name": "Shimeng Yu"
                    }
                ],
                "author_detail": {
                    "name": "Shimeng Yu"
                },
                "author": "Shimeng Yu",
                "arxiv_comment": "14 pages, 15 Figures, 6 Tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.06304v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.06304v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.ET",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "B.8.2; B.3.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07545v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07545v1",
                "updated": "2025-03-10T17:12:47Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    17,
                    12,
                    47,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T17:12:47Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    17,
                    12,
                    47,
                    0,
                    69,
                    0
                ],
                "title": "Queueing, Predictions, and LLMs: Challenges and Open Problems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Queueing, Predictions, and LLMs: Challenges and Open Problems"
                },
                "summary": "Queueing systems present many opportunities for applying machine-learning\npredictions, such as estimated service times, to improve system performance.\nThis integration raises numerous open questions about how predictions can be\neffectively leveraged to improve scheduling decisions. Recent studies explore\nqueues with predicted service times, typically aiming to minimize job time in\nthe system. We review these works, highlight the effectiveness of predictions,\nand present open questions on queue performance. We then move to consider an\nimportant practical example of using predictions in scheduling, namely Large\nLanguage Model (LLM) systems, which presents novel scheduling challenges and\nhighlights the potential for predictions to improve performance. In particular,\nwe consider LLMs performing inference. Inference requests (jobs) in LLM systems\nare inherently complex; they have variable inference times, dynamic memory\nfootprints that are constrained by key-value (KV) store memory limitations, and\nmultiple possible preemption approaches that affect performance differently. We\nprovide background on the important aspects of scheduling in LLM systems, and\nintroduce new models and open problems that arise from them. We argue that\nthere are significant opportunities for applying insights and analysis from\nqueueing theory to scheduling in LLM systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Queueing systems present many opportunities for applying machine-learning\npredictions, such as estimated service times, to improve system performance.\nThis integration raises numerous open questions about how predictions can be\neffectively leveraged to improve scheduling decisions. Recent studies explore\nqueues with predicted service times, typically aiming to minimize job time in\nthe system. We review these works, highlight the effectiveness of predictions,\nand present open questions on queue performance. We then move to consider an\nimportant practical example of using predictions in scheduling, namely Large\nLanguage Model (LLM) systems, which presents novel scheduling challenges and\nhighlights the potential for predictions to improve performance. In particular,\nwe consider LLMs performing inference. Inference requests (jobs) in LLM systems\nare inherently complex; they have variable inference times, dynamic memory\nfootprints that are constrained by key-value (KV) store memory limitations, and\nmultiple possible preemption approaches that affect performance differently. We\nprovide background on the important aspects of scheduling in LLM systems, and\nintroduce new models and open problems that arise from them. We argue that\nthere are significant opportunities for applying insights and analysis from\nqueueing theory to scheduling in LLM systems."
                },
                "authors": [
                    {
                        "name": "Michael Mitzenmacher"
                    },
                    {
                        "name": "Rana Shahout"
                    }
                ],
                "author_detail": {
                    "name": "Rana Shahout"
                },
                "author": "Rana Shahout",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07545v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07545v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2503.18944v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18944v1",
                "updated": "2025-03-24T17:59:11Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    17,
                    59,
                    11,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T17:59:11Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    17,
                    59,
                    11,
                    0,
                    83,
                    0
                ],
                "title": "DINO in the Room: Leveraging 2D Foundation Models for 3D Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DINO in the Room: Leveraging 2D Foundation Models for 3D Segmentation"
                },
                "summary": "Vision foundation models (VFMs) trained on large-scale image datasets provide\nhigh-quality features that have significantly advanced 2D visual recognition.\nHowever, their potential in 3D vision remains largely untapped, despite the\ncommon availability of 2D images alongside 3D point cloud datasets. While\nsignificant research has been dedicated to 2D-3D fusion, recent\nstate-of-the-art 3D methods predominantly focus on 3D data, leaving the\nintegration of VFMs into 3D models underexplored. In this work, we challenge\nthis trend by introducing DITR, a simple yet effective approach that extracts\n2D foundation model features, projects them to 3D, and finally injects them\ninto a 3D point cloud segmentation model. DITR achieves state-of-the-art\nresults on both indoor and outdoor 3D semantic segmentation benchmarks. To\nenable the use of VFMs even when images are unavailable during inference, we\nfurther propose to distill 2D foundation models into a 3D backbone as a\npretraining task. By initializing the 3D backbone with knowledge distilled from\n2D VFMs, we create a strong basis for downstream 3D segmentation tasks,\nultimately boosting performance across various datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision foundation models (VFMs) trained on large-scale image datasets provide\nhigh-quality features that have significantly advanced 2D visual recognition.\nHowever, their potential in 3D vision remains largely untapped, despite the\ncommon availability of 2D images alongside 3D point cloud datasets. While\nsignificant research has been dedicated to 2D-3D fusion, recent\nstate-of-the-art 3D methods predominantly focus on 3D data, leaving the\nintegration of VFMs into 3D models underexplored. In this work, we challenge\nthis trend by introducing DITR, a simple yet effective approach that extracts\n2D foundation model features, projects them to 3D, and finally injects them\ninto a 3D point cloud segmentation model. DITR achieves state-of-the-art\nresults on both indoor and outdoor 3D semantic segmentation benchmarks. To\nenable the use of VFMs even when images are unavailable during inference, we\nfurther propose to distill 2D foundation models into a 3D backbone as a\npretraining task. By initializing the 3D backbone with knowledge distilled from\n2D VFMs, we create a strong basis for downstream 3D segmentation tasks,\nultimately boosting performance across various datasets."
                },
                "authors": [
                    {
                        "name": "Karim Abou Zeid"
                    },
                    {
                        "name": "Kadir Yilmaz"
                    },
                    {
                        "name": "Daan de Geus"
                    },
                    {
                        "name": "Alexander Hermans"
                    },
                    {
                        "name": "David Adrian"
                    },
                    {
                        "name": "Timm Linder"
                    },
                    {
                        "name": "Bastian Leibe"
                    }
                ],
                "author_detail": {
                    "name": "Bastian Leibe"
                },
                "author": "Bastian Leibe",
                "arxiv_comment": "Project page at https://vision.rwth-aachen.de/DITR",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18944v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18944v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18943v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18943v1",
                "updated": "2025-03-24T17:59:07Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    17,
                    59,
                    7,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T17:59:07Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    17,
                    59,
                    7,
                    0,
                    83,
                    0
                ],
                "title": "SlowFast-LLaVA-1.5: A Family of Token-Efficient Video Large Language\n  Models for Long-Form Video Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SlowFast-LLaVA-1.5: A Family of Token-Efficient Video Large Language\n  Models for Long-Form Video Understanding"
                },
                "summary": "We introduce SlowFast-LLaVA-1.5 (abbreviated as SF-LLaVA-1.5), a family of\nvideo large language models (LLMs) offering a token-efficient solution for\nlong-form video understanding. This model family employs the two-stream\nSlowFast mechanism, enabling efficient modeling of long-range temporal context\nto meet the demand for lightweight, mobile-friendly Video LLMs. We provide\nmodels ranging from 1B to 7B parameters, optimized through a streamlined\ntraining pipeline and a high-quality data mixture composed of publicly\navailable datasets. Experimental results demonstrate that SF-LLaVA-1.5 achieves\ncompetitive performance on a wide range of video and image benchmarks, with\nrobust results across all model sizes. Notably, SF-LLaVA-1.5 achieves\nstate-of-the-art results in long-form video understanding (e.g., LongVideoBench\nand MLVU) and excels at small scales (1B and 3B) across various video\nbenchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce SlowFast-LLaVA-1.5 (abbreviated as SF-LLaVA-1.5), a family of\nvideo large language models (LLMs) offering a token-efficient solution for\nlong-form video understanding. This model family employs the two-stream\nSlowFast mechanism, enabling efficient modeling of long-range temporal context\nto meet the demand for lightweight, mobile-friendly Video LLMs. We provide\nmodels ranging from 1B to 7B parameters, optimized through a streamlined\ntraining pipeline and a high-quality data mixture composed of publicly\navailable datasets. Experimental results demonstrate that SF-LLaVA-1.5 achieves\ncompetitive performance on a wide range of video and image benchmarks, with\nrobust results across all model sizes. Notably, SF-LLaVA-1.5 achieves\nstate-of-the-art results in long-form video understanding (e.g., LongVideoBench\nand MLVU) and excels at small scales (1B and 3B) across various video\nbenchmarks."
                },
                "authors": [
                    {
                        "name": "Mingze Xu"
                    },
                    {
                        "name": "Mingfei Gao"
                    },
                    {
                        "name": "Shiyu Li"
                    },
                    {
                        "name": "Jiasen Lu"
                    },
                    {
                        "name": "Zhe Gan"
                    },
                    {
                        "name": "Zhengfeng Lai"
                    },
                    {
                        "name": "Meng Cao"
                    },
                    {
                        "name": "Kai Kang"
                    },
                    {
                        "name": "Yinfei Yang"
                    },
                    {
                        "name": "Afshin Dehghan"
                    }
                ],
                "author_detail": {
                    "name": "Afshin Dehghan"
                },
                "author": "Afshin Dehghan",
                "arxiv_comment": "Technical report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18943v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18943v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18942v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18942v1",
                "updated": "2025-03-24T17:59:04Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    17,
                    59,
                    4,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T17:59:04Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    17,
                    59,
                    4,
                    0,
                    83,
                    0
                ],
                "title": "Video-T1: Test-Time Scaling for Video Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video-T1: Test-Time Scaling for Video Generation"
                },
                "summary": "With the scale capability of increasing training data, model size, and\ncomputational cost, video generation has achieved impressive results in digital\ncreation, enabling users to express creativity across various domains.\nRecently, researchers in Large Language Models (LLMs) have expanded the scaling\nto test-time, which can significantly improve LLM performance by using more\ninference-time computation. Instead of scaling up video foundation models\nthrough expensive training costs, we explore the power of Test-Time Scaling\n(TTS) in video generation, aiming to answer the question: if a video generation\nmodel is allowed to use non-trivial amount of inference-time compute, how much\ncan it improve generation quality given a challenging text prompt. In this\nwork, we reinterpret the test-time scaling of video generation as a searching\nproblem to sample better trajectories from Gaussian noise space to the target\nvideo distribution. Specifically, we build the search space with test-time\nverifiers to provide feedback and heuristic algorithms to guide searching\nprocess. Given a text prompt, we first explore an intuitive linear search\nstrategy by increasing noise candidates at inference time. As full-step\ndenoising all frames simultaneously requires heavy test-time computation costs,\nwe further design a more efficient TTS method for video generation called\nTree-of-Frames (ToF) that adaptively expands and prunes video branches in an\nautoregressive manner. Extensive experiments on text-conditioned video\ngeneration benchmarks demonstrate that increasing test-time compute\nconsistently leads to significant improvements in the quality of videos.\nProject page: https://liuff19.github.io/Video-T1",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the scale capability of increasing training data, model size, and\ncomputational cost, video generation has achieved impressive results in digital\ncreation, enabling users to express creativity across various domains.\nRecently, researchers in Large Language Models (LLMs) have expanded the scaling\nto test-time, which can significantly improve LLM performance by using more\ninference-time computation. Instead of scaling up video foundation models\nthrough expensive training costs, we explore the power of Test-Time Scaling\n(TTS) in video generation, aiming to answer the question: if a video generation\nmodel is allowed to use non-trivial amount of inference-time compute, how much\ncan it improve generation quality given a challenging text prompt. In this\nwork, we reinterpret the test-time scaling of video generation as a searching\nproblem to sample better trajectories from Gaussian noise space to the target\nvideo distribution. Specifically, we build the search space with test-time\nverifiers to provide feedback and heuristic algorithms to guide searching\nprocess. Given a text prompt, we first explore an intuitive linear search\nstrategy by increasing noise candidates at inference time. As full-step\ndenoising all frames simultaneously requires heavy test-time computation costs,\nwe further design a more efficient TTS method for video generation called\nTree-of-Frames (ToF) that adaptively expands and prunes video branches in an\nautoregressive manner. Extensive experiments on text-conditioned video\ngeneration benchmarks demonstrate that increasing test-time compute\nconsistently leads to significant improvements in the quality of videos.\nProject page: https://liuff19.github.io/Video-T1"
                },
                "authors": [
                    {
                        "name": "Fangfu Liu"
                    },
                    {
                        "name": "Hanyang Wang"
                    },
                    {
                        "name": "Yimo Cai"
                    },
                    {
                        "name": "Kaiyan Zhang"
                    },
                    {
                        "name": "Xiaohang Zhan"
                    },
                    {
                        "name": "Yueqi Duan"
                    }
                ],
                "author_detail": {
                    "name": "Yueqi Duan"
                },
                "author": "Yueqi Duan",
                "arxiv_comment": "Project page: https://liuff19.github.io/Video-T1",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18942v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18942v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18941v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18941v1",
                "updated": "2025-03-24T17:59:03Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    17,
                    59,
                    3,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T17:59:03Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    17,
                    59,
                    3,
                    0,
                    83,
                    0
                ],
                "title": "Exploring Training and Inference Scaling Laws in Generative Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Training and Inference Scaling Laws in Generative Retrieval"
                },
                "summary": "Generative retrieval has emerged as a novel paradigm that leverages large\nlanguage models (LLMs) to autoregressively generate document identifiers.\nAlthough promising, the mechanisms that underpin its performance and\nscalability remain largely unclear. We conduct a systematic investigation of\ntraining and inference scaling laws in generative retrieval, exploring how\nmodel size, training data scale, and inference-time compute jointly influence\nretrieval performance. To address the lack of suitable metrics, we propose a\nnovel evaluation measure inspired by contrastive entropy and generation loss,\nproviding a continuous performance signal that enables robust comparisons\nacross diverse generative retrieval methods. Our experiments show that\nn-gram-based methods demonstrate strong alignment with both training and\ninference scaling laws, especially when paired with larger LLMs. Furthermore,\nincreasing inference computation yields substantial performance gains,\nrevealing that generative retrieval can significantly benefit from higher\ncompute budgets at inference. Across these settings, LLaMA models consistently\noutperform T5 models, suggesting a particular advantage for larger decoder-only\nmodels in generative retrieval. Taken together, our findings underscore that\nmodel sizes, data availability, and inference computation interact to unlock\nthe full potential of generative retrieval, offering new insights for designing\nand optimizing future systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative retrieval has emerged as a novel paradigm that leverages large\nlanguage models (LLMs) to autoregressively generate document identifiers.\nAlthough promising, the mechanisms that underpin its performance and\nscalability remain largely unclear. We conduct a systematic investigation of\ntraining and inference scaling laws in generative retrieval, exploring how\nmodel size, training data scale, and inference-time compute jointly influence\nretrieval performance. To address the lack of suitable metrics, we propose a\nnovel evaluation measure inspired by contrastive entropy and generation loss,\nproviding a continuous performance signal that enables robust comparisons\nacross diverse generative retrieval methods. Our experiments show that\nn-gram-based methods demonstrate strong alignment with both training and\ninference scaling laws, especially when paired with larger LLMs. Furthermore,\nincreasing inference computation yields substantial performance gains,\nrevealing that generative retrieval can significantly benefit from higher\ncompute budgets at inference. Across these settings, LLaMA models consistently\noutperform T5 models, suggesting a particular advantage for larger decoder-only\nmodels in generative retrieval. Taken together, our findings underscore that\nmodel sizes, data availability, and inference computation interact to unlock\nthe full potential of generative retrieval, offering new insights for designing\nand optimizing future systems."
                },
                "authors": [
                    {
                        "name": "Hongru Cai"
                    },
                    {
                        "name": "Yongqi Li"
                    },
                    {
                        "name": "Ruifeng Yuan"
                    },
                    {
                        "name": "Wenjie Wang"
                    },
                    {
                        "name": "Zhen Zhang"
                    },
                    {
                        "name": "Wenjie Li"
                    },
                    {
                        "name": "Tat-Seng Chua"
                    }
                ],
                "author_detail": {
                    "name": "Tat-Seng Chua"
                },
                "author": "Tat-Seng Chua",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18941v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18941v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18940v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18940v1",
                "updated": "2025-03-24T17:59:02Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    17,
                    59,
                    2,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T17:59:02Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    17,
                    59,
                    2,
                    0,
                    83,
                    0
                ],
                "title": "Training-free Diffusion Acceleration with Bottleneck Sampling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training-free Diffusion Acceleration with Bottleneck Sampling"
                },
                "summary": "Diffusion models have demonstrated remarkable capabilities in visual content\ngeneration but remain challenging to deploy due to their high computational\ncost during inference. This computational burden primarily arises from the\nquadratic complexity of self-attention with respect to image or video\nresolution. While existing acceleration methods often compromise output quality\nor necessitate costly retraining, we observe that most diffusion models are\npre-trained at lower resolutions, presenting an opportunity to exploit these\nlow-resolution priors for more efficient inference without degrading\nperformance. In this work, we introduce Bottleneck Sampling, a training-free\nframework that leverages low-resolution priors to reduce computational overhead\nwhile preserving output fidelity. Bottleneck Sampling follows a high-low-high\ndenoising workflow: it performs high-resolution denoising in the initial and\nfinal stages while operating at lower resolutions in intermediate steps. To\nmitigate aliasing and blurring artifacts, we further refine the resolution\ntransition points and adaptively shift the denoising timesteps at each stage.\nWe evaluate Bottleneck Sampling on both image and video generation tasks, where\nextensive experiments demonstrate that it accelerates inference by up to\n3$\\times$ for image generation and 2.5$\\times$ for video generation, all while\nmaintaining output quality comparable to the standard full-resolution sampling\nprocess across multiple evaluation metrics. Code is available at:\nhttps://github.com/tyfeld/Bottleneck-Sampling",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have demonstrated remarkable capabilities in visual content\ngeneration but remain challenging to deploy due to their high computational\ncost during inference. This computational burden primarily arises from the\nquadratic complexity of self-attention with respect to image or video\nresolution. While existing acceleration methods often compromise output quality\nor necessitate costly retraining, we observe that most diffusion models are\npre-trained at lower resolutions, presenting an opportunity to exploit these\nlow-resolution priors for more efficient inference without degrading\nperformance. In this work, we introduce Bottleneck Sampling, a training-free\nframework that leverages low-resolution priors to reduce computational overhead\nwhile preserving output fidelity. Bottleneck Sampling follows a high-low-high\ndenoising workflow: it performs high-resolution denoising in the initial and\nfinal stages while operating at lower resolutions in intermediate steps. To\nmitigate aliasing and blurring artifacts, we further refine the resolution\ntransition points and adaptively shift the denoising timesteps at each stage.\nWe evaluate Bottleneck Sampling on both image and video generation tasks, where\nextensive experiments demonstrate that it accelerates inference by up to\n3$\\times$ for image generation and 2.5$\\times$ for video generation, all while\nmaintaining output quality comparable to the standard full-resolution sampling\nprocess across multiple evaluation metrics. Code is available at:\nhttps://github.com/tyfeld/Bottleneck-Sampling"
                },
                "authors": [
                    {
                        "name": "Ye Tian"
                    },
                    {
                        "name": "Xin Xia"
                    },
                    {
                        "name": "Yuxi Ren"
                    },
                    {
                        "name": "Shanchuan Lin"
                    },
                    {
                        "name": "Xing Wang"
                    },
                    {
                        "name": "Xuefeng Xiao"
                    },
                    {
                        "name": "Yunhai Tong"
                    },
                    {
                        "name": "Ling Yang"
                    },
                    {
                        "name": "Bin Cui"
                    }
                ],
                "author_detail": {
                    "name": "Bin Cui"
                },
                "author": "Bin Cui",
                "arxiv_comment": "Code Repo: https://github.com/tyfeld/Bottleneck-Sampling ,Project\n  Page: https://tyfeld.github.io/BottleneckSampling.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18940v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18940v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18931v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18931v1",
                "updated": "2025-03-24T17:52:47Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    17,
                    52,
                    47,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T17:52:47Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    17,
                    52,
                    47,
                    0,
                    83,
                    0
                ],
                "title": "CoMP: Continual Multimodal Pre-training for Vision Foundation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoMP: Continual Multimodal Pre-training for Vision Foundation Models"
                },
                "summary": "Pre-trained Vision Foundation Models (VFMs) provide strong visual\nrepresentations for a wide range of applications. In this paper, we continually\npre-train prevailing VFMs in a multimodal manner such that they can\neffortlessly process visual inputs of varying sizes and produce visual\nrepresentations that are more aligned with language representations, regardless\nof their original pre-training process. To this end, we introduce CoMP, a\ncarefully designed multimodal pre-training pipeline. CoMP uses a Continual\nRotary Position Embedding to support native resolution continual pre-training,\nand an Alignment Loss between visual and textual features through language\nprototypes to align multimodal representations. By three-stage training, our\nVFMs achieve remarkable improvements not only in multimodal understanding but\nalso in other downstream tasks such as classification and segmentation.\nRemarkably, CoMP-SigLIP achieves scores of 66.7 on ChartQA and 75.9 on DocVQA\nwith a 0.5B LLM, while maintaining an 87.4% accuracy on ImageNet-1K and a 49.5\nmIoU on ADE20K under frozen chunk evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pre-trained Vision Foundation Models (VFMs) provide strong visual\nrepresentations for a wide range of applications. In this paper, we continually\npre-train prevailing VFMs in a multimodal manner such that they can\neffortlessly process visual inputs of varying sizes and produce visual\nrepresentations that are more aligned with language representations, regardless\nof their original pre-training process. To this end, we introduce CoMP, a\ncarefully designed multimodal pre-training pipeline. CoMP uses a Continual\nRotary Position Embedding to support native resolution continual pre-training,\nand an Alignment Loss between visual and textual features through language\nprototypes to align multimodal representations. By three-stage training, our\nVFMs achieve remarkable improvements not only in multimodal understanding but\nalso in other downstream tasks such as classification and segmentation.\nRemarkably, CoMP-SigLIP achieves scores of 66.7 on ChartQA and 75.9 on DocVQA\nwith a 0.5B LLM, while maintaining an 87.4% accuracy on ImageNet-1K and a 49.5\nmIoU on ADE20K under frozen chunk evaluation."
                },
                "authors": [
                    {
                        "name": "Yitong Chen"
                    },
                    {
                        "name": "Lingchen Meng"
                    },
                    {
                        "name": "Wujian Peng"
                    },
                    {
                        "name": "Zuxuan Wu"
                    },
                    {
                        "name": "Yu-Gang Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Yu-Gang Jiang"
                },
                "author": "Yu-Gang Jiang",
                "arxiv_comment": "Code is available in https://github.com/SliMM-X/CoMP-MM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18931v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18931v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17236v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17236v2",
                "updated": "2025-03-24T17:51:54Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    17,
                    51,
                    54,
                    0,
                    83,
                    0
                ],
                "published": "2024-10-22T17:54:45Z",
                "published_parsed": [
                    2024,
                    10,
                    22,
                    17,
                    54,
                    45,
                    1,
                    296,
                    0
                ],
                "title": "Large Language Models Empowered Personalized Web Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models Empowered Personalized Web Agents"
                },
                "summary": "Web agents have emerged as a promising direction to automate Web task\ncompletion based on user instructions, significantly enhancing user experience.\nRecently, Web agents have evolved from traditional agents to Large Language\nModels (LLMs)-based Web agents. Despite their success, existing LLM-based Web\nagents overlook the importance of personalized data (e.g., user profiles and\nhistorical Web behaviors) in assisting the understanding of users' personalized\ninstructions and executing customized actions. To overcome the limitation, we\nfirst formulate the task of LLM-empowered personalized Web agents, which\nintegrate personalized data and user instructions to personalize instruction\ncomprehension and action execution. To address the absence of a comprehensive\nevaluation benchmark, we construct a Personalized Web Agent Benchmark\n(PersonalWAB), featuring user instructions, personalized user data, Web\nfunctions, and two evaluation paradigms across three personalized Web tasks.\nMoreover, we propose a Personalized User Memory-enhanced Alignment (PUMA)\nframework to adapt LLMs to the personalized Web agent task. PUMA utilizes a\nmemory bank with a task-specific retrieval strategy to filter relevant\nhistorical Web behaviors. Based on the behaviors, PUMA then aligns LLMs for\npersonalized action execution through fine-tuning and direct preference\noptimization. Extensive experiments validate the superiority of PUMA over\nexisting Web agents on PersonalWAB.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Web agents have emerged as a promising direction to automate Web task\ncompletion based on user instructions, significantly enhancing user experience.\nRecently, Web agents have evolved from traditional agents to Large Language\nModels (LLMs)-based Web agents. Despite their success, existing LLM-based Web\nagents overlook the importance of personalized data (e.g., user profiles and\nhistorical Web behaviors) in assisting the understanding of users' personalized\ninstructions and executing customized actions. To overcome the limitation, we\nfirst formulate the task of LLM-empowered personalized Web agents, which\nintegrate personalized data and user instructions to personalize instruction\ncomprehension and action execution. To address the absence of a comprehensive\nevaluation benchmark, we construct a Personalized Web Agent Benchmark\n(PersonalWAB), featuring user instructions, personalized user data, Web\nfunctions, and two evaluation paradigms across three personalized Web tasks.\nMoreover, we propose a Personalized User Memory-enhanced Alignment (PUMA)\nframework to adapt LLMs to the personalized Web agent task. PUMA utilizes a\nmemory bank with a task-specific retrieval strategy to filter relevant\nhistorical Web behaviors. Based on the behaviors, PUMA then aligns LLMs for\npersonalized action execution through fine-tuning and direct preference\noptimization. Extensive experiments validate the superiority of PUMA over\nexisting Web agents on PersonalWAB."
                },
                "authors": [
                    {
                        "name": "Hongru Cai"
                    },
                    {
                        "name": "Yongqi Li"
                    },
                    {
                        "name": "Wenjie Wang"
                    },
                    {
                        "name": "Fengbin Zhu"
                    },
                    {
                        "name": "Xiaoyu Shen"
                    },
                    {
                        "name": "Wenjie Li"
                    },
                    {
                        "name": "Tat-Seng Chua"
                    }
                ],
                "author_detail": {
                    "name": "Tat-Seng Chua"
                },
                "author": "Tat-Seng Chua",
                "arxiv_comment": "Accepted to WWW 2025. The code and data are available on the project\n  website https://hongrucai.github.io/PersonalWAB/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17236v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17236v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18929v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18929v1",
                "updated": "2025-03-24T17:51:39Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    17,
                    51,
                    39,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T17:51:39Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    17,
                    51,
                    39,
                    0,
                    83,
                    0
                ],
                "title": "Trajectory Balance with Asynchrony: Decoupling Exploration and Learning\n  for Fast, Scalable LLM Post-Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trajectory Balance with Asynchrony: Decoupling Exploration and Learning\n  for Fast, Scalable LLM Post-Training"
                },
                "summary": "Reinforcement learning (RL) is a critical component of large language model\n(LLM) post-training. However, existing on-policy algorithms used for\npost-training are inherently incompatible with the use of experience replay\nbuffers, which can be populated scalably by distributed off-policy actors to\nenhance exploration as compute increases. We propose efficiently obtaining this\nbenefit of replay buffers via Trajectory Balance with Asynchrony (TBA), a\nmassively scalable LLM RL system. In contrast to existing approaches, TBA uses\na larger fraction of compute on search, constantly generating off-policy data\nfor a central replay buffer. A training node simultaneously samples data from\nthis buffer based on reward or recency to update the policy using Trajectory\nBalance (TB), a diversity-seeking RL objective introduced for GFlowNets. TBA\noffers three key advantages: (1) decoupled training and search, speeding up\ntraining wall-clock time by 4x or more; (2) improved diversity through\nlarge-scale off-policy sampling; and (3) scalable search for sparse reward\nsettings. On mathematical reasoning, preference-tuning, and automated\nred-teaming (diverse and representative post-training tasks), TBA produces\nspeed and performance improvements over strong baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning (RL) is a critical component of large language model\n(LLM) post-training. However, existing on-policy algorithms used for\npost-training are inherently incompatible with the use of experience replay\nbuffers, which can be populated scalably by distributed off-policy actors to\nenhance exploration as compute increases. We propose efficiently obtaining this\nbenefit of replay buffers via Trajectory Balance with Asynchrony (TBA), a\nmassively scalable LLM RL system. In contrast to existing approaches, TBA uses\na larger fraction of compute on search, constantly generating off-policy data\nfor a central replay buffer. A training node simultaneously samples data from\nthis buffer based on reward or recency to update the policy using Trajectory\nBalance (TB), a diversity-seeking RL objective introduced for GFlowNets. TBA\noffers three key advantages: (1) decoupled training and search, speeding up\ntraining wall-clock time by 4x or more; (2) improved diversity through\nlarge-scale off-policy sampling; and (3) scalable search for sparse reward\nsettings. On mathematical reasoning, preference-tuning, and automated\nred-teaming (diverse and representative post-training tasks), TBA produces\nspeed and performance improvements over strong baselines."
                },
                "authors": [
                    {
                        "name": "Brian R. Bartoldson"
                    },
                    {
                        "name": "Siddarth Venkatraman"
                    },
                    {
                        "name": "James Diffenderfer"
                    },
                    {
                        "name": "Moksh Jain"
                    },
                    {
                        "name": "Tal Ben-Nun"
                    },
                    {
                        "name": "Seanie Lee"
                    },
                    {
                        "name": "Minsu Kim"
                    },
                    {
                        "name": "Johan Obando-Ceron"
                    },
                    {
                        "name": "Yoshua Bengio"
                    },
                    {
                        "name": "Bhavya Kailkhura"
                    }
                ],
                "author_detail": {
                    "name": "Bhavya Kailkhura"
                },
                "author": "Bhavya Kailkhura",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18929v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18929v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18923v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18923v1",
                "updated": "2025-03-24T17:46:09Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    17,
                    46,
                    9,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T17:46:09Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    17,
                    46,
                    9,
                    0,
                    83,
                    0
                ],
                "title": "Video SimpleQA: Towards Factuality Evaluation in Large Video Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video SimpleQA: Towards Factuality Evaluation in Large Video Language\n  Models"
                },
                "summary": "Recent advancements in Large Video Language Models (LVLMs) have highlighted\ntheir potential for multi-modal understanding, yet evaluating their factual\ngrounding in video contexts remains a critical unsolved challenge. To address\nthis gap, we introduce Video SimpleQA, the first comprehensive benchmark\ntailored for factuality evaluation of LVLMs. Our work distinguishes from\nexisting video benchmarks through the following key features: 1) Knowledge\nrequired: demanding integration of external knowledge beyond the explicit\nnarrative; 2) Fact-seeking question: targeting objective, undisputed events or\nrelationships, avoiding subjective interpretation; 3) Definitive & short-form\nanswer: Answers are crafted as unambiguous and definitively correct in a short\nformat, enabling automated evaluation through LLM-as-a-judge frameworks with\nminimal scoring variance; 4) External-source verified: All annotations undergo\nrigorous validation against authoritative external references to ensure the\nreliability; 5) Temporal reasoning required: The annotated question types\nencompass both static single-frame understanding and dynamic temporal\nreasoning, explicitly evaluating LVLMs factuality under the long-context\ndependencies. We extensively evaluate 41 state-of-the-art LVLMs and summarize\nkey findings as follows: 1) Current LVLMs exhibit notable deficiencies in\nfactual adherence, particularly for open-source models. The best-performing\nmodel Gemini-1.5-Pro achieves merely an F-score of 54.4%; 2) Test-time compute\nparadigms show insignificant performance gains, revealing fundamental\nconstraints for enhancing factuality through post-hoc computation; 3)\nRetrieval-Augmented Generation demonstrates consistent improvements at the cost\nof additional inference time overhead, presenting a critical\nefficiency-performance trade-off.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Video Language Models (LVLMs) have highlighted\ntheir potential for multi-modal understanding, yet evaluating their factual\ngrounding in video contexts remains a critical unsolved challenge. To address\nthis gap, we introduce Video SimpleQA, the first comprehensive benchmark\ntailored for factuality evaluation of LVLMs. Our work distinguishes from\nexisting video benchmarks through the following key features: 1) Knowledge\nrequired: demanding integration of external knowledge beyond the explicit\nnarrative; 2) Fact-seeking question: targeting objective, undisputed events or\nrelationships, avoiding subjective interpretation; 3) Definitive & short-form\nanswer: Answers are crafted as unambiguous and definitively correct in a short\nformat, enabling automated evaluation through LLM-as-a-judge frameworks with\nminimal scoring variance; 4) External-source verified: All annotations undergo\nrigorous validation against authoritative external references to ensure the\nreliability; 5) Temporal reasoning required: The annotated question types\nencompass both static single-frame understanding and dynamic temporal\nreasoning, explicitly evaluating LVLMs factuality under the long-context\ndependencies. We extensively evaluate 41 state-of-the-art LVLMs and summarize\nkey findings as follows: 1) Current LVLMs exhibit notable deficiencies in\nfactual adherence, particularly for open-source models. The best-performing\nmodel Gemini-1.5-Pro achieves merely an F-score of 54.4%; 2) Test-time compute\nparadigms show insignificant performance gains, revealing fundamental\nconstraints for enhancing factuality through post-hoc computation; 3)\nRetrieval-Augmented Generation demonstrates consistent improvements at the cost\nof additional inference time overhead, presenting a critical\nefficiency-performance trade-off."
                },
                "authors": [
                    {
                        "name": "Meng Cao"
                    },
                    {
                        "name": "Pengfei Hu"
                    },
                    {
                        "name": "Yingyao Wang"
                    },
                    {
                        "name": "Jihao Gu"
                    },
                    {
                        "name": "Haoran Tang"
                    },
                    {
                        "name": "Haoze Zhao"
                    },
                    {
                        "name": "Jiahua Dong"
                    },
                    {
                        "name": "Wangbo Yu"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Ian Reid"
                    },
                    {
                        "name": "Xiaodan Liang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaodan Liang"
                },
                "author": "Xiaodan Liang",
                "arxiv_comment": "24 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18923v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18923v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18912v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18912v1",
                "updated": "2025-03-24T17:25:44Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    17,
                    25,
                    44,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T17:25:44Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    17,
                    25,
                    44,
                    0,
                    83,
                    0
                ],
                "title": "Causal Links Between Anthropogenic Emissions and Air Pollution Dynamics\n  in Delhi",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal Links Between Anthropogenic Emissions and Air Pollution Dynamics\n  in Delhi"
                },
                "summary": "Air pollution poses significant health and environmental challenges,\nparticularly in rapidly urbanizing regions. Delhi-National Capital Region\nexperiences air pollution episodes due to complex interactions between\nanthropogenic emissions and meteorological conditions. Understanding the causal\ndrivers of key pollutants such as $PM_{2.5}$ and ground $O_3$ is crucial for\ndeveloping effective mitigation strategies. This study investigates the causal\nlinks of anthropogenic emissions on $PM_{2.5}$ and $O_3$ concentrations using\npredictive modeling and causal inference techniques. Integrating\nhigh-resolution air quality data from Jan 2018 to Aug 2023 across 32 monitoring\nstations, we develop predictive regression models that incorporate\nmeteorological variables (temperature and relative humidity), pollutant\nconcentrations ($NO_2, SO_2, CO$), and seasonal harmonic components to capture\nboth diurnal and annual cycles. Here, we show that reductions in anthropogenic\nemissions lead to significant decreases in $PM_{2.5}$ levels, whereas their\neffect on $O_3$ remains marginal and statistically insignificant. To address\nspatial heterogeneity, we employ Gaussian Process modeling. Further, we use\nGranger causality analysis and counterfactual simulation to establish direct\ncausal links. Validation using real-world data from the COVID-19 lockdown\nconfirms that reduced emissions led to a substantial drop in $PM_{2.5}$ but\nonly a slight, insignificant change in $O_3$. The findings highlight the\nnecessity of targeted emission reduction policies while emphasizing the need\nfor integrated strategies addressing both particulate and ozone pollution.\nThese insights are crucial for policymakers designing air pollution\ninterventions in other megacities, and offer a scalable methodology for\ntackling complex urban air pollution through data-driven decision-making.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Air pollution poses significant health and environmental challenges,\nparticularly in rapidly urbanizing regions. Delhi-National Capital Region\nexperiences air pollution episodes due to complex interactions between\nanthropogenic emissions and meteorological conditions. Understanding the causal\ndrivers of key pollutants such as $PM_{2.5}$ and ground $O_3$ is crucial for\ndeveloping effective mitigation strategies. This study investigates the causal\nlinks of anthropogenic emissions on $PM_{2.5}$ and $O_3$ concentrations using\npredictive modeling and causal inference techniques. Integrating\nhigh-resolution air quality data from Jan 2018 to Aug 2023 across 32 monitoring\nstations, we develop predictive regression models that incorporate\nmeteorological variables (temperature and relative humidity), pollutant\nconcentrations ($NO_2, SO_2, CO$), and seasonal harmonic components to capture\nboth diurnal and annual cycles. Here, we show that reductions in anthropogenic\nemissions lead to significant decreases in $PM_{2.5}$ levels, whereas their\neffect on $O_3$ remains marginal and statistically insignificant. To address\nspatial heterogeneity, we employ Gaussian Process modeling. Further, we use\nGranger causality analysis and counterfactual simulation to establish direct\ncausal links. Validation using real-world data from the COVID-19 lockdown\nconfirms that reduced emissions led to a substantial drop in $PM_{2.5}$ but\nonly a slight, insignificant change in $O_3$. The findings highlight the\nnecessity of targeted emission reduction policies while emphasizing the need\nfor integrated strategies addressing both particulate and ozone pollution.\nThese insights are crucial for policymakers designing air pollution\ninterventions in other megacities, and offer a scalable methodology for\ntackling complex urban air pollution through data-driven decision-making."
                },
                "authors": [
                    {
                        "name": "Sourish Das"
                    },
                    {
                        "name": "Sudeep Shukla"
                    },
                    {
                        "name": "Alka Yadav"
                    },
                    {
                        "name": "Anirban Chakraborti"
                    }
                ],
                "author_detail": {
                    "name": "Anirban Chakraborti"
                },
                "author": "Anirban Chakraborti",
                "arxiv_comment": "16 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18912v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18912v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ao-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.soc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18908v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18908v1",
                "updated": "2025-03-24T17:20:35Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    17,
                    20,
                    35,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T17:20:35Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    17,
                    20,
                    35,
                    0,
                    83,
                    0
                ],
                "title": "FFN Fusion: Rethinking Sequential Computation in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FFN Fusion: Rethinking Sequential Computation in Large Language Models"
                },
                "summary": "We introduce FFN Fusion, an architectural optimization technique that reduces\nsequential computation in large language models by identifying and exploiting\nnatural opportunities for parallelization. Our key insight is that sequences of\nFeed-Forward Network (FFN) layers, particularly those remaining after the\nremoval of specific attention layers, can often be parallelized with minimal\naccuracy impact. We develop a principled methodology for identifying and fusing\nsuch sequences, transforming them into parallel operations that significantly\nreduce inference latency while preserving model behavior. Applying these\ntechniques to Llama-3.1-405B-Instruct, we create Llama-Nemotron-Ultra-253B-Base\n(Ultra-253B-Base), an efficient and soon-to-be publicly available model that\nachieves a 1.71X speedup in inference latency and 35X lower per-token cost\nwhile maintaining strong performance across benchmarks. Through extensive\nexperiments on models from 49B to 253B parameters, we demonstrate that FFN\nFusion becomes increasingly effective at larger scales and can complement\nexisting optimization techniques like quantization and pruning. Most\nintriguingly, we find that even full transformer blocks containing both\nattention and FFN layers can sometimes be parallelized, suggesting new\ndirections for neural architecture design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce FFN Fusion, an architectural optimization technique that reduces\nsequential computation in large language models by identifying and exploiting\nnatural opportunities for parallelization. Our key insight is that sequences of\nFeed-Forward Network (FFN) layers, particularly those remaining after the\nremoval of specific attention layers, can often be parallelized with minimal\naccuracy impact. We develop a principled methodology for identifying and fusing\nsuch sequences, transforming them into parallel operations that significantly\nreduce inference latency while preserving model behavior. Applying these\ntechniques to Llama-3.1-405B-Instruct, we create Llama-Nemotron-Ultra-253B-Base\n(Ultra-253B-Base), an efficient and soon-to-be publicly available model that\nachieves a 1.71X speedup in inference latency and 35X lower per-token cost\nwhile maintaining strong performance across benchmarks. Through extensive\nexperiments on models from 49B to 253B parameters, we demonstrate that FFN\nFusion becomes increasingly effective at larger scales and can complement\nexisting optimization techniques like quantization and pruning. Most\nintriguingly, we find that even full transformer blocks containing both\nattention and FFN layers can sometimes be parallelized, suggesting new\ndirections for neural architecture design."
                },
                "authors": [
                    {
                        "name": "Akhiad Bercovich"
                    },
                    {
                        "name": "Mohammad Dabbah"
                    },
                    {
                        "name": "Omri Puny"
                    },
                    {
                        "name": "Ido Galil"
                    },
                    {
                        "name": "Amnon Geifman"
                    },
                    {
                        "name": "Yonatan Geifman"
                    },
                    {
                        "name": "Izhak Golan"
                    },
                    {
                        "name": "Ehud Karpas"
                    },
                    {
                        "name": "Itay Levy"
                    },
                    {
                        "name": "Zach Moshe"
                    },
                    {
                        "name": "Najeeb Nabwani"
                    },
                    {
                        "name": "Tomer Ronen"
                    },
                    {
                        "name": "Itamar Schen"
                    },
                    {
                        "name": "Elad Segal"
                    },
                    {
                        "name": "Ido Shahaf"
                    },
                    {
                        "name": "Oren Tropp"
                    },
                    {
                        "name": "Ran Zilberstein"
                    },
                    {
                        "name": "Ran El-Yaniv"
                    }
                ],
                "author_detail": {
                    "name": "Ran El-Yaniv"
                },
                "author": "Ran El-Yaniv",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18908v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18908v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.16095v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.16095v2",
                "updated": "2025-03-24T17:20:34Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    17,
                    20,
                    34,
                    0,
                    83,
                    0
                ],
                "published": "2024-04-24T18:00:01Z",
                "published_parsed": [
                    2024,
                    4,
                    24,
                    18,
                    0,
                    1,
                    2,
                    115,
                    0
                ],
                "title": "Long-range multipartite entanglement near measurement-induced\n  transitions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-range multipartite entanglement near measurement-induced\n  transitions"
                },
                "summary": "Measurements profoundly impact quantum systems, and can be used to create\nnovel states of matter out of equilibrium. We investigate the multipartite\nentanglement structure that emerges in hybrid quantum circuits involving\nunitaries and measurements. We describe how a balance between measurements and\nunitary evolution can lead to multipartite entanglement spreading to distances\nfar greater than what is found in non-monitored systems, thus evading the usual\nfate of entanglement. We introduce a graphical representation based on spanning\ngraphs that allows to infer the evolution of genuine multipartite entanglement\nfor general subregions. We exemplify our findings on hybrid random Haar\ncircuits that realize a 1d measurement-induced dynamical phase transition,\nwhere we find genuine 3-party entanglement at all separations. At criticality,\nour data is consistent with power-law decay with a tripartite exponent strictly\nlarger than the one of the bipartite logarithmic negativity. The 4-party case\nis also explored. Finally, we discuss how our approach can provide fundamental\ninsights regarding entanglement dynamics for a wide class of quantum circuits\nand architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measurements profoundly impact quantum systems, and can be used to create\nnovel states of matter out of equilibrium. We investigate the multipartite\nentanglement structure that emerges in hybrid quantum circuits involving\nunitaries and measurements. We describe how a balance between measurements and\nunitary evolution can lead to multipartite entanglement spreading to distances\nfar greater than what is found in non-monitored systems, thus evading the usual\nfate of entanglement. We introduce a graphical representation based on spanning\ngraphs that allows to infer the evolution of genuine multipartite entanglement\nfor general subregions. We exemplify our findings on hybrid random Haar\ncircuits that realize a 1d measurement-induced dynamical phase transition,\nwhere we find genuine 3-party entanglement at all separations. At criticality,\nour data is consistent with power-law decay with a tripartite exponent strictly\nlarger than the one of the bipartite logarithmic negativity. The 4-party case\nis also explored. Finally, we discuss how our approach can provide fundamental\ninsights regarding entanglement dynamics for a wide class of quantum circuits\nand architectures."
                },
                "authors": [
                    {
                        "name": "Sebastien J Avakian"
                    },
                    {
                        "name": "T. Pereg-Barnea"
                    },
                    {
                        "name": "William Witczak-Krempa"
                    }
                ],
                "author_detail": {
                    "name": "William Witczak-Krempa"
                },
                "author": "William Witczak-Krempa",
                "arxiv_comment": "5+2 pages, 4+2 figures. v2: Accepted in Phys Rev Research.\n  Substantial additions regarding the power-law scaling of entanglement at\n  criticality",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.16095v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.16095v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.dis-nn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.stat-mech",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.str-el",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19348v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19348v2",
                "updated": "2025-03-24T17:19:27Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    17,
                    19,
                    27,
                    0,
                    83,
                    0
                ],
                "published": "2025-01-31T17:52:03Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    17,
                    52,
                    3,
                    4,
                    31,
                    0
                ],
                "title": "Characterizing User Behavior: The Interplay Between Mobility Patterns\n  and Mobile Traffic",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Characterizing User Behavior: The Interplay Between Mobility Patterns\n  and Mobile Traffic"
                },
                "summary": "Mobile devices have become essential for capturing human activity, and\neXtended Data Records (XDRs) offer rich opportunities for detailed user\nbehavior modeling, which is useful for designing personalized digital services.\nPrevious studies have primarily focused on aggregated mobile traffic and\nmobility analyses, often neglecting individual-level insights. This paper\nintroduces a novel approach that explores the dependency between traffic and\nmobility behaviors at the user level. By analyzing 13 individual features that\nencompass traffic patterns and various mobility aspects, we enhance the\nunderstanding of how these behaviors interact. Our advanced user modeling\nframework integrates traffic and mobility behaviors over time, allowing for\nfine-grained dependencies while maintaining population heterogeneity through\nuser-specific signatures. Furthermore, we develop a Markov model that infers\ntraffic behavior from mobility and vice versa, prioritizing significant\ndependencies while addressing privacy concerns. Using a week-long XDR dataset\nfrom 1,337,719 users across several provinces in Chile, we validate our\napproach, demonstrating its robustness and applicability in accurately\ninferring user behavior and matching mobility and traffic profiles across\ndiverse urban contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mobile devices have become essential for capturing human activity, and\neXtended Data Records (XDRs) offer rich opportunities for detailed user\nbehavior modeling, which is useful for designing personalized digital services.\nPrevious studies have primarily focused on aggregated mobile traffic and\nmobility analyses, often neglecting individual-level insights. This paper\nintroduces a novel approach that explores the dependency between traffic and\nmobility behaviors at the user level. By analyzing 13 individual features that\nencompass traffic patterns and various mobility aspects, we enhance the\nunderstanding of how these behaviors interact. Our advanced user modeling\nframework integrates traffic and mobility behaviors over time, allowing for\nfine-grained dependencies while maintaining population heterogeneity through\nuser-specific signatures. Furthermore, we develop a Markov model that infers\ntraffic behavior from mobility and vice versa, prioritizing significant\ndependencies while addressing privacy concerns. Using a week-long XDR dataset\nfrom 1,337,719 users across several provinces in Chile, we validate our\napproach, demonstrating its robustness and applicability in accurately\ninferring user behavior and matching mobility and traffic profiles across\ndiverse urban contexts."
                },
                "authors": [
                    {
                        "name": "Anne Josiane Kouam"
                    },
                    {
                        "name": "Aline Carneiro Viana"
                    },
                    {
                        "name": "Mariano G. Beir"
                    },
                    {
                        "name": "Leo Ferres"
                    },
                    {
                        "name": "Luca Pappalardo"
                    }
                ],
                "author_detail": {
                    "name": "Luca Pappalardo"
                },
                "author": "Luca Pappalardo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19348v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19348v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18899v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18899v1",
                "updated": "2025-03-24T17:13:25Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    17,
                    13,
                    25,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T17:13:25Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    17,
                    13,
                    25,
                    0,
                    83,
                    0
                ],
                "title": "Statistical Proof of Execution (SPEX)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Statistical Proof of Execution (SPEX)"
                },
                "summary": "Many real-world applications are increasingly incorporating automated\ndecision-making, driven by the widespread adoption of ML/AI inference for\nplanning and guidance. This study examines the growing need for verifiable\ncomputing in autonomous decision-making. We formalize the problem of verifiable\ncomputing and introduce a sampling-based protocol that is significantly faster,\nmore cost-effective, and simpler than existing methods. Furthermore, we tackle\nthe challenges posed by non-determinism, proposing a set of strategies to\neffectively manage common scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many real-world applications are increasingly incorporating automated\ndecision-making, driven by the widespread adoption of ML/AI inference for\nplanning and guidance. This study examines the growing need for verifiable\ncomputing in autonomous decision-making. We formalize the problem of verifiable\ncomputing and introduce a sampling-based protocol that is significantly faster,\nmore cost-effective, and simpler than existing methods. Furthermore, we tackle\nthe challenges posed by non-determinism, proposing a set of strategies to\neffectively manage common scenarios."
                },
                "authors": [
                    {
                        "name": "Michele Dallachiesa"
                    },
                    {
                        "name": "Antonio Pitasi"
                    },
                    {
                        "name": "David Pinger"
                    },
                    {
                        "name": "Josh Goodbody"
                    },
                    {
                        "name": "Luis Vaello"
                    }
                ],
                "author_detail": {
                    "name": "Luis Vaello"
                },
                "author": "Luis Vaello",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18899v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18899v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18893v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18893v1",
                "updated": "2025-03-24T17:06:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    17,
                    6,
                    37,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T17:06:37Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    17,
                    6,
                    37,
                    0,
                    83,
                    0
                ],
                "title": "xKV: Cross-Layer SVD for KV-Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "xKV: Cross-Layer SVD for KV-Cache Compression"
                },
                "summary": "Large Language Models (LLMs) with long context windows enable powerful\napplications but come at the cost of high memory consumption to store the Key\nand Value states (KV-Cache). Recent studies attempted to merge KV-cache from\nmultiple layers into shared representations, yet these approaches either\nrequire expensive pretraining or rely on assumptions of high per-token cosine\nsimilarity across layers which generally does not hold in practice. We find\nthat the dominant singular vectors are remarkably well-aligned across multiple\nlayers of the KV-Cache. Exploiting this insight, we propose xKV, a simple\npost-training method that applies Singular Value Decomposition (SVD) on the\nKV-Cache of grouped layers. xKV consolidates the KV-Cache of multiple layers\ninto a shared low-rank subspace, significantly reducing KV-Cache sizes. Through\nextensive evaluations on the RULER long-context benchmark with widely-used LLMs\n(e.g., Llama-3.1 and Qwen2.5), xKV achieves up to 6.8x higher compression rates\nthan state-of-the-art inter-layer technique while improving accuracy by 2.7%.\nMoreover, xKV is compatible with the emerging Multi-Head Latent Attention (MLA)\n(e.g., DeepSeek-Coder-V2), yielding a notable 3x compression rates on coding\ntasks without performance degradation. These results highlight xKV's strong\ncapability and versatility in addressing memory bottlenecks for long-context\nLLM inference. Our code is publicly available at:\nhttps://github.com/abdelfattah-lab/xKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) with long context windows enable powerful\napplications but come at the cost of high memory consumption to store the Key\nand Value states (KV-Cache). Recent studies attempted to merge KV-cache from\nmultiple layers into shared representations, yet these approaches either\nrequire expensive pretraining or rely on assumptions of high per-token cosine\nsimilarity across layers which generally does not hold in practice. We find\nthat the dominant singular vectors are remarkably well-aligned across multiple\nlayers of the KV-Cache. Exploiting this insight, we propose xKV, a simple\npost-training method that applies Singular Value Decomposition (SVD) on the\nKV-Cache of grouped layers. xKV consolidates the KV-Cache of multiple layers\ninto a shared low-rank subspace, significantly reducing KV-Cache sizes. Through\nextensive evaluations on the RULER long-context benchmark with widely-used LLMs\n(e.g., Llama-3.1 and Qwen2.5), xKV achieves up to 6.8x higher compression rates\nthan state-of-the-art inter-layer technique while improving accuracy by 2.7%.\nMoreover, xKV is compatible with the emerging Multi-Head Latent Attention (MLA)\n(e.g., DeepSeek-Coder-V2), yielding a notable 3x compression rates on coding\ntasks without performance degradation. These results highlight xKV's strong\ncapability and versatility in addressing memory bottlenecks for long-context\nLLM inference. Our code is publicly available at:\nhttps://github.com/abdelfattah-lab/xKV."
                },
                "authors": [
                    {
                        "name": "Chi-Chih Chang"
                    },
                    {
                        "name": "Chien-Yu Lin"
                    },
                    {
                        "name": "Yash Akhauri"
                    },
                    {
                        "name": "Wei-Cheng Lin"
                    },
                    {
                        "name": "Kai-Chiang Wu"
                    },
                    {
                        "name": "Luis Ceze"
                    },
                    {
                        "name": "Mohamed S. Abdelfattah"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed S. Abdelfattah"
                },
                "author": "Mohamed S. Abdelfattah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18893v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18893v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18891v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18891v1",
                "updated": "2025-03-24T17:04:55Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    17,
                    4,
                    55,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T17:04:55Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    17,
                    4,
                    55,
                    0,
                    83,
                    0
                ],
                "title": "AgentDropout: Dynamic Agent Elimination for Token-Efficient and\n  High-Performance LLM-Based Multi-Agent Collaboration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AgentDropout: Dynamic Agent Elimination for Token-Efficient and\n  High-Performance LLM-Based Multi-Agent Collaboration"
                },
                "summary": "Multi-agent systems (MAS) based on large language models (LLMs) have\ndemonstrated significant potential in collaborative problem-solving. However,\nthey still face substantial challenges of low communication efficiency and\nsuboptimal task performance, making the careful design of the agents'\ncommunication topologies particularly important. Inspired by the management\ntheory that roles in an efficient team are often dynamically adjusted, we\npropose AgentDropout, which identifies redundant agents and communication\nacross different communication rounds by optimizing the adjacency matrices of\nthe communication graphs and eliminates them to enhance both token efficiency\nand task performance. Compared to state-of-the-art methods, AgentDropout\nachieves an average reduction of 21.6% in prompt token consumption and 18.4% in\ncompletion token consumption, along with a performance improvement of 1.14 on\nthe tasks. Furthermore, the extended experiments demonstrate that AgentDropout\nachieves notable domain transferability and structure robustness, revealing its\nreliability and effectiveness. We release our code at\nhttps://github.com/wangzx1219/AgentDropout.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-agent systems (MAS) based on large language models (LLMs) have\ndemonstrated significant potential in collaborative problem-solving. However,\nthey still face substantial challenges of low communication efficiency and\nsuboptimal task performance, making the careful design of the agents'\ncommunication topologies particularly important. Inspired by the management\ntheory that roles in an efficient team are often dynamically adjusted, we\npropose AgentDropout, which identifies redundant agents and communication\nacross different communication rounds by optimizing the adjacency matrices of\nthe communication graphs and eliminates them to enhance both token efficiency\nand task performance. Compared to state-of-the-art methods, AgentDropout\nachieves an average reduction of 21.6% in prompt token consumption and 18.4% in\ncompletion token consumption, along with a performance improvement of 1.14 on\nthe tasks. Furthermore, the extended experiments demonstrate that AgentDropout\nachieves notable domain transferability and structure robustness, revealing its\nreliability and effectiveness. We release our code at\nhttps://github.com/wangzx1219/AgentDropout."
                },
                "authors": [
                    {
                        "name": "Zhexuan Wang"
                    },
                    {
                        "name": "Yutong Wang"
                    },
                    {
                        "name": "Xuebo Liu"
                    },
                    {
                        "name": "Liang Ding"
                    },
                    {
                        "name": "Miao Zhang"
                    },
                    {
                        "name": "Jie Liu"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18891v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18891v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18878v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18878v1",
                "updated": "2025-03-24T16:54:26Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    16,
                    54,
                    26,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T16:54:26Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    16,
                    54,
                    26,
                    0,
                    83,
                    0
                ],
                "title": "I Have Covered All the Bases Here: Interpreting Reasoning Features in\n  Large Language Models via Sparse Autoencoders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "I Have Covered All the Bases Here: Interpreting Reasoning Features in\n  Large Language Models via Sparse Autoencoders"
                },
                "summary": "Large Language Models (LLMs) have achieved remarkable success in natural\nlanguage processing. Recent advances have led to the developing of a new class\nof reasoning LLMs; for example, open-source DeepSeek-R1 has achieved\nstate-of-the-art performance by integrating deep thinking and complex\nreasoning. Despite these impressive capabilities, the internal reasoning\nmechanisms of such models remain unexplored. In this work, we employ Sparse\nAutoencoders (SAEs), a method to learn a sparse decomposition of latent\nrepresentations of a neural network into interpretable features, to identify\nfeatures that drive reasoning in the DeepSeek-R1 series of models. First, we\npropose an approach to extract candidate ''reasoning features'' from SAE\nrepresentations. We validate these features through empirical analysis and\ninterpretability methods, demonstrating their direct correlation with the\nmodel's reasoning abilities. Crucially, we demonstrate that steering these\nfeatures systematically enhances reasoning performance, offering the first\nmechanistic account of reasoning in LLMs. Code available at\nhttps://github.com/AIRI-Institute/SAE-Reasoning",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved remarkable success in natural\nlanguage processing. Recent advances have led to the developing of a new class\nof reasoning LLMs; for example, open-source DeepSeek-R1 has achieved\nstate-of-the-art performance by integrating deep thinking and complex\nreasoning. Despite these impressive capabilities, the internal reasoning\nmechanisms of such models remain unexplored. In this work, we employ Sparse\nAutoencoders (SAEs), a method to learn a sparse decomposition of latent\nrepresentations of a neural network into interpretable features, to identify\nfeatures that drive reasoning in the DeepSeek-R1 series of models. First, we\npropose an approach to extract candidate ''reasoning features'' from SAE\nrepresentations. We validate these features through empirical analysis and\ninterpretability methods, demonstrating their direct correlation with the\nmodel's reasoning abilities. Crucially, we demonstrate that steering these\nfeatures systematically enhances reasoning performance, offering the first\nmechanistic account of reasoning in LLMs. Code available at\nhttps://github.com/AIRI-Institute/SAE-Reasoning"
                },
                "authors": [
                    {
                        "name": "Andrey Galichin"
                    },
                    {
                        "name": "Alexey Dontsov"
                    },
                    {
                        "name": "Polina Druzhinina"
                    },
                    {
                        "name": "Anton Razzhigaev"
                    },
                    {
                        "name": "Oleg Y. Rogov"
                    },
                    {
                        "name": "Elena Tutubalina"
                    },
                    {
                        "name": "Ivan Oseledets"
                    }
                ],
                "author_detail": {
                    "name": "Ivan Oseledets"
                },
                "author": "Ivan Oseledets",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18878v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18878v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18875v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18875v1",
                "updated": "2025-03-24T16:50:48Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    16,
                    50,
                    48,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T16:50:48Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    16,
                    50,
                    48,
                    0,
                    83,
                    0
                ],
                "title": "A primer on inference and prediction with epidemic renewal models and\n  sequential Monte Carlo",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A primer on inference and prediction with epidemic renewal models and\n  sequential Monte Carlo"
                },
                "summary": "Renewal models are widely used in statistical epidemiology as\nsemi-mechanistic models of disease transmission. While primarily used for\nestimating the instantaneous reproduction number, they can also be used for\ngenerating projections, estimating elimination probabilities, modelling the\neffect of interventions, and more. We demonstrate how simple sequential Monte\nCarlo methods (also known as particle filters) can be used to perform inference\non these models. Our goal is to acquaint a reader who has a working knowledge\nof statistical inference with these methods and models and to provide a\npractical guide to their implementation. We focus on these methods' flexibility\nand their ability to handle multiple statistical and other biases\nsimultaneously. We leverage this flexibility to unify existing methods for\nestimating the instantaneous reproduction number and generating projections. A\ncompanion website \"SMC and epidemic renewal models\" provides additional worked\nexamples, self-contained code to reproduce the examples presented here, and\nadditional materials.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Renewal models are widely used in statistical epidemiology as\nsemi-mechanistic models of disease transmission. While primarily used for\nestimating the instantaneous reproduction number, they can also be used for\ngenerating projections, estimating elimination probabilities, modelling the\neffect of interventions, and more. We demonstrate how simple sequential Monte\nCarlo methods (also known as particle filters) can be used to perform inference\non these models. Our goal is to acquaint a reader who has a working knowledge\nof statistical inference with these methods and models and to provide a\npractical guide to their implementation. We focus on these methods' flexibility\nand their ability to handle multiple statistical and other biases\nsimultaneously. We leverage this flexibility to unify existing methods for\nestimating the instantaneous reproduction number and generating projections. A\ncompanion website \"SMC and epidemic renewal models\" provides additional worked\nexamples, self-contained code to reproduce the examples presented here, and\nadditional materials."
                },
                "authors": [
                    {
                        "name": "Nicholas Steyn"
                    },
                    {
                        "name": "Kris V. Parag"
                    },
                    {
                        "name": "Robin N. Thompson"
                    },
                    {
                        "name": "Christl A. Donnelly"
                    }
                ],
                "author_detail": {
                    "name": "Christl A. Donnelly"
                },
                "author": "Christl A. Donnelly",
                "arxiv_comment": "Companion website: https://nicsteyn2.github.io/SMCforRt/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18875v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18875v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18873v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18873v1",
                "updated": "2025-03-24T16:48:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    16,
                    48,
                    42,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T16:48:42Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    16,
                    48,
                    42,
                    0,
                    83,
                    0
                ],
                "title": "Efficient Self-Supervised Adaptation for Medical Image Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Self-Supervised Adaptation for Medical Image Analysis"
                },
                "summary": "Self-supervised adaptation (SSA) improves foundation model transfer to\nmedical domains but is computationally prohibitive. Although parameter\nefficient fine-tuning methods such as LoRA have been explored for supervised\nadaptation, their effectiveness for SSA remains unknown. In this work, we\nintroduce efficient self-supervised adaptation (ESSA), a framework that applies\nparameter-efficient fine-tuning techniques to SSA with the aim of reducing\ncomputational cost and improving adaptation performance. Among the methods\ntested, Attention Projection Layer Adaptation (APLA) sets a new\nstate-of-the-art, consistently surpassing full-parameter SSA and supervised\nfine-tuning across diverse medical tasks, while reducing GPU memory by up to\n40.1% and increasing training throughput by 25.2%, all while maintaining\ninference efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-supervised adaptation (SSA) improves foundation model transfer to\nmedical domains but is computationally prohibitive. Although parameter\nefficient fine-tuning methods such as LoRA have been explored for supervised\nadaptation, their effectiveness for SSA remains unknown. In this work, we\nintroduce efficient self-supervised adaptation (ESSA), a framework that applies\nparameter-efficient fine-tuning techniques to SSA with the aim of reducing\ncomputational cost and improving adaptation performance. Among the methods\ntested, Attention Projection Layer Adaptation (APLA) sets a new\nstate-of-the-art, consistently surpassing full-parameter SSA and supervised\nfine-tuning across diverse medical tasks, while reducing GPU memory by up to\n40.1% and increasing training throughput by 25.2%, all while maintaining\ninference efficiency."
                },
                "authors": [
                    {
                        "name": "Moein Sorkhei"
                    },
                    {
                        "name": "Emir Konuk"
                    },
                    {
                        "name": "Jingyu Guo"
                    },
                    {
                        "name": "Christos Matsoukas"
                    },
                    {
                        "name": "Kevin Smith"
                    }
                ],
                "author_detail": {
                    "name": "Kevin Smith"
                },
                "author": "Kevin Smith",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18873v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18873v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18869v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18869v1",
                "updated": "2025-03-24T16:44:32Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    16,
                    44,
                    32,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T16:44:32Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    16,
                    44,
                    32,
                    0,
                    83,
                    0
                ],
                "title": "Reimagining Memory Access for LLM Inference: Compression-Aware Memory\n  Controller Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reimagining Memory Access for LLM Inference: Compression-Aware Memory\n  Controller Design"
                },
                "summary": "The efficiency of Large Language Model~(LLM) inference is often constrained\nby substantial memory bandwidth and capacity demands. Existing techniques, such\nas pruning, quantization, and mixture of experts/depth, reduce memory capacity\nand/or bandwidth consumption at the cost of slight degradation in inference\nquality. This paper introduces a design solution that further alleviates memory\nbottlenecks by enhancing the on-chip memory controller in AI accelerators to\nachieve two main objectives: (1) significantly reducing memory capacity and\nbandwidth usage through lossless block compression~(e.g., LZ4 and ZSTD) of\nmodel weights and key-value (KV) cache without compromising inference quality,\nand (2) enabling memory bandwidth and energy consumption to scale\nproportionally with context-dependent dynamic quantization. These goals are\naccomplished by equipping the on-chip memory controller with mechanisms to\nimprove fine-grained bit-level accessibility and compressibility of weights and\nKV cache through LLM-aware configuration of in-memory placement and\nrepresentation. Experimental results on publicly available LLMs demonstrate the\neffectiveness of this approach, showing memory footprint reductions of 25.2\\%\nfor model weights and 46.9\\% for KV cache. In addition, our hardware prototype\nat 4\\,GHz and 32 lanes (7\\,nm) achieves 8\\,TB/s throughput with a modest area\noverhead (under 3.8\\,mm\\(^2\\)), which underscores the viability of LLM-aware\nmemory control as a key to efficient large-scale inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The efficiency of Large Language Model~(LLM) inference is often constrained\nby substantial memory bandwidth and capacity demands. Existing techniques, such\nas pruning, quantization, and mixture of experts/depth, reduce memory capacity\nand/or bandwidth consumption at the cost of slight degradation in inference\nquality. This paper introduces a design solution that further alleviates memory\nbottlenecks by enhancing the on-chip memory controller in AI accelerators to\nachieve two main objectives: (1) significantly reducing memory capacity and\nbandwidth usage through lossless block compression~(e.g., LZ4 and ZSTD) of\nmodel weights and key-value (KV) cache without compromising inference quality,\nand (2) enabling memory bandwidth and energy consumption to scale\nproportionally with context-dependent dynamic quantization. These goals are\naccomplished by equipping the on-chip memory controller with mechanisms to\nimprove fine-grained bit-level accessibility and compressibility of weights and\nKV cache through LLM-aware configuration of in-memory placement and\nrepresentation. Experimental results on publicly available LLMs demonstrate the\neffectiveness of this approach, showing memory footprint reductions of 25.2\\%\nfor model weights and 46.9\\% for KV cache. In addition, our hardware prototype\nat 4\\,GHz and 32 lanes (7\\,nm) achieves 8\\,TB/s throughput with a modest area\noverhead (under 3.8\\,mm\\(^2\\)), which underscores the viability of LLM-aware\nmemory control as a key to efficient large-scale inference."
                },
                "authors": [
                    {
                        "name": "Rui Xie"
                    },
                    {
                        "name": "Asad Ul Haq"
                    },
                    {
                        "name": "Linsen Ma"
                    },
                    {
                        "name": "Yunhua Fang"
                    },
                    {
                        "name": "Zirak Burzin Engineer"
                    },
                    {
                        "name": "Liu Liu"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "author": "Tong Zhang",
                "arxiv_comment": "9 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18869v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18869v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18866v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18866v1",
                "updated": "2025-03-24T16:41:23Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    16,
                    41,
                    23,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T16:41:23Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    16,
                    41,
                    23,
                    0,
                    83,
                    0
                ],
                "title": "Reasoning to Learn from Latent Thoughts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning to Learn from Latent Thoughts"
                },
                "summary": "Compute scaling for language model (LM) pretraining has outpaced the growth\nof human-written texts, leading to concerns that data will become the\nbottleneck to LM scaling. To continue scaling pretraining in this\ndata-constrained regime, we propose that explicitly modeling and inferring the\nlatent thoughts that underlie the text generation process can significantly\nimprove pretraining data efficiency. Intuitively, our approach views web text\nas the compressed final outcome of a verbose human thought process and that the\nlatent thoughts contain important contextual knowledge and reasoning steps that\nare critical to data-efficient learning. We empirically demonstrate the\neffectiveness of our approach through data-constrained continued pretraining\nfor math. We first show that synthetic data approaches to inferring latent\nthoughts significantly improve data efficiency, outperforming training on the\nsame amount of raw data (5.7\\% $\\rightarrow$ 25.4\\% on MATH). Furthermore, we\ndemonstrate latent thought inference without a strong teacher, where an LM\nbootstraps its own performance by using an EM algorithm to iteratively improve\nthe capability of the trained LM and the quality of thought-augmented\npretraining data. We show that a 1B LM can bootstrap its performance across at\nleast three iterations and significantly outperform baselines trained on raw\ndata, with increasing gains from additional inference compute when performing\nthe E-step. The gains from inference scaling and EM iterations suggest new\nopportunities for scaling data-constrained pretraining.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compute scaling for language model (LM) pretraining has outpaced the growth\nof human-written texts, leading to concerns that data will become the\nbottleneck to LM scaling. To continue scaling pretraining in this\ndata-constrained regime, we propose that explicitly modeling and inferring the\nlatent thoughts that underlie the text generation process can significantly\nimprove pretraining data efficiency. Intuitively, our approach views web text\nas the compressed final outcome of a verbose human thought process and that the\nlatent thoughts contain important contextual knowledge and reasoning steps that\nare critical to data-efficient learning. We empirically demonstrate the\neffectiveness of our approach through data-constrained continued pretraining\nfor math. We first show that synthetic data approaches to inferring latent\nthoughts significantly improve data efficiency, outperforming training on the\nsame amount of raw data (5.7\\% $\\rightarrow$ 25.4\\% on MATH). Furthermore, we\ndemonstrate latent thought inference without a strong teacher, where an LM\nbootstraps its own performance by using an EM algorithm to iteratively improve\nthe capability of the trained LM and the quality of thought-augmented\npretraining data. We show that a 1B LM can bootstrap its performance across at\nleast three iterations and significantly outperform baselines trained on raw\ndata, with increasing gains from additional inference compute when performing\nthe E-step. The gains from inference scaling and EM iterations suggest new\nopportunities for scaling data-constrained pretraining."
                },
                "authors": [
                    {
                        "name": "Yangjun Ruan"
                    },
                    {
                        "name": "Neil Band"
                    },
                    {
                        "name": "Chris J. Maddison"
                    },
                    {
                        "name": "Tatsunori Hashimoto"
                    }
                ],
                "author_detail": {
                    "name": "Tatsunori Hashimoto"
                },
                "author": "Tatsunori Hashimoto",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18866v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18866v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18865v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18865v1",
                "updated": "2025-03-24T16:41:17Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    16,
                    41,
                    17,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T16:41:17Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    16,
                    41,
                    17,
                    0,
                    83,
                    0
                ],
                "title": "Structuring Scientific Innovation: A Framework for Modeling and\n  Discovering Impactful Knowledge Combinations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Structuring Scientific Innovation: A Framework for Modeling and\n  Discovering Impactful Knowledge Combinations"
                },
                "summary": "The emergence of large language models offers new possibilities for\nstructured exploration of scientific knowledge. Rather than viewing scientific\ndiscovery as isolated ideas or content, we propose a structured approach that\nemphasizes the role of method combinations in shaping disruptive insights.\nSpecifically, we investigate how knowledge unit--especially those tied to\nmethodological design--can be modeled and recombined to yield research\nbreakthroughs.Our proposed framework addresses two key challenges. First, we\nintroduce a contrastive learning-based mechanism to identify distinguishing\nfeatures of historically disruptive method combinations within problem-driven\ncontexts.Second, we propose a reasoning-guided Monte Carlo search algorithm\nthat leverages the chain-of-thought capability of LLMs to identify promising\nknowledge recombinations for new problem statements.Empirical studies across\nmultiple domains show that the framework is capable of modeling the structural\ndynamics of innovation and successfully highlights combinations with high\ndisruptive potential.This research provides a new path for computationally\nguided scientific ideation grounded in structured reasoning and historical data\nmodeling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of large language models offers new possibilities for\nstructured exploration of scientific knowledge. Rather than viewing scientific\ndiscovery as isolated ideas or content, we propose a structured approach that\nemphasizes the role of method combinations in shaping disruptive insights.\nSpecifically, we investigate how knowledge unit--especially those tied to\nmethodological design--can be modeled and recombined to yield research\nbreakthroughs.Our proposed framework addresses two key challenges. First, we\nintroduce a contrastive learning-based mechanism to identify distinguishing\nfeatures of historically disruptive method combinations within problem-driven\ncontexts.Second, we propose a reasoning-guided Monte Carlo search algorithm\nthat leverages the chain-of-thought capability of LLMs to identify promising\nknowledge recombinations for new problem statements.Empirical studies across\nmultiple domains show that the framework is capable of modeling the structural\ndynamics of innovation and successfully highlights combinations with high\ndisruptive potential.This research provides a new path for computationally\nguided scientific ideation grounded in structured reasoning and historical data\nmodeling."
                },
                "authors": [
                    {
                        "name": "Junlan Chen"
                    },
                    {
                        "name": "Kexin Zhang"
                    },
                    {
                        "name": "Daifeng Li"
                    },
                    {
                        "name": "Yangyang Feng"
                    },
                    {
                        "name": "Yuxuan Zhang"
                    },
                    {
                        "name": "Bowen Deng"
                    }
                ],
                "author_detail": {
                    "name": "Bowen Deng"
                },
                "author": "Bowen Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18865v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18865v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18862v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18862v1",
                "updated": "2025-03-24T16:38:31Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    16,
                    38,
                    31,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T16:38:31Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    16,
                    38,
                    31,
                    0,
                    83,
                    0
                ],
                "title": "Exploring the Integration of Key-Value Attention Into Pure and Hybrid\n  Transformers for Semantic Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the Integration of Key-Value Attention Into Pure and Hybrid\n  Transformers for Semantic Segmentation"
                },
                "summary": "While CNNs were long considered state of the art for image processing, the\nintroduction of Transformer architectures has challenged this position. While\nachieving excellent results in image classification and segmentation,\nTransformers remain inherently reliant on large training datasets and remain\ncomputationally expensive. A newly introduced Transformer derivative named KV\nTransformer shows promising results in synthetic, NLP, and image classification\ntasks, while reducing complexity and memory usage. This is especially conducive\nto use cases where local inference is required, such as medical screening\napplications. We endeavoured to further evaluate the merit of KV Transformers\non semantic segmentation tasks, specifically in the domain of medical imaging.\nBy directly comparing traditional and KV variants of the same base\narchitectures, we provide further insight into the practical tradeoffs of\nreduced model complexity. We observe a notable reduction in parameter count and\nmultiply accumulate operations, while achieving similar performance from most\nof the KV variant models when directly compared to their QKV implementation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While CNNs were long considered state of the art for image processing, the\nintroduction of Transformer architectures has challenged this position. While\nachieving excellent results in image classification and segmentation,\nTransformers remain inherently reliant on large training datasets and remain\ncomputationally expensive. A newly introduced Transformer derivative named KV\nTransformer shows promising results in synthetic, NLP, and image classification\ntasks, while reducing complexity and memory usage. This is especially conducive\nto use cases where local inference is required, such as medical screening\napplications. We endeavoured to further evaluate the merit of KV Transformers\non semantic segmentation tasks, specifically in the domain of medical imaging.\nBy directly comparing traditional and KV variants of the same base\narchitectures, we provide further insight into the practical tradeoffs of\nreduced model complexity. We observe a notable reduction in parameter count and\nmultiply accumulate operations, while achieving similar performance from most\nof the KV variant models when directly compared to their QKV implementation."
                },
                "authors": [
                    {
                        "name": "DeShin Hwa"
                    },
                    {
                        "name": "Tobias Holmes"
                    },
                    {
                        "name": "Klaus Drechsler"
                    }
                ],
                "author_detail": {
                    "name": "Klaus Drechsler"
                },
                "author": "Klaus Drechsler",
                "arxiv_doi": "10.1007/978-3-658-47422-5_71",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-658-47422-5_71",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.18862v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18862v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "6 pages, 3 figures, Preprint. Final version published in:\n  Bildverarbeitung f\\\"ur die Medizin 2025, Springer. DOI:\n  https://doi.org/10.1007/978-3-658-47422-5_71",
                "arxiv_journal_ref": "Bildverarbeitung f\\\"ur die Medizin 2025. BVM 2025. Informatik\n  aktuell. Springer Vieweg, Wiesbaden, pp 305-310",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18854v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18854v1",
                "updated": "2025-03-24T16:32:17Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    16,
                    32,
                    17,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T16:32:17Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    16,
                    32,
                    17,
                    0,
                    83,
                    0
                ],
                "title": "MC-LLaVA: Multi-Concept Personalized Vision-Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MC-LLaVA: Multi-Concept Personalized Vision-Language Model"
                },
                "summary": "Current vision-language models (VLMs) show exceptional abilities across\ndiverse tasks, such as visual question answering. To enhance user experience,\nrecent studies investigate VLM personalization to understand user-provided\nconcepts. However, they mainly focus on single-concept personalization,\nneglecting the existence and interplay of multiple concepts, which limits\nreal-world applicability. This paper proposes the first multi-concept\npersonalization paradigm, MC-LLaVA. Specifically, MC-LLaVA employs a\nmulti-concept instruction tuning strategy, effectively integrating multiple\nconcepts in a single training step. To reduce the costs related to joint\ntraining, we propose a personalized textual prompt that uses visual token\ninformation to initialize concept tokens. Additionally, we introduce a\npersonalized visual prompt during inference, aggregating location confidence\nmaps for enhanced recognition and grounding capabilities. To advance\nmulti-concept personalization research, we further contribute a high-quality\ninstruction tuning dataset. We carefully collect images with multiple\ncharacters and objects from movies and manually generate question-answer\nsamples for multi-concept scenarios, featuring superior diversity.\nComprehensive qualitative and quantitative experiments demonstrate that\nMC-LLaVA can achieve impressive multi-concept personalized responses, paving\nthe way for VLMs to become better user-specific assistants. The code and\ndataset will be publicly available at\n$\\href{https://github.com/arctanxarc/MC-LLaVA}{https://github.com/arctanxarc/MC-LLaVA}$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current vision-language models (VLMs) show exceptional abilities across\ndiverse tasks, such as visual question answering. To enhance user experience,\nrecent studies investigate VLM personalization to understand user-provided\nconcepts. However, they mainly focus on single-concept personalization,\nneglecting the existence and interplay of multiple concepts, which limits\nreal-world applicability. This paper proposes the first multi-concept\npersonalization paradigm, MC-LLaVA. Specifically, MC-LLaVA employs a\nmulti-concept instruction tuning strategy, effectively integrating multiple\nconcepts in a single training step. To reduce the costs related to joint\ntraining, we propose a personalized textual prompt that uses visual token\ninformation to initialize concept tokens. Additionally, we introduce a\npersonalized visual prompt during inference, aggregating location confidence\nmaps for enhanced recognition and grounding capabilities. To advance\nmulti-concept personalization research, we further contribute a high-quality\ninstruction tuning dataset. We carefully collect images with multiple\ncharacters and objects from movies and manually generate question-answer\nsamples for multi-concept scenarios, featuring superior diversity.\nComprehensive qualitative and quantitative experiments demonstrate that\nMC-LLaVA can achieve impressive multi-concept personalized responses, paving\nthe way for VLMs to become better user-specific assistants. The code and\ndataset will be publicly available at\n$\\href{https://github.com/arctanxarc/MC-LLaVA}{https://github.com/arctanxarc/MC-LLaVA}$."
                },
                "authors": [
                    {
                        "name": "Ruichuan An"
                    },
                    {
                        "name": "Sihan Yang"
                    },
                    {
                        "name": "Ming Lu"
                    },
                    {
                        "name": "Renrui Zhang"
                    },
                    {
                        "name": "Kai Zeng"
                    },
                    {
                        "name": "Yulin Luo"
                    },
                    {
                        "name": "Jiajun Cao"
                    },
                    {
                        "name": "Hao Liang"
                    },
                    {
                        "name": "Ying Chen"
                    },
                    {
                        "name": "Qi She"
                    },
                    {
                        "name": "Shanghang Zhang"
                    },
                    {
                        "name": "Wentao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wentao Zhang"
                },
                "author": "Wentao Zhang",
                "arxiv_comment": "The code and dataset will be publicly available at\n  $\\href{https://github.com/arctanxarc/MC-LLaVA}{https://github.com/arctanxarc/MC-LLaVA}$",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18854v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18854v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.05597v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.05597v2",
                "updated": "2025-03-24T16:22:24Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    16,
                    22,
                    24,
                    0,
                    83,
                    0
                ],
                "published": "2024-05-09T07:37:37Z",
                "published_parsed": [
                    2024,
                    5,
                    9,
                    7,
                    37,
                    37,
                    3,
                    130,
                    0
                ],
                "title": "The empirical copula process in high dimensions: Stute's representation\n  and applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The empirical copula process in high dimensions: Stute's representation\n  and applications"
                },
                "summary": "The empirical copula process, a fundamental tool for copula inference, is\nstudied in the high dimensional regime where the dimension is allowed to grow\nto infinity exponentially in the sample size. Under natural, weak smoothness\nassumptions on the underlying copula, it is shown that Stute's representation\nis valid in the following sense: all low-dimensional margins of fixed dimension\nof the empirical copula process can be approximated by a functional of the\nlow-dimensional margins of the standard empirical process, with the almost sure\nerror term being uniform in the margins. The result has numerous potential\napplications, and is exemplary applied to the problem of testing pairwise\nstochastic independence in high dimensions, leading to various extensions of\nrecent results in the literature: for certain test statistics based on pairwise\nassociation measures, type-I error control is obtained for models beyond mutual\nindependence. Moreover, bootstrap-based critical values are shown to yield\nstrong control of the familywise error rate for a large class of data\ngenerating processes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The empirical copula process, a fundamental tool for copula inference, is\nstudied in the high dimensional regime where the dimension is allowed to grow\nto infinity exponentially in the sample size. Under natural, weak smoothness\nassumptions on the underlying copula, it is shown that Stute's representation\nis valid in the following sense: all low-dimensional margins of fixed dimension\nof the empirical copula process can be approximated by a functional of the\nlow-dimensional margins of the standard empirical process, with the almost sure\nerror term being uniform in the margins. The result has numerous potential\napplications, and is exemplary applied to the problem of testing pairwise\nstochastic independence in high dimensions, leading to various extensions of\nrecent results in the literature: for certain test statistics based on pairwise\nassociation measures, type-I error control is obtained for models beyond mutual\nindependence. Moreover, bootstrap-based critical values are shown to yield\nstrong control of the familywise error rate for a large class of data\ngenerating processes."
                },
                "authors": [
                    {
                        "name": "Axel Bcher"
                    },
                    {
                        "name": "Cambyse Pakzad"
                    }
                ],
                "author_detail": {
                    "name": "Cambyse Pakzad"
                },
                "author": "Cambyse Pakzad",
                "arxiv_comment": "34 pages, including an appendix of 7 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.05597v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.05597v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "Primary 62G20, 62H05, secondary 62G09",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18840v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18840v1",
                "updated": "2025-03-24T16:13:04Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    16,
                    13,
                    4,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T16:13:04Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    16,
                    13,
                    4,
                    0,
                    83,
                    0
                ],
                "title": "Learning to segment anatomy and lesions from disparately labeled sources\n  in brain MRI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to segment anatomy and lesions from disparately labeled sources\n  in brain MRI"
                },
                "summary": "Segmenting healthy tissue structures alongside lesions in brain Magnetic\nResonance Images (MRI) remains a challenge for today's algorithms due to\nlesion-caused disruption of the anatomy and lack of jointly labeled training\ndatasets, where both healthy tissues and lesions are labeled on the same\nimages. In this paper, we propose a method that is robust to lesion-caused\ndisruptions and can be trained from disparately labeled training sets, i.e.,\nwithout requiring jointly labeled samples, to automatically segment both. In\ncontrast to prior work, we decouple healthy tissue and lesion segmentation in\ntwo paths to leverage multi-sequence acquisitions and merge information with an\nattention mechanism. During inference, an image-specific adaptation reduces\nadverse influences of lesion regions on healthy tissue predictions. During\ntraining, the adaptation is taken into account through meta-learning and\nco-training is used to learn from disparately labeled training images. Our\nmodel shows an improved performance on several anatomical structures and\nlesions on a publicly available brain glioblastoma dataset compared to the\nstate-of-the-art segmentation methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Segmenting healthy tissue structures alongside lesions in brain Magnetic\nResonance Images (MRI) remains a challenge for today's algorithms due to\nlesion-caused disruption of the anatomy and lack of jointly labeled training\ndatasets, where both healthy tissues and lesions are labeled on the same\nimages. In this paper, we propose a method that is robust to lesion-caused\ndisruptions and can be trained from disparately labeled training sets, i.e.,\nwithout requiring jointly labeled samples, to automatically segment both. In\ncontrast to prior work, we decouple healthy tissue and lesion segmentation in\ntwo paths to leverage multi-sequence acquisitions and merge information with an\nattention mechanism. During inference, an image-specific adaptation reduces\nadverse influences of lesion regions on healthy tissue predictions. During\ntraining, the adaptation is taken into account through meta-learning and\nco-training is used to learn from disparately labeled training images. Our\nmodel shows an improved performance on several anatomical structures and\nlesions on a publicly available brain glioblastoma dataset compared to the\nstate-of-the-art segmentation methods."
                },
                "authors": [
                    {
                        "name": "Meva Himmetoglu"
                    },
                    {
                        "name": "Ilja Ciernik"
                    },
                    {
                        "name": "Ender Konukoglu"
                    }
                ],
                "author_detail": {
                    "name": "Ender Konukoglu"
                },
                "author": "Ender Konukoglu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18840v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18840v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18836v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18836v1",
                "updated": "2025-03-24T16:10:51Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    16,
                    10,
                    51,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T16:10:51Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    16,
                    10,
                    51,
                    0,
                    83,
                    0
                ],
                "title": "Dual-domain Multi-path Self-supervised Diffusion Model for Accelerated\n  MRI Reconstruction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dual-domain Multi-path Self-supervised Diffusion Model for Accelerated\n  MRI Reconstruction"
                },
                "summary": "Magnetic resonance imaging (MRI) is a vital diagnostic tool, but its\ninherently long acquisition times reduce clinical efficiency and patient\ncomfort. Recent advancements in deep learning, particularly diffusion models,\nhave improved accelerated MRI reconstruction. However, existing diffusion\nmodels' training often relies on fully sampled data, models incur high\ncomputational costs, and often lack uncertainty estimation, limiting their\nclinical applicability. To overcome these challenges, we propose a novel\nframework, called Dual-domain Multi-path Self-supervised Diffusion Model\n(DMSM), that integrates a self-supervised dual-domain diffusion model training\nscheme, a lightweight hybrid attention network for the reconstruction diffusion\nmodel, and a multi-path inference strategy, to enhance reconstruction accuracy,\nefficiency, and explainability. Unlike traditional diffusion-based models, DMSM\neliminates the dependency on training from fully sampled data, making it more\npractical for real-world clinical settings. We evaluated DMSM on two human MRI\ndatasets, demonstrating that it achieves favorable performance over several\nsupervised and self-supervised baselines, particularly in preserving fine\nanatomical structures and suppressing artifacts under high acceleration\nfactors. Additionally, our model generates uncertainty maps that correlate\nreasonably well with reconstruction errors, offering valuable clinically\ninterpretable guidance and potentially enhancing diagnostic confidence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Magnetic resonance imaging (MRI) is a vital diagnostic tool, but its\ninherently long acquisition times reduce clinical efficiency and patient\ncomfort. Recent advancements in deep learning, particularly diffusion models,\nhave improved accelerated MRI reconstruction. However, existing diffusion\nmodels' training often relies on fully sampled data, models incur high\ncomputational costs, and often lack uncertainty estimation, limiting their\nclinical applicability. To overcome these challenges, we propose a novel\nframework, called Dual-domain Multi-path Self-supervised Diffusion Model\n(DMSM), that integrates a self-supervised dual-domain diffusion model training\nscheme, a lightweight hybrid attention network for the reconstruction diffusion\nmodel, and a multi-path inference strategy, to enhance reconstruction accuracy,\nefficiency, and explainability. Unlike traditional diffusion-based models, DMSM\neliminates the dependency on training from fully sampled data, making it more\npractical for real-world clinical settings. We evaluated DMSM on two human MRI\ndatasets, demonstrating that it achieves favorable performance over several\nsupervised and self-supervised baselines, particularly in preserving fine\nanatomical structures and suppressing artifacts under high acceleration\nfactors. Additionally, our model generates uncertainty maps that correlate\nreasonably well with reconstruction errors, offering valuable clinically\ninterpretable guidance and potentially enhancing diagnostic confidence."
                },
                "authors": [
                    {
                        "name": "Yuxuan Zhang"
                    },
                    {
                        "name": "Jinkui Hao"
                    },
                    {
                        "name": "Bo Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Bo Zhou"
                },
                "author": "Bo Zhou",
                "arxiv_comment": "10 pages, 8 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18836v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18836v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18831v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18831v1",
                "updated": "2025-03-24T16:08:57Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    16,
                    8,
                    57,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T16:08:57Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    16,
                    8,
                    57,
                    0,
                    83,
                    0
                ],
                "title": "An improved central limit theorem for the empirical sliced Wasserstein\n  distance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An improved central limit theorem for the empirical sliced Wasserstein\n  distance"
                },
                "summary": "Optimal transport theory has become a fundamental tool for handling diverse\ntypes of data, with growing applications across various fields. However, the\nWasserstein distance presents significant computational and statistical\nchallenges in high-dimensional settings. To address these issues, alternative\ndistances such as the sliced Wasserstein distance, which leverages\none-dimensional projections, have been introduced. In this work, we establish a\nnovel central limit theorem for the p-sliced Wasserstein distance, for p>1,\nusing the Efron-Stein inequality-a technique that has proven effective in\nrelated problems. This approach yields a central limit theorem centered at the\nexpected value of the empirical cost, under mild regularity conditions.\nNotably, unlike the general Wasserstein distance in arbitrary dimensions, we\ndemonstrate that, under specific assumptions, the centering constants can be\nreplaced by the population cost, which is essential for statistical inference.\nThis generalizes and significantly refines existing results for the\none-dimensional case. Consequently, we present the first asymptotically valid\ninference framework for the sliced Wasserstein distance applicable to measures\nthat are not necessarily compactly supported, for p>1. Finally, we address key\npractical aspects for inference, including Monte Carlo estimation of the\nintegral and estimation of the asymptotic variance, ensuring applicability in\nreal-world scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimal transport theory has become a fundamental tool for handling diverse\ntypes of data, with growing applications across various fields. However, the\nWasserstein distance presents significant computational and statistical\nchallenges in high-dimensional settings. To address these issues, alternative\ndistances such as the sliced Wasserstein distance, which leverages\none-dimensional projections, have been introduced. In this work, we establish a\nnovel central limit theorem for the p-sliced Wasserstein distance, for p>1,\nusing the Efron-Stein inequality-a technique that has proven effective in\nrelated problems. This approach yields a central limit theorem centered at the\nexpected value of the empirical cost, under mild regularity conditions.\nNotably, unlike the general Wasserstein distance in arbitrary dimensions, we\ndemonstrate that, under specific assumptions, the centering constants can be\nreplaced by the population cost, which is essential for statistical inference.\nThis generalizes and significantly refines existing results for the\none-dimensional case. Consequently, we present the first asymptotically valid\ninference framework for the sliced Wasserstein distance applicable to measures\nthat are not necessarily compactly supported, for p>1. Finally, we address key\npractical aspects for inference, including Monte Carlo estimation of the\nintegral and estimation of the asymptotic variance, ensuring applicability in\nreal-world scenarios."
                },
                "authors": [
                    {
                        "name": "David Rodrguez-Vtores"
                    },
                    {
                        "name": "Eustasio del Barrio"
                    },
                    {
                        "name": "Jean-Michel Loubes"
                    }
                ],
                "author_detail": {
                    "name": "Jean-Michel Loubes"
                },
                "author": "Jean-Michel Loubes",
                "arxiv_comment": "33 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18831v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18831v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62E20",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18825v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18825v1",
                "updated": "2025-03-24T16:06:04Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    16,
                    6,
                    4,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T16:06:04Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    16,
                    6,
                    4,
                    0,
                    83,
                    0
                ],
                "title": "EconEvals: Benchmarks and Litmus Tests for LLM Agents in Unknown\n  Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EconEvals: Benchmarks and Litmus Tests for LLM Agents in Unknown\n  Environments"
                },
                "summary": "We develop benchmarks for LLM agents that act in, learn from, and strategize\nin unknown environments, the specifications of which the LLM agent must learn\nover time from deliberate exploration. Our benchmarks consist of\ndecision-making tasks derived from key problems in economics. To forestall\nsaturation, the benchmark tasks are synthetically generated with scalable\ndifficulty levels. Additionally, we propose litmus tests, a new kind of\nquantitative measure for LLMs and LLM agents. Unlike benchmarks, litmus tests\nquantify differences in character, values, and tendencies of LLMs and LLM\nagents, by considering their behavior when faced with tradeoffs (e.g.,\nefficiency versus equality) where there is no objectively right or wrong\nbehavior. Overall, our benchmarks and litmus tests assess the abilities and\ntendencies of LLM agents in tackling complex economic problems in diverse\nsettings spanning procurement, scheduling, task allocation, and pricing --\napplications that should grow in importance as such agents are further\nintegrated into the economy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We develop benchmarks for LLM agents that act in, learn from, and strategize\nin unknown environments, the specifications of which the LLM agent must learn\nover time from deliberate exploration. Our benchmarks consist of\ndecision-making tasks derived from key problems in economics. To forestall\nsaturation, the benchmark tasks are synthetically generated with scalable\ndifficulty levels. Additionally, we propose litmus tests, a new kind of\nquantitative measure for LLMs and LLM agents. Unlike benchmarks, litmus tests\nquantify differences in character, values, and tendencies of LLMs and LLM\nagents, by considering their behavior when faced with tradeoffs (e.g.,\nefficiency versus equality) where there is no objectively right or wrong\nbehavior. Overall, our benchmarks and litmus tests assess the abilities and\ntendencies of LLM agents in tackling complex economic problems in diverse\nsettings spanning procurement, scheduling, task allocation, and pricing --\napplications that should grow in importance as such agents are further\nintegrated into the economy."
                },
                "authors": [
                    {
                        "name": "Sara Fish"
                    },
                    {
                        "name": "Julia Shephard"
                    },
                    {
                        "name": "Minkai Li"
                    },
                    {
                        "name": "Ran I. Shorrer"
                    },
                    {
                        "name": "Yannai A. Gonczarowski"
                    }
                ],
                "author_detail": {
                    "name": "Yannai A. Gonczarowski"
                },
                "author": "Yannai A. Gonczarowski",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18825v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18825v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18813v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18813v1",
                "updated": "2025-03-24T15:54:10Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    15,
                    54,
                    10,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T15:54:10Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    15,
                    54,
                    10,
                    0,
                    83,
                    0
                ],
                "title": "Defeating Prompt Injections by Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Defeating Prompt Injections by Design"
                },
                "summary": "Large Language Models (LLMs) are increasingly deployed in agentic systems\nthat interact with an external environment. However, LLM agents are vulnerable\nto prompt injection attacks when handling untrusted data. In this paper we\npropose CaMeL, a robust defense that creates a protective system layer around\nthe LLM, securing it even when underlying models may be susceptible to attacks.\nTo operate, CaMeL explicitly extracts the control and data flows from the\n(trusted) query; therefore, the untrusted data retrieved by the LLM can never\nimpact the program flow. To further improve security, CaMeL relies on a notion\nof a capability to prevent the exfiltration of private data over unauthorized\ndata flows. We demonstrate effectiveness of CaMeL by solving $67\\%$ of tasks\nwith provable security in AgentDojo [NeurIPS 2024], a recent agentic security\nbenchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly deployed in agentic systems\nthat interact with an external environment. However, LLM agents are vulnerable\nto prompt injection attacks when handling untrusted data. In this paper we\npropose CaMeL, a robust defense that creates a protective system layer around\nthe LLM, securing it even when underlying models may be susceptible to attacks.\nTo operate, CaMeL explicitly extracts the control and data flows from the\n(trusted) query; therefore, the untrusted data retrieved by the LLM can never\nimpact the program flow. To further improve security, CaMeL relies on a notion\nof a capability to prevent the exfiltration of private data over unauthorized\ndata flows. We demonstrate effectiveness of CaMeL by solving $67\\%$ of tasks\nwith provable security in AgentDojo [NeurIPS 2024], a recent agentic security\nbenchmark."
                },
                "authors": [
                    {
                        "name": "Edoardo Debenedetti"
                    },
                    {
                        "name": "Ilia Shumailov"
                    },
                    {
                        "name": "Tianqi Fan"
                    },
                    {
                        "name": "Jamie Hayes"
                    },
                    {
                        "name": "Nicholas Carlini"
                    },
                    {
                        "name": "Daniel Fabian"
                    },
                    {
                        "name": "Christoph Kern"
                    },
                    {
                        "name": "Chongyang Shi"
                    },
                    {
                        "name": "Andreas Terzis"
                    },
                    {
                        "name": "Florian Tramr"
                    }
                ],
                "author_detail": {
                    "name": "Florian Tramr"
                },
                "author": "Florian Tramr",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18813v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18813v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18809v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18809v1",
                "updated": "2025-03-24T15:50:20Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    15,
                    50,
                    20,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T15:50:20Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    15,
                    50,
                    20,
                    0,
                    83,
                    0
                ],
                "title": "Classical Planning with LLM-Generated Heuristics: Challenging the State\n  of the Art with Python Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Classical Planning with LLM-Generated Heuristics: Challenging the State\n  of the Art with Python Code"
                },
                "summary": "In recent years, large language models (LLMs) have shown remarkable\ncapabilities in various artificial intelligence problems. However, they fail to\nplan reliably, even when prompted with a detailed definition of the planning\ntask. Attempts to improve their planning capabilities, such as chain-of-thought\nprompting, fine-tuning, and explicit \"reasoning\" still yield incorrect plans\nand usually fail to generalize to larger tasks. In this paper, we show how to\nuse LLMs to generate correct plans, even for out-of-distribution tasks of\nincreasing size. For a given planning domain, we ask an LLM to generate several\ndomain-dependent heuristic functions in the form of Python code, evaluate them\non a set of training tasks within a greedy best-first search, and choose the\nstrongest one. The resulting LLM-generated heuristics solve many more unseen\ntest tasks than state-of-the-art domain-independent heuristics for classical\nplanning. They are even competitive with the strongest learning algorithm for\ndomain-dependent planning. These findings are especially remarkable given that\nour proof-of-concept implementation is based on an unoptimized Python planner\nand the baselines all build upon highly optimized C++ code. In some domains,\nthe LLM-generated heuristics expand fewer states than the baselines, revealing\nthat they are not only efficiently computable, but sometimes even more\ninformative than the state-of-the-art heuristics. Overall, our results show\nthat sampling a set of planning heuristic function programs can significantly\nimprove the planning capabilities of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, large language models (LLMs) have shown remarkable\ncapabilities in various artificial intelligence problems. However, they fail to\nplan reliably, even when prompted with a detailed definition of the planning\ntask. Attempts to improve their planning capabilities, such as chain-of-thought\nprompting, fine-tuning, and explicit \"reasoning\" still yield incorrect plans\nand usually fail to generalize to larger tasks. In this paper, we show how to\nuse LLMs to generate correct plans, even for out-of-distribution tasks of\nincreasing size. For a given planning domain, we ask an LLM to generate several\ndomain-dependent heuristic functions in the form of Python code, evaluate them\non a set of training tasks within a greedy best-first search, and choose the\nstrongest one. The resulting LLM-generated heuristics solve many more unseen\ntest tasks than state-of-the-art domain-independent heuristics for classical\nplanning. They are even competitive with the strongest learning algorithm for\ndomain-dependent planning. These findings are especially remarkable given that\nour proof-of-concept implementation is based on an unoptimized Python planner\nand the baselines all build upon highly optimized C++ code. In some domains,\nthe LLM-generated heuristics expand fewer states than the baselines, revealing\nthat they are not only efficiently computable, but sometimes even more\ninformative than the state-of-the-art heuristics. Overall, our results show\nthat sampling a set of planning heuristic function programs can significantly\nimprove the planning capabilities of LLMs."
                },
                "authors": [
                    {
                        "name": "Augusto B. Corra"
                    },
                    {
                        "name": "Andr G. Pereira"
                    },
                    {
                        "name": "Jendrik Seipp"
                    }
                ],
                "author_detail": {
                    "name": "Jendrik Seipp"
                },
                "author": "Jendrik Seipp",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18809v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18809v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18792v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18792v1",
                "updated": "2025-03-24T15:39:25Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    15,
                    39,
                    25,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T15:39:25Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    15,
                    39,
                    25,
                    0,
                    83,
                    0
                ],
                "title": "REALM: A Dataset of Real-World LLM Use Cases",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "REALM: A Dataset of Real-World LLM Use Cases"
                },
                "summary": "Large Language Models, such as the GPT series, have driven significant\nindustrial applications, leading to economic and societal transformations.\nHowever, a comprehensive understanding of their real-world applications remains\nlimited. To address this, we introduce REALM, a dataset of over 94,000 LLM use\ncases collected from Reddit and news articles. REALM captures two key\ndimensions: the diverse applications of LLMs and the demographics of their\nusers. It categorizes LLM applications and explores how users' occupations\nrelate to the types of applications they use. By integrating real-world data,\nREALM offers insights into LLM adoption across different domains, providing a\nfoundation for future research on their evolving societal roles. A dedicated\ndashboard https://realm-e7682.web.app/ presents the data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models, such as the GPT series, have driven significant\nindustrial applications, leading to economic and societal transformations.\nHowever, a comprehensive understanding of their real-world applications remains\nlimited. To address this, we introduce REALM, a dataset of over 94,000 LLM use\ncases collected from Reddit and news articles. REALM captures two key\ndimensions: the diverse applications of LLMs and the demographics of their\nusers. It categorizes LLM applications and explores how users' occupations\nrelate to the types of applications they use. By integrating real-world data,\nREALM offers insights into LLM adoption across different domains, providing a\nfoundation for future research on their evolving societal roles. A dedicated\ndashboard https://realm-e7682.web.app/ presents the data."
                },
                "authors": [
                    {
                        "name": "Jingwen Cheng"
                    },
                    {
                        "name": "Kshitish Ghate"
                    },
                    {
                        "name": "Wenyue Hua"
                    },
                    {
                        "name": "William Yang Wang"
                    },
                    {
                        "name": "Hong Shen"
                    },
                    {
                        "name": "Fei Fang"
                    }
                ],
                "author_detail": {
                    "name": "Fei Fang"
                },
                "author": "Fei Fang",
                "arxiv_comment": "9 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18792v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18792v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.17811v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.17811v2",
                "updated": "2025-03-24T15:32:40Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    15,
                    32,
                    40,
                    0,
                    83,
                    0
                ],
                "published": "2024-05-28T04:13:21Z",
                "published_parsed": [
                    2024,
                    5,
                    28,
                    4,
                    13,
                    21,
                    1,
                    149,
                    0
                ],
                "title": "Mani-GS: Gaussian Splatting Manipulation with Triangular Mesh",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mani-GS: Gaussian Splatting Manipulation with Triangular Mesh"
                },
                "summary": "Neural 3D representations such as Neural Radiance Fields (NeRF), excel at\nproducing photo-realistic rendering results but lack the flexibility for\nmanipulation and editing which is crucial for content creation. Previous works\nhave attempted to address this issue by deforming a NeRF in canonical space or\nmanipulating the radiance field based on an explicit mesh. However,\nmanipulating NeRF is not highly controllable and requires a long training and\ninference time. With the emergence of 3D Gaussian Splatting (3DGS), extremely\nhigh-fidelity novel view synthesis can be achieved using an explicit\npoint-based 3D representation with much faster training and rendering speed.\nHowever, there is still a lack of effective means to manipulate 3DGS freely\nwhile maintaining rendering quality. In this work, we aim to tackle the\nchallenge of achieving manipulable photo-realistic rendering. We propose to\nutilize a triangular mesh to manipulate 3DGS directly with self-adaptation.\nThis approach reduces the need to design various algorithms for different types\nof Gaussian manipulation. By utilizing a triangle shape-aware Gaussian binding\nand adapting method, we can achieve 3DGS manipulation and preserve\nhigh-fidelity rendering after manipulation. Our approach is capable of handling\nlarge deformations, local manipulations, and soft body simulations while\nkeeping high-quality rendering. Furthermore, we demonstrate that our method is\nalso effective with inaccurate meshes extracted from 3DGS. Experiments\nconducted demonstrate the effectiveness of our method and its superiority over\nbaseline approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural 3D representations such as Neural Radiance Fields (NeRF), excel at\nproducing photo-realistic rendering results but lack the flexibility for\nmanipulation and editing which is crucial for content creation. Previous works\nhave attempted to address this issue by deforming a NeRF in canonical space or\nmanipulating the radiance field based on an explicit mesh. However,\nmanipulating NeRF is not highly controllable and requires a long training and\ninference time. With the emergence of 3D Gaussian Splatting (3DGS), extremely\nhigh-fidelity novel view synthesis can be achieved using an explicit\npoint-based 3D representation with much faster training and rendering speed.\nHowever, there is still a lack of effective means to manipulate 3DGS freely\nwhile maintaining rendering quality. In this work, we aim to tackle the\nchallenge of achieving manipulable photo-realistic rendering. We propose to\nutilize a triangular mesh to manipulate 3DGS directly with self-adaptation.\nThis approach reduces the need to design various algorithms for different types\nof Gaussian manipulation. By utilizing a triangle shape-aware Gaussian binding\nand adapting method, we can achieve 3DGS manipulation and preserve\nhigh-fidelity rendering after manipulation. Our approach is capable of handling\nlarge deformations, local manipulations, and soft body simulations while\nkeeping high-quality rendering. Furthermore, we demonstrate that our method is\nalso effective with inaccurate meshes extracted from 3DGS. Experiments\nconducted demonstrate the effectiveness of our method and its superiority over\nbaseline approaches."
                },
                "authors": [
                    {
                        "name": "Xiangjun Gao"
                    },
                    {
                        "name": "Xiaoyu Li"
                    },
                    {
                        "name": "Yiyu Zhuang"
                    },
                    {
                        "name": "Qi Zhang"
                    },
                    {
                        "name": "Wenbo Hu"
                    },
                    {
                        "name": "Chaopeng Zhang"
                    },
                    {
                        "name": "Yao Yao"
                    },
                    {
                        "name": "Ying Shan"
                    },
                    {
                        "name": "Long Quan"
                    }
                ],
                "author_detail": {
                    "name": "Long Quan"
                },
                "author": "Long Quan",
                "arxiv_comment": "CVPR 2025. Project page here: https://gaoxiangjun.github.io/mani_gs/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.17811v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.17811v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07696v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07696v2",
                "updated": "2025-03-24T15:28:30Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    15,
                    28,
                    30,
                    0,
                    83,
                    0
                ],
                "published": "2025-01-13T21:15:26Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    21,
                    15,
                    26,
                    0,
                    13,
                    0
                ],
                "title": "Inferring collective synchrony observing spiking of one or several\n  neurons",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inferring collective synchrony observing spiking of one or several\n  neurons"
                },
                "summary": "We tackle a quantification of synchrony in a large ensemble of interacting\nneurons from the observation of spiking events. In a simulation study, we\nefficiently infer the synchrony level in a neuronal population from a point\nprocess reflecting spiking of a small number of units and even from a single\nneuron. We introduce a synchrony measure (order parameter) based on the\nBartlett covariance density; this quantity can be easily computed from the\nrecorded point process. This measure is robust concerning missed spikes and, if\ncomputed from observing several neurons, does not require spike sorting. We\nillustrate the approach by modeling populations of spiking or bursting neurons,\nincluding the case of sparse synchrony.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We tackle a quantification of synchrony in a large ensemble of interacting\nneurons from the observation of spiking events. In a simulation study, we\nefficiently infer the synchrony level in a neuronal population from a point\nprocess reflecting spiking of a small number of units and even from a single\nneuron. We introduce a synchrony measure (order parameter) based on the\nBartlett covariance density; this quantity can be easily computed from the\nrecorded point process. This measure is robust concerning missed spikes and, if\ncomputed from observing several neurons, does not require spike sorting. We\nillustrate the approach by modeling populations of spiking or bursting neurons,\nincluding the case of sparse synchrony."
                },
                "authors": [
                    {
                        "name": "Arkady Pikovsky"
                    },
                    {
                        "name": "Michael Rosenblum"
                    }
                ],
                "author_detail": {
                    "name": "Michael Rosenblum"
                },
                "author": "Michael Rosenblum",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07696v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07696v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.NC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04598v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04598v2",
                "updated": "2025-03-24T15:27:13Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    15,
                    27,
                    13,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-06T16:40:48Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    16,
                    40,
                    48,
                    3,
                    65,
                    0
                ],
                "title": "HybridNorm: Towards Stable and Efficient Transformer Training via Hybrid\n  Normalization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HybridNorm: Towards Stable and Efficient Transformer Training via Hybrid\n  Normalization"
                },
                "summary": "Transformers have become the de facto architecture for a wide range of\nmachine learning tasks, particularly in large language models (LLMs). Despite\ntheir remarkable performance, challenges remain in training deep transformer\nnetworks, especially regarding the location of layer normalization. While\nPre-Norm structures facilitate easier training due to their more prominent\nidentity path, they often yield suboptimal performance compared to Post-Norm.\nIn this paper, we propose $\\textbf{HybridNorm}$, a straightforward yet\neffective hybrid normalization strategy that integrates the advantages of both\nPre-Norm and Post-Norm approaches. Specifically, HybridNorm employs QKV\nnormalization within the attention mechanism and Post-Norm in the feed-forward\nnetwork (FFN) of each transformer block. This design not only stabilizes\ntraining but also enhances performance, particularly in the context of LLMs.\nComprehensive experiments in both dense and sparse architectures show that\nHybridNorm consistently outperforms both Pre-Norm and Post-Norm approaches,\nachieving state-of-the-art results across various benchmarks. These findings\nhighlight the potential of HybridNorm as a more stable and effective technique\nfor improving the training and performance of deep transformer models. Code is\navailable at https://github.com/BryceZhuo/HybridNorm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers have become the de facto architecture for a wide range of\nmachine learning tasks, particularly in large language models (LLMs). Despite\ntheir remarkable performance, challenges remain in training deep transformer\nnetworks, especially regarding the location of layer normalization. While\nPre-Norm structures facilitate easier training due to their more prominent\nidentity path, they often yield suboptimal performance compared to Post-Norm.\nIn this paper, we propose $\\textbf{HybridNorm}$, a straightforward yet\neffective hybrid normalization strategy that integrates the advantages of both\nPre-Norm and Post-Norm approaches. Specifically, HybridNorm employs QKV\nnormalization within the attention mechanism and Post-Norm in the feed-forward\nnetwork (FFN) of each transformer block. This design not only stabilizes\ntraining but also enhances performance, particularly in the context of LLMs.\nComprehensive experiments in both dense and sparse architectures show that\nHybridNorm consistently outperforms both Pre-Norm and Post-Norm approaches,\nachieving state-of-the-art results across various benchmarks. These findings\nhighlight the potential of HybridNorm as a more stable and effective technique\nfor improving the training and performance of deep transformer models. Code is\navailable at https://github.com/BryceZhuo/HybridNorm."
                },
                "authors": [
                    {
                        "name": "Zhijian Zhuo"
                    },
                    {
                        "name": "Yutao Zeng"
                    },
                    {
                        "name": "Ya Wang"
                    },
                    {
                        "name": "Sijun Zhang"
                    },
                    {
                        "name": "Jian Yang"
                    },
                    {
                        "name": "Xiaoqing Li"
                    },
                    {
                        "name": "Xun Zhou"
                    },
                    {
                        "name": "Jinwen Ma"
                    }
                ],
                "author_detail": {
                    "name": "Jinwen Ma"
                },
                "author": "Jinwen Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04598v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04598v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18773v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18773v1",
                "updated": "2025-03-24T15:22:41Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    15,
                    22,
                    41,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T15:22:41Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    15,
                    22,
                    41,
                    0,
                    83,
                    0
                ],
                "title": "BitDecoding: Unlocking Tensor Cores for Long-Context LLMs Decoding with\n  Low-Bit KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BitDecoding: Unlocking Tensor Cores for Long-Context LLMs Decoding with\n  Low-Bit KV Cache"
                },
                "summary": "The growing adoption of long-context Large Language Models (LLMs) has\nintroduced significant memory and computational challenges in autoregressive\ndecoding due to the expanding Key-Value (KV) cache. KV cache quantization has\nemerged as a promising solution, with prior work showing that 4-bit or even\n2-bit quantization can maintain model accuracy while reducing memory costs.\nHowever, despite these benefits, preliminary implementations for the low-bit KV\ncache struggle to deliver the expected speedup due to quantization and\ndequantization overheads and the lack of Tensor Cores utilization. In this\nwork, we propose BitDecoding, a GPU-optimized framework that unlocks Tensor\nCores for efficient decoding with low-bit KV cache. Efficiently leveraging\nTensor Cores for low-bit KV cache is challenging due to the dynamic nature of\nKV cache generation at each decoding step. BitDecoding addresses these\nchallenges with a Tensor Cores-Centric BitFusion Scheme that ensures data\nlayout compatibility to enable high utilization of Tensor Cores. Additionally,\nBitDecoding incorporates a warp-efficient parallel decoding kernel and a\nfine-grained asynchronous pipeline, minimizing dequantization overhead and\nimproving computational efficiency. Experiments show that BitDecoding achieves\nup to 7.5x speedup on RTX 4090, 4.8x on A100, and 8.9x on H100, compared to\nFP16 FlashDecoding-v2. It also outperforms the state-of-the-art low-bit KV\ncache implementation (QServe) by up to 4.3x. On LLaMA-3.1-8B with a 128K\nsequence length, BitDecoding reduces single-batch decoding latency by 3x,\ndemonstrating its effectiveness in long-context generation scenarios. The code\nis available at https://github.com/DD-DuDa/BitDecoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing adoption of long-context Large Language Models (LLMs) has\nintroduced significant memory and computational challenges in autoregressive\ndecoding due to the expanding Key-Value (KV) cache. KV cache quantization has\nemerged as a promising solution, with prior work showing that 4-bit or even\n2-bit quantization can maintain model accuracy while reducing memory costs.\nHowever, despite these benefits, preliminary implementations for the low-bit KV\ncache struggle to deliver the expected speedup due to quantization and\ndequantization overheads and the lack of Tensor Cores utilization. In this\nwork, we propose BitDecoding, a GPU-optimized framework that unlocks Tensor\nCores for efficient decoding with low-bit KV cache. Efficiently leveraging\nTensor Cores for low-bit KV cache is challenging due to the dynamic nature of\nKV cache generation at each decoding step. BitDecoding addresses these\nchallenges with a Tensor Cores-Centric BitFusion Scheme that ensures data\nlayout compatibility to enable high utilization of Tensor Cores. Additionally,\nBitDecoding incorporates a warp-efficient parallel decoding kernel and a\nfine-grained asynchronous pipeline, minimizing dequantization overhead and\nimproving computational efficiency. Experiments show that BitDecoding achieves\nup to 7.5x speedup on RTX 4090, 4.8x on A100, and 8.9x on H100, compared to\nFP16 FlashDecoding-v2. It also outperforms the state-of-the-art low-bit KV\ncache implementation (QServe) by up to 4.3x. On LLaMA-3.1-8B with a 128K\nsequence length, BitDecoding reduces single-batch decoding latency by 3x,\ndemonstrating its effectiveness in long-context generation scenarios. The code\nis available at https://github.com/DD-DuDa/BitDecoding."
                },
                "authors": [
                    {
                        "name": "Dayou Du"
                    },
                    {
                        "name": "Shijie Cao"
                    },
                    {
                        "name": "Jianyi Cheng"
                    },
                    {
                        "name": "Ting Cao"
                    },
                    {
                        "name": "Mao Yang"
                    }
                ],
                "author_detail": {
                    "name": "Mao Yang"
                },
                "author": "Mao Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18773v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18773v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18769v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18769v1",
                "updated": "2025-03-24T15:16:51Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    15,
                    16,
                    51,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T15:16:51Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    15,
                    16,
                    51,
                    0,
                    83,
                    0
                ],
                "title": "AlphaSpace: Enabling Robotic Actions through Semantic Tokenization and\n  Symbolic Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AlphaSpace: Enabling Robotic Actions through Semantic Tokenization and\n  Symbolic Reasoning"
                },
                "summary": "This paper presents AlphaSpace, a novel methodology designed to enhance the\nspatial reasoning capabilities of large language models (LLMs) for 3D Cartesian\nspace navigation. AlphaSpace employs a semantics-based tokenization strategy,\nencoding height information through specialized semantic tokens, and integrates\nprimarily symbolic synthetic reasoning data. This approach enables LLMs to\naccurately manipulate objects by positioning them at specific [x, y, z]\ncoordinates. Experimental results demonstrate that AlphaSpace significantly\noutperforms existing models on manipulation subtasks, achieving a total\naccuracy of 66.67%, compared to 37.5% for GPT-4o and 29.17% for Claude 3.5\nSonnet.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents AlphaSpace, a novel methodology designed to enhance the\nspatial reasoning capabilities of large language models (LLMs) for 3D Cartesian\nspace navigation. AlphaSpace employs a semantics-based tokenization strategy,\nencoding height information through specialized semantic tokens, and integrates\nprimarily symbolic synthetic reasoning data. This approach enables LLMs to\naccurately manipulate objects by positioning them at specific [x, y, z]\ncoordinates. Experimental results demonstrate that AlphaSpace significantly\noutperforms existing models on manipulation subtasks, achieving a total\naccuracy of 66.67%, compared to 37.5% for GPT-4o and 29.17% for Claude 3.5\nSonnet."
                },
                "authors": [
                    {
                        "name": "Alan Dao"
                    },
                    {
                        "name": "Dinh Bach Vu"
                    },
                    {
                        "name": "Bui Quang Huy"
                    }
                ],
                "author_detail": {
                    "name": "Bui Quang Huy"
                },
                "arxiv_affiliation": "Gia Tuan Dao",
                "author": "Bui Quang Huy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18769v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18769v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16514v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16514v2",
                "updated": "2025-03-24T15:14:06Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    15,
                    14,
                    6,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-15T23:43:06Z",
                "published_parsed": [
                    2025,
                    3,
                    15,
                    23,
                    43,
                    6,
                    5,
                    74,
                    0
                ],
                "title": "VeriMind: Agentic LLM for Automated Verilog Generation with a Novel\n  Evaluation Metric",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VeriMind: Agentic LLM for Automated Verilog Generation with a Novel\n  Evaluation Metric"
                },
                "summary": "Designing Verilog modules requires meticulous attention to correctness,\nefficiency, and adherence to design specifications. However, manually writing\nVerilog code remains a complex and time-consuming task that demands both expert\nknowledge and iterative refinement. Leveraging recent advancements in large\nlanguage models (LLMs) and their structured text generation capabilities, we\npropose VeriMind, an agentic LLM framework for Verilog code generation that\nsignificantly automates and optimizes the synthesis process. Unlike traditional\nLLM-based code generators, VeriMind employs a structured reasoning approach:\ngiven a user-provided prompt describing design requirements, the system first\nformulates a detailed train of thought before the final Verilog code is\ngenerated. This multi-step methodology enhances interpretability, accuracy, and\nadaptability in hardware design. In addition, we introduce a novel evaluation\nmetric-pass@ARC-which combines the conventional pass@k measure with Average\nRefinement Cycles (ARC) to capture both success rate and the efficiency of\niterative refinement. Experimental results on diverse hardware design tasks\ndemonstrated that our approach achieved up to $8.3\\%$ improvement on pass@k\nmetric and $8.1\\%$ on pass@ARC metric. These findings underscore the\ntransformative potential of agentic LLMs in automated hardware design, RTL\ndevelopment, and digital system synthesis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Designing Verilog modules requires meticulous attention to correctness,\nefficiency, and adherence to design specifications. However, manually writing\nVerilog code remains a complex and time-consuming task that demands both expert\nknowledge and iterative refinement. Leveraging recent advancements in large\nlanguage models (LLMs) and their structured text generation capabilities, we\npropose VeriMind, an agentic LLM framework for Verilog code generation that\nsignificantly automates and optimizes the synthesis process. Unlike traditional\nLLM-based code generators, VeriMind employs a structured reasoning approach:\ngiven a user-provided prompt describing design requirements, the system first\nformulates a detailed train of thought before the final Verilog code is\ngenerated. This multi-step methodology enhances interpretability, accuracy, and\nadaptability in hardware design. In addition, we introduce a novel evaluation\nmetric-pass@ARC-which combines the conventional pass@k measure with Average\nRefinement Cycles (ARC) to capture both success rate and the efficiency of\niterative refinement. Experimental results on diverse hardware design tasks\ndemonstrated that our approach achieved up to $8.3\\%$ improvement on pass@k\nmetric and $8.1\\%$ on pass@ARC metric. These findings underscore the\ntransformative potential of agentic LLMs in automated hardware design, RTL\ndevelopment, and digital system synthesis."
                },
                "authors": [
                    {
                        "name": "Bardia Nadimi"
                    },
                    {
                        "name": "Ghali Omar Boutaib"
                    },
                    {
                        "name": "Hao Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Hao Zheng"
                },
                "author": "Hao Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16514v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16514v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18765v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18765v1",
                "updated": "2025-03-24T15:12:41Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    15,
                    12,
                    41,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T15:12:41Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    15,
                    12,
                    41,
                    0,
                    83,
                    0
                ],
                "title": "Group Decision-Making System with Sentiment Analysis of Discussion Chat\n  and Fuzzy Consensus Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Group Decision-Making System with Sentiment Analysis of Discussion Chat\n  and Fuzzy Consensus Modeling"
                },
                "summary": "Group Decision-Making (GDM) plays a crucial role in various real-life\nscenarios where individuals express their opinions in natural language rather\nthan structured numerical values. Traditional GDM approaches often overlook the\nsubjectivity and ambiguity present in human discussions, making it challenging\nto achieve a fair and consensus-driven decision. This paper proposes a fuzzy\nconsensus-based group decision-making system that integrates sentiment and\nemotion analysis to extract preference values from textual inputs. The proposed\nframework combines explicit voting preferences with sentiment scores derived\nfrom chat discussions, which are then processed using a Fuzzy Inference System\n(FIS) to compute a total preference score for each alternative and determine\nthe top-ranked option. To ensure fairness in group decision-making, we\nintroduce a fuzzy logic-based consensus measurement model that evaluates\nparticipants' agreement and confidence levels to assess overall feedback. To\nillustrate the effectiveness of our approach, we apply the methodology to a\nrestaurant selection scenario, where a group of individuals must decide on a\ndining option based on brief chat discussions. The results demonstrate that the\nfuzzy consensus mechanism successfully aggregates individual preferences and\nensures a balanced outcome that accurately reflects group sentiment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Group Decision-Making (GDM) plays a crucial role in various real-life\nscenarios where individuals express their opinions in natural language rather\nthan structured numerical values. Traditional GDM approaches often overlook the\nsubjectivity and ambiguity present in human discussions, making it challenging\nto achieve a fair and consensus-driven decision. This paper proposes a fuzzy\nconsensus-based group decision-making system that integrates sentiment and\nemotion analysis to extract preference values from textual inputs. The proposed\nframework combines explicit voting preferences with sentiment scores derived\nfrom chat discussions, which are then processed using a Fuzzy Inference System\n(FIS) to compute a total preference score for each alternative and determine\nthe top-ranked option. To ensure fairness in group decision-making, we\nintroduce a fuzzy logic-based consensus measurement model that evaluates\nparticipants' agreement and confidence levels to assess overall feedback. To\nillustrate the effectiveness of our approach, we apply the methodology to a\nrestaurant selection scenario, where a group of individuals must decide on a\ndining option based on brief chat discussions. The results demonstrate that the\nfuzzy consensus mechanism successfully aggregates individual preferences and\nensures a balanced outcome that accurately reflects group sentiment."
                },
                "authors": [
                    {
                        "name": "Adilet Yerkin"
                    },
                    {
                        "name": "Pakizar Shamoi"
                    }
                ],
                "author_detail": {
                    "name": "Pakizar Shamoi"
                },
                "author": "Pakizar Shamoi",
                "arxiv_comment": "Submitted to FUZZ-IEEE 2025 - 2025 IEEE International Conference on\n  Fuzzy Systems",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18765v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18765v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18760v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18760v1",
                "updated": "2025-03-24T15:09:03Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    15,
                    9,
                    3,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T15:09:03Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    15,
                    9,
                    3,
                    0,
                    83,
                    0
                ],
                "title": "Synthetic Function Demonstrations Improve Generation in Low-Resource\n  Programming Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synthetic Function Demonstrations Improve Generation in Low-Resource\n  Programming Languages"
                },
                "summary": "A key consideration when training an LLM is whether the target language is\nmore or less resourced, whether this is English compared to Welsh, or Python\ncompared to Excel. Typical training data for programming languages consist of\nreal program demonstrations coupled with human-written comments. Here we\npresent novel approaches to the creation of such data for low resource\nprogramming languages. We generate fully-synthetic, textbook-quality\ndemonstrations of common library functions in an example domain of Excel\nformulas, using a teacher model. We then finetune an underperforming student\nmodel, and show improvement on 2 question-answering datasets recast into the\nExcel domain. We show advantages of finetuning over standard, off-the-shelf RAG\napproaches, which can offer only modest improvement due to the unfamiliar\ntarget domain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A key consideration when training an LLM is whether the target language is\nmore or less resourced, whether this is English compared to Welsh, or Python\ncompared to Excel. Typical training data for programming languages consist of\nreal program demonstrations coupled with human-written comments. Here we\npresent novel approaches to the creation of such data for low resource\nprogramming languages. We generate fully-synthetic, textbook-quality\ndemonstrations of common library functions in an example domain of Excel\nformulas, using a teacher model. We then finetune an underperforming student\nmodel, and show improvement on 2 question-answering datasets recast into the\nExcel domain. We show advantages of finetuning over standard, off-the-shelf RAG\napproaches, which can offer only modest improvement due to the unfamiliar\ntarget domain."
                },
                "authors": [
                    {
                        "name": "Nick McKenna"
                    },
                    {
                        "name": "Xinnuo Xu"
                    },
                    {
                        "name": "Jack Williams"
                    },
                    {
                        "name": "Nick Wilson"
                    },
                    {
                        "name": "Benjamin Van Durme"
                    },
                    {
                        "name": "Christian Poelitz"
                    }
                ],
                "author_detail": {
                    "name": "Christian Poelitz"
                },
                "author": "Christian Poelitz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18760v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18760v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19824v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19824v3",
                "updated": "2025-03-24T15:07:22Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    15,
                    7,
                    22,
                    0,
                    83,
                    0
                ],
                "published": "2024-11-29T16:34:46Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    16,
                    34,
                    46,
                    4,
                    334,
                    0
                ],
                "title": "SAT-HMR: Real-Time Multi-Person 3D Mesh Estimation via Scale-Adaptive\n  Tokens",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SAT-HMR: Real-Time Multi-Person 3D Mesh Estimation via Scale-Adaptive\n  Tokens"
                },
                "summary": "We propose a one-stage framework for real-time multi-person 3D human mesh\nestimation from a single RGB image. While current one-stage methods, which\nfollow a DETR-style pipeline, achieve state-of-the-art (SOTA) performance with\nhigh-resolution inputs, we observe that this particularly benefits the\nestimation of individuals in smaller scales of the image (e.g., those far from\nthe camera), but at the cost of significantly increased computation overhead.\nTo address this, we introduce scale-adaptive tokens that are dynamically\nadjusted based on the relative scale of each individual in the image within the\nDETR framework. Specifically, individuals in smaller scales are processed at\nhigher resolutions, larger ones at lower resolutions, and background regions\nare further distilled. These scale-adaptive tokens more efficiently encode the\nimage features, facilitating subsequent decoding to regress the human mesh,\nwhile allowing the model to allocate computational resources more effectively\nand focus on more challenging cases. Experiments show that our method preserves\nthe accuracy benefits of high-resolution processing while substantially\nreducing computational cost, achieving real-time inference with performance\ncomparable to SOTA methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a one-stage framework for real-time multi-person 3D human mesh\nestimation from a single RGB image. While current one-stage methods, which\nfollow a DETR-style pipeline, achieve state-of-the-art (SOTA) performance with\nhigh-resolution inputs, we observe that this particularly benefits the\nestimation of individuals in smaller scales of the image (e.g., those far from\nthe camera), but at the cost of significantly increased computation overhead.\nTo address this, we introduce scale-adaptive tokens that are dynamically\nadjusted based on the relative scale of each individual in the image within the\nDETR framework. Specifically, individuals in smaller scales are processed at\nhigher resolutions, larger ones at lower resolutions, and background regions\nare further distilled. These scale-adaptive tokens more efficiently encode the\nimage features, facilitating subsequent decoding to regress the human mesh,\nwhile allowing the model to allocate computational resources more effectively\nand focus on more challenging cases. Experiments show that our method preserves\nthe accuracy benefits of high-resolution processing while substantially\nreducing computational cost, achieving real-time inference with performance\ncomparable to SOTA methods."
                },
                "authors": [
                    {
                        "name": "Chi Su"
                    },
                    {
                        "name": "Xiaoxuan Ma"
                    },
                    {
                        "name": "Jiajun Su"
                    },
                    {
                        "name": "Yizhou Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yizhou Wang"
                },
                "author": "Yizhou Wang",
                "arxiv_comment": "18 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19824v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19824v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18748v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18748v1",
                "updated": "2025-03-24T14:57:17Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    14,
                    57,
                    17,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T14:57:17Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    14,
                    57,
                    17,
                    0,
                    83,
                    0
                ],
                "title": "Simulation-Driven Balancing of Competitive Game Levels with\n  Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulation-Driven Balancing of Competitive Game Levels with\n  Reinforcement Learning"
                },
                "summary": "The balancing process for game levels in competitive two-player contexts\ninvolves a lot of manual work and testing, particularly for non-symmetrical\ngame levels. In this work, we frame game balancing as a procedural content\ngeneration task and propose an architecture for automatically balancing of\ntile-based levels within the PCGRL framework (procedural content generation via\nreinforcement learning). Our architecture is divided into three parts: (1) a\nlevel generator, (2) a balancing agent, and (3) a reward modeling simulation.\nThrough repeated simulations, the balancing agent receives rewards for\nadjusting the level towards a given balancing objective, such as equal win\nrates for all players. To this end, we propose new swap-based representations\nto improve the robustness of playability, thereby enabling agents to balance\ngame levels more effectively and quickly compared to traditional PCGRL. By\nanalyzing the agent's swapping behavior, we can infer which tile types have the\nmost impact on the balance. We validate our approach in the Neural MMO (NMMO)\nenvironment in a competitive two-player scenario. In this extended conference\npaper, we present improved results, explore the applicability of the method to\nvarious forms of balancing beyond equal balancing, compare the performance to\nanother search-based approach, and discuss the application of existing fairness\nmetrics to game balancing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The balancing process for game levels in competitive two-player contexts\ninvolves a lot of manual work and testing, particularly for non-symmetrical\ngame levels. In this work, we frame game balancing as a procedural content\ngeneration task and propose an architecture for automatically balancing of\ntile-based levels within the PCGRL framework (procedural content generation via\nreinforcement learning). Our architecture is divided into three parts: (1) a\nlevel generator, (2) a balancing agent, and (3) a reward modeling simulation.\nThrough repeated simulations, the balancing agent receives rewards for\nadjusting the level towards a given balancing objective, such as equal win\nrates for all players. To this end, we propose new swap-based representations\nto improve the robustness of playability, thereby enabling agents to balance\ngame levels more effectively and quickly compared to traditional PCGRL. By\nanalyzing the agent's swapping behavior, we can infer which tile types have the\nmost impact on the balance. We validate our approach in the Neural MMO (NMMO)\nenvironment in a competitive two-player scenario. In this extended conference\npaper, we present improved results, explore the applicability of the method to\nvarious forms of balancing beyond equal balancing, compare the performance to\nanother search-based approach, and discuss the application of existing fairness\nmetrics to game balancing."
                },
                "authors": [
                    {
                        "name": "Florian Rupp"
                    },
                    {
                        "name": "Manuel Eberhardinger"
                    },
                    {
                        "name": "Kai Eckert"
                    }
                ],
                "author_detail": {
                    "name": "Kai Eckert"
                },
                "author": "Kai Eckert",
                "arxiv_doi": "10.1109/TG.2024.3399536",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TG.2024.3399536",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.18748v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18748v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Preprint of the journal (IEEE Transactions on Games) paper of the\n  same name",
                "arxiv_journal_ref": "IEEE Transactions on Games, vol. 16, no. 4, pp. 903-913, 2024",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.00686v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.00686v5",
                "updated": "2025-03-24T14:48:43Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    14,
                    48,
                    43,
                    0,
                    83,
                    0
                ],
                "published": "2024-02-01T15:48:40Z",
                "published_parsed": [
                    2024,
                    2,
                    1,
                    15,
                    48,
                    40,
                    3,
                    32,
                    0
                ],
                "title": "Maximum a posteriori testing in statistical inverse problems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Maximum a posteriori testing in statistical inverse problems"
                },
                "summary": "This paper is concerned with a Bayesian approach to testing hypotheses in\nstatistical inverse problems. Based on the posterior distribution $\\Pi\n\\left(\\cdot |Y = y\\right)$, we want to infer whether a feature $\\langle\\varphi,\nu^\\dagger\\rangle$ of the unknown quantity of interest $u^\\dagger$ is positive.\nThis can be done by the so-called maximum a posteriori test. We provide a\nfrequentistic analysis of this test's properties such as level and power, and\nprove that it is a regularized test in the sense of Kretschmann et al. (2024).\nFurthermore we provide lower bounds for its power under classical spectral\nsource conditions in case of Gaussian priors. Numerical simulations illustrate\nits superior performance both in moderately and severely ill-posed situations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper is concerned with a Bayesian approach to testing hypotheses in\nstatistical inverse problems. Based on the posterior distribution $\\Pi\n\\left(\\cdot |Y = y\\right)$, we want to infer whether a feature $\\langle\\varphi,\nu^\\dagger\\rangle$ of the unknown quantity of interest $u^\\dagger$ is positive.\nThis can be done by the so-called maximum a posteriori test. We provide a\nfrequentistic analysis of this test's properties such as level and power, and\nprove that it is a regularized test in the sense of Kretschmann et al. (2024).\nFurthermore we provide lower bounds for its power under classical spectral\nsource conditions in case of Gaussian priors. Numerical simulations illustrate\nits superior performance both in moderately and severely ill-posed situations."
                },
                "authors": [
                    {
                        "name": "Remo Kretschmann"
                    },
                    {
                        "name": "Frank Werner"
                    }
                ],
                "author_detail": {
                    "name": "Frank Werner"
                },
                "author": "Frank Werner",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.00686v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.00686v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "47A52, 62F15, 62G10, 62G20, 65J20, 65J22",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13178v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13178v2",
                "updated": "2025-03-24T14:47:05Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    14,
                    47,
                    5,
                    0,
                    83,
                    0
                ],
                "published": "2025-02-18T07:35:35Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    7,
                    35,
                    35,
                    1,
                    49,
                    0
                ],
                "title": "Benchmarking Post-Training Quantization in LLMs: Comprehensive Taxonomy,\n  Unified Evaluation, and Comparative Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking Post-Training Quantization in LLMs: Comprehensive Taxonomy,\n  Unified Evaluation, and Comparative Analysis"
                },
                "summary": "Post-training Quantization (PTQ) technique has been extensively adopted for\nlarge language models (LLMs) compression owing to its efficiency and low\nresource requirement. However, current research lacks a in-depth analysis of\nthe superior and applicable scenarios of each PTQ strategy. In addition,\nexisting algorithms focus primarily on performance, overlooking the trade-off\namong model size, performance, and quantization bitwidth. To mitigate these\nconfusions, we provide a novel benchmark for LLMs PTQ in this paper. Firstly,\nin order to support our benchmark, we propose a comprehensive taxonomy for\nexisting mainstream methods by scrutinizing their computational strategies\n(e.g., optimization-based, compensation-based, etc.). Then, we conduct\nextensive experiments with the baseline within each class, covering models with\nvarious sizes (7B-70B), bitwidths, training levels (LLaMA1/2/3/3.1),\narchitectures (Mixtral, DeepSeekMoE and Mamba) and modality (LLaVA1.5 and\nVILA1.5) on a wide range of evaluation metrics.Through comparative analysis on\nthe results, we summarize the superior of each PTQ strategy and\nmodelsize-bitwidth trade-off considering the performance. For example, our\nbenchmark reveals that compensation-based technique demonstrates outstanding\ncross-architecture robustness and extremely low-bit PTQ for ultra large models\nshould be reexamined. Finally, we further accordingly claim that a practical\ncombination of compensation and other PTQ strategy can achieve SOTA various\nrobustness. We believe that our benchmark will provide valuable recommendations\nfor the deployment of LLMs and future research on PTQ approaches.We conduct an\nrepository for our benchmark at https://github.com/zjq0455/PTQ_Benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-training Quantization (PTQ) technique has been extensively adopted for\nlarge language models (LLMs) compression owing to its efficiency and low\nresource requirement. However, current research lacks a in-depth analysis of\nthe superior and applicable scenarios of each PTQ strategy. In addition,\nexisting algorithms focus primarily on performance, overlooking the trade-off\namong model size, performance, and quantization bitwidth. To mitigate these\nconfusions, we provide a novel benchmark for LLMs PTQ in this paper. Firstly,\nin order to support our benchmark, we propose a comprehensive taxonomy for\nexisting mainstream methods by scrutinizing their computational strategies\n(e.g., optimization-based, compensation-based, etc.). Then, we conduct\nextensive experiments with the baseline within each class, covering models with\nvarious sizes (7B-70B), bitwidths, training levels (LLaMA1/2/3/3.1),\narchitectures (Mixtral, DeepSeekMoE and Mamba) and modality (LLaVA1.5 and\nVILA1.5) on a wide range of evaluation metrics.Through comparative analysis on\nthe results, we summarize the superior of each PTQ strategy and\nmodelsize-bitwidth trade-off considering the performance. For example, our\nbenchmark reveals that compensation-based technique demonstrates outstanding\ncross-architecture robustness and extremely low-bit PTQ for ultra large models\nshould be reexamined. Finally, we further accordingly claim that a practical\ncombination of compensation and other PTQ strategy can achieve SOTA various\nrobustness. We believe that our benchmark will provide valuable recommendations\nfor the deployment of LLMs and future research on PTQ approaches.We conduct an\nrepository for our benchmark at https://github.com/zjq0455/PTQ_Benchmark."
                },
                "authors": [
                    {
                        "name": "Jiaqi Zhao"
                    },
                    {
                        "name": "Ming Wang"
                    },
                    {
                        "name": "Miao Zhang"
                    },
                    {
                        "name": "Yuzhang Shang"
                    },
                    {
                        "name": "Xuebo Liu"
                    },
                    {
                        "name": "Yaowei Wang"
                    },
                    {
                        "name": "Min Zhang"
                    },
                    {
                        "name": "Liqiang Nie"
                    }
                ],
                "author_detail": {
                    "name": "Liqiang Nie"
                },
                "author": "Liqiang Nie",
                "arxiv_comment": "17 pages, 3 fugures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13178v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13178v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18732v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18732v1",
                "updated": "2025-03-24T14:40:17Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    14,
                    40,
                    17,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T14:40:17Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    14,
                    40,
                    17,
                    0,
                    83,
                    0
                ],
                "title": "Embedding computational neurorehabilitation in clinical practice using a\n  modular intelligent health system",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embedding computational neurorehabilitation in clinical practice using a\n  modular intelligent health system"
                },
                "summary": "A significant and rising proportion of the global population suffer from\nnon-communicable diseases, such as neurological disorders. Neurorehabilitation\naims to restore function and independence of neurological patients through\nproviding interdisciplinary therapeutic interventions. Computational\nneurorehabilitation, an automated simulation approach to dynamically optimize\ntreatment effectivity, is a promising tool to ensure that each patient has the\nbest therapy for their current status. However, computational\nneurorehabilitation relies on integrated data flows between clinical\nassessments, predictive models, and healthcare professionals. Current\nneurorehabilitation practice is limited by low levels of digitalization and low\ndata interoperability. We here propose and demonstrate an embedded intelligent\nhealth system that enables detailed digital data collection in a modular\nfashion, real-time data flows between patients, models, and clinicians,\nclinical integration, and multi-context capacities, as required for\ncomputational neurorehabilitation approaches. We give an outlook on how modern\nexploratory data analysis tools can be integrated to facilitate model\ndevelopment and knowledge inference from secondary use of observational data\nthis system collects. With this blueprint, we contribute towards the\ndevelopment of integrated computational neurorehabilitation workflows for\nclinical practice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A significant and rising proportion of the global population suffer from\nnon-communicable diseases, such as neurological disorders. Neurorehabilitation\naims to restore function and independence of neurological patients through\nproviding interdisciplinary therapeutic interventions. Computational\nneurorehabilitation, an automated simulation approach to dynamically optimize\ntreatment effectivity, is a promising tool to ensure that each patient has the\nbest therapy for their current status. However, computational\nneurorehabilitation relies on integrated data flows between clinical\nassessments, predictive models, and healthcare professionals. Current\nneurorehabilitation practice is limited by low levels of digitalization and low\ndata interoperability. We here propose and demonstrate an embedded intelligent\nhealth system that enables detailed digital data collection in a modular\nfashion, real-time data flows between patients, models, and clinicians,\nclinical integration, and multi-context capacities, as required for\ncomputational neurorehabilitation approaches. We give an outlook on how modern\nexploratory data analysis tools can be integrated to facilitate model\ndevelopment and knowledge inference from secondary use of observational data\nthis system collects. With this blueprint, we contribute towards the\ndevelopment of integrated computational neurorehabilitation workflows for\nclinical practice."
                },
                "authors": [
                    {
                        "name": "Thomas Weikert"
                    },
                    {
                        "name": "Eljas Roellin"
                    },
                    {
                        "name": "Monica Prez-Serrano"
                    },
                    {
                        "name": "Elisa Du"
                    },
                    {
                        "name": "Lukas Heumos"
                    },
                    {
                        "name": "Fabian J. Theis"
                    },
                    {
                        "name": "Diego Paez-Granados"
                    },
                    {
                        "name": "Chris Easthope Awai"
                    }
                ],
                "author_detail": {
                    "name": "Chris Easthope Awai"
                },
                "author": "Chris Easthope Awai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18732v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18732v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16883v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16883v2",
                "updated": "2025-03-24T14:38:36Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    14,
                    38,
                    36,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-21T06:35:49Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    6,
                    35,
                    49,
                    4,
                    80,
                    0
                ],
                "title": "Assessing the Reliability and Validity of GPT-4 in Annotating Emotion\n  Appraisal Ratings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing the Reliability and Validity of GPT-4 in Annotating Emotion\n  Appraisal Ratings"
                },
                "summary": "Appraisal theories suggest that emotions arise from subjective evaluations of\nevents, referred to as appraisals. The taxonomy of appraisals is quite diverse,\nand they are usually given ratings on a Likert scale to be annotated in an\nexperiencer-annotator or reader-annotator paradigm. This paper studies GPT-4 as\na reader-annotator of 21 specific appraisal ratings in different prompt\nsettings, aiming to evaluate and improve its performance compared to human\nannotators. We found that GPT-4 is an effective reader-annotator that performs\nclose to or even slightly better than human annotators, and its results can be\nsignificantly improved by using a majority voting of five completions. GPT-4\nalso effectively predicts appraisal ratings and emotion labels using a single\nprompt, but adding instruction complexity results in poorer performance. We\nalso found that longer event descriptions lead to more accurate annotations for\nboth model and human annotator ratings. This work contributes to the growing\nusage of LLMs in psychology and the strategies for improving GPT-4 performance\nin annotating appraisals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Appraisal theories suggest that emotions arise from subjective evaluations of\nevents, referred to as appraisals. The taxonomy of appraisals is quite diverse,\nand they are usually given ratings on a Likert scale to be annotated in an\nexperiencer-annotator or reader-annotator paradigm. This paper studies GPT-4 as\na reader-annotator of 21 specific appraisal ratings in different prompt\nsettings, aiming to evaluate and improve its performance compared to human\nannotators. We found that GPT-4 is an effective reader-annotator that performs\nclose to or even slightly better than human annotators, and its results can be\nsignificantly improved by using a majority voting of five completions. GPT-4\nalso effectively predicts appraisal ratings and emotion labels using a single\nprompt, but adding instruction complexity results in poorer performance. We\nalso found that longer event descriptions lead to more accurate annotations for\nboth model and human annotator ratings. This work contributes to the growing\nusage of LLMs in psychology and the strategies for improving GPT-4 performance\nin annotating appraisals."
                },
                "authors": [
                    {
                        "name": "Deniss Ruder"
                    },
                    {
                        "name": "Andero Uusberg"
                    },
                    {
                        "name": "Kairit Sirts"
                    }
                ],
                "author_detail": {
                    "name": "Kairit Sirts"
                },
                "author": "Kairit Sirts",
                "arxiv_comment": "CLPsych 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16883v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16883v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18731v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18731v1",
                "updated": "2025-03-24T14:38:33Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    14,
                    38,
                    33,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T14:38:33Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    14,
                    38,
                    33,
                    0,
                    83,
                    0
                ],
                "title": "Thermalizer: Stable autoregressive neural emulation of spatiotemporal\n  chaos",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Thermalizer: Stable autoregressive neural emulation of spatiotemporal\n  chaos"
                },
                "summary": "Autoregressive surrogate models (or \\textit{emulators}) of spatiotemporal\nsystems provide an avenue for fast, approximate predictions, with broad\napplications across science and engineering. At inference time, however, these\nmodels are generally unable to provide predictions over long time rollouts due\nto accumulation of errors leading to diverging trajectories. In essence,\nemulators operate out of distribution, and controlling the online distribution\nquickly becomes intractable in large-scale settings. To address this\nfundamental issue, and focusing on time-stationary systems admitting an\ninvariant measure, we leverage diffusion models to obtain an implicit estimator\nof the score of this invariant measure. We show that this model of the score\nfunction can be used to stabilize autoregressive emulator rollouts by applying\non-the-fly denoising during inference, a process we call\n\\textit{thermalization}. Thermalizing an emulator rollout is shown to extend\nthe time horizon of stable predictions by an order of magnitude in complex\nsystems exhibiting turbulent and chaotic behavior, opening up a novel\napplication of diffusion models in the context of neural emulation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive surrogate models (or \\textit{emulators}) of spatiotemporal\nsystems provide an avenue for fast, approximate predictions, with broad\napplications across science and engineering. At inference time, however, these\nmodels are generally unable to provide predictions over long time rollouts due\nto accumulation of errors leading to diverging trajectories. In essence,\nemulators operate out of distribution, and controlling the online distribution\nquickly becomes intractable in large-scale settings. To address this\nfundamental issue, and focusing on time-stationary systems admitting an\ninvariant measure, we leverage diffusion models to obtain an implicit estimator\nof the score of this invariant measure. We show that this model of the score\nfunction can be used to stabilize autoregressive emulator rollouts by applying\non-the-fly denoising during inference, a process we call\n\\textit{thermalization}. Thermalizing an emulator rollout is shown to extend\nthe time horizon of stable predictions by an order of magnitude in complex\nsystems exhibiting turbulent and chaotic behavior, opening up a novel\napplication of diffusion models in the context of neural emulation."
                },
                "authors": [
                    {
                        "name": "Chris Pedersen"
                    },
                    {
                        "name": "Laure Zanna"
                    },
                    {
                        "name": "Joan Bruna"
                    }
                ],
                "author_detail": {
                    "name": "Joan Bruna"
                },
                "author": "Joan Bruna",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18731v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18731v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18719v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18719v1",
                "updated": "2025-03-24T14:30:38Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    14,
                    30,
                    38,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T14:30:38Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    14,
                    30,
                    38,
                    0,
                    83,
                    0
                ],
                "title": "Boosting Resolution Generalization of Diffusion Transformers with\n  Randomized Positional Encodings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Boosting Resolution Generalization of Diffusion Transformers with\n  Randomized Positional Encodings"
                },
                "summary": "Resolution generalization in image generation tasks enables the production of\nhigher-resolution images with lower training resolution overhead. However, a\nsignificant challenge in resolution generalization, particularly in the widely\nused Diffusion Transformers, lies in the mismatch between the positional\nencodings encountered during testing and those used during training. While\nexisting methods have employed techniques such as interpolation, extrapolation,\nor their combinations, none have fully resolved this issue. In this paper, we\npropose a novel two-dimensional randomized positional encodings (RPE-2D)\nframework that focuses on learning positional order of image patches instead of\nthe specific distances between them, enabling seamless high- and low-resolution\nimage generation without requiring high- and low-resolution image training.\nSpecifically, RPE-2D independently selects positions over a broader range along\nboth the horizontal and vertical axes, ensuring that all position encodings are\ntrained during the inference phase, thus improving resolution generalization.\nAdditionally, we propose a random data augmentation technique to enhance the\nmodeling of position order. To address the issue of image cropping caused by\nthe augmentation, we introduce corresponding micro-conditioning to enable the\nmodel to perceive the specific cropping patterns. On the ImageNet dataset, our\nproposed RPE-2D achieves state-of-the-art resolution generalization\nperformance, outperforming existing competitive methods when trained at a\nresolution of $256 \\times 256$ and inferred at $384 \\times 384$ and $512 \\times\n512$, as well as when scaling from $512 \\times 512$ to $768 \\times 768$ and\n$1024 \\times 1024$. And it also exhibits outstanding capabilities in\nlow-resolution image generation, multi-stage training acceleration and\nmulti-resolution inheritance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Resolution generalization in image generation tasks enables the production of\nhigher-resolution images with lower training resolution overhead. However, a\nsignificant challenge in resolution generalization, particularly in the widely\nused Diffusion Transformers, lies in the mismatch between the positional\nencodings encountered during testing and those used during training. While\nexisting methods have employed techniques such as interpolation, extrapolation,\nor their combinations, none have fully resolved this issue. In this paper, we\npropose a novel two-dimensional randomized positional encodings (RPE-2D)\nframework that focuses on learning positional order of image patches instead of\nthe specific distances between them, enabling seamless high- and low-resolution\nimage generation without requiring high- and low-resolution image training.\nSpecifically, RPE-2D independently selects positions over a broader range along\nboth the horizontal and vertical axes, ensuring that all position encodings are\ntrained during the inference phase, thus improving resolution generalization.\nAdditionally, we propose a random data augmentation technique to enhance the\nmodeling of position order. To address the issue of image cropping caused by\nthe augmentation, we introduce corresponding micro-conditioning to enable the\nmodel to perceive the specific cropping patterns. On the ImageNet dataset, our\nproposed RPE-2D achieves state-of-the-art resolution generalization\nperformance, outperforming existing competitive methods when trained at a\nresolution of $256 \\times 256$ and inferred at $384 \\times 384$ and $512 \\times\n512$, as well as when scaling from $512 \\times 512$ to $768 \\times 768$ and\n$1024 \\times 1024$. And it also exhibits outstanding capabilities in\nlow-resolution image generation, multi-stage training acceleration and\nmulti-resolution inheritance."
                },
                "authors": [
                    {
                        "name": "Cong Liu"
                    },
                    {
                        "name": "Liang Hou"
                    },
                    {
                        "name": "Mingwu Zheng"
                    },
                    {
                        "name": "Xin Tao"
                    },
                    {
                        "name": "Pengfei Wan"
                    },
                    {
                        "name": "Di Zhang"
                    },
                    {
                        "name": "Kun Gai"
                    }
                ],
                "author_detail": {
                    "name": "Kun Gai"
                },
                "author": "Kun Gai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18719v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18719v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.11905v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.11905v3",
                "updated": "2025-03-24T14:30:31Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    14,
                    30,
                    31,
                    0,
                    83,
                    0
                ],
                "published": "2024-07-16T16:38:47Z",
                "published_parsed": [
                    2024,
                    7,
                    16,
                    16,
                    38,
                    47,
                    1,
                    198,
                    0
                ],
                "title": "An Overview and Solution for Democratizing AI Workflows at the Network\n  Edge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Overview and Solution for Democratizing AI Workflows at the Network\n  Edge"
                },
                "summary": "With the process of democratization of the network edge, hardware and\nsoftware for networks are becoming available to the public, overcoming the\nconfines of traditional cloud providers and network operators. This trend,\ncoupled with the increasing importance of AI in 6G and beyond cellular\nnetworks, presents opportunities for innovative AI applications and systems at\nthe network edge. While AI models and services are well-managed in cloud\nsystems, achieving similar maturity for serving network needs remains an open\nchallenge. Existing open solutions are emerging and are yet to consider\ndemocratization requirements. In this work, we identify key requirements for\ndemocratization and propose NAOMI, a solution for democratizing AI/ML workflows\nat the network edge designed based on those requirements. Guided by the\nfunctionality and overlap analysis of the O-RAN AI/ML workflow architecture and\nMLOps systems, coupled with the survey of open-source AI/ML tools, we develop a\nmodular, scalable, and distributed hardware architecture-independent solution.\nNAOMI leverages state-of-the-art open-source tools and can be deployed on\ndistributed clusters of heterogeneous devices. The results show that NAOMI\nperforms up to 40% better in deployment time and up to 73% faster in AI/ML\nworkflow execution for larger datasets compared to AI/ML Framework, a\nrepresentative open network access solution, while performing inference and\nutilizing resources on par with its counterpart.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the process of democratization of the network edge, hardware and\nsoftware for networks are becoming available to the public, overcoming the\nconfines of traditional cloud providers and network operators. This trend,\ncoupled with the increasing importance of AI in 6G and beyond cellular\nnetworks, presents opportunities for innovative AI applications and systems at\nthe network edge. While AI models and services are well-managed in cloud\nsystems, achieving similar maturity for serving network needs remains an open\nchallenge. Existing open solutions are emerging and are yet to consider\ndemocratization requirements. In this work, we identify key requirements for\ndemocratization and propose NAOMI, a solution for democratizing AI/ML workflows\nat the network edge designed based on those requirements. Guided by the\nfunctionality and overlap analysis of the O-RAN AI/ML workflow architecture and\nMLOps systems, coupled with the survey of open-source AI/ML tools, we develop a\nmodular, scalable, and distributed hardware architecture-independent solution.\nNAOMI leverages state-of-the-art open-source tools and can be deployed on\ndistributed clusters of heterogeneous devices. The results show that NAOMI\nperforms up to 40% better in deployment time and up to 73% faster in AI/ML\nworkflow execution for larger datasets compared to AI/ML Framework, a\nrepresentative open network access solution, while performing inference and\nutilizing resources on par with its counterpart."
                },
                "authors": [
                    {
                        "name": "Andrej op"
                    },
                    {
                        "name": "Bla Bertalani"
                    },
                    {
                        "name": "Carolina Fortuna"
                    }
                ],
                "author_detail": {
                    "name": "Carolina Fortuna"
                },
                "author": "Carolina Fortuna",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.11905v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.11905v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10562v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10562v2",
                "updated": "2025-03-24T14:22:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    14,
                    22,
                    53,
                    0,
                    83,
                    0
                ],
                "published": "2024-08-20T06:03:40Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    6,
                    3,
                    40,
                    1,
                    233,
                    0
                ],
                "title": "Kalib: Easy Hand-Eye Calibration with Reference Point Tracking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Kalib: Easy Hand-Eye Calibration with Reference Point Tracking"
                },
                "summary": "Hand-eye calibration aims to estimate the transformation between a camera and\na robot. Traditional methods rely on fiducial markers, which require\nconsiderable manual effort and precise setup. Recent advances in deep learning\nhave introduced markerless techniques but come with more prerequisites, such as\nretraining networks for each robot, and accessing accurate mesh models for data\ngeneration. In this paper, we propose Kalib, an automatic and easy-to-setup\nhand-eye calibration method that leverages the generalizability of visual\nfoundation models to overcome these challenges. It features only two basic\nprerequisites, the robot's kinematic chain and a predefined reference point on\nthe robot. During calibration, the reference point is tracked in the camera\nspace. Its corresponding 3D coordinates in the robot coordinate can be inferred\nby forward kinematics. Then, a PnP solver directly estimates the transformation\nbetween the camera and the robot without training new networks or accessing\nmesh models. Evaluations in simulated and real-world benchmarks show that Kalib\nachieves good accuracy with a lower manual workload compared with recent\nbaseline methods. We also demonstrate its application in multiple real-world\nsettings with various robot arms and grippers. Kalib's user-friendly design and\nminimal setup requirements make it a possible solution for continuous operation\nin unstructured environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hand-eye calibration aims to estimate the transformation between a camera and\na robot. Traditional methods rely on fiducial markers, which require\nconsiderable manual effort and precise setup. Recent advances in deep learning\nhave introduced markerless techniques but come with more prerequisites, such as\nretraining networks for each robot, and accessing accurate mesh models for data\ngeneration. In this paper, we propose Kalib, an automatic and easy-to-setup\nhand-eye calibration method that leverages the generalizability of visual\nfoundation models to overcome these challenges. It features only two basic\nprerequisites, the robot's kinematic chain and a predefined reference point on\nthe robot. During calibration, the reference point is tracked in the camera\nspace. Its corresponding 3D coordinates in the robot coordinate can be inferred\nby forward kinematics. Then, a PnP solver directly estimates the transformation\nbetween the camera and the robot without training new networks or accessing\nmesh models. Evaluations in simulated and real-world benchmarks show that Kalib\nachieves good accuracy with a lower manual workload compared with recent\nbaseline methods. We also demonstrate its application in multiple real-world\nsettings with various robot arms and grippers. Kalib's user-friendly design and\nminimal setup requirements make it a possible solution for continuous operation\nin unstructured environments."
                },
                "authors": [
                    {
                        "name": "Tutian Tang"
                    },
                    {
                        "name": "Minghao Liu"
                    },
                    {
                        "name": "Wenqiang Xu"
                    },
                    {
                        "name": "Cewu Lu"
                    }
                ],
                "author_detail": {
                    "name": "Cewu Lu"
                },
                "author": "Cewu Lu",
                "arxiv_comment": "The code, data, and supplementary materials are available at\n  https://sites.google.com/view/hand-eye-kalib",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10562v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10562v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.14368v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.14368v2",
                "updated": "2025-03-24T14:21:55Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    14,
                    21,
                    55,
                    0,
                    83,
                    0
                ],
                "published": "2024-03-21T12:50:15Z",
                "published_parsed": [
                    2024,
                    3,
                    21,
                    12,
                    50,
                    15,
                    3,
                    81,
                    0
                ],
                "title": "CAGE: Unsupervised Visual Composition and Animation for Controllable\n  Video Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAGE: Unsupervised Visual Composition and Animation for Controllable\n  Video Generation"
                },
                "summary": "The field of video generation has expanded significantly in recent years,\nwith controllable and compositional video generation garnering considerable\ninterest. Most methods rely on leveraging annotations such as text, objects'\nbounding boxes, and motion cues, which require substantial human effort and\nthus limit their scalability. In contrast, we address the challenge of\ncontrollable and compositional video generation without any annotations by\nintroducing a novel unsupervised approach. Our model is trained from scratch on\na dataset of unannotated videos. At inference time, it can compose plausible\nnovel scenes and animate objects by placing object parts at the desired\nlocations in space and time. The core innovation of our method lies in the\nunified control format and the training process, where video generation is\nconditioned on a randomly selected subset of pre-trained self-supervised local\nfeatures. This conditioning compels the model to learn how to inpaint the\nmissing information in the video both spatially and temporally, thereby\nlearning the inherent compositionality of a scene and the dynamics of moving\nobjects. The abstraction level and the imposed invariance of the conditioning\ninput to minor visual perturbations enable control over object motion by simply\nusing the same features at all the desired future locations. We call our model\nCAGE, which stands for visual Composition and Animation for video GEneration.\nWe conduct extensive experiments to validate the effectiveness of CAGE across\nvarious scenarios, demonstrating its capability to accurately follow the\ncontrol and to generate high-quality videos that exhibit coherent scene\ncomposition and realistic animation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The field of video generation has expanded significantly in recent years,\nwith controllable and compositional video generation garnering considerable\ninterest. Most methods rely on leveraging annotations such as text, objects'\nbounding boxes, and motion cues, which require substantial human effort and\nthus limit their scalability. In contrast, we address the challenge of\ncontrollable and compositional video generation without any annotations by\nintroducing a novel unsupervised approach. Our model is trained from scratch on\na dataset of unannotated videos. At inference time, it can compose plausible\nnovel scenes and animate objects by placing object parts at the desired\nlocations in space and time. The core innovation of our method lies in the\nunified control format and the training process, where video generation is\nconditioned on a randomly selected subset of pre-trained self-supervised local\nfeatures. This conditioning compels the model to learn how to inpaint the\nmissing information in the video both spatially and temporally, thereby\nlearning the inherent compositionality of a scene and the dynamics of moving\nobjects. The abstraction level and the imposed invariance of the conditioning\ninput to minor visual perturbations enable control over object motion by simply\nusing the same features at all the desired future locations. We call our model\nCAGE, which stands for visual Composition and Animation for video GEneration.\nWe conduct extensive experiments to validate the effectiveness of CAGE across\nvarious scenarios, demonstrating its capability to accurately follow the\ncontrol and to generate high-quality videos that exhibit coherent scene\ncomposition and realistic animation."
                },
                "authors": [
                    {
                        "name": "Aram Davtyan"
                    },
                    {
                        "name": "Sepehr Sameni"
                    },
                    {
                        "name": "Bjrn Ommer"
                    },
                    {
                        "name": "Paolo Favaro"
                    }
                ],
                "author_detail": {
                    "name": "Paolo Favaro"
                },
                "author": "Paolo Favaro",
                "arxiv_comment": "Published at AAAI2025; Project website:\n  https://araachie.github.io/cage",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.14368v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.14368v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16942v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16942v3",
                "updated": "2025-03-25T08:12:22Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    8,
                    12,
                    22,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-21T08:40:35Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    8,
                    40,
                    35,
                    4,
                    80,
                    0
                ],
                "title": "Re-HOLD: Video Hand Object Interaction Reenactment via adaptive\n  Layout-instructed Diffusion Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Re-HOLD: Video Hand Object Interaction Reenactment via adaptive\n  Layout-instructed Diffusion Model"
                },
                "summary": "Current digital human studies focusing on lip-syncing and body movement are\nno longer sufficient to meet the growing industrial demand, while human video\ngeneration techniques that support interacting with real-world environments\n(e.g., objects) have not been well investigated. Despite human hand synthesis\nalready being an intricate problem, generating objects in contact with hands\nand their interactions presents an even more challenging task, especially when\nthe objects exhibit obvious variations in size and shape. To tackle these\nissues, we present a novel video Reenactment framework focusing on Human-Object\nInteraction (HOI) via an adaptive Layout-instructed Diffusion model (Re-HOLD).\nOur key insight is to employ specialized layout representation for hands and\nobjects, respectively. Such representations enable effective disentanglement of\nhand modeling and object adaptation to diverse motion sequences. To further\nimprove the generation quality of HOI, we design an interactive textural\nenhancement module for both hands and objects by introducing two independent\nmemory banks. We also propose a layout adjustment strategy for the cross-object\nreenactment scenario to adaptively adjust unreasonable layouts caused by\ndiverse object sizes during inference. Comprehensive qualitative and\nquantitative evaluations demonstrate that our proposed framework significantly\noutperforms existing methods. Project page: https://fyycs.github.io/Re-HOLD.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current digital human studies focusing on lip-syncing and body movement are\nno longer sufficient to meet the growing industrial demand, while human video\ngeneration techniques that support interacting with real-world environments\n(e.g., objects) have not been well investigated. Despite human hand synthesis\nalready being an intricate problem, generating objects in contact with hands\nand their interactions presents an even more challenging task, especially when\nthe objects exhibit obvious variations in size and shape. To tackle these\nissues, we present a novel video Reenactment framework focusing on Human-Object\nInteraction (HOI) via an adaptive Layout-instructed Diffusion model (Re-HOLD).\nOur key insight is to employ specialized layout representation for hands and\nobjects, respectively. Such representations enable effective disentanglement of\nhand modeling and object adaptation to diverse motion sequences. To further\nimprove the generation quality of HOI, we design an interactive textural\nenhancement module for both hands and objects by introducing two independent\nmemory banks. We also propose a layout adjustment strategy for the cross-object\nreenactment scenario to adaptively adjust unreasonable layouts caused by\ndiverse object sizes during inference. Comprehensive qualitative and\nquantitative evaluations demonstrate that our proposed framework significantly\noutperforms existing methods. Project page: https://fyycs.github.io/Re-HOLD."
                },
                "authors": [
                    {
                        "name": "Yingying Fan"
                    },
                    {
                        "name": "Quanwei Yang"
                    },
                    {
                        "name": "Kaisiyuan Wang"
                    },
                    {
                        "name": "Hang Zhou"
                    },
                    {
                        "name": "Yingying Li"
                    },
                    {
                        "name": "Haocheng Feng"
                    },
                    {
                        "name": "Errui Ding"
                    },
                    {
                        "name": "Yu Wu"
                    },
                    {
                        "name": "Jingdong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jingdong Wang"
                },
                "author": "Jingdong Wang",
                "arxiv_comment": "Accepted to CVPR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16942v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16942v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18706v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18706v1",
                "updated": "2025-03-24T14:17:57Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    14,
                    17,
                    57,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T14:17:57Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    14,
                    17,
                    57,
                    0,
                    83,
                    0
                ],
                "title": "Energy-Efficient Dynamic Training and Inference for GNN-Based Network\n  Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Energy-Efficient Dynamic Training and Inference for GNN-Based Network\n  Modeling"
                },
                "summary": "Efficient network modeling is essential for resource optimization and network\nplanning in next-generation large-scale complex networks. Traditional\napproaches, such as queuing theory-based modeling and packet-based simulators,\ncan be inefficient due to the assumption made and the computational expense,\nrespectively. To address these challenges, we propose an innovative\nenergy-efficient dynamic orchestration of Graph Neural Networks (GNN) based\nmodel training and inference framework for context-aware network modeling and\npredictions. We have developed a low-complexity solution framework, QAG, that\nis a Quantum approximation optimization (QAO) algorithm for Adaptive\norchestration of GNN-based network modeling. We leverage the tripartite graph\nmodel to represent a multi-application system with many compute nodes.\nThereafter, we apply the constrained graph-cutting using QAO to find the\nfeasible energy-efficient configurations of the GNN-based model and deploying\nthem on the available compute nodes to meet the network modeling application\nrequirements. The proposed QAG scheme closely matches the optimum and offers\natleast a 50% energy saving while meeting the application requirements with 60%\nlower churn-rate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient network modeling is essential for resource optimization and network\nplanning in next-generation large-scale complex networks. Traditional\napproaches, such as queuing theory-based modeling and packet-based simulators,\ncan be inefficient due to the assumption made and the computational expense,\nrespectively. To address these challenges, we propose an innovative\nenergy-efficient dynamic orchestration of Graph Neural Networks (GNN) based\nmodel training and inference framework for context-aware network modeling and\npredictions. We have developed a low-complexity solution framework, QAG, that\nis a Quantum approximation optimization (QAO) algorithm for Adaptive\norchestration of GNN-based network modeling. We leverage the tripartite graph\nmodel to represent a multi-application system with many compute nodes.\nThereafter, we apply the constrained graph-cutting using QAO to find the\nfeasible energy-efficient configurations of the GNN-based model and deploying\nthem on the available compute nodes to meet the network modeling application\nrequirements. The proposed QAG scheme closely matches the optimum and offers\natleast a 50% energy saving while meeting the application requirements with 60%\nlower churn-rate."
                },
                "authors": [
                    {
                        "name": "Chetna Singhal"
                    },
                    {
                        "name": "Yassine Hadjadj-Aoul"
                    }
                ],
                "author_detail": {
                    "name": "Yassine Hadjadj-Aoul"
                },
                "author": "Yassine Hadjadj-Aoul",
                "arxiv_comment": "Accepted in IEEE WCNC 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18706v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18706v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18698v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18698v1",
                "updated": "2025-03-24T14:10:30Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    14,
                    10,
                    30,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T14:10:30Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    14,
                    10,
                    30,
                    0,
                    83,
                    0
                ],
                "title": "Wireless Hearables With Programmable Speech AI Accelerators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wireless Hearables With Programmable Speech AI Accelerators"
                },
                "summary": "The conventional wisdom has been that designing ultra-compact,\nbattery-constrained wireless hearables with on-device speech AI models is\nchallenging due to the high computational demands of streaming deep learning\nmodels. Speech AI models require continuous, real-time audio processing,\nimposing strict computational and I/O constraints. We present NeuralAids, a\nfully on-device speech AI system for wireless hearables, enabling real-time\nspeech enhancement and denoising on compact, battery-constrained devices. Our\nsystem bridges the gap between state-of-the-art deep learning for speech\nenhancement and low-power AI hardware by making three key technical\ncontributions: 1) a wireless hearable platform integrating a speech AI\naccelerator for efficient on-device streaming inference, 2) an optimized\ndual-path neural network designed for low-latency, high-quality speech\nenhancement, and 3) a hardware-software co-design that uses mixed-precision\nquantization and quantization-aware training to achieve real-time performance\nunder strict power constraints. Our system processes 6 ms audio chunks in\nreal-time, achieving an inference time of 5.54 ms while consuming 71.6 mW. In\nreal-world evaluations, including a user study with 28 participants, our system\noutperforms prior on-device models in speech quality and noise suppression,\npaving the way for next-generation intelligent wireless hearables that can\nenhance hearing entirely on-device.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The conventional wisdom has been that designing ultra-compact,\nbattery-constrained wireless hearables with on-device speech AI models is\nchallenging due to the high computational demands of streaming deep learning\nmodels. Speech AI models require continuous, real-time audio processing,\nimposing strict computational and I/O constraints. We present NeuralAids, a\nfully on-device speech AI system for wireless hearables, enabling real-time\nspeech enhancement and denoising on compact, battery-constrained devices. Our\nsystem bridges the gap between state-of-the-art deep learning for speech\nenhancement and low-power AI hardware by making three key technical\ncontributions: 1) a wireless hearable platform integrating a speech AI\naccelerator for efficient on-device streaming inference, 2) an optimized\ndual-path neural network designed for low-latency, high-quality speech\nenhancement, and 3) a hardware-software co-design that uses mixed-precision\nquantization and quantization-aware training to achieve real-time performance\nunder strict power constraints. Our system processes 6 ms audio chunks in\nreal-time, achieving an inference time of 5.54 ms while consuming 71.6 mW. In\nreal-world evaluations, including a user study with 28 participants, our system\noutperforms prior on-device models in speech quality and noise suppression,\npaving the way for next-generation intelligent wireless hearables that can\nenhance hearing entirely on-device."
                },
                "authors": [
                    {
                        "name": "Malek Itani"
                    },
                    {
                        "name": "Tuochao Chen"
                    },
                    {
                        "name": "Arun Raghavan"
                    },
                    {
                        "name": "Gavriel Kohlberg"
                    },
                    {
                        "name": "Shyamnath Gollakota"
                    }
                ],
                "author_detail": {
                    "name": "Shyamnath Gollakota"
                },
                "author": "Shyamnath Gollakota",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18698v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18698v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18693v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18693v2",
                "updated": "2025-03-25T02:09:27Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    2,
                    9,
                    27,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-24T14:03:42Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    14,
                    3,
                    42,
                    0,
                    83,
                    0
                ],
                "title": "TARDIS: Mitigating Temporal Misalignment via Representation Steering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TARDIS: Mitigating Temporal Misalignment via Representation Steering"
                },
                "summary": "Language models often struggle with temporal misalignment, performance\ndegradation caused by shifts in the temporal distribution of data. Continuously\nupdating models to avoid degradation is expensive. Can models be adapted\nwithout updating model weights? We present TARDIS, an unsupervised\nrepresentation editing method that addresses this challenge. TARDIS extracts\nsteering vectors from unlabeled data and adjusts the model's representations to\nbetter align with the target time period's distribution. Our experiments reveal\nthat TARDIS enhances downstream task performance without the need for\nfine-tuning, can mitigate temporal misalignment even when exact target time\nperiod data is unavailable, and remains efficient even when the temporal\ninformation of the target data points is unknown at inference time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language models often struggle with temporal misalignment, performance\ndegradation caused by shifts in the temporal distribution of data. Continuously\nupdating models to avoid degradation is expensive. Can models be adapted\nwithout updating model weights? We present TARDIS, an unsupervised\nrepresentation editing method that addresses this challenge. TARDIS extracts\nsteering vectors from unlabeled data and adjusts the model's representations to\nbetter align with the target time period's distribution. Our experiments reveal\nthat TARDIS enhances downstream task performance without the need for\nfine-tuning, can mitigate temporal misalignment even when exact target time\nperiod data is unavailable, and remains efficient even when the temporal\ninformation of the target data points is unknown at inference time."
                },
                "authors": [
                    {
                        "name": "Changho Shin"
                    },
                    {
                        "name": "Xinya Yan"
                    },
                    {
                        "name": "Suenggwan Jo"
                    },
                    {
                        "name": "Sungjun Cho"
                    },
                    {
                        "name": "Shourjo Aditya Chaudhuri"
                    },
                    {
                        "name": "Frederic Sala"
                    }
                ],
                "author_detail": {
                    "name": "Frederic Sala"
                },
                "author": "Frederic Sala",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18693v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18693v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09019v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09019v2",
                "updated": "2025-03-24T13:47:26Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    13,
                    47,
                    26,
                    0,
                    83,
                    0
                ],
                "published": "2024-09-13T17:47:20Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    17,
                    47,
                    20,
                    4,
                    257,
                    0
                ],
                "title": "Nonequilibrium Phenomenology of Identified Particle Spectra in Heavy-Ion\n  Collisions at LHC Energies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nonequilibrium Phenomenology of Identified Particle Spectra in Heavy-Ion\n  Collisions at LHC Energies"
                },
                "summary": "We employ the Zubarev approach of the non-equilibrium statistical operator to\ninvestigate the enhancement of the low-$p_T$ region of pion spectra,\nintroducing an effective pion chemical potential to describe the overpopulation\nof low-energy pion states. We test a corresponding freeze-out approach by\nanalyzing the transverse-momentum spectra of identified particles measured\nrecently with high precision by the ALICE Collaboration in Pb+Pb collisions at\nCERN LHC. A blast-wave model and a blast-wave-based particle generator, coupled\nto a hadronic transport model, are utilized. Bayesian inference methods are\napplied to extract the most probable sets of thermodynamic parameters at the\nchemical freeze-out hypersurface. Both models for the overpopulated pion\nstates, the hadronic transport model and the thermal model with a nonzero pion\nchemical potential, provide a satisfactory description of the observed pion\nspectra. However, both approaches contain approximations which can be improved\nwithin a systematic nonequilibrium approach. We demonstrate that the\nintroduction of a nonequilibrium pion chemical potential offers an efficient\nalternative to the conventional explanation of the low-$p_T$ enhancement,\ntypically attributed to resonance decays with subsequent thermalization. A\nsimilar discussion holds also for the kaon spectra.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We employ the Zubarev approach of the non-equilibrium statistical operator to\ninvestigate the enhancement of the low-$p_T$ region of pion spectra,\nintroducing an effective pion chemical potential to describe the overpopulation\nof low-energy pion states. We test a corresponding freeze-out approach by\nanalyzing the transverse-momentum spectra of identified particles measured\nrecently with high precision by the ALICE Collaboration in Pb+Pb collisions at\nCERN LHC. A blast-wave model and a blast-wave-based particle generator, coupled\nto a hadronic transport model, are utilized. Bayesian inference methods are\napplied to extract the most probable sets of thermodynamic parameters at the\nchemical freeze-out hypersurface. Both models for the overpopulated pion\nstates, the hadronic transport model and the thermal model with a nonzero pion\nchemical potential, provide a satisfactory description of the observed pion\nspectra. However, both approaches contain approximations which can be improved\nwithin a systematic nonequilibrium approach. We demonstrate that the\nintroduction of a nonequilibrium pion chemical potential offers an efficient\nalternative to the conventional explanation of the low-$p_T$ enhancement,\ntypically attributed to resonance decays with subsequent thermalization. A\nsimilar discussion holds also for the kaon spectra."
                },
                "authors": [
                    {
                        "name": "Oleksandr Vitiuk"
                    },
                    {
                        "name": "David Blaschke"
                    },
                    {
                        "name": "Benjamin Dnigus"
                    },
                    {
                        "name": "Gerd Rpke"
                    }
                ],
                "author_detail": {
                    "name": "Gerd Rpke"
                },
                "author": "Gerd Rpke",
                "arxiv_comment": "7 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09019v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09019v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nucl-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05869v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05869v4",
                "updated": "2025-03-24T13:41:43Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    13,
                    41,
                    43,
                    0,
                    83,
                    0
                ],
                "published": "2024-10-08T09:57:14Z",
                "published_parsed": [
                    2024,
                    10,
                    8,
                    9,
                    57,
                    14,
                    1,
                    282,
                    0
                ],
                "title": "Believing is Seeing: Unobserved Object Detection using Generative Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Believing is Seeing: Unobserved Object Detection using Generative Models"
                },
                "summary": "Can objects that are not visible in an image -- but are in the vicinity of\nthe camera -- be detected? This study introduces the novel tasks of 2D, 2.5D\nand 3D unobserved object detection for predicting the location of nearby\nobjects that are occluded or lie outside the image frame. We adapt several\nstate-of-the-art pre-trained generative models to address this task, including\n2D and 3D diffusion models and vision-language models, and show that they can\nbe used to infer the presence of objects that are not directly observed. To\nbenchmark this task, we propose a suite of metrics that capture different\naspects of performance. Our empirical evaluation on indoor scenes from the\nRealEstate10k and NYU Depth v2 datasets demonstrate results that motivate the\nuse of generative models for the unobserved object detection task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can objects that are not visible in an image -- but are in the vicinity of\nthe camera -- be detected? This study introduces the novel tasks of 2D, 2.5D\nand 3D unobserved object detection for predicting the location of nearby\nobjects that are occluded or lie outside the image frame. We adapt several\nstate-of-the-art pre-trained generative models to address this task, including\n2D and 3D diffusion models and vision-language models, and show that they can\nbe used to infer the presence of objects that are not directly observed. To\nbenchmark this task, we propose a suite of metrics that capture different\naspects of performance. Our empirical evaluation on indoor scenes from the\nRealEstate10k and NYU Depth v2 datasets demonstrate results that motivate the\nuse of generative models for the unobserved object detection task."
                },
                "authors": [
                    {
                        "name": "Subhransu S. Bhattacharjee"
                    },
                    {
                        "name": "Dylan Campbell"
                    },
                    {
                        "name": "Rahul Shome"
                    }
                ],
                "author_detail": {
                    "name": "Rahul Shome"
                },
                "author": "Rahul Shome",
                "arxiv_comment": "IEEE/CVF Computer Vision and Pattern Recognition 2025; 22 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05869v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05869v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18666v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18666v1",
                "updated": "2025-03-24T13:31:48Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    13,
                    31,
                    48,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T13:31:48Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    13,
                    31,
                    48,
                    0,
                    83,
                    0
                ],
                "title": "AgentSpec: Customizable Runtime Enforcement for Safe and Reliable LLM\n  Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AgentSpec: Customizable Runtime Enforcement for Safe and Reliable LLM\n  Agents"
                },
                "summary": "Agents built on LLMs are increasingly deployed across diverse domains,\nautomating complex decision-making and task execution. However, their autonomy\nintroduces safety risks, including security vulnerabilities, legal violations,\nand unintended harmful actions. Existing mitigation methods, such as\nmodel-based safeguards and early enforcement strategies, fall short in\nrobustness, interpretability, and adaptability. To address these challenges, we\npropose AgentSpec, a lightweight domain-specific language for specifying and\nenforcing runtime constraints on LLM agents. With AgentSpec, users define\nstructured rules that incorporate triggers, predicates, and enforcement\nmechanisms, ensuring agents operate within predefined safety boundaries. We\nimplement AgentSpec across multiple domains, including code execution, embodied\nagents, and autonomous driving, demonstrating its adaptability and\neffectiveness. Our evaluation shows that AgentSpec successfully prevents unsafe\nexecutions in over 90% of code agent cases, eliminates all hazardous actions in\nembodied agent tasks, and enforces 100% compliance by autonomous vehicles\n(AVs). Despite its strong safety guarantees, AgentSpec remains computationally\nlightweight, with overheads in milliseconds. By combining interpretability,\nmodularity, and efficiency, AgentSpec provides a practical and scalable\nsolution for enforcing LLM agent safety across diverse applications. We also\nautomate the generation of rules using LLMs and assess their effectiveness. Our\nevaluation shows that the rules generated by OpenAI o1 achieve a precision of\n95.56% and recall of 70.96% for embodied agents, successfully identifying\n87.26% of the risky code, and prevent AVs from breaking laws in 5 out of 8\nscenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agents built on LLMs are increasingly deployed across diverse domains,\nautomating complex decision-making and task execution. However, their autonomy\nintroduces safety risks, including security vulnerabilities, legal violations,\nand unintended harmful actions. Existing mitigation methods, such as\nmodel-based safeguards and early enforcement strategies, fall short in\nrobustness, interpretability, and adaptability. To address these challenges, we\npropose AgentSpec, a lightweight domain-specific language for specifying and\nenforcing runtime constraints on LLM agents. With AgentSpec, users define\nstructured rules that incorporate triggers, predicates, and enforcement\nmechanisms, ensuring agents operate within predefined safety boundaries. We\nimplement AgentSpec across multiple domains, including code execution, embodied\nagents, and autonomous driving, demonstrating its adaptability and\neffectiveness. Our evaluation shows that AgentSpec successfully prevents unsafe\nexecutions in over 90% of code agent cases, eliminates all hazardous actions in\nembodied agent tasks, and enforces 100% compliance by autonomous vehicles\n(AVs). Despite its strong safety guarantees, AgentSpec remains computationally\nlightweight, with overheads in milliseconds. By combining interpretability,\nmodularity, and efficiency, AgentSpec provides a practical and scalable\nsolution for enforcing LLM agent safety across diverse applications. We also\nautomate the generation of rules using LLMs and assess their effectiveness. Our\nevaluation shows that the rules generated by OpenAI o1 achieve a precision of\n95.56% and recall of 70.96% for embodied agents, successfully identifying\n87.26% of the risky code, and prevent AVs from breaking laws in 5 out of 8\nscenarios."
                },
                "authors": [
                    {
                        "name": "Haoyu Wang"
                    },
                    {
                        "name": "Christopher M. Poskitt"
                    },
                    {
                        "name": "Jun Sun"
                    }
                ],
                "author_detail": {
                    "name": "Jun Sun"
                },
                "author": "Jun Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18666v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18666v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18665v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18665v1",
                "updated": "2025-03-24T13:30:47Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    13,
                    30,
                    47,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T13:30:47Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    13,
                    30,
                    47,
                    0,
                    83,
                    0
                ],
                "title": "Boosting Virtual Agent Learning and Reasoning: A Step-wise,\n  Multi-dimensional, and Generalist Reward Model with Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Boosting Virtual Agent Learning and Reasoning: A Step-wise,\n  Multi-dimensional, and Generalist Reward Model with Benchmark"
                },
                "summary": "The development of Generalist Virtual Agents (GVAs) powered by Multimodal\nLarge Language Models (MLLMs) has shown significant promise in autonomous task\nexecution. However, current training paradigms face critical limitations,\nincluding reliance on outcome supervision and labor-intensive human\nannotations. To address these challenges, we propose Similar, a Step-wise\nMulti-dimensional Generalist Reward Model, which offers fine-grained signals\nfor agent training and can choose better action for inference-time scaling.\nSpecifically, we begin by systematically defining five dimensions for\nevaluating agent actions. Building on this framework, we design an MCTS-P\nalgorithm to automatically collect and annotate step-wise, five-dimensional\nagent execution data. Using this data, we train Similar with the Triple-M\nstrategy. Furthermore, we introduce the first benchmark in the virtual agent\ndomain for step-wise, multi-dimensional reward model training and evaluation,\nnamed SRM. This benchmark consists of two components: SRMTrain, which serves as\nthe training set for Similar, and SRMEval, a manually selected test set for\nevaluating the reward model. Experimental results demonstrate that Similar,\nthrough its step-wise, multi-dimensional assessment and synergistic gain,\nprovides GVAs with effective intermediate signals during both training and\ninference-time scaling. The code is available at\nhttps://github.com/Galery23/Similar-v1.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The development of Generalist Virtual Agents (GVAs) powered by Multimodal\nLarge Language Models (MLLMs) has shown significant promise in autonomous task\nexecution. However, current training paradigms face critical limitations,\nincluding reliance on outcome supervision and labor-intensive human\nannotations. To address these challenges, we propose Similar, a Step-wise\nMulti-dimensional Generalist Reward Model, which offers fine-grained signals\nfor agent training and can choose better action for inference-time scaling.\nSpecifically, we begin by systematically defining five dimensions for\nevaluating agent actions. Building on this framework, we design an MCTS-P\nalgorithm to automatically collect and annotate step-wise, five-dimensional\nagent execution data. Using this data, we train Similar with the Triple-M\nstrategy. Furthermore, we introduce the first benchmark in the virtual agent\ndomain for step-wise, multi-dimensional reward model training and evaluation,\nnamed SRM. This benchmark consists of two components: SRMTrain, which serves as\nthe training set for Similar, and SRMEval, a manually selected test set for\nevaluating the reward model. Experimental results demonstrate that Similar,\nthrough its step-wise, multi-dimensional assessment and synergistic gain,\nprovides GVAs with effective intermediate signals during both training and\ninference-time scaling. The code is available at\nhttps://github.com/Galery23/Similar-v1."
                },
                "authors": [
                    {
                        "name": "Bingchen Miao"
                    },
                    {
                        "name": "Yang Wu"
                    },
                    {
                        "name": "Minghe Gao"
                    },
                    {
                        "name": "Qifan Yu"
                    },
                    {
                        "name": "Wendong Bu"
                    },
                    {
                        "name": "Wenqiao Zhang"
                    },
                    {
                        "name": "Yunfei Li"
                    },
                    {
                        "name": "Siliang Tang"
                    },
                    {
                        "name": "Tat-Seng Chua"
                    },
                    {
                        "name": "Juncheng Li"
                    }
                ],
                "author_detail": {
                    "name": "Juncheng Li"
                },
                "author": "Juncheng Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18665v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18665v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02542v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02542v3",
                "updated": "2025-03-25T08:38:21Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    8,
                    38,
                    21,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-04T12:12:37Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    12,
                    12,
                    37,
                    1,
                    63,
                    0
                ],
                "title": "Efficient Long Sequential Low-rank Adaptive Attention for Click-through\n  rate Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Long Sequential Low-rank Adaptive Attention for Click-through\n  rate Prediction"
                },
                "summary": "In the context of burgeoning user historical behavior data, Accurate\nclick-through rate(CTR) prediction requires effective modeling of lengthy user\nbehavior sequences. As the volume of such data keeps swelling, the focus of\nresearch has shifted towards developing effective long-term behavior modeling\nmethods to capture latent user interests. Nevertheless, the complexity\nintroduced by large scale data brings about computational hurdles. There is a\npressing need to strike a balance between achieving high model performance and\nmeeting the strict response time requirements of online services. While\nexisting retrieval-based methods (e.g., similarity filtering or attention\napproximation) achieve practical runtime efficiency, they inherently compromise\ninformation fidelity through aggressive sequence truncation or attention\nsparsification. This paper presents a novel attention mechanism. It overcomes\nthe shortcomings of existing methods while ensuring computational efficiency.\nThis mechanism learn compressed representation of sequence with length $L$ via\nlow-rank projection matrices (rank $r \\ll L$), reducing attention complexity\nfrom $O(L)$ to $O(r)$. It also integrates a uniquely designed loss function to\npreserve nonlinearity of attention. In the inference stage, the mechanism\nadopts matrix absorption and prestorage strategies. These strategies enable it\nto effectively satisfy online constraints. Comprehensive offline and online\nexperiments demonstrate that the proposed method outperforms current\nstate-of-the-art solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the context of burgeoning user historical behavior data, Accurate\nclick-through rate(CTR) prediction requires effective modeling of lengthy user\nbehavior sequences. As the volume of such data keeps swelling, the focus of\nresearch has shifted towards developing effective long-term behavior modeling\nmethods to capture latent user interests. Nevertheless, the complexity\nintroduced by large scale data brings about computational hurdles. There is a\npressing need to strike a balance between achieving high model performance and\nmeeting the strict response time requirements of online services. While\nexisting retrieval-based methods (e.g., similarity filtering or attention\napproximation) achieve practical runtime efficiency, they inherently compromise\ninformation fidelity through aggressive sequence truncation or attention\nsparsification. This paper presents a novel attention mechanism. It overcomes\nthe shortcomings of existing methods while ensuring computational efficiency.\nThis mechanism learn compressed representation of sequence with length $L$ via\nlow-rank projection matrices (rank $r \\ll L$), reducing attention complexity\nfrom $O(L)$ to $O(r)$. It also integrates a uniquely designed loss function to\npreserve nonlinearity of attention. In the inference stage, the mechanism\nadopts matrix absorption and prestorage strategies. These strategies enable it\nto effectively satisfy online constraints. Comprehensive offline and online\nexperiments demonstrate that the proposed method outperforms current\nstate-of-the-art solutions."
                },
                "authors": [
                    {
                        "name": "Xin Song"
                    },
                    {
                        "name": "Xiaochen Li"
                    },
                    {
                        "name": "Jinxin Hu"
                    },
                    {
                        "name": "Hong Wen"
                    },
                    {
                        "name": "Zulong Chen"
                    },
                    {
                        "name": "Yu Zhang"
                    },
                    {
                        "name": "Xiaoyi Zeng"
                    },
                    {
                        "name": "Jing Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jing Zhang"
                },
                "author": "Jing Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02542v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02542v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14502v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14502v3",
                "updated": "2025-03-24T13:16:03Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    13,
                    16,
                    3,
                    0,
                    83,
                    0
                ],
                "published": "2025-02-20T12:31:03Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    12,
                    31,
                    3,
                    3,
                    51,
                    0
                ],
                "title": "How Much Knowledge Can You Pack into a LoRA Adapter without Harming LLM?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Much Knowledge Can You Pack into a LoRA Adapter without Harming LLM?"
                },
                "summary": "The performance of Large Language Models (LLMs) on many tasks is greatly\nlimited by the knowledge learned during pre-training and stored in the model's\nparameters. Low-rank adaptation (LoRA) is a popular and efficient training\ntechnique for updating or domain-specific adaptation of LLMs. In this study, we\ninvestigate how new facts can be incorporated into the LLM using LoRA without\ncompromising the previously learned knowledge. We fine-tuned\nLlama-3.1-8B-instruct using LoRA with varying amounts of new knowledge. Our\nexperiments have shown that the best results are obtained when the training\ndata contains a mixture of known and new facts. However, this approach is still\npotentially harmful because the model's performance on external\nquestion-answering benchmarks declines after such fine-tuning. When the\ntraining data is biased towards certain entities, the model tends to regress to\nfew overrepresented answers. In addition, we found that the model becomes more\nconfident and refuses to provide an answer in only few cases. These findings\nhighlight the potential pitfalls of LoRA-based LLM updates and underscore the\nimportance of training data composition and tuning parameters to balance new\nknowledge integration and general model capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The performance of Large Language Models (LLMs) on many tasks is greatly\nlimited by the knowledge learned during pre-training and stored in the model's\nparameters. Low-rank adaptation (LoRA) is a popular and efficient training\ntechnique for updating or domain-specific adaptation of LLMs. In this study, we\ninvestigate how new facts can be incorporated into the LLM using LoRA without\ncompromising the previously learned knowledge. We fine-tuned\nLlama-3.1-8B-instruct using LoRA with varying amounts of new knowledge. Our\nexperiments have shown that the best results are obtained when the training\ndata contains a mixture of known and new facts. However, this approach is still\npotentially harmful because the model's performance on external\nquestion-answering benchmarks declines after such fine-tuning. When the\ntraining data is biased towards certain entities, the model tends to regress to\nfew overrepresented answers. In addition, we found that the model becomes more\nconfident and refuses to provide an answer in only few cases. These findings\nhighlight the potential pitfalls of LoRA-based LLM updates and underscore the\nimportance of training data composition and tuning parameters to balance new\nknowledge integration and general model capabilities."
                },
                "authors": [
                    {
                        "name": "Sergey Pletenev"
                    },
                    {
                        "name": "Maria Marina"
                    },
                    {
                        "name": "Daniil Moskovskiy"
                    },
                    {
                        "name": "Vasily Konovalov"
                    },
                    {
                        "name": "Pavel Braslavski"
                    },
                    {
                        "name": "Alexander Panchenko"
                    },
                    {
                        "name": "Mikhail Salnikov"
                    }
                ],
                "author_detail": {
                    "name": "Mikhail Salnikov"
                },
                "author": "Mikhail Salnikov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14502v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14502v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18637v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18637v1",
                "updated": "2025-03-24T13:00:25Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    13,
                    0,
                    25,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T13:00:25Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    13,
                    0,
                    25,
                    0,
                    83,
                    0
                ],
                "title": "Unbiasing through Textual Descriptions: Mitigating Representation Bias\n  in Video Benchmarks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unbiasing through Textual Descriptions: Mitigating Representation Bias\n  in Video Benchmarks"
                },
                "summary": "We propose a new \"Unbiased through Textual Description (UTD)\" video benchmark\nbased on unbiased subsets of existing video classification and retrieval\ndatasets to enable a more robust assessment of video understanding\ncapabilities. Namely, we tackle the problem that current video benchmarks may\nsuffer from different representation biases, e.g., object bias or single-frame\nbias, where mere recognition of objects or utilization of only a single frame\nis sufficient for correct prediction. We leverage VLMs and LLMs to analyze and\ndebias benchmarks from such representation biases. Specifically, we generate\nframe-wise textual descriptions of videos, filter them for specific information\n(e.g. only objects) and leverage them to examine representation biases across\nthree dimensions: 1) concept bias - determining if a specific concept (e.g.,\nobjects) alone suffice for prediction; 2) temporal bias - assessing if temporal\ninformation contributes to prediction; and 3) common sense vs. dataset bias -\nevaluating whether zero-shot reasoning or dataset correlations contribute to\nprediction. We conduct a systematic analysis of 12 popular video classification\nand retrieval datasets and create new object-debiased test splits for these\ndatasets. Moreover, we benchmark 30 state-of-the-art video models on original\nand debiased splits and analyze biases in the models. To facilitate the future\ndevelopment of more robust video understanding benchmarks and models, we\nrelease: \"UTD-descriptions\", a dataset with our rich structured descriptions\nfor each dataset, and \"UTD-splits\", a dataset of object-debiased test splits.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a new \"Unbiased through Textual Description (UTD)\" video benchmark\nbased on unbiased subsets of existing video classification and retrieval\ndatasets to enable a more robust assessment of video understanding\ncapabilities. Namely, we tackle the problem that current video benchmarks may\nsuffer from different representation biases, e.g., object bias or single-frame\nbias, where mere recognition of objects or utilization of only a single frame\nis sufficient for correct prediction. We leverage VLMs and LLMs to analyze and\ndebias benchmarks from such representation biases. Specifically, we generate\nframe-wise textual descriptions of videos, filter them for specific information\n(e.g. only objects) and leverage them to examine representation biases across\nthree dimensions: 1) concept bias - determining if a specific concept (e.g.,\nobjects) alone suffice for prediction; 2) temporal bias - assessing if temporal\ninformation contributes to prediction; and 3) common sense vs. dataset bias -\nevaluating whether zero-shot reasoning or dataset correlations contribute to\nprediction. We conduct a systematic analysis of 12 popular video classification\nand retrieval datasets and create new object-debiased test splits for these\ndatasets. Moreover, we benchmark 30 state-of-the-art video models on original\nand debiased splits and analyze biases in the models. To facilitate the future\ndevelopment of more robust video understanding benchmarks and models, we\nrelease: \"UTD-descriptions\", a dataset with our rich structured descriptions\nfor each dataset, and \"UTD-splits\", a dataset of object-debiased test splits."
                },
                "authors": [
                    {
                        "name": "Nina Shvetsova"
                    },
                    {
                        "name": "Arsha Nagrani"
                    },
                    {
                        "name": "Bernt Schiele"
                    },
                    {
                        "name": "Hilde Kuehne"
                    },
                    {
                        "name": "Christian Rupprecht"
                    }
                ],
                "author_detail": {
                    "name": "Christian Rupprecht"
                },
                "author": "Christian Rupprecht",
                "arxiv_comment": "To be published at CVPR 2025, project webpage\n  https://utd-project.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18637v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18637v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18634v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18634v1",
                "updated": "2025-03-24T12:52:26Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    12,
                    52,
                    26,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T12:52:26Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    12,
                    52,
                    26,
                    0,
                    83,
                    0
                ],
                "title": "Adaptive Machine Learning for Resource-Constrained Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Machine Learning for Resource-Constrained Environments"
                },
                "summary": "The Internet of Things is an example domain where data is perpetually\ngenerated in ever-increasing quantities, reflecting the proliferation of\nconnected devices and the formation of continuous data streams over time.\nConsequently, the demand for ad-hoc, cost-effective machine learning solutions\nmust adapt to this evolving data influx. This study tackles the task of\noffloading in small gateways, exacerbated by their dynamic availability over\ntime. An approach leveraging CPU utilization metrics using online and continual\nmachine learning techniques is proposed to predict gateway availability. These\nmethods are compared to popular machine learning algorithms and a recent\ntime-series foundation model, Lag-Llama, for fine-tuned and zero-shot setups.\nTheir performance is benchmarked on a dataset of CPU utilization measurements\nover time from an IoT gateway and focuses on model metrics such as prediction\nerrors, training and inference times, and memory consumption. Our primary\nobjective is to study new efficient ways to predict CPU performance in IoT\nenvironments. Across various scenarios, our findings highlight that ensemble\nand online methods offer promising results for this task in terms of accuracy\nwhile maintaining a low resource footprint.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Internet of Things is an example domain where data is perpetually\ngenerated in ever-increasing quantities, reflecting the proliferation of\nconnected devices and the formation of continuous data streams over time.\nConsequently, the demand for ad-hoc, cost-effective machine learning solutions\nmust adapt to this evolving data influx. This study tackles the task of\noffloading in small gateways, exacerbated by their dynamic availability over\ntime. An approach leveraging CPU utilization metrics using online and continual\nmachine learning techniques is proposed to predict gateway availability. These\nmethods are compared to popular machine learning algorithms and a recent\ntime-series foundation model, Lag-Llama, for fine-tuned and zero-shot setups.\nTheir performance is benchmarked on a dataset of CPU utilization measurements\nover time from an IoT gateway and focuses on model metrics such as prediction\nerrors, training and inference times, and memory consumption. Our primary\nobjective is to study new efficient ways to predict CPU performance in IoT\nenvironments. Across various scenarios, our findings highlight that ensemble\nand online methods offer promising results for this task in terms of accuracy\nwhile maintaining a low resource footprint."
                },
                "authors": [
                    {
                        "name": "Sebastin A. Cajas Ordez"
                    },
                    {
                        "name": "Jaydeep Samanta"
                    },
                    {
                        "name": "Andrs L. Surez-Cetrulo"
                    },
                    {
                        "name": "Ricardo Simn Carbajo"
                    }
                ],
                "author_detail": {
                    "name": "Ricardo Simn Carbajo"
                },
                "author": "Ricardo Simn Carbajo",
                "arxiv_doi": "10.1007/978-3-031-82346-6_1",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-031-82346-6_1",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.18634v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18634v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "17 pages, 11 figures, accepted at DELTA 2024 (Workshop on Discovering\n  Drift Phenomena in Evolving Landscapes), co-located with ACM SIGKDD 2024.\n  This preprint has not undergone peer review. The Version of Record is\n  available at https://doi.org/10.1007/978-3-031-82346-6_1",
                "arxiv_journal_ref": "Discovering Drift Phenomena in Evolving Landscapes, Lecture Notes\n  in Computer Science, LNCS 15013, Springer, 2025, pp. 3-19",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T05, 62M20",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.6; H.2.8",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11039v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11039v5",
                "updated": "2025-03-24T12:49:29Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    12,
                    49,
                    29,
                    0,
                    83,
                    0
                ],
                "published": "2025-01-19T13:14:53Z",
                "published_parsed": [
                    2025,
                    1,
                    19,
                    13,
                    14,
                    53,
                    6,
                    19,
                    0
                ],
                "title": "Model Predictive Task Sampling for Efficient and Robust Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model Predictive Task Sampling for Efficient and Robust Adaptation"
                },
                "summary": "Foundation models have revolutionized general-purpose problem-solving,\noffering rapid task adaptation through pretraining, meta-training, and\nfinetuning. Recent crucial advances in these paradigms reveal the importance of\nchallenging task prioritized sampling to enhance adaptation robustness under\ndistribution shifts. However, ranking task difficulties over iteration as a\npreliminary step typically requires exhaustive task evaluation, which is\npractically unaffordable in computation and data-annotation. This study\nprovides a novel perspective to illuminate the possibility of leveraging the\ndual importance of adaptation robustness and learning efficiency, particularly\nin scenarios where task evaluation is risky or costly, such as iterative\nagent-environment interactions for robotic policy evaluation or computationally\nintensive inference steps for finetuning foundation models. Firstly, we\nintroduce Model Predictive Task Sampling (MPTS), a framework that bridges the\ntask space and adaptation risk landscape, providing a theoretical foundation\nfor robust active task sampling. MPTS employs a generative model to\ncharacterize the episodic optimization process and predicts task-specific\nadaptation risk via posterior inference. The resulting risk learner amortizes\nthe costly evaluation of task adaptation performance and provably approximates\ntask difficulty rankings. MPTS seamlessly integrates into zero-shot, few-shot,\nand supervised finetuning settings. Empirically, we conduct extensive\nexperiments in pattern recognition using foundation models and sequential\ndecision-making. Our results demonstrate that MPTS significantly enhances\nadaptation robustness for tail or out-of-distribution (OOD) tasks and improves\nlearning efficiency compared to state-of-the-art (SOTA) methods. The code is\navailable at the project site https://github.com/thu-rllab/MPTS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Foundation models have revolutionized general-purpose problem-solving,\noffering rapid task adaptation through pretraining, meta-training, and\nfinetuning. Recent crucial advances in these paradigms reveal the importance of\nchallenging task prioritized sampling to enhance adaptation robustness under\ndistribution shifts. However, ranking task difficulties over iteration as a\npreliminary step typically requires exhaustive task evaluation, which is\npractically unaffordable in computation and data-annotation. This study\nprovides a novel perspective to illuminate the possibility of leveraging the\ndual importance of adaptation robustness and learning efficiency, particularly\nin scenarios where task evaluation is risky or costly, such as iterative\nagent-environment interactions for robotic policy evaluation or computationally\nintensive inference steps for finetuning foundation models. Firstly, we\nintroduce Model Predictive Task Sampling (MPTS), a framework that bridges the\ntask space and adaptation risk landscape, providing a theoretical foundation\nfor robust active task sampling. MPTS employs a generative model to\ncharacterize the episodic optimization process and predicts task-specific\nadaptation risk via posterior inference. The resulting risk learner amortizes\nthe costly evaluation of task adaptation performance and provably approximates\ntask difficulty rankings. MPTS seamlessly integrates into zero-shot, few-shot,\nand supervised finetuning settings. Empirically, we conduct extensive\nexperiments in pattern recognition using foundation models and sequential\ndecision-making. Our results demonstrate that MPTS significantly enhances\nadaptation robustness for tail or out-of-distribution (OOD) tasks and improves\nlearning efficiency compared to state-of-the-art (SOTA) methods. The code is\navailable at the project site https://github.com/thu-rllab/MPTS."
                },
                "authors": [
                    {
                        "name": "Qi Cheems Wang"
                    },
                    {
                        "name": "Zehao Xiao"
                    },
                    {
                        "name": "Yixiu Mao"
                    },
                    {
                        "name": "Yun Qu"
                    },
                    {
                        "name": "Jiayi Shen"
                    },
                    {
                        "name": "Yiqin Lv"
                    },
                    {
                        "name": "Xiangyang Ji"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyang Ji"
                },
                "author": "Xiangyang Ji",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11039v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11039v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18627v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18627v1",
                "updated": "2025-03-24T12:43:11Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    12,
                    43,
                    11,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T12:43:11Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    12,
                    43,
                    11,
                    0,
                    83,
                    0
                ],
                "title": "Dig2DIG: Dig into Diffusion Information Gains for Image Fusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dig2DIG: Dig into Diffusion Information Gains for Image Fusion"
                },
                "summary": "Image fusion integrates complementary information from multi-source images to\ngenerate more informative results. Recently, the diffusion model, which\ndemonstrates unprecedented generative potential, has been explored in image\nfusion. However, these approaches typically incorporate predefined multimodal\nguidance into diffusion, failing to capture the dynamically changing\nsignificance of each modality, while lacking theoretical guarantees. To address\nthis issue, we reveal a significant spatio-temporal imbalance in image\ndenoising; specifically, the diffusion model produces dynamic information gains\nin different image regions with denoising steps. Based on this observation, we\nDig into the Diffusion Information Gains (Dig2DIG) and theoretically derive a\ndiffusion-based dynamic image fusion framework that provably reduces the upper\nbound of the generalization error. Accordingly, we introduce diffusion\ninformation gains (DIG) to quantify the information contribution of each\nmodality at different denoising steps, thereby providing dynamic guidance\nduring the fusion process. Extensive experiments on multiple fusion scenarios\nconfirm that our method outperforms existing diffusion-based approaches in\nterms of both fusion quality and inference efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Image fusion integrates complementary information from multi-source images to\ngenerate more informative results. Recently, the diffusion model, which\ndemonstrates unprecedented generative potential, has been explored in image\nfusion. However, these approaches typically incorporate predefined multimodal\nguidance into diffusion, failing to capture the dynamically changing\nsignificance of each modality, while lacking theoretical guarantees. To address\nthis issue, we reveal a significant spatio-temporal imbalance in image\ndenoising; specifically, the diffusion model produces dynamic information gains\nin different image regions with denoising steps. Based on this observation, we\nDig into the Diffusion Information Gains (Dig2DIG) and theoretically derive a\ndiffusion-based dynamic image fusion framework that provably reduces the upper\nbound of the generalization error. Accordingly, we introduce diffusion\ninformation gains (DIG) to quantify the information contribution of each\nmodality at different denoising steps, thereby providing dynamic guidance\nduring the fusion process. Extensive experiments on multiple fusion scenarios\nconfirm that our method outperforms existing diffusion-based approaches in\nterms of both fusion quality and inference efficiency."
                },
                "authors": [
                    {
                        "name": "Bing Cao"
                    },
                    {
                        "name": "Baoshuo Cai"
                    },
                    {
                        "name": "Changqing Zhang"
                    },
                    {
                        "name": "Qinghua Hu"
                    }
                ],
                "author_detail": {
                    "name": "Qinghua Hu"
                },
                "author": "Qinghua Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18627v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18627v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05843v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05843v3",
                "updated": "2025-03-24T12:22:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    12,
                    22,
                    37,
                    0,
                    83,
                    0
                ],
                "published": "2025-02-09T10:30:54Z",
                "published_parsed": [
                    2025,
                    2,
                    9,
                    10,
                    30,
                    54,
                    6,
                    40,
                    0
                ],
                "title": "From Objects to Events: Unlocking Complex Visual Understanding in Object\n  Detectors via LLM-guided Symbolic Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Objects to Events: Unlocking Complex Visual Understanding in Object\n  Detectors via LLM-guided Symbolic Reasoning"
                },
                "summary": "Our key innovation lies in bridging the semantic gap between object detection\nand event understanding without requiring expensive task-specific training. The\nproposed plug-and-play framework interfaces with any open-vocabulary detector\nwhile extending their inherent capabilities across architectures. At its core,\nour approach combines (i) a symbolic regression mechanism exploring\nrelationship patterns among detected entities and (ii) a LLM-guided\nstrategically guiding the search toward meaningful expressions. These\ndiscovered symbolic rules transform low-level visual perception into\ninterpretable event understanding, providing a transparent reasoning path from\nobjects to events with strong transferability across domains.We compared our\ntraining-free framework against specialized event recognition systems across\ndiverse application domains. Experiments demonstrate that our framework\nenhances multiple object detector architectures to recognize complex events\nsuch as illegal fishing activities (75% AUROC, +8.36% improvement),\nconstruction safety violations (+15.77%), and abnormal crowd behaviors\n(+23.16%). The code will be released soon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Our key innovation lies in bridging the semantic gap between object detection\nand event understanding without requiring expensive task-specific training. The\nproposed plug-and-play framework interfaces with any open-vocabulary detector\nwhile extending their inherent capabilities across architectures. At its core,\nour approach combines (i) a symbolic regression mechanism exploring\nrelationship patterns among detected entities and (ii) a LLM-guided\nstrategically guiding the search toward meaningful expressions. These\ndiscovered symbolic rules transform low-level visual perception into\ninterpretable event understanding, providing a transparent reasoning path from\nobjects to events with strong transferability across domains.We compared our\ntraining-free framework against specialized event recognition systems across\ndiverse application domains. Experiments demonstrate that our framework\nenhances multiple object detector architectures to recognize complex events\nsuch as illegal fishing activities (75% AUROC, +8.36% improvement),\nconstruction safety violations (+15.77%), and abnormal crowd behaviors\n(+23.16%). The code will be released soon."
                },
                "authors": [
                    {
                        "name": "Yuhui Zeng"
                    },
                    {
                        "name": "Haoxiang Wu"
                    },
                    {
                        "name": "Wenjie Nie"
                    },
                    {
                        "name": "Xiawu Zheng"
                    },
                    {
                        "name": "Guangyao Chen"
                    },
                    {
                        "name": "Yunhang Shen"
                    },
                    {
                        "name": "Jun Peng"
                    },
                    {
                        "name": "Yonghong Tian"
                    },
                    {
                        "name": "Rongrong Ji"
                    }
                ],
                "author_detail": {
                    "name": "Rongrong Ji"
                },
                "author": "Rongrong Ji",
                "arxiv_comment": "13 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05843v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05843v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18617v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18617v1",
                "updated": "2025-03-24T12:20:24Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    12,
                    20,
                    24,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T12:20:24Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    12,
                    20,
                    24,
                    0,
                    83,
                    0
                ],
                "title": "Scaling Laws for Emulation of Stellar Spectra",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Laws for Emulation of Stellar Spectra"
                },
                "summary": "Neural network-based emulators for the inference of stellar parameters and\nelemental abundances represent an increasingly popular methodology in modern\nspectroscopic surveys. However, these approaches are often constrained by their\nemulation precision and domain transfer capabilities. Greater generalizability\nhas previously been achieved only with significantly larger model\narchitectures, as demonstrated by Transformer-based models in natural language\nprocessing. This observation aligns with neural scaling laws, where model\nperformance predictably improves with increased model size, computational\nresources allocated to model training, and training data volume. In this study,\nwe demonstrate that these scaling laws also apply to Transformer-based spectral\nemulators in astronomy. Building upon our previous work with TransformerPayne\nand incorporating Maximum Update Parametrization techniques from natural\nlanguage models, we provide training guidelines for scaling models to achieve\noptimal performance. Our results show that within the explored parameter space,\nclear scaling relationships emerge. These findings suggest that optimal\ncomputational resource allocation requires balanced scaling. Specifically,\ngiven a tenfold increase in training compute, achieving an optimal seven-fold\nreduction in mean squared error necessitates an approximately 2.5-fold increase\nin dataset size and a 3.8-fold increase in model size. This study establishes a\nfoundation for developing spectral foundational models with enhanced domain\ntransfer capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural network-based emulators for the inference of stellar parameters and\nelemental abundances represent an increasingly popular methodology in modern\nspectroscopic surveys. However, these approaches are often constrained by their\nemulation precision and domain transfer capabilities. Greater generalizability\nhas previously been achieved only with significantly larger model\narchitectures, as demonstrated by Transformer-based models in natural language\nprocessing. This observation aligns with neural scaling laws, where model\nperformance predictably improves with increased model size, computational\nresources allocated to model training, and training data volume. In this study,\nwe demonstrate that these scaling laws also apply to Transformer-based spectral\nemulators in astronomy. Building upon our previous work with TransformerPayne\nand incorporating Maximum Update Parametrization techniques from natural\nlanguage models, we provide training guidelines for scaling models to achieve\noptimal performance. Our results show that within the explored parameter space,\nclear scaling relationships emerge. These findings suggest that optimal\ncomputational resource allocation requires balanced scaling. Specifically,\ngiven a tenfold increase in training compute, achieving an optimal seven-fold\nreduction in mean squared error necessitates an approximately 2.5-fold increase\nin dataset size and a 3.8-fold increase in model size. This study establishes a\nfoundation for developing spectral foundational models with enhanced domain\ntransfer capabilities."
                },
                "authors": [
                    {
                        "name": "Tomasz Raski"
                    },
                    {
                        "name": "Yuan-Sen Ting"
                    }
                ],
                "author_detail": {
                    "name": "Yuan-Sen Ting"
                },
                "author": "Yuan-Sen Ting",
                "arxiv_comment": "25 pages, 11 figures, submitted to OJA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18617v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18617v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.14280v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.14280v5",
                "updated": "2025-03-24T12:14:43Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    12,
                    14,
                    43,
                    0,
                    83,
                    0
                ],
                "published": "2024-03-21T10:39:44Z",
                "published_parsed": [
                    2024,
                    3,
                    21,
                    10,
                    39,
                    44,
                    3,
                    81,
                    0
                ],
                "title": "Large Language Models for Blockchain Security: A Systematic Literature\n  Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models for Blockchain Security: A Systematic Literature\n  Review"
                },
                "summary": "Large Language Models (LLMs) have emerged as powerful tools across various\ndomains within cyber security. Notably, recent studies are increasingly\nexploring LLMs applied to the context of blockchain security (BS). However,\nthere remains a gap in a comprehensive understanding regarding the full scope\nof applications, impacts, and potential constraints of LLMs on blockchain\nsecurity. To fill this gap, we undertake a literature review focusing on the\nstudies that apply LLMs in blockchain security (LLM4BS).\n  Our study aims to comprehensively analyze and understand existing research,\nand elucidate how LLMs contribute to enhancing the security of blockchain\nsystems. Through a thorough examination of existing literature, we delve into\nthe integration of LLMs into various aspects of blockchain security. We explore\nthe mechanisms through which LLMs can bolster blockchain security, including\ntheir applications in smart contract auditing, transaction anomaly detection,\nvulnerability repair, program analysis of smart contracts, and serving as\nparticipants in the cryptocurrency community. Furthermore, we assess the\nchallenges and limitations associated with leveraging LLMs for enhancing\nblockchain security, considering factors such as scalability, privacy concerns,\nand ethical concerns. Our thorough review sheds light on the opportunities and\npotential risks of tasks on LLM4BS, providing valuable insights for\nresearchers, practitioners, and policymakers alike.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have emerged as powerful tools across various\ndomains within cyber security. Notably, recent studies are increasingly\nexploring LLMs applied to the context of blockchain security (BS). However,\nthere remains a gap in a comprehensive understanding regarding the full scope\nof applications, impacts, and potential constraints of LLMs on blockchain\nsecurity. To fill this gap, we undertake a literature review focusing on the\nstudies that apply LLMs in blockchain security (LLM4BS).\n  Our study aims to comprehensively analyze and understand existing research,\nand elucidate how LLMs contribute to enhancing the security of blockchain\nsystems. Through a thorough examination of existing literature, we delve into\nthe integration of LLMs into various aspects of blockchain security. We explore\nthe mechanisms through which LLMs can bolster blockchain security, including\ntheir applications in smart contract auditing, transaction anomaly detection,\nvulnerability repair, program analysis of smart contracts, and serving as\nparticipants in the cryptocurrency community. Furthermore, we assess the\nchallenges and limitations associated with leveraging LLMs for enhancing\nblockchain security, considering factors such as scalability, privacy concerns,\nand ethical concerns. Our thorough review sheds light on the opportunities and\npotential risks of tasks on LLM4BS, providing valuable insights for\nresearchers, practitioners, and policymakers alike."
                },
                "authors": [
                    {
                        "name": "Zheyuan He"
                    },
                    {
                        "name": "Zihao Li"
                    },
                    {
                        "name": "Sen Yang"
                    },
                    {
                        "name": "He Ye"
                    },
                    {
                        "name": "Ao Qiao"
                    },
                    {
                        "name": "Xiaosong Zhang"
                    },
                    {
                        "name": "Xiapu Luo"
                    },
                    {
                        "name": "Ting Chen"
                    }
                ],
                "author_detail": {
                    "name": "Ting Chen"
                },
                "author": "Ting Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.14280v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.14280v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.13002v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.13002v4",
                "updated": "2025-03-24T12:10:38Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    12,
                    10,
                    38,
                    0,
                    83,
                    0
                ],
                "published": "2024-03-13T02:53:36Z",
                "published_parsed": [
                    2024,
                    3,
                    13,
                    2,
                    53,
                    36,
                    2,
                    73,
                    0
                ],
                "title": "AutoTRIZ: Automating Engineering Innovation with TRIZ and Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoTRIZ: Automating Engineering Innovation with TRIZ and Large Language\n  Models"
                },
                "summary": "Various ideation methods, such as morphological analysis and\ndesign-by-analogy, have been developed to aid creative problem-solving and\ninnovation. Among them, the Theory of Inventive Problem Solving (TRIZ) stands\nout as one of the best-known methods. However, the complexity of TRIZ and its\nreliance on users' knowledge, experience, and reasoning capabilities limit its\npracticality. To address this, we introduce AutoTRIZ, an artificial ideation\nsystem that integrates Large Language Models (LLMs) to automate and enhance the\nTRIZ methodology. By leveraging LLMs' vast pre-trained knowledge and advanced\nreasoning capabilities, AutoTRIZ offers a novel, generative, and interpretable\napproach to engineering innovation. AutoTRIZ takes a problem statement from the\nuser as its initial input, automatically conduct the TRIZ reasoning process and\ngenerates a structured solution report. We demonstrate and evaluate the\neffectiveness of AutoTRIZ through comparative experiments with textbook cases\nand a real-world application in the design of a Battery Thermal Management\nSystem (BTMS). Moreover, the proposed LLM-based framework holds the potential\nfor extension to automate other knowledge-based ideation methods, such as\nSCAMPER, Design Heuristics, and Design-by-Analogy, paving the way for a new era\nof AI-driven innovation tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Various ideation methods, such as morphological analysis and\ndesign-by-analogy, have been developed to aid creative problem-solving and\ninnovation. Among them, the Theory of Inventive Problem Solving (TRIZ) stands\nout as one of the best-known methods. However, the complexity of TRIZ and its\nreliance on users' knowledge, experience, and reasoning capabilities limit its\npracticality. To address this, we introduce AutoTRIZ, an artificial ideation\nsystem that integrates Large Language Models (LLMs) to automate and enhance the\nTRIZ methodology. By leveraging LLMs' vast pre-trained knowledge and advanced\nreasoning capabilities, AutoTRIZ offers a novel, generative, and interpretable\napproach to engineering innovation. AutoTRIZ takes a problem statement from the\nuser as its initial input, automatically conduct the TRIZ reasoning process and\ngenerates a structured solution report. We demonstrate and evaluate the\neffectiveness of AutoTRIZ through comparative experiments with textbook cases\nand a real-world application in the design of a Battery Thermal Management\nSystem (BTMS). Moreover, the proposed LLM-based framework holds the potential\nfor extension to automate other knowledge-based ideation methods, such as\nSCAMPER, Design Heuristics, and Design-by-Analogy, paving the way for a new era\nof AI-driven innovation tools."
                },
                "authors": [
                    {
                        "name": "Shuo Jiang"
                    },
                    {
                        "name": "Weifeng Li"
                    },
                    {
                        "name": "Yuping Qian"
                    },
                    {
                        "name": "Yangjun Zhang"
                    },
                    {
                        "name": "Jianxi Luo"
                    }
                ],
                "author_detail": {
                    "name": "Jianxi Luo"
                },
                "author": "Jianxi Luo",
                "arxiv_comment": "28 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.13002v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.13002v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; I.2.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18608v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18608v1",
                "updated": "2025-03-24T12:05:45Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    12,
                    5,
                    45,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T12:05:45Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    12,
                    5,
                    45,
                    0,
                    83,
                    0
                ],
                "title": "AutoBayes: A Compositional Framework for Generalized Variational\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoBayes: A Compositional Framework for Generalized Variational\n  Inference"
                },
                "summary": "We introduce a new compositional framework for generalized variational\ninference, clarifying the different parts of a model, how they interact, and\nhow they compose. We explain that both exact Bayesian inference and the loss\nfunctions typical of variational inference (such as variational free energy and\nits generalizations) satisfy chain rules akin to that of reverse-mode automatic\ndifferentiation, and we advocate for exploiting this to build and optimize\nmodels accordingly. To this end, we construct a series of compositional tools:\nfor building models; for constructing their inversions; for attaching local\nloss functions; and for exposing parameters. Finally, we explain how the\nresulting parameterized statistical games may be optimized locally, too. We\nillustrate our framework with a number of classic examples, pointing to new\nareas of extensibility that are revealed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a new compositional framework for generalized variational\ninference, clarifying the different parts of a model, how they interact, and\nhow they compose. We explain that both exact Bayesian inference and the loss\nfunctions typical of variational inference (such as variational free energy and\nits generalizations) satisfy chain rules akin to that of reverse-mode automatic\ndifferentiation, and we advocate for exploiting this to build and optimize\nmodels accordingly. To this end, we construct a series of compositional tools:\nfor building models; for constructing their inversions; for attaching local\nloss functions; and for exposing parameters. Finally, we explain how the\nresulting parameterized statistical games may be optimized locally, too. We\nillustrate our framework with a number of classic examples, pointing to new\nareas of extensibility that are revealed."
                },
                "authors": [
                    {
                        "name": "Toby St Clere Smithe"
                    },
                    {
                        "name": "Marco Perin"
                    }
                ],
                "author_detail": {
                    "name": "Marco Perin"
                },
                "author": "Marco Perin",
                "arxiv_comment": "15 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18608v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18608v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18604v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18604v1",
                "updated": "2025-03-24T12:03:55Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    12,
                    3,
                    55,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T12:03:55Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    12,
                    3,
                    55,
                    0,
                    83,
                    0
                ],
                "title": "Redshift Distributions of Fast Radio Bursts Inferred Using Clustering in\n  Dispersion Measure Space",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Redshift Distributions of Fast Radio Bursts Inferred Using Clustering in\n  Dispersion Measure Space"
                },
                "summary": "Fast radio bursts (FRBs), millisecond-duration radio transient events,\npossess the potential to serve as excellent cosmological probes. The FRB\nredshift distribution contains information about the FRB sources, providing key\nconstraints on the types of engines. However, it is quite challenging to obtain\nthe FRB redshifts due to the poor localization and the faintness of the host\ngalaxies. This reality severely restricts the application prospects and study\nof the physical origins of FRBs. We propose that the clustering of observed\nFRBs can be an effective approach to address this issue without needing to\naccurately model dispersion measure (DM) contributions from the host galaxy and\nthe immediate environment of the source. Using the clustering of $5\\times 10^7$\nsimulated FRBs from future observations with sensitivity similar to the second\nphase of the Square Kilometre Array, we show that in extragalactic DM space,\nthe redshift distributions can be accurately reconstructed, and the mean\nredshift for FRBs between 384.8 and 1450.3 $\\rm pc\\,cm^{-3}$ can be constrained\nto $\\sim\\!0.001\\pm0.003 (1+z)$. The results demonstrate the potential of FRB\nclustering to constrain redshift distributions and provide valuable insights\ninto FRB source models and cosmological applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast radio bursts (FRBs), millisecond-duration radio transient events,\npossess the potential to serve as excellent cosmological probes. The FRB\nredshift distribution contains information about the FRB sources, providing key\nconstraints on the types of engines. However, it is quite challenging to obtain\nthe FRB redshifts due to the poor localization and the faintness of the host\ngalaxies. This reality severely restricts the application prospects and study\nof the physical origins of FRBs. We propose that the clustering of observed\nFRBs can be an effective approach to address this issue without needing to\naccurately model dispersion measure (DM) contributions from the host galaxy and\nthe immediate environment of the source. Using the clustering of $5\\times 10^7$\nsimulated FRBs from future observations with sensitivity similar to the second\nphase of the Square Kilometre Array, we show that in extragalactic DM space,\nthe redshift distributions can be accurately reconstructed, and the mean\nredshift for FRBs between 384.8 and 1450.3 $\\rm pc\\,cm^{-3}$ can be constrained\nto $\\sim\\!0.001\\pm0.003 (1+z)$. The results demonstrate the potential of FRB\nclustering to constrain redshift distributions and provide valuable insights\ninto FRB source models and cosmological applications."
                },
                "authors": [
                    {
                        "name": "Hui Peng"
                    },
                    {
                        "name": "Yu Yu"
                    }
                ],
                "author_detail": {
                    "name": "Yu Yu"
                },
                "author": "Yu Yu",
                "arxiv_doi": "10.3847/1538-4357/adbbda",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.3847/1538-4357/adbbda",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.18604v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18604v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "10 pages, 6 figures, accepted for publication in APJ",
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.16990v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.16990v6",
                "updated": "2025-03-24T11:57:19Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    11,
                    57,
                    19,
                    0,
                    83,
                    0
                ],
                "published": "2024-01-30T13:22:21Z",
                "published_parsed": [
                    2024,
                    1,
                    30,
                    13,
                    22,
                    21,
                    1,
                    30,
                    0
                ],
                "title": "Recovery and inference of causal effects with sequential adjustment for\n  confounding and attrition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recovery and inference of causal effects with sequential adjustment for\n  confounding and attrition"
                },
                "summary": "Confounding bias and selection bias bring two significant challenges to the\nvalidity of conclusions drawn from applied causal inference. The latter can\nstem from informative missingness, such as in cases of attrition. We introduce\nthe Sequential Adjustment Criteria (SAC), which extend available graphical\nconditions for recovering causal effects from confounding and attrition using\nsequential regressions, allowing for the inclusion of post-exposure and\nforbidden variables in the adjustment sets. We propose an estimator for the\nrecovered Average Treatment Effect (ATE) based on Targeted Minimum-Loss\nEstimation (TMLE), which exhibits multiple robustness under certain technical\nconditions. This approach ensures consistency even in scenarios where the\nDouble Inverse Probability Weighting (DIPW) and the naive plug-in sequential\nregressions approaches fall short. Through a simulation study, we assess the\nperformance of the proposed estimator against alternative methods across\ndifferent graph setups and model specification scenarios. As a motivating\napplication, we examine the effect of pharmacological treatment for\nAttention-Deficit/Hyperactivity Disorder (ADHD) upon the scores obtained by\ndiagnosed Norwegian schoolchildren in national tests using observational data\n($n=9\\,352$). Our findings align with the accumulated clinical evidence,\naffirming a positive but small impact of medication on academic achievement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Confounding bias and selection bias bring two significant challenges to the\nvalidity of conclusions drawn from applied causal inference. The latter can\nstem from informative missingness, such as in cases of attrition. We introduce\nthe Sequential Adjustment Criteria (SAC), which extend available graphical\nconditions for recovering causal effects from confounding and attrition using\nsequential regressions, allowing for the inclusion of post-exposure and\nforbidden variables in the adjustment sets. We propose an estimator for the\nrecovered Average Treatment Effect (ATE) based on Targeted Minimum-Loss\nEstimation (TMLE), which exhibits multiple robustness under certain technical\nconditions. This approach ensures consistency even in scenarios where the\nDouble Inverse Probability Weighting (DIPW) and the naive plug-in sequential\nregressions approaches fall short. Through a simulation study, we assess the\nperformance of the proposed estimator against alternative methods across\ndifferent graph setups and model specification scenarios. As a motivating\napplication, we examine the effect of pharmacological treatment for\nAttention-Deficit/Hyperactivity Disorder (ADHD) upon the scores obtained by\ndiagnosed Norwegian schoolchildren in national tests using observational data\n($n=9\\,352$). Our findings align with the accumulated clinical evidence,\naffirming a positive but small impact of medication on academic achievement."
                },
                "authors": [
                    {
                        "name": "Johan de Aguas"
                    },
                    {
                        "name": "Johan Pensar"
                    },
                    {
                        "name": "Toms Varnet Prez"
                    },
                    {
                        "name": "Guido Biele"
                    }
                ],
                "author_detail": {
                    "name": "Guido Biele"
                },
                "author": "Guido Biele",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.16990v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.16990v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62A09, 62D20",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18599v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18599v1",
                "updated": "2025-03-24T11:56:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    11,
                    56,
                    50,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T11:56:50Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    11,
                    56,
                    50,
                    0,
                    83,
                    0
                ],
                "title": "Oaken: Fast and Efficient LLM Serving with Online-Offline Hybrid KV\n  Cache Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Oaken: Fast and Efficient LLM Serving with Online-Offline Hybrid KV\n  Cache Quantization"
                },
                "summary": "Modern Large Language Model serving system batches multiple requests to\nachieve high throughput, while batching attention operations is challenging,\nrendering memory bandwidth a critical bottleneck. The community relies on\nhigh-end GPUs with multiple high-bandwidth memory channels. Unfortunately,\nHBM's high bandwidth often comes at the expense of limited memory capacity,\nwhich reduces core utilization and increases costs. Recent advancements\nenabling longer contexts for LLMs have substantially increased the key-value\ncache size, further intensifying the pressures on memory capacity. The\nliterature has explored KV cache quantization techniques, which commonly use\nlow bitwidth for most values, selectively using higher bitwidth for outlier\nvalues. While this approach helps achieve high accuracy and low bitwidth\nsimultaneously, it comes with the limitation that cost for online outlier\ndetection is excessively high, negating the advantages. We propose Oaken, an\nacceleration solution that achieves high accuracy and high performance\nsimultaneously through co-designing algorithm and hardware. To effectively find\na sweet spot in the accuracy-performance trade-off space of KV cache\nquantization, Oaken employs an online-offline hybrid approach, setting outlier\nthresholds offline, which are then used to determine the quantization scale\nonline. To translate the proposed algorithmic technique into tangible\nperformance gains, Oaken also comes with custom quantization engines and memory\nmanagement units that can be integrated with any LLM accelerators. We built an\nOaken accelerator on top of an LLM accelerator, LPU, and conducted a\ncomprehensive evaluation. Our experiments show that for a batch size of 256,\nOaken achieves up to 1.58x throughput improvement over NVIDIA A100 GPU,\nincurring a minimal accuracy loss of only 0.54\\% on average, compared to\nstate-of-the-art KV cache quantization techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern Large Language Model serving system batches multiple requests to\nachieve high throughput, while batching attention operations is challenging,\nrendering memory bandwidth a critical bottleneck. The community relies on\nhigh-end GPUs with multiple high-bandwidth memory channels. Unfortunately,\nHBM's high bandwidth often comes at the expense of limited memory capacity,\nwhich reduces core utilization and increases costs. Recent advancements\nenabling longer contexts for LLMs have substantially increased the key-value\ncache size, further intensifying the pressures on memory capacity. The\nliterature has explored KV cache quantization techniques, which commonly use\nlow bitwidth for most values, selectively using higher bitwidth for outlier\nvalues. While this approach helps achieve high accuracy and low bitwidth\nsimultaneously, it comes with the limitation that cost for online outlier\ndetection is excessively high, negating the advantages. We propose Oaken, an\nacceleration solution that achieves high accuracy and high performance\nsimultaneously through co-designing algorithm and hardware. To effectively find\na sweet spot in the accuracy-performance trade-off space of KV cache\nquantization, Oaken employs an online-offline hybrid approach, setting outlier\nthresholds offline, which are then used to determine the quantization scale\nonline. To translate the proposed algorithmic technique into tangible\nperformance gains, Oaken also comes with custom quantization engines and memory\nmanagement units that can be integrated with any LLM accelerators. We built an\nOaken accelerator on top of an LLM accelerator, LPU, and conducted a\ncomprehensive evaluation. Our experiments show that for a batch size of 256,\nOaken achieves up to 1.58x throughput improvement over NVIDIA A100 GPU,\nincurring a minimal accuracy loss of only 0.54\\% on average, compared to\nstate-of-the-art KV cache quantization techniques."
                },
                "authors": [
                    {
                        "name": "Minsu Kim"
                    },
                    {
                        "name": "Seongmin Hong"
                    },
                    {
                        "name": "RyeoWook Ko"
                    },
                    {
                        "name": "Soongyu Choi"
                    },
                    {
                        "name": "Hunjong Lee"
                    },
                    {
                        "name": "Junsoo Kim"
                    },
                    {
                        "name": "Joo-Young Kim"
                    },
                    {
                        "name": "Jongse Park"
                    }
                ],
                "author_detail": {
                    "name": "Jongse Park"
                },
                "author": "Jongse Park",
                "arxiv_comment": "15 pages, 14 figures, and 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18599v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18599v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18597v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18597v1",
                "updated": "2025-03-24T11:55:35Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    11,
                    55,
                    35,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T11:55:35Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    11,
                    55,
                    35,
                    0,
                    83,
                    0
                ],
                "title": "Regression Testing with a Natural Language Oracle",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Regression Testing with a Natural Language Oracle"
                },
                "summary": "As software is evolving, code changes can introduce regression bugs or affect\nthe behavior in other unintended ways. Traditional regression test generation\nis impractical for detecting unintended behavioral changes, because it reports\nall behavioral differences as potential regressions. However, most code changes\nare intended to change the behavior in some way, e.g., to fix a bug or to add a\nnew feature. This paper presents Testora, an automated approach that detects\nregressions by comparing the intentions of a code change against behavioral\ndifferences caused by the code change. Given a pull request (PR), Testora\nqueries an LLM to generate tests that exercise the modified code, compares the\nbehavior of the original and modified code, and classifies any behavioral\ndifferences as intended or unintended. For the classification, we present an\nLLM-based technique that leverages the natural language information associated\nwith the PR, such as the title, description, and commit messages -- effectively\nproviding a natural language oracle for regression testing. Applying Testora to\nPRs of complex and popular Python projects, we find 19 regression bugs and 11\nPRs that, despite having another intention, coincidentally fix a bug. Out of 13\nregressions reported to the developers, 10 have been confirmed and 8 have\nalready been fixed. The costs of using Testora are acceptable for real-world\ndeployment, with 12.3 minutes to check a PR and LLM costs of only $0.003 per\nPR. We envision our approach to be used before or shortly after a code change\ngets merged into a code base, providing a way to early on detect regressions\nthat are not caught by traditional approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As software is evolving, code changes can introduce regression bugs or affect\nthe behavior in other unintended ways. Traditional regression test generation\nis impractical for detecting unintended behavioral changes, because it reports\nall behavioral differences as potential regressions. However, most code changes\nare intended to change the behavior in some way, e.g., to fix a bug or to add a\nnew feature. This paper presents Testora, an automated approach that detects\nregressions by comparing the intentions of a code change against behavioral\ndifferences caused by the code change. Given a pull request (PR), Testora\nqueries an LLM to generate tests that exercise the modified code, compares the\nbehavior of the original and modified code, and classifies any behavioral\ndifferences as intended or unintended. For the classification, we present an\nLLM-based technique that leverages the natural language information associated\nwith the PR, such as the title, description, and commit messages -- effectively\nproviding a natural language oracle for regression testing. Applying Testora to\nPRs of complex and popular Python projects, we find 19 regression bugs and 11\nPRs that, despite having another intention, coincidentally fix a bug. Out of 13\nregressions reported to the developers, 10 have been confirmed and 8 have\nalready been fixed. The costs of using Testora are acceptable for real-world\ndeployment, with 12.3 minutes to check a PR and LLM costs of only $0.003 per\nPR. We envision our approach to be used before or shortly after a code change\ngets merged into a code base, providing a way to early on detect regressions\nthat are not caught by traditional approaches."
                },
                "authors": [
                    {
                        "name": "Michael Pradel"
                    }
                ],
                "author_detail": {
                    "name": "Michael Pradel"
                },
                "author": "Michael Pradel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18597v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18597v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18596v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18596v1",
                "updated": "2025-03-24T11:53:06Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    11,
                    53,
                    6,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T11:53:06Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    11,
                    53,
                    6,
                    0,
                    83,
                    0
                ],
                "title": "LinkAlign: Scalable Schema Linking for Real-World Large-Scale\n  Multi-Database Text-to-SQL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LinkAlign: Scalable Schema Linking for Real-World Large-Scale\n  Multi-Database Text-to-SQL"
                },
                "summary": "Schema linking is a critical bottleneck in achieving human-level performance\nin Text-to-SQL tasks, particularly in real-world large-scale multi-database\nscenarios. Addressing schema linking faces two major challenges: (1) Database\nRetrieval: selecting the correct database from a large schema pool in\nmulti-database settings, while filtering out irrelevant ones. (2) Schema Item\nGrounding: accurately identifying the relevant tables and columns from within a\nlarge and redundant schema for SQL generation. To address this, we introduce\nLinkAlign, a novel framework that can effectively adapt existing baselines to\nreal-world environments by systematically addressing schema linking. Our\nframework comprises three key steps: multi-round semantic enhanced retrieval\nand irrelevant information isolation for Challenge 1, and schema extraction\nenhancement for Challenge 2. We evaluate our method performance of schema\nlinking on the SPIDER and BIRD benchmarks, and the ability to adapt existing\nText-to-SQL models to real-world environments on the SPIDER 2.0-lite benchmark.\nExperiments show that LinkAlign outperforms existing baselines in\nmulti-database settings, demonstrating its effectiveness and robustness. On the\nother hand, our method ranks highest among models excluding those using long\nchain-of-thought reasoning LLMs. This work bridges the gap between current\nresearch and real-world scenarios, providing a practical solution for robust\nand scalable schema linking. The codes are available at\nhttps://github.com/Satissss/LinkAlign.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Schema linking is a critical bottleneck in achieving human-level performance\nin Text-to-SQL tasks, particularly in real-world large-scale multi-database\nscenarios. Addressing schema linking faces two major challenges: (1) Database\nRetrieval: selecting the correct database from a large schema pool in\nmulti-database settings, while filtering out irrelevant ones. (2) Schema Item\nGrounding: accurately identifying the relevant tables and columns from within a\nlarge and redundant schema for SQL generation. To address this, we introduce\nLinkAlign, a novel framework that can effectively adapt existing baselines to\nreal-world environments by systematically addressing schema linking. Our\nframework comprises three key steps: multi-round semantic enhanced retrieval\nand irrelevant information isolation for Challenge 1, and schema extraction\nenhancement for Challenge 2. We evaluate our method performance of schema\nlinking on the SPIDER and BIRD benchmarks, and the ability to adapt existing\nText-to-SQL models to real-world environments on the SPIDER 2.0-lite benchmark.\nExperiments show that LinkAlign outperforms existing baselines in\nmulti-database settings, demonstrating its effectiveness and robustness. On the\nother hand, our method ranks highest among models excluding those using long\nchain-of-thought reasoning LLMs. This work bridges the gap between current\nresearch and real-world scenarios, providing a practical solution for robust\nand scalable schema linking. The codes are available at\nhttps://github.com/Satissss/LinkAlign."
                },
                "authors": [
                    {
                        "name": "Yihan Wang"
                    },
                    {
                        "name": "Peiyu Liu"
                    },
                    {
                        "name": "Xin Yang"
                    }
                ],
                "author_detail": {
                    "name": "Xin Yang"
                },
                "author": "Xin Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18596v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18596v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18590v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18590v1",
                "updated": "2025-03-24T11:47:32Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    11,
                    47,
                    32,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T11:47:32Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    11,
                    47,
                    32,
                    0,
                    83,
                    0
                ],
                "title": "Target Speaker Selection for Neural Network Beamforming in Multi-Speaker\n  Scenarios",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Target Speaker Selection for Neural Network Beamforming in Multi-Speaker\n  Scenarios"
                },
                "summary": "We propose a speaker selection mechanism (SSM) for the training of an\nend-to-end beamforming neural network, based on recent findings that a listener\nusually looks to the target speaker with a certain undershot angle. The\nmechanism allows the neural network model to learn toward which speaker to\nfocus, during training, in a multi-speaker scenario, based on the position of\nlistener and speakers. However, only audio information is necessary during\ninference. We perform acoustic simulations demonstrating the feasibility and\nperformance when the SSM is employed in training. The results show significant\nincrease in speech intelligibility, quality, and distortion metrics when\ncompared to the minimum variance distortionless filter and the same neural\nnetwork model trained without SSM. The success of the proposed method is a\nsignificant step forward toward the solution of the cocktail party problem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a speaker selection mechanism (SSM) for the training of an\nend-to-end beamforming neural network, based on recent findings that a listener\nusually looks to the target speaker with a certain undershot angle. The\nmechanism allows the neural network model to learn toward which speaker to\nfocus, during training, in a multi-speaker scenario, based on the position of\nlistener and speakers. However, only audio information is necessary during\ninference. We perform acoustic simulations demonstrating the feasibility and\nperformance when the SSM is employed in training. The results show significant\nincrease in speech intelligibility, quality, and distortion metrics when\ncompared to the minimum variance distortionless filter and the same neural\nnetwork model trained without SSM. The success of the proposed method is a\nsignificant step forward toward the solution of the cocktail party problem."
                },
                "authors": [
                    {
                        "name": "Luan Vincius Fiorio"
                    },
                    {
                        "name": "Bruno Defraene"
                    },
                    {
                        "name": "Johan David"
                    },
                    {
                        "name": "Alex Young"
                    },
                    {
                        "name": "Frans Widdershoven"
                    },
                    {
                        "name": "Wim van Houtum"
                    },
                    {
                        "name": "Ronald M. Aarts"
                    }
                ],
                "author_detail": {
                    "name": "Ronald M. Aarts"
                },
                "author": "Ronald M. Aarts",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18590v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18590v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18589v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18589v1",
                "updated": "2025-03-24T11:46:58Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    11,
                    46,
                    58,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T11:46:58Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    11,
                    46,
                    58,
                    0,
                    83,
                    0
                ],
                "title": "Unified Uncertainty-Aware Diffusion for Multi-Agent Trajectory Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unified Uncertainty-Aware Diffusion for Multi-Agent Trajectory Modeling"
                },
                "summary": "Multi-agent trajectory modeling has primarily focused on forecasting future\nstates, often overlooking broader tasks like trajectory completion, which are\ncrucial for real-world applications such as correcting tracking data. Existing\nmethods also generally predict agents' states without offering any state-wise\nmeasure of uncertainty. Moreover, popular multi-modal sampling methods lack any\nerror probability estimates for each generated scene under the same prior\nobservations, making it difficult to rank the predictions during inference\ntime. We introduce U2Diff, a \\textbf{unified} diffusion model designed to\nhandle trajectory completion while providing state-wise \\textbf{uncertainty}\nestimates jointly. This uncertainty estimation is achieved by augmenting the\nsimple denoising loss with the negative log-likelihood of the predicted noise\nand propagating latent space uncertainty to the real state space. Additionally,\nwe incorporate a Rank Neural Network in post-processing to enable \\textbf{error\nprobability} estimation for each generated mode, demonstrating a strong\ncorrelation with the error relative to ground truth. Our method outperforms the\nstate-of-the-art solutions in trajectory completion and forecasting across four\nchallenging sports datasets (NBA, Basketball-U, Football-U, Soccer-U),\nhighlighting the effectiveness of uncertainty and error probability estimation.\nVideo at https://youtu.be/ngw4D4eJToE",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-agent trajectory modeling has primarily focused on forecasting future\nstates, often overlooking broader tasks like trajectory completion, which are\ncrucial for real-world applications such as correcting tracking data. Existing\nmethods also generally predict agents' states without offering any state-wise\nmeasure of uncertainty. Moreover, popular multi-modal sampling methods lack any\nerror probability estimates for each generated scene under the same prior\nobservations, making it difficult to rank the predictions during inference\ntime. We introduce U2Diff, a \\textbf{unified} diffusion model designed to\nhandle trajectory completion while providing state-wise \\textbf{uncertainty}\nestimates jointly. This uncertainty estimation is achieved by augmenting the\nsimple denoising loss with the negative log-likelihood of the predicted noise\nand propagating latent space uncertainty to the real state space. Additionally,\nwe incorporate a Rank Neural Network in post-processing to enable \\textbf{error\nprobability} estimation for each generated mode, demonstrating a strong\ncorrelation with the error relative to ground truth. Our method outperforms the\nstate-of-the-art solutions in trajectory completion and forecasting across four\nchallenging sports datasets (NBA, Basketball-U, Football-U, Soccer-U),\nhighlighting the effectiveness of uncertainty and error probability estimation.\nVideo at https://youtu.be/ngw4D4eJToE"
                },
                "authors": [
                    {
                        "name": "Guillem Capellera"
                    },
                    {
                        "name": "Antonio Rubio"
                    },
                    {
                        "name": "Luis Ferraz"
                    },
                    {
                        "name": "Antonio Agudo"
                    }
                ],
                "author_detail": {
                    "name": "Antonio Agudo"
                },
                "author": "Antonio Agudo",
                "arxiv_comment": "Accepted to CVPR 2025 conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18589v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18589v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.10819v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.10819v2",
                "updated": "2025-03-24T11:46:14Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    11,
                    46,
                    14,
                    0,
                    83,
                    0
                ],
                "published": "2024-06-16T06:56:53Z",
                "published_parsed": [
                    2024,
                    6,
                    16,
                    6,
                    56,
                    53,
                    6,
                    168,
                    0
                ],
                "title": "GUI-World: A Video Benchmark and Dataset for Multimodal GUI-oriented\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GUI-World: A Video Benchmark and Dataset for Multimodal GUI-oriented\n  Understanding"
                },
                "summary": "Recently, Multimodal Large Language Models (MLLMs) have been used as agents\nto control keyboard and mouse inputs by directly perceiving the Graphical User\nInterface (GUI) and generating corresponding commands. However, current agents\nprimarily demonstrate strong understanding capabilities in static environments\nand are mainly applied to relatively simple domains, such as Web or mobile\ninterfaces. We argue that a robust GUI agent should be capable of perceiving\ntemporal information on the GUI, including dynamic Web content and multi-step\ntasks. Additionally, it should possess a comprehensive understanding of various\nGUI scenarios, including desktop software and multi-window interactions. To\nthis end, this paper introduces a new dataset, termed GUI-World, which features\nmeticulously crafted Human-MLLM annotations, extensively covering six GUI\nscenarios and eight types of GUI-oriented questions in three formats. We\nevaluate the capabilities of current state-of-the-art MLLMs, including Image\nLLMs and Video LLMs, in understanding various types of GUI content, especially\ndynamic and sequential content. Our findings reveal that current models\nstruggle with dynamic GUI content without manually annotated keyframes or\noperation history. On the other hand, Video LLMs fall short in all GUI-oriented\ntasks given the sparse GUI video dataset. Therefore, we take the initial step\nof leveraging a fine-tuned Video LLM, GUI-Vid, as a GUI-oriented assistant,\ndemonstrating an improved understanding of various GUI tasks. However, due to\nthe limitations in the performance of base LLMs, we conclude that using video\nLLMs as GUI agents remains a significant challenge. We believe our work\nprovides valuable insights for future research in dynamic GUI content\nunderstanding. All the dataset and code are publicly available at:\nhttps://gui-world.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, Multimodal Large Language Models (MLLMs) have been used as agents\nto control keyboard and mouse inputs by directly perceiving the Graphical User\nInterface (GUI) and generating corresponding commands. However, current agents\nprimarily demonstrate strong understanding capabilities in static environments\nand are mainly applied to relatively simple domains, such as Web or mobile\ninterfaces. We argue that a robust GUI agent should be capable of perceiving\ntemporal information on the GUI, including dynamic Web content and multi-step\ntasks. Additionally, it should possess a comprehensive understanding of various\nGUI scenarios, including desktop software and multi-window interactions. To\nthis end, this paper introduces a new dataset, termed GUI-World, which features\nmeticulously crafted Human-MLLM annotations, extensively covering six GUI\nscenarios and eight types of GUI-oriented questions in three formats. We\nevaluate the capabilities of current state-of-the-art MLLMs, including Image\nLLMs and Video LLMs, in understanding various types of GUI content, especially\ndynamic and sequential content. Our findings reveal that current models\nstruggle with dynamic GUI content without manually annotated keyframes or\noperation history. On the other hand, Video LLMs fall short in all GUI-oriented\ntasks given the sparse GUI video dataset. Therefore, we take the initial step\nof leveraging a fine-tuned Video LLM, GUI-Vid, as a GUI-oriented assistant,\ndemonstrating an improved understanding of various GUI tasks. However, due to\nthe limitations in the performance of base LLMs, we conclude that using video\nLLMs as GUI agents remains a significant challenge. We believe our work\nprovides valuable insights for future research in dynamic GUI content\nunderstanding. All the dataset and code are publicly available at:\nhttps://gui-world.github.io."
                },
                "authors": [
                    {
                        "name": "Dongping Chen"
                    },
                    {
                        "name": "Yue Huang"
                    },
                    {
                        "name": "Siyuan Wu"
                    },
                    {
                        "name": "Jingyu Tang"
                    },
                    {
                        "name": "Liuyi Chen"
                    },
                    {
                        "name": "Yilin Bai"
                    },
                    {
                        "name": "Zhigang He"
                    },
                    {
                        "name": "Chenlong Wang"
                    },
                    {
                        "name": "Huichi Zhou"
                    },
                    {
                        "name": "Yiqiang Li"
                    },
                    {
                        "name": "Tianshuo Zhou"
                    },
                    {
                        "name": "Yue Yu"
                    },
                    {
                        "name": "Chujie Gao"
                    },
                    {
                        "name": "Qihui Zhang"
                    },
                    {
                        "name": "Yi Gui"
                    },
                    {
                        "name": "Zhen Li"
                    },
                    {
                        "name": "Yao Wan"
                    },
                    {
                        "name": "Pan Zhou"
                    },
                    {
                        "name": "Jianfeng Gao"
                    },
                    {
                        "name": "Lichao Sun"
                    }
                ],
                "author_detail": {
                    "name": "Lichao Sun"
                },
                "author": "Lichao Sun",
                "arxiv_comment": "Accepted by ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.10819v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.10819v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15637v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15637v3",
                "updated": "2025-03-24T11:46:10Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    11,
                    46,
                    10,
                    0,
                    83,
                    0
                ],
                "published": "2024-11-23T19:27:49Z",
                "published_parsed": [
                    2024,
                    11,
                    23,
                    19,
                    27,
                    49,
                    5,
                    328,
                    0
                ],
                "title": "GraphGrad: Efficient Estimation of Sparse Polynomial Representations for\n  General State-Space Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GraphGrad: Efficient Estimation of Sparse Polynomial Representations for\n  General State-Space Models"
                },
                "summary": "State-space models (SSMs) are a powerful statistical tool for modelling\ntime-varying systems via a latent state. In these models, the latent state is\nnever directly observed. Instead, a sequence of observations related to the\nstate is available. The state-space model is defined by the state dynamics and\nthe observation model, both of which are described by parametric distributions.\nEstimation of parameters of these distributions is a very challenging, but\nessential, task for performing inference and prediction. Furthermore, it is\ntypical that not all states of the system interact. We can therefore encode the\ninteraction of the states via a graph, usually not fully connected. However,\nmost parameter estimation methods do not take advantage of this feature. In\nthis work, we propose GraphGrad, a fully automatic approach for obtaining\nsparse estimates of the state interactions of a non-linear state-space model\nvia a polynomial approximation. This novel methodology unveils the latent\nstructure of the data-generating process, allowing us to infer both the\nstructure and value of a rich and efficient parameterisation of a general\nstate-space model. Our method utilises a differentiable particle filter to\noptimise a Monte Carlo likelihood estimator. It also promotes sparsity in the\nestimated system through the use of suitable proximity updates, known to be\nmore efficient and stable than subgradient methods. As shown in our paper, a\nnumber of well-known dynamical systems can be accurately represented and\nrecovered by our method, providing basis for application to real-world\nscenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "State-space models (SSMs) are a powerful statistical tool for modelling\ntime-varying systems via a latent state. In these models, the latent state is\nnever directly observed. Instead, a sequence of observations related to the\nstate is available. The state-space model is defined by the state dynamics and\nthe observation model, both of which are described by parametric distributions.\nEstimation of parameters of these distributions is a very challenging, but\nessential, task for performing inference and prediction. Furthermore, it is\ntypical that not all states of the system interact. We can therefore encode the\ninteraction of the states via a graph, usually not fully connected. However,\nmost parameter estimation methods do not take advantage of this feature. In\nthis work, we propose GraphGrad, a fully automatic approach for obtaining\nsparse estimates of the state interactions of a non-linear state-space model\nvia a polynomial approximation. This novel methodology unveils the latent\nstructure of the data-generating process, allowing us to infer both the\nstructure and value of a rich and efficient parameterisation of a general\nstate-space model. Our method utilises a differentiable particle filter to\noptimise a Monte Carlo likelihood estimator. It also promotes sparsity in the\nestimated system through the use of suitable proximity updates, known to be\nmore efficient and stable than subgradient methods. As shown in our paper, a\nnumber of well-known dynamical systems can be accurately represented and\nrecovered by our method, providing basis for application to real-world\nscenarios."
                },
                "authors": [
                    {
                        "name": "Benjamin Cox"
                    },
                    {
                        "name": "Emilie Chouzenoux"
                    },
                    {
                        "name": "Victor Elvira"
                    }
                ],
                "author_detail": {
                    "name": "Victor Elvira"
                },
                "author": "Victor Elvira",
                "arxiv_comment": "update to accepted version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15637v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15637v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.03387v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.03387v3",
                "updated": "2025-03-24T11:44:59Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    11,
                    44,
                    59,
                    0,
                    83,
                    0
                ],
                "published": "2024-07-03T08:36:13Z",
                "published_parsed": [
                    2024,
                    7,
                    3,
                    8,
                    36,
                    13,
                    2,
                    185,
                    0
                ],
                "title": "ConCodeEval: Evaluating Large Language Models for Code Constraints in\n  Domain-Specific Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ConCodeEval: Evaluating Large Language Models for Code Constraints in\n  Domain-Specific Languages"
                },
                "summary": "Recent work shows Large Language Models (LLMs) struggle to understand natural\nlanguage constraints for various text generation tasks in zero- and few-shot\nsettings. While, in the code domain, there is wide usage of constraints in code\nformat to maintain the integrity of code written in Domain-Specific Languages\n(DSLs) like JSON and YAML which are widely used for system-level programming\ntasks in enterprises. Given that LLMs are increasingly used for system-level\ncode tasks, evaluating if they can comprehend these code constraints is\ncrucial. However, no work has been done to evaluate their controllability over\ncode constraints. Hence, we introduce ConCodeEval, a first-of-its-kind\nbenchmark having two novel tasks for code constraints across five\nrepresentations. Our findings suggest that language models struggle with code\nconstraints. Code languages that perform excellently for normal code tasks do\nnot perform well when the same languages represent fine-grained constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent work shows Large Language Models (LLMs) struggle to understand natural\nlanguage constraints for various text generation tasks in zero- and few-shot\nsettings. While, in the code domain, there is wide usage of constraints in code\nformat to maintain the integrity of code written in Domain-Specific Languages\n(DSLs) like JSON and YAML which are widely used for system-level programming\ntasks in enterprises. Given that LLMs are increasingly used for system-level\ncode tasks, evaluating if they can comprehend these code constraints is\ncrucial. However, no work has been done to evaluate their controllability over\ncode constraints. Hence, we introduce ConCodeEval, a first-of-its-kind\nbenchmark having two novel tasks for code constraints across five\nrepresentations. Our findings suggest that language models struggle with code\nconstraints. Code languages that perform excellently for normal code tasks do\nnot perform well when the same languages represent fine-grained constraints."
                },
                "authors": [
                    {
                        "name": "Mehant Kammakomati"
                    },
                    {
                        "name": "Sameer Pimparkhede"
                    },
                    {
                        "name": "Srikanth Tamilselvam"
                    },
                    {
                        "name": "Prince Kumar"
                    },
                    {
                        "name": "Pushpak Bhattacharyya"
                    }
                ],
                "author_detail": {
                    "name": "Pushpak Bhattacharyya"
                },
                "author": "Pushpak Bhattacharyya",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.03387v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.03387v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11702v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11702v3",
                "updated": "2025-03-24T11:42:16Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    11,
                    42,
                    16,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-12T09:32:43Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    9,
                    32,
                    43,
                    2,
                    71,
                    0
                ],
                "title": "Toward a method for LLM-enabled Indoor Navigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toward a method for LLM-enabled Indoor Navigation"
                },
                "summary": "Indoor navigation presents unique challenges due to complex layouts, lack of\nGPS signals, and accessibility concerns. Existing solutions often struggle with\nreal-time adaptability and user-specific needs. In this work, we explore the\npotential of a Large Language Model (LLM), i.e., ChatGPT, to generate natural,\ncontext-aware navigation instructions from indoor map images. We design and\nevaluate test cases across different real-world environments, analyzing the\neffectiveness of LLMs in interpreting spatial layouts, handling user\nconstraints, and planning efficient routes. Our findings demonstrate the\npotential of LLMs for supporting personalized indoor navigation, with an\naverage of 50.54% correct indications and a maximum of 77.78%. The results do\nnot appear to depend on the complexity of the layout or the complexity of the\nexpected path, but rather on the number of points of interest and the abundance\nof visual information, which negatively affect the performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Indoor navigation presents unique challenges due to complex layouts, lack of\nGPS signals, and accessibility concerns. Existing solutions often struggle with\nreal-time adaptability and user-specific needs. In this work, we explore the\npotential of a Large Language Model (LLM), i.e., ChatGPT, to generate natural,\ncontext-aware navigation instructions from indoor map images. We design and\nevaluate test cases across different real-world environments, analyzing the\neffectiveness of LLMs in interpreting spatial layouts, handling user\nconstraints, and planning efficient routes. Our findings demonstrate the\npotential of LLMs for supporting personalized indoor navigation, with an\naverage of 50.54% correct indications and a maximum of 77.78%. The results do\nnot appear to depend on the complexity of the layout or the complexity of the\nexpected path, but rather on the number of points of interest and the abundance\nof visual information, which negatively affect the performance."
                },
                "authors": [
                    {
                        "name": "Alberto Coffrini"
                    },
                    {
                        "name": "Mohammad Amin Zadenoori"
                    },
                    {
                        "name": "Paolo Barsocchi"
                    },
                    {
                        "name": "Francesco Furfari"
                    },
                    {
                        "name": "Antonino Crivello"
                    },
                    {
                        "name": "Alessio Ferrari"
                    }
                ],
                "author_detail": {
                    "name": "Alessio Ferrari"
                },
                "author": "Alessio Ferrari",
                "arxiv_comment": "7 pages, 3 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11702v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11702v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18584v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18584v1",
                "updated": "2025-03-24T11:41:47Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    11,
                    41,
                    47,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T11:41:47Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    11,
                    41,
                    47,
                    0,
                    83,
                    0
                ],
                "title": "A Universal Model Combining Differential Equations and Neural Networks\n  for Ball Trajectory Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Universal Model Combining Differential Equations and Neural Networks\n  for Ball Trajectory Prediction"
                },
                "summary": "This paper presents a data driven universal ball trajectory prediction method\nintegrated with physics equations. Existing methods are designed for specific\nball types and struggle to generalize. This challenge arises from three key\nfactors. First, learning-based models require large datasets but suffer from\naccuracy drops in unseen scenarios. Second, physics-based models rely on\ncomplex formulas and detailed inputs, yet accurately obtaining ball states,\nsuch as spin, is often impractical. Third, integrating physical principles with\nneural networks to achieve high accuracy, fast inference, and strong\ngeneralization remains difficult. To address these issues, we propose an\ninnovative approach that incorporates physics-based equations and neural\nnetworks. We first derive three generalized physical formulas. Then, using a\nneural network and observed trajectory points, we infer certain parameters\nwhile fitting the remaining ones. These formulas enable precise trajectory\nprediction with minimal training data: only a few dozen samples. Extensive\nexperiments demonstrate our method superiority in generalization, real-time\nperformance, and accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a data driven universal ball trajectory prediction method\nintegrated with physics equations. Existing methods are designed for specific\nball types and struggle to generalize. This challenge arises from three key\nfactors. First, learning-based models require large datasets but suffer from\naccuracy drops in unseen scenarios. Second, physics-based models rely on\ncomplex formulas and detailed inputs, yet accurately obtaining ball states,\nsuch as spin, is often impractical. Third, integrating physical principles with\nneural networks to achieve high accuracy, fast inference, and strong\ngeneralization remains difficult. To address these issues, we propose an\ninnovative approach that incorporates physics-based equations and neural\nnetworks. We first derive three generalized physical formulas. Then, using a\nneural network and observed trajectory points, we infer certain parameters\nwhile fitting the remaining ones. These formulas enable precise trajectory\nprediction with minimal training data: only a few dozen samples. Extensive\nexperiments demonstrate our method superiority in generalization, real-time\nperformance, and accuracy."
                },
                "authors": [
                    {
                        "name": "Zhiwei Shi"
                    },
                    {
                        "name": "Chengxi Zhu"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Jun Yan"
                    },
                    {
                        "name": "Zheyun Qin"
                    },
                    {
                        "name": "Songquan Shi"
                    },
                    {
                        "name": "Zhumin Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhumin Chen"
                },
                "author": "Zhumin Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18584v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18584v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18579v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18579v1",
                "updated": "2025-03-24T11:36:13Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    11,
                    36,
                    13,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T11:36:13Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    11,
                    36,
                    13,
                    0,
                    83,
                    0
                ],
                "title": "Unsupervised Variational Acoustic Clustering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unsupervised Variational Acoustic Clustering"
                },
                "summary": "We propose an unsupervised variational acoustic clustering model for\nclustering audio data in the time-frequency domain. The model leverages\nvariational inference, extended to an autoencoder framework, with a Gaussian\nmixture model as a prior for the latent space. Specifically designed for audio\napplications, we introduce a convolutional-recurrent variational autoencoder\noptimized for efficient time-frequency processing. Our experimental results\nconsidering a spoken digits dataset demonstrate a significant improvement in\naccuracy and clustering performance compared to traditional methods, showcasing\nthe model's enhanced ability to capture complex audio patterns.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose an unsupervised variational acoustic clustering model for\nclustering audio data in the time-frequency domain. The model leverages\nvariational inference, extended to an autoencoder framework, with a Gaussian\nmixture model as a prior for the latent space. Specifically designed for audio\napplications, we introduce a convolutional-recurrent variational autoencoder\noptimized for efficient time-frequency processing. Our experimental results\nconsidering a spoken digits dataset demonstrate a significant improvement in\naccuracy and clustering performance compared to traditional methods, showcasing\nthe model's enhanced ability to capture complex audio patterns."
                },
                "authors": [
                    {
                        "name": "Luan Vincius Fiorio"
                    },
                    {
                        "name": "Bruno Defraene"
                    },
                    {
                        "name": "Johan David"
                    },
                    {
                        "name": "Frans Widdershoven"
                    },
                    {
                        "name": "Wim van Houtum"
                    },
                    {
                        "name": "Ronald M. Aarts"
                    }
                ],
                "author_detail": {
                    "name": "Ronald M. Aarts"
                },
                "author": "Ronald M. Aarts",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18579v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18579v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12138v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12138v4",
                "updated": "2025-03-24T11:30:32Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    11,
                    30,
                    32,
                    0,
                    83,
                    0
                ],
                "published": "2025-02-17T18:54:05Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    18,
                    54,
                    5,
                    0,
                    48,
                    0
                ],
                "title": "FLARE: Feed-forward Geometry, Appearance and Camera Estimation from\n  Uncalibrated Sparse Views",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FLARE: Feed-forward Geometry, Appearance and Camera Estimation from\n  Uncalibrated Sparse Views"
                },
                "summary": "We present FLARE, a feed-forward model designed to infer high-quality camera\nposes and 3D geometry from uncalibrated sparse-view images (i.e., as few as 2-8\ninputs), which is a challenging yet practical setting in real-world\napplications. Our solution features a cascaded learning paradigm with camera\npose serving as the critical bridge, recognizing its essential role in mapping\n3D structures onto 2D image planes. Concretely, FLARE starts with camera pose\nestimation, whose results condition the subsequent learning of geometric\nstructure and appearance, optimized through the objectives of geometry\nreconstruction and novel-view synthesis. Utilizing large-scale public datasets\nfor training, our method delivers state-of-the-art performance in the tasks of\npose estimation, geometry reconstruction, and novel view synthesis, while\nmaintaining the inference efficiency (i.e., less than 0.5 seconds). The project\npage and code can be found at: https://zhanghe3z.github.io/FLARE/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present FLARE, a feed-forward model designed to infer high-quality camera\nposes and 3D geometry from uncalibrated sparse-view images (i.e., as few as 2-8\ninputs), which is a challenging yet practical setting in real-world\napplications. Our solution features a cascaded learning paradigm with camera\npose serving as the critical bridge, recognizing its essential role in mapping\n3D structures onto 2D image planes. Concretely, FLARE starts with camera pose\nestimation, whose results condition the subsequent learning of geometric\nstructure and appearance, optimized through the objectives of geometry\nreconstruction and novel-view synthesis. Utilizing large-scale public datasets\nfor training, our method delivers state-of-the-art performance in the tasks of\npose estimation, geometry reconstruction, and novel view synthesis, while\nmaintaining the inference efficiency (i.e., less than 0.5 seconds). The project\npage and code can be found at: https://zhanghe3z.github.io/FLARE/"
                },
                "authors": [
                    {
                        "name": "Shangzhan Zhang"
                    },
                    {
                        "name": "Jianyuan Wang"
                    },
                    {
                        "name": "Yinghao Xu"
                    },
                    {
                        "name": "Nan Xue"
                    },
                    {
                        "name": "Christian Rupprecht"
                    },
                    {
                        "name": "Xiaowei Zhou"
                    },
                    {
                        "name": "Yujun Shen"
                    },
                    {
                        "name": "Gordon Wetzstein"
                    }
                ],
                "author_detail": {
                    "name": "Gordon Wetzstein"
                },
                "author": "Gordon Wetzstein",
                "arxiv_comment": "CVPR 2025. Website: https://zhanghe3z.github.io/FLARE/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12138v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12138v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18565v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18565v1",
                "updated": "2025-03-24T11:18:25Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    11,
                    18,
                    25,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T11:18:25Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    11,
                    18,
                    25,
                    0,
                    83,
                    0
                ],
                "title": "Distil-xLSTM: Learning Attention Mechanisms through Recurrent Structures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distil-xLSTM: Learning Attention Mechanisms through Recurrent Structures"
                },
                "summary": "The current era of Natural Language Processing (NLP) is dominated by\nTransformer models. However, novel architectures relying on recurrent\nmechanisms, such as xLSTM and Mamba, have been proposed as alternatives to\nattention-based models. Although computation is done differently than with the\nattention mechanism mechanism, these recurrent models yield good results and\nsometimes even outperform state-of-the-art attention-based models. In this\nwork, we propose Distil-xLSTM, an xLSTM-based Small Language Model (SLM)\ntrained by distilling knowledge from a Large Language Model (LLM) that shows\npromising results while being compute and scale efficient. Our Distil-xLSTM\nfocuses on approximating a transformer-based model attention parametrization\nusing its recurrent sequence mixing components and shows good results with\nminimal training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The current era of Natural Language Processing (NLP) is dominated by\nTransformer models. However, novel architectures relying on recurrent\nmechanisms, such as xLSTM and Mamba, have been proposed as alternatives to\nattention-based models. Although computation is done differently than with the\nattention mechanism mechanism, these recurrent models yield good results and\nsometimes even outperform state-of-the-art attention-based models. In this\nwork, we propose Distil-xLSTM, an xLSTM-based Small Language Model (SLM)\ntrained by distilling knowledge from a Large Language Model (LLM) that shows\npromising results while being compute and scale efficient. Our Distil-xLSTM\nfocuses on approximating a transformer-based model attention parametrization\nusing its recurrent sequence mixing components and shows good results with\nminimal training."
                },
                "authors": [
                    {
                        "name": "Abdoul Majid O. Thiombiano"
                    },
                    {
                        "name": "Brahim Hnich"
                    },
                    {
                        "name": "Ali Ben Mrad"
                    },
                    {
                        "name": "Mohamed Wiem Mkaouer"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed Wiem Mkaouer"
                },
                "author": "Mohamed Wiem Mkaouer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18565v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18565v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18562v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18562v1",
                "updated": "2025-03-24T11:16:41Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    11,
                    16,
                    41,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T11:16:41Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    11,
                    16,
                    41,
                    0,
                    83,
                    0
                ],
                "title": "Self-Reported Confidence of Large Language Models in Gastroenterology:\n  Analysis of Commercial, Open-Source, and Quantized Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-Reported Confidence of Large Language Models in Gastroenterology:\n  Analysis of Commercial, Open-Source, and Quantized Models"
                },
                "summary": "This study evaluated self-reported response certainty across several large\nlanguage models (GPT, Claude, Llama, Phi, Mistral, Gemini, Gemma, and Qwen)\nusing 300 gastroenterology board-style questions. The highest-performing models\n(GPT-o1 preview, GPT-4o, and Claude-3.5-Sonnet) achieved Brier scores of\n0.15-0.2 and AUROC of 0.6. Although newer models demonstrated improved\nperformance, all exhibited a consistent tendency towards overconfidence.\nUncertainty estimation presents a significant challenge to the safe use of LLMs\nin healthcare. Keywords: Large Language Models; Confidence Elicitation;\nArtificial Intelligence; Gastroenterology; Uncertainty Quantification",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study evaluated self-reported response certainty across several large\nlanguage models (GPT, Claude, Llama, Phi, Mistral, Gemini, Gemma, and Qwen)\nusing 300 gastroenterology board-style questions. The highest-performing models\n(GPT-o1 preview, GPT-4o, and Claude-3.5-Sonnet) achieved Brier scores of\n0.15-0.2 and AUROC of 0.6. Although newer models demonstrated improved\nperformance, all exhibited a consistent tendency towards overconfidence.\nUncertainty estimation presents a significant challenge to the safe use of LLMs\nin healthcare. Keywords: Large Language Models; Confidence Elicitation;\nArtificial Intelligence; Gastroenterology; Uncertainty Quantification"
                },
                "authors": [
                    {
                        "name": "Nariman Naderi"
                    },
                    {
                        "name": "Seyed Amir Ahmad Safavi-Naini"
                    },
                    {
                        "name": "Thomas Savage"
                    },
                    {
                        "name": "Zahra Atf"
                    },
                    {
                        "name": "Peter Lewis"
                    },
                    {
                        "name": "Girish Nadkarni"
                    },
                    {
                        "name": "Ali Soroush"
                    }
                ],
                "author_detail": {
                    "name": "Ali Soroush"
                },
                "author": "Ali Soroush",
                "arxiv_comment": "35 pages, 5 figures, 1 table, 7 supplementary figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18562v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18562v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18560v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18560v1",
                "updated": "2025-03-24T11:14:32Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    11,
                    14,
                    32,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T11:14:32Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    11,
                    14,
                    32,
                    0,
                    83,
                    0
                ],
                "title": "Simultaneous Inference Bands for Autocorrelations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simultaneous Inference Bands for Autocorrelations"
                },
                "summary": "Sample autocorrelograms typically come with significance bands (non-rejection\nregions) for the null hypothesis of temporal independence. These bands have two\nshortcomings. First, they build on pointwise intervals and suffer from joint\nundercoverage (overrejection) under the null hypothesis. Second, if this null\nis clearly violated one would rather prefer to see confidence bands to quantify\nestimation uncertainty. We propose and discuss both simultaneous significance\nbands and simultaneous confidence bands for time series and series of\nregression residuals. They are as easy to construct as their pointwise\ncounterparts and at the same time provide an intuitive and visual\nquantification of sampling uncertainty as well as valid statistical inference.\nFor regression residuals, we show that for static regressions the asymptotic\nvariances underlying the construction of the bands are as for observed time\nseries and for dynamic regressions (with lagged endogenous regressors) we show\nhow they need to be adjusted. We study theoretical properties of simultaneous\nsignificance bands and two types of simultaneous confidence bands (sup-t and\nBonferroni) and analyse their finite-sample performance in a simulation study.\nFinally, we illustrate the use of the bands in an application to monthly US\ninflation and residuals from Phillips curve regressions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sample autocorrelograms typically come with significance bands (non-rejection\nregions) for the null hypothesis of temporal independence. These bands have two\nshortcomings. First, they build on pointwise intervals and suffer from joint\nundercoverage (overrejection) under the null hypothesis. Second, if this null\nis clearly violated one would rather prefer to see confidence bands to quantify\nestimation uncertainty. We propose and discuss both simultaneous significance\nbands and simultaneous confidence bands for time series and series of\nregression residuals. They are as easy to construct as their pointwise\ncounterparts and at the same time provide an intuitive and visual\nquantification of sampling uncertainty as well as valid statistical inference.\nFor regression residuals, we show that for static regressions the asymptotic\nvariances underlying the construction of the bands are as for observed time\nseries and for dynamic regressions (with lagged endogenous regressors) we show\nhow they need to be adjusted. We study theoretical properties of simultaneous\nsignificance bands and two types of simultaneous confidence bands (sup-t and\nBonferroni) and analyse their finite-sample performance in a simulation study.\nFinally, we illustrate the use of the bands in an application to monthly US\ninflation and residuals from Phillips curve regressions."
                },
                "authors": [
                    {
                        "name": "Uwe Hassler"
                    },
                    {
                        "name": "Marc-Oliver Pohle"
                    },
                    {
                        "name": "Tanja Zahn"
                    }
                ],
                "author_detail": {
                    "name": "Tanja Zahn"
                },
                "author": "Tanja Zahn",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18560v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18560v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18559v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18559v2",
                "updated": "2025-03-25T02:43:16Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    2,
                    43,
                    16,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-24T11:13:33Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    11,
                    13,
                    33,
                    0,
                    83,
                    0
                ],
                "title": "AMD-Hummingbird: Towards an Efficient Text-to-Video Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AMD-Hummingbird: Towards an Efficient Text-to-Video Model"
                },
                "summary": "Text-to-Video (T2V) generation has attracted significant attention for its\nability to synthesize realistic videos from textual descriptions. However,\nexisting models struggle to balance computational efficiency and high visual\nquality, particularly on resource-limited devices, e.g.,iGPUs and mobile\nphones. Most prior work prioritizes visual fidelity while overlooking the need\nfor smaller, more efficient models suitable for real-world deployment. To\naddress this challenge, we propose a lightweight T2V framework, termed\nHummingbird, which prunes existing models and enhances visual quality through\nvisual feedback learning. Our approach reduces the size of the U-Net from 1.4\nbillion to 0.7 billion parameters, significantly improving efficiency while\npreserving high-quality video generation. Additionally, we introduce a novel\ndata processing pipeline that leverages Large Language Models (LLMs) and Video\nQuality Assessment (VQA) models to enhance the quality of both text prompts and\nvideo data. To support user-driven training and style customization, we\npublicly release the full training code, including data processing and model\ntraining. Extensive experiments show that our method achieves a 31X speedup\ncompared to state-of-the-art models such as VideoCrafter2, while also attaining\nthe highest overall score on VBench. Moreover, our method supports the\ngeneration of videos with up to 26 frames, addressing the limitations of\nexisting U-Net-based methods in long video generation. Notably, the entire\ntraining process requires only four GPUs, yet delivers performance competitive\nwith existing leading methods. Hummingbird presents a practical and efficient\nsolution for T2V generation, combining high performance, scalability, and\nflexibility for real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-Video (T2V) generation has attracted significant attention for its\nability to synthesize realistic videos from textual descriptions. However,\nexisting models struggle to balance computational efficiency and high visual\nquality, particularly on resource-limited devices, e.g.,iGPUs and mobile\nphones. Most prior work prioritizes visual fidelity while overlooking the need\nfor smaller, more efficient models suitable for real-world deployment. To\naddress this challenge, we propose a lightweight T2V framework, termed\nHummingbird, which prunes existing models and enhances visual quality through\nvisual feedback learning. Our approach reduces the size of the U-Net from 1.4\nbillion to 0.7 billion parameters, significantly improving efficiency while\npreserving high-quality video generation. Additionally, we introduce a novel\ndata processing pipeline that leverages Large Language Models (LLMs) and Video\nQuality Assessment (VQA) models to enhance the quality of both text prompts and\nvideo data. To support user-driven training and style customization, we\npublicly release the full training code, including data processing and model\ntraining. Extensive experiments show that our method achieves a 31X speedup\ncompared to state-of-the-art models such as VideoCrafter2, while also attaining\nthe highest overall score on VBench. Moreover, our method supports the\ngeneration of videos with up to 26 frames, addressing the limitations of\nexisting U-Net-based methods in long video generation. Notably, the entire\ntraining process requires only four GPUs, yet delivers performance competitive\nwith existing leading methods. Hummingbird presents a practical and efficient\nsolution for T2V generation, combining high performance, scalability, and\nflexibility for real-world applications."
                },
                "authors": [
                    {
                        "name": "Takashi Isobe"
                    },
                    {
                        "name": "He Cui"
                    },
                    {
                        "name": "Dong Zhou"
                    },
                    {
                        "name": "Mengmeng Ge"
                    },
                    {
                        "name": "Dong Li"
                    },
                    {
                        "name": "Emad Barsoum"
                    }
                ],
                "author_detail": {
                    "name": "Emad Barsoum"
                },
                "author": "Emad Barsoum",
                "arxiv_comment": "Homepage:\n  https://www.amd.com/en/developer/resources/technical-articles/amd-hummingbird-0-9b-text-to-video-diffusion-model-with-4-step-inferencing.html|\n  GitHub: https://github.com/AMD-AIG-AIMA/AMD-Hummingbird-T2V",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18559v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18559v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18557v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18557v1",
                "updated": "2025-03-24T11:10:52Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    11,
                    10,
                    52,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T11:10:52Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    11,
                    10,
                    52,
                    0,
                    83,
                    0
                ],
                "title": "LeanStereo: A Leaner Backbone based Stereo Network",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LeanStereo: A Leaner Backbone based Stereo Network"
                },
                "summary": "Recently, end-to-end deep networks based stereo matching methods, mainly\nbecause of their performance, have gained popularity. However, this improvement\nin performance comes at the cost of increased computational and memory\nbandwidth requirements, thus necessitating specialized hardware (GPUs); even\nthen, these methods have large inference times compared to classical methods.\nThis limits their applicability in real-world applications. Although we desire\nhigh accuracy stereo methods albeit with reasonable inference time. To this\nend, we propose a fast end-to-end stereo matching method. Majority of this\nspeedup comes from integrating a leaner backbone. To recover the performance\nlost because of a leaner backbone, we propose to use learned attention weights\nbased cost volume combined with LogL1 loss for stereo matching. Using LogL1\nloss not only improves the overall performance of the proposed network but also\nleads to faster convergence. We do a detailed empirical evaluation of different\ndesign choices and show that our method requires 4x less operations and is also\nabout 9 to 14x faster compared to the state of the art methods like ACVNet [1],\nLEAStereo [2] and CFNet [3] while giving comparable performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, end-to-end deep networks based stereo matching methods, mainly\nbecause of their performance, have gained popularity. However, this improvement\nin performance comes at the cost of increased computational and memory\nbandwidth requirements, thus necessitating specialized hardware (GPUs); even\nthen, these methods have large inference times compared to classical methods.\nThis limits their applicability in real-world applications. Although we desire\nhigh accuracy stereo methods albeit with reasonable inference time. To this\nend, we propose a fast end-to-end stereo matching method. Majority of this\nspeedup comes from integrating a leaner backbone. To recover the performance\nlost because of a leaner backbone, we propose to use learned attention weights\nbased cost volume combined with LogL1 loss for stereo matching. Using LogL1\nloss not only improves the overall performance of the proposed network but also\nleads to faster convergence. We do a detailed empirical evaluation of different\ndesign choices and show that our method requires 4x less operations and is also\nabout 9 to 14x faster compared to the state of the art methods like ACVNet [1],\nLEAStereo [2] and CFNet [3] while giving comparable performance."
                },
                "authors": [
                    {
                        "name": "Rafia Rahim"
                    },
                    {
                        "name": "Samuel Woerz"
                    },
                    {
                        "name": "Andreas Zell"
                    }
                ],
                "author_detail": {
                    "name": "Andreas Zell"
                },
                "author": "Andreas Zell",
                "arxiv_doi": "10.1109/IJCNN54540.2023.10191380",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/IJCNN54540.2023.10191380",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.18557v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18557v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "8 pages, 4 figures",
                "arxiv_journal_ref": "2023 International Joint Conference on Neural Networks (IJCNN)",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18544v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18544v1",
                "updated": "2025-03-24T10:56:57Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    10,
                    56,
                    57,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T10:56:57Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    10,
                    56,
                    57,
                    0,
                    83,
                    0
                ],
                "title": "Distilling Stereo Networks for Performant and Efficient Leaner Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distilling Stereo Networks for Performant and Efficient Leaner Networks"
                },
                "summary": "Knowledge distillation has been quite popular in vision for tasks like\nclassification and segmentation however not much work has been done for\ndistilling state-of-the-art stereo matching methods despite their range of\napplications. One of the reasons for its lack of use in stereo matching\nnetworks is due to the inherent complexity of these networks, where a typical\nnetwork is composed of multiple two- and three-dimensional modules. In this\nwork, we systematically combine the insights from state-of-the-art stereo\nmethods with general knowledge-distillation techniques to develop a joint\nframework for stereo networks distillation with competitive results and faster\ninference. Moreover, we show, via a detailed empirical analysis, that\ndistilling knowledge from the stereo network requires careful design of the\ncomplete distillation pipeline starting from backbone to the right selection of\ndistillation points and corresponding loss functions. This results in the\nstudent networks that are not only leaner and faster but give excellent\nperformance . For instance, our student network while performing better than\nthe performance oriented methods like PSMNet [1], CFNet [2], and LEAStereo [3])\non benchmark SceneFlow dataset, is 8x, 5x, and 8x faster respectively.\nFurthermore, compared to speed oriented methods having inference time less than\n100ms, our student networks perform better than all the tested methods. In\naddition, our student network also shows better generalization capabilities\nwhen tested on unseen datasets like ETH3D and Middlebury.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge distillation has been quite popular in vision for tasks like\nclassification and segmentation however not much work has been done for\ndistilling state-of-the-art stereo matching methods despite their range of\napplications. One of the reasons for its lack of use in stereo matching\nnetworks is due to the inherent complexity of these networks, where a typical\nnetwork is composed of multiple two- and three-dimensional modules. In this\nwork, we systematically combine the insights from state-of-the-art stereo\nmethods with general knowledge-distillation techniques to develop a joint\nframework for stereo networks distillation with competitive results and faster\ninference. Moreover, we show, via a detailed empirical analysis, that\ndistilling knowledge from the stereo network requires careful design of the\ncomplete distillation pipeline starting from backbone to the right selection of\ndistillation points and corresponding loss functions. This results in the\nstudent networks that are not only leaner and faster but give excellent\nperformance . For instance, our student network while performing better than\nthe performance oriented methods like PSMNet [1], CFNet [2], and LEAStereo [3])\non benchmark SceneFlow dataset, is 8x, 5x, and 8x faster respectively.\nFurthermore, compared to speed oriented methods having inference time less than\n100ms, our student networks perform better than all the tested methods. In\naddition, our student network also shows better generalization capabilities\nwhen tested on unseen datasets like ETH3D and Middlebury."
                },
                "authors": [
                    {
                        "name": "Rafia Rahim"
                    },
                    {
                        "name": "Samuel Woerz"
                    },
                    {
                        "name": "Andreas Zell"
                    }
                ],
                "author_detail": {
                    "name": "Andreas Zell"
                },
                "author": "Andreas Zell",
                "arxiv_doi": "10.1109/IJCNN54540.2023.10191503",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/IJCNN54540.2023.10191503",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.18544v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18544v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "8 pages, 3 figures. Published in: 2023 International Joint Conference\n  on Neural Networks (IJCNN)",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17034v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17034v3",
                "updated": "2025-03-24T10:31:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    10,
                    31,
                    53,
                    0,
                    83,
                    0
                ],
                "published": "2025-02-24T10:34:35Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    10,
                    34,
                    35,
                    0,
                    55,
                    0
                ],
                "title": "Evolution 6.0: Evolving Robotic Capabilities Through Generative Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evolution 6.0: Evolving Robotic Capabilities Through Generative Design"
                },
                "summary": "We propose a new concept, Evolution 6.0, which represents the evolution of\nrobotics driven by Generative AI. When a robot lacks the necessary tools to\naccomplish a task requested by a human, it autonomously designs the required\ninstruments and learns how to use them to achieve the goal. Evolution 6.0 is an\nautonomous robotic system powered by Vision-Language Models (VLMs),\nVision-Language Action (VLA) models, and Text-to-3D generative models for tool\ndesign and task execution. The system comprises two key modules: the Tool\nGeneration Module, which fabricates task-specific tools from visual and textual\ndata, and the Action Generation Module, which converts natural language\ninstructions into robotic actions. It integrates QwenVLM for environmental\nunderstanding, OpenVLA for task execution, and Llama-Mesh for 3D tool\ngeneration. Evaluation results demonstrate a 90% success rate for tool\ngeneration with a 10-second inference time, and action generation achieving\n83.5% in physical and visual generalization, 70% in motion generalization, and\n37% in semantic generalization. Future improvements will focus on bimanual\nmanipulation, expanded task capabilities, and enhanced environmental\ninterpretation to improve real-world adaptability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a new concept, Evolution 6.0, which represents the evolution of\nrobotics driven by Generative AI. When a robot lacks the necessary tools to\naccomplish a task requested by a human, it autonomously designs the required\ninstruments and learns how to use them to achieve the goal. Evolution 6.0 is an\nautonomous robotic system powered by Vision-Language Models (VLMs),\nVision-Language Action (VLA) models, and Text-to-3D generative models for tool\ndesign and task execution. The system comprises two key modules: the Tool\nGeneration Module, which fabricates task-specific tools from visual and textual\ndata, and the Action Generation Module, which converts natural language\ninstructions into robotic actions. It integrates QwenVLM for environmental\nunderstanding, OpenVLA for task execution, and Llama-Mesh for 3D tool\ngeneration. Evaluation results demonstrate a 90% success rate for tool\ngeneration with a 10-second inference time, and action generation achieving\n83.5% in physical and visual generalization, 70% in motion generalization, and\n37% in semantic generalization. Future improvements will focus on bimanual\nmanipulation, expanded task capabilities, and enhanced environmental\ninterpretation to improve real-world adaptability."
                },
                "authors": [
                    {
                        "name": "Muhammad Haris Khan"
                    },
                    {
                        "name": "Artyom Myshlyaev"
                    },
                    {
                        "name": "Artem Lykov"
                    },
                    {
                        "name": "Miguel Altamirano Cabrera"
                    },
                    {
                        "name": "Dzmitry Tsetserukou"
                    }
                ],
                "author_detail": {
                    "name": "Dzmitry Tsetserukou"
                },
                "author": "Dzmitry Tsetserukou",
                "arxiv_comment": "Submitted to IROS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17034v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17034v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18526v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18526v1",
                "updated": "2025-03-24T10:31:31Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    10,
                    31,
                    31,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T10:31:31Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    10,
                    31,
                    31,
                    0,
                    83,
                    0
                ],
                "title": "SciClaims: An End-to-End Generative System for Biomedical Claim Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SciClaims: An End-to-End Generative System for Biomedical Claim Analysis"
                },
                "summary": "Validating key claims in scientific literature, particularly in biomedical\nresearch, is essential for ensuring accuracy and advancing knowledge. This\nprocess is critical in sectors like the pharmaceutical industry, where rapid\nscientific progress requires automation and deep domain expertise. However,\ncurrent solutions have significant limitations. They lack end-to-end pipelines\nencompassing all claim extraction, evidence retrieval, and verification steps;\nrely on complex NLP and information retrieval pipelines prone to multiple\nfailure points; and often fail to provide clear, user-friendly justifications\nfor claim verification outcomes. To address these challenges, we introduce\nSciClaims, an advanced system powered by state-of-the-art large language models\n(LLMs) that seamlessly integrates the entire scientific claim analysis process.\nSciClaims outperforms previous approaches in both claim extraction and\nverification without requiring additional fine-tuning, setting a new benchmark\nfor automated scientific claim analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Validating key claims in scientific literature, particularly in biomedical\nresearch, is essential for ensuring accuracy and advancing knowledge. This\nprocess is critical in sectors like the pharmaceutical industry, where rapid\nscientific progress requires automation and deep domain expertise. However,\ncurrent solutions have significant limitations. They lack end-to-end pipelines\nencompassing all claim extraction, evidence retrieval, and verification steps;\nrely on complex NLP and information retrieval pipelines prone to multiple\nfailure points; and often fail to provide clear, user-friendly justifications\nfor claim verification outcomes. To address these challenges, we introduce\nSciClaims, an advanced system powered by state-of-the-art large language models\n(LLMs) that seamlessly integrates the entire scientific claim analysis process.\nSciClaims outperforms previous approaches in both claim extraction and\nverification without requiring additional fine-tuning, setting a new benchmark\nfor automated scientific claim analysis."
                },
                "authors": [
                    {
                        "name": "Ral Ortega"
                    },
                    {
                        "name": "Jos Manuel Gmez-Prez"
                    }
                ],
                "author_detail": {
                    "name": "Jos Manuel Gmez-Prez"
                },
                "author": "Jos Manuel Gmez-Prez",
                "arxiv_comment": "Pre-print version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18526v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18526v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.00565v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.00565v3",
                "updated": "2025-03-24T10:30:14Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    10,
                    30,
                    14,
                    0,
                    83,
                    0
                ],
                "published": "2024-06-01T22:19:54Z",
                "published_parsed": [
                    2024,
                    6,
                    1,
                    22,
                    19,
                    54,
                    5,
                    153,
                    0
                ],
                "title": "Efficient Massive Black Hole Binary parameter estimation for LISA using\n  Sequential Neural Likelihood",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Massive Black Hole Binary parameter estimation for LISA using\n  Sequential Neural Likelihood"
                },
                "summary": "The inspiral, merger, and ringdown of Massive Black Hole Binaries (MBHBs) is\none the main sources of Gravitational Waves (GWs) for the future Laser\nInterferometer Space Antenna (LISA), an ESA-led mission in the implementation\nphase. It is expected that LISA will detect these systems throughout the entire\nobservable universe. Robust and efficient data analysis algorithms are\nnecessary to detect and estimate physical parameters for these systems. In this\nwork, we explore the application of Sequential Neural Likelihood, a\nsimulation-based inference algorithm, to detect and characterize MBHB GW\nsignals in synthetic LISA data. We describe in detail the different elements of\nthe method, their performance and possible alternatives that can be used to\nenhance the performance. Instead of sampling from the conventional likelihood\nfunction, which requires a forward simulation for each evaluation, this method\nconstructs a surrogate likelihood that is ultimately described by a neural\nnetwork trained from a dataset of simulations of the MBHB signals and noise.\nOne important advantage of this method is that, given that the likelihood is\nindependent of the priors, we can iteratively train models that target specific\nobservations in a fraction of the time and computational cost that other\ntraditional and machine learning-based strategies would require. Because of the\niterative nature of the method, we are able to train models to obtain\nqualitatively similar posteriors with less than 2\\% of the simulator calls that\nMarkov Chain Monte Carlo methods would require. We compare these posteriors\nwith those obtained from Markov Chain Monte Carlo techniques and discuss the\ndifferences that appear, in particular in relation with the important role that\ndata compression has in the modular implementation of the method that we\npresent. We also discuss different strategies to improve the performance of the\nalgorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The inspiral, merger, and ringdown of Massive Black Hole Binaries (MBHBs) is\none the main sources of Gravitational Waves (GWs) for the future Laser\nInterferometer Space Antenna (LISA), an ESA-led mission in the implementation\nphase. It is expected that LISA will detect these systems throughout the entire\nobservable universe. Robust and efficient data analysis algorithms are\nnecessary to detect and estimate physical parameters for these systems. In this\nwork, we explore the application of Sequential Neural Likelihood, a\nsimulation-based inference algorithm, to detect and characterize MBHB GW\nsignals in synthetic LISA data. We describe in detail the different elements of\nthe method, their performance and possible alternatives that can be used to\nenhance the performance. Instead of sampling from the conventional likelihood\nfunction, which requires a forward simulation for each evaluation, this method\nconstructs a surrogate likelihood that is ultimately described by a neural\nnetwork trained from a dataset of simulations of the MBHB signals and noise.\nOne important advantage of this method is that, given that the likelihood is\nindependent of the priors, we can iteratively train models that target specific\nobservations in a fraction of the time and computational cost that other\ntraditional and machine learning-based strategies would require. Because of the\niterative nature of the method, we are able to train models to obtain\nqualitatively similar posteriors with less than 2\\% of the simulator calls that\nMarkov Chain Monte Carlo methods would require. We compare these posteriors\nwith those obtained from Markov Chain Monte Carlo techniques and discuss the\ndifferences that appear, in particular in relation with the important role that\ndata compression has in the modular implementation of the method that we\npresent. We also discuss different strategies to improve the performance of the\nalgorithms."
                },
                "authors": [
                    {
                        "name": "Ivn Martn Vlchez"
                    },
                    {
                        "name": "Carlos F. Sopuerta"
                    }
                ],
                "author_detail": {
                    "name": "Carlos F. Sopuerta"
                },
                "author": "Carlos F. Sopuerta",
                "arxiv_comment": "JCAP LaTeX style, 46 pages, 13 figures, 20 plot files, 3 tables.\n  Version accepted for publication in JCAP",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.00565v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.00565v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18525v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18525v1",
                "updated": "2025-03-24T10:29:47Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    10,
                    29,
                    47,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T10:29:47Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    10,
                    29,
                    47,
                    0,
                    83,
                    0
                ],
                "title": "P3Nav: A Unified Framework for Embodied Navigation Integrating\n  Perception, Planning, and Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "P3Nav: A Unified Framework for Embodied Navigation Integrating\n  Perception, Planning, and Prediction"
                },
                "summary": "In language-guided visual navigation, agents locate target objects in unseen\nenvironments using natural language instructions. For reliable navigation in\nunfamiliar scenes, agents must possess strong perception, planning, and\nprediction capabilities. Additionally, when agents revisit previously explored\nareas during long-term navigation, they may retain irrelevant and redundant\nhistorical perceptions, leading to suboptimal results. In this work, we\nintroduce \\textbf{P3Nav}, a unified framework that integrates\n\\textbf{P}erception, \\textbf{P}lanning, and \\textbf{P}rediction capabilities\nthrough \\textbf{Multitask Collaboration} on navigation and embodied question\nanswering (EQA) tasks, thereby enhancing navigation performance. Furthermore,\nP3Nav employs an \\textbf{Adaptive 3D-aware History Sampling} strategy to\neffectively and efficiently utilize historical observations. By leveraging the\nlarge language models (LLM), P3Nav comprehends diverse commands and complex\nvisual scenes, resulting in appropriate navigation actions. P3Nav achieves a\n75\\% success rate in object goal navigation on the\n$\\mathrm{CHORES}$-$\\mathbb{S}$ benchmark, setting a new state-of-the-art\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In language-guided visual navigation, agents locate target objects in unseen\nenvironments using natural language instructions. For reliable navigation in\nunfamiliar scenes, agents must possess strong perception, planning, and\nprediction capabilities. Additionally, when agents revisit previously explored\nareas during long-term navigation, they may retain irrelevant and redundant\nhistorical perceptions, leading to suboptimal results. In this work, we\nintroduce \\textbf{P3Nav}, a unified framework that integrates\n\\textbf{P}erception, \\textbf{P}lanning, and \\textbf{P}rediction capabilities\nthrough \\textbf{Multitask Collaboration} on navigation and embodied question\nanswering (EQA) tasks, thereby enhancing navigation performance. Furthermore,\nP3Nav employs an \\textbf{Adaptive 3D-aware History Sampling} strategy to\neffectively and efficiently utilize historical observations. By leveraging the\nlarge language models (LLM), P3Nav comprehends diverse commands and complex\nvisual scenes, resulting in appropriate navigation actions. P3Nav achieves a\n75\\% success rate in object goal navigation on the\n$\\mathrm{CHORES}$-$\\mathbb{S}$ benchmark, setting a new state-of-the-art\nperformance."
                },
                "authors": [
                    {
                        "name": "Yufeng Zhong"
                    },
                    {
                        "name": "Chengjian Feng"
                    },
                    {
                        "name": "Feng Yan"
                    },
                    {
                        "name": "Fanfan Liu"
                    },
                    {
                        "name": "Liming Zheng"
                    },
                    {
                        "name": "Lin Ma"
                    }
                ],
                "author_detail": {
                    "name": "Lin Ma"
                },
                "author": "Lin Ma",
                "arxiv_comment": "14 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18525v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18525v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03214v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03214v3",
                "updated": "2025-03-24T10:29:33Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    10,
                    29,
                    33,
                    0,
                    83,
                    0
                ],
                "published": "2024-12-04T11:05:01Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    11,
                    5,
                    1,
                    2,
                    339,
                    0
                ],
                "title": "Continual Low-Rank Scaled Dot-product Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continual Low-Rank Scaled Dot-product Attention"
                },
                "summary": "Transformers are widely used for their ability to capture data relations in\nsequence processing, with great success for a wide range of static tasks.\nHowever, the computational and memory footprint of their main component, i.e.,\nthe Scaled Dot-product Attention, is commonly overlooked. This makes their\nadoption in applications involving stream data processing with constraints in\nresponse latency, computational and memory resources infeasible. Some works\nhave proposed methods to lower the computational cost of Transformers, i.e.\nlow-rank approximations, sparsity in attention, and efficient formulations for\nContinual Inference. In this paper, we introduce a new formulation of the\nScaled Dot-product Attention based on the Nystr\\\"om approximation that is\nsuitable for Continual Inference. In experiments on Online Audio Classification\nand Online Action Detection tasks, the proposed Continual Scaled Dot-product\nAttention can lower the number of operations by up to three orders of magnitude\ncompared to the original Transformers while retaining the predictive\nperformance of competing models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers are widely used for their ability to capture data relations in\nsequence processing, with great success for a wide range of static tasks.\nHowever, the computational and memory footprint of their main component, i.e.,\nthe Scaled Dot-product Attention, is commonly overlooked. This makes their\nadoption in applications involving stream data processing with constraints in\nresponse latency, computational and memory resources infeasible. Some works\nhave proposed methods to lower the computational cost of Transformers, i.e.\nlow-rank approximations, sparsity in attention, and efficient formulations for\nContinual Inference. In this paper, we introduce a new formulation of the\nScaled Dot-product Attention based on the Nystr\\\"om approximation that is\nsuitable for Continual Inference. In experiments on Online Audio Classification\nand Online Action Detection tasks, the proposed Continual Scaled Dot-product\nAttention can lower the number of operations by up to three orders of magnitude\ncompared to the original Transformers while retaining the predictive\nperformance of competing models."
                },
                "authors": [
                    {
                        "name": "Gins Carreto Picn"
                    },
                    {
                        "name": "Illia Oleksiienko"
                    },
                    {
                        "name": "Lukas Hedegaard"
                    },
                    {
                        "name": "Arian Bakhtiarnia"
                    },
                    {
                        "name": "Alexandros Iosifidis"
                    }
                ],
                "author_detail": {
                    "name": "Alexandros Iosifidis"
                },
                "author": "Alexandros Iosifidis",
                "arxiv_comment": "14 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03214v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03214v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11774v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11774v3",
                "updated": "2025-03-24T10:25:29Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    10,
                    25,
                    29,
                    0,
                    83,
                    0
                ],
                "published": "2024-10-15T16:55:10Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    16,
                    55,
                    10,
                    1,
                    289,
                    0
                ],
                "title": "Fractal Calibration for long-tailed object detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fractal Calibration for long-tailed object detection"
                },
                "summary": "Real-world datasets follow an imbalanced distribution, which poses\nsignificant challenges in rare-category object detection. Recent studies tackle\nthis problem by developing re-weighting and re-sampling methods, that utilise\nthe class frequencies of the dataset. However, these techniques focus solely on\nthe frequency statistics and ignore the distribution of the classes in image\nspace, missing important information. In contrast to them, we propose FRActal\nCALibration (FRACAL): a novel post-calibration method for long-tailed object\ndetection. FRACAL devises a logit adjustment method that utilises the fractal\ndimension to estimate how uniformly classes are distributed in image space.\nDuring inference, it uses the fractal dimension to inversely downweight the\nprobabilities of uniformly spaced class predictions achieving balance in two\naxes: between frequent and rare categories, and between uniformly spaced and\nsparsely spaced classes. FRACAL is a post-processing method and it does not\nrequire any training, also it can be combined with many off-the-shelf models\nsuch as one-stage sigmoid detectors and two-stage instance segmentation models.\nFRACAL boosts the rare class performance by up to 8.6% and surpasses all\nprevious methods on LVIS dataset, while showing good generalisation to other\ndatasets such as COCO, V3Det and OpenImages. We provide the code at\nhttps://github.com/kostas1515/FRACAL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-world datasets follow an imbalanced distribution, which poses\nsignificant challenges in rare-category object detection. Recent studies tackle\nthis problem by developing re-weighting and re-sampling methods, that utilise\nthe class frequencies of the dataset. However, these techniques focus solely on\nthe frequency statistics and ignore the distribution of the classes in image\nspace, missing important information. In contrast to them, we propose FRActal\nCALibration (FRACAL): a novel post-calibration method for long-tailed object\ndetection. FRACAL devises a logit adjustment method that utilises the fractal\ndimension to estimate how uniformly classes are distributed in image space.\nDuring inference, it uses the fractal dimension to inversely downweight the\nprobabilities of uniformly spaced class predictions achieving balance in two\naxes: between frequent and rare categories, and between uniformly spaced and\nsparsely spaced classes. FRACAL is a post-processing method and it does not\nrequire any training, also it can be combined with many off-the-shelf models\nsuch as one-stage sigmoid detectors and two-stage instance segmentation models.\nFRACAL boosts the rare class performance by up to 8.6% and surpasses all\nprevious methods on LVIS dataset, while showing good generalisation to other\ndatasets such as COCO, V3Det and OpenImages. We provide the code at\nhttps://github.com/kostas1515/FRACAL."
                },
                "authors": [
                    {
                        "name": "Konstantinos Panagiotis Alexandridis"
                    },
                    {
                        "name": "Ismail Elezi"
                    },
                    {
                        "name": "Jiankang Deng"
                    },
                    {
                        "name": "Anh Nguyen"
                    },
                    {
                        "name": "Shan Luo"
                    }
                ],
                "author_detail": {
                    "name": "Shan Luo"
                },
                "author": "Shan Luo",
                "arxiv_comment": "CVPR2025 (camera-ready)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11774v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11774v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18523v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18523v1",
                "updated": "2025-03-24T10:25:00Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    10,
                    25,
                    0,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T10:25:00Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    10,
                    25,
                    0,
                    0,
                    83,
                    0
                ],
                "title": "Minimax Rate-Optimal Inference for Individualized Quantile Treatment\n  Effects in High-dimensional Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Minimax Rate-Optimal Inference for Individualized Quantile Treatment\n  Effects in High-dimensional Models"
                },
                "summary": "The quantification of treatment effects plays an important role in a wide\nrange of applications, including policy making and bio-pharmaceutical research.\nIn this article, we study the quantile treatment effect (QTE) while addressing\ntwo specific types of heterogeneities: (a) personalized heterogeneity, which\ncaptures the varying treatment effects for different individuals, and (b)\nquantile heterogeneity, which accounts for how the impact of covariates varies\nacross different quantile levels. A well-designed debiased estimator for the\nindividualized quantile treatment effect (IQTE) is proposed to capture such\nheterogeneities effectively. We show that this estimator converges weakly to a\nGaussian process as a function of the quantile levels and propose valid\nstatistical inference methods, including the construction of confidence\nintervals and the development of hypothesis testing decision rules. In\naddition, the minimax optimality frameworks for these inference procedures are\nestablished. Specifically, we derive the minimax optimal rates for the expected\nlength of confidence intervals and the magnitude of the detection boundary for\nhypothesis testing procedures, illustrating the superiority of the proposed\nestimator. The effectiveness of our methods is demonstrated through extensive\nsimulations and an analysis of the National Health and Nutrition Examination\nSurvey (NHANES) datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The quantification of treatment effects plays an important role in a wide\nrange of applications, including policy making and bio-pharmaceutical research.\nIn this article, we study the quantile treatment effect (QTE) while addressing\ntwo specific types of heterogeneities: (a) personalized heterogeneity, which\ncaptures the varying treatment effects for different individuals, and (b)\nquantile heterogeneity, which accounts for how the impact of covariates varies\nacross different quantile levels. A well-designed debiased estimator for the\nindividualized quantile treatment effect (IQTE) is proposed to capture such\nheterogeneities effectively. We show that this estimator converges weakly to a\nGaussian process as a function of the quantile levels and propose valid\nstatistical inference methods, including the construction of confidence\nintervals and the development of hypothesis testing decision rules. In\naddition, the minimax optimality frameworks for these inference procedures are\nestablished. Specifically, we derive the minimax optimal rates for the expected\nlength of confidence intervals and the magnitude of the detection boundary for\nhypothesis testing procedures, illustrating the superiority of the proposed\nestimator. The effectiveness of our methods is demonstrated through extensive\nsimulations and an analysis of the National Health and Nutrition Examination\nSurvey (NHANES) datasets."
                },
                "authors": [
                    {
                        "name": "Jiachen Sun"
                    },
                    {
                        "name": "Yin Xia"
                    }
                ],
                "author_detail": {
                    "name": "Yin Xia"
                },
                "author": "Yin Xia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18523v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18523v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11425v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11425v3",
                "updated": "2025-03-24T10:18:56Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    10,
                    18,
                    56,
                    0,
                    83,
                    0
                ],
                "published": "2025-01-20T11:46:04Z",
                "published_parsed": [
                    2025,
                    1,
                    20,
                    11,
                    46,
                    4,
                    0,
                    20,
                    0
                ],
                "title": "Agent-R: Training Language Model Agents to Reflect via Iterative\n  Self-Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agent-R: Training Language Model Agents to Reflect via Iterative\n  Self-Training"
                },
                "summary": "Large Language Models (LLMs) agents are increasingly pivotal for addressing\ncomplex tasks in interactive environments. Existing work mainly focuses on\nenhancing performance through behavior cloning from stronger experts, yet such\napproaches often falter in real-world applications, mainly due to the inability\nto recover from errors. However, step-level critique data is difficult and\nexpensive to collect. Automating and dynamically constructing self-critique\ndatasets is thus crucial to empowering models with intelligent agent\ncapabilities. In this work, we propose an iterative self-training framework,\nAgent-R, that enables language Agent to Reflect on the fly. Unlike traditional\nmethods that reward or penalize actions based on correctness, Agent-R leverages\nMCTS to construct training data that recover correct trajectories from\nerroneous ones. A key challenge of agent reflection lies in the necessity for\ntimely revision rather than waiting until the end of a rollout. To address\nthis, we introduce a model-guided critique construction mechanism: the actor\nmodel identifies the first error step (within its current capability) in a\nfailed trajectory. Starting from it, we splice it with the adjacent correct\npath, which shares the same parent node in the tree. This strategy enables the\nmodel to learn reflection based on its current policy, therefore yielding\nbetter learning efficiency. To further explore the scalability of this\nself-improvement paradigm, we investigate iterative refinement of both error\ncorrection capabilities and dataset construction. Our findings demonstrate that\nAgent-R continuously improves the model's ability to recover from errors and\nenables timely error correction. Experiments on three interactive environments\nshow that Agent-R effectively equips agents to correct erroneous actions while\navoiding loops, achieving superior performance compared to baseline methods\n(+5.59%).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) agents are increasingly pivotal for addressing\ncomplex tasks in interactive environments. Existing work mainly focuses on\nenhancing performance through behavior cloning from stronger experts, yet such\napproaches often falter in real-world applications, mainly due to the inability\nto recover from errors. However, step-level critique data is difficult and\nexpensive to collect. Automating and dynamically constructing self-critique\ndatasets is thus crucial to empowering models with intelligent agent\ncapabilities. In this work, we propose an iterative self-training framework,\nAgent-R, that enables language Agent to Reflect on the fly. Unlike traditional\nmethods that reward or penalize actions based on correctness, Agent-R leverages\nMCTS to construct training data that recover correct trajectories from\nerroneous ones. A key challenge of agent reflection lies in the necessity for\ntimely revision rather than waiting until the end of a rollout. To address\nthis, we introduce a model-guided critique construction mechanism: the actor\nmodel identifies the first error step (within its current capability) in a\nfailed trajectory. Starting from it, we splice it with the adjacent correct\npath, which shares the same parent node in the tree. This strategy enables the\nmodel to learn reflection based on its current policy, therefore yielding\nbetter learning efficiency. To further explore the scalability of this\nself-improvement paradigm, we investigate iterative refinement of both error\ncorrection capabilities and dataset construction. Our findings demonstrate that\nAgent-R continuously improves the model's ability to recover from errors and\nenables timely error correction. Experiments on three interactive environments\nshow that Agent-R effectively equips agents to correct erroneous actions while\navoiding loops, achieving superior performance compared to baseline methods\n(+5.59%)."
                },
                "authors": [
                    {
                        "name": "Siyu Yuan"
                    },
                    {
                        "name": "Zehui Chen"
                    },
                    {
                        "name": "Zhiheng Xi"
                    },
                    {
                        "name": "Junjie Ye"
                    },
                    {
                        "name": "Zhengyin Du"
                    },
                    {
                        "name": "Jiecao Chen"
                    }
                ],
                "author_detail": {
                    "name": "Jiecao Chen"
                },
                "author": "Jiecao Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11425v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11425v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11335v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11335v2",
                "updated": "2025-03-24T10:10:38Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    10,
                    10,
                    38,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-14T12:03:29Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    12,
                    3,
                    29,
                    4,
                    73,
                    0
                ],
                "title": "APLA: A Simple Adaptation Method for Vision Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "APLA: A Simple Adaptation Method for Vision Transformers"
                },
                "summary": "Existing adaptation techniques typically require architectural modifications\nor added parameters, leading to high computational costs and complexity. We\nintroduce Attention Projection Layer Adaptation (APLA), a simple approach to\nadapt vision transformers (ViTs) without altering the architecture or adding\nparameters. Through a systematic analysis, we find that the layer immediately\nafter the attention mechanism is crucial for adaptation. By updating only this\nprojection layer, or even just a random subset of this layer's weights, APLA\nachieves state-of-the-art performance while reducing GPU memory usage by up to\n52.63% and training time by up to 43.0%, with no extra cost at inference.\nAcross 46 datasets covering a variety of tasks including scene classification,\nmedical imaging, satellite imaging, and fine-grained classification, APLA\nconsistently outperforms 17 other leading adaptation methods, including full\nfine-tuning, on classification, segmentation, and detection tasks. The code is\navailable at https://github.com/MoeinSorkhei/APLA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing adaptation techniques typically require architectural modifications\nor added parameters, leading to high computational costs and complexity. We\nintroduce Attention Projection Layer Adaptation (APLA), a simple approach to\nadapt vision transformers (ViTs) without altering the architecture or adding\nparameters. Through a systematic analysis, we find that the layer immediately\nafter the attention mechanism is crucial for adaptation. By updating only this\nprojection layer, or even just a random subset of this layer's weights, APLA\nachieves state-of-the-art performance while reducing GPU memory usage by up to\n52.63% and training time by up to 43.0%, with no extra cost at inference.\nAcross 46 datasets covering a variety of tasks including scene classification,\nmedical imaging, satellite imaging, and fine-grained classification, APLA\nconsistently outperforms 17 other leading adaptation methods, including full\nfine-tuning, on classification, segmentation, and detection tasks. The code is\navailable at https://github.com/MoeinSorkhei/APLA."
                },
                "authors": [
                    {
                        "name": "Moein Sorkhei"
                    },
                    {
                        "name": "Emir Konuk"
                    },
                    {
                        "name": "Kevin Smith"
                    },
                    {
                        "name": "Christos Matsoukas"
                    }
                ],
                "author_detail": {
                    "name": "Christos Matsoukas"
                },
                "author": "Christos Matsoukas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11335v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11335v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.09420v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.09420v3",
                "updated": "2025-03-24T10:09:54Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    10,
                    9,
                    54,
                    0,
                    83,
                    0
                ],
                "published": "2024-01-17T18:59:26Z",
                "published_parsed": [
                    2024,
                    1,
                    17,
                    18,
                    59,
                    26,
                    2,
                    17,
                    0
                ],
                "title": "LionHeart: A Layer-based Mapping Framework for Heterogeneous Systems\n  with Analog In-Memory Computing Tiles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LionHeart: A Layer-based Mapping Framework for Heterogeneous Systems\n  with Analog In-Memory Computing Tiles"
                },
                "summary": "When arranged in a crossbar configuration, resistive memory devices can be\nused to execute Matrix-Vector Multiplications (MVMs), the most dominant\noperation of many Machine Learning (ML) algorithms, in constant time\ncomplexity. Nonetheless, when performing computations in the analog domain,\nnovel challenges are introduced in terms of arithmetic precision and\nstochasticity, due to non-ideal circuit and device behaviour. Moreover, these\nnon-idealities have a temporal dimension, resulting in a degrading application\naccuracy over time. Facing these challenges, we propose a novel framework,\nnamed LionHeart, to obtain hybrid analog-digital mappings to execute Deep\nLearning (DL) inference workloads using heterogeneous accelerators. The\naccuracy-constrained mappings derived by LionHeart showcase, across different\nConvolutional Neural Networks (CNNs) and one transformer-based network, high\naccuracy and potential for speedup. The results of the full system simulations\nhighlight run-time reductions and energy efficiency gains that exceed 6X, with\na user-defined accuracy threshold for a fully digital floating point\nimplementation. LionHeart is open-sourced here:\nhttps://github.com/IBM/lionheart.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When arranged in a crossbar configuration, resistive memory devices can be\nused to execute Matrix-Vector Multiplications (MVMs), the most dominant\noperation of many Machine Learning (ML) algorithms, in constant time\ncomplexity. Nonetheless, when performing computations in the analog domain,\nnovel challenges are introduced in terms of arithmetic precision and\nstochasticity, due to non-ideal circuit and device behaviour. Moreover, these\nnon-idealities have a temporal dimension, resulting in a degrading application\naccuracy over time. Facing these challenges, we propose a novel framework,\nnamed LionHeart, to obtain hybrid analog-digital mappings to execute Deep\nLearning (DL) inference workloads using heterogeneous accelerators. The\naccuracy-constrained mappings derived by LionHeart showcase, across different\nConvolutional Neural Networks (CNNs) and one transformer-based network, high\naccuracy and potential for speedup. The results of the full system simulations\nhighlight run-time reductions and energy efficiency gains that exceed 6X, with\na user-defined accuracy threshold for a fully digital floating point\nimplementation. LionHeart is open-sourced here:\nhttps://github.com/IBM/lionheart."
                },
                "authors": [
                    {
                        "name": "Corey Lammie"
                    },
                    {
                        "name": "Yuxuan Wang"
                    },
                    {
                        "name": "Flavio Ponzina"
                    },
                    {
                        "name": "Joshua Klein"
                    },
                    {
                        "name": "Hadjer Benmeziane"
                    },
                    {
                        "name": "Marina Zapater"
                    },
                    {
                        "name": "Irem Boybat"
                    },
                    {
                        "name": "Abu Sebastian"
                    },
                    {
                        "name": "Giovanni Ansaloni"
                    },
                    {
                        "name": "David Atienza"
                    }
                ],
                "author_detail": {
                    "name": "David Atienza"
                },
                "author": "David Atienza",
                "arxiv_doi": "10.1109/TETC.2025.3546128",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TETC.2025.3546128",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2401.09420v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.09420v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by IEEE Transactions on Emerging Topics in Computing",
                "arxiv_primary_category": {
                    "term": "cs.ET",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2503.18943v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18943v1",
                "updated": "2025-03-24T17:59:07Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    17,
                    59,
                    7,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T17:59:07Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    17,
                    59,
                    7,
                    0,
                    83,
                    0
                ],
                "title": "SlowFast-LLaVA-1.5: A Family of Token-Efficient Video Large Language\n  Models for Long-Form Video Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SlowFast-LLaVA-1.5: A Family of Token-Efficient Video Large Language\n  Models for Long-Form Video Understanding"
                },
                "summary": "We introduce SlowFast-LLaVA-1.5 (abbreviated as SF-LLaVA-1.5), a family of\nvideo large language models (LLMs) offering a token-efficient solution for\nlong-form video understanding. This model family employs the two-stream\nSlowFast mechanism, enabling efficient modeling of long-range temporal context\nto meet the demand for lightweight, mobile-friendly Video LLMs. We provide\nmodels ranging from 1B to 7B parameters, optimized through a streamlined\ntraining pipeline and a high-quality data mixture composed of publicly\navailable datasets. Experimental results demonstrate that SF-LLaVA-1.5 achieves\ncompetitive performance on a wide range of video and image benchmarks, with\nrobust results across all model sizes. Notably, SF-LLaVA-1.5 achieves\nstate-of-the-art results in long-form video understanding (e.g., LongVideoBench\nand MLVU) and excels at small scales (1B and 3B) across various video\nbenchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce SlowFast-LLaVA-1.5 (abbreviated as SF-LLaVA-1.5), a family of\nvideo large language models (LLMs) offering a token-efficient solution for\nlong-form video understanding. This model family employs the two-stream\nSlowFast mechanism, enabling efficient modeling of long-range temporal context\nto meet the demand for lightweight, mobile-friendly Video LLMs. We provide\nmodels ranging from 1B to 7B parameters, optimized through a streamlined\ntraining pipeline and a high-quality data mixture composed of publicly\navailable datasets. Experimental results demonstrate that SF-LLaVA-1.5 achieves\ncompetitive performance on a wide range of video and image benchmarks, with\nrobust results across all model sizes. Notably, SF-LLaVA-1.5 achieves\nstate-of-the-art results in long-form video understanding (e.g., LongVideoBench\nand MLVU) and excels at small scales (1B and 3B) across various video\nbenchmarks."
                },
                "authors": [
                    {
                        "name": "Mingze Xu"
                    },
                    {
                        "name": "Mingfei Gao"
                    },
                    {
                        "name": "Shiyu Li"
                    },
                    {
                        "name": "Jiasen Lu"
                    },
                    {
                        "name": "Zhe Gan"
                    },
                    {
                        "name": "Zhengfeng Lai"
                    },
                    {
                        "name": "Meng Cao"
                    },
                    {
                        "name": "Kai Kang"
                    },
                    {
                        "name": "Yinfei Yang"
                    },
                    {
                        "name": "Afshin Dehghan"
                    }
                ],
                "author_detail": {
                    "name": "Afshin Dehghan"
                },
                "author": "Afshin Dehghan",
                "arxiv_comment": "Technical report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18943v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18943v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18942v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18942v1",
                "updated": "2025-03-24T17:59:04Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    17,
                    59,
                    4,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T17:59:04Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    17,
                    59,
                    4,
                    0,
                    83,
                    0
                ],
                "title": "Video-T1: Test-Time Scaling for Video Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video-T1: Test-Time Scaling for Video Generation"
                },
                "summary": "With the scale capability of increasing training data, model size, and\ncomputational cost, video generation has achieved impressive results in digital\ncreation, enabling users to express creativity across various domains.\nRecently, researchers in Large Language Models (LLMs) have expanded the scaling\nto test-time, which can significantly improve LLM performance by using more\ninference-time computation. Instead of scaling up video foundation models\nthrough expensive training costs, we explore the power of Test-Time Scaling\n(TTS) in video generation, aiming to answer the question: if a video generation\nmodel is allowed to use non-trivial amount of inference-time compute, how much\ncan it improve generation quality given a challenging text prompt. In this\nwork, we reinterpret the test-time scaling of video generation as a searching\nproblem to sample better trajectories from Gaussian noise space to the target\nvideo distribution. Specifically, we build the search space with test-time\nverifiers to provide feedback and heuristic algorithms to guide searching\nprocess. Given a text prompt, we first explore an intuitive linear search\nstrategy by increasing noise candidates at inference time. As full-step\ndenoising all frames simultaneously requires heavy test-time computation costs,\nwe further design a more efficient TTS method for video generation called\nTree-of-Frames (ToF) that adaptively expands and prunes video branches in an\nautoregressive manner. Extensive experiments on text-conditioned video\ngeneration benchmarks demonstrate that increasing test-time compute\nconsistently leads to significant improvements in the quality of videos.\nProject page: https://liuff19.github.io/Video-T1",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the scale capability of increasing training data, model size, and\ncomputational cost, video generation has achieved impressive results in digital\ncreation, enabling users to express creativity across various domains.\nRecently, researchers in Large Language Models (LLMs) have expanded the scaling\nto test-time, which can significantly improve LLM performance by using more\ninference-time computation. Instead of scaling up video foundation models\nthrough expensive training costs, we explore the power of Test-Time Scaling\n(TTS) in video generation, aiming to answer the question: if a video generation\nmodel is allowed to use non-trivial amount of inference-time compute, how much\ncan it improve generation quality given a challenging text prompt. In this\nwork, we reinterpret the test-time scaling of video generation as a searching\nproblem to sample better trajectories from Gaussian noise space to the target\nvideo distribution. Specifically, we build the search space with test-time\nverifiers to provide feedback and heuristic algorithms to guide searching\nprocess. Given a text prompt, we first explore an intuitive linear search\nstrategy by increasing noise candidates at inference time. As full-step\ndenoising all frames simultaneously requires heavy test-time computation costs,\nwe further design a more efficient TTS method for video generation called\nTree-of-Frames (ToF) that adaptively expands and prunes video branches in an\nautoregressive manner. Extensive experiments on text-conditioned video\ngeneration benchmarks demonstrate that increasing test-time compute\nconsistently leads to significant improvements in the quality of videos.\nProject page: https://liuff19.github.io/Video-T1"
                },
                "authors": [
                    {
                        "name": "Fangfu Liu"
                    },
                    {
                        "name": "Hanyang Wang"
                    },
                    {
                        "name": "Yimo Cai"
                    },
                    {
                        "name": "Kaiyan Zhang"
                    },
                    {
                        "name": "Xiaohang Zhan"
                    },
                    {
                        "name": "Yueqi Duan"
                    }
                ],
                "author_detail": {
                    "name": "Yueqi Duan"
                },
                "author": "Yueqi Duan",
                "arxiv_comment": "Project page: https://liuff19.github.io/Video-T1",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18942v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18942v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18941v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18941v1",
                "updated": "2025-03-24T17:59:03Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    17,
                    59,
                    3,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T17:59:03Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    17,
                    59,
                    3,
                    0,
                    83,
                    0
                ],
                "title": "Exploring Training and Inference Scaling Laws in Generative Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Training and Inference Scaling Laws in Generative Retrieval"
                },
                "summary": "Generative retrieval has emerged as a novel paradigm that leverages large\nlanguage models (LLMs) to autoregressively generate document identifiers.\nAlthough promising, the mechanisms that underpin its performance and\nscalability remain largely unclear. We conduct a systematic investigation of\ntraining and inference scaling laws in generative retrieval, exploring how\nmodel size, training data scale, and inference-time compute jointly influence\nretrieval performance. To address the lack of suitable metrics, we propose a\nnovel evaluation measure inspired by contrastive entropy and generation loss,\nproviding a continuous performance signal that enables robust comparisons\nacross diverse generative retrieval methods. Our experiments show that\nn-gram-based methods demonstrate strong alignment with both training and\ninference scaling laws, especially when paired with larger LLMs. Furthermore,\nincreasing inference computation yields substantial performance gains,\nrevealing that generative retrieval can significantly benefit from higher\ncompute budgets at inference. Across these settings, LLaMA models consistently\noutperform T5 models, suggesting a particular advantage for larger decoder-only\nmodels in generative retrieval. Taken together, our findings underscore that\nmodel sizes, data availability, and inference computation interact to unlock\nthe full potential of generative retrieval, offering new insights for designing\nand optimizing future systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative retrieval has emerged as a novel paradigm that leverages large\nlanguage models (LLMs) to autoregressively generate document identifiers.\nAlthough promising, the mechanisms that underpin its performance and\nscalability remain largely unclear. We conduct a systematic investigation of\ntraining and inference scaling laws in generative retrieval, exploring how\nmodel size, training data scale, and inference-time compute jointly influence\nretrieval performance. To address the lack of suitable metrics, we propose a\nnovel evaluation measure inspired by contrastive entropy and generation loss,\nproviding a continuous performance signal that enables robust comparisons\nacross diverse generative retrieval methods. Our experiments show that\nn-gram-based methods demonstrate strong alignment with both training and\ninference scaling laws, especially when paired with larger LLMs. Furthermore,\nincreasing inference computation yields substantial performance gains,\nrevealing that generative retrieval can significantly benefit from higher\ncompute budgets at inference. Across these settings, LLaMA models consistently\noutperform T5 models, suggesting a particular advantage for larger decoder-only\nmodels in generative retrieval. Taken together, our findings underscore that\nmodel sizes, data availability, and inference computation interact to unlock\nthe full potential of generative retrieval, offering new insights for designing\nand optimizing future systems."
                },
                "authors": [
                    {
                        "name": "Hongru Cai"
                    },
                    {
                        "name": "Yongqi Li"
                    },
                    {
                        "name": "Ruifeng Yuan"
                    },
                    {
                        "name": "Wenjie Wang"
                    },
                    {
                        "name": "Zhen Zhang"
                    },
                    {
                        "name": "Wenjie Li"
                    },
                    {
                        "name": "Tat-Seng Chua"
                    }
                ],
                "author_detail": {
                    "name": "Tat-Seng Chua"
                },
                "author": "Tat-Seng Chua",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18941v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18941v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18931v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18931v1",
                "updated": "2025-03-24T17:52:47Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    17,
                    52,
                    47,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T17:52:47Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    17,
                    52,
                    47,
                    0,
                    83,
                    0
                ],
                "title": "CoMP: Continual Multimodal Pre-training for Vision Foundation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoMP: Continual Multimodal Pre-training for Vision Foundation Models"
                },
                "summary": "Pre-trained Vision Foundation Models (VFMs) provide strong visual\nrepresentations for a wide range of applications. In this paper, we continually\npre-train prevailing VFMs in a multimodal manner such that they can\neffortlessly process visual inputs of varying sizes and produce visual\nrepresentations that are more aligned with language representations, regardless\nof their original pre-training process. To this end, we introduce CoMP, a\ncarefully designed multimodal pre-training pipeline. CoMP uses a Continual\nRotary Position Embedding to support native resolution continual pre-training,\nand an Alignment Loss between visual and textual features through language\nprototypes to align multimodal representations. By three-stage training, our\nVFMs achieve remarkable improvements not only in multimodal understanding but\nalso in other downstream tasks such as classification and segmentation.\nRemarkably, CoMP-SigLIP achieves scores of 66.7 on ChartQA and 75.9 on DocVQA\nwith a 0.5B LLM, while maintaining an 87.4% accuracy on ImageNet-1K and a 49.5\nmIoU on ADE20K under frozen chunk evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pre-trained Vision Foundation Models (VFMs) provide strong visual\nrepresentations for a wide range of applications. In this paper, we continually\npre-train prevailing VFMs in a multimodal manner such that they can\neffortlessly process visual inputs of varying sizes and produce visual\nrepresentations that are more aligned with language representations, regardless\nof their original pre-training process. To this end, we introduce CoMP, a\ncarefully designed multimodal pre-training pipeline. CoMP uses a Continual\nRotary Position Embedding to support native resolution continual pre-training,\nand an Alignment Loss between visual and textual features through language\nprototypes to align multimodal representations. By three-stage training, our\nVFMs achieve remarkable improvements not only in multimodal understanding but\nalso in other downstream tasks such as classification and segmentation.\nRemarkably, CoMP-SigLIP achieves scores of 66.7 on ChartQA and 75.9 on DocVQA\nwith a 0.5B LLM, while maintaining an 87.4% accuracy on ImageNet-1K and a 49.5\nmIoU on ADE20K under frozen chunk evaluation."
                },
                "authors": [
                    {
                        "name": "Yitong Chen"
                    },
                    {
                        "name": "Lingchen Meng"
                    },
                    {
                        "name": "Wujian Peng"
                    },
                    {
                        "name": "Zuxuan Wu"
                    },
                    {
                        "name": "Yu-Gang Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Yu-Gang Jiang"
                },
                "author": "Yu-Gang Jiang",
                "arxiv_comment": "Code is available in https://github.com/SliMM-X/CoMP-MM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18931v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18931v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17236v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17236v2",
                "updated": "2025-03-24T17:51:54Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    17,
                    51,
                    54,
                    0,
                    83,
                    0
                ],
                "published": "2024-10-22T17:54:45Z",
                "published_parsed": [
                    2024,
                    10,
                    22,
                    17,
                    54,
                    45,
                    1,
                    296,
                    0
                ],
                "title": "Large Language Models Empowered Personalized Web Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models Empowered Personalized Web Agents"
                },
                "summary": "Web agents have emerged as a promising direction to automate Web task\ncompletion based on user instructions, significantly enhancing user experience.\nRecently, Web agents have evolved from traditional agents to Large Language\nModels (LLMs)-based Web agents. Despite their success, existing LLM-based Web\nagents overlook the importance of personalized data (e.g., user profiles and\nhistorical Web behaviors) in assisting the understanding of users' personalized\ninstructions and executing customized actions. To overcome the limitation, we\nfirst formulate the task of LLM-empowered personalized Web agents, which\nintegrate personalized data and user instructions to personalize instruction\ncomprehension and action execution. To address the absence of a comprehensive\nevaluation benchmark, we construct a Personalized Web Agent Benchmark\n(PersonalWAB), featuring user instructions, personalized user data, Web\nfunctions, and two evaluation paradigms across three personalized Web tasks.\nMoreover, we propose a Personalized User Memory-enhanced Alignment (PUMA)\nframework to adapt LLMs to the personalized Web agent task. PUMA utilizes a\nmemory bank with a task-specific retrieval strategy to filter relevant\nhistorical Web behaviors. Based on the behaviors, PUMA then aligns LLMs for\npersonalized action execution through fine-tuning and direct preference\noptimization. Extensive experiments validate the superiority of PUMA over\nexisting Web agents on PersonalWAB.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Web agents have emerged as a promising direction to automate Web task\ncompletion based on user instructions, significantly enhancing user experience.\nRecently, Web agents have evolved from traditional agents to Large Language\nModels (LLMs)-based Web agents. Despite their success, existing LLM-based Web\nagents overlook the importance of personalized data (e.g., user profiles and\nhistorical Web behaviors) in assisting the understanding of users' personalized\ninstructions and executing customized actions. To overcome the limitation, we\nfirst formulate the task of LLM-empowered personalized Web agents, which\nintegrate personalized data and user instructions to personalize instruction\ncomprehension and action execution. To address the absence of a comprehensive\nevaluation benchmark, we construct a Personalized Web Agent Benchmark\n(PersonalWAB), featuring user instructions, personalized user data, Web\nfunctions, and two evaluation paradigms across three personalized Web tasks.\nMoreover, we propose a Personalized User Memory-enhanced Alignment (PUMA)\nframework to adapt LLMs to the personalized Web agent task. PUMA utilizes a\nmemory bank with a task-specific retrieval strategy to filter relevant\nhistorical Web behaviors. Based on the behaviors, PUMA then aligns LLMs for\npersonalized action execution through fine-tuning and direct preference\noptimization. Extensive experiments validate the superiority of PUMA over\nexisting Web agents on PersonalWAB."
                },
                "authors": [
                    {
                        "name": "Hongru Cai"
                    },
                    {
                        "name": "Yongqi Li"
                    },
                    {
                        "name": "Wenjie Wang"
                    },
                    {
                        "name": "Fengbin Zhu"
                    },
                    {
                        "name": "Xiaoyu Shen"
                    },
                    {
                        "name": "Wenjie Li"
                    },
                    {
                        "name": "Tat-Seng Chua"
                    }
                ],
                "author_detail": {
                    "name": "Tat-Seng Chua"
                },
                "author": "Tat-Seng Chua",
                "arxiv_comment": "Accepted to WWW 2025. The code and data are available on the project\n  website https://hongrucai.github.io/PersonalWAB/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17236v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17236v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18929v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18929v1",
                "updated": "2025-03-24T17:51:39Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    17,
                    51,
                    39,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T17:51:39Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    17,
                    51,
                    39,
                    0,
                    83,
                    0
                ],
                "title": "Trajectory Balance with Asynchrony: Decoupling Exploration and Learning\n  for Fast, Scalable LLM Post-Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trajectory Balance with Asynchrony: Decoupling Exploration and Learning\n  for Fast, Scalable LLM Post-Training"
                },
                "summary": "Reinforcement learning (RL) is a critical component of large language model\n(LLM) post-training. However, existing on-policy algorithms used for\npost-training are inherently incompatible with the use of experience replay\nbuffers, which can be populated scalably by distributed off-policy actors to\nenhance exploration as compute increases. We propose efficiently obtaining this\nbenefit of replay buffers via Trajectory Balance with Asynchrony (TBA), a\nmassively scalable LLM RL system. In contrast to existing approaches, TBA uses\na larger fraction of compute on search, constantly generating off-policy data\nfor a central replay buffer. A training node simultaneously samples data from\nthis buffer based on reward or recency to update the policy using Trajectory\nBalance (TB), a diversity-seeking RL objective introduced for GFlowNets. TBA\noffers three key advantages: (1) decoupled training and search, speeding up\ntraining wall-clock time by 4x or more; (2) improved diversity through\nlarge-scale off-policy sampling; and (3) scalable search for sparse reward\nsettings. On mathematical reasoning, preference-tuning, and automated\nred-teaming (diverse and representative post-training tasks), TBA produces\nspeed and performance improvements over strong baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning (RL) is a critical component of large language model\n(LLM) post-training. However, existing on-policy algorithms used for\npost-training are inherently incompatible with the use of experience replay\nbuffers, which can be populated scalably by distributed off-policy actors to\nenhance exploration as compute increases. We propose efficiently obtaining this\nbenefit of replay buffers via Trajectory Balance with Asynchrony (TBA), a\nmassively scalable LLM RL system. In contrast to existing approaches, TBA uses\na larger fraction of compute on search, constantly generating off-policy data\nfor a central replay buffer. A training node simultaneously samples data from\nthis buffer based on reward or recency to update the policy using Trajectory\nBalance (TB), a diversity-seeking RL objective introduced for GFlowNets. TBA\noffers three key advantages: (1) decoupled training and search, speeding up\ntraining wall-clock time by 4x or more; (2) improved diversity through\nlarge-scale off-policy sampling; and (3) scalable search for sparse reward\nsettings. On mathematical reasoning, preference-tuning, and automated\nred-teaming (diverse and representative post-training tasks), TBA produces\nspeed and performance improvements over strong baselines."
                },
                "authors": [
                    {
                        "name": "Brian R. Bartoldson"
                    },
                    {
                        "name": "Siddarth Venkatraman"
                    },
                    {
                        "name": "James Diffenderfer"
                    },
                    {
                        "name": "Moksh Jain"
                    },
                    {
                        "name": "Tal Ben-Nun"
                    },
                    {
                        "name": "Seanie Lee"
                    },
                    {
                        "name": "Minsu Kim"
                    },
                    {
                        "name": "Johan Obando-Ceron"
                    },
                    {
                        "name": "Yoshua Bengio"
                    },
                    {
                        "name": "Bhavya Kailkhura"
                    }
                ],
                "author_detail": {
                    "name": "Bhavya Kailkhura"
                },
                "author": "Bhavya Kailkhura",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18929v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18929v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18923v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18923v1",
                "updated": "2025-03-24T17:46:09Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    17,
                    46,
                    9,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T17:46:09Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    17,
                    46,
                    9,
                    0,
                    83,
                    0
                ],
                "title": "Video SimpleQA: Towards Factuality Evaluation in Large Video Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video SimpleQA: Towards Factuality Evaluation in Large Video Language\n  Models"
                },
                "summary": "Recent advancements in Large Video Language Models (LVLMs) have highlighted\ntheir potential for multi-modal understanding, yet evaluating their factual\ngrounding in video contexts remains a critical unsolved challenge. To address\nthis gap, we introduce Video SimpleQA, the first comprehensive benchmark\ntailored for factuality evaluation of LVLMs. Our work distinguishes from\nexisting video benchmarks through the following key features: 1) Knowledge\nrequired: demanding integration of external knowledge beyond the explicit\nnarrative; 2) Fact-seeking question: targeting objective, undisputed events or\nrelationships, avoiding subjective interpretation; 3) Definitive & short-form\nanswer: Answers are crafted as unambiguous and definitively correct in a short\nformat, enabling automated evaluation through LLM-as-a-judge frameworks with\nminimal scoring variance; 4) External-source verified: All annotations undergo\nrigorous validation against authoritative external references to ensure the\nreliability; 5) Temporal reasoning required: The annotated question types\nencompass both static single-frame understanding and dynamic temporal\nreasoning, explicitly evaluating LVLMs factuality under the long-context\ndependencies. We extensively evaluate 41 state-of-the-art LVLMs and summarize\nkey findings as follows: 1) Current LVLMs exhibit notable deficiencies in\nfactual adherence, particularly for open-source models. The best-performing\nmodel Gemini-1.5-Pro achieves merely an F-score of 54.4%; 2) Test-time compute\nparadigms show insignificant performance gains, revealing fundamental\nconstraints for enhancing factuality through post-hoc computation; 3)\nRetrieval-Augmented Generation demonstrates consistent improvements at the cost\nof additional inference time overhead, presenting a critical\nefficiency-performance trade-off.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Video Language Models (LVLMs) have highlighted\ntheir potential for multi-modal understanding, yet evaluating their factual\ngrounding in video contexts remains a critical unsolved challenge. To address\nthis gap, we introduce Video SimpleQA, the first comprehensive benchmark\ntailored for factuality evaluation of LVLMs. Our work distinguishes from\nexisting video benchmarks through the following key features: 1) Knowledge\nrequired: demanding integration of external knowledge beyond the explicit\nnarrative; 2) Fact-seeking question: targeting objective, undisputed events or\nrelationships, avoiding subjective interpretation; 3) Definitive & short-form\nanswer: Answers are crafted as unambiguous and definitively correct in a short\nformat, enabling automated evaluation through LLM-as-a-judge frameworks with\nminimal scoring variance; 4) External-source verified: All annotations undergo\nrigorous validation against authoritative external references to ensure the\nreliability; 5) Temporal reasoning required: The annotated question types\nencompass both static single-frame understanding and dynamic temporal\nreasoning, explicitly evaluating LVLMs factuality under the long-context\ndependencies. We extensively evaluate 41 state-of-the-art LVLMs and summarize\nkey findings as follows: 1) Current LVLMs exhibit notable deficiencies in\nfactual adherence, particularly for open-source models. The best-performing\nmodel Gemini-1.5-Pro achieves merely an F-score of 54.4%; 2) Test-time compute\nparadigms show insignificant performance gains, revealing fundamental\nconstraints for enhancing factuality through post-hoc computation; 3)\nRetrieval-Augmented Generation demonstrates consistent improvements at the cost\nof additional inference time overhead, presenting a critical\nefficiency-performance trade-off."
                },
                "authors": [
                    {
                        "name": "Meng Cao"
                    },
                    {
                        "name": "Pengfei Hu"
                    },
                    {
                        "name": "Yingyao Wang"
                    },
                    {
                        "name": "Jihao Gu"
                    },
                    {
                        "name": "Haoran Tang"
                    },
                    {
                        "name": "Haoze Zhao"
                    },
                    {
                        "name": "Jiahua Dong"
                    },
                    {
                        "name": "Wangbo Yu"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Ian Reid"
                    },
                    {
                        "name": "Xiaodan Liang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaodan Liang"
                },
                "author": "Xiaodan Liang",
                "arxiv_comment": "24 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18923v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18923v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18915v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18915v1",
                "updated": "2025-03-24T17:29:04Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    17,
                    29,
                    4,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T17:29:04Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    17,
                    29,
                    4,
                    0,
                    83,
                    0
                ],
                "title": "Signal Propagation in RIS-Aided 5G Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Signal Propagation in RIS-Aided 5G Systems"
                },
                "summary": "In this paper, we conduct an in-depth analysis of radio signal propagation\ncharacteristics within the urban environment of Poznan (Poland). The study\nspecifically addresses the deployment of a 5th generation (5G NR - New Radio)\nRadio Access Network (RAN), which comprises 8 strategically positioned Base\nStations (BSs). These base stations are configured with either Single Input\nSingle Output (SISO) or Multiple Input Multiple Output (MIMO) antenna\ntechnologies, contingent upon the specific requirements of the network cells\nthey serve. A key focus of our research is the integration of 15 reflecting\narrays, known as Reconfigurable Intelligent Surfaces (RISs), which were\ninstalled throughout the study area. These RISs were deployed at various\nsuspension heights to evaluate their impact on radio signal propagation and\ncoverage. By exploring the influence of these RIS matrices, our research sheds\nlight on their potential to significantly enhance signal quality, particularly\nin urban environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we conduct an in-depth analysis of radio signal propagation\ncharacteristics within the urban environment of Poznan (Poland). The study\nspecifically addresses the deployment of a 5th generation (5G NR - New Radio)\nRadio Access Network (RAN), which comprises 8 strategically positioned Base\nStations (BSs). These base stations are configured with either Single Input\nSingle Output (SISO) or Multiple Input Multiple Output (MIMO) antenna\ntechnologies, contingent upon the specific requirements of the network cells\nthey serve. A key focus of our research is the integration of 15 reflecting\narrays, known as Reconfigurable Intelligent Surfaces (RISs), which were\ninstalled throughout the study area. These RISs were deployed at various\nsuspension heights to evaluate their impact on radio signal propagation and\ncoverage. By exploring the influence of these RIS matrices, our research sheds\nlight on their potential to significantly enhance signal quality, particularly\nin urban environments."
                },
                "authors": [
                    {
                        "name": "Adam Samorzewski"
                    },
                    {
                        "name": "Adrian Kliks"
                    }
                ],
                "author_detail": {
                    "name": "Adrian Kliks"
                },
                "author": "Adrian Kliks",
                "arxiv_doi": "10.1109/WiMob61911.2024.10770363",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/WiMob61911.2024.10770363",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.18915v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18915v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "20th International Conference on Wireless and Mobile Computing,\n  Networking and Communications (WiMob 2024), Paris, France, 2024, pp. 443-448",
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18906v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18906v1",
                "updated": "2025-03-24T17:18:02Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    17,
                    18,
                    2,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T17:18:02Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    17,
                    18,
                    2,
                    0,
                    83,
                    0
                ],
                "title": "Entanglement swapping systems toward a quantum internet",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Entanglement swapping systems toward a quantum internet"
                },
                "summary": "We demonstrate conditional entanglement swapping, i.e. teleportation of\nentanglement, between time-bin qubits at the telecommunication wavelength of\n1536.4 nm with high fidelity of 87\\%. Our system is deployable, utilizing\nmodular, off-the-shelf, fiber-coupled, and electrically controlled components\nsuch as electro-optic modulators. It leverages the precise timing resolution of\nsuperconducting nanowire detectors, which are controlled and read out via a\ncustom developed graphical user interface. The swapping process is described,\ninterpreted, and guided using characteristic function-based analytical modeling\nthat accounts for realistic imperfections. Our system supports quantum\nnetworking protocols, including source-independent quantum key distribution,\nwith an estimated secret key rate of approximately 0.5 bits per sifted bit.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We demonstrate conditional entanglement swapping, i.e. teleportation of\nentanglement, between time-bin qubits at the telecommunication wavelength of\n1536.4 nm with high fidelity of 87\\%. Our system is deployable, utilizing\nmodular, off-the-shelf, fiber-coupled, and electrically controlled components\nsuch as electro-optic modulators. It leverages the precise timing resolution of\nsuperconducting nanowire detectors, which are controlled and read out via a\ncustom developed graphical user interface. The swapping process is described,\ninterpreted, and guided using characteristic function-based analytical modeling\nthat accounts for realistic imperfections. Our system supports quantum\nnetworking protocols, including source-independent quantum key distribution,\nwith an estimated secret key rate of approximately 0.5 bits per sifted bit."
                },
                "authors": [
                    {
                        "name": "Samantha I. Davis"
                    },
                    {
                        "name": "Raju Valivarthi"
                    },
                    {
                        "name": "Andrew Cameron"
                    },
                    {
                        "name": "Cristian Pena"
                    },
                    {
                        "name": "Si Xie"
                    },
                    {
                        "name": "Lautaro Narvaez"
                    },
                    {
                        "name": "Nikolai Lauk"
                    },
                    {
                        "name": "Chang Li"
                    },
                    {
                        "name": "Kelsie Taylor"
                    },
                    {
                        "name": "Rahaf Youssef"
                    },
                    {
                        "name": "Christina Wang"
                    },
                    {
                        "name": "Keshav Kapoor"
                    },
                    {
                        "name": "Boris Korzh"
                    },
                    {
                        "name": "Neil Sinclair"
                    },
                    {
                        "name": "Matthew Shaw"
                    },
                    {
                        "name": "Panagiotis Spentzouris"
                    },
                    {
                        "name": "Maria Spiropulu"
                    }
                ],
                "author_detail": {
                    "name": "Maria Spiropulu"
                },
                "author": "Maria Spiropulu",
                "arxiv_comment": "17 pages, 11 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18906v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18906v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18893v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18893v1",
                "updated": "2025-03-24T17:06:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    17,
                    6,
                    37,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T17:06:37Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    17,
                    6,
                    37,
                    0,
                    83,
                    0
                ],
                "title": "xKV: Cross-Layer SVD for KV-Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "xKV: Cross-Layer SVD for KV-Cache Compression"
                },
                "summary": "Large Language Models (LLMs) with long context windows enable powerful\napplications but come at the cost of high memory consumption to store the Key\nand Value states (KV-Cache). Recent studies attempted to merge KV-cache from\nmultiple layers into shared representations, yet these approaches either\nrequire expensive pretraining or rely on assumptions of high per-token cosine\nsimilarity across layers which generally does not hold in practice. We find\nthat the dominant singular vectors are remarkably well-aligned across multiple\nlayers of the KV-Cache. Exploiting this insight, we propose xKV, a simple\npost-training method that applies Singular Value Decomposition (SVD) on the\nKV-Cache of grouped layers. xKV consolidates the KV-Cache of multiple layers\ninto a shared low-rank subspace, significantly reducing KV-Cache sizes. Through\nextensive evaluations on the RULER long-context benchmark with widely-used LLMs\n(e.g., Llama-3.1 and Qwen2.5), xKV achieves up to 6.8x higher compression rates\nthan state-of-the-art inter-layer technique while improving accuracy by 2.7%.\nMoreover, xKV is compatible with the emerging Multi-Head Latent Attention (MLA)\n(e.g., DeepSeek-Coder-V2), yielding a notable 3x compression rates on coding\ntasks without performance degradation. These results highlight xKV's strong\ncapability and versatility in addressing memory bottlenecks for long-context\nLLM inference. Our code is publicly available at:\nhttps://github.com/abdelfattah-lab/xKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) with long context windows enable powerful\napplications but come at the cost of high memory consumption to store the Key\nand Value states (KV-Cache). Recent studies attempted to merge KV-cache from\nmultiple layers into shared representations, yet these approaches either\nrequire expensive pretraining or rely on assumptions of high per-token cosine\nsimilarity across layers which generally does not hold in practice. We find\nthat the dominant singular vectors are remarkably well-aligned across multiple\nlayers of the KV-Cache. Exploiting this insight, we propose xKV, a simple\npost-training method that applies Singular Value Decomposition (SVD) on the\nKV-Cache of grouped layers. xKV consolidates the KV-Cache of multiple layers\ninto a shared low-rank subspace, significantly reducing KV-Cache sizes. Through\nextensive evaluations on the RULER long-context benchmark with widely-used LLMs\n(e.g., Llama-3.1 and Qwen2.5), xKV achieves up to 6.8x higher compression rates\nthan state-of-the-art inter-layer technique while improving accuracy by 2.7%.\nMoreover, xKV is compatible with the emerging Multi-Head Latent Attention (MLA)\n(e.g., DeepSeek-Coder-V2), yielding a notable 3x compression rates on coding\ntasks without performance degradation. These results highlight xKV's strong\ncapability and versatility in addressing memory bottlenecks for long-context\nLLM inference. Our code is publicly available at:\nhttps://github.com/abdelfattah-lab/xKV."
                },
                "authors": [
                    {
                        "name": "Chi-Chih Chang"
                    },
                    {
                        "name": "Chien-Yu Lin"
                    },
                    {
                        "name": "Yash Akhauri"
                    },
                    {
                        "name": "Wei-Cheng Lin"
                    },
                    {
                        "name": "Kai-Chiang Wu"
                    },
                    {
                        "name": "Luis Ceze"
                    },
                    {
                        "name": "Mohamed S. Abdelfattah"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed S. Abdelfattah"
                },
                "author": "Mohamed S. Abdelfattah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18893v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18893v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18891v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18891v1",
                "updated": "2025-03-24T17:04:55Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    17,
                    4,
                    55,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T17:04:55Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    17,
                    4,
                    55,
                    0,
                    83,
                    0
                ],
                "title": "AgentDropout: Dynamic Agent Elimination for Token-Efficient and\n  High-Performance LLM-Based Multi-Agent Collaboration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AgentDropout: Dynamic Agent Elimination for Token-Efficient and\n  High-Performance LLM-Based Multi-Agent Collaboration"
                },
                "summary": "Multi-agent systems (MAS) based on large language models (LLMs) have\ndemonstrated significant potential in collaborative problem-solving. However,\nthey still face substantial challenges of low communication efficiency and\nsuboptimal task performance, making the careful design of the agents'\ncommunication topologies particularly important. Inspired by the management\ntheory that roles in an efficient team are often dynamically adjusted, we\npropose AgentDropout, which identifies redundant agents and communication\nacross different communication rounds by optimizing the adjacency matrices of\nthe communication graphs and eliminates them to enhance both token efficiency\nand task performance. Compared to state-of-the-art methods, AgentDropout\nachieves an average reduction of 21.6% in prompt token consumption and 18.4% in\ncompletion token consumption, along with a performance improvement of 1.14 on\nthe tasks. Furthermore, the extended experiments demonstrate that AgentDropout\nachieves notable domain transferability and structure robustness, revealing its\nreliability and effectiveness. We release our code at\nhttps://github.com/wangzx1219/AgentDropout.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-agent systems (MAS) based on large language models (LLMs) have\ndemonstrated significant potential in collaborative problem-solving. However,\nthey still face substantial challenges of low communication efficiency and\nsuboptimal task performance, making the careful design of the agents'\ncommunication topologies particularly important. Inspired by the management\ntheory that roles in an efficient team are often dynamically adjusted, we\npropose AgentDropout, which identifies redundant agents and communication\nacross different communication rounds by optimizing the adjacency matrices of\nthe communication graphs and eliminates them to enhance both token efficiency\nand task performance. Compared to state-of-the-art methods, AgentDropout\nachieves an average reduction of 21.6% in prompt token consumption and 18.4% in\ncompletion token consumption, along with a performance improvement of 1.14 on\nthe tasks. Furthermore, the extended experiments demonstrate that AgentDropout\nachieves notable domain transferability and structure robustness, revealing its\nreliability and effectiveness. We release our code at\nhttps://github.com/wangzx1219/AgentDropout."
                },
                "authors": [
                    {
                        "name": "Zhexuan Wang"
                    },
                    {
                        "name": "Yutong Wang"
                    },
                    {
                        "name": "Xuebo Liu"
                    },
                    {
                        "name": "Liang Ding"
                    },
                    {
                        "name": "Miao Zhang"
                    },
                    {
                        "name": "Jie Liu"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18891v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18891v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18883v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18883v1",
                "updated": "2025-03-24T16:58:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    16,
                    58,
                    37,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T16:58:37Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    16,
                    58,
                    37,
                    0,
                    83,
                    0
                ],
                "title": "Efficient and Accurate Scene Text Recognition with Cascaded-Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient and Accurate Scene Text Recognition with Cascaded-Transformers"
                },
                "summary": "In recent years, vision transformers with text decoder have demonstrated\nremarkable performance on Scene Text Recognition (STR) due to their ability to\ncapture long-range dependencies and contextual relationships with high learning\ncapacity. However, the computational and memory demands of these models are\nsignificant, limiting their deployment in resource-constrained applications. To\naddress this challenge, we propose an efficient and accurate STR system.\nSpecifically, we focus on improving the efficiency of encoder models by\nintroducing a cascaded-transformers structure. This structure progressively\nreduces the vision token size during the encoding step, effectively eliminating\nredundant tokens and reducing computational cost. Our experimental results\nconfirm that our STR system achieves comparable performance to state-of-the-art\nbaselines while substantially decreasing computational requirements. In\nparticular, for large-models, the accuracy remains same, 92.77 to 92.68, while\ncomputational complexity is almost halved with our structure.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, vision transformers with text decoder have demonstrated\nremarkable performance on Scene Text Recognition (STR) due to their ability to\ncapture long-range dependencies and contextual relationships with high learning\ncapacity. However, the computational and memory demands of these models are\nsignificant, limiting their deployment in resource-constrained applications. To\naddress this challenge, we propose an efficient and accurate STR system.\nSpecifically, we focus on improving the efficiency of encoder models by\nintroducing a cascaded-transformers structure. This structure progressively\nreduces the vision token size during the encoding step, effectively eliminating\nredundant tokens and reducing computational cost. Our experimental results\nconfirm that our STR system achieves comparable performance to state-of-the-art\nbaselines while substantially decreasing computational requirements. In\nparticular, for large-models, the accuracy remains same, 92.77 to 92.68, while\ncomputational complexity is almost halved with our structure."
                },
                "authors": [
                    {
                        "name": "Savas Ozkan"
                    },
                    {
                        "name": "Andrea Maracani"
                    },
                    {
                        "name": "Hyowon Kim"
                    },
                    {
                        "name": "Sijun Cho"
                    },
                    {
                        "name": "Eunchung Noh"
                    },
                    {
                        "name": "Jeongwon Min"
                    },
                    {
                        "name": "Jung Min Cho"
                    },
                    {
                        "name": "Mete Ozay"
                    }
                ],
                "author_detail": {
                    "name": "Mete Ozay"
                },
                "author": "Mete Ozay",
                "arxiv_comment": "Accepted to ACM-MMSys2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18883v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18883v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18878v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18878v1",
                "updated": "2025-03-24T16:54:26Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    16,
                    54,
                    26,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T16:54:26Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    16,
                    54,
                    26,
                    0,
                    83,
                    0
                ],
                "title": "I Have Covered All the Bases Here: Interpreting Reasoning Features in\n  Large Language Models via Sparse Autoencoders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "I Have Covered All the Bases Here: Interpreting Reasoning Features in\n  Large Language Models via Sparse Autoencoders"
                },
                "summary": "Large Language Models (LLMs) have achieved remarkable success in natural\nlanguage processing. Recent advances have led to the developing of a new class\nof reasoning LLMs; for example, open-source DeepSeek-R1 has achieved\nstate-of-the-art performance by integrating deep thinking and complex\nreasoning. Despite these impressive capabilities, the internal reasoning\nmechanisms of such models remain unexplored. In this work, we employ Sparse\nAutoencoders (SAEs), a method to learn a sparse decomposition of latent\nrepresentations of a neural network into interpretable features, to identify\nfeatures that drive reasoning in the DeepSeek-R1 series of models. First, we\npropose an approach to extract candidate ''reasoning features'' from SAE\nrepresentations. We validate these features through empirical analysis and\ninterpretability methods, demonstrating their direct correlation with the\nmodel's reasoning abilities. Crucially, we demonstrate that steering these\nfeatures systematically enhances reasoning performance, offering the first\nmechanistic account of reasoning in LLMs. Code available at\nhttps://github.com/AIRI-Institute/SAE-Reasoning",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved remarkable success in natural\nlanguage processing. Recent advances have led to the developing of a new class\nof reasoning LLMs; for example, open-source DeepSeek-R1 has achieved\nstate-of-the-art performance by integrating deep thinking and complex\nreasoning. Despite these impressive capabilities, the internal reasoning\nmechanisms of such models remain unexplored. In this work, we employ Sparse\nAutoencoders (SAEs), a method to learn a sparse decomposition of latent\nrepresentations of a neural network into interpretable features, to identify\nfeatures that drive reasoning in the DeepSeek-R1 series of models. First, we\npropose an approach to extract candidate ''reasoning features'' from SAE\nrepresentations. We validate these features through empirical analysis and\ninterpretability methods, demonstrating their direct correlation with the\nmodel's reasoning abilities. Crucially, we demonstrate that steering these\nfeatures systematically enhances reasoning performance, offering the first\nmechanistic account of reasoning in LLMs. Code available at\nhttps://github.com/AIRI-Institute/SAE-Reasoning"
                },
                "authors": [
                    {
                        "name": "Andrey Galichin"
                    },
                    {
                        "name": "Alexey Dontsov"
                    },
                    {
                        "name": "Polina Druzhinina"
                    },
                    {
                        "name": "Anton Razzhigaev"
                    },
                    {
                        "name": "Oleg Y. Rogov"
                    },
                    {
                        "name": "Elena Tutubalina"
                    },
                    {
                        "name": "Ivan Oseledets"
                    }
                ],
                "author_detail": {
                    "name": "Ivan Oseledets"
                },
                "author": "Ivan Oseledets",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18878v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18878v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18869v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18869v1",
                "updated": "2025-03-24T16:44:32Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    16,
                    44,
                    32,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T16:44:32Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    16,
                    44,
                    32,
                    0,
                    83,
                    0
                ],
                "title": "Reimagining Memory Access for LLM Inference: Compression-Aware Memory\n  Controller Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reimagining Memory Access for LLM Inference: Compression-Aware Memory\n  Controller Design"
                },
                "summary": "The efficiency of Large Language Model~(LLM) inference is often constrained\nby substantial memory bandwidth and capacity demands. Existing techniques, such\nas pruning, quantization, and mixture of experts/depth, reduce memory capacity\nand/or bandwidth consumption at the cost of slight degradation in inference\nquality. This paper introduces a design solution that further alleviates memory\nbottlenecks by enhancing the on-chip memory controller in AI accelerators to\nachieve two main objectives: (1) significantly reducing memory capacity and\nbandwidth usage through lossless block compression~(e.g., LZ4 and ZSTD) of\nmodel weights and key-value (KV) cache without compromising inference quality,\nand (2) enabling memory bandwidth and energy consumption to scale\nproportionally with context-dependent dynamic quantization. These goals are\naccomplished by equipping the on-chip memory controller with mechanisms to\nimprove fine-grained bit-level accessibility and compressibility of weights and\nKV cache through LLM-aware configuration of in-memory placement and\nrepresentation. Experimental results on publicly available LLMs demonstrate the\neffectiveness of this approach, showing memory footprint reductions of 25.2\\%\nfor model weights and 46.9\\% for KV cache. In addition, our hardware prototype\nat 4\\,GHz and 32 lanes (7\\,nm) achieves 8\\,TB/s throughput with a modest area\noverhead (under 3.8\\,mm\\(^2\\)), which underscores the viability of LLM-aware\nmemory control as a key to efficient large-scale inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The efficiency of Large Language Model~(LLM) inference is often constrained\nby substantial memory bandwidth and capacity demands. Existing techniques, such\nas pruning, quantization, and mixture of experts/depth, reduce memory capacity\nand/or bandwidth consumption at the cost of slight degradation in inference\nquality. This paper introduces a design solution that further alleviates memory\nbottlenecks by enhancing the on-chip memory controller in AI accelerators to\nachieve two main objectives: (1) significantly reducing memory capacity and\nbandwidth usage through lossless block compression~(e.g., LZ4 and ZSTD) of\nmodel weights and key-value (KV) cache without compromising inference quality,\nand (2) enabling memory bandwidth and energy consumption to scale\nproportionally with context-dependent dynamic quantization. These goals are\naccomplished by equipping the on-chip memory controller with mechanisms to\nimprove fine-grained bit-level accessibility and compressibility of weights and\nKV cache through LLM-aware configuration of in-memory placement and\nrepresentation. Experimental results on publicly available LLMs demonstrate the\neffectiveness of this approach, showing memory footprint reductions of 25.2\\%\nfor model weights and 46.9\\% for KV cache. In addition, our hardware prototype\nat 4\\,GHz and 32 lanes (7\\,nm) achieves 8\\,TB/s throughput with a modest area\noverhead (under 3.8\\,mm\\(^2\\)), which underscores the viability of LLM-aware\nmemory control as a key to efficient large-scale inference."
                },
                "authors": [
                    {
                        "name": "Rui Xie"
                    },
                    {
                        "name": "Asad Ul Haq"
                    },
                    {
                        "name": "Linsen Ma"
                    },
                    {
                        "name": "Yunhua Fang"
                    },
                    {
                        "name": "Zirak Burzin Engineer"
                    },
                    {
                        "name": "Liu Liu"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "author": "Tong Zhang",
                "arxiv_comment": "9 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18869v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18869v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18865v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18865v1",
                "updated": "2025-03-24T16:41:17Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    16,
                    41,
                    17,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T16:41:17Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    16,
                    41,
                    17,
                    0,
                    83,
                    0
                ],
                "title": "Structuring Scientific Innovation: A Framework for Modeling and\n  Discovering Impactful Knowledge Combinations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Structuring Scientific Innovation: A Framework for Modeling and\n  Discovering Impactful Knowledge Combinations"
                },
                "summary": "The emergence of large language models offers new possibilities for\nstructured exploration of scientific knowledge. Rather than viewing scientific\ndiscovery as isolated ideas or content, we propose a structured approach that\nemphasizes the role of method combinations in shaping disruptive insights.\nSpecifically, we investigate how knowledge unit--especially those tied to\nmethodological design--can be modeled and recombined to yield research\nbreakthroughs.Our proposed framework addresses two key challenges. First, we\nintroduce a contrastive learning-based mechanism to identify distinguishing\nfeatures of historically disruptive method combinations within problem-driven\ncontexts.Second, we propose a reasoning-guided Monte Carlo search algorithm\nthat leverages the chain-of-thought capability of LLMs to identify promising\nknowledge recombinations for new problem statements.Empirical studies across\nmultiple domains show that the framework is capable of modeling the structural\ndynamics of innovation and successfully highlights combinations with high\ndisruptive potential.This research provides a new path for computationally\nguided scientific ideation grounded in structured reasoning and historical data\nmodeling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of large language models offers new possibilities for\nstructured exploration of scientific knowledge. Rather than viewing scientific\ndiscovery as isolated ideas or content, we propose a structured approach that\nemphasizes the role of method combinations in shaping disruptive insights.\nSpecifically, we investigate how knowledge unit--especially those tied to\nmethodological design--can be modeled and recombined to yield research\nbreakthroughs.Our proposed framework addresses two key challenges. First, we\nintroduce a contrastive learning-based mechanism to identify distinguishing\nfeatures of historically disruptive method combinations within problem-driven\ncontexts.Second, we propose a reasoning-guided Monte Carlo search algorithm\nthat leverages the chain-of-thought capability of LLMs to identify promising\nknowledge recombinations for new problem statements.Empirical studies across\nmultiple domains show that the framework is capable of modeling the structural\ndynamics of innovation and successfully highlights combinations with high\ndisruptive potential.This research provides a new path for computationally\nguided scientific ideation grounded in structured reasoning and historical data\nmodeling."
                },
                "authors": [
                    {
                        "name": "Junlan Chen"
                    },
                    {
                        "name": "Kexin Zhang"
                    },
                    {
                        "name": "Daifeng Li"
                    },
                    {
                        "name": "Yangyang Feng"
                    },
                    {
                        "name": "Yuxuan Zhang"
                    },
                    {
                        "name": "Bowen Deng"
                    }
                ],
                "author_detail": {
                    "name": "Bowen Deng"
                },
                "author": "Bowen Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18865v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18865v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18825v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18825v1",
                "updated": "2025-03-24T16:06:04Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    16,
                    6,
                    4,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T16:06:04Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    16,
                    6,
                    4,
                    0,
                    83,
                    0
                ],
                "title": "EconEvals: Benchmarks and Litmus Tests for LLM Agents in Unknown\n  Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EconEvals: Benchmarks and Litmus Tests for LLM Agents in Unknown\n  Environments"
                },
                "summary": "We develop benchmarks for LLM agents that act in, learn from, and strategize\nin unknown environments, the specifications of which the LLM agent must learn\nover time from deliberate exploration. Our benchmarks consist of\ndecision-making tasks derived from key problems in economics. To forestall\nsaturation, the benchmark tasks are synthetically generated with scalable\ndifficulty levels. Additionally, we propose litmus tests, a new kind of\nquantitative measure for LLMs and LLM agents. Unlike benchmarks, litmus tests\nquantify differences in character, values, and tendencies of LLMs and LLM\nagents, by considering their behavior when faced with tradeoffs (e.g.,\nefficiency versus equality) where there is no objectively right or wrong\nbehavior. Overall, our benchmarks and litmus tests assess the abilities and\ntendencies of LLM agents in tackling complex economic problems in diverse\nsettings spanning procurement, scheduling, task allocation, and pricing --\napplications that should grow in importance as such agents are further\nintegrated into the economy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We develop benchmarks for LLM agents that act in, learn from, and strategize\nin unknown environments, the specifications of which the LLM agent must learn\nover time from deliberate exploration. Our benchmarks consist of\ndecision-making tasks derived from key problems in economics. To forestall\nsaturation, the benchmark tasks are synthetically generated with scalable\ndifficulty levels. Additionally, we propose litmus tests, a new kind of\nquantitative measure for LLMs and LLM agents. Unlike benchmarks, litmus tests\nquantify differences in character, values, and tendencies of LLMs and LLM\nagents, by considering their behavior when faced with tradeoffs (e.g.,\nefficiency versus equality) where there is no objectively right or wrong\nbehavior. Overall, our benchmarks and litmus tests assess the abilities and\ntendencies of LLM agents in tackling complex economic problems in diverse\nsettings spanning procurement, scheduling, task allocation, and pricing --\napplications that should grow in importance as such agents are further\nintegrated into the economy."
                },
                "authors": [
                    {
                        "name": "Sara Fish"
                    },
                    {
                        "name": "Julia Shephard"
                    },
                    {
                        "name": "Minkai Li"
                    },
                    {
                        "name": "Ran I. Shorrer"
                    },
                    {
                        "name": "Yannai A. Gonczarowski"
                    }
                ],
                "author_detail": {
                    "name": "Yannai A. Gonczarowski"
                },
                "author": "Yannai A. Gonczarowski",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18825v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18825v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18813v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18813v1",
                "updated": "2025-03-24T15:54:10Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    15,
                    54,
                    10,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T15:54:10Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    15,
                    54,
                    10,
                    0,
                    83,
                    0
                ],
                "title": "Defeating Prompt Injections by Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Defeating Prompt Injections by Design"
                },
                "summary": "Large Language Models (LLMs) are increasingly deployed in agentic systems\nthat interact with an external environment. However, LLM agents are vulnerable\nto prompt injection attacks when handling untrusted data. In this paper we\npropose CaMeL, a robust defense that creates a protective system layer around\nthe LLM, securing it even when underlying models may be susceptible to attacks.\nTo operate, CaMeL explicitly extracts the control and data flows from the\n(trusted) query; therefore, the untrusted data retrieved by the LLM can never\nimpact the program flow. To further improve security, CaMeL relies on a notion\nof a capability to prevent the exfiltration of private data over unauthorized\ndata flows. We demonstrate effectiveness of CaMeL by solving $67\\%$ of tasks\nwith provable security in AgentDojo [NeurIPS 2024], a recent agentic security\nbenchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly deployed in agentic systems\nthat interact with an external environment. However, LLM agents are vulnerable\nto prompt injection attacks when handling untrusted data. In this paper we\npropose CaMeL, a robust defense that creates a protective system layer around\nthe LLM, securing it even when underlying models may be susceptible to attacks.\nTo operate, CaMeL explicitly extracts the control and data flows from the\n(trusted) query; therefore, the untrusted data retrieved by the LLM can never\nimpact the program flow. To further improve security, CaMeL relies on a notion\nof a capability to prevent the exfiltration of private data over unauthorized\ndata flows. We demonstrate effectiveness of CaMeL by solving $67\\%$ of tasks\nwith provable security in AgentDojo [NeurIPS 2024], a recent agentic security\nbenchmark."
                },
                "authors": [
                    {
                        "name": "Edoardo Debenedetti"
                    },
                    {
                        "name": "Ilia Shumailov"
                    },
                    {
                        "name": "Tianqi Fan"
                    },
                    {
                        "name": "Jamie Hayes"
                    },
                    {
                        "name": "Nicholas Carlini"
                    },
                    {
                        "name": "Daniel Fabian"
                    },
                    {
                        "name": "Christoph Kern"
                    },
                    {
                        "name": "Chongyang Shi"
                    },
                    {
                        "name": "Andreas Terzis"
                    },
                    {
                        "name": "Florian Tramr"
                    }
                ],
                "author_detail": {
                    "name": "Florian Tramr"
                },
                "author": "Florian Tramr",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18813v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18813v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18809v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18809v1",
                "updated": "2025-03-24T15:50:20Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    15,
                    50,
                    20,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T15:50:20Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    15,
                    50,
                    20,
                    0,
                    83,
                    0
                ],
                "title": "Classical Planning with LLM-Generated Heuristics: Challenging the State\n  of the Art with Python Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Classical Planning with LLM-Generated Heuristics: Challenging the State\n  of the Art with Python Code"
                },
                "summary": "In recent years, large language models (LLMs) have shown remarkable\ncapabilities in various artificial intelligence problems. However, they fail to\nplan reliably, even when prompted with a detailed definition of the planning\ntask. Attempts to improve their planning capabilities, such as chain-of-thought\nprompting, fine-tuning, and explicit \"reasoning\" still yield incorrect plans\nand usually fail to generalize to larger tasks. In this paper, we show how to\nuse LLMs to generate correct plans, even for out-of-distribution tasks of\nincreasing size. For a given planning domain, we ask an LLM to generate several\ndomain-dependent heuristic functions in the form of Python code, evaluate them\non a set of training tasks within a greedy best-first search, and choose the\nstrongest one. The resulting LLM-generated heuristics solve many more unseen\ntest tasks than state-of-the-art domain-independent heuristics for classical\nplanning. They are even competitive with the strongest learning algorithm for\ndomain-dependent planning. These findings are especially remarkable given that\nour proof-of-concept implementation is based on an unoptimized Python planner\nand the baselines all build upon highly optimized C++ code. In some domains,\nthe LLM-generated heuristics expand fewer states than the baselines, revealing\nthat they are not only efficiently computable, but sometimes even more\ninformative than the state-of-the-art heuristics. Overall, our results show\nthat sampling a set of planning heuristic function programs can significantly\nimprove the planning capabilities of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, large language models (LLMs) have shown remarkable\ncapabilities in various artificial intelligence problems. However, they fail to\nplan reliably, even when prompted with a detailed definition of the planning\ntask. Attempts to improve their planning capabilities, such as chain-of-thought\nprompting, fine-tuning, and explicit \"reasoning\" still yield incorrect plans\nand usually fail to generalize to larger tasks. In this paper, we show how to\nuse LLMs to generate correct plans, even for out-of-distribution tasks of\nincreasing size. For a given planning domain, we ask an LLM to generate several\ndomain-dependent heuristic functions in the form of Python code, evaluate them\non a set of training tasks within a greedy best-first search, and choose the\nstrongest one. The resulting LLM-generated heuristics solve many more unseen\ntest tasks than state-of-the-art domain-independent heuristics for classical\nplanning. They are even competitive with the strongest learning algorithm for\ndomain-dependent planning. These findings are especially remarkable given that\nour proof-of-concept implementation is based on an unoptimized Python planner\nand the baselines all build upon highly optimized C++ code. In some domains,\nthe LLM-generated heuristics expand fewer states than the baselines, revealing\nthat they are not only efficiently computable, but sometimes even more\ninformative than the state-of-the-art heuristics. Overall, our results show\nthat sampling a set of planning heuristic function programs can significantly\nimprove the planning capabilities of LLMs."
                },
                "authors": [
                    {
                        "name": "Augusto B. Corra"
                    },
                    {
                        "name": "Andr G. Pereira"
                    },
                    {
                        "name": "Jendrik Seipp"
                    }
                ],
                "author_detail": {
                    "name": "Jendrik Seipp"
                },
                "author": "Jendrik Seipp",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18809v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18809v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18800v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18800v1",
                "updated": "2025-03-24T15:46:18Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    15,
                    46,
                    18,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T15:46:18Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    15,
                    46,
                    18,
                    0,
                    83,
                    0
                ],
                "title": "Development of portable cosmic-ray muon detector array for muography",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Development of portable cosmic-ray muon detector array for muography"
                },
                "summary": "As the multidisciplinary applications of cosmic-ray muons expand to\nlarge-scale and wide-area scenarios, the construction of cosmic-ray muon\ndetector arrays has become a key solution to overcome the hardware limitations\nof individual detector. For muography, the array-based detector design enables\nfast-scanning of large target objects, allowing for rapid identification of\ndensity variation regions, which can improve the efficiency of tomography. This\npaper integrates scintillator detector technology with Internet of things (IoT)\ntechnology, proposing a novel array networking model for nationwide deployment.\nThe model enables long-distance data collection and distribution, laying the\nfoundation for future multidisciplinary applications such as muography and\nother fields.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the multidisciplinary applications of cosmic-ray muons expand to\nlarge-scale and wide-area scenarios, the construction of cosmic-ray muon\ndetector arrays has become a key solution to overcome the hardware limitations\nof individual detector. For muography, the array-based detector design enables\nfast-scanning of large target objects, allowing for rapid identification of\ndensity variation regions, which can improve the efficiency of tomography. This\npaper integrates scintillator detector technology with Internet of things (IoT)\ntechnology, proposing a novel array networking model for nationwide deployment.\nThe model enables long-distance data collection and distribution, laying the\nfoundation for future multidisciplinary applications such as muography and\nother fields."
                },
                "authors": [
                    {
                        "name": "Yunsong Ning"
                    },
                    {
                        "name": "Yi Yuan"
                    },
                    {
                        "name": "Tao Yu"
                    },
                    {
                        "name": "Hongyu Chen"
                    },
                    {
                        "name": "Chengyan Xie"
                    },
                    {
                        "name": "Hui Jiang"
                    },
                    {
                        "name": "Hesheng Liu"
                    },
                    {
                        "name": "Guihao Lu"
                    },
                    {
                        "name": "Mingchen Sun"
                    },
                    {
                        "name": "Yu Chen"
                    },
                    {
                        "name": "Jian Tang"
                    }
                ],
                "author_detail": {
                    "name": "Jian Tang"
                },
                "author": "Jian Tang",
                "arxiv_comment": "7 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18800v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18800v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.geo-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04832v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04832v4",
                "updated": "2025-03-24T15:42:11Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    15,
                    42,
                    11,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-05T10:59:32Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    10,
                    59,
                    32,
                    2,
                    64,
                    0
                ],
                "title": "Lightweight Embedded FPGA Deployment of Learned Image Compression with\n  Knowledge Distillation and Hybrid Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lightweight Embedded FPGA Deployment of Learned Image Compression with\n  Knowledge Distillation and Hybrid Quantization"
                },
                "summary": "Learnable Image Compression (LIC) has shown the potential to outperform\nstandardized video codecs in RD efficiency, prompting the research for\nhardware-friendly implementations. Most existing LIC hardware implementations\nprioritize latency to RD-efficiency and through an extensive exploration of the\nhardware design space. We present a novel design paradigm where the burden of\ntuning the design for a specific hardware platform is shifted towards model\ndimensioning and without compromising on RD-efficiency. First, we design a\nframework for distilling a leaner student LIC model from a reference teacher:\nby tuning a single model hyperparameters, we can meet the constraints of\ndifferent hardware platforms without a complex hardware design exploration.\nSecond, we propose a hardware-friendly implementation of the Generalized\nDivisive Normalization - GDN activation that preserves RD efficiency even post\nparameter quantization. Third, we design a pipelined FPGA configuration which\ntakes full advantage of available FPGA resources by leveraging parallel\nprocessing and optimizing resource allocation. Our experiments with a state of\nthe art LIC model show that we outperform all existing FPGA implementations\nwhile performing very close to the original model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learnable Image Compression (LIC) has shown the potential to outperform\nstandardized video codecs in RD efficiency, prompting the research for\nhardware-friendly implementations. Most existing LIC hardware implementations\nprioritize latency to RD-efficiency and through an extensive exploration of the\nhardware design space. We present a novel design paradigm where the burden of\ntuning the design for a specific hardware platform is shifted towards model\ndimensioning and without compromising on RD-efficiency. First, we design a\nframework for distilling a leaner student LIC model from a reference teacher:\nby tuning a single model hyperparameters, we can meet the constraints of\ndifferent hardware platforms without a complex hardware design exploration.\nSecond, we propose a hardware-friendly implementation of the Generalized\nDivisive Normalization - GDN activation that preserves RD efficiency even post\nparameter quantization. Third, we design a pipelined FPGA configuration which\ntakes full advantage of available FPGA resources by leveraging parallel\nprocessing and optimizing resource allocation. Our experiments with a state of\nthe art LIC model show that we outperform all existing FPGA implementations\nwhile performing very close to the original model."
                },
                "authors": [
                    {
                        "name": "Alaa Mazouz"
                    },
                    {
                        "name": "Sumanta Chaudhuri"
                    },
                    {
                        "name": "Marco Cagnanzzo"
                    },
                    {
                        "name": "Mihai Mitrea"
                    },
                    {
                        "name": "Enzo Tartaglione"
                    },
                    {
                        "name": "Attilio Fiandrotti"
                    }
                ],
                "author_detail": {
                    "name": "Attilio Fiandrotti"
                },
                "author": "Attilio Fiandrotti",
                "arxiv_comment": "Submitted to IEEE Transactions on Circuits and Systems for Video\n  Technology in March 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04832v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04832v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18792v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18792v1",
                "updated": "2025-03-24T15:39:25Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    15,
                    39,
                    25,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T15:39:25Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    15,
                    39,
                    25,
                    0,
                    83,
                    0
                ],
                "title": "REALM: A Dataset of Real-World LLM Use Cases",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "REALM: A Dataset of Real-World LLM Use Cases"
                },
                "summary": "Large Language Models, such as the GPT series, have driven significant\nindustrial applications, leading to economic and societal transformations.\nHowever, a comprehensive understanding of their real-world applications remains\nlimited. To address this, we introduce REALM, a dataset of over 94,000 LLM use\ncases collected from Reddit and news articles. REALM captures two key\ndimensions: the diverse applications of LLMs and the demographics of their\nusers. It categorizes LLM applications and explores how users' occupations\nrelate to the types of applications they use. By integrating real-world data,\nREALM offers insights into LLM adoption across different domains, providing a\nfoundation for future research on their evolving societal roles. A dedicated\ndashboard https://realm-e7682.web.app/ presents the data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models, such as the GPT series, have driven significant\nindustrial applications, leading to economic and societal transformations.\nHowever, a comprehensive understanding of their real-world applications remains\nlimited. To address this, we introduce REALM, a dataset of over 94,000 LLM use\ncases collected from Reddit and news articles. REALM captures two key\ndimensions: the diverse applications of LLMs and the demographics of their\nusers. It categorizes LLM applications and explores how users' occupations\nrelate to the types of applications they use. By integrating real-world data,\nREALM offers insights into LLM adoption across different domains, providing a\nfoundation for future research on their evolving societal roles. A dedicated\ndashboard https://realm-e7682.web.app/ presents the data."
                },
                "authors": [
                    {
                        "name": "Jingwen Cheng"
                    },
                    {
                        "name": "Kshitish Ghate"
                    },
                    {
                        "name": "Wenyue Hua"
                    },
                    {
                        "name": "William Yang Wang"
                    },
                    {
                        "name": "Hong Shen"
                    },
                    {
                        "name": "Fei Fang"
                    }
                ],
                "author_detail": {
                    "name": "Fei Fang"
                },
                "author": "Fei Fang",
                "arxiv_comment": "9 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18792v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18792v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04598v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04598v2",
                "updated": "2025-03-24T15:27:13Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    15,
                    27,
                    13,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-06T16:40:48Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    16,
                    40,
                    48,
                    3,
                    65,
                    0
                ],
                "title": "HybridNorm: Towards Stable and Efficient Transformer Training via Hybrid\n  Normalization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HybridNorm: Towards Stable and Efficient Transformer Training via Hybrid\n  Normalization"
                },
                "summary": "Transformers have become the de facto architecture for a wide range of\nmachine learning tasks, particularly in large language models (LLMs). Despite\ntheir remarkable performance, challenges remain in training deep transformer\nnetworks, especially regarding the location of layer normalization. While\nPre-Norm structures facilitate easier training due to their more prominent\nidentity path, they often yield suboptimal performance compared to Post-Norm.\nIn this paper, we propose $\\textbf{HybridNorm}$, a straightforward yet\neffective hybrid normalization strategy that integrates the advantages of both\nPre-Norm and Post-Norm approaches. Specifically, HybridNorm employs QKV\nnormalization within the attention mechanism and Post-Norm in the feed-forward\nnetwork (FFN) of each transformer block. This design not only stabilizes\ntraining but also enhances performance, particularly in the context of LLMs.\nComprehensive experiments in both dense and sparse architectures show that\nHybridNorm consistently outperforms both Pre-Norm and Post-Norm approaches,\nachieving state-of-the-art results across various benchmarks. These findings\nhighlight the potential of HybridNorm as a more stable and effective technique\nfor improving the training and performance of deep transformer models. Code is\navailable at https://github.com/BryceZhuo/HybridNorm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers have become the de facto architecture for a wide range of\nmachine learning tasks, particularly in large language models (LLMs). Despite\ntheir remarkable performance, challenges remain in training deep transformer\nnetworks, especially regarding the location of layer normalization. While\nPre-Norm structures facilitate easier training due to their more prominent\nidentity path, they often yield suboptimal performance compared to Post-Norm.\nIn this paper, we propose $\\textbf{HybridNorm}$, a straightforward yet\neffective hybrid normalization strategy that integrates the advantages of both\nPre-Norm and Post-Norm approaches. Specifically, HybridNorm employs QKV\nnormalization within the attention mechanism and Post-Norm in the feed-forward\nnetwork (FFN) of each transformer block. This design not only stabilizes\ntraining but also enhances performance, particularly in the context of LLMs.\nComprehensive experiments in both dense and sparse architectures show that\nHybridNorm consistently outperforms both Pre-Norm and Post-Norm approaches,\nachieving state-of-the-art results across various benchmarks. These findings\nhighlight the potential of HybridNorm as a more stable and effective technique\nfor improving the training and performance of deep transformer models. Code is\navailable at https://github.com/BryceZhuo/HybridNorm."
                },
                "authors": [
                    {
                        "name": "Zhijian Zhuo"
                    },
                    {
                        "name": "Yutao Zeng"
                    },
                    {
                        "name": "Ya Wang"
                    },
                    {
                        "name": "Sijun Zhang"
                    },
                    {
                        "name": "Jian Yang"
                    },
                    {
                        "name": "Xiaoqing Li"
                    },
                    {
                        "name": "Xun Zhou"
                    },
                    {
                        "name": "Jinwen Ma"
                    }
                ],
                "author_detail": {
                    "name": "Jinwen Ma"
                },
                "author": "Jinwen Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04598v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04598v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2307.06608v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2307.06608v3",
                "updated": "2025-03-24T15:27:02Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    15,
                    27,
                    2,
                    0,
                    83,
                    0
                ],
                "published": "2023-07-13T08:10:48Z",
                "published_parsed": [
                    2023,
                    7,
                    13,
                    8,
                    10,
                    48,
                    3,
                    194,
                    0
                ],
                "title": "MF-CLIP: Leveraging CLIP as Surrogate Models for No-box Adversarial\n  Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MF-CLIP: Leveraging CLIP as Surrogate Models for No-box Adversarial\n  Attacks"
                },
                "summary": "The vulnerability of Deep Neural Networks (DNNs) to adversarial attacks poses\na significant challenge to their deployment in safety-critical applications.\nWhile extensive research has addressed various attack scenarios, the no-box\nattack setting where adversaries have no prior knowledge, including access to\ntraining data of the target model, remains relatively underexplored despite its\npractical relevance. This work presents a systematic investigation into\nleveraging large-scale Vision-Language Models (VLMs), particularly CLIP, as\nsurrogate models for executing no-box attacks. Our theoretical and empirical\nanalyses reveal a key limitation in the execution of no-box attacks stemming\nfrom insufficient discriminative capabilities for direct application of vanilla\nCLIP as a surrogate model. To address this limitation, we propose MF-CLIP: a\nnovel framework that enhances CLIP's effectiveness as a surrogate model through\nmargin-aware feature space optimization. Comprehensive evaluations across\ndiverse architectures and datasets demonstrate that MF-CLIP substantially\nadvances the state-of-the-art in no-box attacks, surpassing existing baselines\nby 15.23% on standard models and achieving a 9.52% improvement on adversarially\ntrained models. Our code will be made publicly available to facilitate\nreproducibility and future research in this direction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The vulnerability of Deep Neural Networks (DNNs) to adversarial attacks poses\na significant challenge to their deployment in safety-critical applications.\nWhile extensive research has addressed various attack scenarios, the no-box\nattack setting where adversaries have no prior knowledge, including access to\ntraining data of the target model, remains relatively underexplored despite its\npractical relevance. This work presents a systematic investigation into\nleveraging large-scale Vision-Language Models (VLMs), particularly CLIP, as\nsurrogate models for executing no-box attacks. Our theoretical and empirical\nanalyses reveal a key limitation in the execution of no-box attacks stemming\nfrom insufficient discriminative capabilities for direct application of vanilla\nCLIP as a surrogate model. To address this limitation, we propose MF-CLIP: a\nnovel framework that enhances CLIP's effectiveness as a surrogate model through\nmargin-aware feature space optimization. Comprehensive evaluations across\ndiverse architectures and datasets demonstrate that MF-CLIP substantially\nadvances the state-of-the-art in no-box attacks, surpassing existing baselines\nby 15.23% on standard models and achieving a 9.52% improvement on adversarially\ntrained models. Our code will be made publicly available to facilitate\nreproducibility and future research in this direction."
                },
                "authors": [
                    {
                        "name": "Jiaming Zhang"
                    },
                    {
                        "name": "Lingyu Qiu"
                    },
                    {
                        "name": "Qi Yi"
                    },
                    {
                        "name": "Yige Li"
                    },
                    {
                        "name": "Jitao Sang"
                    },
                    {
                        "name": "Changsheng Xu"
                    },
                    {
                        "name": "Dit-Yan Yeung"
                    }
                ],
                "author_detail": {
                    "name": "Dit-Yan Yeung"
                },
                "author": "Dit-Yan Yeung",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2307.06608v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2307.06608v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18773v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18773v1",
                "updated": "2025-03-24T15:22:41Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    15,
                    22,
                    41,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T15:22:41Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    15,
                    22,
                    41,
                    0,
                    83,
                    0
                ],
                "title": "BitDecoding: Unlocking Tensor Cores for Long-Context LLMs Decoding with\n  Low-Bit KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BitDecoding: Unlocking Tensor Cores for Long-Context LLMs Decoding with\n  Low-Bit KV Cache"
                },
                "summary": "The growing adoption of long-context Large Language Models (LLMs) has\nintroduced significant memory and computational challenges in autoregressive\ndecoding due to the expanding Key-Value (KV) cache. KV cache quantization has\nemerged as a promising solution, with prior work showing that 4-bit or even\n2-bit quantization can maintain model accuracy while reducing memory costs.\nHowever, despite these benefits, preliminary implementations for the low-bit KV\ncache struggle to deliver the expected speedup due to quantization and\ndequantization overheads and the lack of Tensor Cores utilization. In this\nwork, we propose BitDecoding, a GPU-optimized framework that unlocks Tensor\nCores for efficient decoding with low-bit KV cache. Efficiently leveraging\nTensor Cores for low-bit KV cache is challenging due to the dynamic nature of\nKV cache generation at each decoding step. BitDecoding addresses these\nchallenges with a Tensor Cores-Centric BitFusion Scheme that ensures data\nlayout compatibility to enable high utilization of Tensor Cores. Additionally,\nBitDecoding incorporates a warp-efficient parallel decoding kernel and a\nfine-grained asynchronous pipeline, minimizing dequantization overhead and\nimproving computational efficiency. Experiments show that BitDecoding achieves\nup to 7.5x speedup on RTX 4090, 4.8x on A100, and 8.9x on H100, compared to\nFP16 FlashDecoding-v2. It also outperforms the state-of-the-art low-bit KV\ncache implementation (QServe) by up to 4.3x. On LLaMA-3.1-8B with a 128K\nsequence length, BitDecoding reduces single-batch decoding latency by 3x,\ndemonstrating its effectiveness in long-context generation scenarios. The code\nis available at https://github.com/DD-DuDa/BitDecoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing adoption of long-context Large Language Models (LLMs) has\nintroduced significant memory and computational challenges in autoregressive\ndecoding due to the expanding Key-Value (KV) cache. KV cache quantization has\nemerged as a promising solution, with prior work showing that 4-bit or even\n2-bit quantization can maintain model accuracy while reducing memory costs.\nHowever, despite these benefits, preliminary implementations for the low-bit KV\ncache struggle to deliver the expected speedup due to quantization and\ndequantization overheads and the lack of Tensor Cores utilization. In this\nwork, we propose BitDecoding, a GPU-optimized framework that unlocks Tensor\nCores for efficient decoding with low-bit KV cache. Efficiently leveraging\nTensor Cores for low-bit KV cache is challenging due to the dynamic nature of\nKV cache generation at each decoding step. BitDecoding addresses these\nchallenges with a Tensor Cores-Centric BitFusion Scheme that ensures data\nlayout compatibility to enable high utilization of Tensor Cores. Additionally,\nBitDecoding incorporates a warp-efficient parallel decoding kernel and a\nfine-grained asynchronous pipeline, minimizing dequantization overhead and\nimproving computational efficiency. Experiments show that BitDecoding achieves\nup to 7.5x speedup on RTX 4090, 4.8x on A100, and 8.9x on H100, compared to\nFP16 FlashDecoding-v2. It also outperforms the state-of-the-art low-bit KV\ncache implementation (QServe) by up to 4.3x. On LLaMA-3.1-8B with a 128K\nsequence length, BitDecoding reduces single-batch decoding latency by 3x,\ndemonstrating its effectiveness in long-context generation scenarios. The code\nis available at https://github.com/DD-DuDa/BitDecoding."
                },
                "authors": [
                    {
                        "name": "Dayou Du"
                    },
                    {
                        "name": "Shijie Cao"
                    },
                    {
                        "name": "Jianyi Cheng"
                    },
                    {
                        "name": "Ting Cao"
                    },
                    {
                        "name": "Mao Yang"
                    }
                ],
                "author_detail": {
                    "name": "Mao Yang"
                },
                "author": "Mao Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18773v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18773v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03550v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03550v2",
                "updated": "2025-03-24T15:19:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    15,
                    19,
                    50,
                    0,
                    83,
                    0
                ],
                "published": "2024-12-04T18:47:11Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    18,
                    47,
                    11,
                    2,
                    339,
                    0
                ],
                "title": "Teaching an Old Dog New Tricks: Verifiable FHE Using Commodity Hardware",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Teaching an Old Dog New Tricks: Verifiable FHE Using Commodity Hardware"
                },
                "summary": "We present Argos, a simple approach for adding verifiability to fully\nhomomorphic encryption (FHE) schemes using trusted hardware. Traditional\napproaches to verifiable FHE require expensive cryptographic proofs, which\nincur an overhead of up to seven orders of magnitude on top of FHE, making them\nimpractical.\n  With Argos, we show that trusted hardware can be securely used to provide\nverifiability for FHE computations, with minimal overhead relative to the\nbaseline FHE computation. An important contribution of Argos is showing that\nthe major security pitfall associated with trusted hardware, microarchitectural\nside channels, can be completely mitigated by excluding any secrets from the\nCPU and the memory hierarchy. This is made possible by focusing on building a\nplatform that only enforces program and data integrity and not confidentiality.\nAll secrets related to the attestation mechanism are kept in a separate\ncoprocessor (e.g., a TPM)-inaccessible to any software-based attacker. Relying\non a discrete TPM typically incurs significant performance overhead, which is\nwhy (insecure) software-based TPMs are used in practice. As a second\ncontribution, we show that for FHE applications, the attestation protocol can\nbe adapted to only incur a fixed cost.\n  Argos requires no dedicated hardware extensions and is supported on commodity\nprocessors from 2008 onward. Our prototype implementation introduces 3%\noverhead for FHE evaluation, and 8% for more complex protocols. In particular,\nwe show that Argos can be used for real-world applications of FHE, such as\nprivate information retrieval (PIR) and private set intersection (PSI), where\nproviding verifiability is imperative. By demonstrating how to combine\ncryptography with trusted hardware, Argos paves the way for widespread\ndeployment of FHE-based protocols beyond the semi-honest setting, without the\noverhead of cryptographic proofs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Argos, a simple approach for adding verifiability to fully\nhomomorphic encryption (FHE) schemes using trusted hardware. Traditional\napproaches to verifiable FHE require expensive cryptographic proofs, which\nincur an overhead of up to seven orders of magnitude on top of FHE, making them\nimpractical.\n  With Argos, we show that trusted hardware can be securely used to provide\nverifiability for FHE computations, with minimal overhead relative to the\nbaseline FHE computation. An important contribution of Argos is showing that\nthe major security pitfall associated with trusted hardware, microarchitectural\nside channels, can be completely mitigated by excluding any secrets from the\nCPU and the memory hierarchy. This is made possible by focusing on building a\nplatform that only enforces program and data integrity and not confidentiality.\nAll secrets related to the attestation mechanism are kept in a separate\ncoprocessor (e.g., a TPM)-inaccessible to any software-based attacker. Relying\non a discrete TPM typically incurs significant performance overhead, which is\nwhy (insecure) software-based TPMs are used in practice. As a second\ncontribution, we show that for FHE applications, the attestation protocol can\nbe adapted to only incur a fixed cost.\n  Argos requires no dedicated hardware extensions and is supported on commodity\nprocessors from 2008 onward. Our prototype implementation introduces 3%\noverhead for FHE evaluation, and 8% for more complex protocols. In particular,\nwe show that Argos can be used for real-world applications of FHE, such as\nprivate information retrieval (PIR) and private set intersection (PSI), where\nproviding verifiability is imperative. By demonstrating how to combine\ncryptography with trusted hardware, Argos paves the way for widespread\ndeployment of FHE-based protocols beyond the semi-honest setting, without the\noverhead of cryptographic proofs."
                },
                "authors": [
                    {
                        "name": "Jules Drean"
                    },
                    {
                        "name": "Fisher Jepsen"
                    },
                    {
                        "name": "Edward Suh"
                    },
                    {
                        "name": "Srini Devadas"
                    },
                    {
                        "name": "Aamer Jaleel"
                    },
                    {
                        "name": "Gururaj Saileshwar"
                    }
                ],
                "author_detail": {
                    "name": "Gururaj Saileshwar"
                },
                "author": "Gururaj Saileshwar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03550v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03550v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18769v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18769v1",
                "updated": "2025-03-24T15:16:51Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    15,
                    16,
                    51,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T15:16:51Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    15,
                    16,
                    51,
                    0,
                    83,
                    0
                ],
                "title": "AlphaSpace: Enabling Robotic Actions through Semantic Tokenization and\n  Symbolic Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AlphaSpace: Enabling Robotic Actions through Semantic Tokenization and\n  Symbolic Reasoning"
                },
                "summary": "This paper presents AlphaSpace, a novel methodology designed to enhance the\nspatial reasoning capabilities of large language models (LLMs) for 3D Cartesian\nspace navigation. AlphaSpace employs a semantics-based tokenization strategy,\nencoding height information through specialized semantic tokens, and integrates\nprimarily symbolic synthetic reasoning data. This approach enables LLMs to\naccurately manipulate objects by positioning them at specific [x, y, z]\ncoordinates. Experimental results demonstrate that AlphaSpace significantly\noutperforms existing models on manipulation subtasks, achieving a total\naccuracy of 66.67%, compared to 37.5% for GPT-4o and 29.17% for Claude 3.5\nSonnet.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents AlphaSpace, a novel methodology designed to enhance the\nspatial reasoning capabilities of large language models (LLMs) for 3D Cartesian\nspace navigation. AlphaSpace employs a semantics-based tokenization strategy,\nencoding height information through specialized semantic tokens, and integrates\nprimarily symbolic synthetic reasoning data. This approach enables LLMs to\naccurately manipulate objects by positioning them at specific [x, y, z]\ncoordinates. Experimental results demonstrate that AlphaSpace significantly\noutperforms existing models on manipulation subtasks, achieving a total\naccuracy of 66.67%, compared to 37.5% for GPT-4o and 29.17% for Claude 3.5\nSonnet."
                },
                "authors": [
                    {
                        "name": "Alan Dao"
                    },
                    {
                        "name": "Dinh Bach Vu"
                    },
                    {
                        "name": "Bui Quang Huy"
                    }
                ],
                "author_detail": {
                    "name": "Bui Quang Huy"
                },
                "arxiv_affiliation": "Gia Tuan Dao",
                "author": "Bui Quang Huy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18769v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18769v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16514v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16514v2",
                "updated": "2025-03-24T15:14:06Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    15,
                    14,
                    6,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-15T23:43:06Z",
                "published_parsed": [
                    2025,
                    3,
                    15,
                    23,
                    43,
                    6,
                    5,
                    74,
                    0
                ],
                "title": "VeriMind: Agentic LLM for Automated Verilog Generation with a Novel\n  Evaluation Metric",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VeriMind: Agentic LLM for Automated Verilog Generation with a Novel\n  Evaluation Metric"
                },
                "summary": "Designing Verilog modules requires meticulous attention to correctness,\nefficiency, and adherence to design specifications. However, manually writing\nVerilog code remains a complex and time-consuming task that demands both expert\nknowledge and iterative refinement. Leveraging recent advancements in large\nlanguage models (LLMs) and their structured text generation capabilities, we\npropose VeriMind, an agentic LLM framework for Verilog code generation that\nsignificantly automates and optimizes the synthesis process. Unlike traditional\nLLM-based code generators, VeriMind employs a structured reasoning approach:\ngiven a user-provided prompt describing design requirements, the system first\nformulates a detailed train of thought before the final Verilog code is\ngenerated. This multi-step methodology enhances interpretability, accuracy, and\nadaptability in hardware design. In addition, we introduce a novel evaluation\nmetric-pass@ARC-which combines the conventional pass@k measure with Average\nRefinement Cycles (ARC) to capture both success rate and the efficiency of\niterative refinement. Experimental results on diverse hardware design tasks\ndemonstrated that our approach achieved up to $8.3\\%$ improvement on pass@k\nmetric and $8.1\\%$ on pass@ARC metric. These findings underscore the\ntransformative potential of agentic LLMs in automated hardware design, RTL\ndevelopment, and digital system synthesis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Designing Verilog modules requires meticulous attention to correctness,\nefficiency, and adherence to design specifications. However, manually writing\nVerilog code remains a complex and time-consuming task that demands both expert\nknowledge and iterative refinement. Leveraging recent advancements in large\nlanguage models (LLMs) and their structured text generation capabilities, we\npropose VeriMind, an agentic LLM framework for Verilog code generation that\nsignificantly automates and optimizes the synthesis process. Unlike traditional\nLLM-based code generators, VeriMind employs a structured reasoning approach:\ngiven a user-provided prompt describing design requirements, the system first\nformulates a detailed train of thought before the final Verilog code is\ngenerated. This multi-step methodology enhances interpretability, accuracy, and\nadaptability in hardware design. In addition, we introduce a novel evaluation\nmetric-pass@ARC-which combines the conventional pass@k measure with Average\nRefinement Cycles (ARC) to capture both success rate and the efficiency of\niterative refinement. Experimental results on diverse hardware design tasks\ndemonstrated that our approach achieved up to $8.3\\%$ improvement on pass@k\nmetric and $8.1\\%$ on pass@ARC metric. These findings underscore the\ntransformative potential of agentic LLMs in automated hardware design, RTL\ndevelopment, and digital system synthesis."
                },
                "authors": [
                    {
                        "name": "Bardia Nadimi"
                    },
                    {
                        "name": "Ghali Omar Boutaib"
                    },
                    {
                        "name": "Hao Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Hao Zheng"
                },
                "author": "Hao Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16514v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16514v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18760v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18760v1",
                "updated": "2025-03-24T15:09:03Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    15,
                    9,
                    3,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T15:09:03Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    15,
                    9,
                    3,
                    0,
                    83,
                    0
                ],
                "title": "Synthetic Function Demonstrations Improve Generation in Low-Resource\n  Programming Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synthetic Function Demonstrations Improve Generation in Low-Resource\n  Programming Languages"
                },
                "summary": "A key consideration when training an LLM is whether the target language is\nmore or less resourced, whether this is English compared to Welsh, or Python\ncompared to Excel. Typical training data for programming languages consist of\nreal program demonstrations coupled with human-written comments. Here we\npresent novel approaches to the creation of such data for low resource\nprogramming languages. We generate fully-synthetic, textbook-quality\ndemonstrations of common library functions in an example domain of Excel\nformulas, using a teacher model. We then finetune an underperforming student\nmodel, and show improvement on 2 question-answering datasets recast into the\nExcel domain. We show advantages of finetuning over standard, off-the-shelf RAG\napproaches, which can offer only modest improvement due to the unfamiliar\ntarget domain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A key consideration when training an LLM is whether the target language is\nmore or less resourced, whether this is English compared to Welsh, or Python\ncompared to Excel. Typical training data for programming languages consist of\nreal program demonstrations coupled with human-written comments. Here we\npresent novel approaches to the creation of such data for low resource\nprogramming languages. We generate fully-synthetic, textbook-quality\ndemonstrations of common library functions in an example domain of Excel\nformulas, using a teacher model. We then finetune an underperforming student\nmodel, and show improvement on 2 question-answering datasets recast into the\nExcel domain. We show advantages of finetuning over standard, off-the-shelf RAG\napproaches, which can offer only modest improvement due to the unfamiliar\ntarget domain."
                },
                "authors": [
                    {
                        "name": "Nick McKenna"
                    },
                    {
                        "name": "Xinnuo Xu"
                    },
                    {
                        "name": "Jack Williams"
                    },
                    {
                        "name": "Nick Wilson"
                    },
                    {
                        "name": "Benjamin Van Durme"
                    },
                    {
                        "name": "Christian Poelitz"
                    }
                ],
                "author_detail": {
                    "name": "Christian Poelitz"
                },
                "author": "Christian Poelitz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18760v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18760v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13178v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13178v2",
                "updated": "2025-03-24T14:47:05Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    14,
                    47,
                    5,
                    0,
                    83,
                    0
                ],
                "published": "2025-02-18T07:35:35Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    7,
                    35,
                    35,
                    1,
                    49,
                    0
                ],
                "title": "Benchmarking Post-Training Quantization in LLMs: Comprehensive Taxonomy,\n  Unified Evaluation, and Comparative Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking Post-Training Quantization in LLMs: Comprehensive Taxonomy,\n  Unified Evaluation, and Comparative Analysis"
                },
                "summary": "Post-training Quantization (PTQ) technique has been extensively adopted for\nlarge language models (LLMs) compression owing to its efficiency and low\nresource requirement. However, current research lacks a in-depth analysis of\nthe superior and applicable scenarios of each PTQ strategy. In addition,\nexisting algorithms focus primarily on performance, overlooking the trade-off\namong model size, performance, and quantization bitwidth. To mitigate these\nconfusions, we provide a novel benchmark for LLMs PTQ in this paper. Firstly,\nin order to support our benchmark, we propose a comprehensive taxonomy for\nexisting mainstream methods by scrutinizing their computational strategies\n(e.g., optimization-based, compensation-based, etc.). Then, we conduct\nextensive experiments with the baseline within each class, covering models with\nvarious sizes (7B-70B), bitwidths, training levels (LLaMA1/2/3/3.1),\narchitectures (Mixtral, DeepSeekMoE and Mamba) and modality (LLaVA1.5 and\nVILA1.5) on a wide range of evaluation metrics.Through comparative analysis on\nthe results, we summarize the superior of each PTQ strategy and\nmodelsize-bitwidth trade-off considering the performance. For example, our\nbenchmark reveals that compensation-based technique demonstrates outstanding\ncross-architecture robustness and extremely low-bit PTQ for ultra large models\nshould be reexamined. Finally, we further accordingly claim that a practical\ncombination of compensation and other PTQ strategy can achieve SOTA various\nrobustness. We believe that our benchmark will provide valuable recommendations\nfor the deployment of LLMs and future research on PTQ approaches.We conduct an\nrepository for our benchmark at https://github.com/zjq0455/PTQ_Benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-training Quantization (PTQ) technique has been extensively adopted for\nlarge language models (LLMs) compression owing to its efficiency and low\nresource requirement. However, current research lacks a in-depth analysis of\nthe superior and applicable scenarios of each PTQ strategy. In addition,\nexisting algorithms focus primarily on performance, overlooking the trade-off\namong model size, performance, and quantization bitwidth. To mitigate these\nconfusions, we provide a novel benchmark for LLMs PTQ in this paper. Firstly,\nin order to support our benchmark, we propose a comprehensive taxonomy for\nexisting mainstream methods by scrutinizing their computational strategies\n(e.g., optimization-based, compensation-based, etc.). Then, we conduct\nextensive experiments with the baseline within each class, covering models with\nvarious sizes (7B-70B), bitwidths, training levels (LLaMA1/2/3/3.1),\narchitectures (Mixtral, DeepSeekMoE and Mamba) and modality (LLaVA1.5 and\nVILA1.5) on a wide range of evaluation metrics.Through comparative analysis on\nthe results, we summarize the superior of each PTQ strategy and\nmodelsize-bitwidth trade-off considering the performance. For example, our\nbenchmark reveals that compensation-based technique demonstrates outstanding\ncross-architecture robustness and extremely low-bit PTQ for ultra large models\nshould be reexamined. Finally, we further accordingly claim that a practical\ncombination of compensation and other PTQ strategy can achieve SOTA various\nrobustness. We believe that our benchmark will provide valuable recommendations\nfor the deployment of LLMs and future research on PTQ approaches.We conduct an\nrepository for our benchmark at https://github.com/zjq0455/PTQ_Benchmark."
                },
                "authors": [
                    {
                        "name": "Jiaqi Zhao"
                    },
                    {
                        "name": "Ming Wang"
                    },
                    {
                        "name": "Miao Zhang"
                    },
                    {
                        "name": "Yuzhang Shang"
                    },
                    {
                        "name": "Xuebo Liu"
                    },
                    {
                        "name": "Yaowei Wang"
                    },
                    {
                        "name": "Min Zhang"
                    },
                    {
                        "name": "Liqiang Nie"
                    }
                ],
                "author_detail": {
                    "name": "Liqiang Nie"
                },
                "author": "Liqiang Nie",
                "arxiv_comment": "17 pages, 3 fugures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13178v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13178v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16883v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16883v2",
                "updated": "2025-03-24T14:38:36Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    14,
                    38,
                    36,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-21T06:35:49Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    6,
                    35,
                    49,
                    4,
                    80,
                    0
                ],
                "title": "Assessing the Reliability and Validity of GPT-4 in Annotating Emotion\n  Appraisal Ratings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing the Reliability and Validity of GPT-4 in Annotating Emotion\n  Appraisal Ratings"
                },
                "summary": "Appraisal theories suggest that emotions arise from subjective evaluations of\nevents, referred to as appraisals. The taxonomy of appraisals is quite diverse,\nand they are usually given ratings on a Likert scale to be annotated in an\nexperiencer-annotator or reader-annotator paradigm. This paper studies GPT-4 as\na reader-annotator of 21 specific appraisal ratings in different prompt\nsettings, aiming to evaluate and improve its performance compared to human\nannotators. We found that GPT-4 is an effective reader-annotator that performs\nclose to or even slightly better than human annotators, and its results can be\nsignificantly improved by using a majority voting of five completions. GPT-4\nalso effectively predicts appraisal ratings and emotion labels using a single\nprompt, but adding instruction complexity results in poorer performance. We\nalso found that longer event descriptions lead to more accurate annotations for\nboth model and human annotator ratings. This work contributes to the growing\nusage of LLMs in psychology and the strategies for improving GPT-4 performance\nin annotating appraisals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Appraisal theories suggest that emotions arise from subjective evaluations of\nevents, referred to as appraisals. The taxonomy of appraisals is quite diverse,\nand they are usually given ratings on a Likert scale to be annotated in an\nexperiencer-annotator or reader-annotator paradigm. This paper studies GPT-4 as\na reader-annotator of 21 specific appraisal ratings in different prompt\nsettings, aiming to evaluate and improve its performance compared to human\nannotators. We found that GPT-4 is an effective reader-annotator that performs\nclose to or even slightly better than human annotators, and its results can be\nsignificantly improved by using a majority voting of five completions. GPT-4\nalso effectively predicts appraisal ratings and emotion labels using a single\nprompt, but adding instruction complexity results in poorer performance. We\nalso found that longer event descriptions lead to more accurate annotations for\nboth model and human annotator ratings. This work contributes to the growing\nusage of LLMs in psychology and the strategies for improving GPT-4 performance\nin annotating appraisals."
                },
                "authors": [
                    {
                        "name": "Deniss Ruder"
                    },
                    {
                        "name": "Andero Uusberg"
                    },
                    {
                        "name": "Kairit Sirts"
                    }
                ],
                "author_detail": {
                    "name": "Kairit Sirts"
                },
                "author": "Kairit Sirts",
                "arxiv_comment": "CLPsych 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16883v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16883v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.11905v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.11905v3",
                "updated": "2025-03-24T14:30:31Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    14,
                    30,
                    31,
                    0,
                    83,
                    0
                ],
                "published": "2024-07-16T16:38:47Z",
                "published_parsed": [
                    2024,
                    7,
                    16,
                    16,
                    38,
                    47,
                    1,
                    198,
                    0
                ],
                "title": "An Overview and Solution for Democratizing AI Workflows at the Network\n  Edge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Overview and Solution for Democratizing AI Workflows at the Network\n  Edge"
                },
                "summary": "With the process of democratization of the network edge, hardware and\nsoftware for networks are becoming available to the public, overcoming the\nconfines of traditional cloud providers and network operators. This trend,\ncoupled with the increasing importance of AI in 6G and beyond cellular\nnetworks, presents opportunities for innovative AI applications and systems at\nthe network edge. While AI models and services are well-managed in cloud\nsystems, achieving similar maturity for serving network needs remains an open\nchallenge. Existing open solutions are emerging and are yet to consider\ndemocratization requirements. In this work, we identify key requirements for\ndemocratization and propose NAOMI, a solution for democratizing AI/ML workflows\nat the network edge designed based on those requirements. Guided by the\nfunctionality and overlap analysis of the O-RAN AI/ML workflow architecture and\nMLOps systems, coupled with the survey of open-source AI/ML tools, we develop a\nmodular, scalable, and distributed hardware architecture-independent solution.\nNAOMI leverages state-of-the-art open-source tools and can be deployed on\ndistributed clusters of heterogeneous devices. The results show that NAOMI\nperforms up to 40% better in deployment time and up to 73% faster in AI/ML\nworkflow execution for larger datasets compared to AI/ML Framework, a\nrepresentative open network access solution, while performing inference and\nutilizing resources on par with its counterpart.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the process of democratization of the network edge, hardware and\nsoftware for networks are becoming available to the public, overcoming the\nconfines of traditional cloud providers and network operators. This trend,\ncoupled with the increasing importance of AI in 6G and beyond cellular\nnetworks, presents opportunities for innovative AI applications and systems at\nthe network edge. While AI models and services are well-managed in cloud\nsystems, achieving similar maturity for serving network needs remains an open\nchallenge. Existing open solutions are emerging and are yet to consider\ndemocratization requirements. In this work, we identify key requirements for\ndemocratization and propose NAOMI, a solution for democratizing AI/ML workflows\nat the network edge designed based on those requirements. Guided by the\nfunctionality and overlap analysis of the O-RAN AI/ML workflow architecture and\nMLOps systems, coupled with the survey of open-source AI/ML tools, we develop a\nmodular, scalable, and distributed hardware architecture-independent solution.\nNAOMI leverages state-of-the-art open-source tools and can be deployed on\ndistributed clusters of heterogeneous devices. The results show that NAOMI\nperforms up to 40% better in deployment time and up to 73% faster in AI/ML\nworkflow execution for larger datasets compared to AI/ML Framework, a\nrepresentative open network access solution, while performing inference and\nutilizing resources on par with its counterpart."
                },
                "authors": [
                    {
                        "name": "Andrej op"
                    },
                    {
                        "name": "Bla Bertalani"
                    },
                    {
                        "name": "Carolina Fortuna"
                    }
                ],
                "author_detail": {
                    "name": "Carolina Fortuna"
                },
                "author": "Carolina Fortuna",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.11905v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.11905v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18666v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18666v1",
                "updated": "2025-03-24T13:31:48Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    13,
                    31,
                    48,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T13:31:48Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    13,
                    31,
                    48,
                    0,
                    83,
                    0
                ],
                "title": "AgentSpec: Customizable Runtime Enforcement for Safe and Reliable LLM\n  Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AgentSpec: Customizable Runtime Enforcement for Safe and Reliable LLM\n  Agents"
                },
                "summary": "Agents built on LLMs are increasingly deployed across diverse domains,\nautomating complex decision-making and task execution. However, their autonomy\nintroduces safety risks, including security vulnerabilities, legal violations,\nand unintended harmful actions. Existing mitigation methods, such as\nmodel-based safeguards and early enforcement strategies, fall short in\nrobustness, interpretability, and adaptability. To address these challenges, we\npropose AgentSpec, a lightweight domain-specific language for specifying and\nenforcing runtime constraints on LLM agents. With AgentSpec, users define\nstructured rules that incorporate triggers, predicates, and enforcement\nmechanisms, ensuring agents operate within predefined safety boundaries. We\nimplement AgentSpec across multiple domains, including code execution, embodied\nagents, and autonomous driving, demonstrating its adaptability and\neffectiveness. Our evaluation shows that AgentSpec successfully prevents unsafe\nexecutions in over 90% of code agent cases, eliminates all hazardous actions in\nembodied agent tasks, and enforces 100% compliance by autonomous vehicles\n(AVs). Despite its strong safety guarantees, AgentSpec remains computationally\nlightweight, with overheads in milliseconds. By combining interpretability,\nmodularity, and efficiency, AgentSpec provides a practical and scalable\nsolution for enforcing LLM agent safety across diverse applications. We also\nautomate the generation of rules using LLMs and assess their effectiveness. Our\nevaluation shows that the rules generated by OpenAI o1 achieve a precision of\n95.56% and recall of 70.96% for embodied agents, successfully identifying\n87.26% of the risky code, and prevent AVs from breaking laws in 5 out of 8\nscenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agents built on LLMs are increasingly deployed across diverse domains,\nautomating complex decision-making and task execution. However, their autonomy\nintroduces safety risks, including security vulnerabilities, legal violations,\nand unintended harmful actions. Existing mitigation methods, such as\nmodel-based safeguards and early enforcement strategies, fall short in\nrobustness, interpretability, and adaptability. To address these challenges, we\npropose AgentSpec, a lightweight domain-specific language for specifying and\nenforcing runtime constraints on LLM agents. With AgentSpec, users define\nstructured rules that incorporate triggers, predicates, and enforcement\nmechanisms, ensuring agents operate within predefined safety boundaries. We\nimplement AgentSpec across multiple domains, including code execution, embodied\nagents, and autonomous driving, demonstrating its adaptability and\neffectiveness. Our evaluation shows that AgentSpec successfully prevents unsafe\nexecutions in over 90% of code agent cases, eliminates all hazardous actions in\nembodied agent tasks, and enforces 100% compliance by autonomous vehicles\n(AVs). Despite its strong safety guarantees, AgentSpec remains computationally\nlightweight, with overheads in milliseconds. By combining interpretability,\nmodularity, and efficiency, AgentSpec provides a practical and scalable\nsolution for enforcing LLM agent safety across diverse applications. We also\nautomate the generation of rules using LLMs and assess their effectiveness. Our\nevaluation shows that the rules generated by OpenAI o1 achieve a precision of\n95.56% and recall of 70.96% for embodied agents, successfully identifying\n87.26% of the risky code, and prevent AVs from breaking laws in 5 out of 8\nscenarios."
                },
                "authors": [
                    {
                        "name": "Haoyu Wang"
                    },
                    {
                        "name": "Christopher M. Poskitt"
                    },
                    {
                        "name": "Jun Sun"
                    }
                ],
                "author_detail": {
                    "name": "Jun Sun"
                },
                "author": "Jun Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18666v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18666v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14502v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14502v3",
                "updated": "2025-03-24T13:16:03Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    13,
                    16,
                    3,
                    0,
                    83,
                    0
                ],
                "published": "2025-02-20T12:31:03Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    12,
                    31,
                    3,
                    3,
                    51,
                    0
                ],
                "title": "How Much Knowledge Can You Pack into a LoRA Adapter without Harming LLM?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Much Knowledge Can You Pack into a LoRA Adapter without Harming LLM?"
                },
                "summary": "The performance of Large Language Models (LLMs) on many tasks is greatly\nlimited by the knowledge learned during pre-training and stored in the model's\nparameters. Low-rank adaptation (LoRA) is a popular and efficient training\ntechnique for updating or domain-specific adaptation of LLMs. In this study, we\ninvestigate how new facts can be incorporated into the LLM using LoRA without\ncompromising the previously learned knowledge. We fine-tuned\nLlama-3.1-8B-instruct using LoRA with varying amounts of new knowledge. Our\nexperiments have shown that the best results are obtained when the training\ndata contains a mixture of known and new facts. However, this approach is still\npotentially harmful because the model's performance on external\nquestion-answering benchmarks declines after such fine-tuning. When the\ntraining data is biased towards certain entities, the model tends to regress to\nfew overrepresented answers. In addition, we found that the model becomes more\nconfident and refuses to provide an answer in only few cases. These findings\nhighlight the potential pitfalls of LoRA-based LLM updates and underscore the\nimportance of training data composition and tuning parameters to balance new\nknowledge integration and general model capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The performance of Large Language Models (LLMs) on many tasks is greatly\nlimited by the knowledge learned during pre-training and stored in the model's\nparameters. Low-rank adaptation (LoRA) is a popular and efficient training\ntechnique for updating or domain-specific adaptation of LLMs. In this study, we\ninvestigate how new facts can be incorporated into the LLM using LoRA without\ncompromising the previously learned knowledge. We fine-tuned\nLlama-3.1-8B-instruct using LoRA with varying amounts of new knowledge. Our\nexperiments have shown that the best results are obtained when the training\ndata contains a mixture of known and new facts. However, this approach is still\npotentially harmful because the model's performance on external\nquestion-answering benchmarks declines after such fine-tuning. When the\ntraining data is biased towards certain entities, the model tends to regress to\nfew overrepresented answers. In addition, we found that the model becomes more\nconfident and refuses to provide an answer in only few cases. These findings\nhighlight the potential pitfalls of LoRA-based LLM updates and underscore the\nimportance of training data composition and tuning parameters to balance new\nknowledge integration and general model capabilities."
                },
                "authors": [
                    {
                        "name": "Sergey Pletenev"
                    },
                    {
                        "name": "Maria Marina"
                    },
                    {
                        "name": "Daniil Moskovskiy"
                    },
                    {
                        "name": "Vasily Konovalov"
                    },
                    {
                        "name": "Pavel Braslavski"
                    },
                    {
                        "name": "Alexander Panchenko"
                    },
                    {
                        "name": "Mikhail Salnikov"
                    }
                ],
                "author_detail": {
                    "name": "Mikhail Salnikov"
                },
                "author": "Mikhail Salnikov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14502v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14502v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18637v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18637v1",
                "updated": "2025-03-24T13:00:25Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    13,
                    0,
                    25,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T13:00:25Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    13,
                    0,
                    25,
                    0,
                    83,
                    0
                ],
                "title": "Unbiasing through Textual Descriptions: Mitigating Representation Bias\n  in Video Benchmarks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unbiasing through Textual Descriptions: Mitigating Representation Bias\n  in Video Benchmarks"
                },
                "summary": "We propose a new \"Unbiased through Textual Description (UTD)\" video benchmark\nbased on unbiased subsets of existing video classification and retrieval\ndatasets to enable a more robust assessment of video understanding\ncapabilities. Namely, we tackle the problem that current video benchmarks may\nsuffer from different representation biases, e.g., object bias or single-frame\nbias, where mere recognition of objects or utilization of only a single frame\nis sufficient for correct prediction. We leverage VLMs and LLMs to analyze and\ndebias benchmarks from such representation biases. Specifically, we generate\nframe-wise textual descriptions of videos, filter them for specific information\n(e.g. only objects) and leverage them to examine representation biases across\nthree dimensions: 1) concept bias - determining if a specific concept (e.g.,\nobjects) alone suffice for prediction; 2) temporal bias - assessing if temporal\ninformation contributes to prediction; and 3) common sense vs. dataset bias -\nevaluating whether zero-shot reasoning or dataset correlations contribute to\nprediction. We conduct a systematic analysis of 12 popular video classification\nand retrieval datasets and create new object-debiased test splits for these\ndatasets. Moreover, we benchmark 30 state-of-the-art video models on original\nand debiased splits and analyze biases in the models. To facilitate the future\ndevelopment of more robust video understanding benchmarks and models, we\nrelease: \"UTD-descriptions\", a dataset with our rich structured descriptions\nfor each dataset, and \"UTD-splits\", a dataset of object-debiased test splits.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a new \"Unbiased through Textual Description (UTD)\" video benchmark\nbased on unbiased subsets of existing video classification and retrieval\ndatasets to enable a more robust assessment of video understanding\ncapabilities. Namely, we tackle the problem that current video benchmarks may\nsuffer from different representation biases, e.g., object bias or single-frame\nbias, where mere recognition of objects or utilization of only a single frame\nis sufficient for correct prediction. We leverage VLMs and LLMs to analyze and\ndebias benchmarks from such representation biases. Specifically, we generate\nframe-wise textual descriptions of videos, filter them for specific information\n(e.g. only objects) and leverage them to examine representation biases across\nthree dimensions: 1) concept bias - determining if a specific concept (e.g.,\nobjects) alone suffice for prediction; 2) temporal bias - assessing if temporal\ninformation contributes to prediction; and 3) common sense vs. dataset bias -\nevaluating whether zero-shot reasoning or dataset correlations contribute to\nprediction. We conduct a systematic analysis of 12 popular video classification\nand retrieval datasets and create new object-debiased test splits for these\ndatasets. Moreover, we benchmark 30 state-of-the-art video models on original\nand debiased splits and analyze biases in the models. To facilitate the future\ndevelopment of more robust video understanding benchmarks and models, we\nrelease: \"UTD-descriptions\", a dataset with our rich structured descriptions\nfor each dataset, and \"UTD-splits\", a dataset of object-debiased test splits."
                },
                "authors": [
                    {
                        "name": "Nina Shvetsova"
                    },
                    {
                        "name": "Arsha Nagrani"
                    },
                    {
                        "name": "Bernt Schiele"
                    },
                    {
                        "name": "Hilde Kuehne"
                    },
                    {
                        "name": "Christian Rupprecht"
                    }
                ],
                "author_detail": {
                    "name": "Christian Rupprecht"
                },
                "author": "Christian Rupprecht",
                "arxiv_comment": "To be published at CVPR 2025, project webpage\n  https://utd-project.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18637v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18637v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.11183v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.11183v2",
                "updated": "2025-03-24T12:48:08Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    12,
                    48,
                    8,
                    0,
                    83,
                    0
                ],
                "published": "2024-02-17T03:47:38Z",
                "published_parsed": [
                    2024,
                    2,
                    17,
                    3,
                    47,
                    38,
                    5,
                    48,
                    0
                ],
                "title": "Materiality and Risk in the Age of Pervasive AI Sensors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Materiality and Risk in the Age of Pervasive AI Sensors"
                },
                "summary": "Artificial intelligence (AI) systems connected to sensor-laden devices are\nbecoming pervasive, which has notable implications for a range of AI risks,\nincluding to privacy, the environment, autonomy and more. There is therefore a\ngrowing need for increased accountability around the responsible development\nand deployment of these technologies. Here we highlight the dimensions of risk\nassociated with AI systems that arise from the material affordances of sensors\nand their underlying calculative models. We propose a sensor-sensitive\nframework for diagnosing these risks, complementing existing approaches such as\nthe US National Institute of Standards and Technology AI Risk Management\nFramework and the European Union AI Act, and discuss its implementation. We\nconclude by advocating for increased attention to the materiality of\nalgorithmic systems, and of on-device AI sensors in particular, and highlight\nthe need for development of a sensor design paradigm that empowers users and\ncommunities and leads to a future of increased fairness, accountability and\ntransparency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial intelligence (AI) systems connected to sensor-laden devices are\nbecoming pervasive, which has notable implications for a range of AI risks,\nincluding to privacy, the environment, autonomy and more. There is therefore a\ngrowing need for increased accountability around the responsible development\nand deployment of these technologies. Here we highlight the dimensions of risk\nassociated with AI systems that arise from the material affordances of sensors\nand their underlying calculative models. We propose a sensor-sensitive\nframework for diagnosing these risks, complementing existing approaches such as\nthe US National Institute of Standards and Technology AI Risk Management\nFramework and the European Union AI Act, and discuss its implementation. We\nconclude by advocating for increased attention to the materiality of\nalgorithmic systems, and of on-device AI sensors in particular, and highlight\nthe need for development of a sensor design paradigm that empowers users and\ncommunities and leads to a future of increased fairness, accountability and\ntransparency."
                },
                "authors": [
                    {
                        "name": "Mona Sloane"
                    },
                    {
                        "name": "Emanuel Moss"
                    },
                    {
                        "name": "Susan Kennedy"
                    },
                    {
                        "name": "Matthew Stewart"
                    },
                    {
                        "name": "Pete Warden"
                    },
                    {
                        "name": "Brian Plancher"
                    },
                    {
                        "name": "Vijay Janapa Reddi"
                    }
                ],
                "author_detail": {
                    "name": "Vijay Janapa Reddi"
                },
                "author": "Vijay Janapa Reddi",
                "arxiv_doi": "10.1038/s42256-025-01017-7",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1038/s42256-025-01017-7",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2402.11183v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.11183v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Nature Machine Intelligence (2025): 1-12",
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05843v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05843v3",
                "updated": "2025-03-24T12:22:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    12,
                    22,
                    37,
                    0,
                    83,
                    0
                ],
                "published": "2025-02-09T10:30:54Z",
                "published_parsed": [
                    2025,
                    2,
                    9,
                    10,
                    30,
                    54,
                    6,
                    40,
                    0
                ],
                "title": "From Objects to Events: Unlocking Complex Visual Understanding in Object\n  Detectors via LLM-guided Symbolic Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Objects to Events: Unlocking Complex Visual Understanding in Object\n  Detectors via LLM-guided Symbolic Reasoning"
                },
                "summary": "Our key innovation lies in bridging the semantic gap between object detection\nand event understanding without requiring expensive task-specific training. The\nproposed plug-and-play framework interfaces with any open-vocabulary detector\nwhile extending their inherent capabilities across architectures. At its core,\nour approach combines (i) a symbolic regression mechanism exploring\nrelationship patterns among detected entities and (ii) a LLM-guided\nstrategically guiding the search toward meaningful expressions. These\ndiscovered symbolic rules transform low-level visual perception into\ninterpretable event understanding, providing a transparent reasoning path from\nobjects to events with strong transferability across domains.We compared our\ntraining-free framework against specialized event recognition systems across\ndiverse application domains. Experiments demonstrate that our framework\nenhances multiple object detector architectures to recognize complex events\nsuch as illegal fishing activities (75% AUROC, +8.36% improvement),\nconstruction safety violations (+15.77%), and abnormal crowd behaviors\n(+23.16%). The code will be released soon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Our key innovation lies in bridging the semantic gap between object detection\nand event understanding without requiring expensive task-specific training. The\nproposed plug-and-play framework interfaces with any open-vocabulary detector\nwhile extending their inherent capabilities across architectures. At its core,\nour approach combines (i) a symbolic regression mechanism exploring\nrelationship patterns among detected entities and (ii) a LLM-guided\nstrategically guiding the search toward meaningful expressions. These\ndiscovered symbolic rules transform low-level visual perception into\ninterpretable event understanding, providing a transparent reasoning path from\nobjects to events with strong transferability across domains.We compared our\ntraining-free framework against specialized event recognition systems across\ndiverse application domains. Experiments demonstrate that our framework\nenhances multiple object detector architectures to recognize complex events\nsuch as illegal fishing activities (75% AUROC, +8.36% improvement),\nconstruction safety violations (+15.77%), and abnormal crowd behaviors\n(+23.16%). The code will be released soon."
                },
                "authors": [
                    {
                        "name": "Yuhui Zeng"
                    },
                    {
                        "name": "Haoxiang Wu"
                    },
                    {
                        "name": "Wenjie Nie"
                    },
                    {
                        "name": "Xiawu Zheng"
                    },
                    {
                        "name": "Guangyao Chen"
                    },
                    {
                        "name": "Yunhang Shen"
                    },
                    {
                        "name": "Jun Peng"
                    },
                    {
                        "name": "Yonghong Tian"
                    },
                    {
                        "name": "Rongrong Ji"
                    }
                ],
                "author_detail": {
                    "name": "Rongrong Ji"
                },
                "author": "Rongrong Ji",
                "arxiv_comment": "13 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05843v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05843v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.14280v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.14280v5",
                "updated": "2025-03-24T12:14:43Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    12,
                    14,
                    43,
                    0,
                    83,
                    0
                ],
                "published": "2024-03-21T10:39:44Z",
                "published_parsed": [
                    2024,
                    3,
                    21,
                    10,
                    39,
                    44,
                    3,
                    81,
                    0
                ],
                "title": "Large Language Models for Blockchain Security: A Systematic Literature\n  Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models for Blockchain Security: A Systematic Literature\n  Review"
                },
                "summary": "Large Language Models (LLMs) have emerged as powerful tools across various\ndomains within cyber security. Notably, recent studies are increasingly\nexploring LLMs applied to the context of blockchain security (BS). However,\nthere remains a gap in a comprehensive understanding regarding the full scope\nof applications, impacts, and potential constraints of LLMs on blockchain\nsecurity. To fill this gap, we undertake a literature review focusing on the\nstudies that apply LLMs in blockchain security (LLM4BS).\n  Our study aims to comprehensively analyze and understand existing research,\nand elucidate how LLMs contribute to enhancing the security of blockchain\nsystems. Through a thorough examination of existing literature, we delve into\nthe integration of LLMs into various aspects of blockchain security. We explore\nthe mechanisms through which LLMs can bolster blockchain security, including\ntheir applications in smart contract auditing, transaction anomaly detection,\nvulnerability repair, program analysis of smart contracts, and serving as\nparticipants in the cryptocurrency community. Furthermore, we assess the\nchallenges and limitations associated with leveraging LLMs for enhancing\nblockchain security, considering factors such as scalability, privacy concerns,\nand ethical concerns. Our thorough review sheds light on the opportunities and\npotential risks of tasks on LLM4BS, providing valuable insights for\nresearchers, practitioners, and policymakers alike.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have emerged as powerful tools across various\ndomains within cyber security. Notably, recent studies are increasingly\nexploring LLMs applied to the context of blockchain security (BS). However,\nthere remains a gap in a comprehensive understanding regarding the full scope\nof applications, impacts, and potential constraints of LLMs on blockchain\nsecurity. To fill this gap, we undertake a literature review focusing on the\nstudies that apply LLMs in blockchain security (LLM4BS).\n  Our study aims to comprehensively analyze and understand existing research,\nand elucidate how LLMs contribute to enhancing the security of blockchain\nsystems. Through a thorough examination of existing literature, we delve into\nthe integration of LLMs into various aspects of blockchain security. We explore\nthe mechanisms through which LLMs can bolster blockchain security, including\ntheir applications in smart contract auditing, transaction anomaly detection,\nvulnerability repair, program analysis of smart contracts, and serving as\nparticipants in the cryptocurrency community. Furthermore, we assess the\nchallenges and limitations associated with leveraging LLMs for enhancing\nblockchain security, considering factors such as scalability, privacy concerns,\nand ethical concerns. Our thorough review sheds light on the opportunities and\npotential risks of tasks on LLM4BS, providing valuable insights for\nresearchers, practitioners, and policymakers alike."
                },
                "authors": [
                    {
                        "name": "Zheyuan He"
                    },
                    {
                        "name": "Zihao Li"
                    },
                    {
                        "name": "Sen Yang"
                    },
                    {
                        "name": "He Ye"
                    },
                    {
                        "name": "Ao Qiao"
                    },
                    {
                        "name": "Xiaosong Zhang"
                    },
                    {
                        "name": "Xiapu Luo"
                    },
                    {
                        "name": "Ting Chen"
                    }
                ],
                "author_detail": {
                    "name": "Ting Chen"
                },
                "author": "Ting Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.14280v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.14280v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.13002v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.13002v4",
                "updated": "2025-03-24T12:10:38Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    12,
                    10,
                    38,
                    0,
                    83,
                    0
                ],
                "published": "2024-03-13T02:53:36Z",
                "published_parsed": [
                    2024,
                    3,
                    13,
                    2,
                    53,
                    36,
                    2,
                    73,
                    0
                ],
                "title": "AutoTRIZ: Automating Engineering Innovation with TRIZ and Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoTRIZ: Automating Engineering Innovation with TRIZ and Large Language\n  Models"
                },
                "summary": "Various ideation methods, such as morphological analysis and\ndesign-by-analogy, have been developed to aid creative problem-solving and\ninnovation. Among them, the Theory of Inventive Problem Solving (TRIZ) stands\nout as one of the best-known methods. However, the complexity of TRIZ and its\nreliance on users' knowledge, experience, and reasoning capabilities limit its\npracticality. To address this, we introduce AutoTRIZ, an artificial ideation\nsystem that integrates Large Language Models (LLMs) to automate and enhance the\nTRIZ methodology. By leveraging LLMs' vast pre-trained knowledge and advanced\nreasoning capabilities, AutoTRIZ offers a novel, generative, and interpretable\napproach to engineering innovation. AutoTRIZ takes a problem statement from the\nuser as its initial input, automatically conduct the TRIZ reasoning process and\ngenerates a structured solution report. We demonstrate and evaluate the\neffectiveness of AutoTRIZ through comparative experiments with textbook cases\nand a real-world application in the design of a Battery Thermal Management\nSystem (BTMS). Moreover, the proposed LLM-based framework holds the potential\nfor extension to automate other knowledge-based ideation methods, such as\nSCAMPER, Design Heuristics, and Design-by-Analogy, paving the way for a new era\nof AI-driven innovation tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Various ideation methods, such as morphological analysis and\ndesign-by-analogy, have been developed to aid creative problem-solving and\ninnovation. Among them, the Theory of Inventive Problem Solving (TRIZ) stands\nout as one of the best-known methods. However, the complexity of TRIZ and its\nreliance on users' knowledge, experience, and reasoning capabilities limit its\npracticality. To address this, we introduce AutoTRIZ, an artificial ideation\nsystem that integrates Large Language Models (LLMs) to automate and enhance the\nTRIZ methodology. By leveraging LLMs' vast pre-trained knowledge and advanced\nreasoning capabilities, AutoTRIZ offers a novel, generative, and interpretable\napproach to engineering innovation. AutoTRIZ takes a problem statement from the\nuser as its initial input, automatically conduct the TRIZ reasoning process and\ngenerates a structured solution report. We demonstrate and evaluate the\neffectiveness of AutoTRIZ through comparative experiments with textbook cases\nand a real-world application in the design of a Battery Thermal Management\nSystem (BTMS). Moreover, the proposed LLM-based framework holds the potential\nfor extension to automate other knowledge-based ideation methods, such as\nSCAMPER, Design Heuristics, and Design-by-Analogy, paving the way for a new era\nof AI-driven innovation tools."
                },
                "authors": [
                    {
                        "name": "Shuo Jiang"
                    },
                    {
                        "name": "Weifeng Li"
                    },
                    {
                        "name": "Yuping Qian"
                    },
                    {
                        "name": "Yangjun Zhang"
                    },
                    {
                        "name": "Jianxi Luo"
                    }
                ],
                "author_detail": {
                    "name": "Jianxi Luo"
                },
                "author": "Jianxi Luo",
                "arxiv_comment": "28 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.13002v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.13002v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; I.2.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18599v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18599v1",
                "updated": "2025-03-24T11:56:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    11,
                    56,
                    50,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T11:56:50Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    11,
                    56,
                    50,
                    0,
                    83,
                    0
                ],
                "title": "Oaken: Fast and Efficient LLM Serving with Online-Offline Hybrid KV\n  Cache Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Oaken: Fast and Efficient LLM Serving with Online-Offline Hybrid KV\n  Cache Quantization"
                },
                "summary": "Modern Large Language Model serving system batches multiple requests to\nachieve high throughput, while batching attention operations is challenging,\nrendering memory bandwidth a critical bottleneck. The community relies on\nhigh-end GPUs with multiple high-bandwidth memory channels. Unfortunately,\nHBM's high bandwidth often comes at the expense of limited memory capacity,\nwhich reduces core utilization and increases costs. Recent advancements\nenabling longer contexts for LLMs have substantially increased the key-value\ncache size, further intensifying the pressures on memory capacity. The\nliterature has explored KV cache quantization techniques, which commonly use\nlow bitwidth for most values, selectively using higher bitwidth for outlier\nvalues. While this approach helps achieve high accuracy and low bitwidth\nsimultaneously, it comes with the limitation that cost for online outlier\ndetection is excessively high, negating the advantages. We propose Oaken, an\nacceleration solution that achieves high accuracy and high performance\nsimultaneously through co-designing algorithm and hardware. To effectively find\na sweet spot in the accuracy-performance trade-off space of KV cache\nquantization, Oaken employs an online-offline hybrid approach, setting outlier\nthresholds offline, which are then used to determine the quantization scale\nonline. To translate the proposed algorithmic technique into tangible\nperformance gains, Oaken also comes with custom quantization engines and memory\nmanagement units that can be integrated with any LLM accelerators. We built an\nOaken accelerator on top of an LLM accelerator, LPU, and conducted a\ncomprehensive evaluation. Our experiments show that for a batch size of 256,\nOaken achieves up to 1.58x throughput improvement over NVIDIA A100 GPU,\nincurring a minimal accuracy loss of only 0.54\\% on average, compared to\nstate-of-the-art KV cache quantization techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern Large Language Model serving system batches multiple requests to\nachieve high throughput, while batching attention operations is challenging,\nrendering memory bandwidth a critical bottleneck. The community relies on\nhigh-end GPUs with multiple high-bandwidth memory channels. Unfortunately,\nHBM's high bandwidth often comes at the expense of limited memory capacity,\nwhich reduces core utilization and increases costs. Recent advancements\nenabling longer contexts for LLMs have substantially increased the key-value\ncache size, further intensifying the pressures on memory capacity. The\nliterature has explored KV cache quantization techniques, which commonly use\nlow bitwidth for most values, selectively using higher bitwidth for outlier\nvalues. While this approach helps achieve high accuracy and low bitwidth\nsimultaneously, it comes with the limitation that cost for online outlier\ndetection is excessively high, negating the advantages. We propose Oaken, an\nacceleration solution that achieves high accuracy and high performance\nsimultaneously through co-designing algorithm and hardware. To effectively find\na sweet spot in the accuracy-performance trade-off space of KV cache\nquantization, Oaken employs an online-offline hybrid approach, setting outlier\nthresholds offline, which are then used to determine the quantization scale\nonline. To translate the proposed algorithmic technique into tangible\nperformance gains, Oaken also comes with custom quantization engines and memory\nmanagement units that can be integrated with any LLM accelerators. We built an\nOaken accelerator on top of an LLM accelerator, LPU, and conducted a\ncomprehensive evaluation. Our experiments show that for a batch size of 256,\nOaken achieves up to 1.58x throughput improvement over NVIDIA A100 GPU,\nincurring a minimal accuracy loss of only 0.54\\% on average, compared to\nstate-of-the-art KV cache quantization techniques."
                },
                "authors": [
                    {
                        "name": "Minsu Kim"
                    },
                    {
                        "name": "Seongmin Hong"
                    },
                    {
                        "name": "RyeoWook Ko"
                    },
                    {
                        "name": "Soongyu Choi"
                    },
                    {
                        "name": "Hunjong Lee"
                    },
                    {
                        "name": "Junsoo Kim"
                    },
                    {
                        "name": "Joo-Young Kim"
                    },
                    {
                        "name": "Jongse Park"
                    }
                ],
                "author_detail": {
                    "name": "Jongse Park"
                },
                "author": "Jongse Park",
                "arxiv_comment": "15 pages, 14 figures, and 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18599v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18599v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18597v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18597v1",
                "updated": "2025-03-24T11:55:35Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    11,
                    55,
                    35,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T11:55:35Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    11,
                    55,
                    35,
                    0,
                    83,
                    0
                ],
                "title": "Regression Testing with a Natural Language Oracle",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Regression Testing with a Natural Language Oracle"
                },
                "summary": "As software is evolving, code changes can introduce regression bugs or affect\nthe behavior in other unintended ways. Traditional regression test generation\nis impractical for detecting unintended behavioral changes, because it reports\nall behavioral differences as potential regressions. However, most code changes\nare intended to change the behavior in some way, e.g., to fix a bug or to add a\nnew feature. This paper presents Testora, an automated approach that detects\nregressions by comparing the intentions of a code change against behavioral\ndifferences caused by the code change. Given a pull request (PR), Testora\nqueries an LLM to generate tests that exercise the modified code, compares the\nbehavior of the original and modified code, and classifies any behavioral\ndifferences as intended or unintended. For the classification, we present an\nLLM-based technique that leverages the natural language information associated\nwith the PR, such as the title, description, and commit messages -- effectively\nproviding a natural language oracle for regression testing. Applying Testora to\nPRs of complex and popular Python projects, we find 19 regression bugs and 11\nPRs that, despite having another intention, coincidentally fix a bug. Out of 13\nregressions reported to the developers, 10 have been confirmed and 8 have\nalready been fixed. The costs of using Testora are acceptable for real-world\ndeployment, with 12.3 minutes to check a PR and LLM costs of only $0.003 per\nPR. We envision our approach to be used before or shortly after a code change\ngets merged into a code base, providing a way to early on detect regressions\nthat are not caught by traditional approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As software is evolving, code changes can introduce regression bugs or affect\nthe behavior in other unintended ways. Traditional regression test generation\nis impractical for detecting unintended behavioral changes, because it reports\nall behavioral differences as potential regressions. However, most code changes\nare intended to change the behavior in some way, e.g., to fix a bug or to add a\nnew feature. This paper presents Testora, an automated approach that detects\nregressions by comparing the intentions of a code change against behavioral\ndifferences caused by the code change. Given a pull request (PR), Testora\nqueries an LLM to generate tests that exercise the modified code, compares the\nbehavior of the original and modified code, and classifies any behavioral\ndifferences as intended or unintended. For the classification, we present an\nLLM-based technique that leverages the natural language information associated\nwith the PR, such as the title, description, and commit messages -- effectively\nproviding a natural language oracle for regression testing. Applying Testora to\nPRs of complex and popular Python projects, we find 19 regression bugs and 11\nPRs that, despite having another intention, coincidentally fix a bug. Out of 13\nregressions reported to the developers, 10 have been confirmed and 8 have\nalready been fixed. The costs of using Testora are acceptable for real-world\ndeployment, with 12.3 minutes to check a PR and LLM costs of only $0.003 per\nPR. We envision our approach to be used before or shortly after a code change\ngets merged into a code base, providing a way to early on detect regressions\nthat are not caught by traditional approaches."
                },
                "authors": [
                    {
                        "name": "Michael Pradel"
                    }
                ],
                "author_detail": {
                    "name": "Michael Pradel"
                },
                "author": "Michael Pradel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18597v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18597v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18596v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18596v1",
                "updated": "2025-03-24T11:53:06Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    11,
                    53,
                    6,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T11:53:06Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    11,
                    53,
                    6,
                    0,
                    83,
                    0
                ],
                "title": "LinkAlign: Scalable Schema Linking for Real-World Large-Scale\n  Multi-Database Text-to-SQL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LinkAlign: Scalable Schema Linking for Real-World Large-Scale\n  Multi-Database Text-to-SQL"
                },
                "summary": "Schema linking is a critical bottleneck in achieving human-level performance\nin Text-to-SQL tasks, particularly in real-world large-scale multi-database\nscenarios. Addressing schema linking faces two major challenges: (1) Database\nRetrieval: selecting the correct database from a large schema pool in\nmulti-database settings, while filtering out irrelevant ones. (2) Schema Item\nGrounding: accurately identifying the relevant tables and columns from within a\nlarge and redundant schema for SQL generation. To address this, we introduce\nLinkAlign, a novel framework that can effectively adapt existing baselines to\nreal-world environments by systematically addressing schema linking. Our\nframework comprises three key steps: multi-round semantic enhanced retrieval\nand irrelevant information isolation for Challenge 1, and schema extraction\nenhancement for Challenge 2. We evaluate our method performance of schema\nlinking on the SPIDER and BIRD benchmarks, and the ability to adapt existing\nText-to-SQL models to real-world environments on the SPIDER 2.0-lite benchmark.\nExperiments show that LinkAlign outperforms existing baselines in\nmulti-database settings, demonstrating its effectiveness and robustness. On the\nother hand, our method ranks highest among models excluding those using long\nchain-of-thought reasoning LLMs. This work bridges the gap between current\nresearch and real-world scenarios, providing a practical solution for robust\nand scalable schema linking. The codes are available at\nhttps://github.com/Satissss/LinkAlign.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Schema linking is a critical bottleneck in achieving human-level performance\nin Text-to-SQL tasks, particularly in real-world large-scale multi-database\nscenarios. Addressing schema linking faces two major challenges: (1) Database\nRetrieval: selecting the correct database from a large schema pool in\nmulti-database settings, while filtering out irrelevant ones. (2) Schema Item\nGrounding: accurately identifying the relevant tables and columns from within a\nlarge and redundant schema for SQL generation. To address this, we introduce\nLinkAlign, a novel framework that can effectively adapt existing baselines to\nreal-world environments by systematically addressing schema linking. Our\nframework comprises three key steps: multi-round semantic enhanced retrieval\nand irrelevant information isolation for Challenge 1, and schema extraction\nenhancement for Challenge 2. We evaluate our method performance of schema\nlinking on the SPIDER and BIRD benchmarks, and the ability to adapt existing\nText-to-SQL models to real-world environments on the SPIDER 2.0-lite benchmark.\nExperiments show that LinkAlign outperforms existing baselines in\nmulti-database settings, demonstrating its effectiveness and robustness. On the\nother hand, our method ranks highest among models excluding those using long\nchain-of-thought reasoning LLMs. This work bridges the gap between current\nresearch and real-world scenarios, providing a practical solution for robust\nand scalable schema linking. The codes are available at\nhttps://github.com/Satissss/LinkAlign."
                },
                "authors": [
                    {
                        "name": "Yihan Wang"
                    },
                    {
                        "name": "Peiyu Liu"
                    },
                    {
                        "name": "Xin Yang"
                    }
                ],
                "author_detail": {
                    "name": "Xin Yang"
                },
                "author": "Xin Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18596v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18596v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.10819v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.10819v2",
                "updated": "2025-03-24T11:46:14Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    11,
                    46,
                    14,
                    0,
                    83,
                    0
                ],
                "published": "2024-06-16T06:56:53Z",
                "published_parsed": [
                    2024,
                    6,
                    16,
                    6,
                    56,
                    53,
                    6,
                    168,
                    0
                ],
                "title": "GUI-World: A Video Benchmark and Dataset for Multimodal GUI-oriented\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GUI-World: A Video Benchmark and Dataset for Multimodal GUI-oriented\n  Understanding"
                },
                "summary": "Recently, Multimodal Large Language Models (MLLMs) have been used as agents\nto control keyboard and mouse inputs by directly perceiving the Graphical User\nInterface (GUI) and generating corresponding commands. However, current agents\nprimarily demonstrate strong understanding capabilities in static environments\nand are mainly applied to relatively simple domains, such as Web or mobile\ninterfaces. We argue that a robust GUI agent should be capable of perceiving\ntemporal information on the GUI, including dynamic Web content and multi-step\ntasks. Additionally, it should possess a comprehensive understanding of various\nGUI scenarios, including desktop software and multi-window interactions. To\nthis end, this paper introduces a new dataset, termed GUI-World, which features\nmeticulously crafted Human-MLLM annotations, extensively covering six GUI\nscenarios and eight types of GUI-oriented questions in three formats. We\nevaluate the capabilities of current state-of-the-art MLLMs, including Image\nLLMs and Video LLMs, in understanding various types of GUI content, especially\ndynamic and sequential content. Our findings reveal that current models\nstruggle with dynamic GUI content without manually annotated keyframes or\noperation history. On the other hand, Video LLMs fall short in all GUI-oriented\ntasks given the sparse GUI video dataset. Therefore, we take the initial step\nof leveraging a fine-tuned Video LLM, GUI-Vid, as a GUI-oriented assistant,\ndemonstrating an improved understanding of various GUI tasks. However, due to\nthe limitations in the performance of base LLMs, we conclude that using video\nLLMs as GUI agents remains a significant challenge. We believe our work\nprovides valuable insights for future research in dynamic GUI content\nunderstanding. All the dataset and code are publicly available at:\nhttps://gui-world.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, Multimodal Large Language Models (MLLMs) have been used as agents\nto control keyboard and mouse inputs by directly perceiving the Graphical User\nInterface (GUI) and generating corresponding commands. However, current agents\nprimarily demonstrate strong understanding capabilities in static environments\nand are mainly applied to relatively simple domains, such as Web or mobile\ninterfaces. We argue that a robust GUI agent should be capable of perceiving\ntemporal information on the GUI, including dynamic Web content and multi-step\ntasks. Additionally, it should possess a comprehensive understanding of various\nGUI scenarios, including desktop software and multi-window interactions. To\nthis end, this paper introduces a new dataset, termed GUI-World, which features\nmeticulously crafted Human-MLLM annotations, extensively covering six GUI\nscenarios and eight types of GUI-oriented questions in three formats. We\nevaluate the capabilities of current state-of-the-art MLLMs, including Image\nLLMs and Video LLMs, in understanding various types of GUI content, especially\ndynamic and sequential content. Our findings reveal that current models\nstruggle with dynamic GUI content without manually annotated keyframes or\noperation history. On the other hand, Video LLMs fall short in all GUI-oriented\ntasks given the sparse GUI video dataset. Therefore, we take the initial step\nof leveraging a fine-tuned Video LLM, GUI-Vid, as a GUI-oriented assistant,\ndemonstrating an improved understanding of various GUI tasks. However, due to\nthe limitations in the performance of base LLMs, we conclude that using video\nLLMs as GUI agents remains a significant challenge. We believe our work\nprovides valuable insights for future research in dynamic GUI content\nunderstanding. All the dataset and code are publicly available at:\nhttps://gui-world.github.io."
                },
                "authors": [
                    {
                        "name": "Dongping Chen"
                    },
                    {
                        "name": "Yue Huang"
                    },
                    {
                        "name": "Siyuan Wu"
                    },
                    {
                        "name": "Jingyu Tang"
                    },
                    {
                        "name": "Liuyi Chen"
                    },
                    {
                        "name": "Yilin Bai"
                    },
                    {
                        "name": "Zhigang He"
                    },
                    {
                        "name": "Chenlong Wang"
                    },
                    {
                        "name": "Huichi Zhou"
                    },
                    {
                        "name": "Yiqiang Li"
                    },
                    {
                        "name": "Tianshuo Zhou"
                    },
                    {
                        "name": "Yue Yu"
                    },
                    {
                        "name": "Chujie Gao"
                    },
                    {
                        "name": "Qihui Zhang"
                    },
                    {
                        "name": "Yi Gui"
                    },
                    {
                        "name": "Zhen Li"
                    },
                    {
                        "name": "Yao Wan"
                    },
                    {
                        "name": "Pan Zhou"
                    },
                    {
                        "name": "Jianfeng Gao"
                    },
                    {
                        "name": "Lichao Sun"
                    }
                ],
                "author_detail": {
                    "name": "Lichao Sun"
                },
                "author": "Lichao Sun",
                "arxiv_comment": "Accepted by ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.10819v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.10819v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.03387v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.03387v3",
                "updated": "2025-03-24T11:44:59Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    11,
                    44,
                    59,
                    0,
                    83,
                    0
                ],
                "published": "2024-07-03T08:36:13Z",
                "published_parsed": [
                    2024,
                    7,
                    3,
                    8,
                    36,
                    13,
                    2,
                    185,
                    0
                ],
                "title": "ConCodeEval: Evaluating Large Language Models for Code Constraints in\n  Domain-Specific Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ConCodeEval: Evaluating Large Language Models for Code Constraints in\n  Domain-Specific Languages"
                },
                "summary": "Recent work shows Large Language Models (LLMs) struggle to understand natural\nlanguage constraints for various text generation tasks in zero- and few-shot\nsettings. While, in the code domain, there is wide usage of constraints in code\nformat to maintain the integrity of code written in Domain-Specific Languages\n(DSLs) like JSON and YAML which are widely used for system-level programming\ntasks in enterprises. Given that LLMs are increasingly used for system-level\ncode tasks, evaluating if they can comprehend these code constraints is\ncrucial. However, no work has been done to evaluate their controllability over\ncode constraints. Hence, we introduce ConCodeEval, a first-of-its-kind\nbenchmark having two novel tasks for code constraints across five\nrepresentations. Our findings suggest that language models struggle with code\nconstraints. Code languages that perform excellently for normal code tasks do\nnot perform well when the same languages represent fine-grained constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent work shows Large Language Models (LLMs) struggle to understand natural\nlanguage constraints for various text generation tasks in zero- and few-shot\nsettings. While, in the code domain, there is wide usage of constraints in code\nformat to maintain the integrity of code written in Domain-Specific Languages\n(DSLs) like JSON and YAML which are widely used for system-level programming\ntasks in enterprises. Given that LLMs are increasingly used for system-level\ncode tasks, evaluating if they can comprehend these code constraints is\ncrucial. However, no work has been done to evaluate their controllability over\ncode constraints. Hence, we introduce ConCodeEval, a first-of-its-kind\nbenchmark having two novel tasks for code constraints across five\nrepresentations. Our findings suggest that language models struggle with code\nconstraints. Code languages that perform excellently for normal code tasks do\nnot perform well when the same languages represent fine-grained constraints."
                },
                "authors": [
                    {
                        "name": "Mehant Kammakomati"
                    },
                    {
                        "name": "Sameer Pimparkhede"
                    },
                    {
                        "name": "Srikanth Tamilselvam"
                    },
                    {
                        "name": "Prince Kumar"
                    },
                    {
                        "name": "Pushpak Bhattacharyya"
                    }
                ],
                "author_detail": {
                    "name": "Pushpak Bhattacharyya"
                },
                "author": "Pushpak Bhattacharyya",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.03387v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.03387v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11702v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11702v3",
                "updated": "2025-03-24T11:42:16Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    11,
                    42,
                    16,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-12T09:32:43Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    9,
                    32,
                    43,
                    2,
                    71,
                    0
                ],
                "title": "Toward a method for LLM-enabled Indoor Navigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toward a method for LLM-enabled Indoor Navigation"
                },
                "summary": "Indoor navigation presents unique challenges due to complex layouts, lack of\nGPS signals, and accessibility concerns. Existing solutions often struggle with\nreal-time adaptability and user-specific needs. In this work, we explore the\npotential of a Large Language Model (LLM), i.e., ChatGPT, to generate natural,\ncontext-aware navigation instructions from indoor map images. We design and\nevaluate test cases across different real-world environments, analyzing the\neffectiveness of LLMs in interpreting spatial layouts, handling user\nconstraints, and planning efficient routes. Our findings demonstrate the\npotential of LLMs for supporting personalized indoor navigation, with an\naverage of 50.54% correct indications and a maximum of 77.78%. The results do\nnot appear to depend on the complexity of the layout or the complexity of the\nexpected path, but rather on the number of points of interest and the abundance\nof visual information, which negatively affect the performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Indoor navigation presents unique challenges due to complex layouts, lack of\nGPS signals, and accessibility concerns. Existing solutions often struggle with\nreal-time adaptability and user-specific needs. In this work, we explore the\npotential of a Large Language Model (LLM), i.e., ChatGPT, to generate natural,\ncontext-aware navigation instructions from indoor map images. We design and\nevaluate test cases across different real-world environments, analyzing the\neffectiveness of LLMs in interpreting spatial layouts, handling user\nconstraints, and planning efficient routes. Our findings demonstrate the\npotential of LLMs for supporting personalized indoor navigation, with an\naverage of 50.54% correct indications and a maximum of 77.78%. The results do\nnot appear to depend on the complexity of the layout or the complexity of the\nexpected path, but rather on the number of points of interest and the abundance\nof visual information, which negatively affect the performance."
                },
                "authors": [
                    {
                        "name": "Alberto Coffrini"
                    },
                    {
                        "name": "Mohammad Amin Zadenoori"
                    },
                    {
                        "name": "Paolo Barsocchi"
                    },
                    {
                        "name": "Francesco Furfari"
                    },
                    {
                        "name": "Antonino Crivello"
                    },
                    {
                        "name": "Alessio Ferrari"
                    }
                ],
                "author_detail": {
                    "name": "Alessio Ferrari"
                },
                "author": "Alessio Ferrari",
                "arxiv_comment": "7 pages, 3 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11702v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11702v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18568v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18568v1",
                "updated": "2025-03-24T11:24:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    11,
                    24,
                    37,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T11:24:37Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    11,
                    24,
                    37,
                    0,
                    83,
                    0
                ],
                "title": "A generalisable data-augmented turbulence model with progressive and\n  interpretable corrections",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A generalisable data-augmented turbulence model with progressive and\n  interpretable corrections"
                },
                "summary": "The integration of interpretability and generalisability in data-driven\nturbulence modelling remains a fundamental challenge for computational fluid\ndynamics applications. This study yields a generalisable advancement of the\n$k$-$\\omega$ Shear Stress Transport (SST) model through a progressive\ndata-augmented framework, combining Bayesian optimisation with physics-guided\ncorrections to improve the predictions of anisotropy-induced secondary flows\nand flow separation simultaneously. Two interpretable modifications are\nsystematically embedded: 1) a non-linear Reynolds stress anisotropy correction\nto enhance secondary flow predictions, and 2) an activation-based separation\ncorrection in the $omega$-equation, regulated by an optimised power-law\nfunction to locally adjust turbulent viscosity under adverse pressure\ngradients. The model is trained using a multi-case computational fluid\ndynamics-driven \\textit{a posteriori} approach, incorporating periodic hills,\nduct flow, and channel flow to balance correction efficacy with baseline\nconsistency. Validation across multiple unseen cases -- spanning flat-plate\nboundary layers, high-Reynolds-number periodic hills, and flow over diverse\nobstacle configurations -- demonstrates enhanced accuracy in velocity profiles,\nrecirculation zones, streamwise vorticity, and skin friction distributions\nwhile retaining the robustness of the original $k$-$\\omega$ SST in attached\nflows. Sparsity-enforced regression ensures reduced parametric complexity,\npreserving computational efficiency and physical transparency. Results\nunderscore the framework's ability to generalise across geometries and Reynolds\nnumbers without destabilising corrections, offering a validated framework\ntoward deployable, data-augmented turbulence models for numerical simulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of interpretability and generalisability in data-driven\nturbulence modelling remains a fundamental challenge for computational fluid\ndynamics applications. This study yields a generalisable advancement of the\n$k$-$\\omega$ Shear Stress Transport (SST) model through a progressive\ndata-augmented framework, combining Bayesian optimisation with physics-guided\ncorrections to improve the predictions of anisotropy-induced secondary flows\nand flow separation simultaneously. Two interpretable modifications are\nsystematically embedded: 1) a non-linear Reynolds stress anisotropy correction\nto enhance secondary flow predictions, and 2) an activation-based separation\ncorrection in the $omega$-equation, regulated by an optimised power-law\nfunction to locally adjust turbulent viscosity under adverse pressure\ngradients. The model is trained using a multi-case computational fluid\ndynamics-driven \\textit{a posteriori} approach, incorporating periodic hills,\nduct flow, and channel flow to balance correction efficacy with baseline\nconsistency. Validation across multiple unseen cases -- spanning flat-plate\nboundary layers, high-Reynolds-number periodic hills, and flow over diverse\nobstacle configurations -- demonstrates enhanced accuracy in velocity profiles,\nrecirculation zones, streamwise vorticity, and skin friction distributions\nwhile retaining the robustness of the original $k$-$\\omega$ SST in attached\nflows. Sparsity-enforced regression ensures reduced parametric complexity,\npreserving computational efficiency and physical transparency. Results\nunderscore the framework's ability to generalise across geometries and Reynolds\nnumbers without destabilising corrections, offering a validated framework\ntoward deployable, data-augmented turbulence models for numerical simulations."
                },
                "authors": [
                    {
                        "name": "Mario J. Rincn"
                    },
                    {
                        "name": "Martino Reclari"
                    },
                    {
                        "name": "Xiang I. A. Yang"
                    },
                    {
                        "name": "Mahdi Abkar"
                    }
                ],
                "author_detail": {
                    "name": "Mahdi Abkar"
                },
                "author": "Mahdi Abkar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18568v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18568v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.flu-dyn",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.flu-dyn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18565v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18565v1",
                "updated": "2025-03-24T11:18:25Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    11,
                    18,
                    25,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T11:18:25Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    11,
                    18,
                    25,
                    0,
                    83,
                    0
                ],
                "title": "Distil-xLSTM: Learning Attention Mechanisms through Recurrent Structures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distil-xLSTM: Learning Attention Mechanisms through Recurrent Structures"
                },
                "summary": "The current era of Natural Language Processing (NLP) is dominated by\nTransformer models. However, novel architectures relying on recurrent\nmechanisms, such as xLSTM and Mamba, have been proposed as alternatives to\nattention-based models. Although computation is done differently than with the\nattention mechanism mechanism, these recurrent models yield good results and\nsometimes even outperform state-of-the-art attention-based models. In this\nwork, we propose Distil-xLSTM, an xLSTM-based Small Language Model (SLM)\ntrained by distilling knowledge from a Large Language Model (LLM) that shows\npromising results while being compute and scale efficient. Our Distil-xLSTM\nfocuses on approximating a transformer-based model attention parametrization\nusing its recurrent sequence mixing components and shows good results with\nminimal training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The current era of Natural Language Processing (NLP) is dominated by\nTransformer models. However, novel architectures relying on recurrent\nmechanisms, such as xLSTM and Mamba, have been proposed as alternatives to\nattention-based models. Although computation is done differently than with the\nattention mechanism mechanism, these recurrent models yield good results and\nsometimes even outperform state-of-the-art attention-based models. In this\nwork, we propose Distil-xLSTM, an xLSTM-based Small Language Model (SLM)\ntrained by distilling knowledge from a Large Language Model (LLM) that shows\npromising results while being compute and scale efficient. Our Distil-xLSTM\nfocuses on approximating a transformer-based model attention parametrization\nusing its recurrent sequence mixing components and shows good results with\nminimal training."
                },
                "authors": [
                    {
                        "name": "Abdoul Majid O. Thiombiano"
                    },
                    {
                        "name": "Brahim Hnich"
                    },
                    {
                        "name": "Ali Ben Mrad"
                    },
                    {
                        "name": "Mohamed Wiem Mkaouer"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed Wiem Mkaouer"
                },
                "author": "Mohamed Wiem Mkaouer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18565v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18565v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18562v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18562v1",
                "updated": "2025-03-24T11:16:41Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    11,
                    16,
                    41,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T11:16:41Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    11,
                    16,
                    41,
                    0,
                    83,
                    0
                ],
                "title": "Self-Reported Confidence of Large Language Models in Gastroenterology:\n  Analysis of Commercial, Open-Source, and Quantized Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-Reported Confidence of Large Language Models in Gastroenterology:\n  Analysis of Commercial, Open-Source, and Quantized Models"
                },
                "summary": "This study evaluated self-reported response certainty across several large\nlanguage models (GPT, Claude, Llama, Phi, Mistral, Gemini, Gemma, and Qwen)\nusing 300 gastroenterology board-style questions. The highest-performing models\n(GPT-o1 preview, GPT-4o, and Claude-3.5-Sonnet) achieved Brier scores of\n0.15-0.2 and AUROC of 0.6. Although newer models demonstrated improved\nperformance, all exhibited a consistent tendency towards overconfidence.\nUncertainty estimation presents a significant challenge to the safe use of LLMs\nin healthcare. Keywords: Large Language Models; Confidence Elicitation;\nArtificial Intelligence; Gastroenterology; Uncertainty Quantification",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study evaluated self-reported response certainty across several large\nlanguage models (GPT, Claude, Llama, Phi, Mistral, Gemini, Gemma, and Qwen)\nusing 300 gastroenterology board-style questions. The highest-performing models\n(GPT-o1 preview, GPT-4o, and Claude-3.5-Sonnet) achieved Brier scores of\n0.15-0.2 and AUROC of 0.6. Although newer models demonstrated improved\nperformance, all exhibited a consistent tendency towards overconfidence.\nUncertainty estimation presents a significant challenge to the safe use of LLMs\nin healthcare. Keywords: Large Language Models; Confidence Elicitation;\nArtificial Intelligence; Gastroenterology; Uncertainty Quantification"
                },
                "authors": [
                    {
                        "name": "Nariman Naderi"
                    },
                    {
                        "name": "Seyed Amir Ahmad Safavi-Naini"
                    },
                    {
                        "name": "Thomas Savage"
                    },
                    {
                        "name": "Zahra Atf"
                    },
                    {
                        "name": "Peter Lewis"
                    },
                    {
                        "name": "Girish Nadkarni"
                    },
                    {
                        "name": "Ali Soroush"
                    }
                ],
                "author_detail": {
                    "name": "Ali Soroush"
                },
                "author": "Ali Soroush",
                "arxiv_comment": "35 pages, 5 figures, 1 table, 7 supplementary figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18562v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18562v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18559v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18559v2",
                "updated": "2025-03-25T02:43:16Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    2,
                    43,
                    16,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-24T11:13:33Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    11,
                    13,
                    33,
                    0,
                    83,
                    0
                ],
                "title": "AMD-Hummingbird: Towards an Efficient Text-to-Video Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AMD-Hummingbird: Towards an Efficient Text-to-Video Model"
                },
                "summary": "Text-to-Video (T2V) generation has attracted significant attention for its\nability to synthesize realistic videos from textual descriptions. However,\nexisting models struggle to balance computational efficiency and high visual\nquality, particularly on resource-limited devices, e.g.,iGPUs and mobile\nphones. Most prior work prioritizes visual fidelity while overlooking the need\nfor smaller, more efficient models suitable for real-world deployment. To\naddress this challenge, we propose a lightweight T2V framework, termed\nHummingbird, which prunes existing models and enhances visual quality through\nvisual feedback learning. Our approach reduces the size of the U-Net from 1.4\nbillion to 0.7 billion parameters, significantly improving efficiency while\npreserving high-quality video generation. Additionally, we introduce a novel\ndata processing pipeline that leverages Large Language Models (LLMs) and Video\nQuality Assessment (VQA) models to enhance the quality of both text prompts and\nvideo data. To support user-driven training and style customization, we\npublicly release the full training code, including data processing and model\ntraining. Extensive experiments show that our method achieves a 31X speedup\ncompared to state-of-the-art models such as VideoCrafter2, while also attaining\nthe highest overall score on VBench. Moreover, our method supports the\ngeneration of videos with up to 26 frames, addressing the limitations of\nexisting U-Net-based methods in long video generation. Notably, the entire\ntraining process requires only four GPUs, yet delivers performance competitive\nwith existing leading methods. Hummingbird presents a practical and efficient\nsolution for T2V generation, combining high performance, scalability, and\nflexibility for real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-Video (T2V) generation has attracted significant attention for its\nability to synthesize realistic videos from textual descriptions. However,\nexisting models struggle to balance computational efficiency and high visual\nquality, particularly on resource-limited devices, e.g.,iGPUs and mobile\nphones. Most prior work prioritizes visual fidelity while overlooking the need\nfor smaller, more efficient models suitable for real-world deployment. To\naddress this challenge, we propose a lightweight T2V framework, termed\nHummingbird, which prunes existing models and enhances visual quality through\nvisual feedback learning. Our approach reduces the size of the U-Net from 1.4\nbillion to 0.7 billion parameters, significantly improving efficiency while\npreserving high-quality video generation. Additionally, we introduce a novel\ndata processing pipeline that leverages Large Language Models (LLMs) and Video\nQuality Assessment (VQA) models to enhance the quality of both text prompts and\nvideo data. To support user-driven training and style customization, we\npublicly release the full training code, including data processing and model\ntraining. Extensive experiments show that our method achieves a 31X speedup\ncompared to state-of-the-art models such as VideoCrafter2, while also attaining\nthe highest overall score on VBench. Moreover, our method supports the\ngeneration of videos with up to 26 frames, addressing the limitations of\nexisting U-Net-based methods in long video generation. Notably, the entire\ntraining process requires only four GPUs, yet delivers performance competitive\nwith existing leading methods. Hummingbird presents a practical and efficient\nsolution for T2V generation, combining high performance, scalability, and\nflexibility for real-world applications."
                },
                "authors": [
                    {
                        "name": "Takashi Isobe"
                    },
                    {
                        "name": "He Cui"
                    },
                    {
                        "name": "Dong Zhou"
                    },
                    {
                        "name": "Mengmeng Ge"
                    },
                    {
                        "name": "Dong Li"
                    },
                    {
                        "name": "Emad Barsoum"
                    }
                ],
                "author_detail": {
                    "name": "Emad Barsoum"
                },
                "author": "Emad Barsoum",
                "arxiv_comment": "Homepage:\n  https://www.amd.com/en/developer/resources/technical-articles/amd-hummingbird-0-9b-text-to-video-diffusion-model-with-4-step-inferencing.html|\n  GitHub: https://github.com/AMD-AIG-AIMA/AMD-Hummingbird-T2V",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18559v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18559v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17333v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17333v2",
                "updated": "2025-03-24T11:00:35Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    11,
                    0,
                    35,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-21T17:33:03Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    17,
                    33,
                    3,
                    4,
                    80,
                    0
                ],
                "title": "Register Dispersion: Reducing the Footprint of the Vector Register File\n  in Vector Engines of Low-Cost RISC-V CPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Register Dispersion: Reducing the Footprint of the Vector Register File\n  in Vector Engines of Low-Cost RISC-V CPUs"
                },
                "summary": "The deployment of Machine Learning (ML) applications at the edge on\nresource-constrained devices has accentuated the need for efficient ML\nprocessing on low-cost processors. While traditional CPUs provide programming\nflexibility, their general-purpose architecture often lacks the throughput\nrequired for complex ML models. The augmentation of a RISC-V processor with a\nvector unit can provide substantial data-level parallelism. However, increasing\nthe data-level parallelism supported by vector processing would make the Vector\nRegister File (VRF) a major area consumer in ultra low-cost processors, since\n32 vector registers are required for RISC-V Vector ISA compliance. This work\nleverages the insight that many ML vectorized kernels require a small number of\nactive vector registers, and proposes the use of a physically smaller VRF that\ndynamically caches only the vector registers currently accessed by the\napplication. This approach, called Register Dispersion, maps the architectural\nvector registers to a smaller set of physical registers. The proposed\nISA-compliant VRF is significantly smaller than a full-size VRF and operates\nlike a conventional cache, i.e., it only stores the most recently accessed\nvector registers. Essential registers remain readily accessible within the\ncompact VRF, while the others are offloaded to the cache/memory sub-system. The\ncompact VRF design is demonstrated to yield substantial area and power savings,\nas compared to using a full VRF, with no or minimal impact on performance. This\neffective trade-off renders the inclusion of vector units in low-cost\nprocessors feasible and practical.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of Machine Learning (ML) applications at the edge on\nresource-constrained devices has accentuated the need for efficient ML\nprocessing on low-cost processors. While traditional CPUs provide programming\nflexibility, their general-purpose architecture often lacks the throughput\nrequired for complex ML models. The augmentation of a RISC-V processor with a\nvector unit can provide substantial data-level parallelism. However, increasing\nthe data-level parallelism supported by vector processing would make the Vector\nRegister File (VRF) a major area consumer in ultra low-cost processors, since\n32 vector registers are required for RISC-V Vector ISA compliance. This work\nleverages the insight that many ML vectorized kernels require a small number of\nactive vector registers, and proposes the use of a physically smaller VRF that\ndynamically caches only the vector registers currently accessed by the\napplication. This approach, called Register Dispersion, maps the architectural\nvector registers to a smaller set of physical registers. The proposed\nISA-compliant VRF is significantly smaller than a full-size VRF and operates\nlike a conventional cache, i.e., it only stores the most recently accessed\nvector registers. Essential registers remain readily accessible within the\ncompact VRF, while the others are offloaded to the cache/memory sub-system. The\ncompact VRF design is demonstrated to yield substantial area and power savings,\nas compared to using a full VRF, with no or minimal impact on performance. This\neffective trade-off renders the inclusion of vector units in low-cost\nprocessors feasible and practical."
                },
                "authors": [
                    {
                        "name": "Vasileios Titopoulos"
                    },
                    {
                        "name": "George Alexakis"
                    },
                    {
                        "name": "Kosmas Alexandridis"
                    },
                    {
                        "name": "Chrysostomos Nicopoulos"
                    },
                    {
                        "name": "Giorgos Dimitrakopoulos"
                    }
                ],
                "author_detail": {
                    "name": "Giorgos Dimitrakopoulos"
                },
                "author": "Giorgos Dimitrakopoulos",
                "arxiv_comment": "22nd ACM International Conference on Computing Frontiers (CF' 25)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17333v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17333v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18545v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18545v1",
                "updated": "2025-03-24T10:59:23Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    10,
                    59,
                    23,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T10:59:23Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    10,
                    59,
                    23,
                    0,
                    83,
                    0
                ],
                "title": "Communication-aware planning for robot teams deployment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Communication-aware planning for robot teams deployment"
                },
                "summary": "In the present work we address the problem of deploying a team of robots in a\nscenario where some locations of interest must be reached. Thus, a planning for\na deployment is required, before sending the robots. The obstacles, the limited\ncommunication range, and the need of communicating to a base station, constrain\nthe connectivity of the team and the deployment planning. We propose a method\nconsisting of three algorithms: a distributed path planner to obtain\ncommunication-aware trajectories; a deployment planner providing dual-use of\nthe robots, visiting primary goals and performing connectivity tasks; and a\nclustering algorithm to allocate the tasks to robots, and obtain the best goal\nvisit order for the mission.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the present work we address the problem of deploying a team of robots in a\nscenario where some locations of interest must be reached. Thus, a planning for\na deployment is required, before sending the robots. The obstacles, the limited\ncommunication range, and the need of communicating to a base station, constrain\nthe connectivity of the team and the deployment planning. We propose a method\nconsisting of three algorithms: a distributed path planner to obtain\ncommunication-aware trajectories; a deployment planner providing dual-use of\nthe robots, visiting primary goals and performing connectivity tasks; and a\nclustering algorithm to allocate the tasks to robots, and obtain the best goal\nvisit order for the mission."
                },
                "authors": [
                    {
                        "name": "Yaroslav Marchukov"
                    },
                    {
                        "name": "Luis Montano"
                    }
                ],
                "author_detail": {
                    "name": "Luis Montano"
                },
                "author": "Luis Montano",
                "arxiv_doi": "10.1016/j.ifacol.2017.08.1210",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.ifacol.2017.08.1210",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.18545v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18545v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18526v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18526v1",
                "updated": "2025-03-24T10:31:31Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    10,
                    31,
                    31,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T10:31:31Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    10,
                    31,
                    31,
                    0,
                    83,
                    0
                ],
                "title": "SciClaims: An End-to-End Generative System for Biomedical Claim Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SciClaims: An End-to-End Generative System for Biomedical Claim Analysis"
                },
                "summary": "Validating key claims in scientific literature, particularly in biomedical\nresearch, is essential for ensuring accuracy and advancing knowledge. This\nprocess is critical in sectors like the pharmaceutical industry, where rapid\nscientific progress requires automation and deep domain expertise. However,\ncurrent solutions have significant limitations. They lack end-to-end pipelines\nencompassing all claim extraction, evidence retrieval, and verification steps;\nrely on complex NLP and information retrieval pipelines prone to multiple\nfailure points; and often fail to provide clear, user-friendly justifications\nfor claim verification outcomes. To address these challenges, we introduce\nSciClaims, an advanced system powered by state-of-the-art large language models\n(LLMs) that seamlessly integrates the entire scientific claim analysis process.\nSciClaims outperforms previous approaches in both claim extraction and\nverification without requiring additional fine-tuning, setting a new benchmark\nfor automated scientific claim analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Validating key claims in scientific literature, particularly in biomedical\nresearch, is essential for ensuring accuracy and advancing knowledge. This\nprocess is critical in sectors like the pharmaceutical industry, where rapid\nscientific progress requires automation and deep domain expertise. However,\ncurrent solutions have significant limitations. They lack end-to-end pipelines\nencompassing all claim extraction, evidence retrieval, and verification steps;\nrely on complex NLP and information retrieval pipelines prone to multiple\nfailure points; and often fail to provide clear, user-friendly justifications\nfor claim verification outcomes. To address these challenges, we introduce\nSciClaims, an advanced system powered by state-of-the-art large language models\n(LLMs) that seamlessly integrates the entire scientific claim analysis process.\nSciClaims outperforms previous approaches in both claim extraction and\nverification without requiring additional fine-tuning, setting a new benchmark\nfor automated scientific claim analysis."
                },
                "authors": [
                    {
                        "name": "Ral Ortega"
                    },
                    {
                        "name": "Jos Manuel Gmez-Prez"
                    }
                ],
                "author_detail": {
                    "name": "Jos Manuel Gmez-Prez"
                },
                "author": "Jos Manuel Gmez-Prez",
                "arxiv_comment": "Pre-print version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18526v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18526v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18525v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18525v1",
                "updated": "2025-03-24T10:29:47Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    10,
                    29,
                    47,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T10:29:47Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    10,
                    29,
                    47,
                    0,
                    83,
                    0
                ],
                "title": "P3Nav: A Unified Framework for Embodied Navigation Integrating\n  Perception, Planning, and Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "P3Nav: A Unified Framework for Embodied Navigation Integrating\n  Perception, Planning, and Prediction"
                },
                "summary": "In language-guided visual navigation, agents locate target objects in unseen\nenvironments using natural language instructions. For reliable navigation in\nunfamiliar scenes, agents must possess strong perception, planning, and\nprediction capabilities. Additionally, when agents revisit previously explored\nareas during long-term navigation, they may retain irrelevant and redundant\nhistorical perceptions, leading to suboptimal results. In this work, we\nintroduce \\textbf{P3Nav}, a unified framework that integrates\n\\textbf{P}erception, \\textbf{P}lanning, and \\textbf{P}rediction capabilities\nthrough \\textbf{Multitask Collaboration} on navigation and embodied question\nanswering (EQA) tasks, thereby enhancing navigation performance. Furthermore,\nP3Nav employs an \\textbf{Adaptive 3D-aware History Sampling} strategy to\neffectively and efficiently utilize historical observations. By leveraging the\nlarge language models (LLM), P3Nav comprehends diverse commands and complex\nvisual scenes, resulting in appropriate navigation actions. P3Nav achieves a\n75\\% success rate in object goal navigation on the\n$\\mathrm{CHORES}$-$\\mathbb{S}$ benchmark, setting a new state-of-the-art\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In language-guided visual navigation, agents locate target objects in unseen\nenvironments using natural language instructions. For reliable navigation in\nunfamiliar scenes, agents must possess strong perception, planning, and\nprediction capabilities. Additionally, when agents revisit previously explored\nareas during long-term navigation, they may retain irrelevant and redundant\nhistorical perceptions, leading to suboptimal results. In this work, we\nintroduce \\textbf{P3Nav}, a unified framework that integrates\n\\textbf{P}erception, \\textbf{P}lanning, and \\textbf{P}rediction capabilities\nthrough \\textbf{Multitask Collaboration} on navigation and embodied question\nanswering (EQA) tasks, thereby enhancing navigation performance. Furthermore,\nP3Nav employs an \\textbf{Adaptive 3D-aware History Sampling} strategy to\neffectively and efficiently utilize historical observations. By leveraging the\nlarge language models (LLM), P3Nav comprehends diverse commands and complex\nvisual scenes, resulting in appropriate navigation actions. P3Nav achieves a\n75\\% success rate in object goal navigation on the\n$\\mathrm{CHORES}$-$\\mathbb{S}$ benchmark, setting a new state-of-the-art\nperformance."
                },
                "authors": [
                    {
                        "name": "Yufeng Zhong"
                    },
                    {
                        "name": "Chengjian Feng"
                    },
                    {
                        "name": "Feng Yan"
                    },
                    {
                        "name": "Fanfan Liu"
                    },
                    {
                        "name": "Liming Zheng"
                    },
                    {
                        "name": "Lin Ma"
                    }
                ],
                "author_detail": {
                    "name": "Lin Ma"
                },
                "author": "Lin Ma",
                "arxiv_comment": "14 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18525v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18525v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11425v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11425v3",
                "updated": "2025-03-24T10:18:56Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    10,
                    18,
                    56,
                    0,
                    83,
                    0
                ],
                "published": "2025-01-20T11:46:04Z",
                "published_parsed": [
                    2025,
                    1,
                    20,
                    11,
                    46,
                    4,
                    0,
                    20,
                    0
                ],
                "title": "Agent-R: Training Language Model Agents to Reflect via Iterative\n  Self-Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agent-R: Training Language Model Agents to Reflect via Iterative\n  Self-Training"
                },
                "summary": "Large Language Models (LLMs) agents are increasingly pivotal for addressing\ncomplex tasks in interactive environments. Existing work mainly focuses on\nenhancing performance through behavior cloning from stronger experts, yet such\napproaches often falter in real-world applications, mainly due to the inability\nto recover from errors. However, step-level critique data is difficult and\nexpensive to collect. Automating and dynamically constructing self-critique\ndatasets is thus crucial to empowering models with intelligent agent\ncapabilities. In this work, we propose an iterative self-training framework,\nAgent-R, that enables language Agent to Reflect on the fly. Unlike traditional\nmethods that reward or penalize actions based on correctness, Agent-R leverages\nMCTS to construct training data that recover correct trajectories from\nerroneous ones. A key challenge of agent reflection lies in the necessity for\ntimely revision rather than waiting until the end of a rollout. To address\nthis, we introduce a model-guided critique construction mechanism: the actor\nmodel identifies the first error step (within its current capability) in a\nfailed trajectory. Starting from it, we splice it with the adjacent correct\npath, which shares the same parent node in the tree. This strategy enables the\nmodel to learn reflection based on its current policy, therefore yielding\nbetter learning efficiency. To further explore the scalability of this\nself-improvement paradigm, we investigate iterative refinement of both error\ncorrection capabilities and dataset construction. Our findings demonstrate that\nAgent-R continuously improves the model's ability to recover from errors and\nenables timely error correction. Experiments on three interactive environments\nshow that Agent-R effectively equips agents to correct erroneous actions while\navoiding loops, achieving superior performance compared to baseline methods\n(+5.59%).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) agents are increasingly pivotal for addressing\ncomplex tasks in interactive environments. Existing work mainly focuses on\nenhancing performance through behavior cloning from stronger experts, yet such\napproaches often falter in real-world applications, mainly due to the inability\nto recover from errors. However, step-level critique data is difficult and\nexpensive to collect. Automating and dynamically constructing self-critique\ndatasets is thus crucial to empowering models with intelligent agent\ncapabilities. In this work, we propose an iterative self-training framework,\nAgent-R, that enables language Agent to Reflect on the fly. Unlike traditional\nmethods that reward or penalize actions based on correctness, Agent-R leverages\nMCTS to construct training data that recover correct trajectories from\nerroneous ones. A key challenge of agent reflection lies in the necessity for\ntimely revision rather than waiting until the end of a rollout. To address\nthis, we introduce a model-guided critique construction mechanism: the actor\nmodel identifies the first error step (within its current capability) in a\nfailed trajectory. Starting from it, we splice it with the adjacent correct\npath, which shares the same parent node in the tree. This strategy enables the\nmodel to learn reflection based on its current policy, therefore yielding\nbetter learning efficiency. To further explore the scalability of this\nself-improvement paradigm, we investigate iterative refinement of both error\ncorrection capabilities and dataset construction. Our findings demonstrate that\nAgent-R continuously improves the model's ability to recover from errors and\nenables timely error correction. Experiments on three interactive environments\nshow that Agent-R effectively equips agents to correct erroneous actions while\navoiding loops, achieving superior performance compared to baseline methods\n(+5.59%)."
                },
                "authors": [
                    {
                        "name": "Siyu Yuan"
                    },
                    {
                        "name": "Zehui Chen"
                    },
                    {
                        "name": "Zhiheng Xi"
                    },
                    {
                        "name": "Junjie Ye"
                    },
                    {
                        "name": "Zhengyin Du"
                    },
                    {
                        "name": "Jiecao Chen"
                    }
                ],
                "author_detail": {
                    "name": "Jiecao Chen"
                },
                "author": "Jiecao Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11425v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11425v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18502v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18502v1",
                "updated": "2025-03-24T09:58:44Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    9,
                    58,
                    44,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T09:58:44Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    9,
                    58,
                    44,
                    0,
                    83,
                    0
                ],
                "title": "Autoregressive Language Models for Knowledge Base Population: A case\n  study in the space mission domain",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive Language Models for Knowledge Base Population: A case\n  study in the space mission domain"
                },
                "summary": "Knowledge base population KBP plays a crucial role in populating and\nmaintaining knowledge bases up-to-date in organizations by leveraging domain\ncorpora. Motivated by the increasingly large context windows supported by large\nlanguage models, we propose to fine-tune an autoregressive language model for\nend-toend KPB. Our case study involves the population of a space mission\nknowledge graph. To fine-tune the model we generate a dataset for end-to-end\nKBP tapping into existing domain resources. Our case study shows that\nfine-tuned language models of limited size can achieve competitive and even\nhigher accuracy than larger models in the KBP task. Smaller models specialized\nfor KBP offer affordable deployment and lower-cost inference. Moreover, KBP\nspecialist models do not require the ontology to be included in the prompt,\nallowing for more space in the context for additional input text or output\nserialization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge base population KBP plays a crucial role in populating and\nmaintaining knowledge bases up-to-date in organizations by leveraging domain\ncorpora. Motivated by the increasingly large context windows supported by large\nlanguage models, we propose to fine-tune an autoregressive language model for\nend-toend KPB. Our case study involves the population of a space mission\nknowledge graph. To fine-tune the model we generate a dataset for end-to-end\nKBP tapping into existing domain resources. Our case study shows that\nfine-tuned language models of limited size can achieve competitive and even\nhigher accuracy than larger models in the KBP task. Smaller models specialized\nfor KBP offer affordable deployment and lower-cost inference. Moreover, KBP\nspecialist models do not require the ontology to be included in the prompt,\nallowing for more space in the context for additional input text or output\nserialization."
                },
                "authors": [
                    {
                        "name": "Andrs Garca-Silva"
                    },
                    {
                        "name": "Jos Manuel Gmez-Prez"
                    }
                ],
                "author_detail": {
                    "name": "Jos Manuel Gmez-Prez"
                },
                "author": "Jos Manuel Gmez-Prez",
                "arxiv_comment": "Pre-print version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18502v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18502v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.20085v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.20085v3",
                "updated": "2025-03-24T09:58:24Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    9,
                    58,
                    24,
                    0,
                    83,
                    0
                ],
                "published": "2024-06-28T17:53:18Z",
                "published_parsed": [
                    2024,
                    6,
                    28,
                    17,
                    53,
                    18,
                    4,
                    180,
                    0
                ],
                "title": "Auto Cherry-Picker: Learning from High-quality Generative Data Driven by\n  Language",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auto Cherry-Picker: Learning from High-quality Generative Data Driven by\n  Language"
                },
                "summary": "Diffusion models can generate realistic and diverse images, potentially\nfacilitating data availability for data-intensive perception tasks. However,\nleveraging these models to boost performance on downstream tasks with synthetic\ndata poses several challenges, including aligning with real data distribution,\nscaling synthetic sample volumes, and ensuring their quality. To bridge these\ngaps, we present \\textbf{A}uto \\textbf{C}herry-\\textbf{P}icker (ACP), a novel\nframework that generates high-quality cross-modality training samples at scale\nto augment perception and multi-modal training. ACP first uses LLMs to sample\ndescriptions and layouts based on object combinations from real data priors,\neliminating the need for ground truth image captions or annotations. Next, we\nuse an off-the-shelf controllable diffusion model to generate multiple images.\nThen, the generated data are refined using a comprehensively designed metric,\nComposite Layout and Image Score (CLIS), to ensure quality. Our customized\nsynthetic high-quality samples boost performance in various scenarios,\nespecially in addressing challenges associated with long-tailed distribution\nand imbalanced datasets. Experiment results on downstream tasks demonstrate\nthat ACP can significantly improve the performance of existing models. In\naddition, we find a positive correlation between CLIS and performance gains in\ndownstream tasks. This finding shows the potential for evaluation metrics as\nthe role for various visual perception and MLLM tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models can generate realistic and diverse images, potentially\nfacilitating data availability for data-intensive perception tasks. However,\nleveraging these models to boost performance on downstream tasks with synthetic\ndata poses several challenges, including aligning with real data distribution,\nscaling synthetic sample volumes, and ensuring their quality. To bridge these\ngaps, we present \\textbf{A}uto \\textbf{C}herry-\\textbf{P}icker (ACP), a novel\nframework that generates high-quality cross-modality training samples at scale\nto augment perception and multi-modal training. ACP first uses LLMs to sample\ndescriptions and layouts based on object combinations from real data priors,\neliminating the need for ground truth image captions or annotations. Next, we\nuse an off-the-shelf controllable diffusion model to generate multiple images.\nThen, the generated data are refined using a comprehensively designed metric,\nComposite Layout and Image Score (CLIS), to ensure quality. Our customized\nsynthetic high-quality samples boost performance in various scenarios,\nespecially in addressing challenges associated with long-tailed distribution\nand imbalanced datasets. Experiment results on downstream tasks demonstrate\nthat ACP can significantly improve the performance of existing models. In\naddition, we find a positive correlation between CLIS and performance gains in\ndownstream tasks. This finding shows the potential for evaluation metrics as\nthe role for various visual perception and MLLM tasks."
                },
                "authors": [
                    {
                        "name": "Yicheng Chen"
                    },
                    {
                        "name": "Xiangtai Li"
                    },
                    {
                        "name": "Yining Li"
                    },
                    {
                        "name": "Yanhong Zeng"
                    },
                    {
                        "name": "Jianzong Wu"
                    },
                    {
                        "name": "Xiangyu Zhao"
                    },
                    {
                        "name": "Kai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Kai Chen"
                },
                "author": "Kai Chen",
                "arxiv_comment": "Accepted to CVPR2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.20085v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.20085v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18494v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18494v1",
                "updated": "2025-03-24T09:48:59Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    9,
                    48,
                    59,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T09:48:59Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    9,
                    48,
                    59,
                    0,
                    83,
                    0
                ],
                "title": "Verbal Process Supervision Elicits Better Coding Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Verbal Process Supervision Elicits Better Coding Agents"
                },
                "summary": "The emergence of large language models and their applications as AI agents\nhave significantly advanced state-of-the-art code generation benchmarks,\ntransforming modern software engineering tasks. However, even with test-time\ncomputed reasoning models, these systems still struggle with complex software\nengineering challenges. This work introduces CURA, a code understanding and\nreasoning agent system enhanced with verbal process supervision (VPS),\nachieving a 3.65\\% improvement over baseline models on challenging benchmarks\nlike BigCodeBench. Furthermore, CURA, when paired with the o3-mini model and\nVPS techniques, attains state-of-the-art performance. This work represents a\nstep forward in integrating reasoning-driven architectures with LLM-based code\ngeneration, enabling agentic reasoning for language models to solve complex\nsoftware engineering tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of large language models and their applications as AI agents\nhave significantly advanced state-of-the-art code generation benchmarks,\ntransforming modern software engineering tasks. However, even with test-time\ncomputed reasoning models, these systems still struggle with complex software\nengineering challenges. This work introduces CURA, a code understanding and\nreasoning agent system enhanced with verbal process supervision (VPS),\nachieving a 3.65\\% improvement over baseline models on challenging benchmarks\nlike BigCodeBench. Furthermore, CURA, when paired with the o3-mini model and\nVPS techniques, attains state-of-the-art performance. This work represents a\nstep forward in integrating reasoning-driven architectures with LLM-based code\ngeneration, enabling agentic reasoning for language models to solve complex\nsoftware engineering tasks."
                },
                "authors": [
                    {
                        "name": "Hao-Yuan Chen"
                    },
                    {
                        "name": "Cheng-Pong Huang"
                    },
                    {
                        "name": "Jui-Ming Yao"
                    }
                ],
                "author_detail": {
                    "name": "Jui-Ming Yao"
                },
                "author": "Jui-Ming Yao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18494v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18494v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18492v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18492v1",
                "updated": "2025-03-24T09:46:05Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    9,
                    46,
                    5,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T09:46:05Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    9,
                    46,
                    5,
                    0,
                    83,
                    0
                ],
                "title": "Safeguarding Mobile GUI Agent via Logic-based Action Verification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Safeguarding Mobile GUI Agent via Logic-based Action Verification"
                },
                "summary": "Large Foundation Models (LFMs) have unlocked new possibilities in\nhuman-computer interaction, particularly with the rise of mobile Graphical User\nInterface (GUI) Agents capable of interpreting GUIs. These agents promise to\nrevolutionize mobile computing by allowing users to automate complex mobile\ntasks through simple natural language instructions. However, the inherent\nprobabilistic nature of LFMs, coupled with the ambiguity and context-dependence\nof mobile tasks, makes LFM-based automation unreliable and prone to errors. To\naddress this critical challenge, we introduce VeriSafe Agent (VSA): a formal\nverification system that serves as a logically grounded safeguard for Mobile\nGUI Agents. VSA is designed to deterministically ensure that an agent's actions\nstrictly align with user intent before conducting an action. At its core, VSA\nintroduces a novel autoformalization technique that translates natural language\nuser instructions into a formally verifiable specification, expressed in our\ndomain-specific language (DSL). This enables runtime, rule-based verification,\nallowing VSA to detect and prevent erroneous actions executing an action,\neither by providing corrective feedback or halting unsafe behavior. To the best\nof our knowledge, VSA is the first attempt to bring the rigor of formal\nverification to GUI agent. effectively bridging the gap between LFM-driven\nautomation and formal software verification. We implement VSA using\noff-the-shelf LLM services (GPT-4o) and evaluate its performance on 300 user\ninstructions across 18 widely used mobile apps. The results demonstrate that\nVSA achieves 94.3%-98.33% accuracy in verifying agent actions, representing a\nsignificant 20.4%-25.6% improvement over existing LLM-based verification\nmethods, and consequently increases the GUI agent's task completion rate by\n90%-130%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Foundation Models (LFMs) have unlocked new possibilities in\nhuman-computer interaction, particularly with the rise of mobile Graphical User\nInterface (GUI) Agents capable of interpreting GUIs. These agents promise to\nrevolutionize mobile computing by allowing users to automate complex mobile\ntasks through simple natural language instructions. However, the inherent\nprobabilistic nature of LFMs, coupled with the ambiguity and context-dependence\nof mobile tasks, makes LFM-based automation unreliable and prone to errors. To\naddress this critical challenge, we introduce VeriSafe Agent (VSA): a formal\nverification system that serves as a logically grounded safeguard for Mobile\nGUI Agents. VSA is designed to deterministically ensure that an agent's actions\nstrictly align with user intent before conducting an action. At its core, VSA\nintroduces a novel autoformalization technique that translates natural language\nuser instructions into a formally verifiable specification, expressed in our\ndomain-specific language (DSL). This enables runtime, rule-based verification,\nallowing VSA to detect and prevent erroneous actions executing an action,\neither by providing corrective feedback or halting unsafe behavior. To the best\nof our knowledge, VSA is the first attempt to bring the rigor of formal\nverification to GUI agent. effectively bridging the gap between LFM-driven\nautomation and formal software verification. We implement VSA using\noff-the-shelf LLM services (GPT-4o) and evaluate its performance on 300 user\ninstructions across 18 widely used mobile apps. The results demonstrate that\nVSA achieves 94.3%-98.33% accuracy in verifying agent actions, representing a\nsignificant 20.4%-25.6% improvement over existing LLM-based verification\nmethods, and consequently increases the GUI agent's task completion rate by\n90%-130%."
                },
                "authors": [
                    {
                        "name": "Jungjae Lee"
                    },
                    {
                        "name": "Dongjae Lee"
                    },
                    {
                        "name": "Chihun Choi"
                    },
                    {
                        "name": "Youngmin Im"
                    },
                    {
                        "name": "Jaeyoung Wi"
                    },
                    {
                        "name": "Kihong Heo"
                    },
                    {
                        "name": "Sangeun Oh"
                    },
                    {
                        "name": "Sunjae Lee"
                    },
                    {
                        "name": "Insik Shin"
                    }
                ],
                "author_detail": {
                    "name": "Insik Shin"
                },
                "author": "Insik Shin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18492v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18492v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18487v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18487v1",
                "updated": "2025-03-24T09:40:46Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    9,
                    40,
                    46,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T09:40:46Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    9,
                    40,
                    46,
                    0,
                    83,
                    0
                ],
                "title": "Large Language Models powered Network Attack Detection: Architecture,\n  Opportunities and Case Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models powered Network Attack Detection: Architecture,\n  Opportunities and Case Study"
                },
                "summary": "Network attack detection is a pivotal technology to identify network anomaly\nand classify malicious traffic. Large Language Models (LLMs) are trained on a\nvast corpus of text, have amassed remarkable capabilities of\ncontext-understanding and commonsense knowledge. This has opened up a new door\nfor network threat detection. Researchers have already initiated discussions\nregarding the application of LLMs on specific cyber-security tasks.\nUnfortunately, there is still a lack of comprehensive elaboration how to mine\nLLMs' potentials in network threat detections, as well as the opportunities and\nchallenges. In this paper, we mainly focus on the classification of malicious\ntraffic from the perspective of LLMs' capability. We present a holistic view of\nthe architecture of LLM-powered network attack detection, including\nPre-training, Fine-tuning, and Detection. Especially, by exploring the\nknowledge and capabilities of LLM, we identify three distinct roles LLM can act\nin network attack detection: \\textit{Classifier, Encoder, and Predictor}. For\neach of them, the modeling paradigm, opportunities and challenges are\nelaborated. Finally, we present our design on LLM-powered DDoS detection as a\ncase study. The proposed framework attains accurate detection on carpet bombing\nDDoS by exploiting LLMs' capabilities in contextual mining. The evaluation\nshows its efficacy, exhibiting a nearly $35$\\% improvement compared to existing\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Network attack detection is a pivotal technology to identify network anomaly\nand classify malicious traffic. Large Language Models (LLMs) are trained on a\nvast corpus of text, have amassed remarkable capabilities of\ncontext-understanding and commonsense knowledge. This has opened up a new door\nfor network threat detection. Researchers have already initiated discussions\nregarding the application of LLMs on specific cyber-security tasks.\nUnfortunately, there is still a lack of comprehensive elaboration how to mine\nLLMs' potentials in network threat detections, as well as the opportunities and\nchallenges. In this paper, we mainly focus on the classification of malicious\ntraffic from the perspective of LLMs' capability. We present a holistic view of\nthe architecture of LLM-powered network attack detection, including\nPre-training, Fine-tuning, and Detection. Especially, by exploring the\nknowledge and capabilities of LLM, we identify three distinct roles LLM can act\nin network attack detection: \\textit{Classifier, Encoder, and Predictor}. For\neach of them, the modeling paradigm, opportunities and challenges are\nelaborated. Finally, we present our design on LLM-powered DDoS detection as a\ncase study. The proposed framework attains accurate detection on carpet bombing\nDDoS by exploiting LLMs' capabilities in contextual mining. The evaluation\nshows its efficacy, exhibiting a nearly $35$\\% improvement compared to existing\nsystems."
                },
                "authors": [
                    {
                        "name": "Xinggong Zhang"
                    },
                    {
                        "name": "Qingyang Li"
                    },
                    {
                        "name": "Yunpeng Tan"
                    },
                    {
                        "name": "Zongming Guo"
                    },
                    {
                        "name": "Lei Zhang"
                    },
                    {
                        "name": "Yong Cui"
                    }
                ],
                "author_detail": {
                    "name": "Yong Cui"
                },
                "author": "Yong Cui",
                "arxiv_comment": "submitted for peer-review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18487v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18487v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01192v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01192v3",
                "updated": "2025-03-24T09:40:02Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    9,
                    40,
                    2,
                    0,
                    83,
                    0
                ],
                "published": "2025-01-02T10:55:41Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    10,
                    55,
                    41,
                    3,
                    2,
                    0
                ],
                "title": "Bridging the Early Science Gap with Artificial Intelligence: Evaluating\n  Large Language Models as Tools for Early Childhood Science Education",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridging the Early Science Gap with Artificial Intelligence: Evaluating\n  Large Language Models as Tools for Early Childhood Science Education"
                },
                "summary": "Early childhood science education is crucial for developing scientific\nliteracy, yet translating complex scientific concepts into age-appropriate\ncontent remains challenging for educators. Our study evaluates four leading\nLarge Language Models (LLMs) - GPT-4, Claude, Gemini, and Llama - on their\nability to generate preschool-appropriate scientific explanations across\nbiology, chemistry, and physics. Through systematic evaluation by 30 nursery\nteachers using established pedagogical criteria, we identify significant\ndifferences in the models' capabilities to create engaging, accurate, and\ndevelopmentally appropriate content. Unexpectedly, Claude outperformed other\nmodels, particularly in biological topics, while all LLMs struggled with\nabstract chemical concepts. Our findings provide practical insights for\neducators leveraging AI in early science education and offer guidance for\ndevelopers working to enhance LLMs' educational applications. The results\nhighlight the potential and current limitations of using LLMs to bridge the\nearly childhood science literacy gap.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Early childhood science education is crucial for developing scientific\nliteracy, yet translating complex scientific concepts into age-appropriate\ncontent remains challenging for educators. Our study evaluates four leading\nLarge Language Models (LLMs) - GPT-4, Claude, Gemini, and Llama - on their\nability to generate preschool-appropriate scientific explanations across\nbiology, chemistry, and physics. Through systematic evaluation by 30 nursery\nteachers using established pedagogical criteria, we identify significant\ndifferences in the models' capabilities to create engaging, accurate, and\ndevelopmentally appropriate content. Unexpectedly, Claude outperformed other\nmodels, particularly in biological topics, while all LLMs struggled with\nabstract chemical concepts. Our findings provide practical insights for\neducators leveraging AI in early science education and offer guidance for\ndevelopers working to enhance LLMs' educational applications. The results\nhighlight the potential and current limitations of using LLMs to bridge the\nearly childhood science literacy gap."
                },
                "authors": [
                    {
                        "name": "Annika Bush"
                    },
                    {
                        "name": "Amin Alibakhshi"
                    }
                ],
                "author_detail": {
                    "name": "Amin Alibakhshi"
                },
                "author": "Amin Alibakhshi",
                "arxiv_doi": "10.1145/3706599.3721261",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3706599.3721261",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.01192v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01192v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "CHI 2025 Late Breaking Work",
                "arxiv_journal_ref": "Extended Abstracts of the CHI Conference on Human Factors in\n  Computing Systems, 2025, Yokohama, Japan. ACM, New York, NY, USA, 6 pages",
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18483v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18483v1",
                "updated": "2025-03-24T09:35:28Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    9,
                    35,
                    28,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T09:35:28Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    9,
                    35,
                    28,
                    0,
                    83,
                    0
                ],
                "title": "Explaining Domain Shifts in Language: Concept erasing for Interpretable\n  Image Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explaining Domain Shifts in Language: Concept erasing for Interpretable\n  Image Classification"
                },
                "summary": "Concept-based models can map black-box representations to\nhuman-understandable concepts, which makes the decision-making process more\ntransparent and then allows users to understand the reason behind predictions.\nHowever, domain-specific concepts often impact the final predictions, which\nsubsequently undermine the model generalization capabilities, and prevent the\nmodel from being used in high-stake applications. In this paper, we propose a\nnovel Language-guided Concept-Erasing (LanCE) framework. In particular, we\nempirically demonstrate that pre-trained vision-language models (VLMs) can\napproximate distinct visual domain shifts via domain descriptors while\nprompting large Language Models (LLMs) can easily simulate a wide range of\ndescriptors of unseen visual domains. Then, we introduce a novel plug-in domain\ndescriptor orthogonality (DDO) regularizer to mitigate the impact of these\ndomain-specific concepts on the final predictions. Notably, the DDO regularizer\nis agnostic to the design of concept-based models and we integrate it into\nseveral prevailing models. Through evaluation of domain generalization on four\nstandard benchmarks and three newly introduced benchmarks, we demonstrate that\nDDO can significantly improve the out-of-distribution (OOD) generalization over\nthe previous state-of-the-art concept-based models.Our code is available at\nhttps://github.com/joeyz0z/LanCE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Concept-based models can map black-box representations to\nhuman-understandable concepts, which makes the decision-making process more\ntransparent and then allows users to understand the reason behind predictions.\nHowever, domain-specific concepts often impact the final predictions, which\nsubsequently undermine the model generalization capabilities, and prevent the\nmodel from being used in high-stake applications. In this paper, we propose a\nnovel Language-guided Concept-Erasing (LanCE) framework. In particular, we\nempirically demonstrate that pre-trained vision-language models (VLMs) can\napproximate distinct visual domain shifts via domain descriptors while\nprompting large Language Models (LLMs) can easily simulate a wide range of\ndescriptors of unseen visual domains. Then, we introduce a novel plug-in domain\ndescriptor orthogonality (DDO) regularizer to mitigate the impact of these\ndomain-specific concepts on the final predictions. Notably, the DDO regularizer\nis agnostic to the design of concept-based models and we integrate it into\nseveral prevailing models. Through evaluation of domain generalization on four\nstandard benchmarks and three newly introduced benchmarks, we demonstrate that\nDDO can significantly improve the out-of-distribution (OOD) generalization over\nthe previous state-of-the-art concept-based models.Our code is available at\nhttps://github.com/joeyz0z/LanCE."
                },
                "authors": [
                    {
                        "name": "Zequn Zeng"
                    },
                    {
                        "name": "Yudi Su"
                    },
                    {
                        "name": "Jianqiao Sun"
                    },
                    {
                        "name": "Tiansheng Wen"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Zhengjue Wang"
                    },
                    {
                        "name": "Bo Chen"
                    },
                    {
                        "name": "Hongwei Liu"
                    },
                    {
                        "name": "Jiawei Ma"
                    }
                ],
                "author_detail": {
                    "name": "Jiawei Ma"
                },
                "author": "Jiawei Ma",
                "arxiv_comment": "Accepted by CVPR2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18483v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18483v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.21321v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.21321v2",
                "updated": "2025-03-24T09:34:38Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    9,
                    34,
                    38,
                    0,
                    83,
                    0
                ],
                "published": "2025-02-28T18:59:54Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    18,
                    59,
                    54,
                    4,
                    59,
                    0
                ],
                "title": "LLM Post-Training: A Deep Dive into Reasoning Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Post-Training: A Deep Dive into Reasoning Large Language Models"
                },
                "summary": "Large Language Models (LLMs) have transformed the natural language processing\nlandscape and brought to life diverse applications. Pretraining on vast\nweb-scale data has laid the foundation for these models, yet the research\ncommunity is now increasingly shifting focus toward post-training techniques to\nachieve further breakthroughs. While pretraining provides a broad linguistic\nfoundation, post-training methods enable LLMs to refine their knowledge,\nimprove reasoning, enhance factual accuracy, and align more effectively with\nuser intents and ethical considerations. Fine-tuning, reinforcement learning,\nand test-time scaling have emerged as critical strategies for optimizing LLMs\nperformance, ensuring robustness, and improving adaptability across various\nreal-world tasks. This survey provides a systematic exploration of\npost-training methodologies, analyzing their role in refining LLMs beyond\npretraining, addressing key challenges such as catastrophic forgetting, reward\nhacking, and inference-time trade-offs. We highlight emerging directions in\nmodel alignment, scalable adaptation, and inference-time reasoning, and outline\nfuture research directions. We also provide a public repository to continually\ntrack developments in this fast-evolving field:\nhttps://github.com/mbzuai-oryx/Awesome-LLM-Post-training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have transformed the natural language processing\nlandscape and brought to life diverse applications. Pretraining on vast\nweb-scale data has laid the foundation for these models, yet the research\ncommunity is now increasingly shifting focus toward post-training techniques to\nachieve further breakthroughs. While pretraining provides a broad linguistic\nfoundation, post-training methods enable LLMs to refine their knowledge,\nimprove reasoning, enhance factual accuracy, and align more effectively with\nuser intents and ethical considerations. Fine-tuning, reinforcement learning,\nand test-time scaling have emerged as critical strategies for optimizing LLMs\nperformance, ensuring robustness, and improving adaptability across various\nreal-world tasks. This survey provides a systematic exploration of\npost-training methodologies, analyzing their role in refining LLMs beyond\npretraining, addressing key challenges such as catastrophic forgetting, reward\nhacking, and inference-time trade-offs. We highlight emerging directions in\nmodel alignment, scalable adaptation, and inference-time reasoning, and outline\nfuture research directions. We also provide a public repository to continually\ntrack developments in this fast-evolving field:\nhttps://github.com/mbzuai-oryx/Awesome-LLM-Post-training."
                },
                "authors": [
                    {
                        "name": "Komal Kumar"
                    },
                    {
                        "name": "Tajamul Ashraf"
                    },
                    {
                        "name": "Omkar Thawakar"
                    },
                    {
                        "name": "Rao Muhammad Anwer"
                    },
                    {
                        "name": "Hisham Cholakkal"
                    },
                    {
                        "name": "Mubarak Shah"
                    },
                    {
                        "name": "Ming-Hsuan Yang"
                    },
                    {
                        "name": "Phillip H. S. Torr"
                    },
                    {
                        "name": "Fahad Shahbaz Khan"
                    },
                    {
                        "name": "Salman Khan"
                    }
                ],
                "author_detail": {
                    "name": "Salman Khan"
                },
                "author": "Salman Khan",
                "arxiv_comment": "32 pages, 7 figures, 3 tables, 377 references. Github Repo:\n  https://github.com/mbzuai-oryx/Awesome-LLM-Post-training",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.21321v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.21321v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01653v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01653v2",
                "updated": "2025-03-24T09:20:07Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    9,
                    20,
                    7,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-03T15:28:26Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    15,
                    28,
                    26,
                    0,
                    62,
                    0
                ],
                "title": "Distilled Prompt Learning for Incomplete Multimodal Survival Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distilled Prompt Learning for Incomplete Multimodal Survival Prediction"
                },
                "summary": "The integration of multimodal data including pathology images and gene\nprofiles is widely applied in precise survival prediction. Despite recent\nadvances in multimodal survival models, collecting complete modalities for\nmultimodal fusion still poses a significant challenge, hindering their\napplication in clinical settings. Current approaches tackling incomplete\nmodalities often fall short, as they typically compensate for only a limited\npart of the knowledge of missing modalities. To address this issue, we propose\na Distilled Prompt Learning framework (DisPro) to utilize the strong robustness\nof Large Language Models (LLMs) to missing modalities, which employs two-stage\nprompting for compensation of comprehensive information for missing modalities.\nIn the first stage, Unimodal Prompting (UniPro) distills the knowledge\ndistribution of each modality, preparing for supplementing modality-specific\nknowledge of the missing modality in the subsequent stage. In the second stage,\nMultimodal Prompting (MultiPro) leverages available modalities as prompts for\nLLMs to infer the missing modality, which provides modality-common information.\nSimultaneously, the unimodal knowledge acquired in the first stage is injected\ninto multimodal inference to compensate for the modality-specific knowledge of\nthe missing modality. Extensive experiments covering various missing scenarios\ndemonstrated the superiority of the proposed method. The code is available at\nhttps://github.com/Innse/DisPro.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of multimodal data including pathology images and gene\nprofiles is widely applied in precise survival prediction. Despite recent\nadvances in multimodal survival models, collecting complete modalities for\nmultimodal fusion still poses a significant challenge, hindering their\napplication in clinical settings. Current approaches tackling incomplete\nmodalities often fall short, as they typically compensate for only a limited\npart of the knowledge of missing modalities. To address this issue, we propose\na Distilled Prompt Learning framework (DisPro) to utilize the strong robustness\nof Large Language Models (LLMs) to missing modalities, which employs two-stage\nprompting for compensation of comprehensive information for missing modalities.\nIn the first stage, Unimodal Prompting (UniPro) distills the knowledge\ndistribution of each modality, preparing for supplementing modality-specific\nknowledge of the missing modality in the subsequent stage. In the second stage,\nMultimodal Prompting (MultiPro) leverages available modalities as prompts for\nLLMs to infer the missing modality, which provides modality-common information.\nSimultaneously, the unimodal knowledge acquired in the first stage is injected\ninto multimodal inference to compensate for the modality-specific knowledge of\nthe missing modality. Extensive experiments covering various missing scenarios\ndemonstrated the superiority of the proposed method. The code is available at\nhttps://github.com/Innse/DisPro."
                },
                "authors": [
                    {
                        "name": "Yingxue Xu"
                    },
                    {
                        "name": "Fengtao Zhou"
                    },
                    {
                        "name": "Chenyu Zhao"
                    },
                    {
                        "name": "Yihui Wang"
                    },
                    {
                        "name": "Can Yang"
                    },
                    {
                        "name": "Hao Chen"
                    }
                ],
                "author_detail": {
                    "name": "Hao Chen"
                },
                "author": "Hao Chen",
                "arxiv_comment": "Accepted by CVPR2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01653v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01653v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10636v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10636v2",
                "updated": "2025-03-24T09:17:13Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    9,
                    17,
                    13,
                    0,
                    83,
                    0
                ],
                "published": "2024-10-14T15:48:09Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    15,
                    48,
                    9,
                    0,
                    288,
                    0
                ],
                "title": "Adapt-$\\infty$: Scalable Continual Multimodal Instruction Tuning via\n  Dynamic Data Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adapt-$\\infty$: Scalable Continual Multimodal Instruction Tuning via\n  Dynamic Data Selection"
                },
                "summary": "Visual instruction datasets from various distributors are released at\ndifferent times and often contain a significant number of semantically\nredundant text-image pairs, depending on their task compositions (i.e., skills)\nor reference sources. This redundancy greatly limits the efficient deployment\nof continually adaptable multimodal large language models, hindering their\nability to refine existing skills and acquire new competencies over time. We\nreframe the problem of lifelong Instruction Tuning (LiIT) via data selection,\nwhere the model automatically selects beneficial samples to learn from earlier\nand new datasets based on the current state of acquired knowledge in the model.\nWe propose Adapt-$\\infty$, a new multi-way and adaptive data selection approach\nthat dynamically balances sample efficiency and effectiveness during LiIT. We\nfirst construct pseudo-skill clusters by grouping gradient-based sample\nvectors. Next, we select the best-performing data selector for each skill\ncluster from a pool of selector experts, including our newly proposed scoring\nfunction, Image Grounding score. This data selector samples a subset of the\nmost important samples from each skill cluster for training. To prevent the\ncontinuous increase in the size of the dataset pool during LiIT, we introduce a\ncluster-wise permanent data pruning strategy to remove the most semantically\nredundant samples from each cluster, keeping computational requirements\nmanageable. We validate the effectiveness and efficiency of Adapt-$\\infty$ over\na sequence of multimodal instruction tuning datasets with various tasks,\nincluding (Knowledge) VQA, multilingual, grounding, reasoning, language-only,\nand multi-image comprehension. Training with samples selected by Adapt-$\\infty$\nalleviates catastrophic forgetting, especially for rare tasks, and promotes\nforward transfer across the continuum using only a fraction of the original\ndata.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual instruction datasets from various distributors are released at\ndifferent times and often contain a significant number of semantically\nredundant text-image pairs, depending on their task compositions (i.e., skills)\nor reference sources. This redundancy greatly limits the efficient deployment\nof continually adaptable multimodal large language models, hindering their\nability to refine existing skills and acquire new competencies over time. We\nreframe the problem of lifelong Instruction Tuning (LiIT) via data selection,\nwhere the model automatically selects beneficial samples to learn from earlier\nand new datasets based on the current state of acquired knowledge in the model.\nWe propose Adapt-$\\infty$, a new multi-way and adaptive data selection approach\nthat dynamically balances sample efficiency and effectiveness during LiIT. We\nfirst construct pseudo-skill clusters by grouping gradient-based sample\nvectors. Next, we select the best-performing data selector for each skill\ncluster from a pool of selector experts, including our newly proposed scoring\nfunction, Image Grounding score. This data selector samples a subset of the\nmost important samples from each skill cluster for training. To prevent the\ncontinuous increase in the size of the dataset pool during LiIT, we introduce a\ncluster-wise permanent data pruning strategy to remove the most semantically\nredundant samples from each cluster, keeping computational requirements\nmanageable. We validate the effectiveness and efficiency of Adapt-$\\infty$ over\na sequence of multimodal instruction tuning datasets with various tasks,\nincluding (Knowledge) VQA, multilingual, grounding, reasoning, language-only,\nand multi-image comprehension. Training with samples selected by Adapt-$\\infty$\nalleviates catastrophic forgetting, especially for rare tasks, and promotes\nforward transfer across the continuum using only a fraction of the original\ndata."
                },
                "authors": [
                    {
                        "name": "Adyasha Maharana"
                    },
                    {
                        "name": "Jaehong Yoon"
                    },
                    {
                        "name": "Tianlong Chen"
                    },
                    {
                        "name": "Mohit Bansal"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Bansal"
                },
                "author": "Mohit Bansal",
                "arxiv_comment": "First two authors contributed equally. Code:\n  https://github.com/adymaharana/adapt-inf",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10636v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10636v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12547v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12547v3",
                "updated": "2025-03-24T09:10:35Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    9,
                    10,
                    35,
                    0,
                    83,
                    0
                ],
                "published": "2025-01-21T23:54:17Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    23,
                    54,
                    17,
                    1,
                    21,
                    0
                ],
                "title": "Human-like conceptual representations emerge from language prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human-like conceptual representations emerge from language prediction"
                },
                "summary": "People acquire concepts through rich physical and social experiences and use\nthem to understand the world. In contrast, large language models (LLMs),\ntrained exclusively through next-token prediction over language data, exhibit\nremarkably human-like behaviors. Are these models developing concepts akin to\nhumans, and if so, how are such concepts represented and organized? To address\nthese questions, we reframed the classic reverse dictionary task to simulate\nhuman concept inference in context and investigated the emergence of human-like\nconceptual representations within LLMs. Our results demonstrate that LLMs can\nflexibly derive concepts from linguistic descriptions in relation to contextual\ncues about other concepts. The derived representations converged towards a\nshared, context-independent structure that effectively predicted human behavior\nacross key psychological phenomena, including computation of similarities,\ncategories and semantic scales. Moreover, these representations aligned well\nwith neural activity patterns in the human brain, even in response to visual\nrather than linguistic stimuli, providing evidence for biological plausibility.\nThese findings establish that structured, human-like conceptual representations\ncan naturally emerge from language prediction without real-world grounding.\nMore broadly, our work positions LLMs as promising computational tools for\nunderstanding complex human cognition and paves the way for better alignment\nbetween artificial and human intelligence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "People acquire concepts through rich physical and social experiences and use\nthem to understand the world. In contrast, large language models (LLMs),\ntrained exclusively through next-token prediction over language data, exhibit\nremarkably human-like behaviors. Are these models developing concepts akin to\nhumans, and if so, how are such concepts represented and organized? To address\nthese questions, we reframed the classic reverse dictionary task to simulate\nhuman concept inference in context and investigated the emergence of human-like\nconceptual representations within LLMs. Our results demonstrate that LLMs can\nflexibly derive concepts from linguistic descriptions in relation to contextual\ncues about other concepts. The derived representations converged towards a\nshared, context-independent structure that effectively predicted human behavior\nacross key psychological phenomena, including computation of similarities,\ncategories and semantic scales. Moreover, these representations aligned well\nwith neural activity patterns in the human brain, even in response to visual\nrather than linguistic stimuli, providing evidence for biological plausibility.\nThese findings establish that structured, human-like conceptual representations\ncan naturally emerge from language prediction without real-world grounding.\nMore broadly, our work positions LLMs as promising computational tools for\nunderstanding complex human cognition and paves the way for better alignment\nbetween artificial and human intelligence."
                },
                "authors": [
                    {
                        "name": "Ningyu Xu"
                    },
                    {
                        "name": "Qi Zhang"
                    },
                    {
                        "name": "Chao Du"
                    },
                    {
                        "name": "Qiang Luo"
                    },
                    {
                        "name": "Xipeng Qiu"
                    },
                    {
                        "name": "Xuanjing Huang"
                    },
                    {
                        "name": "Menghan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Menghan Zhang"
                },
                "author": "Menghan Zhang",
                "arxiv_comment": "51 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12547v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12547v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18460v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18460v1",
                "updated": "2025-03-24T09:04:49Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    9,
                    4,
                    49,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T09:04:49Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    9,
                    4,
                    49,
                    0,
                    83,
                    0
                ],
                "title": "ModiGen: A Large Language Model-Based Workflow for Multi-Task Modelica\n  Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ModiGen: A Large Language Model-Based Workflow for Multi-Task Modelica\n  Code Generation"
                },
                "summary": "Modelica is a widely adopted language for simulating complex physical\nsystems, yet effective model creation and optimization require substantial\ndomain expertise. Although large language models (LLMs) have demonstrated\npromising capabilities in code generation, their application to modeling\nremains largely unexplored. To address this gap, we have developed benchmark\ndatasets specifically designed to evaluate the performance of LLMs in\ngenerating Modelica component models and test cases. Our evaluation reveals\nsubstantial limitations in current LLMs, as the generated code often fails to\nsimulate successfully. To overcome these challenges, we propose a specialized\nworkflow that integrates supervised fine-tuning, graph retrieval-augmented\ngeneration, and feedback optimization to improve the accuracy and reliability\nof Modelica code generation. The evaluation results demonstrate significant\nperformance gains: the maximum improvement in pass@1 reached 0.3349 for the\ncomponent generation task and 0.2457 for the test case generation task. This\nresearch underscores the potential of LLMs to advance intelligent modeling\ntools and offers valuable insights for future developments in system modeling\nand engineering applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modelica is a widely adopted language for simulating complex physical\nsystems, yet effective model creation and optimization require substantial\ndomain expertise. Although large language models (LLMs) have demonstrated\npromising capabilities in code generation, their application to modeling\nremains largely unexplored. To address this gap, we have developed benchmark\ndatasets specifically designed to evaluate the performance of LLMs in\ngenerating Modelica component models and test cases. Our evaluation reveals\nsubstantial limitations in current LLMs, as the generated code often fails to\nsimulate successfully. To overcome these challenges, we propose a specialized\nworkflow that integrates supervised fine-tuning, graph retrieval-augmented\ngeneration, and feedback optimization to improve the accuracy and reliability\nof Modelica code generation. The evaluation results demonstrate significant\nperformance gains: the maximum improvement in pass@1 reached 0.3349 for the\ncomponent generation task and 0.2457 for the test case generation task. This\nresearch underscores the potential of LLMs to advance intelligent modeling\ntools and offers valuable insights for future developments in system modeling\nand engineering applications."
                },
                "authors": [
                    {
                        "name": "Jiahui Xiang"
                    },
                    {
                        "name": "Tong Ye"
                    },
                    {
                        "name": "Peiyu Liu"
                    },
                    {
                        "name": "Yinan Zhang"
                    },
                    {
                        "name": "Wenhai Wang"
                    }
                ],
                "author_detail": {
                    "name": "Wenhai Wang"
                },
                "author": "Wenhai Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18460v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18460v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18454v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18454v1",
                "updated": "2025-03-24T08:58:49Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    8,
                    58,
                    49,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T08:58:49Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    8,
                    58,
                    49,
                    0,
                    83,
                    0
                ],
                "title": "InPO: Inversion Preference Optimization with Reparametrized DDIM for\n  Efficient Diffusion Model Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InPO: Inversion Preference Optimization with Reparametrized DDIM for\n  Efficient Diffusion Model Alignment"
                },
                "summary": "Without using explicit reward, direct preference optimization (DPO) employs\npaired human preference data to fine-tune generative models, a method that has\ngarnered considerable attention in large language models (LLMs). However,\nexploration of aligning text-to-image (T2I) diffusion models with human\npreferences remains limited. In comparison to supervised fine-tuning, existing\nmethods that align diffusion model suffer from low training efficiency and\nsubpar generation quality due to the long Markov chain process and the\nintractability of the reverse process. To address these limitations, we\nintroduce DDIM-InPO, an efficient method for direct preference alignment of\ndiffusion models. Our approach conceptualizes diffusion model as a single-step\ngenerative model, allowing us to fine-tune the outputs of specific latent\nvariables selectively. In order to accomplish this objective, we first assign\nimplicit rewards to any latent variable directly via a reparameterization\ntechnique. Then we construct an Inversion technique to estimate appropriate\nlatent variables for preference optimization. This modification process enables\nthe diffusion model to only fine-tune the outputs of latent variables that have\na strong correlation with the preference dataset. Experimental results indicate\nthat our DDIM-InPO achieves state-of-the-art performance with just 400 steps of\nfine-tuning, surpassing all preference aligning baselines for T2I diffusion\nmodels in human preference evaluation tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Without using explicit reward, direct preference optimization (DPO) employs\npaired human preference data to fine-tune generative models, a method that has\ngarnered considerable attention in large language models (LLMs). However,\nexploration of aligning text-to-image (T2I) diffusion models with human\npreferences remains limited. In comparison to supervised fine-tuning, existing\nmethods that align diffusion model suffer from low training efficiency and\nsubpar generation quality due to the long Markov chain process and the\nintractability of the reverse process. To address these limitations, we\nintroduce DDIM-InPO, an efficient method for direct preference alignment of\ndiffusion models. Our approach conceptualizes diffusion model as a single-step\ngenerative model, allowing us to fine-tune the outputs of specific latent\nvariables selectively. In order to accomplish this objective, we first assign\nimplicit rewards to any latent variable directly via a reparameterization\ntechnique. Then we construct an Inversion technique to estimate appropriate\nlatent variables for preference optimization. This modification process enables\nthe diffusion model to only fine-tune the outputs of latent variables that have\na strong correlation with the preference dataset. Experimental results indicate\nthat our DDIM-InPO achieves state-of-the-art performance with just 400 steps of\nfine-tuning, surpassing all preference aligning baselines for T2I diffusion\nmodels in human preference evaluation tasks."
                },
                "authors": [
                    {
                        "name": "Yunhong Lu"
                    },
                    {
                        "name": "Qichao Wang"
                    },
                    {
                        "name": "Hengyuan Cao"
                    },
                    {
                        "name": "Xierui Wang"
                    },
                    {
                        "name": "Xiaoyin Xu"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "arxiv_comment": "Accepted by CVPR2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18454v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18454v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15029v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15029v2",
                "updated": "2025-03-24T08:50:39Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    8,
                    50,
                    39,
                    0,
                    83,
                    0
                ],
                "published": "2024-10-19T07:59:41Z",
                "published_parsed": [
                    2024,
                    10,
                    19,
                    7,
                    59,
                    41,
                    5,
                    293,
                    0
                ],
                "title": "Enhancing Multimodal Sentiment Analysis for Missing Modality through\n  Self-Distillation and Unified Modality Cross-Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Multimodal Sentiment Analysis for Missing Modality through\n  Self-Distillation and Unified Modality Cross-Attention"
                },
                "summary": "In multimodal sentiment analysis, collecting text data is often more\nchallenging than video or audio due to higher annotation costs and inconsistent\nautomatic speech recognition (ASR) quality. To address this challenge, our\nstudy has developed a robust model that effectively integrates multimodal\nsentiment information, even in the absence of text modality. Specifically, we\nhave developed a Double-Flow Self-Distillation Framework, including Unified\nModality Cross-Attention (UMCA) and Modality Imagination Autoencoder (MIA),\nwhich excels at processing both scenarios with complete modalities and those\nwith missing text modality. In detail, when the text modality is missing, our\nframework uses the LLM-based model to simulate the text representation from the\naudio modality, while the MIA module supplements information from the other two\nmodalities to make the simulated text representation similar to the real text\nrepresentation. To further align the simulated and real representations, and to\nenable the model to capture the continuous nature of sample orders in sentiment\nvalence regression tasks, we have also introduced the Rank-N Contrast (RNC)\nloss function. When testing on the CMU-MOSEI, our model achieved outstanding\nperformance on MAE and significantly outperformed other models when text\nmodality is missing. The code is available at:\nhttps://github.com/WarmCongee/SDUMC",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In multimodal sentiment analysis, collecting text data is often more\nchallenging than video or audio due to higher annotation costs and inconsistent\nautomatic speech recognition (ASR) quality. To address this challenge, our\nstudy has developed a robust model that effectively integrates multimodal\nsentiment information, even in the absence of text modality. Specifically, we\nhave developed a Double-Flow Self-Distillation Framework, including Unified\nModality Cross-Attention (UMCA) and Modality Imagination Autoencoder (MIA),\nwhich excels at processing both scenarios with complete modalities and those\nwith missing text modality. In detail, when the text modality is missing, our\nframework uses the LLM-based model to simulate the text representation from the\naudio modality, while the MIA module supplements information from the other two\nmodalities to make the simulated text representation similar to the real text\nrepresentation. To further align the simulated and real representations, and to\nenable the model to capture the continuous nature of sample orders in sentiment\nvalence regression tasks, we have also introduced the Rank-N Contrast (RNC)\nloss function. When testing on the CMU-MOSEI, our model achieved outstanding\nperformance on MAE and significantly outperformed other models when text\nmodality is missing. The code is available at:\nhttps://github.com/WarmCongee/SDUMC"
                },
                "authors": [
                    {
                        "name": "Yuzhe Weng"
                    },
                    {
                        "name": "Haotian Wang"
                    },
                    {
                        "name": "Tian Gao"
                    },
                    {
                        "name": "Kewei Li"
                    },
                    {
                        "name": "Shutong Niu"
                    },
                    {
                        "name": "Jun Du"
                    }
                ],
                "author_detail": {
                    "name": "Jun Du"
                },
                "author": "Jun Du",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15029v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15029v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18445v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18445v1",
                "updated": "2025-03-24T08:46:52Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    8,
                    46,
                    52,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T08:46:52Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    8,
                    46,
                    52,
                    0,
                    83,
                    0
                ],
                "title": "Benchmarking Multi-modal Semantic Segmentation under Sensor Failures:\n  Missing and Noisy Modality Robustness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking Multi-modal Semantic Segmentation under Sensor Failures:\n  Missing and Noisy Modality Robustness"
                },
                "summary": "Multi-modal semantic segmentation (MMSS) addresses the limitations of\nsingle-modality data by integrating complementary information across\nmodalities. Despite notable progress, a significant gap persists between\nresearch and real-world deployment due to variability and uncertainty in\nmulti-modal data quality. Robustness has thus become essential for practical\nMMSS applications. However, the absence of standardized benchmarks for\nevaluating robustness hinders further advancement. To address this, we first\nsurvey existing MMSS literature and categorize representative methods to\nprovide a structured overview. We then introduce a robustness benchmark that\nevaluates MMSS models under three scenarios: Entire-Missing Modality (EMM),\nRandom-Missing Modality (RMM), and Noisy Modality (NM). From a probabilistic\nstandpoint, we model modality failure under two conditions: (1) all damaged\ncombinations are equally probable; (2) each modality fails independently\nfollowing a Bernoulli distribution. Based on these, we propose four\nmetrics-$mIoU^{Avg}_{EMM}$, $mIoU^{E}_{EMM}$, $mIoU^{Avg}_{RMM}$, and\n$mIoU^{E}_{RMM}$-to assess model robustness under EMM and RMM. This work\nprovides the first dedicated benchmark for MMSS robustness, offering new\ninsights and tools to advance the field. Source code is available at\nhttps://github.com/Chenfei-Liao/Multi-Modal-Semantic-Segmentation-Robustness-Benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-modal semantic segmentation (MMSS) addresses the limitations of\nsingle-modality data by integrating complementary information across\nmodalities. Despite notable progress, a significant gap persists between\nresearch and real-world deployment due to variability and uncertainty in\nmulti-modal data quality. Robustness has thus become essential for practical\nMMSS applications. However, the absence of standardized benchmarks for\nevaluating robustness hinders further advancement. To address this, we first\nsurvey existing MMSS literature and categorize representative methods to\nprovide a structured overview. We then introduce a robustness benchmark that\nevaluates MMSS models under three scenarios: Entire-Missing Modality (EMM),\nRandom-Missing Modality (RMM), and Noisy Modality (NM). From a probabilistic\nstandpoint, we model modality failure under two conditions: (1) all damaged\ncombinations are equally probable; (2) each modality fails independently\nfollowing a Bernoulli distribution. Based on these, we propose four\nmetrics-$mIoU^{Avg}_{EMM}$, $mIoU^{E}_{EMM}$, $mIoU^{Avg}_{RMM}$, and\n$mIoU^{E}_{RMM}$-to assess model robustness under EMM and RMM. This work\nprovides the first dedicated benchmark for MMSS robustness, offering new\ninsights and tools to advance the field. Source code is available at\nhttps://github.com/Chenfei-Liao/Multi-Modal-Semantic-Segmentation-Robustness-Benchmark."
                },
                "authors": [
                    {
                        "name": "Chenfei Liao"
                    },
                    {
                        "name": "Kaiyu Lei"
                    },
                    {
                        "name": "Xu Zheng"
                    },
                    {
                        "name": "Junha Moon"
                    },
                    {
                        "name": "Zhixiong Wang"
                    },
                    {
                        "name": "Yixuan Wang"
                    },
                    {
                        "name": "Danda Pani Paudel"
                    },
                    {
                        "name": "Luc Van Gool"
                    },
                    {
                        "name": "Xuming Hu"
                    }
                ],
                "author_detail": {
                    "name": "Xuming Hu"
                },
                "author": "Xuming Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18445v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18445v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18434v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18434v1",
                "updated": "2025-03-24T08:32:54Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    8,
                    32,
                    54,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T08:32:54Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    8,
                    32,
                    54,
                    0,
                    83,
                    0
                ],
                "title": "A Simple yet Effective Layout Token in Large Language Models for\n  Document Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Simple yet Effective Layout Token in Large Language Models for\n  Document Understanding"
                },
                "summary": "Recent methods that integrate spatial layouts with text for document\nunderstanding in large language models (LLMs) have shown promising results. A\ncommonly used method is to represent layout information as text tokens and\ninterleave them with text content as inputs to the LLMs. However, such a method\nstill demonstrates limitations, as it requires additional position IDs for\ntokens that are used to represent layout information. Due to the constraint on\nmax position IDs, assigning them to layout information reduces those available\nfor text content, reducing the capacity for the model to learn from the text\nduring training, while also introducing a large number of potentially untrained\nposition IDs during long-context inference, which can hinder performance on\ndocument understanding tasks. To address these issues, we propose LayTokenLLM,\na simple yet effective method for document understanding. LayTokenLLM\nrepresents layout information as a single token per text segment and uses a\nspecialized positional encoding scheme. It shares position IDs between text and\nlayout tokens, eliminating the need for additional position IDs. This design\nmaintains the model's capacity to learn from text while mitigating long-context\nissues during inference. Furthermore, a novel pre-training objective called\nNext Interleaved Text and Layout Token Prediction (NTLP) is devised to enhance\ncross-modality learning between text and layout tokens. Extensive experiments\nshow that LayTokenLLM outperforms existing layout-integrated LLMs and MLLMs of\nsimilar scales on multi-page document understanding tasks, as well as most\nsingle-page tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent methods that integrate spatial layouts with text for document\nunderstanding in large language models (LLMs) have shown promising results. A\ncommonly used method is to represent layout information as text tokens and\ninterleave them with text content as inputs to the LLMs. However, such a method\nstill demonstrates limitations, as it requires additional position IDs for\ntokens that are used to represent layout information. Due to the constraint on\nmax position IDs, assigning them to layout information reduces those available\nfor text content, reducing the capacity for the model to learn from the text\nduring training, while also introducing a large number of potentially untrained\nposition IDs during long-context inference, which can hinder performance on\ndocument understanding tasks. To address these issues, we propose LayTokenLLM,\na simple yet effective method for document understanding. LayTokenLLM\nrepresents layout information as a single token per text segment and uses a\nspecialized positional encoding scheme. It shares position IDs between text and\nlayout tokens, eliminating the need for additional position IDs. This design\nmaintains the model's capacity to learn from text while mitigating long-context\nissues during inference. Furthermore, a novel pre-training objective called\nNext Interleaved Text and Layout Token Prediction (NTLP) is devised to enhance\ncross-modality learning between text and layout tokens. Extensive experiments\nshow that LayTokenLLM outperforms existing layout-integrated LLMs and MLLMs of\nsimilar scales on multi-page document understanding tasks, as well as most\nsingle-page tasks."
                },
                "authors": [
                    {
                        "name": "Zhaoqing Zhu"
                    },
                    {
                        "name": "Chuwei Luo"
                    },
                    {
                        "name": "Zirui Shao"
                    },
                    {
                        "name": "Feiyu Gao"
                    },
                    {
                        "name": "Hangdi Xing"
                    },
                    {
                        "name": "Qi Zheng"
                    },
                    {
                        "name": "Ji Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ji Zhang"
                },
                "author": "Ji Zhang",
                "arxiv_comment": "CVPR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18434v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18434v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18432v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18432v1",
                "updated": "2025-03-24T08:28:34Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    8,
                    28,
                    34,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T08:28:34Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    8,
                    28,
                    34,
                    0,
                    83,
                    0
                ],
                "title": "Teaching LLMs for Step-Level Automatic Math Correction via Reinforcement\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Teaching LLMs for Step-Level Automatic Math Correction via Reinforcement\n  Learning"
                },
                "summary": "Automatic math correction aims to check students' solutions to mathematical\nproblems via artificial intelligence technologies. Most existing studies focus\non judging the final answer at the problem level, while they ignore detailed\nfeedback on each step in a math problem-solving process, which requires\nabilities of semantic understanding and reasoning. In this paper, we propose a\nreinforcement learning (RL)-based method to boost large language model (LLM)\nfor step-level automatic math correction, named StepAMC. Particularly, we\nconvert the step-level automatic math correction within the text classification\ntask into an RL problem to enhance the reasoning capabilities of LLMs. Then, we\ndesign a space-constrained policy network to improve the stability of RL. Then,\nwe introduce a fine-grained reward network to convert the binary human feedback\ninto a continuous value. We conduct extensive experiments over two benchmark\ndatasets and the results show that our model outperforms the eleven strong\nbaselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic math correction aims to check students' solutions to mathematical\nproblems via artificial intelligence technologies. Most existing studies focus\non judging the final answer at the problem level, while they ignore detailed\nfeedback on each step in a math problem-solving process, which requires\nabilities of semantic understanding and reasoning. In this paper, we propose a\nreinforcement learning (RL)-based method to boost large language model (LLM)\nfor step-level automatic math correction, named StepAMC. Particularly, we\nconvert the step-level automatic math correction within the text classification\ntask into an RL problem to enhance the reasoning capabilities of LLMs. Then, we\ndesign a space-constrained policy network to improve the stability of RL. Then,\nwe introduce a fine-grained reward network to convert the binary human feedback\ninto a continuous value. We conduct extensive experiments over two benchmark\ndatasets and the results show that our model outperforms the eleven strong\nbaselines."
                },
                "authors": [
                    {
                        "name": "Junsong Li"
                    },
                    {
                        "name": "Jie Zhou"
                    },
                    {
                        "name": "Yutao Yang"
                    },
                    {
                        "name": "Bihao Zhan"
                    },
                    {
                        "name": "Qianjun Pan"
                    },
                    {
                        "name": "Yuyang Ding"
                    },
                    {
                        "name": "Qin Chen"
                    },
                    {
                        "name": "Jiang Bo"
                    },
                    {
                        "name": "Xin Lin"
                    },
                    {
                        "name": "Liang He"
                    }
                ],
                "author_detail": {
                    "name": "Liang He"
                },
                "author": "Liang He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18432v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18432v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17174v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17174v3",
                "updated": "2025-03-24T08:23:08Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    8,
                    23,
                    8,
                    0,
                    83,
                    0
                ],
                "published": "2024-09-20T08:28:23Z",
                "published_parsed": [
                    2024,
                    9,
                    20,
                    8,
                    28,
                    23,
                    4,
                    264,
                    0
                ],
                "title": "CSCE: Boosting LLM Reasoning by Simultaneous Enhancing of Causal\n  Significance and Consistency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CSCE: Boosting LLM Reasoning by Simultaneous Enhancing of Causal\n  Significance and Consistency"
                },
                "summary": "Chain-based reasoning methods like chain of thought (CoT) play a rising role\nin solving reasoning tasks for large language models (LLMs). However, the\ncausal hallucinations between a step of reasoning and corresponding state\ntransitions are becoming a significant obstacle to advancing LLMs' reasoning\ncapabilities, especially in long-range reasoning tasks. This paper proposes a\nnon-chain-based reasoning framework for simultaneous consideration of causal\nsignificance and consistency, i.e., the Causal Significance and Consistency\nEnhancer (CSCE). We customize LLM's loss function utilizing treatment effect\nassessments to enhance its reasoning ability from two aspects: causal\nsignificance and consistency. This ensures that the model captures essential\ncausal relationships and maintains robust and consistent performance across\nvarious scenarios. Additionally, we transform the reasoning process from the\ncascading multiple one-step reasoning commonly used in Chain-Based methods,\nlike CoT, to a causal-enhanced method that outputs the entire reasoning process\nin one go, further improving the model's reasoning efficiency. Extensive\nexperiments show that our method improves both the reasoning success rate and\nspeed. These improvements further demonstrate that non-chain-based methods can\nalso aid LLMs in completing reasoning tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-based reasoning methods like chain of thought (CoT) play a rising role\nin solving reasoning tasks for large language models (LLMs). However, the\ncausal hallucinations between a step of reasoning and corresponding state\ntransitions are becoming a significant obstacle to advancing LLMs' reasoning\ncapabilities, especially in long-range reasoning tasks. This paper proposes a\nnon-chain-based reasoning framework for simultaneous consideration of causal\nsignificance and consistency, i.e., the Causal Significance and Consistency\nEnhancer (CSCE). We customize LLM's loss function utilizing treatment effect\nassessments to enhance its reasoning ability from two aspects: causal\nsignificance and consistency. This ensures that the model captures essential\ncausal relationships and maintains robust and consistent performance across\nvarious scenarios. Additionally, we transform the reasoning process from the\ncascading multiple one-step reasoning commonly used in Chain-Based methods,\nlike CoT, to a causal-enhanced method that outputs the entire reasoning process\nin one go, further improving the model's reasoning efficiency. Extensive\nexperiments show that our method improves both the reasoning success rate and\nspeed. These improvements further demonstrate that non-chain-based methods can\nalso aid LLMs in completing reasoning tasks."
                },
                "authors": [
                    {
                        "name": "Kangsheng Wang"
                    },
                    {
                        "name": "Xiao Zhang"
                    },
                    {
                        "name": "Juntao Lyu"
                    },
                    {
                        "name": "Tianyu Hu"
                    },
                    {
                        "name": "Huimin Ma"
                    }
                ],
                "author_detail": {
                    "name": "Huimin Ma"
                },
                "author": "Huimin Ma",
                "arxiv_comment": "6 pages,4 figures. This paper has been accepted for presentation at\n  IEEE International Conference on Multimedia & Expo 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17174v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17174v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18422v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18422v1",
                "updated": "2025-03-24T08:06:39Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    8,
                    6,
                    39,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T08:06:39Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    8,
                    6,
                    39,
                    0,
                    83,
                    0
                ],
                "title": "Breaking the Encoder Barrier for Seamless Video-Language Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breaking the Encoder Barrier for Seamless Video-Language Understanding"
                },
                "summary": "Most Video-Large Language Models (Video-LLMs) adopt an encoder-decoder\nframework, where a vision encoder extracts frame-wise features for processing\nby a language model. However, this approach incurs high computational costs,\nintroduces resolution biases, and struggles to capture fine-grained multimodal\ninteractions. To overcome these limitations, we propose ELVA, an encoder-free\nVideo-LLM that directly models nuanced video-language interactions without\nrelying on a vision encoder. ELVA employs token merging to construct a\nbottom-up hierarchical representation and incorporates a video guidance\nsupervisor for direct spatiotemporal representation learning. Additionally, a\nhybrid-resolution mechanism strategically integrates high- and low-resolution\nframes as inputs to achieve an optimal balance between performance and\nefficiency. With only 7M publicly available video-text pairs, ELVA achieves\nperformance on par with encoder-based Video-LLMs while reducing FLOPs by up to\n95\\% and inference latency by 92\\%, offering a scalable and efficient solution\nfor real-time video understanding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Most Video-Large Language Models (Video-LLMs) adopt an encoder-decoder\nframework, where a vision encoder extracts frame-wise features for processing\nby a language model. However, this approach incurs high computational costs,\nintroduces resolution biases, and struggles to capture fine-grained multimodal\ninteractions. To overcome these limitations, we propose ELVA, an encoder-free\nVideo-LLM that directly models nuanced video-language interactions without\nrelying on a vision encoder. ELVA employs token merging to construct a\nbottom-up hierarchical representation and incorporates a video guidance\nsupervisor for direct spatiotemporal representation learning. Additionally, a\nhybrid-resolution mechanism strategically integrates high- and low-resolution\nframes as inputs to achieve an optimal balance between performance and\nefficiency. With only 7M publicly available video-text pairs, ELVA achieves\nperformance on par with encoder-based Video-LLMs while reducing FLOPs by up to\n95\\% and inference latency by 92\\%, offering a scalable and efficient solution\nfor real-time video understanding."
                },
                "authors": [
                    {
                        "name": "Handong Li"
                    },
                    {
                        "name": "Yiyuan Zhang"
                    },
                    {
                        "name": "Longteng Guo"
                    },
                    {
                        "name": "Xiangyu Yue"
                    },
                    {
                        "name": "Jing Liu"
                    }
                ],
                "author_detail": {
                    "name": "Jing Liu"
                },
                "author": "Jing Liu",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18422v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18422v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17038v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17038v2",
                "updated": "2025-03-24T07:29:28Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    7,
                    29,
                    28,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-21T10:48:35Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    10,
                    48,
                    35,
                    4,
                    80,
                    0
                ],
                "title": "Arm DynamIQ Shared Unit and Real-Time: An Empirical Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Arm DynamIQ Shared Unit and Real-Time: An Empirical Evaluation"
                },
                "summary": "The increasing complexity of embedded hardware platforms poses significant\nchallenges for real-time workloads. Architectural features such as Intel RDT,\nArm QoS, and Arm MPAM are either unavailable on commercial embedded platforms\nor designed primarily for server environments optimized for average-case\nperformance and might fail to deliver the expected real-time guarantees. Arm\nDynamIQ Shared Unit (DSU) includes isolation features-among others, hardware\nper-way cache partitioning-that can improve the real-time guarantees of complex\nembedded multicore systems and facilitate real-time analysis. However, the DSU\nalso targets average cases, and its real-time capabilities have not yet been\nevaluated. This paper presents the first comprehensive analysis of three\nreal-world deployments of the Arm DSU on Rockchip RK3568, Rockchip RK3588, and\nNVIDIA Orin platforms. We integrate support for the DSU at the operating system\nand hypervisor level and conduct a large-scale evaluation using both synthetic\nand real-world benchmarks with varying types and intensities of interference.\nOur results make extensive use of performance counters and indicate that,\nalthough effective, the quality of partitioning and isolation provided by the\nDSU depends on the type and the intensity of the interfering workloads. In\naddition, we uncover and analyze in detail the correlation between benchmarks\nand different types and intensities of interference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing complexity of embedded hardware platforms poses significant\nchallenges for real-time workloads. Architectural features such as Intel RDT,\nArm QoS, and Arm MPAM are either unavailable on commercial embedded platforms\nor designed primarily for server environments optimized for average-case\nperformance and might fail to deliver the expected real-time guarantees. Arm\nDynamIQ Shared Unit (DSU) includes isolation features-among others, hardware\nper-way cache partitioning-that can improve the real-time guarantees of complex\nembedded multicore systems and facilitate real-time analysis. However, the DSU\nalso targets average cases, and its real-time capabilities have not yet been\nevaluated. This paper presents the first comprehensive analysis of three\nreal-world deployments of the Arm DSU on Rockchip RK3568, Rockchip RK3588, and\nNVIDIA Orin platforms. We integrate support for the DSU at the operating system\nand hypervisor level and conduct a large-scale evaluation using both synthetic\nand real-world benchmarks with varying types and intensities of interference.\nOur results make extensive use of performance counters and indicate that,\nalthough effective, the quality of partitioning and isolation provided by the\nDSU depends on the type and the intensity of the interfering workloads. In\naddition, we uncover and analyze in detail the correlation between benchmarks\nand different types and intensities of interference."
                },
                "authors": [
                    {
                        "name": "Ashutosh Pradhan"
                    },
                    {
                        "name": "Daniele Ottaviano"
                    },
                    {
                        "name": "Yi Jiang"
                    },
                    {
                        "name": "Haozheng Huang"
                    },
                    {
                        "name": "Alexander Zuepke"
                    },
                    {
                        "name": "Andrea Bastoni"
                    },
                    {
                        "name": "Marco Caccamo"
                    }
                ],
                "author_detail": {
                    "name": "Marco Caccamo"
                },
                "author": "Marco Caccamo",
                "arxiv_comment": "Accepted for publication in the Proceedings of the 31st IEEE\n  Real-Time and Embedded Technology and Applications Symposium (RTAS 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17038v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17038v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68M20",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.3; C.4; D.4.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18394v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18394v1",
                "updated": "2025-03-24T07:05:55Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    7,
                    5,
                    55,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T07:05:55Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    7,
                    5,
                    55,
                    0,
                    83,
                    0
                ],
                "title": "Solving Situation Puzzles with Large Language Model and External\n  Reformulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Solving Situation Puzzles with Large Language Model and External\n  Reformulation"
                },
                "summary": "In recent years, large language models (LLMs) have shown an impressive\nability to perform arithmetic and symbolic reasoning tasks. However, we found\nthat LLMs (e.g., ChatGPT) cannot perform well on reasoning that requires\nmultiple rounds of dialogue, especially when solving situation puzzles.\nSpecifically, LLMs intend to ask very detailed questions focusing on a specific\naspect or same/similar questions after several rounds of Q&As. To help LLMs get\nout of the above dilemma, we propose a novel external reformulation\nmethodology, where the situation puzzle will be reformulated after several\nrounds of Q&A or when the LLMs raise an incorrect guess. Experiments show\nsuperior performance (e.g., win rate, number of question/guess attempts) of our\nmethod than directly using LLMs for solving situation puzzles, highlighting the\npotential of strategic problem reformulation to enhance the reasoning\ncapabilities of LLMs in complex interactive scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, large language models (LLMs) have shown an impressive\nability to perform arithmetic and symbolic reasoning tasks. However, we found\nthat LLMs (e.g., ChatGPT) cannot perform well on reasoning that requires\nmultiple rounds of dialogue, especially when solving situation puzzles.\nSpecifically, LLMs intend to ask very detailed questions focusing on a specific\naspect or same/similar questions after several rounds of Q&As. To help LLMs get\nout of the above dilemma, we propose a novel external reformulation\nmethodology, where the situation puzzle will be reformulated after several\nrounds of Q&A or when the LLMs raise an incorrect guess. Experiments show\nsuperior performance (e.g., win rate, number of question/guess attempts) of our\nmethod than directly using LLMs for solving situation puzzles, highlighting the\npotential of strategic problem reformulation to enhance the reasoning\ncapabilities of LLMs in complex interactive scenarios."
                },
                "authors": [
                    {
                        "name": "Kun Li"
                    },
                    {
                        "name": "Xinwei Chen"
                    },
                    {
                        "name": "Tianyou Song"
                    },
                    {
                        "name": "Chengrui Zhou"
                    },
                    {
                        "name": "Zhuoran Liu"
                    },
                    {
                        "name": "Zhenyan Zhang"
                    },
                    {
                        "name": "Jiangjian Guo"
                    },
                    {
                        "name": "Qing Shan"
                    }
                ],
                "author_detail": {
                    "name": "Qing Shan"
                },
                "author": "Qing Shan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18394v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18394v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18377v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18377v1",
                "updated": "2025-03-24T06:17:30Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    6,
                    17,
                    30,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T06:17:30Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    6,
                    17,
                    30,
                    0,
                    83,
                    0
                ],
                "title": "Maximum Redundancy Pruning: A Principle-Driven Layerwise Sparsity\n  Allocation for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Maximum Redundancy Pruning: A Principle-Driven Layerwise Sparsity\n  Allocation for LLMs"
                },
                "summary": "Large language models (LLMs) have demonstrated impressive capabilities, but\ntheir enormous size poses significant challenges for deployment in real-world\napplications. To address this issue, researchers have sought to apply network\npruning techniques to LLMs. A critical challenge in pruning is allocation the\nsparsity for each layer. Recent sparsity allocation methods is often based on\nheuristics or search that can easily lead to suboptimal performance. In this\npaper, we conducted an extensive investigation into various LLMs and revealed\nthree significant discoveries: (1) the layerwise pruning sensitivity (LPS) of\nLLMs is highly non-uniform, (2) the choice of pruning metric affects LPS, and\n(3) the performance of a sparse model is related to the uniformity of its\nlayerwise redundancy level. Based on these observations, we propose that the\nlayerwise sparsity of LLMs should adhere to three principles:\n\\emph{non-uniformity}, \\emph{pruning metric dependency}, and \\emph{uniform\nlayerwise redundancy level} in the pruned model. To this end, we proposed\nMaximum Redundancy Pruning (MRP), an iterative pruning algorithm that prunes in\nthe most redundant layers (\\emph{i.e.}, those with the highest non-outlier\nratio) at each iteration. The achieved layerwise sparsity aligns with the\noutlined principles. We conducted extensive experiments on publicly available\nLLMs, including the LLaMA2 and OPT, across various benchmarks. Experimental\nresults validate the effectiveness of MRP, demonstrating its superiority over\nprevious methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated impressive capabilities, but\ntheir enormous size poses significant challenges for deployment in real-world\napplications. To address this issue, researchers have sought to apply network\npruning techniques to LLMs. A critical challenge in pruning is allocation the\nsparsity for each layer. Recent sparsity allocation methods is often based on\nheuristics or search that can easily lead to suboptimal performance. In this\npaper, we conducted an extensive investigation into various LLMs and revealed\nthree significant discoveries: (1) the layerwise pruning sensitivity (LPS) of\nLLMs is highly non-uniform, (2) the choice of pruning metric affects LPS, and\n(3) the performance of a sparse model is related to the uniformity of its\nlayerwise redundancy level. Based on these observations, we propose that the\nlayerwise sparsity of LLMs should adhere to three principles:\n\\emph{non-uniformity}, \\emph{pruning metric dependency}, and \\emph{uniform\nlayerwise redundancy level} in the pruned model. To this end, we proposed\nMaximum Redundancy Pruning (MRP), an iterative pruning algorithm that prunes in\nthe most redundant layers (\\emph{i.e.}, those with the highest non-outlier\nratio) at each iteration. The achieved layerwise sparsity aligns with the\noutlined principles. We conducted extensive experiments on publicly available\nLLMs, including the LLaMA2 and OPT, across various benchmarks. Experimental\nresults validate the effectiveness of MRP, demonstrating its superiority over\nprevious methods."
                },
                "authors": [
                    {
                        "name": "Chang Gao"
                    },
                    {
                        "name": "Kang Zhao"
                    },
                    {
                        "name": "Jianfei Chen"
                    },
                    {
                        "name": "Liping Jing"
                    }
                ],
                "author_detail": {
                    "name": "Liping Jing"
                },
                "author": "Liping Jing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18377v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18377v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18360v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18360v1",
                "updated": "2025-03-24T05:42:05Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    5,
                    42,
                    5,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T05:42:05Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    5,
                    42,
                    5,
                    0,
                    83,
                    0
                ],
                "title": "J&H: Evaluating the Robustness of Large Language Models Under\n  Knowledge-Injection Attacks in Legal Domain",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "J&H: Evaluating the Robustness of Large Language Models Under\n  Knowledge-Injection Attacks in Legal Domain"
                },
                "summary": "As the scale and capabilities of Large Language Models (LLMs) increase, their\napplications in knowledge-intensive fields such as legal domain have garnered\nwidespread attention. However, it remains doubtful whether these LLMs make\njudgments based on domain knowledge for reasoning. If LLMs base their judgments\nsolely on specific words or patterns, rather than on the underlying logic of\nthe language, the ''LLM-as-judges'' paradigm poses substantial risks in the\nreal-world applications. To address this question, we propose a method of legal\nknowledge injection attacks for robustness testing, thereby inferring whether\nLLMs have learned legal knowledge and reasoning logic. In this paper, we\npropose J&H: an evaluation framework for detecting the robustness of LLMs under\nknowledge injection attacks in the legal domain. The aim of the framework is to\nexplore whether LLMs perform deductive reasoning when accomplishing legal\ntasks. To further this aim, we have attacked each part of the reasoning logic\nunderlying these tasks (major premise, minor premise, and conclusion\ngeneration). We have collected mistakes that legal experts might make in\njudicial decisions in the real world, such as typos, legal synonyms, inaccurate\nexternal legal statutes retrieval. However, in real legal practice, legal\nexperts tend to overlook these mistakes and make judgments based on logic.\nHowever, when faced with these errors, LLMs are likely to be misled by\ntypographical errors and may not utilize logic in their judgments. We conducted\nknowledge injection attacks on existing general and domain-specific LLMs.\nCurrent LLMs are not robust against the attacks employed in our experiments. In\naddition we propose and compare several methods to enhance the knowledge\nrobustness of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the scale and capabilities of Large Language Models (LLMs) increase, their\napplications in knowledge-intensive fields such as legal domain have garnered\nwidespread attention. However, it remains doubtful whether these LLMs make\njudgments based on domain knowledge for reasoning. If LLMs base their judgments\nsolely on specific words or patterns, rather than on the underlying logic of\nthe language, the ''LLM-as-judges'' paradigm poses substantial risks in the\nreal-world applications. To address this question, we propose a method of legal\nknowledge injection attacks for robustness testing, thereby inferring whether\nLLMs have learned legal knowledge and reasoning logic. In this paper, we\npropose J&H: an evaluation framework for detecting the robustness of LLMs under\nknowledge injection attacks in the legal domain. The aim of the framework is to\nexplore whether LLMs perform deductive reasoning when accomplishing legal\ntasks. To further this aim, we have attacked each part of the reasoning logic\nunderlying these tasks (major premise, minor premise, and conclusion\ngeneration). We have collected mistakes that legal experts might make in\njudicial decisions in the real world, such as typos, legal synonyms, inaccurate\nexternal legal statutes retrieval. However, in real legal practice, legal\nexperts tend to overlook these mistakes and make judgments based on logic.\nHowever, when faced with these errors, LLMs are likely to be misled by\ntypographical errors and may not utilize logic in their judgments. We conducted\nknowledge injection attacks on existing general and domain-specific LLMs.\nCurrent LLMs are not robust against the attacks employed in our experiments. In\naddition we propose and compare several methods to enhance the knowledge\nrobustness of LLMs."
                },
                "authors": [
                    {
                        "name": "Yiran Hu"
                    },
                    {
                        "name": "Huanghai Liu"
                    },
                    {
                        "name": "Qingjing Chen"
                    },
                    {
                        "name": "Ning Zheng"
                    },
                    {
                        "name": "Chong Wang"
                    },
                    {
                        "name": "Yun Liu"
                    },
                    {
                        "name": "Charles L. A. Clarke"
                    },
                    {
                        "name": "Weixing Shen"
                    }
                ],
                "author_detail": {
                    "name": "Weixing Shen"
                },
                "author": "Weixing Shen",
                "arxiv_comment": "10 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18360v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18360v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12486v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12486v3",
                "updated": "2025-03-24T05:14:24Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    5,
                    14,
                    24,
                    0,
                    83,
                    0
                ],
                "published": "2025-02-18T03:15:55Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    3,
                    15,
                    55,
                    1,
                    49,
                    0
                ],
                "title": "EPO: Explicit Policy Optimization for Strategic Reasoning in LLMs via\n  Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EPO: Explicit Policy Optimization for Strategic Reasoning in LLMs via\n  Reinforcement Learning"
                },
                "summary": "Large Language Models (LLMs) have shown impressive reasoning capabilities in\nwell-defined problems with clear solutions, such as mathematics and coding.\nHowever, they still struggle with complex real-world scenarios like business\nnegotiations, which require strategic reasoning-an ability to navigate dynamic\nenvironments and align long-term goals amidst uncertainty. Existing methods for\nstrategic reasoning face challenges in adaptability, scalability, and\ntransferring strategies to new contexts. To address these issues, we propose\nexplicit policy optimization (EPO) for strategic reasoning, featuring an LLM\nthat provides strategies in open-ended action space and can be plugged into\narbitrary LLM agents to motivate goal-directed behavior. To improve\nadaptability and policy transferability, we train the strategic reasoning model\nvia multi-turn reinforcement learning (RL) using process rewards and iterative\nself-play, without supervised fine-tuning (SFT) as a preliminary step.\nExperiments across social and physical domains demonstrate EPO's ability of\nlong-term goal alignment through enhanced strategic reasoning, achieving\nstate-of-the-art performance on social dialogue and web navigation tasks. Our\nfindings reveal various collaborative reasoning mechanisms emergent in EPO and\nits effectiveness in generating novel strategies, underscoring its potential\nfor strategic reasoning in real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown impressive reasoning capabilities in\nwell-defined problems with clear solutions, such as mathematics and coding.\nHowever, they still struggle with complex real-world scenarios like business\nnegotiations, which require strategic reasoning-an ability to navigate dynamic\nenvironments and align long-term goals amidst uncertainty. Existing methods for\nstrategic reasoning face challenges in adaptability, scalability, and\ntransferring strategies to new contexts. To address these issues, we propose\nexplicit policy optimization (EPO) for strategic reasoning, featuring an LLM\nthat provides strategies in open-ended action space and can be plugged into\narbitrary LLM agents to motivate goal-directed behavior. To improve\nadaptability and policy transferability, we train the strategic reasoning model\nvia multi-turn reinforcement learning (RL) using process rewards and iterative\nself-play, without supervised fine-tuning (SFT) as a preliminary step.\nExperiments across social and physical domains demonstrate EPO's ability of\nlong-term goal alignment through enhanced strategic reasoning, achieving\nstate-of-the-art performance on social dialogue and web navigation tasks. Our\nfindings reveal various collaborative reasoning mechanisms emergent in EPO and\nits effectiveness in generating novel strategies, underscoring its potential\nfor strategic reasoning in real-world applications."
                },
                "authors": [
                    {
                        "name": "Xiaoqian Liu"
                    },
                    {
                        "name": "Ke Wang"
                    },
                    {
                        "name": "Yongbin Li"
                    },
                    {
                        "name": "Yuchuan Wu"
                    },
                    {
                        "name": "Wentao Ma"
                    },
                    {
                        "name": "Aobo Kong"
                    },
                    {
                        "name": "Fei Huang"
                    },
                    {
                        "name": "Jianbin Jiao"
                    },
                    {
                        "name": "Junge Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Junge Zhang"
                },
                "author": "Junge Zhang",
                "arxiv_comment": "22 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12486v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12486v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11022v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11022v2",
                "updated": "2025-03-24T05:05:38Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    5,
                    5,
                    38,
                    0,
                    83,
                    0
                ],
                "published": "2024-11-17T09:58:37Z",
                "published_parsed": [
                    2024,
                    11,
                    17,
                    9,
                    58,
                    37,
                    6,
                    322,
                    0
                ],
                "title": "ASiM: Improving Transparency of SRAM-based Analog Compute-in-Memory\n  Research with an Open-Source Simulation Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ASiM: Improving Transparency of SRAM-based Analog Compute-in-Memory\n  Research with an Open-Source Simulation Framework"
                },
                "summary": "SRAM-based Analog Compute-in-Memory (ACiM) demonstrates promising energy\nefficiency for deep neural network (DNN) processing. Although recent aggressive\ndesign strategies have led to successive improvements on efficiency, there is\nlimited discussion regarding the accompanying inference accuracy challenges.\nGiven the growing difficulty in validating ACiM circuits with full-scale DNNs,\nstandardized modeling methodology and open-source inference simulator are\nurgently needed. This paper presents ASiM, a simulation framework specifically\ndesigned to assess inference quality, enabling comparisons of ACiM prototype\nchips and guiding design decisions. ASiM works as a plug-and-play tool that\nintegrates seamlessly with the PyTorch ecosystem, offering speed and ease of\nuse. Using ASiM, we conducted a comprehensive analysis of how various design\nfactors impact DNN inference. We observed that activation encoding can tolerate\ncertain levels of quantization noise, indicating a substantial potential for\nbit-parallel scheme to enhance energy efficiency. However, inference accuracy\nis susceptible to noise, as ACiM circuits typically use limited ADC dynamic\nrange, making even small errors down to 1 LSB significantly deteriorates\naccuracy. This underscores the need for high design standards, especially for\ncomplex DNN models and challenging tasks. In response to these findings, we\npropose two solutions: Hybrid Compute-in-Memory architecture and majority\nvoting to secure accurate computation of MSB cycles. These approaches improve\ninference quality while maintaining energy efficiency benefits of ACiM,\noffering promising pathways toward reliable ACiM deployment in real-world\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SRAM-based Analog Compute-in-Memory (ACiM) demonstrates promising energy\nefficiency for deep neural network (DNN) processing. Although recent aggressive\ndesign strategies have led to successive improvements on efficiency, there is\nlimited discussion regarding the accompanying inference accuracy challenges.\nGiven the growing difficulty in validating ACiM circuits with full-scale DNNs,\nstandardized modeling methodology and open-source inference simulator are\nurgently needed. This paper presents ASiM, a simulation framework specifically\ndesigned to assess inference quality, enabling comparisons of ACiM prototype\nchips and guiding design decisions. ASiM works as a plug-and-play tool that\nintegrates seamlessly with the PyTorch ecosystem, offering speed and ease of\nuse. Using ASiM, we conducted a comprehensive analysis of how various design\nfactors impact DNN inference. We observed that activation encoding can tolerate\ncertain levels of quantization noise, indicating a substantial potential for\nbit-parallel scheme to enhance energy efficiency. However, inference accuracy\nis susceptible to noise, as ACiM circuits typically use limited ADC dynamic\nrange, making even small errors down to 1 LSB significantly deteriorates\naccuracy. This underscores the need for high design standards, especially for\ncomplex DNN models and challenging tasks. In response to these findings, we\npropose two solutions: Hybrid Compute-in-Memory architecture and majority\nvoting to secure accurate computation of MSB cycles. These approaches improve\ninference quality while maintaining energy efficiency benefits of ACiM,\noffering promising pathways toward reliable ACiM deployment in real-world\napplications."
                },
                "authors": [
                    {
                        "name": "Wenlun Zhang"
                    },
                    {
                        "name": "Shimpei Ando"
                    },
                    {
                        "name": "Yung-Chin Chen"
                    },
                    {
                        "name": "Kentaro Yoshioka"
                    }
                ],
                "author_detail": {
                    "name": "Kentaro Yoshioka"
                },
                "author": "Kentaro Yoshioka",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11022v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11022v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18320v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18320v1",
                "updated": "2025-03-24T03:59:06Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    3,
                    59,
                    6,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T03:59:06Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    3,
                    59,
                    6,
                    0,
                    83,
                    0
                ],
                "title": "Bridging Writing Manner Gap in Visual Instruction Tuning by Creating\n  LLM-aligned Instructions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridging Writing Manner Gap in Visual Instruction Tuning by Creating\n  LLM-aligned Instructions"
                },
                "summary": "In the realm of Large Multi-modal Models (LMMs), the instruction quality\nduring the visual instruction tuning stage significantly influences the\nperformance of modality alignment. In this paper, we assess the instruction\nquality from a unique perspective termed \\textbf{Writing Manner}, which\nencompasses the selection of vocabulary, grammar and sentence structure to\nconvey specific semantics. We argue that there exists a substantial writing\nmanner gap between the visual instructions and the base Large Language Models\n(LLMs) within LMMs. This gap forces the pre-trained base LLMs to deviate from\ntheir original writing styles, leading to capability degradation of both base\nLLMs and LMMs. To bridge the writing manner gap while preserving the original\nsemantics, we propose directly leveraging the base LLM to align the writing\nmanner of soft-format visual instructions with that of the base LLM itself,\nresulting in novel LLM-aligned instructions. The manual writing manner\nevaluation results demonstrate that our approach successfully minimizes the\nwriting manner gap. By utilizing LLM-aligned instructions, the baseline models\nLLaVA-7B and QwenVL demonstrate enhanced resistance to hallucinations and\nnon-trivial comprehensive improvements across all $15$ visual and language\nbenchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the realm of Large Multi-modal Models (LMMs), the instruction quality\nduring the visual instruction tuning stage significantly influences the\nperformance of modality alignment. In this paper, we assess the instruction\nquality from a unique perspective termed \\textbf{Writing Manner}, which\nencompasses the selection of vocabulary, grammar and sentence structure to\nconvey specific semantics. We argue that there exists a substantial writing\nmanner gap between the visual instructions and the base Large Language Models\n(LLMs) within LMMs. This gap forces the pre-trained base LLMs to deviate from\ntheir original writing styles, leading to capability degradation of both base\nLLMs and LMMs. To bridge the writing manner gap while preserving the original\nsemantics, we propose directly leveraging the base LLM to align the writing\nmanner of soft-format visual instructions with that of the base LLM itself,\nresulting in novel LLM-aligned instructions. The manual writing manner\nevaluation results demonstrate that our approach successfully minimizes the\nwriting manner gap. By utilizing LLM-aligned instructions, the baseline models\nLLaVA-7B and QwenVL demonstrate enhanced resistance to hallucinations and\nnon-trivial comprehensive improvements across all $15$ visual and language\nbenchmarks."
                },
                "authors": [
                    {
                        "name": "Dong Jing"
                    },
                    {
                        "name": "Nanyi Fei"
                    },
                    {
                        "name": "Zhiwu Lu"
                    }
                ],
                "author_detail": {
                    "name": "Zhiwu Lu"
                },
                "author": "Zhiwu Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18320v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18320v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18316v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18316v1",
                "updated": "2025-03-24T03:51:09Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    3,
                    51,
                    9,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T03:51:09Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    3,
                    51,
                    9,
                    0,
                    83,
                    0
                ],
                "title": "Knowledge Transfer from LLMs to Provenance Analysis: A\n  Semantic-Augmented Method for APT Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Transfer from LLMs to Provenance Analysis: A\n  Semantic-Augmented Method for APT Detection"
                },
                "summary": "Advanced Persistent Threats (APTs) have caused significant losses across a\nwide range of sectors, including the theft of sensitive data and harm to system\nintegrity. As attack techniques grow increasingly sophisticated and stealthy,\nthe arms race between cyber defenders and attackers continues to intensify. The\nrevolutionary impact of Large Language Models (LLMs) has opened up numerous\nopportunities in various fields, including cybersecurity. An intriguing\nquestion arises: can the extensive knowledge embedded in LLMs be harnessed for\nprovenance analysis and play a positive role in identifying previously unknown\nmalicious events? To seek a deeper understanding of this issue, we propose a\nnew strategy for taking advantage of LLMs in provenance-based threat detection.\nIn our design, the state-of-the-art LLM offers additional details in provenance\ndata interpretation, leveraging their knowledge of system calls, software\nidentity, and high-level understanding of application execution context. The\nadvanced contextualized embedding capability is further utilized to capture the\nrich semantics of event descriptions. We comprehensively examine the quality of\nthe resulting embeddings, and it turns out that they offer promising avenues.\nSubsequently, machine learning models built upon these embeddings demonstrated\noutstanding performance on real-world data. In our evaluation, supervised\nthreat detection achieves a precision of 99.0%, and semi-supervised anomaly\ndetection attains a precision of 96.9%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advanced Persistent Threats (APTs) have caused significant losses across a\nwide range of sectors, including the theft of sensitive data and harm to system\nintegrity. As attack techniques grow increasingly sophisticated and stealthy,\nthe arms race between cyber defenders and attackers continues to intensify. The\nrevolutionary impact of Large Language Models (LLMs) has opened up numerous\nopportunities in various fields, including cybersecurity. An intriguing\nquestion arises: can the extensive knowledge embedded in LLMs be harnessed for\nprovenance analysis and play a positive role in identifying previously unknown\nmalicious events? To seek a deeper understanding of this issue, we propose a\nnew strategy for taking advantage of LLMs in provenance-based threat detection.\nIn our design, the state-of-the-art LLM offers additional details in provenance\ndata interpretation, leveraging their knowledge of system calls, software\nidentity, and high-level understanding of application execution context. The\nadvanced contextualized embedding capability is further utilized to capture the\nrich semantics of event descriptions. We comprehensively examine the quality of\nthe resulting embeddings, and it turns out that they offer promising avenues.\nSubsequently, machine learning models built upon these embeddings demonstrated\noutstanding performance on real-world data. In our evaluation, supervised\nthreat detection achieves a precision of 99.0%, and semi-supervised anomaly\ndetection attains a precision of 96.9%."
                },
                "authors": [
                    {
                        "name": "Fei Zuo"
                    },
                    {
                        "name": "Junghwan Rhee"
                    },
                    {
                        "name": "Yung Ryn Choe"
                    }
                ],
                "author_detail": {
                    "name": "Yung Ryn Choe"
                },
                "author": "Yung Ryn Choe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18316v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18316v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18313v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18313v1",
                "updated": "2025-03-24T03:32:13Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    3,
                    32,
                    13,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T03:32:13Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    3,
                    32,
                    13,
                    0,
                    83,
                    0
                ],
                "title": "DeepFund: Will LLM be Professional at Fund Investment? A Live Arena\n  Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepFund: Will LLM be Professional at Fund Investment? A Live Arena\n  Perspective"
                },
                "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities across\nvarious domains, but their effectiveness in financial decision making,\nparticularly in fund investment, remains inadequately evaluated. Current\nbenchmarks primarily assess LLMs understanding of financial documents rather\nthan their ability to manage assets or analyze trading opportunities in dynamic\nmarket conditions. A critical limitation in existing evaluation methodologies\nis the backtesting approach, which suffers from information leakage when LLMs\nare evaluated on historical data they may have encountered during pretraining.\nThis paper introduces DeepFund, a comprehensive platform for evaluating LLM\nbased trading strategies in a simulated live environment. Our approach\nimplements a multi agent framework where LLMs serve as both analysts and\nmanagers, creating a realistic simulation of investment decision making. The\nplatform employs a forward testing methodology that mitigates information\nleakage by evaluating models on market data released after their training\ncutoff dates. We provide a web interface that visualizes model performance\nacross different market conditions and investment parameters, enabling detailed\ncomparative analysis. Through DeepFund, we aim to provide a more accurate and\nfair assessment of LLMs capabilities in fund investment, offering insights into\ntheir potential real world applications in financial markets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated impressive capabilities across\nvarious domains, but their effectiveness in financial decision making,\nparticularly in fund investment, remains inadequately evaluated. Current\nbenchmarks primarily assess LLMs understanding of financial documents rather\nthan their ability to manage assets or analyze trading opportunities in dynamic\nmarket conditions. A critical limitation in existing evaluation methodologies\nis the backtesting approach, which suffers from information leakage when LLMs\nare evaluated on historical data they may have encountered during pretraining.\nThis paper introduces DeepFund, a comprehensive platform for evaluating LLM\nbased trading strategies in a simulated live environment. Our approach\nimplements a multi agent framework where LLMs serve as both analysts and\nmanagers, creating a realistic simulation of investment decision making. The\nplatform employs a forward testing methodology that mitigates information\nleakage by evaluating models on market data released after their training\ncutoff dates. We provide a web interface that visualizes model performance\nacross different market conditions and investment parameters, enabling detailed\ncomparative analysis. Through DeepFund, we aim to provide a more accurate and\nfair assessment of LLMs capabilities in fund investment, offering insights into\ntheir potential real world applications in financial markets."
                },
                "authors": [
                    {
                        "name": "Changlun Li"
                    },
                    {
                        "name": "Yao Shi"
                    },
                    {
                        "name": "Yuyu Luo"
                    },
                    {
                        "name": "Nan Tang"
                    }
                ],
                "author_detail": {
                    "name": "Nan Tang"
                },
                "author": "Nan Tang",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18313v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18313v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13990v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13990v4",
                "updated": "2025-03-24T03:15:28Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    3,
                    15,
                    28,
                    0,
                    83,
                    0
                ],
                "published": "2024-11-21T10:00:52Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    10,
                    0,
                    52,
                    3,
                    326,
                    0
                ],
                "title": "Repository-level Code Translation Benchmark Targeting Rust",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Repository-level Code Translation Benchmark Targeting Rust"
                },
                "summary": "Recent advancements in large language models (LLMs) have demonstrated\nimpressive capabilities in code translation, typically evaluated using\nbenchmarks like CodeTransOcean. However, these benchmarks fail to capture\nreal-world complexities by focusing primarily on simple function-level\ntranslations and overlooking repository-level context (e.g., dependencies).\nMoreover, LLMs' effectiveness in translating to newer, low-resource languages\nlike Rust remains largely underexplored. To address this gap, we introduce\nRustRepoTrans, the first repository-level code translation benchmark,\ncomprising 375 tasks translating into Rust from C++, Java, and Python. Using\nthis benchmark, we evaluate four state-of-the-art LLMs, analyzing their errors\nto assess limitations in complex translation scenarios. Among them, Claude-3.5\nperforms best with 43.5% Pass@1, excelling in both basic functionality and\nadditional translation abilities, such as noise robustness and syntactical\ndifference identification. However, even Claude-3.5 experiences a 30.8%\nperformance drop (Pass@1 from 74.3% to 43.5%) when handling repository-level\ncontext compared to previous benchmarks without such context. We also find that\nLLMs struggle with language differences in complex tasks, and dependencies\nfurther increase translation difficulty.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have demonstrated\nimpressive capabilities in code translation, typically evaluated using\nbenchmarks like CodeTransOcean. However, these benchmarks fail to capture\nreal-world complexities by focusing primarily on simple function-level\ntranslations and overlooking repository-level context (e.g., dependencies).\nMoreover, LLMs' effectiveness in translating to newer, low-resource languages\nlike Rust remains largely underexplored. To address this gap, we introduce\nRustRepoTrans, the first repository-level code translation benchmark,\ncomprising 375 tasks translating into Rust from C++, Java, and Python. Using\nthis benchmark, we evaluate four state-of-the-art LLMs, analyzing their errors\nto assess limitations in complex translation scenarios. Among them, Claude-3.5\nperforms best with 43.5% Pass@1, excelling in both basic functionality and\nadditional translation abilities, such as noise robustness and syntactical\ndifference identification. However, even Claude-3.5 experiences a 30.8%\nperformance drop (Pass@1 from 74.3% to 43.5%) when handling repository-level\ncontext compared to previous benchmarks without such context. We also find that\nLLMs struggle with language differences in complex tasks, and dependencies\nfurther increase translation difficulty."
                },
                "authors": [
                    {
                        "name": "Guangsheng Ou"
                    },
                    {
                        "name": "Mingwei Liu"
                    },
                    {
                        "name": "Yuxuan Chen"
                    },
                    {
                        "name": "Xin Peng"
                    },
                    {
                        "name": "Zibin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zibin Zheng"
                },
                "author": "Zibin Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13990v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13990v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18305v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18305v1",
                "updated": "2025-03-24T03:10:34Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    3,
                    10,
                    34,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T03:10:34Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    3,
                    10,
                    34,
                    0,
                    83,
                    0
                ],
                "title": "Enhancing LLM-based Code Translation in Repository Context via Triple\n  Knowledge-Augmented",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing LLM-based Code Translation in Repository Context via Triple\n  Knowledge-Augmented"
                },
                "summary": "Large language models (LLMs) have behaved well in function-level code\ntranslation without repository-level context. However, the performance of LLMs\nin repository-level context code translation remains suboptimal due to complex\ndependencies and context, hindering their adoption in industrial settings. In\nthis work, we propose a novel LLM-based code translation technique K-Trans,\nwhich leverages triple knowledge augmentation to enhance LLM's translation\nquality under repository context in real-world software development. First,\nK-Trans constructs a translation knowledge base by extracting relevant\ninformation from target-language codebases, the repository being translated,\nand prior translation results. Second, for each function to be translated,\nK-Trans retrieves relevant triple knowledge, including target-language code\nsamples, dependency usage examples, and successful translation function pairs,\nserving as references to enhance LLM for translation. Third, K-Trans constructs\na knowledge-augmented translation prompt using the retrieved triple knowledge\nand employs LLMs to generate the translated code while preserving repository\ncontext. It further leverages LLMs for self-debugging, enhancing translation\ncorrectness.\n  The experiments show that K-Trans substantially outperforms the baseline\nadapted from previous work by 19.4%/40.2% relative improvement in pass@1 and\n0.138 in CodeBLEU. It is important to note that the results also demonstrate\nthat each knowledge significantly contributes to K-Trans's effectiveness in\nhandling repository-level context code translation, with dependency usage\nexamples making the most notable contribution. Moreover, as the self-evolution\nprocess progresses, the knowledge base continuously enhances the LLM's\nperformance across various aspects of the repository-level code translation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have behaved well in function-level code\ntranslation without repository-level context. However, the performance of LLMs\nin repository-level context code translation remains suboptimal due to complex\ndependencies and context, hindering their adoption in industrial settings. In\nthis work, we propose a novel LLM-based code translation technique K-Trans,\nwhich leverages triple knowledge augmentation to enhance LLM's translation\nquality under repository context in real-world software development. First,\nK-Trans constructs a translation knowledge base by extracting relevant\ninformation from target-language codebases, the repository being translated,\nand prior translation results. Second, for each function to be translated,\nK-Trans retrieves relevant triple knowledge, including target-language code\nsamples, dependency usage examples, and successful translation function pairs,\nserving as references to enhance LLM for translation. Third, K-Trans constructs\na knowledge-augmented translation prompt using the retrieved triple knowledge\nand employs LLMs to generate the translated code while preserving repository\ncontext. It further leverages LLMs for self-debugging, enhancing translation\ncorrectness.\n  The experiments show that K-Trans substantially outperforms the baseline\nadapted from previous work by 19.4%/40.2% relative improvement in pass@1 and\n0.138 in CodeBLEU. It is important to note that the results also demonstrate\nthat each knowledge significantly contributes to K-Trans's effectiveness in\nhandling repository-level context code translation, with dependency usage\nexamples making the most notable contribution. Moreover, as the self-evolution\nprocess progresses, the knowledge base continuously enhances the LLM's\nperformance across various aspects of the repository-level code translation."
                },
                "authors": [
                    {
                        "name": "Guangsheng Ou"
                    },
                    {
                        "name": "Mingwei Liu"
                    },
                    {
                        "name": "Yuxuan Chen"
                    },
                    {
                        "name": "Xueying Du"
                    },
                    {
                        "name": "Shengbo Wang"
                    },
                    {
                        "name": "Zekai Zhang"
                    },
                    {
                        "name": "Xin Peng"
                    },
                    {
                        "name": "Zibin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zibin Zheng"
                },
                "author": "Zibin Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18305v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18305v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18303v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18303v1",
                "updated": "2025-03-24T03:10:12Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    3,
                    10,
                    12,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T03:10:12Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    3,
                    10,
                    12,
                    0,
                    83,
                    0
                ],
                "title": "How to Capture and Study Conversations Between Research Participants and\n  ChatGPT: GPT for Researchers (g4r.org)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How to Capture and Study Conversations Between Research Participants and\n  ChatGPT: GPT for Researchers (g4r.org)"
                },
                "summary": "As large language models (LLMs) like ChatGPT become increasingly integrated\ninto our everyday lives--from customer service and education to creative work\nand personal productivity--understanding how people interact with these AI\nsystems has become a pressing issue. Despite the widespread use of LLMs,\nresearchers lack standardized tools for systematically studying people's\ninteractions with LLMs. To address this issue, we introduce GPT for Researchers\n(G4R), or g4r.org, a free website that researchers can use to easily create and\nintegrate a GPT Interface into their studies. At g4r.org, researchers can (1)\nenable their study participants to interact with GPT (such as ChatGPT), (2)\ncustomize GPT Interfaces to guide participants' interactions with GPT (e.g.,\nset constraints on topics or adjust GPT's tone or response style), and (3)\ncapture participants' interactions with GPT by downloading data on messages\nexchanged between participants and GPT. By facilitating study participants'\ninteractions with GPT and providing detailed data on these interactions, G4R\ncan support research on topics such as consumer interactions with AI agents or\nLLMs, AI-assisted decision-making, and linguistic patterns in human-AI\ncommunication. With this goal in mind, we provide a step-by-step guide to using\nG4R at g4r.org.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) like ChatGPT become increasingly integrated\ninto our everyday lives--from customer service and education to creative work\nand personal productivity--understanding how people interact with these AI\nsystems has become a pressing issue. Despite the widespread use of LLMs,\nresearchers lack standardized tools for systematically studying people's\ninteractions with LLMs. To address this issue, we introduce GPT for Researchers\n(G4R), or g4r.org, a free website that researchers can use to easily create and\nintegrate a GPT Interface into their studies. At g4r.org, researchers can (1)\nenable their study participants to interact with GPT (such as ChatGPT), (2)\ncustomize GPT Interfaces to guide participants' interactions with GPT (e.g.,\nset constraints on topics or adjust GPT's tone or response style), and (3)\ncapture participants' interactions with GPT by downloading data on messages\nexchanged between participants and GPT. By facilitating study participants'\ninteractions with GPT and providing detailed data on these interactions, G4R\ncan support research on topics such as consumer interactions with AI agents or\nLLMs, AI-assisted decision-making, and linguistic patterns in human-AI\ncommunication. With this goal in mind, we provide a step-by-step guide to using\nG4R at g4r.org."
                },
                "authors": [
                    {
                        "name": "Jin Kim"
                    }
                ],
                "author_detail": {
                    "name": "Jin Kim"
                },
                "author": "Jin Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18303v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18303v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18296v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18296v1",
                "updated": "2025-03-24T03:02:04Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    3,
                    2,
                    4,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T03:02:04Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    3,
                    2,
                    4,
                    0,
                    83,
                    0
                ],
                "title": "Surgical Action Planning with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Surgical Action Planning with Large Language Models"
                },
                "summary": "In robot-assisted minimally invasive surgery, we introduce the Surgical\nAction Planning (SAP) task, which generates future action plans from visual\ninputs to address the absence of intraoperative predictive planning in current\nintelligent applications. SAP shows great potential for enhancing\nintraoperative guidance and automating procedures. However, it faces challenges\nsuch as understanding instrument-action relationships and tracking surgical\nprogress. Large Language Models (LLMs) show promise in understanding surgical\nvideo content but remain underexplored for predictive decision-making in SAP,\nas they focus mainly on retrospective analysis. Challenges like data privacy,\ncomputational demands, and modality-specific constraints further highlight\nsignificant research gaps. To tackle these challenges, we introduce LLM-SAP, a\nLarge Language Models-based Surgical Action Planning framework that predicts\nfuture actions and generates text responses by interpreting natural language\nprompts of surgical goals. The text responses potentially support surgical\neducation, intraoperative decision-making, procedure documentation, and skill\nanalysis. LLM-SAP integrates two novel modules: the Near-History Focus Memory\nModule (NHF-MM) for modeling historical states and the prompts factory for\naction planning. We evaluate LLM-SAP on our constructed CholecT50-SAP dataset\nusing models like Qwen2.5 and Qwen2-VL, demonstrating its effectiveness in\nnext-action prediction. Pre-trained LLMs are tested zero-shot, and supervised\nfine-tuning (SFT) with LoRA is implemented to address data privacy concerns.\nOur experiments show that Qwen2.5-72B-SFT surpasses Qwen2.5-72B with a 19.3%\nhigher accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In robot-assisted minimally invasive surgery, we introduce the Surgical\nAction Planning (SAP) task, which generates future action plans from visual\ninputs to address the absence of intraoperative predictive planning in current\nintelligent applications. SAP shows great potential for enhancing\nintraoperative guidance and automating procedures. However, it faces challenges\nsuch as understanding instrument-action relationships and tracking surgical\nprogress. Large Language Models (LLMs) show promise in understanding surgical\nvideo content but remain underexplored for predictive decision-making in SAP,\nas they focus mainly on retrospective analysis. Challenges like data privacy,\ncomputational demands, and modality-specific constraints further highlight\nsignificant research gaps. To tackle these challenges, we introduce LLM-SAP, a\nLarge Language Models-based Surgical Action Planning framework that predicts\nfuture actions and generates text responses by interpreting natural language\nprompts of surgical goals. The text responses potentially support surgical\neducation, intraoperative decision-making, procedure documentation, and skill\nanalysis. LLM-SAP integrates two novel modules: the Near-History Focus Memory\nModule (NHF-MM) for modeling historical states and the prompts factory for\naction planning. We evaluate LLM-SAP on our constructed CholecT50-SAP dataset\nusing models like Qwen2.5 and Qwen2-VL, demonstrating its effectiveness in\nnext-action prediction. Pre-trained LLMs are tested zero-shot, and supervised\nfine-tuning (SFT) with LoRA is implemented to address data privacy concerns.\nOur experiments show that Qwen2.5-72B-SFT surpasses Qwen2.5-72B with a 19.3%\nhigher accuracy."
                },
                "authors": [
                    {
                        "name": "Mengya Xu"
                    },
                    {
                        "name": "Zhongzhen Huang"
                    },
                    {
                        "name": "Jie Zhang"
                    },
                    {
                        "name": "Xiaofan Zhang"
                    },
                    {
                        "name": "Qi Dou"
                    }
                ],
                "author_detail": {
                    "name": "Qi Dou"
                },
                "author": "Qi Dou",
                "arxiv_comment": "10 pages,4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18296v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18296v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17003v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17003v2",
                "updated": "2025-03-24T02:58:20Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    2,
                    58,
                    20,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-21T10:09:16Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    10,
                    9,
                    16,
                    4,
                    80,
                    0
                ],
                "title": "A Survey on Personalized Alignment -- The Missing Piece for Large\n  Language Models in Real-World Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Personalized Alignment -- The Missing Piece for Large\n  Language Models in Real-World Applications"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities, yet\ntheir transition to real-world applications reveals a critical limitation: the\ninability to adapt to individual preferences while maintaining alignment with\nuniversal human values. Current alignment techniques adopt a one-size-fits-all\napproach that fails to accommodate users' diverse backgrounds and needs. This\npaper presents the first comprehensive survey of personalized alignment-a\nparadigm that enables LLMs to adapt their behavior within ethical boundaries\nbased on individual preferences. We propose a unified framework comprising\npreference memory management, personalized generation, and feedback-based\nalignment, systematically analyzing implementation approaches and evaluating\ntheir effectiveness across various scenarios. By examining current techniques,\npotential risks, and future challenges, this survey provides a structured\nfoundation for developing more adaptable and ethically-aligned LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities, yet\ntheir transition to real-world applications reveals a critical limitation: the\ninability to adapt to individual preferences while maintaining alignment with\nuniversal human values. Current alignment techniques adopt a one-size-fits-all\napproach that fails to accommodate users' diverse backgrounds and needs. This\npaper presents the first comprehensive survey of personalized alignment-a\nparadigm that enables LLMs to adapt their behavior within ethical boundaries\nbased on individual preferences. We propose a unified framework comprising\npreference memory management, personalized generation, and feedback-based\nalignment, systematically analyzing implementation approaches and evaluating\ntheir effectiveness across various scenarios. By examining current techniques,\npotential risks, and future challenges, this survey provides a structured\nfoundation for developing more adaptable and ethically-aligned LLMs."
                },
                "authors": [
                    {
                        "name": "Jian Guan"
                    },
                    {
                        "name": "Junfei Wu"
                    },
                    {
                        "name": "Jia-Nan Li"
                    },
                    {
                        "name": "Chuanqi Cheng"
                    },
                    {
                        "name": "Wei Wu"
                    }
                ],
                "author_detail": {
                    "name": "Wei Wu"
                },
                "author": "Wei Wu",
                "arxiv_comment": "9 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17003v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17003v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04832v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04832v4",
                "updated": "2025-03-24T02:52:38Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    2,
                    52,
                    38,
                    0,
                    83,
                    0
                ],
                "published": "2024-12-06T07:56:14Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    7,
                    56,
                    14,
                    4,
                    341,
                    0
                ],
                "title": "Neural Representation for Wireless Radiation Field Reconstruction: A 3D\n  Gaussian Splatting Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Representation for Wireless Radiation Field Reconstruction: A 3D\n  Gaussian Splatting Approach"
                },
                "summary": "Wireless channel modeling plays a pivotal role in designing, analyzing, and\noptimizing wireless communication systems. Nevertheless, developing an\neffective channel modeling approach has been a long-standing challenge. This\nissue has been escalated due to denser network deployment, larger antenna\narrays, and broader bandwidth in next-generation networks. To address this\nchallenge, we put forth WRF-GS, a novel framework for channel modeling based on\nwireless radiation field (WRF) reconstruction using 3D Gaussian splatting\n(3D-GS). WRF-GS employs 3D Gaussian primitives and neural networks to capture\nthe interactions between the environment and radio signals, enabling efficient\nWRF reconstruction and visualization of the propagation characteristics. The\nreconstructed WRF can then be used to synthesize the spatial spectrum for\ncomprehensive wireless channel characterization. While WRF-GS demonstrates\nremarkable effectiveness, it faces limitations in capturing high-frequency\nsignal variations caused by complex multipath effects. To overcome these\nlimitations, we propose WRF-GS+, an enhanced framework that integrates\nelectromagnetic wave physics into the neural network design. WRF-GS+ leverages\ndeformable 3D Gaussians to model both static and dynamic components of the WRF,\nsignificantly improving its ability to characterize signal variations. In\naddition, WRF-GS+ enhances the splatting process by simplifying the 3D-GS\nmodeling process and improving computational efficiency. Experimental results\ndemonstrate that both WRF-GS and WRF-GS+ outperform baselines for spatial\nspectrum synthesis, including ray tracing and other deep-learning approaches.\nNotably, WRF-GS+ achieves state-of-the-art performance in the received signal\nstrength indication (RSSI) and channel state information (CSI) prediction\ntasks, surpassing existing methods by more than 0.7 dB and 3.36 dB,\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wireless channel modeling plays a pivotal role in designing, analyzing, and\noptimizing wireless communication systems. Nevertheless, developing an\neffective channel modeling approach has been a long-standing challenge. This\nissue has been escalated due to denser network deployment, larger antenna\narrays, and broader bandwidth in next-generation networks. To address this\nchallenge, we put forth WRF-GS, a novel framework for channel modeling based on\nwireless radiation field (WRF) reconstruction using 3D Gaussian splatting\n(3D-GS). WRF-GS employs 3D Gaussian primitives and neural networks to capture\nthe interactions between the environment and radio signals, enabling efficient\nWRF reconstruction and visualization of the propagation characteristics. The\nreconstructed WRF can then be used to synthesize the spatial spectrum for\ncomprehensive wireless channel characterization. While WRF-GS demonstrates\nremarkable effectiveness, it faces limitations in capturing high-frequency\nsignal variations caused by complex multipath effects. To overcome these\nlimitations, we propose WRF-GS+, an enhanced framework that integrates\nelectromagnetic wave physics into the neural network design. WRF-GS+ leverages\ndeformable 3D Gaussians to model both static and dynamic components of the WRF,\nsignificantly improving its ability to characterize signal variations. In\naddition, WRF-GS+ enhances the splatting process by simplifying the 3D-GS\nmodeling process and improving computational efficiency. Experimental results\ndemonstrate that both WRF-GS and WRF-GS+ outperform baselines for spatial\nspectrum synthesis, including ray tracing and other deep-learning approaches.\nNotably, WRF-GS+ achieves state-of-the-art performance in the received signal\nstrength indication (RSSI) and channel state information (CSI) prediction\ntasks, surpassing existing methods by more than 0.7 dB and 3.36 dB,\nrespectively."
                },
                "authors": [
                    {
                        "name": "Chaozheng Wen"
                    },
                    {
                        "name": "Jingwen Tong"
                    },
                    {
                        "name": "Yingdong Hu"
                    },
                    {
                        "name": "Zehong Lin"
                    },
                    {
                        "name": "Jun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhang"
                },
                "author": "Jun Zhang",
                "arxiv_comment": "This is an extended journal version of our previous conference paper\n  that was accepted to the IEEE INFOCOM 2025 at arXiv:2412.04832v2. The code\n  for this version is available at https://github.com/wenchaozheng/WRF-GSplus",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04832v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04832v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18293v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18293v1",
                "updated": "2025-03-24T02:32:02Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    2,
                    32,
                    2,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T02:32:02Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    2,
                    32,
                    2,
                    0,
                    83,
                    0
                ],
                "title": "Fact-checking AI-generated news reports: Can LLMs catch their own lies?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fact-checking AI-generated news reports: Can LLMs catch their own lies?"
                },
                "summary": "In this paper, we evaluate the ability of Large Language Models (LLMs) to\nassess the veracity of claims in ''news reports'' generated by themselves or\nother LLMs. Our goal is to determine whether LLMs can effectively fact-check\ntheir own content, using methods similar to those used to verify claims made by\nhumans. Our findings indicate that LLMs are more effective at assessing claims\nin national or international news stories than in local news stories, better at\nevaluating static information than dynamic information, and better at verifying\ntrue claims compared to false ones. We hypothesize that this disparity arises\nbecause the former types of claims are better represented in the training data.\nAdditionally, we find that incorporating retrieved results from a search engine\nin a Retrieval-Augmented Generation (RAG) setting significantly reduces the\nnumber of claims an LLM cannot assess. However, this approach also increases\nthe occurrence of incorrect assessments, partly due to irrelevant or\nlow-quality search results. This diagnostic study highlights the need for\nfuture research on fact-checking machine-generated reports to prioritize\nimproving the precision and relevance of retrieved information to better\nsupport fact-checking efforts. Furthermore, claims about dynamic events and\nlocal news may require human-in-the-loop fact-checking systems to ensure\naccuracy and reliability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we evaluate the ability of Large Language Models (LLMs) to\nassess the veracity of claims in ''news reports'' generated by themselves or\nother LLMs. Our goal is to determine whether LLMs can effectively fact-check\ntheir own content, using methods similar to those used to verify claims made by\nhumans. Our findings indicate that LLMs are more effective at assessing claims\nin national or international news stories than in local news stories, better at\nevaluating static information than dynamic information, and better at verifying\ntrue claims compared to false ones. We hypothesize that this disparity arises\nbecause the former types of claims are better represented in the training data.\nAdditionally, we find that incorporating retrieved results from a search engine\nin a Retrieval-Augmented Generation (RAG) setting significantly reduces the\nnumber of claims an LLM cannot assess. However, this approach also increases\nthe occurrence of incorrect assessments, partly due to irrelevant or\nlow-quality search results. This diagnostic study highlights the need for\nfuture research on fact-checking machine-generated reports to prioritize\nimproving the precision and relevance of retrieved information to better\nsupport fact-checking efforts. Furthermore, claims about dynamic events and\nlocal news may require human-in-the-loop fact-checking systems to ensure\naccuracy and reliability."
                },
                "authors": [
                    {
                        "name": "Jiayi Yao"
                    },
                    {
                        "name": "Haibo Sun"
                    },
                    {
                        "name": "Nianwen Xue"
                    }
                ],
                "author_detail": {
                    "name": "Nianwen Xue"
                },
                "author": "Nianwen Xue",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18293v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18293v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01439v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01439v3",
                "updated": "2025-03-24T02:28:32Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    2,
                    28,
                    32,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-03T11:44:55Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    11,
                    44,
                    55,
                    0,
                    62,
                    0
                ],
                "title": "AVR: Active Vision-Driven Robotic Precision Manipulation with Viewpoint\n  and Focal Length Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AVR: Active Vision-Driven Robotic Precision Manipulation with Viewpoint\n  and Focal Length Optimization"
                },
                "summary": "Robotic manipulation within dynamic environments presents challenges to\nprecise control and adaptability. Traditional fixed-view camera systems face\nchallenges adapting to change viewpoints and scale variations, limiting\nperception and manipulation precision. To tackle these issues, we propose the\nActive Vision-driven Robotic (AVR) framework, a teleoperation hardware solution\nthat supports dynamic viewpoint and dynamic focal length adjustments to\ncontinuously center targets and maintain optimal scale, accompanied by a\ncorresponding algorithm that effectively enhances the success rates of various\noperational tasks. Using the RoboTwin platform with a real-time image\nprocessing plugin, AVR framework improves task success rates by 5%-16% on five\nmanipulation tasks. Physical deployment on a dual-arm system demonstrates in\ncollaborative tasks and 36% precision in screwdriver insertion, outperforming\nbaselines by over 25%. Experimental results confirm that AVR framework enhances\nenvironmental perception, manipulation repeatability (40% $\\le $1 cm error),\nand robustness in complex scenarios, paving the way for future robotic\nprecision manipulation methods in the pursuit of human-level robot dexterity\nand precision.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robotic manipulation within dynamic environments presents challenges to\nprecise control and adaptability. Traditional fixed-view camera systems face\nchallenges adapting to change viewpoints and scale variations, limiting\nperception and manipulation precision. To tackle these issues, we propose the\nActive Vision-driven Robotic (AVR) framework, a teleoperation hardware solution\nthat supports dynamic viewpoint and dynamic focal length adjustments to\ncontinuously center targets and maintain optimal scale, accompanied by a\ncorresponding algorithm that effectively enhances the success rates of various\noperational tasks. Using the RoboTwin platform with a real-time image\nprocessing plugin, AVR framework improves task success rates by 5%-16% on five\nmanipulation tasks. Physical deployment on a dual-arm system demonstrates in\ncollaborative tasks and 36% precision in screwdriver insertion, outperforming\nbaselines by over 25%. Experimental results confirm that AVR framework enhances\nenvironmental perception, manipulation repeatability (40% $\\le $1 cm error),\nand robustness in complex scenarios, paving the way for future robotic\nprecision manipulation methods in the pursuit of human-level robot dexterity\nand precision."
                },
                "authors": [
                    {
                        "name": "Yushan Liu"
                    },
                    {
                        "name": "Shilong Mu"
                    },
                    {
                        "name": "Xintao Chao"
                    },
                    {
                        "name": "Zizhen Li"
                    },
                    {
                        "name": "Yao Mu"
                    },
                    {
                        "name": "Tianxing Chen"
                    },
                    {
                        "name": "Shoujie Li"
                    },
                    {
                        "name": "Chuqiao Lyu"
                    },
                    {
                        "name": "Xiao-ping Zhang"
                    },
                    {
                        "name": "Wenbo Ding"
                    }
                ],
                "author_detail": {
                    "name": "Wenbo Ding"
                },
                "author": "Wenbo Ding",
                "arxiv_comment": "Previously, there were some problems with our experimental data, and\n  the conclusions need to be further verified. Now that we have completed a\n  full-scale experiment and analysis, and added supporting materials to our\n  website, we hope to be able to resubmit it",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01439v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01439v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18292v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18292v1",
                "updated": "2025-03-24T02:28:04Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    2,
                    28,
                    4,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T02:28:04Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    2,
                    28,
                    4,
                    0,
                    83,
                    0
                ],
                "title": "Jenga: Effective Memory Management for Serving LLM with Heterogeneity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Jenga: Effective Memory Management for Serving LLM with Heterogeneity"
                },
                "summary": "Large language models (LLMs) are widely used but expensive to run, especially\nas inference workloads grow. To lower costs, maximizing the request batch size\nby managing GPU memory efficiently is crucial. While PagedAttention has\nrecently been proposed to improve the efficiency of memory management, we find\nthat the growing heterogeneity in the embeddings dimensions, attention, and\naccess patterns of modern LLM architectures introduces new challenges for\nmemory allocation.\n  In this paper, we present Jenga, a novel memory allocation framework for\nheterogeneous embeddings in LLMs. Jenga tackles two key challenges: (1)\nminimizing memory fragmentation when managing embeddings of different sizes,\nand (2) enabling flexible caching and eviction policies tailored to the\nspecific token-dependency patterns of various layers. Jenga employs a two-level\nmemory allocator, leveraging the least common multiple (LCM) of embedding sizes\nto optimize memory usage and providing APIs to express layer-specific caching\nlogic to enhance memory reuse.\n  We implemente Jenga on vLLM, a state-of-the-art LLM inference engine, and\nevaluate it with diverse LLMs, datasets, and GPU configurations. Evaluations\nshow that Jenga improves GPU memory utilization by up to 79.6%, and increases\nserving throughput by up to 4.92x (1.80x on average).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are widely used but expensive to run, especially\nas inference workloads grow. To lower costs, maximizing the request batch size\nby managing GPU memory efficiently is crucial. While PagedAttention has\nrecently been proposed to improve the efficiency of memory management, we find\nthat the growing heterogeneity in the embeddings dimensions, attention, and\naccess patterns of modern LLM architectures introduces new challenges for\nmemory allocation.\n  In this paper, we present Jenga, a novel memory allocation framework for\nheterogeneous embeddings in LLMs. Jenga tackles two key challenges: (1)\nminimizing memory fragmentation when managing embeddings of different sizes,\nand (2) enabling flexible caching and eviction policies tailored to the\nspecific token-dependency patterns of various layers. Jenga employs a two-level\nmemory allocator, leveraging the least common multiple (LCM) of embedding sizes\nto optimize memory usage and providing APIs to express layer-specific caching\nlogic to enhance memory reuse.\n  We implemente Jenga on vLLM, a state-of-the-art LLM inference engine, and\nevaluate it with diverse LLMs, datasets, and GPU configurations. Evaluations\nshow that Jenga improves GPU memory utilization by up to 79.6%, and increases\nserving throughput by up to 4.92x (1.80x on average)."
                },
                "authors": [
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Kuntai Du"
                    },
                    {
                        "name": "Shu Liu"
                    },
                    {
                        "name": "Woosuk Kwon"
                    },
                    {
                        "name": "Xiangxi Mo"
                    },
                    {
                        "name": "Yufeng Wang"
                    },
                    {
                        "name": "Xiaoxuan Liu"
                    },
                    {
                        "name": "Kaichao You"
                    },
                    {
                        "name": "Zhuohan Li"
                    },
                    {
                        "name": "Mingsheng Long"
                    },
                    {
                        "name": "Jidong Zhai"
                    },
                    {
                        "name": "Joseph Gonzalez"
                    },
                    {
                        "name": "Ion Stoica"
                    }
                ],
                "author_detail": {
                    "name": "Ion Stoica"
                },
                "author": "Ion Stoica",
                "arxiv_comment": "16 pages, 19 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18292v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18292v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12767v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12767v3",
                "updated": "2025-03-24T02:24:19Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    2,
                    24,
                    19,
                    0,
                    83,
                    0
                ],
                "published": "2025-02-18T11:31:52Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    11,
                    31,
                    52,
                    1,
                    49,
                    0
                ],
                "title": "R2-KG: General-Purpose Dual-Agent Framework for Reliable Reasoning on\n  Knowledge Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "R2-KG: General-Purpose Dual-Agent Framework for Reliable Reasoning on\n  Knowledge Graphs"
                },
                "summary": "Recent studies have combined Large Language Models (LLMs) with Knowledge\nGraphs (KGs) to enhance reasoning, improving inference accuracy without\nadditional training while mitigating hallucination. However, existing\nframeworks are often rigid, struggling to adapt to KG or task changes. They\nalso rely heavily on powerful LLMs for reliable (i.e., trustworthy) reasoning.\nTo address this, We introduce R2-KG, a plug-and-play, dual-agent framework that\nseparates reasoning into two roles: an Operator (a low-capacity LLM) that\ngathers evidence and a Supervisor (a high-capacity LLM) that makes final\njudgments. This design is cost-efficient for LLM inference while still\nmaintaining strong reasoning accuracy. Additionally, R2-KG employs an\nAbstention mechanism, generating answers only when sufficient evidence is\ncollected from KG, which significantly enhances reliability. Experiments across\nmultiple KG-based reasoning tasks show that R2-KG consistently outperforms\nbaselines in both accuracy and reliability, regardless of the inherent\ncapability of LLMs used as the Operator. Further experiments reveal that the\nsingle-agent version of R2-KG, equipped with a strict self-consistency\nstrategy, achieves significantly higher-than-baseline reliability while\nreducing inference cost. However, it also leads to a higher abstention rate in\ncomplex KGs. Our findings establish R2-KG as a flexible and cost-effective\nsolution for KG-based reasoning. It reduces reliance on high-capacity LLMs\nwhile ensuring trustworthy inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent studies have combined Large Language Models (LLMs) with Knowledge\nGraphs (KGs) to enhance reasoning, improving inference accuracy without\nadditional training while mitigating hallucination. However, existing\nframeworks are often rigid, struggling to adapt to KG or task changes. They\nalso rely heavily on powerful LLMs for reliable (i.e., trustworthy) reasoning.\nTo address this, We introduce R2-KG, a plug-and-play, dual-agent framework that\nseparates reasoning into two roles: an Operator (a low-capacity LLM) that\ngathers evidence and a Supervisor (a high-capacity LLM) that makes final\njudgments. This design is cost-efficient for LLM inference while still\nmaintaining strong reasoning accuracy. Additionally, R2-KG employs an\nAbstention mechanism, generating answers only when sufficient evidence is\ncollected from KG, which significantly enhances reliability. Experiments across\nmultiple KG-based reasoning tasks show that R2-KG consistently outperforms\nbaselines in both accuracy and reliability, regardless of the inherent\ncapability of LLMs used as the Operator. Further experiments reveal that the\nsingle-agent version of R2-KG, equipped with a strict self-consistency\nstrategy, achieves significantly higher-than-baseline reliability while\nreducing inference cost. However, it also leads to a higher abstention rate in\ncomplex KGs. Our findings establish R2-KG as a flexible and cost-effective\nsolution for KG-based reasoning. It reduces reliance on high-capacity LLMs\nwhile ensuring trustworthy inference."
                },
                "authors": [
                    {
                        "name": "Sumin Jo"
                    },
                    {
                        "name": "Junseong Choi"
                    },
                    {
                        "name": "Jiho Kim"
                    },
                    {
                        "name": "Edward Choi"
                    }
                ],
                "author_detail": {
                    "name": "Edward Choi"
                },
                "author": "Edward Choi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12767v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12767v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16731v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16731v2",
                "updated": "2025-03-24T02:20:54Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    2,
                    20,
                    54,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-20T22:15:42Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    22,
                    15,
                    42,
                    3,
                    79,
                    0
                ],
                "title": "Design and Implementation of an FPGA-Based Tiled Matrix Multiplication\n  Accelerator for Transformer Self-Attention on the Xilinx KV260 SoM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Design and Implementation of an FPGA-Based Tiled Matrix Multiplication\n  Accelerator for Transformer Self-Attention on the Xilinx KV260 SoM"
                },
                "summary": "Transformer-based large language models (LLMs) rely heavily on intensive\nmatrix multiplications for attention and feed-forward layers, with the Q, K,\nand V linear projections in the Multi-Head Self-Attention (MHA) module\nconstituting a decisive performance bottleneck. In this work, we introduce a\nhighly optimized tiled matrix multiplication accelerator on a\nresource-constrained Xilinx KV260 FPGA that not only addresses this challenge\nbut sets a new standard for efficiency and performance. Our design exploits\npersistent on-chip storage, a robust two-level tiling strategy for maximal data\nreuse, and a systolic-like unrolled compute engine that together deliver\nunparalleled speed and energy efficiency. Integrated with DistilBERT for Q, K,\nand V projections, our accelerator achieves an unequivocal 7x speedup over ARM\nCPU implementations (PyTorch) and an extraordinary 200x improvement over naive\nNumPy, reaching a throughput of up to 3.1~GFLOPs for matrix multiplications on\n(64,768) x (768,3072) matrices while operating at a conservative 100 MHz. These\nresults decisively demonstrate the transformative potential of FPGA-based\nacceleration for critical Transformer operations, paving the way for scalable\nand energy-efficient deep learning inference on edge devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models (LLMs) rely heavily on intensive\nmatrix multiplications for attention and feed-forward layers, with the Q, K,\nand V linear projections in the Multi-Head Self-Attention (MHA) module\nconstituting a decisive performance bottleneck. In this work, we introduce a\nhighly optimized tiled matrix multiplication accelerator on a\nresource-constrained Xilinx KV260 FPGA that not only addresses this challenge\nbut sets a new standard for efficiency and performance. Our design exploits\npersistent on-chip storage, a robust two-level tiling strategy for maximal data\nreuse, and a systolic-like unrolled compute engine that together deliver\nunparalleled speed and energy efficiency. Integrated with DistilBERT for Q, K,\nand V projections, our accelerator achieves an unequivocal 7x speedup over ARM\nCPU implementations (PyTorch) and an extraordinary 200x improvement over naive\nNumPy, reaching a throughput of up to 3.1~GFLOPs for matrix multiplications on\n(64,768) x (768,3072) matrices while operating at a conservative 100 MHz. These\nresults decisively demonstrate the transformative potential of FPGA-based\nacceleration for critical Transformer operations, paving the way for scalable\nand energy-efficient deep learning inference on edge devices."
                },
                "authors": [
                    {
                        "name": "Richie Li"
                    },
                    {
                        "name": "Sicheng Chen"
                    }
                ],
                "author_detail": {
                    "name": "Sicheng Chen"
                },
                "author": "Sicheng Chen",
                "arxiv_comment": "7 pages, 4 figures, 2 tables. Prepared in ACM conference style.\n  Preprint under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16731v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16731v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "B.7.1; C.1.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14002v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14002v3",
                "updated": "2025-03-24T02:20:01Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    2,
                    20,
                    1,
                    0,
                    83,
                    0
                ],
                "published": "2025-01-23T12:14:57Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    12,
                    14,
                    57,
                    3,
                    23,
                    0
                ],
                "title": "Advancing Mathematical Reasoning in Language Models: The Impact of\n  Problem-Solving Data, Data Synthesis Methods, and Training Stages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancing Mathematical Reasoning in Language Models: The Impact of\n  Problem-Solving Data, Data Synthesis Methods, and Training Stages"
                },
                "summary": "Mathematical reasoning remains a challenging area for large language models\n(LLMs), prompting the development of math-specific LLMs such as LLEMMA,\nDeepSeekMath, and Qwen2-Math, among others. These models typically follow a\ntwo-stage training paradigm: pre-training with math-related corpora and\npost-training with problem datasets for supervised fine-tuning (SFT). Despite\nthese efforts, the improvements in mathematical reasoning achieved through\ncontinued pre-training (CPT) are often less significant compared to those\nobtained via SFT. This study addresses this discrepancy by exploring\nalternative strategies during the pre-training phase, focusing on the use of\nproblem-solving data over general mathematical corpora. We investigate three\nprimary research questions: (1) Can problem-solving data enhance the model's\nmathematical reasoning capabilities more effectively than general mathematical\ncorpora during CPT? (2) Are synthetic data from the same source equally\neffective, and which synthesis methods are most efficient? (3) How do the\ncapabilities developed from the same problem-solving data differ between the\nCPT and SFT stages, and what factors contribute to these differences? Our\nfindings indicate that problem-solving data significantly enhances the model's\nmathematical capabilities compared to general mathematical corpora. We also\nidentify effective data synthesis methods, demonstrating that the tutorship\namplification synthesis method achieves the best performance. Furthermore,\nwhile SFT facilitates instruction-following abilities, it underperforms\ncompared to CPT with the same data, which can be partially attributed to its\npoor learning capacity for more challenging problem-solving data. These\ninsights provide valuable guidance for optimizing the mathematical reasoning\ncapabilities of LLMs, culminating in our development of a powerful mathematical\nbase model called MathGPT-8B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mathematical reasoning remains a challenging area for large language models\n(LLMs), prompting the development of math-specific LLMs such as LLEMMA,\nDeepSeekMath, and Qwen2-Math, among others. These models typically follow a\ntwo-stage training paradigm: pre-training with math-related corpora and\npost-training with problem datasets for supervised fine-tuning (SFT). Despite\nthese efforts, the improvements in mathematical reasoning achieved through\ncontinued pre-training (CPT) are often less significant compared to those\nobtained via SFT. This study addresses this discrepancy by exploring\nalternative strategies during the pre-training phase, focusing on the use of\nproblem-solving data over general mathematical corpora. We investigate three\nprimary research questions: (1) Can problem-solving data enhance the model's\nmathematical reasoning capabilities more effectively than general mathematical\ncorpora during CPT? (2) Are synthetic data from the same source equally\neffective, and which synthesis methods are most efficient? (3) How do the\ncapabilities developed from the same problem-solving data differ between the\nCPT and SFT stages, and what factors contribute to these differences? Our\nfindings indicate that problem-solving data significantly enhances the model's\nmathematical capabilities compared to general mathematical corpora. We also\nidentify effective data synthesis methods, demonstrating that the tutorship\namplification synthesis method achieves the best performance. Furthermore,\nwhile SFT facilitates instruction-following abilities, it underperforms\ncompared to CPT with the same data, which can be partially attributed to its\npoor learning capacity for more challenging problem-solving data. These\ninsights provide valuable guidance for optimizing the mathematical reasoning\ncapabilities of LLMs, culminating in our development of a powerful mathematical\nbase model called MathGPT-8B."
                },
                "authors": [
                    {
                        "name": "Zui Chen"
                    },
                    {
                        "name": "Tianqiao Liu"
                    },
                    {
                        "name": "Mi Tian"
                    },
                    {
                        "name": "Qing Tong"
                    },
                    {
                        "name": "Weiqi Luo"
                    },
                    {
                        "name": "Zitao Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zitao Liu"
                },
                "author": "Zitao Liu",
                "arxiv_comment": "ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14002v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14002v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18288v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18288v1",
                "updated": "2025-03-24T02:17:41Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    2,
                    17,
                    41,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T02:17:41Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    2,
                    17,
                    41,
                    0,
                    83,
                    0
                ],
                "title": "Sun-Shine: A Large Language Model for Tibetan Culture",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sun-Shine: A Large Language Model for Tibetan Culture"
                },
                "summary": "Tibetan, a minority language in China, features a highly intricate\ngrammatical structure, characterized by four verb tenses and a tense system\nwith frequent irregularities, contributing to its extensive inflectional\ndiversity. Recently, advances in Large Language Models (LLMs) have transformed\nthe paradigm in many domains. Despite the success in other fields, current LLMs\noften fall short in catering to the needs of domain experts like Tibetans, and\nthe potential of LLMs for Tibetan culture is under-explored. The intrinsic\nreasons are the immense and intricate nature of Tibetan culture as well as the\nnecessity for higher granularity and richness in knowledge. Simultaneously, the\ncomplexity and uniqueness of its grammatical structure, coupled with its status\nas a minority ethnic language, contribute to data scarcity, which remains a\nfundamental challenge. To alleviate these issues, we introduce Llama-Sunshine\n(Sun-Shine), the first large language model for Tibetan culture, which is\nexpert in various Tibetan language processing tasks. Sun-Shine incorporates\nstate-of-the-art model architectures optimized for Tibetan's linguistic\nfeatures. We also propose TIB-STC, a comprehensive dataset comprising diverse\nTibetan texts such as literature, religious scripts, news, and conversational\ndata, which is also the first large-scale dataset for Tibetan culture. Though\ncomprehensive experiments, Sun-Shine not only demonstrates a higher level of\nknowledge expertise for Tibetan culture but also gains preliminary embodied\nintelligence capabilities in Tibetan language processing tasks, like language\nmodeling, text classification, machine translation, and syntactic analysis.\nMoreover, it excels in low-resource scenarios, showcasing strong generalization\ncapabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tibetan, a minority language in China, features a highly intricate\ngrammatical structure, characterized by four verb tenses and a tense system\nwith frequent irregularities, contributing to its extensive inflectional\ndiversity. Recently, advances in Large Language Models (LLMs) have transformed\nthe paradigm in many domains. Despite the success in other fields, current LLMs\noften fall short in catering to the needs of domain experts like Tibetans, and\nthe potential of LLMs for Tibetan culture is under-explored. The intrinsic\nreasons are the immense and intricate nature of Tibetan culture as well as the\nnecessity for higher granularity and richness in knowledge. Simultaneously, the\ncomplexity and uniqueness of its grammatical structure, coupled with its status\nas a minority ethnic language, contribute to data scarcity, which remains a\nfundamental challenge. To alleviate these issues, we introduce Llama-Sunshine\n(Sun-Shine), the first large language model for Tibetan culture, which is\nexpert in various Tibetan language processing tasks. Sun-Shine incorporates\nstate-of-the-art model architectures optimized for Tibetan's linguistic\nfeatures. We also propose TIB-STC, a comprehensive dataset comprising diverse\nTibetan texts such as literature, religious scripts, news, and conversational\ndata, which is also the first large-scale dataset for Tibetan culture. Though\ncomprehensive experiments, Sun-Shine not only demonstrates a higher level of\nknowledge expertise for Tibetan culture but also gains preliminary embodied\nintelligence capabilities in Tibetan language processing tasks, like language\nmodeling, text classification, machine translation, and syntactic analysis.\nMoreover, it excels in low-resource scenarios, showcasing strong generalization\ncapabilities."
                },
                "authors": [
                    {
                        "name": "Cheng Huang"
                    },
                    {
                        "name": "Fan Gao"
                    },
                    {
                        "name": "Nyima Tashi"
                    },
                    {
                        "name": "Yutong Liu"
                    },
                    {
                        "name": "Xiangxiang Wang"
                    },
                    {
                        "name": "Thupten Tsering"
                    },
                    {
                        "name": "Ban Ma-bao"
                    },
                    {
                        "name": "Renzeg Duojie"
                    },
                    {
                        "name": "Gadeng Luosang"
                    },
                    {
                        "name": "Rinchen Dongrub"
                    },
                    {
                        "name": "Dorje Tashi"
                    },
                    {
                        "name": "Xiao Feng"
                    },
                    {
                        "name": "Yongbin Yu"
                    }
                ],
                "author_detail": {
                    "name": "Yongbin Yu"
                },
                "author": "Yongbin Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18288v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18288v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20504v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20504v5",
                "updated": "2025-03-24T02:17:34Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    2,
                    17,
                    34,
                    0,
                    83,
                    0
                ],
                "published": "2024-12-29T15:42:24Z",
                "published_parsed": [
                    2024,
                    12,
                    29,
                    15,
                    42,
                    24,
                    6,
                    364,
                    0
                ],
                "title": "ReTaKe: Reducing Temporal and Knowledge Redundancy for Long Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReTaKe: Reducing Temporal and Knowledge Redundancy for Long Video\n  Understanding"
                },
                "summary": "Video Large Language Models (VideoLLMs) have made significant strides in\nvideo understanding but struggle with long videos due to the limitations of\ntheir backbone LLMs. Existing solutions rely on length extrapolation, which is\nmemory-constrained, or visual token compression, which primarily leverages\nlow-level temporal redundancy while overlooking the more effective high-level\nknowledge redundancy. To address this, we propose $\\textbf{ReTaKe}$, a\ntraining-free method with two novel modules DPSelect and PivotKV, to jointly\nreduce both temporal visual redundancy and knowledge redundancy for video\ncompression. To align with the way of human temporal perception, DPSelect\nidentifies keyframes based on inter-frame distance peaks. To leverage LLMs'\nlearned prior knowledge, PivotKV marks the keyframes as pivots and compress\nnon-pivot frames by pruning low-attention tokens in their KV cache. ReTaKe\nenables VideoLLMs to process 8 times longer frames (up to 2048), outperforming\nsimilar-sized models by 3-5% and even rivaling much larger ones on VideoMME,\nMLVU, LongVideoBench, and LVBench. Moreover, by overlapping compression\noperations with prefilling, ReTaKe introduces only ~10% prefilling latency\noverhead while reducing decoding latency by ~20%. Our code is available at\nhttps://github.com/SCZwangxiao/video-ReTaKe.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Large Language Models (VideoLLMs) have made significant strides in\nvideo understanding but struggle with long videos due to the limitations of\ntheir backbone LLMs. Existing solutions rely on length extrapolation, which is\nmemory-constrained, or visual token compression, which primarily leverages\nlow-level temporal redundancy while overlooking the more effective high-level\nknowledge redundancy. To address this, we propose $\\textbf{ReTaKe}$, a\ntraining-free method with two novel modules DPSelect and PivotKV, to jointly\nreduce both temporal visual redundancy and knowledge redundancy for video\ncompression. To align with the way of human temporal perception, DPSelect\nidentifies keyframes based on inter-frame distance peaks. To leverage LLMs'\nlearned prior knowledge, PivotKV marks the keyframes as pivots and compress\nnon-pivot frames by pruning low-attention tokens in their KV cache. ReTaKe\nenables VideoLLMs to process 8 times longer frames (up to 2048), outperforming\nsimilar-sized models by 3-5% and even rivaling much larger ones on VideoMME,\nMLVU, LongVideoBench, and LVBench. Moreover, by overlapping compression\noperations with prefilling, ReTaKe introduces only ~10% prefilling latency\noverhead while reducing decoding latency by ~20%. Our code is available at\nhttps://github.com/SCZwangxiao/video-ReTaKe."
                },
                "authors": [
                    {
                        "name": "Xiao Wang"
                    },
                    {
                        "name": "Qingyi Si"
                    },
                    {
                        "name": "Jianlong Wu"
                    },
                    {
                        "name": "Shiyu Zhu"
                    },
                    {
                        "name": "Li Cao"
                    },
                    {
                        "name": "Liqiang Nie"
                    }
                ],
                "author_detail": {
                    "name": "Liqiang Nie"
                },
                "author": "Liqiang Nie",
                "arxiv_comment": "Rewrite the methods section. Add more ablation studies and results in\n  LongVideoBench. Update metadata",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20504v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20504v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18276v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18276v1",
                "updated": "2025-03-24T01:46:17Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    1,
                    46,
                    17,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T01:46:17Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    1,
                    46,
                    17,
                    0,
                    83,
                    0
                ],
                "title": "Learning Orientation Field for OSM-Guided Autonomous Navigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Orientation Field for OSM-Guided Autonomous Navigation"
                },
                "summary": "OpenStreetMap (OSM) has gained popularity recently in autonomous navigation\ndue to its public accessibility, lower maintenance costs, and broader\ngeographical coverage. However, existing methods often struggle with noisy OSM\ndata and incomplete sensor observations, leading to inaccuracies in trajectory\nplanning. These challenges are particularly evident in complex driving\nscenarios, such as at intersections or facing occlusions. To address these\nchallenges, we propose a robust and explainable two-stage framework to learn an\nOrientation Field (OrField) for robot navigation by integrating LiDAR scans and\nOSM routes. In the first stage, we introduce the novel representation, OrField,\nwhich can provide orientations for each grid on the map, reasoning jointly from\nnoisy LiDAR scans and OSM routes. To generate a robust OrField, we train a deep\nneural network by encoding a versatile initial OrField and output an optimized\nOrField. Based on OrField, we propose two trajectory planners for OSM-guided\nrobot navigation, called Field-RRT* and Field-Bezier, respectively, in the\nsecond stage by improving the Rapidly Exploring Random Tree (RRT) algorithm and\nBezier curve to estimate the trajectories. Thanks to the robustness of OrField\nwhich captures both global and local information, Field-RRT* and Field-Bezier\ncan generate accurate and reliable trajectories even in challenging conditions.\nWe validate our approach through experiments on the SemanticKITTI dataset and\nour own campus dataset. The results demonstrate the effectiveness of our\nmethod, achieving superior performance in complex and noisy conditions. Our\ncode for network training and real-world deployment is available at\nhttps://github.com/IMRL/OriField.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OpenStreetMap (OSM) has gained popularity recently in autonomous navigation\ndue to its public accessibility, lower maintenance costs, and broader\ngeographical coverage. However, existing methods often struggle with noisy OSM\ndata and incomplete sensor observations, leading to inaccuracies in trajectory\nplanning. These challenges are particularly evident in complex driving\nscenarios, such as at intersections or facing occlusions. To address these\nchallenges, we propose a robust and explainable two-stage framework to learn an\nOrientation Field (OrField) for robot navigation by integrating LiDAR scans and\nOSM routes. In the first stage, we introduce the novel representation, OrField,\nwhich can provide orientations for each grid on the map, reasoning jointly from\nnoisy LiDAR scans and OSM routes. To generate a robust OrField, we train a deep\nneural network by encoding a versatile initial OrField and output an optimized\nOrField. Based on OrField, we propose two trajectory planners for OSM-guided\nrobot navigation, called Field-RRT* and Field-Bezier, respectively, in the\nsecond stage by improving the Rapidly Exploring Random Tree (RRT) algorithm and\nBezier curve to estimate the trajectories. Thanks to the robustness of OrField\nwhich captures both global and local information, Field-RRT* and Field-Bezier\ncan generate accurate and reliable trajectories even in challenging conditions.\nWe validate our approach through experiments on the SemanticKITTI dataset and\nour own campus dataset. The results demonstrate the effectiveness of our\nmethod, achieving superior performance in complex and noisy conditions. Our\ncode for network training and real-world deployment is available at\nhttps://github.com/IMRL/OriField."
                },
                "authors": [
                    {
                        "name": "Yuming Huang"
                    },
                    {
                        "name": "Wei Gao"
                    },
                    {
                        "name": "Zhiyuan Zhang"
                    },
                    {
                        "name": "Maani Ghaffari"
                    },
                    {
                        "name": "Dezhen Song"
                    },
                    {
                        "name": "Cheng-Zhong Xu"
                    },
                    {
                        "name": "Hui Kong"
                    }
                ],
                "author_detail": {
                    "name": "Hui Kong"
                },
                "author": "Hui Kong",
                "arxiv_comment": "14 pages, 12 figures, and 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18276v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18276v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18273v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18273v1",
                "updated": "2025-03-24T01:41:24Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    1,
                    41,
                    24,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T01:41:24Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    1,
                    41,
                    24,
                    0,
                    83,
                    0
                ],
                "title": "Analyzing Islamophobic Discourse Using Semi-Coded Terms and LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analyzing Islamophobic Discourse Using Semi-Coded Terms and LLMs"
                },
                "summary": "Islamophobia started evolving into a global phenomenon by attracting\nfollowers across the globe, particularly in Western societies. Thus,\nunderstanding Islamophobia's global spread and online dissemination is crucial.\nThis paper performs a large-scale analysis of specialized, semi-coded\nIslamophobic terms such as (muzrat, pislam, mudslime, mohammedan, muzzies)\nfloated on extremist social platforms, i.e., 4Chan, Gab, Telegram, etc. First,\nwe use large language models (LLMs) to show their ability to understand these\nterms. Second, using Google Perspective API, we also find that Islamophobic\ntext is more toxic compared to other kinds of hate speech. Finally, we use BERT\ntopic modeling approach to extract different topics and Islamophobic discourse\non these social platforms. Our findings indicate that LLMs understand these\nOut-Of-Vocabulary (OOV) slurs; however, measures are still required to control\nsuch discourse. Our topic modeling also indicates that Islamophobic text is\nfound across various political, conspiratorial, and far-right movements and is\nparticularly directed against Muslim immigrants. Taken altogether, we performed\nthe first study on Islamophobic semi-coded terms and shed a global light on\nIslamophobia.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Islamophobia started evolving into a global phenomenon by attracting\nfollowers across the globe, particularly in Western societies. Thus,\nunderstanding Islamophobia's global spread and online dissemination is crucial.\nThis paper performs a large-scale analysis of specialized, semi-coded\nIslamophobic terms such as (muzrat, pislam, mudslime, mohammedan, muzzies)\nfloated on extremist social platforms, i.e., 4Chan, Gab, Telegram, etc. First,\nwe use large language models (LLMs) to show their ability to understand these\nterms. Second, using Google Perspective API, we also find that Islamophobic\ntext is more toxic compared to other kinds of hate speech. Finally, we use BERT\ntopic modeling approach to extract different topics and Islamophobic discourse\non these social platforms. Our findings indicate that LLMs understand these\nOut-Of-Vocabulary (OOV) slurs; however, measures are still required to control\nsuch discourse. Our topic modeling also indicates that Islamophobic text is\nfound across various political, conspiratorial, and far-right movements and is\nparticularly directed against Muslim immigrants. Taken altogether, we performed\nthe first study on Islamophobic semi-coded terms and shed a global light on\nIslamophobia."
                },
                "authors": [
                    {
                        "name": "Raza Ul Mustafa"
                    },
                    {
                        "name": "Roi Dupart"
                    },
                    {
                        "name": "Gabrielle Smith"
                    },
                    {
                        "name": "Noman Ashraf"
                    },
                    {
                        "name": "Nathalie Japkowicz"
                    }
                ],
                "author_detail": {
                    "name": "Nathalie Japkowicz"
                },
                "author": "Nathalie Japkowicz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18273v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18273v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15299v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15299v2",
                "updated": "2025-03-24T01:31:35Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    1,
                    31,
                    35,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-19T15:21:48Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    15,
                    21,
                    48,
                    2,
                    78,
                    0
                ],
                "title": "Inside-Out: Hidden Factual Knowledge in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inside-Out: Hidden Factual Knowledge in LLMs"
                },
                "summary": "This work presents a framework for assessing whether large language models\n(LLMs) encode more factual knowledge in their parameters than what they express\nin their outputs. While a few studies hint at this possibility, none has\nclearly defined or demonstrated this phenomenon. We first propose a formal\ndefinition of knowledge, quantifying it for a given question as the fraction of\ncorrect-incorrect answer pairs where the correct one is ranked higher. This\ngives rise to external and internal knowledge, depending on the information\nused to score individual answer candidates: either the model's observable\ntoken-level probabilities or its intermediate computations. Hidden knowledge\narises when internal knowledge exceeds external knowledge. We then present a\ncase study, applying this framework to three popular open-weights LLMs in a\nclosed-book QA setup. Our results indicate that: (1) LLMs consistently encode\nmore factual knowledge internally than what they express externally, with an\naverage relative gap of 40%. (2) Surprisingly, some knowledge is so deeply\nhidden that a model can internally know an answer perfectly, yet fail to\ngenerate it even once, despite large-scale repeated sampling of 1,000 answers.\nThis reveals fundamental limitations in the generation capabilities of LLMs,\nwhich (3) put a practical constraint on scaling test-time compute via repeated\nanswer sampling in closed-book QA: significant performance improvements remain\ninaccessible because some answers are practically never sampled, yet if they\nwere, we would be guaranteed to rank them first.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work presents a framework for assessing whether large language models\n(LLMs) encode more factual knowledge in their parameters than what they express\nin their outputs. While a few studies hint at this possibility, none has\nclearly defined or demonstrated this phenomenon. We first propose a formal\ndefinition of knowledge, quantifying it for a given question as the fraction of\ncorrect-incorrect answer pairs where the correct one is ranked higher. This\ngives rise to external and internal knowledge, depending on the information\nused to score individual answer candidates: either the model's observable\ntoken-level probabilities or its intermediate computations. Hidden knowledge\narises when internal knowledge exceeds external knowledge. We then present a\ncase study, applying this framework to three popular open-weights LLMs in a\nclosed-book QA setup. Our results indicate that: (1) LLMs consistently encode\nmore factual knowledge internally than what they express externally, with an\naverage relative gap of 40%. (2) Surprisingly, some knowledge is so deeply\nhidden that a model can internally know an answer perfectly, yet fail to\ngenerate it even once, despite large-scale repeated sampling of 1,000 answers.\nThis reveals fundamental limitations in the generation capabilities of LLMs,\nwhich (3) put a practical constraint on scaling test-time compute via repeated\nanswer sampling in closed-book QA: significant performance improvements remain\ninaccessible because some answers are practically never sampled, yet if they\nwere, we would be guaranteed to rank them first."
                },
                "authors": [
                    {
                        "name": "Zorik Gekhman"
                    },
                    {
                        "name": "Eyal Ben David"
                    },
                    {
                        "name": "Hadas Orgad"
                    },
                    {
                        "name": "Eran Ofek"
                    },
                    {
                        "name": "Yonatan Belinkov"
                    },
                    {
                        "name": "Idan Szpektor"
                    },
                    {
                        "name": "Jonathan Herzig"
                    },
                    {
                        "name": "Roi Reichart"
                    }
                ],
                "author_detail": {
                    "name": "Roi Reichart"
                },
                "author": "Roi Reichart",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15299v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15299v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18253v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18253v1",
                "updated": "2025-03-24T00:34:36Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    0,
                    34,
                    36,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T00:34:36Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    0,
                    34,
                    36,
                    0,
                    83,
                    0
                ],
                "title": "Enhancing Multi-Label Emotion Analysis and Corresponding Intensities for\n  Ethiopian Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Multi-Label Emotion Analysis and Corresponding Intensities for\n  Ethiopian Languages"
                },
                "summary": "In this digital world, people freely express their emotions using different\nsocial media platforms. As a result, modeling and integrating\nemotion-understanding models are vital for various human-computer interaction\ntasks such as decision-making, product and customer feedback analysis,\npolitical promotions, marketing research, and social media monitoring. As users\nexpress different emotions simultaneously in a single instance, annotating\nemotions in a multilabel setting such as the EthioEmo (Belay et al., 2025)\ndataset effectively captures this dynamic. Additionally, incorporating\nintensity, or the degree of emotion, is crucial, as emotions can significantly\ndiffer in their expressive strength and impact. This intensity is significant\nfor assessing whether further action is necessary in decision-making processes,\nespecially concerning negative emotions in applications such as healthcare and\nmental health studies. To enhance the EthioEmo dataset, we include annotations\nfor the intensity of each labeled emotion. Furthermore, we evaluate various\nstate-of-the-art encoder-only Pretrained Language Models (PLMs) and\ndecoder-only Large Language Models (LLMs) to provide comprehensive\nbenchmarking.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this digital world, people freely express their emotions using different\nsocial media platforms. As a result, modeling and integrating\nemotion-understanding models are vital for various human-computer interaction\ntasks such as decision-making, product and customer feedback analysis,\npolitical promotions, marketing research, and social media monitoring. As users\nexpress different emotions simultaneously in a single instance, annotating\nemotions in a multilabel setting such as the EthioEmo (Belay et al., 2025)\ndataset effectively captures this dynamic. Additionally, incorporating\nintensity, or the degree of emotion, is crucial, as emotions can significantly\ndiffer in their expressive strength and impact. This intensity is significant\nfor assessing whether further action is necessary in decision-making processes,\nespecially concerning negative emotions in applications such as healthcare and\nmental health studies. To enhance the EthioEmo dataset, we include annotations\nfor the intensity of each labeled emotion. Furthermore, we evaluate various\nstate-of-the-art encoder-only Pretrained Language Models (PLMs) and\ndecoder-only Large Language Models (LLMs) to provide comprehensive\nbenchmarking."
                },
                "authors": [
                    {
                        "name": "Tadesse Destaw Belay"
                    },
                    {
                        "name": "Dawit Ketema Gete"
                    },
                    {
                        "name": "Abinew Ali Ayele"
                    },
                    {
                        "name": "Olga Kolesnikova"
                    },
                    {
                        "name": "Grigori Sidorov"
                    },
                    {
                        "name": "Seid Muhie Yimam"
                    }
                ],
                "author_detail": {
                    "name": "Seid Muhie Yimam"
                },
                "author": "Seid Muhie Yimam",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18253v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18253v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18242v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18242v1",
                "updated": "2025-03-23T23:47:26Z",
                "updated_parsed": [
                    2025,
                    3,
                    23,
                    23,
                    47,
                    26,
                    6,
                    82,
                    0
                ],
                "published": "2025-03-23T23:47:26Z",
                "published_parsed": [
                    2025,
                    3,
                    23,
                    23,
                    47,
                    26,
                    6,
                    82,
                    0
                ],
                "title": "ShED-HD: A Shannon Entropy Distribution Framework for Lightweight\n  Hallucination Detection on Edge Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ShED-HD: A Shannon Entropy Distribution Framework for Lightweight\n  Hallucination Detection on Edge Devices"
                },
                "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities on a\nbroad array of NLP tasks, but their tendency to produce\nhallucinations$\\unicode{x2013}$plausible-sounding but factually incorrect\ncontent$\\unicode{x2013}$poses severe challenges in high-stakes domains.\nExisting hallucination detection methods either bear the computational cost of\nmultiple inference passes or sacrifice accuracy for efficiency with single-pass\napproaches, neither of which is ideal in resource-constrained environments such\nas edge devices. We propose the Shannon Entropy Distribution Hallucination\nDetector (ShED-HD), a novel hallucination detection framework that bridges this\ngap by classifying sequence-level entropy patterns using a lightweight BiLSTM\narchitecture with single-headed attention. In contrast to prior approaches,\nShED-HD efficiently detects distinctive uncertainty patterns across entire\noutput sequences, preserving contextual awareness. Through in-depth evaluation\non three datasets (BioASQ, TriviaQA, and Jeopardy Questions), we show that\nShED-HD significantly outperforms other computationally efficient approaches in\nthe out-of-distribution setting, while achieving comparable performance in the\nin-distribution setting. ShED-HD facilitates hallucination detection that is\nlow-cost, accurate, and generalizable, improving the credibility of content\ngenerated by LLMs in resource-constrained environments where trustworthy AI\nfunctionality is crucial.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated impressive capabilities on a\nbroad array of NLP tasks, but their tendency to produce\nhallucinations$\\unicode{x2013}$plausible-sounding but factually incorrect\ncontent$\\unicode{x2013}$poses severe challenges in high-stakes domains.\nExisting hallucination detection methods either bear the computational cost of\nmultiple inference passes or sacrifice accuracy for efficiency with single-pass\napproaches, neither of which is ideal in resource-constrained environments such\nas edge devices. We propose the Shannon Entropy Distribution Hallucination\nDetector (ShED-HD), a novel hallucination detection framework that bridges this\ngap by classifying sequence-level entropy patterns using a lightweight BiLSTM\narchitecture with single-headed attention. In contrast to prior approaches,\nShED-HD efficiently detects distinctive uncertainty patterns across entire\noutput sequences, preserving contextual awareness. Through in-depth evaluation\non three datasets (BioASQ, TriviaQA, and Jeopardy Questions), we show that\nShED-HD significantly outperforms other computationally efficient approaches in\nthe out-of-distribution setting, while achieving comparable performance in the\nin-distribution setting. ShED-HD facilitates hallucination detection that is\nlow-cost, accurate, and generalizable, improving the credibility of content\ngenerated by LLMs in resource-constrained environments where trustworthy AI\nfunctionality is crucial."
                },
                "authors": [
                    {
                        "name": "Aneesh Vathul"
                    },
                    {
                        "name": "Daniel Lee"
                    },
                    {
                        "name": "Sheryl Chen"
                    },
                    {
                        "name": "Arthi Tasmia"
                    }
                ],
                "author_detail": {
                    "name": "Arthi Tasmia"
                },
                "author": "Arthi Tasmia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18242v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18242v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]